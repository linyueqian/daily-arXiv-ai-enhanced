<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 227]
- [cs.CV](#cs.CV) [Total: 211]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.SD](#cs.SD) [Total: 21]
- [cs.LG](#cs.LG) [Total: 244]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 22]
- [eess.IV](#eess.IV) [Total: 18]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/pdf/2506.01961)
*Jinzhu Yang*

Main category: cs.CL

TL;DR: The study introduces Prompt-bioMRC, a model combining hard and soft prompts, to improve medical NER, outperforming traditional methods and supporting applications like intelligent diagnosis.


<details>
  <summary>Details</summary>
Motivation: To enhance Named Entity Recognition (NER) in the medical domain using prompt learning methods, leveraging advancements like BioBERT for better medical text processing.

Method: Developed the Prompt-bioMRC model, integrating hard template and soft prompt designs to refine medical entity recognition precision and efficiency.

Result: Experiments on diverse medical datasets show the model outperforms traditional NER methods, validating its efficacy.

Conclusion: The study advances automated medical data processing, enabling accurate information extraction and supporting healthcare decision-making.

Abstract: This study is dedicated to exploring the application of prompt learning
methods to advance Named Entity Recognition (NER) within the medical domain. In
recent years, the emergence of large-scale models has driven significant
progress in NER tasks, particularly with the introduction of the BioBERT
language model, which has greatly enhanced NER capabilities in medical texts.
Our research introduces the Prompt-bioMRC model, which integrates both hard
template and soft prompt designs aimed at refining the precision and efficiency
of medical entity recognition. Through extensive experimentation across diverse
medical datasets, our findings consistently demonstrate that our approach
surpasses traditional models. This enhancement not only validates the efficacy
of our methodology but also highlights its potential to provide reliable
technological support for applications like intelligent diagnosis systems. By
leveraging advanced NER techniques, this study contributes to advancing
automated medical data processing, facilitating more accurate medical
information extraction, and supporting efficient healthcare decision-making
processes.

</details>


### [2] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/pdf/2506.01992)
*Lukas Rauch, Moritz Wirth, Denis Huseljic, Marek Herde, Bernhard Sick, Matthias Aßenmacher*

Main category: cs.CL

TL;DR: The study benchmarks the impact of LLM embedding quality on deep active learning (AL) strategies, showing that diversity-based sampling and high-quality embeddings improve early AL performance, while optimal query strategies depend on embedding quality and task context.


<details>
  <summary>Details</summary>
Motivation: To explore the practicality of deep AL by leveraging frozen LLM embeddings to reduce computational costs and systematically evaluate how embedding quality affects AL query strategies.

Method: Benchmarked five top-performing LLM models from MTEB and two baselines across ten text classification tasks, analyzing the synergy between embedding quality and query strategies like Margin sampling and Badge.

Result: Diversity-based sampling with high-quality embeddings boosts early AL performance. Optimal query strategies vary with embedding quality, with Badge showing robustness and Margin sampling achieving task-specific spikes.

Conclusion: AL strategy effectiveness is context-dependent, heavily influenced by embedding quality and task specifics, necessitating tailored evaluations.

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [3] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/pdf/2506.02000)
*Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma*

Main category: cs.CL

TL;DR: NovelHopQA is a new benchmark for evaluating multi-hop QA over long-context narratives, revealing limitations in current LLMs despite their scale.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with long-context multi-hop reasoning, and existing benchmarks don't jointly test these challenges in natural settings.

Method: A keyword-guided pipeline creates hop-separated QA chains from 64k-128k-token novel excerpts, evaluated on six SOTA models with human validation.

Result: Accuracy drops with increased hops and context length, showing scale alone doesn't ensure robust reasoning. Common failures include missed final-hop integration and long-range drift.

Conclusion: NovelHopQA provides a diagnostic tool to stress-test multi-hop reasoning at scale, highlighting LLM limitations.

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models
and apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We noticed
consistent accuracy drops with increased hops and context length, even in
frontier models-revealing that sheer scale does not guarantee robust reasoning.
Our failure mode analysis highlights common breakdowns, such as missed
final-hop integration and long-range drift. NovelHopQA offers a controlled
diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [4] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/pdf/2506.02005)
*Timothy Do, Pranav Saran, Harshita Poojary, Pranav Prabhu, Sean O'Brien, Vasu Sharma, Kevin Zhu*

Main category: cs.CL

TL;DR: A hybrid model combining mBERT, bidirectional LSTM, and a linear classifier addresses figurative language challenges in low-resource languages like Konkani, achieving 78% accuracy in metaphor classification and 83% in idiom classification using attention head pruning.


<details>
  <summary>Details</summary>
Motivation: To tackle the difficulties figurative language poses for NLP systems, especially in low-resource languages such as Konkani.

Method: A hybrid model integrating mBERT, bidirectional LSTM, and a linear classifier, fine-tuned on a new annotated dataset for metaphor classification, with gradient-based attention head pruning.

Result: 78% accuracy in metaphor classification and 83% in idiom classification.

Conclusion: Attention head pruning is effective for building efficient NLP tools in underrepresented languages.

Abstract: In this paper, we address the persistent challenges that figurative language
expressions pose for natural language processing (NLP) systems, particularly in
low-resource languages such as Konkani. We present a hybrid model that
integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM
and a linear classifier. This architecture is fine-tuned on a newly introduced
annotated dataset for metaphor classification, developed as part of this work.
To improve the model's efficiency, we implement a gradient-based attention head
pruning strategy. For metaphor classification, the pruned model achieves an
accuracy of 78%. We also applied our pruning approach to expand on an existing
idiom classification task, achieving 83% accuracy. These results demonstrate
the effectiveness of attention head pruning for building efficient NLP tools in
underrepresented languages.

</details>


### [5] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/pdf/2506.02018)
*Christopher Lee Lübbers*

Main category: cs.CL

TL;DR: The paper introduces a method using human-ranked data and Direct Preference Optimization (DPO) to improve paraphrase-type generation, achieving higher accuracy and human preference ratings.


<details>
  <summary>Details</summary>
Motivation: Existing paraphrase-type generation methods often misalign with human preferences due to reliance on automated metrics and limited human-annotated data.

Method: Leverages a human-ranked paraphrase-type dataset and integrates DPO to align model outputs with human judgments.

Result: DPO-based training improves paraphrase-type generation accuracy by 3 percentage points and raises human preference ratings by 7 percentage points. A detection model achieves high F1 scores for various paraphrase types.

Conclusion: Preference data and DPO training produce more reliable paraphrases, advancing research toward user-aligned language generation and stronger evaluation frameworks.

Abstract: Paraphrasing re-expresses meaning to enhance applications like text
simplification, machine translation, and question-answering. Specific
paraphrase types facilitate accurate semantic analysis and robust language
models. However, existing paraphrase-type generation methods often misalign
with human preferences due to reliance on automated metrics and limited
human-annotated training data, obscuring crucial aspects of semantic fidelity
and linguistic transformations.
  This study addresses this gap by leveraging a human-ranked paraphrase-type
dataset and integrating Direct Preference Optimization (DPO) to align model
outputs directly with human judgments. DPO-based training increases
paraphrase-type generation accuracy by 3 percentage points over a supervised
baseline and raises human preference ratings by 7 percentage points. A newly
created human-annotated dataset supports more rigorous future evaluations.
Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for
addition/deletion, 0.78 for same polarity substitution, and 0.70 for
punctuation changes.
  These findings demonstrate that preference data and DPO training produce more
reliable, semantically accurate paraphrases, enabling downstream applications
such as improved summarization and more robust question-answering. The PTD
model surpasses automated metrics and provides a more reliable framework for
evaluating paraphrase quality, advancing paraphrase-type research toward
richer, user-aligned language generation and establishing a stronger foundation
for future evaluations grounded in human-centric criteria.

</details>


### [6] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/pdf/2506.02019)
*E Fan, Weizong Wang, Tianhan Zhang*

Main category: cs.CL

TL;DR: ChatCFD automates CFD workflows in OpenFOAM using a language model, enabling users to run simulations via natural language with minimal expertise.


<details>
  <summary>Details</summary>
Motivation: CFD is complex and requires expertise; ChatCFD aims to simplify and automate workflows.

Method: Uses a large language model to automate CFD workflows, integrating OpenFOAM knowledge for validation and error handling.

Result: Successfully reproduces published CFD results, even for complex, unseen configurations.

Conclusion: ChatCFD effectively bridges the gap between CFD complexity and user accessibility, demonstrating adaptability beyond basic tasks.

Abstract: Computational Fluid Dynamics (CFD) is essential for scientific and
engineering advancements but is limited by operational complexity and the need
for extensive expertise. This paper presents ChatCFD, a large language
model-driven pipeline that automates CFD workflows within the OpenFOAM
framework. It enables users to configure and execute complex simulations from
natural language prompts or published literature with minimal expertise. The
innovation is its structured approach to database construction, configuration
validation, and error reflection, integrating CFD and OpenFOAM knowledge with
general language models to improve accuracy and adaptability. Validation shows
ChatCFD can autonomously reproduce published CFD results, handling complex,
unseen configurations beyond basic examples, a task challenging for general
language models.

</details>


### [7] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/pdf/2506.02037)
*Feng Wang, Yiding Sun, Jiaxin Mao, Wei Xue, Danqing Xu*

Main category: cs.CL

TL;DR: FinS-Pilot is a new benchmark for evaluating RAG systems in financial applications, addressing gaps in dynamic data integration and confidentiality.


<details>
  <summary>Details</summary>
Motivation: Current financial RAG benchmarks lack dynamic data and face confidentiality issues, limiting evaluation of LLMs in financial domains.

Method: FinS-Pilot uses real-world financial interactions, combining real-time API data and structured text, organized by intent classification.

Result: The benchmark effectively evaluates LLMs for financial tasks, identifying suitable models for applications like equity analysis.

Conclusion: FinS-Pilot provides a practical framework and dataset, advancing financial NLP research.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [8] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/pdf/2506.02041)
*Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, Jinfeng Bai*

Main category: cs.CL

TL;DR: BranchLoRA improves multimodal continual instruction tuning by addressing parameter inefficiency and catastrophic forgetting in MoELoRA, using asymmetric tuning-freezing and task-specific routers.


<details>
  <summary>Details</summary>
Motivation: Existing MoELoRA methods in MCIT suffer from catastrophic forgetting and parameter inefficiency, limiting performance over sequential tasks.

Method: Proposes BranchLoRA with a tuning-freezing mechanism for intra-task specialization and inter-task collaboration, plus task-specific routers and a task selector for inference.

Result: BranchLoRA outperforms MoELoRA on MCIT benchmarks and maintains performance across MLLM sizes.

Conclusion: BranchLoRA effectively mitigates catastrophic forgetting and enhances efficiency in multimodal continual instruction tuning.

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [9] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/pdf/2506.02058)
*Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su*

Main category: cs.CL

TL;DR: The paper introduces KnowSum, a framework to address the 'evaluation crisis' in LLMs by quantifying unseen knowledge, improving assessment accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM evaluations inconsistently reflect actual capabilities due to overlooking unseen knowledge.

Method: KnowSum uses statistical extrapolation from observed knowledge frequencies to estimate unseen knowledge.

Result: Experiments show significant omitted knowledge in standard evaluations, altering LLM rankings.

Conclusion: KnowSum provides a more comprehensive LLM evaluation, revealing hidden knowledge and improving comparative assessments.

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [10] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/pdf/2506.02951)
*Boyi Li, Zhonghan Zhao, Der-Horng Lee, Gaoang Wang*

Main category: cs.CL

TL;DR: AGP is a task-adaptive multi-agent framework optimizing agent quantity and communication topology, outperforming baselines in performance, adaptability, token efficiency, and training speed.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems lack adaptability due to fixed agent numbers and static communication structures, limiting task performance.

Method: AGP uses a two-stage training strategy: independently training soft-pruning networks for agent-specific graphs, then jointly optimizing hard- and soft-pruning for dynamic configuration.

Result: Achieves state-of-the-art results (+2.58%~9.84%), adapts to tasks, reduces token consumption by 90%+, and trains efficiently (10 steps to surpass baselines).

Conclusion: AGP is a high-performing, adaptable, and efficient framework for multi-agent collaboration, excelling across diverse tasks and LLM architectures.

Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-pruning) and communication topology (soft-pruning).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-pruning networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-pruning and
soft-pruning within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.

</details>


### [11] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/pdf/2506.02126)
*Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, Yuyin Zhou*

Main category: cs.CL

TL;DR: The paper evaluates reasoning in LLMs, focusing on knowledge correctness and reasoning quality in medical and math domains, revealing trade-offs between SFT and RL training methods.


<details>
  <summary>Details</summary>
Motivation: To investigate the transparency and quality of reasoning processes in LLMs beyond final-answer accuracy.

Method: Introduces a framework evaluating knowledge correctness (KI) and reasoning quality (InfoGain), tested on R1-distilled and Qwen models with SFT/RL.

Result: SFT boosts accuracy but reduces reasoning quality; RL improves medical reasoning by pruning irrelevant knowledge.

Conclusion: Training methods impact reasoning differently; RL is effective for medical tasks, while SFT is crucial for domain knowledge.

Abstract: Recent advances in reasoning-enhanced Large Language Models such as
OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex
tasks. However, the quality and transparency of their internal reasoning
processes remain underexplored. This work moves beyond the final-answer
accuracy and investigates step-by-step reasoning in the medical and
mathematical domains by explicitly decomposing the thinking trajectories into
two parts: knowledge and reasoning. Specifically, we introduce a fine-grained
evaluation framework that judges: (1) the correctness of knowledge used
(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured
by Information Gain (InfoGain)). Using this framework, we study R1-distilled
and base Qwen models trained with supervised fine-tuning (SFT) and/or
reinforcement learning (RL) in the medical and math domains. Three intriguing
findings emerge: (1) The general reasoning abilities in R1-distilled models do
not transfer effectively to the medical domain through either SFT or RL. (2)
SFT raises final-answer accuracy in both domains, but often at the cost of
reasoning quality: InfoGain drops by 38.9% on average compared with untrained
models; In the medical domain, however, SFT remains crucial because domain
knowledge is indispensable. (3) RL enhances medical reasoning by pruning
inaccurate or irrelevant knowledge from reasoning paths, thereby improving both
reasoning accuracy and knowledge correctness.

</details>


### [12] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/pdf/2506.02132)
*Michael Li, Nishant Subramani*

Main category: cs.CL

TL;DR: The paper investigates how modern transformer-based language models encode lexical identity and inflectional morphology, revealing consistent patterns across architectures and sizes.


<details>
  <summary>Details</summary>
Motivation: To understand how contemporary large language models (LLMs) represent linguistic information compared to early models like BERT and GPT-2.

Method: Train linear and nonlinear classifiers on layer-wise activations to predict word lemmas and inflectional features across 16 models.

Result: Lexical information is concentrated linearly in early layers and nonlinearly in later layers, while inflectional information remains uniformly accessible and linearly separable.

Conclusion: Transformer models organize linguistic information similarly, suggesting these properties are fundamental for next token prediction and learned early in pretraining.

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [13] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/pdf/2506.02147)
*Joshua Rozner, Leonie Weissweiler, Cory Shain*

Main category: cs.CL

TL;DR: The paper investigates whether language models trained on developmentally plausible data (like BabyLM) can learn constructions, showing they do and perform better on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address doubts about the relevance of language models trained on unrealistic data to human language learning by testing models with plausible data.

Method: Uses Rozner et al.'s methods to evaluate constructional learning in BabyLM challenge models trained on plausible data.

Result: Models represent diverse constructions, including hard cases, and perform better on benchmarks if they better represent constructions.

Conclusion: Constructional learning occurs even with plausible data, and it correlates with model performance, supporting its relevance to human language learning.

Abstract: Construction grammar posits that children acquire constructions (form-meaning
pairings) from the statistics of their environment. Recent work supports this
hypothesis by showing sensitivity to constructions in pretrained language
models (PLMs), including one recent study (Rozner et al., 2025) demonstrating
that constructions shape the PLM's output distribution. However, models under
study have generally been trained on developmentally implausible amounts of
data, casting doubt on their relevance to human language learning. Here we use
Rozner et al.'s methods to evaluate constructional learning in models from the
2024 BabyLM challenge. Our results show that even when trained on
developmentally plausible quantities of data, models represent diverse
constructions, even hard cases that are superficially indistinguishable. We
further find correlational evidence that constructional performance may be
functionally relevant: models that better represent constructions perform
better on the BabyLM benchmarks.

</details>


### [14] [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/pdf/2410.03869)
*Wenxuan Wang, Kuiyi Gao, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu*

Main category: cs.CL

TL;DR: The paper introduces Chain-of-Jailbreak (CoJ), a method to bypass safeguards in text-based image generation models by decomposing malicious queries into sub-queries for iterative editing. It achieves a 60% success rate, outperforming other methods (14%), and proposes Think Twice Prompting to defend against CoJ with 95% effectiveness.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the safety of text-based image generation models (e.g., Stable Diffusion, DALL-E 3) against harmful content generation.

Method: Developed CoJ, a jailbreaking attack that decomposes malicious queries into sub-queries for iterative image generation and editing. Evaluated using CoJ-Bench dataset across multiple safety scenarios and models (GPT-4V, GPT-4o, Gemini 1.5, Gemini 1.5 Pro).

Result: CoJ bypasses safeguards in 60% of cases, significantly outperforming other methods (14%). Think Twice Prompting defends against 95% of CoJ attacks.

Conclusion: CoJ exposes vulnerabilities in image generation models, while Think Twice Prompting offers a robust defense, advancing AI safety research.

Abstract: Text-based image generation models, such as Stable Diffusion and DALL-E 3,
hold significant potential in content creation and publishing workflows, making
them the focus in recent years. Despite their remarkable capability to generate
diverse and vivid images, considerable efforts are being made to prevent the
generation of harmful content, such as abusive, violent, or pornographic
material. To assess the safety of existing models, we introduce a novel
jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises
image generation models through a step-by-step editing process. Specifically,
for malicious queries that cannot bypass the safeguards with a single prompt,
we intentionally decompose the query into multiple sub-queries. The image
generation models are then prompted to generate and iteratively edit images
based on these sub-queries. To evaluate the effectiveness of our CoJ attack
method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine
safety scenarios, three types of editing operations, and three editing
elements. Experiments on four widely-used image generation services provided by
GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack
method can successfully bypass the safeguards of models for over 60% cases,
which significantly outperforms other jailbreaking methods (i.e., 14%).
Further, to enhance these models' safety against our CoJ attack method, we also
propose an effective prompting-based method, Think Twice Prompting, that can
successfully defend over 95% of CoJ attack. We release our dataset and code to
facilitate the AI safety research.

</details>


### [15] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/pdf/2506.02157)
*Amir Hussein, Cihan Xiao, Matthew Wiesner, Dan Povey, Leibny Paola Garcia, Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: HENT-SRT, a hierarchical neural transducer, improves speech translation by addressing word reordering and computational costs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing neural transducers struggle with word reordering and computational inefficiency in speech translation, lagging behind attention-based models.

Method: Proposes HENT-SRT with task factorization, self-distillation, CTC regularization, hierarchical encoder, stateless predictor, pruned loss, and blank penalty.

Result: Achieves state-of-the-art performance on Arabic, Spanish, and Mandarin datasets, narrowing the gap with AED models.

Conclusion: HENT-SRT effectively addresses challenges in neural transducers for speech translation, offering robust and efficient performance.

Abstract: Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.

</details>


### [16] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/pdf/2506.02172)
*Dennis Fucci, Marco Gaido, Matteo Negri, Luisa Bentivogli, Andre Martins, Giuseppe Attanasio*

Main category: cs.CL

TL;DR: The paper investigates whether speech translation (ST) models capture speaker gender information and its impact on translation bias, finding newer architectures lack gender encoding, leading to a masculine default bias.


<details>
  <summary>Details</summary>
Motivation: To explore if ST models retain speaker gender information and how this affects gender assignment in translations, addressing potential biases.

Method: Probing methods are used to assess gender encoding in diverse ST models across English-French/Italian/Spanish language pairs.

Result: Traditional encoder-decoder models capture gender, but newer architectures (speech encoder + machine translation via adapters) do not, leading to a masculine default bias.

Conclusion: Newer ST architectures' lack of gender encoding exacerbates translation bias toward masculine defaults, highlighting a need for bias mitigation.

Abstract: Recent studies on interpreting the hidden states of speech models have shown
their ability to capture speaker-specific features, including gender. Does this
finding also hold for speech translation (ST) models? If so, what are the
implications for the speaker's gender assignment in translation? We address
these questions from an interpretability perspective, using probing methods to
assess gender encoding across diverse ST models. Results on three language
directions (English-French/Italian/Spanish) indicate that while traditional
encoder-decoder models capture gender information, newer architectures --
integrating a speech encoder with a machine translation system via adapters --
do not. We also demonstrate that low gender encoding capabilities result in
systems' tendency toward a masculine default, a translation bias that is more
pronounced in newer architectures.

</details>


### [17] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/pdf/2506.02175)
*Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel*

Main category: cs.CL

TL;DR: AI debate improves judgment accuracy and confidence calibration, especially for biased judges, outperforming single-advisor systems. AI judges with human-like personas achieve higher accuracy than humans or default AI.


<details>
  <summary>Details</summary>
Motivation: Address the risk of AI amplifying misinformation and deepening social divides, particularly in consequential domains like public health, by ensuring AI truthfulness through scalable oversight.

Method: Conducted two studies: one with human judges evaluating COVID-19 factuality claims via AI debate or consultancy, and another with personalized AI judges mimicking human belief systems.

Result: Debate improved accuracy by 10% overall, with mainstream judges gaining +15.2% accuracy and skeptical judges +4.7%. AI judges with personas achieved 78.5% accuracy, surpassing humans (70.1%) and default AI (69.8%).

Conclusion: AI debate is a promising approach for scalable, bias-resilient oversight, leveraging diverse human and AI judgments to enhance truthfulness in contested domains.

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics like public health
where factual accuracy directly impacts well-being. Scalable Oversight aims to
ensure AI truthfulness by enabling humans to supervise systems that may exceed
human capabilities--yet humans themselves hold different beliefs and biases
that impair their judgment. We study whether AI debate can guide biased judges
toward the truth by having two AI systems debate opposing sides of
controversial COVID-19 factuality claims where people hold strong prior
beliefs. We conduct two studies: one with human judges holding either
mainstream or skeptical beliefs evaluating factuality claims through
AI-assisted debate or consultancy protocols, and a second examining the same
problem with personalized AI judges designed to mimic these different human
belief systems. In our human study, we find that debate-where two AI advisor
systems present opposing evidence-based arguments-consistently improves
judgment accuracy and confidence calibration, outperforming consultancy with a
single-advisor system by 10% overall. The improvement is most significant for
judges with mainstream beliefs (+15.2% accuracy), though debate also helps
skeptical judges who initially misjudge claims move toward accurate views
(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like
personas achieve even higher accuracy (78.5%) than human judges (70.1%) and
default AI judges without personas (69.8%), suggesting their potential for
supervising frontier AI models. These findings highlight AI debate as a
promising path toward scalable, bias-resilient oversight--leveraging both
diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [18] [Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/pdf/2502.11184)
*Wenxuan Wang, Xiaoyuan Liu, Kuiyi Gao, Jen-tse Huang, Youliang Yuan, Pinjia He, Shuai Wang, Zhaopeng Tu*

Main category: cs.CL

TL;DR: MMSafeAware is a benchmark for evaluating safety awareness in Multimodal Large Language Models (MLLMs), revealing current models' shortcomings in identifying unsafe content and avoiding over-sensitivity.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of MLLMs is challenging, especially in accurately distinguishing safe from unsafe multimodal content.

Method: Introduces MMSafeAware, a benchmark with 29 safety scenarios and 1500 image-prompt pairs, and evaluates nine MLLMs. Explores three improvement methods: prompting-based approaches, visual contrastive decoding, and vision-centric reasoning fine-tuning.

Result: Current MLLMs are insufficiently safe and overly sensitive, with GPT-4V misclassifying 36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. None of the tested methods achieved satisfactory performance.

Conclusion: Developing robust safety awareness in MLLMs is challenging, requiring further research. The benchmark and data are publicly available to aid future studies.

Abstract: Multimodal Large Language Models (MLLMs) have expanded the capabilities of
traditional language models by enabling interaction through both text and
images. However, ensuring the safety of these models remains a significant
challenge, particularly in accurately identifying whether multimodal content is
safe or unsafe-a capability we term safety awareness. In this paper, we
introduce MMSafeAware, the first comprehensive multimodal safety awareness
benchmark designed to evaluate MLLMs across 29 safety scenarios with 1500
carefully curated image-prompt pairs. MMSafeAware includes both unsafe and
over-safety subsets to assess models abilities to correctly identify unsafe
content and avoid over-sensitivity that can hinder helpfulness. Evaluating nine
widely used MLLMs using MMSafeAware reveals that current models are not
sufficiently safe and often overly sensitive; for example, GPT-4V misclassifies
36.1% of unsafe inputs as safe and 59.9% of benign inputs as unsafe. We further
explore three methods to improve safety awareness-prompting-based approaches,
visual contrastive decoding, and vision-centric reasoning fine-tuning-but find
that none achieve satisfactory performance. Our findings highlight the profound
challenges in developing MLLMs with robust safety awareness, underscoring the
need for further research in this area. All the code and data will be publicly
available to facilitate future research.

</details>


### [19] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/pdf/2506.02181)
*Dennis Fucci, Marco Gaido, Matteo Negri, Mauro Cettolo, Luisa Bentivogli*

Main category: cs.CL

TL;DR: The paper investigates acoustic cues used by a modern Conformer-based ASR system, revealing its reliance on specific features like vowel formants, fricative spectra, and plosive bursts.


<details>
  <summary>Details</summary>
Motivation: To clarify the acoustic cues modern ASR models rely on, addressing gaps in prior limited and outdated studies.

Method: Feature attribution technique applied to analyze plosives, fricatives, and vowels in time and frequency domains.

Result: ASR model prioritizes vowel formants (especially in male speech), sibilant fricative spectra, and plosive bursts.

Conclusion: Findings improve ASR interpretability and identify areas for future research on model robustness.

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [20] [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/pdf/2502.12562)
*Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng*

Main category: cs.CL

TL;DR: SEA is a method for enhancing MLLM security by optimizing embeddings of additional modalities using textual data, achieving high-quality results efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost and limitations of existing methods for securing MLLMs against multimodal threats.

Method: SEA optimizes embeddings of additional modalities through gradient updates to expand textual datasets, enabling multimodal safety alignment with only textual data.

Result: SEA synthesizes high-quality embeddings quickly (24s on RTX3090) and significantly improves MLLM security against multimodal threats. A new benchmark, VA-SafetyBench, validates the challenge.

Conclusion: SEA provides an efficient, low-resource solution for multimodal safety alignment, validated by experiments and a new benchmark.

Abstract: Multimodal Large Language Models (MLLMs) have serious security
vulnerabilities.While safety alignment using multimodal datasets consisting of
text and data of additional modalities can effectively enhance MLLM's security,
it is costly to construct these datasets. Existing low-resource security
alignment methods, including textual alignment, have been found to struggle
with the security risks posed by additional modalities. To address this, we
propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes
embeddings of additional modality through gradient updates to expand textual
datasets. This enables multimodal safety alignment training even when only
textual data is available. Extensive experiments on image, video, and
audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding
on a single RTX3090 GPU within 24 seconds. SEA significantly improves the
security of MLLMs when faced with threats from additional modalities. To assess
the security risks introduced by video and audio, we also introduced a new
benchmark called VA-SafetyBench. High attack success rates across multiple
MLLMs validate its challenge. Our code and data will be available at
https://github.com/ZeroNLP/SEA.

</details>


### [21] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/pdf/2506.02204)
*Lindia Tjuatja, Graham Neubig*

Main category: cs.CL

TL;DR: BehaviorBox automates LM comparison by identifying fine-grained text features where one model outperforms another, revealing insights beyond corpus-level perplexity.


<details>
  <summary>Details</summary>
Motivation: Manual evaluation of language models is challenging due to brittle prompts, vague perplexities, and endless benchmarks. Automated methods are needed to find meaningful differences.

Method: BehaviorBox uses performance-aware contextual embeddings to extract coherent features (e.g., specific word groups or contexts) where one LM excels over another.

Result: Applied to models of varying size, family, and post-training, BehaviorBox identifies specific contexts (e.g., conditional phrases) with performance differences.

Conclusion: BehaviorBox provides a scalable, automated way to uncover nuanced LM performance differences, complementing traditional metrics like perplexity.

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [22] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/pdf/2506.02212)
*Ella Rannon, David Burstein*

Main category: cs.CL

TL;DR: The paper reviews how NLP techniques are applied to biological sequence data (genomics, transcriptomics, proteomics), covering methods like word2vec and transformers, and their potential in bioinformatics.


<details>
  <summary>Details</summary>
Motivation: To explore the adaptation of NLP methods for analyzing biological sequences and their potential to advance bioinformatics.

Method: Examines NLP techniques (e.g., word2vec, transformers) and their application to DNA, RNA, and protein sequences, including tokenization and model architectures.

Result: Highlights the effectiveness of NLP methods in tasks like structure prediction and gene expression analysis, showing promise for large-scale genomic data.

Conclusion: NLP integration into bioinformatics offers significant potential for understanding biological processes across life domains.

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [23] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/pdf/2506.02239)
*Sofoklis Kakouros*

Main category: cs.CL

TL;DR: The paper proposes using word informativeness from a pre-trained language model to identify key speech segments for emotion recognition, improving accuracy by focusing on these segments.


<details>
  <summary>Details</summary>
Motivation: Traditional methods compute acoustic features over entire sentences, missing fine-grained variations. This research aims to enhance emotion recognition by identifying semantically important segments.

Method: The approach uses word informativeness to select key segments, then computes acoustic features (prosodic features, functionals, and self-supervised representations) only for these segments.

Result: Results show improved emotion recognition performance when features are computed on selected segments.

Conclusion: Focusing on semantically important segments enhances emotion recognition accuracy, validating the effectiveness of this method.

Abstract: In emotion recognition from speech, a key challenge lies in identifying
speech signal segments that carry the most relevant acoustic variations for
discerning specific emotions. Traditional approaches compute functionals for
features such as energy and F0 over entire sentences or longer speech portions,
potentially missing essential fine-grained variation in the long-form
statistics. This research investigates the use of word informativeness, derived
from a pre-trained language model, to identify semantically important segments.
Acoustic features are then computed exclusively for these identified segments,
enhancing emotion recognition accuracy. The methodology utilizes standard
acoustic prosodic features, their functionals, and self-supervised
representations. Results indicate a notable improvement in recognition
performance when features are computed on segments selected based on word
informativeness, underscoring the effectiveness of this approach.

</details>


### [24] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/pdf/2506.02264)
*Radin Shayanfar, Chu Fei Luo, Rohan Bhambhoria, Samuel Dahan, Xiaodan Zhu*

Main category: cs.CL

TL;DR: CoDial is a framework that converts expert knowledge into executable dialogue logic, enabling non-technical experts to define and refine system behavior with minimal effort.


<details>
  <summary>Details</summary>
Motivation: Teaching specialized tasks to dialogue systems is costly and technically challenging, requiring frameworks that facilitate collaboration between developers and domain experts.

Method: CoDial uses a structured heterogeneous graph to represent expert knowledge and integrates with guardrailing languages like Colang for interpretable, zero-shot dialogue system specification.

Result: CoDial achieves state-of-the-art performance on the STAR dataset and is competitive on MultiWOZ. It also supports iterative improvement via feedback.

Conclusion: CoDial is a practical tool for expert-guided alignment of LLMs in high-stakes domains, enabling efficient and interpretable dialogue system development.

Abstract: It is often challenging to teach specialized, unseen tasks to dialogue
systems due to the high cost of expert knowledge, training data, and high
technical difficulty. To support domain-specific applications - such as law,
medicine, or finance - it is essential to build frameworks that enable
non-technical experts to define, test, and refine system behaviour with minimal
effort. Achieving this requires cross-disciplinary collaboration between
developers and domain specialists. In this work, we introduce a novel
framework, CoDial (Code for Dialogue), that converts expert knowledge,
represented as a novel structured heterogeneous graph, into executable
conversation logic. CoDial can be easily implemented in existing guardrailing
languages, such as Colang, to enable interpretable, modifiable, and true
zero-shot specification of task-oriented dialogue systems. Empirically, CoDial
achieves state-of-the-art performance on the STAR dataset for inference-based
models and is competitive with similar baselines on the well-known MultiWOZ
dataset. We also demonstrate CoDial's iterative improvement via manual and
LLM-aided feedback, making it a practical tool for expert-guided alignment of
LLMs in high-stakes domains.

</details>


### [25] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/pdf/2506.02279)
*Wenzheng Zhang, Xi Victoria Lin, Karl Stratos, Wen-tau Yih, Mingda Chen*

Main category: cs.CL

TL;DR: ImpRAG integrates retrieval and generation into a unified model, eliminating explicit queries and improving generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems separate retrieval and generation, limiting task generalization. ImpRAG aims to unify these processes implicitly.

Method: ImpRAG divides pretrained decoder-only models into specialized layer groups for simultaneous retrieval and generation, using a two-stage inference process.

Result: ImpRAG achieves 3.6-11.5 improvements in exact match scores on 8 knowledge-intensive tasks, demonstrating strong generalization.

Conclusion: ImpRAG's unified approach and parameter balancing enhance performance, enabling models to articulate information needs without explicit queries.

Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval
and generation as separate processes, requiring explicit textual queries to
connect them. This separation can limit the ability of models to generalize
across diverse tasks. In this work, we propose a query-free RAG system, named
ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG
allows models to implicitly express their information needs, eliminating the
need for human-specified queries. By dividing pretrained decoder-only language
models into specialized layer groups, ImpRAG optimizes retrieval and generation
tasks simultaneously. Our approach employs a two-stage inference process, using
the same model parameters and forward pass for both retrieval and generation,
thereby minimizing the disparity between retrievers and language models.
Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves
3.6-11.5 improvements in exact match scores on unseen tasks with diverse
formats, highlighting its effectiveness in enabling models to articulate their
own information needs and generalize across tasks. Our analysis underscores the
importance of balancing retrieval and generation parameters and leveraging
generation perplexities as retrieval training objectives for enhanced
performance.

</details>


### [26] [DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition](https://arxiv.org/pdf/2506.00422)
*Yui Sudo, Yosuke Fukumoto, Muhammad Shakeel, Yifan Peng, Chyi-Jiunn Lin, Shinji Watanabe*

Main category: cs.CL

TL;DR: DYNAC improves contextual biasing in speech recognition by integrating dynamic vocabulary into non-autoregressive models, reducing inference time with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Address the slow inference speed of dynamic vocabulary in autoregressive models and the dependency-capture limitation in non-autoregressive models.

Method: Proposes DYNAC, a self-conditioned CTC method that integrates dynamic vocabulary into intermediate layers to capture token dependencies.

Result: Reduces real-time factor by 81% with only a 0.1-point degradation in word error rate on LibriSpeech 960 test-clean set.

Conclusion: DYNAC effectively balances speed and accuracy for contextual biasing in speech recognition.

Abstract: Contextual biasing (CB) improves automatic speech recognition for rare and
unseen phrases. Recent studies have introduced dynamic vocabulary, which
represents context phrases as expandable tokens in autoregressive (AR) models.
This method improves CB accuracy but with slow inference speed. While dynamic
vocabulary can be applied to non-autoregressive (NAR) models, such as
connectionist temporal classification (CTC), the conditional independence
assumption fails to capture dependencies between static and dynamic tokens.
This paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a
self-conditioned CTC method that integrates dynamic vocabulary into
intermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC
effectively captures dependencies between static and dynamic tokens while
reducing the real-time factor (RTF). Experimental results show that DYNAC
reduces RTF by 81% with a 0.1-point degradation in word error rate on the
LibriSpeech 960 test-clean set.

</details>


### [27] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/pdf/2506.02283)
*Sofoklis Kakouros, Haoyu Chen*

Main category: cs.CL

TL;DR: The study analyzes prosodic features in tennis post-match interviews to classify match outcomes using self-supervised learning (SSL) models like Wav2Vec 2.0 and HuBERT. Results show SSL representations and prosodic cues (e.g., pitch variability) effectively distinguish winners from losers.


<details>
  <summary>Details</summary>
Motivation: To explore whether prosodic characteristics and SSL representations can classify match outcomes (win/loss) based on post-match interviews, leveraging emotional speech patterns.

Method: Analyzed prosodic features (pitch, intensity) and SSL models (Wav2Vec 2.0, HuBERT) from interview recordings. Used machine learning classifiers to differentiate winners and losers.

Result: SSL representations and prosodic cues (e.g., pitch variability) successfully classified match outcomes, capturing emotional speech patterns.

Conclusion: Prosodic features and SSL models are effective tools for classifying match outcomes in post-match tennis interviews, highlighting the link between speech and emotional states.

Abstract: This study examines the prosodic characteristics associated with winning and
losing in post-match tennis interviews. Additionally, this research explores
the potential to classify match outcomes solely based on post-match interview
recordings using prosodic features and self-supervised learning (SSL)
representations. By analyzing prosodic elements such as pitch and intensity,
alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine
whether an athlete has won or lost their match. Traditional acoustic features
and deep speech representations are extracted from the data, and machine
learning classifiers are employed to distinguish between winning and losing
players. Results indicate that SSL representations effectively differentiate
between winning and losing outcomes, capturing subtle speech patterns linked to
emotional states. At the same time, prosodic cues -- such as pitch variability
-- remain strong indicators of victory.

</details>


### [28] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/pdf/2506.02298)
*Thai Hoang, Kung-Hsiang Huang, Shirley Kokane, Jianguo Zhang, Zuxin Liu, Ming Zhu, Jake Grigsby, Tian Lan, Michael S Ryoo, Chien-Sheng Wu, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles*

Main category: cs.CL

TL;DR: LAM SIMULATOR is a framework for training Large Action Models (LAMs) by enabling autonomous exploration of tasks with real-time feedback, improving performance by up to 49.3%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in training LAMs due to the need for high-quality data, especially for multi-step tasks involving planning and feedback.

Method: Introduces LAM SIMULATOR with a dynamic task query generator, tool collection, and interactive environment for LLM Agents to autonomously solve tasks and generate training data.

Result: Models trained with self-generated datasets achieve up to 49.3% performance improvement on benchmarks like ToolBench and CRMArena.

Conclusion: LAM SIMULATOR efficiently reduces human input in dataset creation, accelerating AI agent development.

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [29] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/pdf/2506.02302)
*Russell Scheinberg, Ameeta Agrawal, Amber Shore, So Young Lee*

Main category: cs.CL

TL;DR: Grammar prompting improves LLMs' ability to apply grammatical rules by having them explain rules first, then use those explanations to judge sentence acceptability.


<details>
  <summary>Details</summary>
Motivation: LLMs can explain grammatical rules but struggle to apply them. This work aims to bridge the gap between rule knowledge and application.

Method: Introduces 'grammar prompting': an LLM explains a syntactic rule, and the explanation is fed back to the target model (LLM or SLM) to judge sentence acceptability.

Result: Substantial improvements on multilingual benchmarks (BLiMP, SLING, RuBLiMP), reducing LLM-SLM accuracy gap by 20-56%.

Conclusion: Grammar prompting is a lightweight, effective method to enhance SLMs' performance, closing the gap with LLMs in multilingual tasks.

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [30] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/pdf/2506.02584)
*Sarenne Wallbridge, Christoph Minixhofer, Catherine Lai, Peter Bell*

Main category: cs.CL

TL;DR: The study uses self-supervised learning to analyze how prosody (intonation, tempo, loudness) contributes to speech structure beyond lexical content, showing its value for longer-term perceptual tasks like emotion recognition.


<details>
  <summary>Details</summary>
Motivation: To understand the role of prosody in speech structure independently of lexical content, leveraging self-supervised learning for deeper analysis.

Method: Proposed a Masked Prosody Model using self-supervised learning to examine acoustic correlates of prosody, tested across perceptual labels like word boundaries and emotion recognition.

Result: The model outperformed traditional features (pitch, energy, voice activity) in perceptual tasks, especially for longer-term structures like emotion.

Conclusion: Self-supervised learning reveals the importance of prosody's temporal granularity and its value for complex perceptual tasks compared to classical methods.

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [31] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/pdf/2506.02321)
*Pegah Alipoormolabashi, Ajay Patel, Niranjan Balasubramanian*

Main category: cs.CL

TL;DR: The paper introduces MAUIk, a fairness measure for authorship attribution, revealing high unfairness in models and linking misattribution risk to authors' positions in latent space.


<details>
  <summary>Details</summary>
Motivation: Address fairness in authorship attribution, as misattribution can harm individuals, especially in forensic settings.

Method: Propose MAUIk to measure unfairness by tracking how often authors are wrongly ranked in the top k. Test five models on two datasets.

Result: All models show high unfairness, with misattribution risk higher for authors near the centroid in latent space.

Conclusion: Highlights potential harm and the need for user communication and risk calibration in deploying such models.

Abstract: Authorship misattribution can have profound consequences in real life. In
forensic settings simply being considered as one of the potential authors of an
evidential piece of text or communication can result in undesirable scrutiny.
This raises a fairness question: Is every author in the candidate pool at equal
risk of misattribution? Standard evaluation measures for authorship attribution
systems do not explicitly account for this notion of fairness. We introduce a
simple measure, Misattribution Unfairness Index (MAUIk), which is based on how
often authors are ranked in the top k for texts they did not write. Using this
measure we quantify the unfairness of five models on two different datasets.
All models exhibit high levels of unfairness with increased risks for some
authors. Furthermore, we find that this unfairness relates to how the models
embed the authors as vectors in the latent search space. In particular, we
observe that the risk of misattribution is higher for authors closer to the
centroid (or center) of the embedded authors in the haystack. These results
indicate the potential for harm and the need for communicating with and
calibrating end users on misattribution risk when building and providing such
models for downstream use.

</details>


### [32] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/pdf/2506.02627)
*Ömer Tarik Özyilmaz, Matt Coler, Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: Fine-tuning OpenAI's Whisper on Arabic dialects improves ASR performance, with small MSA data boosts for smaller models. Dialect-pooled models match dialect-specific ones, aiding low-resource ASR.


<details>
  <summary>Details</summary>
Motivation: Commercial ASR systems struggle with Arabic dialects despite supporting MSA. This work explores fine-tuning Whisper for dialectal speech to address this gap.

Method: Fine-tuned Whisper on five Arabic dialects using Mozilla Common Voice (MSA) and MASC (dialects). Evaluated MSA training size, pre-training benefits, and dialect-specific vs. pooled models.

Result: Small MSA fine-tuning data improves smaller models significantly. MSA pre-training offers minimal benefit. Dialect-pooled models perform comparably to dialect-specific ones.

Conclusion: Pooling dialectal data, when balanced, can mitigate data scarcity in low-resource ASR without major performance loss.

Abstract: Although commercial Arabic automatic speech recognition (ASR) systems support
Modern Standard Arabic (MSA), they struggle with dialectal speech. We
investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic
dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common
Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA
training size effects, benefits of pre-training on MSA data, and
dialect-specific versus dialect-pooled models. We find that small amounts of
MSA fine-tuning data yield substantial improvements for smaller models,
matching larger non-fine-tuned models. While MSA pre-training shows minimal
benefit, suggesting limited shared features between MSA and dialects, our
dialect-pooled models perform comparably to dialect-specific ones. This
indicates that pooling dialectal data, when properly balanced, can help address
data scarcity in low-resource ASR without significant performance loss.

</details>


### [33] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/pdf/2506.02326)
*Berk Atil, Namrata Sureddy, Rebecca J. Passonneau*

Main category: cs.CL

TL;DR: TRuST is a dataset for toxicity detection, benchmarking LLMs on toxicity, target groups, and toxic spans. Fine-tuned models outperform zero-shot/few-shot, but performance gaps exist for some groups.


<details>
  <summary>Details</summary>
Motivation: Addressing toxicity in online content, especially from language models, due to its harmful psychological and social impacts.

Method: Introduces TRuST dataset, merging existing datasets with labels for toxicity, target groups, and toxic spans. Benchmarks LLMs on detection tasks.

Result: Fine-tuned models perform better than zero-shot/few-shot, but performance is low for certain groups. Reasoning doesn't improve results.

Conclusion: LLMs lack strong social reasoning skills, and fine-tuning is key for toxicity detection, though challenges remain for specific groups.

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [34] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/pdf/2506.02894)
*Verena Blaschke, Miriam Winkler, Constantin Förster, Gabriele Wenger-Glemser, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces Betthupferl, a dataset for evaluating ASR models on German dialects, highlighting performance differences in dialectal vs. Standard German transcription.


<details>
  <summary>Details</summary>
Motivation: Dialects are underrepresented in ASR research; the study aims to assess model robustness to dialectal variation.

Method: Created Betthupferl dataset with 4 hours of dialectal and 0.5 hours of Standard German speech, analyzed linguistic differences, and benchmarked multilingual ASR models.

Result: ASR models showed varied performance, with outputs sometimes resembling dialectal transcriptions and sometimes normalizing to Standard German.

Conclusion: The study underscores the need for dialect-inclusive ASR research, revealing models' tendencies to retain or normalize dialectal features.

Abstract: Although Germany has a diverse landscape of dialects, they are
underrepresented in current automatic speech recognition (ASR) research. To
enable studies of how robust models are towards dialectal variation, we present
Betthupferl, an evaluation dataset containing four hours of read speech in
three dialect groups spoken in Southeast Germany (Franconian, Bavarian,
Alemannic), and half an hour of Standard German speech. We provide both
dialectal and Standard German transcriptions, and analyze the linguistic
differences between them. We benchmark several multilingual state-of-the-art
ASR models on speech translation into Standard German, and find differences
between how much the output resembles the dialectal vs. standardized
transcriptions. Qualitative error analyses of the best ASR model reveal that it
sometimes normalizes grammatical differences, but often stays closer to the
dialectal constructions.

</details>


### [35] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/pdf/2506.02338)
*Hyungjoo Chae, Dongjin Kang, Jihyuk Kim, Beong-woo Kwak, Sunghyun Park, Haeju Park, Jinyoung Yeo, Moontae Lee, Kyungjae Lee*

Main category: cs.CL

TL;DR: The paper introduces the Long CoT Collection, a dataset of 100K CoT rationales, to reduce reliance on existing models like R1 for training LRMs. The dataset is annotated using short CoT LLMs and achieves quality close to R1, improving reasoning skills and RL performance.


<details>
  <summary>Details</summary>
Motivation: To advance independent LRM development by reducing dependence on existing models like R1, which limits progress in the field.

Method: Constructs the Long CoT Collection dataset using short CoT LLMs, introduces a pipeline to enhance reasoning strategies, and manages overthinking with controllability over thought budgets.

Result: The dataset matches or slightly underperforms R1 in quality. Training on it improves general reasoning and boosts RL gains (2-3x with RLVR).

Conclusion: The Long CoT Collection provides a viable alternative to R1 for training LRMs, enhancing reasoning and RL performance while reducing reliance on existing models.

Abstract: With the release of R1, a publicly available large reasoning model (LRM),
researchers commonly train new LRMs by training language models on R1's long
chain-of-thought (CoT) inferences. While prior works show that LRMs'
capabilities can be reproduced through direct distillation, the continued
reliance on the existing models (e.g., R1) remains a critical limitation in
advancing the field. As a first step toward independent LRM development, this
paper explores the possibility of constructing a long CoT dataset with LLMs
that are not trained for inference-time scaling. To this end, we present the
Long CoT Collection, a dataset of 100K CoT rationales annotated using existing
short CoT LLMs. We develop a pipeline that induces o1's novel reasoning
strategies into short CoT LLMs, enabling them to think longer and introducing
controllability over the thought budget to better manage the overthinking
problem. Our extensive analyses validate that our dataset achieves quality
comparable to--or slightly below--R1. Furthermore, our experiments demonstrate
that training on our dataset not only strengthens general reasoning skills, but
also provides a strong foundation for reinforcement learning--models
initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [36] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/pdf/2506.02979)
*Atsumoto Ohashi, Shinya Iizuka, Jingjing Jiang, Ryuichiro Higashinaka*

Main category: cs.CL

TL;DR: The paper introduces the first publicly available full-duplex spoken dialogue model for Japanese, built on Moshi (an English model), trained via pre-training and fine-tuning, and enhanced with synthetic data, outperforming baselines in naturalness and meaningfulness.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on full-duplex spoken dialogue systems for Japanese, despite their potential to model bidirectional conversation features like overlaps and backchannels.

Method: A two-stage training process: pre-training on large-scale Japanese spoken dialogue data, followed by fine-tuning on high-quality stereo data, augmented with synthetic data from a multi-stream TTS system.

Result: The model surpasses Japanese baseline models in both naturalness and meaningfulness.

Conclusion: The study successfully develops and validates a high-performing full-duplex spoken dialogue model for Japanese, addressing a gap in research.

Abstract: Full-duplex spoken dialogue systems, which can model simultaneous
bidirectional features of human conversations such as speech overlaps and
backchannels, have attracted significant attention recently. However, the study
of full-duplex spoken dialogue systems for the Japanese language has been
limited, and the research on their development in Japanese remains scarce. In
this paper, we present the first publicly available full-duplex spoken dialogue
model in Japanese, which is built upon Moshi, a full-duplex dialogue model in
English. Our model is trained through a two-stage process: pre-training on a
large-scale spoken dialogue data in Japanese, followed by fine-tuning on
high-quality stereo spoken dialogue data. We further enhance the model's
performance by incorporating synthetic dialogue data generated by a
multi-stream text-to-speech system. Evaluation experiments demonstrate that the
trained model outperforms Japanese baseline models in both naturalness and
meaningfulness.

</details>


### [37] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/pdf/2506.02347)
*Jiaming Li, Yukun Chen, Ziqiang Liu, Minghuan Tan, Lei Zhang, Yunshui Li, Run Luo, Longze Chen, Jing Luo, Ahmadreza Argha, Hamid Alinejad-Rokny, Wei Zhou, Min Yang*

Main category: cs.CL

TL;DR: Storyteller, a novel AI-based story generation method, improves narrative coherence and logical consistency using SVO triplets and dynamic modules (STORYLINE and NEKG), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AI story generation methods lack coherence and consistency, limiting their effectiveness. The paper aims to address this gap by mimicking human cognitive processes.

Method: Storyteller uses SVO triplets for plot structure and integrates dynamic modules (STORYLINE and NEKG) to enhance coherence and consistency during generation.

Result: Storyteller achieves an 84.33% average win rate in human evaluations, excelling in coherence, creativity, engagement, and relevance.

Conclusion: Storyteller significantly advances AI story generation by ensuring logical flow and immersive narratives, outperforming prior methods.

Abstract: Stories are central to human culture, serving to share ideas, preserve
traditions, and foster connections. Automatic story generation, a key
advancement in artificial intelligence (AI), offers new possibilities for
creating personalized content, exploring creative ideas, and enhancing
interactive experiences. However, existing methods struggle to maintain
narrative coherence and logical consistency. This disconnect compromises the
overall storytelling experience, underscoring the need for substantial
improvements. Inspired by human cognitive processes, we introduce Storyteller,
a novel approach that systemically improves the coherence and consistency of
automatically generated stories. Storyteller introduces a plot node structure
based on linguistically grounded subject verb object (SVO) triplets, which
capture essential story events and ensure a consistent logical flow. Unlike
previous methods, Storyteller integrates two dynamic modules, the STORYLINE and
narrative entity knowledge graph (NEKG),that continuously interact with the
story generation process. This integration produces structurally sound,
cohesive and immersive narratives. Extensive experiments demonstrate that
Storyteller significantly outperforms existing approaches, achieving an 84.33%
average win rate through human preference evaluation. At the same time, it is
also far ahead in other aspects including creativity, coherence, engagement,
and relevance.

</details>


### [38] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/pdf/2506.02350)
*Herun Wan, Jiaying Wu, Minnan Luo, Zhi Zeng, Zhixiong Su*

Main category: cs.CL

TL;DR: The paper introduces TruthOverTricks, a framework to evaluate shortcut learning in misinformation detection, and proposes SMF, an LLM-augmented data augmentation method to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Misinformation detectors often rely on shortcuts that fail to generalize, especially with LLMs generating convincing misinformation.

Method: TruthOverTricks evaluates shortcut behaviors (intrinsic/extrinsic) across benchmarks. SMF uses paraphrasing, factual summarization, and sentiment normalization for robustness.

Result: Existing detectors degrade with shortcuts. SMF improves robustness across 16 benchmarks.

Conclusion: SMF enhances semantic understanding over shortcuts, with resources publicly available.

Abstract: Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.

</details>


### [39] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/pdf/2506.02351)
*Jeonghun Kang, Soonmok Kwon, Joonseok Lee, Byung-Hak Kim*

Main category: cs.CL

TL;DR: DIAMOND is an LLM-driven agent for baseball highlight summarization, combining sports analytics and natural language reasoning to outperform traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like WPA or computer vision miss strategic depth and are not scalable, while manual curation is resource-intensive.

Method: DIAMOND integrates sabermetric features (Win Expectancy, WPA, Leverage Index) with an LLM module for context-aware highlight selection.

Result: Evaluated on Korean Baseball Organization League games, DIAMOND improved F1-score from 42.9% (WPA-only) to 84.8%, surpassing baselines.

Conclusion: DIAMOND demonstrates the potential of modular, interpretable agent-based frameworks for sports summarization and beyond.

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [40] [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/pdf/2505.13338)
*Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw*

Main category: cs.CL

TL;DR: A novel framework generates QA datasets for speech-LLMs by integrating contextual reasoning and paralinguistic understanding, validated by strong correlation with human-generated data.


<details>
  <summary>Details</summary>
Motivation: Current speech-LLMs lack contextual reasoning and paralinguistic understanding due to insufficient QA datasets covering both aspects.

Method: Proposes a framework combining pseudo paralinguistic label-based data condensation and LLM-based CPQA generation from in-the-wild speech.

Result: Validated effectiveness with Qwen2-Audio-7B-Instruct, showing strong correlation but revealing limitations in empathetic reasoning.

Conclusion: The framework is pioneering for training robust speech-LLMs with paralinguistic reasoning, highlighting the need for better datasets and models.

Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning
alongside paralinguistic understanding, primarily due to the lack of
Question-Answer (QA) datasets that cover both aspects. We propose a novel
framework for dataset generation from in-the-wild speech data, that integrates
contextual reasoning with paralinguistic information. It consists of a pseudo
paralinguistic label-based data condensation of in-the-wild speech and
LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is
validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct
model on a dataset created by our framework and human-generated CPQA dataset.
The results also reveal the speech-LLM's limitations in handling empathetic
reasoning tasks, highlighting the need for such datasets and more robust
models. The proposed framework is first of its kind and has potential in
training more robust speech-LLMs with paralinguistic reasoning capabilities.

</details>


### [41] [Continual Speech Learning with Fused Speech Features](https://arxiv.org/pdf/2506.01496)
*Guitao Wang, Jinming Zhao, Hao Yang, Guilin Qi, Tongtong Wu, Gholamreza Haffari*

Main category: cs.CL

TL;DR: The paper introduces continuous speech learning to address the adaptation gap in speech models, using Whisper and a gated-fusion layer for dynamic feature selection, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional static methods struggle with dynamic and diverse speech data, necessitating adaptive models.

Method: Uses the Whisper encoder-decoder model with a learnable gated-fusion layer for dynamic feature selection in downstream tasks.

Result: Significant accuracy improvements over traditional methods in six speech processing tasks, with better adaptation to new tasks.

Conclusion: Continuous speech learning with dynamic feature selection effectively bridges the adaptation gap in speech models.

Abstract: Rapid growth in speech data demands adaptive models, as traditional static
methods fail to keep pace with dynamic and diverse speech information. We
introduce continuous speech learning, a new set-up targeting at bridging the
adaptation gap in current speech models. We use the encoder-decoder Whisper
model to standardize speech tasks into a generative format. We integrate a
learnable gated-fusion layer on the top of the encoder to dynamically select
task-specific features for downstream tasks. Our approach improves accuracy
significantly over traditional methods in six speech processing tasks,
demonstrating gains in adapting to new speech tasks without full retraining.

</details>


### [42] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/pdf/2506.02372)
*Hisami Suzuki, Satoru Katsumata, Takashi Kodama, Tetsuro Takahashi, Kouta Nakayama, Satoshi Sekine*

Main category: cs.CL

TL;DR: AnswerCarefully is a dataset for improving the safety of Japanese LLM outputs, featuring 1,800 question-answer pairs. It enhances model safety without losing utility and serves as a benchmark for evaluating 12 Japanese LLMs. The dataset now includes English translations for broader use.


<details>
  <summary>Details</summary>
Motivation: To address the need for culturally appropriate and safe outputs from Japanese LLMs, reflecting Japan's socio-cultural context.

Method: Creation of a manually curated dataset (1,800 question-answer pairs) covering risk categories, used for fine-tuning and evaluating Japanese LLMs.

Result: Improved safety in LLM outputs without compromising general utility; benchmarked 12 Japanese LLMs.

Conclusion: AnswerCarefully effectively enhances LLM safety in Japan and can inspire similar datasets in other languages/regions.

Abstract: In this paper we present AnswerCarefully, a dataset for promoting the safety
and appropriateness of Japanese LLM outputs. The dataset consists of 1,800
pairs of questions and reference answers, where the questions require special
attention in answering. It covers a wide range of risk categories established
in prior English-language datasets, but the data samples are original in that
they are manually created to reflect the socio-cultural context of LLM usage in
Japan. We show that using this dataset for instruction to fine-tune a Japanese
LLM led to improved output safety without compromising the utility of general
responses. We also report the results of a safety evaluation of 12 Japanese
LLMs using this dataset as a benchmark. Finally, we describe the latest update
on the dataset which provides English translations and annotations of the
questions, aimed at facilitating the derivation of similar datasets in
different languages and regions.

</details>


### [43] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/pdf/2506.02378)
*Ukyo Honda, Tatsushi Oka*

Main category: cs.CL

TL;DR: X²-ICL extends X-ICL by exploring explanations for all labels, improving robustness in LLMs for out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: ICL struggles with generalization beyond provided demonstrations; X-ICL improves reliability but lacks comprehensive reasoning.

Method: Extends X-ICL by systematically exploring explanations for all possible labels (X²-ICL).

Result: Significantly improved robustness to out-of-distribution data on multiple NLP datasets.

Conclusion: X²-ICL enhances decision-making robustness in LLMs, outperforming existing ICL approaches.

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [44] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/pdf/2506.02391)
*Chuanghao Ding, Jiaping Wang, Ziqing Yang, Xiaoliang Wang, Dahua Lin, Cam-Tu Nguyen, Fei Tan*

Main category: cs.CL

TL;DR: Consultant Decoding (CD) improves speculative decoding by using token-level likelihoods for verification, achieving 2.5x speedup and reducing large model calls to below 10%.


<details>
  <summary>Details</summary>
Motivation: High rejection rates in speculative decoding (SD) undermine efficiency gains, prompting a need for better verification mechanisms.

Method: CD verifies drafts using token-level likelihoods from the LLM, avoiding repeated validation calls.

Result: CD achieves 2.5x speedup, maintains generation quality, and reduces large model calls significantly.

Conclusion: CD outperforms SD and even the target model, offering a more efficient and effective approach.

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [45] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/pdf/2506.02404)
*Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qianwen Zhang, Di Yin, Xing Sun, Xiao Huang*

Main category: cs.CL

TL;DR: GraphRAG-Bench is introduced to rigorously evaluate GraphRAG models with challenging, domain-specific questions, diverse tasks, and a holistic framework, revealing insights into graph architectures and reasoning improvements.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of GraphRAG models are limited by traditional question-answering datasets, failing to assess reasoning capacity comprehensively.

Method: GraphRAG-Bench features college-level, domain-specific questions requiring multi-hop reasoning, diverse task types, and a holistic evaluation framework.

Result: The benchmark demonstrates the utility of graph-based structuring in improving reasoning capabilities, providing insights into graph architectures and retrieval efficacy.

Conclusion: GraphRAG-Bench offers actionable guidance for the research community by comprehensively evaluating GraphRAG models and their reasoning enhancements.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [46] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/pdf/2506.02412)
*Zhengyuan Liu, Geyu Lin, Hui Li Tan, Huayun Zhang, Yanfeng Lu, Xiaoxue Gao, Stella Xin Yin, He Sun, Hock Huan Goh, Lung Hsiang Wong, Nancy F. Chen*

Main category: cs.CL

TL;DR: SingaKids is a dialogic tutor using AI for language learning in kids, integrating image captioning, multilingual interaction, and speech features, showing effectiveness across four languages.


<details>
  <summary>Details</summary>
Motivation: To address challenges in consistent performance across languages and cultural contexts in AI-driven education, while ensuring kid-friendly design for optimal learning.

Method: Developed SingaKids with dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation, enhanced by pre-training and scaffolding optimization.

Result: Empirical studies show SingaKids effectively supports language learning for elementary students across performance levels.

Conclusion: SingaKids demonstrates robust potential for multilingual, interactive language learning in young learners.

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [47] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/pdf/2506.02425)
*Tairan Liu*

Main category: cs.CL

TL;DR: The study uses NLP to analyze gender inequality in English textbooks from 22 countries, revealing consistent male overrepresentation across metrics like character count, firstness, and named entities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of cross-cultural studies on gender inequality in textbooks and quantify disparities using computational methods.

Method: Applied NLP techniques (TF-IDF, GloVe embeddings) to analyze gender patterns in textbooks from 22 countries across 7 cultural spheres.

Result: Male characters are consistently overrepresented in count, firstness, and named entities, with the Latin cultural sphere showing the least disparity.

Conclusion: Gender inequality in textbooks is a global issue, with male overrepresentation pervasive across regions.

Abstract: Textbooks play a critical role in shaping children's understanding of the
world. While previous studies have identified gender inequality in individual
countries' textbooks, few have examined the issue cross-culturally. This study
applies natural language processing methods to quantify gender inequality in
English textbooks from 22 countries across 7 cultural spheres. Metrics include
character count, firstness (which gender is mentioned first), and TF-IDF word
associations by gender. The analysis also identifies gender patterns in proper
names appearing in TF-IDF word lists, tests whether large language models can
distinguish between gendered word lists, and uses GloVe embeddings to examine
how closely keywords associate with each gender. Results show consistent
overrepresentation of male characters in terms of count, firstness, and named
entities. All regions exhibit gender inequality, with the Latin cultural sphere
showing the least disparity.

</details>


### [48] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/pdf/2506.02426)
*Maryam Berijanian, Kuldeep Singh, Amin Sehati*

Main category: cs.CL

TL;DR: Comparative analysis of three AI agent architectures for relation classification using LLMs, showing multi-agent coordination outperforms few-shot prompting and nears fine-tuned model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in entity relationship classification with limited labeled data and complex relational structures.

Method: Evaluated three architectures: reflective self-evaluation, hierarchical task decomposition, and dynamic example generation with cooperative/adversarial prompting.

Result: Multi-agent coordination consistently outperforms few-shot prompting and approaches fine-tuned model performance.

Conclusion: Provides practical guidance for designing modular, generalizable LLM-based systems for structured relation extraction.

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [49] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/pdf/2506.02431)
*Mahammed Kamruzzaman, Abdullah Al Monsur, Gene Louis Kim, Anshuman Chhabra*

Main category: cs.CL

TL;DR: LLMs exhibit nationality-based emotional stereotypes, often misaligning with human responses and cultural norms, particularly for negative emotions.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs reflect emotional stereotypes when assigned nationality-specific personas and how these align with cultural norms.

Method: Analyzed pre-trained LLMs by assigning nationality-specific personas and comparing emotion attributions with cultural norms and human responses.

Result: Found significant nationality-based differences in emotion attributions (e.g., shame, fear, joy) and misalignment with human responses, especially for negative emotions.

Conclusion: LLMs display reductive and potentially biased emotional stereotypes, highlighting the need for addressing such biases in model outputs.

Abstract: Emotions are a fundamental facet of human experience, varying across
individuals, cultural contexts, and nationalities. Given the recent success of
Large Language Models (LLMs) as role-playing agents, we examine whether LLMs
exhibit emotional stereotypes when assigned nationality-specific personas.
Specifically, we investigate how different countries are represented in
pre-trained LLMs through emotion attributions and whether these attributions
align with cultural norms. Our analysis reveals significant nationality-based
differences, with emotions such as shame, fear, and joy being
disproportionately assigned across regions. Furthermore, we observe notable
misalignment between LLM-generated and human emotional responses, particularly
for negative emotions, highlighting the presence of reductive and potentially
biased stereotypes in LLM outputs.

</details>


### [50] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/pdf/2506.02442)
*Utsav Maskey, Mark Dras, Usman Naseem*

Main category: cs.CL

TL;DR: The paper evaluates LLM safety on long-tail encrypted texts, identifying vulnerabilities to mismatched-generalization attacks and assessing safeguards.


<details>
  <summary>Details</summary>
Motivation: To understand LLM safety in long-tail text scenarios and improve robust safety mechanisms.

Method: A two-dimensional framework assessing instruction refusal and generation safety through comprehensive experiments.

Result: Models decrypting ciphers may fail on safety dimensions, leading to unsafe responses or over-refusal.

Conclusion: The study highlights LLM safety gaps and guides future development of stronger safeguards.

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [51] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/pdf/2506.02449)
*Bo Peng, Zhiheng Wang, Heyang Gong, Chaochao Lu*

Main category: cs.CL

TL;DR: The paper proposes an automatic synthetic data generation method to address the scarcity of high-quality data for personalized dialogue systems, introducing the IP-Dialog benchmark and a systematic evaluation framework.


<details>
  <summary>Details</summary>
Motivation: The challenge of evaluating and improving the ability of dialogue systems to infer user backgrounds implicitly due to data scarcity, labor-intensive traditional methods, and privacy concerns.

Method: A novel approach for automatic synthetic data generation, creation of the IP-Dialog benchmark (10 tasks, 12 user attribute types), and a systematic evaluation framework with four metrics and five causal graphs.

Result: Extensive experiments validate the reliability of the dataset and provide insightful observations on models' reasoning pathways.

Conclusion: The proposed method and benchmark effectively address data scarcity and improve the evaluation of implicit personalization in dialogue systems.

Abstract: In modern dialogue systems, the ability to implicitly infer user backgrounds
from conversations and leverage this information for personalized assistance is
crucial. However, the scarcity of high-quality data remains a fundamental
challenge to evaluating and improving this capability. Traditional dataset
construction methods are labor-intensive, resource-demanding, and raise privacy
concerns. To address these issues, we propose a novel approach for automatic
synthetic data generation and introduce the Implicit Personalized Dialogue
(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12
user attribute types. Additionally, we develop a systematic evaluation
framework with four metrics to assess both attribute awareness and reasoning
capabilities. We further propose five causal graphs to elucidate models'
reasoning pathways during implicit personalization. Extensive experiments yield
insightful observations and prove the reliability of our dataset.

</details>


### [52] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/pdf/2506.02454)
*Zhaorui Yang, Bo Pan, Han Wang, Yiyao Wang, Xingyu Liu, Minfeng Zhu, Bo Zhang, Wei Chen*

Main category: cs.CL

TL;DR: The paper proposes a framework (Multimodal DeepResearcher) and a structured representation (FDV) to enable LLMs to generate interleaved text and visualizations, addressing a gap in existing deep research frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing deep research frameworks focus on text-only content, leaving automated generation of multimodal (text and visualization) reports underexplored.

Method: Introduces FDV for structured visualization representation and Multimodal DeepResearcher, a four-stage framework (researching, exemplar report textualization, planning, multimodal report generation).

Result: Multimodal DeepResearcher achieves an 82% win rate over baselines using Claude 3.7 Sonnet, evaluated on MultimodalReportBench with 100 topics and 5 metrics.

Conclusion: The proposed framework effectively addresses the challenge of generating high-quality multimodal reports, demonstrating significant improvement over existing methods.

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [53] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/pdf/2506.02460)
*Yupeng Qi, Ziyu Lyu, Min Yang, Yanlin Wang, Lu Bai, Lixin Cui*

Main category: cs.CL

TL;DR: MidPO is a Mixture of Experts framework for balancing safety and helpfulness in LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing LLM safety without compromising helpfulness is a critical challenge, as current methods either over-prioritize safety or lack adaptability.

Method: MidPO uses a dual-preference optimization approach, creating safety and helpfulness experts, and integrates them via a dynamic routing mechanism in a MoE framework.

Result: MidPO outperforms state-of-the-art methods in both safety and helpfulness on three datasets.

Conclusion: MidPO effectively balances safety and helpfulness in LLMs, offering a superior solution to current limitations.

Abstract: As large language models (LLMs) are increasingly applied across various
domains, enhancing safety while maintaining the helpfulness of LLMs has become
a critical challenge. Recent studies solve this problem through
safety-constrained online preference optimization or safety-constrained offline
preference optimization. However, the safety-constrained online methods often
suffer from excessive safety, which might reduce helpfulness, while the
safety-constrained offline methods perform poorly in adaptively balancing
safety and helpfulness. To address these limitations, we propose MidPO, a
\textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness
\textbf{\underline{d}}ual \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference
enhanced direct preference optimization approach to transform the base model
into two independent experts, termed safety and helpfulness experts, and
fine-tunes the two independent experts for optimal safety or helpfulness
performance. Secondly, to achieve an effective balance between safety and
helpfulness, MidPO incorporates the two experts into the MoE framework and
designs a dynamic routing mechanism to allocate contributions from each expert
adaptively. We conduct quantitative and qualitative experiments on three
popular datasets to demonstrate the proposed MidPO significantly outperforms
state-of-the-art approaches in both safety and helpfulness. The code and models
will be released.

</details>


### [54] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/pdf/2506.02461)
*Chunkit Chan, Yauwai Yim, Hongchuan Zeng, Zhiying Zou, Xinyuan Cheng, Zhifan Sun, Zheye Deng, Kawai Chung, Yuzhuo Ao, Yixiang Fan, Cheng Jiayang, Ercong Nie, Ginny Y. Wong, Helmut Schmid, Hinrich Schütze, Simon See, Yangqiu Song*

Main category: cs.CL

TL;DR: XToM is a multilingual benchmark evaluating LLMs' Theory of Mind (ToM) across five languages, revealing performance disparities despite strong multilingual language understanding.


<details>
  <summary>Details</summary>
Motivation: Existing ToM evaluations in LLMs are limited to English, ignoring linguistic diversity's impact on cognition.

Method: Developed XToM, a validated multilingual benchmark with diverse task scenarios, to assess LLMs like DeepSeek R1.

Result: LLMs show strong multilingual language understanding but inconsistent ToM performance across languages.

Conclusion: LLMs struggle to replicate human-like mentalizing across diverse linguistic contexts.

Abstract: Theory of Mind (ToM), the ability to infer mental states in others, is
pivotal for human social cognition. Existing evaluations of ToM in LLMs are
largely limited to English, neglecting the linguistic diversity that shapes
human cognition. This limitation raises a critical question: can LLMs exhibit
Multilingual Theory of Mind, which is the capacity to reason about mental
states across diverse linguistic contexts? To address this gap, we present
XToM, a rigorously validated multilingual benchmark that evaluates ToM across
five languages and incorporates diverse, contextually rich task scenarios.
Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a
pronounced dissonance: while models excel in multilingual language
understanding, their ToM performance varies across languages. Our findings
expose limitations in LLMs' ability to replicate human-like mentalizing across
linguistic contexts.

</details>


### [55] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/pdf/2506.02478)
*Zijian Li, Xiaocheng Feng, Huixin Liu, Yichong Huang, Ting Liu, Bing Qin*

Main category: cs.CL

TL;DR: The paper introduces FroM, an adaptive model merging method using the Frobenius norm to reduce task interference in fine-tuning scenarios, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Traditional model merging methods face task interference, especially in parameter-efficient fine-tuning, prompting the need for a more effective solution.

Method: Proposes FroM, which measures model parameters using the Frobenius norm and introduces a hyperparameter for control, eliminating the need for training data.

Result: FroM outperforms baseline methods in various fine-tuning scenarios, effectively reducing task interference.

Conclusion: FroM provides a robust solution for merging fine-tuned models, addressing the limitations of traditional methods.

Abstract: With the development of large language models, fine-tuning has emerged as an
effective method to enhance performance in specific scenarios by injecting
domain-specific knowledge. In this context, model merging techniques provide a
solution for fusing knowledge from multiple fine-tuning models by combining
their parameters. However, traditional methods often encounter task
interference when merging full fine-tuning models, and this problem becomes
even more evident in parameter-efficient fine-tuning scenarios. In this paper,
we introduce an improvement to the RegMean method, which indirectly leverages
the training data to approximate the outputs of the linear layers before and
after merging. We propose an adaptive merging method called FroM, which
directly measures the model parameters using the Frobenius norm, without any
training data. By introducing an additional hyperparameter for control, FroM
outperforms baseline methods across various fine-tuning scenarios, alleviating
the task interference problem.

</details>


### [56] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/pdf/2506.02480)
*Yifan Duan, Yihong Tang, Kehai Chen, Liqiang Nie, Min Zhang*

Main category: cs.CL

TL;DR: ORPP (Optimized Role-Playing Prompt) is a framework for optimizing role-playing prompts to enhance LLM performance, outperforming existing methods with plug-and-play capability.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods are computationally expensive or require strong model capabilities, limiting their applicability.

Method: ORPP optimizes prompts by confining the search space to role-playing scenarios, iteratively refining prompts on a small subset and transferring the optimization to other samples.

Result: ORPP surpasses mainstream prompt optimization methods in performance and integrates well with other methods.

Conclusion: ORPP offers a scalable, efficient, and effective solution for prompt optimization in LLMs.

Abstract: High-quality prompts are crucial for eliciting outstanding performance from
large language models (LLMs) on complex tasks. Existing research has explored
model-driven strategies for prompt optimization. However, these methods often
suffer from high computational overhead or require strong optimization
capabilities from the model itself, which limits their broad applicability.To
address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a
framework that enhances model performance by optimizing and generating
role-playing prompts. The core idea of ORPP is to confine the prompt search
space to role-playing scenarios, thereby fully activating the model's intrinsic
capabilities through carefully crafted, high-quality role-playing prompts.
Specifically, ORPP first performs iterative optimization on a small subset of
training samples to generate high-quality role-playing prompts. Then,
leveraging the model's few-shot learning capability, it transfers the
optimization experience to efficiently generate suitable prompts for the
remaining samples.Our experimental results show that ORPP not only matches but
in most cases surpasses existing mainstream prompt optimization methods in
terms of performance. Notably, ORPP demonstrates superior "plug-and-play"
capability. In most cases, it can be integrated with various other prompt
methods and further enhance their effectiveness.

</details>


### [57] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/pdf/2506.02481)
*Inderjeet Nair, Lu Wang*

Main category: cs.CL

TL;DR: The study explores whether value preferences in LLMs inferred from short-form tests align with those in long-form responses, finding weak correlations and modest gains from alignment.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding value-related risks and preferences in LLMs' long-form, practical outputs compared to short-form tests.

Method: Comparison of value preferences from short-form reactions and long-form responses across five LLMs, varying argument counts in long-form outputs.

Result: Weak correlations between short-form and long-form preferences, and between different long-form settings. Alignment improves consistency modestly.

Conclusion: More robust methods are needed to ensure consistent value expression in diverse LLM applications.

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [58] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/pdf/2506.02483)
*Sina Bagheri Nezhad, Ameeta Agrawal*

Main category: cs.CL

TL;DR: NSAR combines neural and symbolic reasoning to improve multi-target reasoning in long-context scenarios, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with multi-target reasoning in long-context settings where information is scattered.

Method: NSAR extracts symbolic facts and generates Python code for complex reasoning, integrating neural and symbolic approaches.

Result: NSAR outperforms vanilla RAG and advanced prompting in accuracy across seven languages and varied context lengths.

Conclusion: Combining symbolic operations with neural inference enhances robust, interpretable, and scalable multilingual reasoning.

Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning
in long-context scenarios where relevant information is scattered across
extensive documents. To address this challenge, we introduce NeuroSymbolic
Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic
reasoning during inference. NSAR explicitly extracts symbolic facts from text
and generates executable Python code to handle complex reasoning steps. Through
extensive experiments across seven languages and diverse context lengths, we
demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and
advanced prompting strategies in accurately identifying and synthesizing
multiple pieces of information. Our results highlight the effectiveness of
combining explicit symbolic operations with neural inference for robust,
interpretable, and scalable reasoning in multilingual settings.

</details>


### [59] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/pdf/2506.02494)
*Junzhe Zhang, Huixuan Zhang, Xinyu Hu, Li Lin, Mingqi Gao, Shi Qiu, Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper introduces Minos-Corpus, a large-scale multimodal evaluation dataset combining human and GPT data for I2T and T2I tasks, and proposes Minos, a model achieving SoTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing work lacks focus on T2I evaluation capabilities and large-scale human evaluation data.

Method: Developed Minos-Corpus, used Data Selection and Balance, Mix-SFT training, and DPO to build Minos on a 7B backbone.

Result: Minos achieves SoTA performance on average and excels in T2I evaluation.

Conclusion: High-quality human data and joint training on I2T and T2I tasks are crucial for effective multimodal evaluation.

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [60] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/pdf/2506.02503)
*Yongjian Li, HaoCheng Chu, Yukun Yan, Zhenghao Liu, Shi Yu, Zheni Zeng, Ruobing Wang, Sen Song, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: KARE-RAG improves RAG by enhancing knowledge utilization through structured representations, DDPO, and contrastive data generation, boosting performance across tasks with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Factual inconsistencies in RAG due to noisy retrieved documents persist despite advanced retrieval methods, highlighting the need for better generative model processing.

Method: KARE-RAG introduces structured knowledge representations, DDPO for error correction, and a contrastive data generation pipeline to refine knowledge utilization.

Result: The method significantly enhances RAG pipelines, improving in-domain and out-of-domain task performance without compromising general capabilities, even with modest training data.

Conclusion: Improving how models process retrieved content can enhance RAG performance across diverse inference paradigms, with data-efficient optimization achievable through targeted learning.

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access broader knowledge sources, yet factual inconsistencies persist due to
noise in retrieved documents-even with advanced retrieval methods. We
demonstrate that enhancing generative models' capacity to process noisy content
is equally critical for robust performance. In this paper, we present KARE-RAG
(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge
utilization through three key innovations: (1) structured knowledge
representations that facilitate error detection during training, (2) Dense
Direct Preference Optimization (DDPO)-a refined training objective that
prioritizes correction of critical errors, and (3) a contrastive data
generation pipeline that maintains semantic consistency while rectifying
factual inaccuracies. Experiments show our method significantly enhances
standard RAG pipelines across model scales, improving both in-domain and
out-of-domain task performance without compromising general capabilities.
Notably, these gains are achieved with modest training data, suggesting
data-efficient optimization is possible through targeted learning strategies.
Our findings establish a new direction for RAG improvement: by improving how
models learn to process retrieved content, we can enhance performance across
diverse inference paradigms. All data and code will be publicly available on
Github.

</details>


### [61] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/pdf/2506.02510)
*Jie Zhu, Junhui Li, Yalong Wen, Xiandong Li, Lifan Guo, Feng Chen*

Main category: cs.CL

TL;DR: The paper introduces $	exttt{M$^3$FinMeeting}$, a multilingual, multi-sector, multi-task benchmark for evaluating LLMs in financial meeting understanding, addressing gaps in current benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current financial benchmarks lack real-world dynamics of financial meetings, prompting the need for a more comprehensive evaluation tool.

Method: The proposed $	exttt{M$^3$FinMeeting}$ dataset supports English, Chinese, and Japanese, covers multiple industry sectors, and includes summarization, QA pair extraction, and question answering tasks.

Result: Experiments with seven LLMs show significant room for improvement, validating the benchmark's effectiveness.

Conclusion: $	exttt{M$^3$FinMeeting}$ effectively assesses LLMs' financial meeting comprehension, highlighting areas for model enhancement.

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [62] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/pdf/2506.02515)
*Zhuohan Xie, Dhruv Sahnan, Debopriyo Banerjee, Georgi Georgiev, Rushil Thareja, Hachem Madmoun, Jinyan Su, Aaryamonvikram Singh, Yuxia Wang, Rui Xing, Fajri Koto, Haonan Li, Ivan Koychev, Tanmoy Chakraborty, Salem Lahlou, Veselin Stoyanov, Preslav Nakov*

Main category: cs.CL

TL;DR: FinChain is a new benchmark for evaluating multi-step financial reasoning, featuring verifiable Chain-of-Thought (CoT) steps and automatic evaluation via ChainEval. It spans 54 topics across 12 domains and highlights gaps in current LLMs' performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of intermediate reasoning steps in financial tasks, limiting progress in multi-step reasoning.

Method: FinChain introduces parameterized templates for diverse reasoning complexity, executable Python traces for data generation, and ChainEval for automatic evaluation.

Result: Benchmarking 30 LLMs reveals significant room for improvement in multi-step financial reasoning, even for state-of-the-art models.

Conclusion: FinChain fills a critical gap in financial reasoning benchmarks and provides tools for advancing multi-step reasoning capabilities.

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [63] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/pdf/2506.02519)
*Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy*

Main category: cs.CL

TL;DR: COLLATE is a framework to improve small LLMs' reasoning without relying on larger models, using diverse rationales and preference optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of using large LLMs due to legal and transparency issues, and improving small LLMs' innate reasoning.

Method: COLLATE generates diverse rationales from multiple LLM instances, then tunes the model to select the best rationale for task performance.

Result: Outperforms baselines on 5 datasets across maths, NLP, and commonsense reasoning, effective for 1B-8B parameter models.

Conclusion: COLLATE enhances small LLMs' reasoning without distillation from larger models, demonstrating scalability and task-specific benefits.

Abstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions
by generating step-by-step rationales. Prior works have utilized this
capability to improve smaller and cheaper LMs (say, with 7B parameters).
However, various practical constraints, such as copyright and legal issues,
owing to lack of transparency in the pre-training data of large (often closed)
models, prevent their use in commercial settings. Little focus has been given
to improving the innate reasoning ability of smaller models without distilling
information from larger LLMs. To address this, we propose COLLATE, a trainable
framework that tunes a (small) LLM to generate those outputs from a pool of
diverse rationales that selectively improves the downstream task. COLLATE
enforces multiple instances of the same LLM to exhibit distinct behavior and
employs them to generate rationales to obtain diverse outputs. The LLM is then
tuned via preference optimization to choose the candidate rationale which
maximizes the likelihood of ground-truth answer. COLLATE outperforms several
trainable and prompting baselines on 5 datasets across 3 domains: maths problem
solving, natural language inference, and commonsense reasoning. We show the eff
icacy of COLLATE on LLMs from different model families across varying parameter
scales (1B to 8B) and demonstrate the benefit of multiple rationale providers
guided by the end task through ablations. Code is released here
(https://github.com/Sohanpatnaik106/collate).

</details>


### [64] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/pdf/2506.02527)
*Yingying Zhuang, Aman Gupta, Anurag Beniwal*

Main category: cs.CL

TL;DR: The paper proposes a weighted sampling strategy for fine-tuning multilingual embedding models to enhance cross-language knowledge sharing, achieving significant performance improvements in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: High-quality knowledge bases are often limited to a few languages, creating a need for effective embedding models to bridge the gap between high-resource and low-resource languages.

Method: A novel weighted sampling strategy for contrastive learning is introduced to fine-tune multilingual embedding models, enabling retrieval with a monolingual knowledge base.

Result: The method improves performance by up to 31.03% in MRR and 33.98% in Recall@3 compared to standard approaches.

Conclusion: The proposed strategy is language-agnostic and effective for multilingual and code-switching scenarios, enhancing cross-language knowledge transfer.

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [65] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/pdf/2506.02532)
*Jinu Lee, Sagnik Mukherjee, Dilek Hakkani-Tur, Julia Hockenmaier*

Main category: cs.CL

TL;DR: ReasoningFlow is a schema for analyzing semantic structures in complex reasoning traces of large reasoning models (LRMs), represented as directed acyclic graphs.


<details>
  <summary>Details</summary>
Motivation: To understand, evaluate, and enhance the reasoning processes of LRMs by analyzing their complex traces.

Method: Introduces ReasoningFlow, which parses reasoning traces into directed acyclic graphs to identify distinct reasoning patterns.

Result: Human-interpretable representation of reasoning traces, enabling better analysis and improvement of LRMs.

Conclusion: ReasoningFlow offers a promising approach to dissect and improve the reasoning capabilities of LRMs.

Abstract: Large reasoning models (LRMs) generate complex reasoning traces with
planning, reflection, verification, and backtracking. In this work, we
introduce ReasoningFlow, a unified schema for analyzing the semantic structures
of these complex traces. ReasoningFlow parses traces into directed acyclic
graphs, enabling the characterization of distinct reasoning patterns as
subgraph structures. This human-interpretable representation offers promising
applications in understanding, evaluating, and enhancing the reasoning
processes of LRMs.

</details>


### [66] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/pdf/2506.02533)
*Maike Behrendt, Stefan Sylvius Wagner, Carina Weinmann, Marike Bormann, Mira Warne, Stefan Harmeling*

Main category: cs.CL

TL;DR: The paper explores how machine learning can improve the quality of political online discussions by addressing common issues and enhancing deliberation.


<details>
  <summary>Details</summary>
Motivation: Political online participation is growing, but the quality of discussions depends on platform design. Machine learning can help improve deliberation.

Method: The study identifies issues in political online discussions and proposes machine learning solutions to enhance deliberation.

Result: Machine learning can counteract common issues in online discussions, improving the deliberative quality of political participation.

Conclusion: Machine learning offers significant potential to enhance the deliberativeness of political online discussions by addressing design and process challenges.

Abstract: Political online participation in the form of discussing political issues and
exchanging opinions among citizens is gaining importance with more and more
formats being held digitally. To come to a decision, a careful discussion and
consideration of opinions and a civil exchange of arguments, which is defined
as the act of deliberation, is desirable. The quality of discussions and
participation processes in terms of their deliberativeness highly depends on
the design of platforms and processes. To facilitate online communication for
both participants and initiators, machine learning methods offer a lot of
potential. In this work we want to showcase which issues occur in political
online discussions and how machine learning can be used to counteract these
issues and enhance deliberation.

</details>


### [67] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/pdf/2506.02536)
*Xin Liu, Lu Wang*

Main category: cs.CL

TL;DR: CoT prompting in LLMs is often verbose and redundant. The study finds that 60% of reasoning steps suffice for correct answers, leading to three efficiency strategies: early stopping, boosting end signals, and supervised stopping. These reduce token usage significantly without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and redundancy in CoT prompting, which increases inference costs without always improving accuracy.

Method: Systematic study of minimum required reasoning steps, followed by three inference-time strategies: early stopping, boosting end signals, and supervised stopping.

Result: Methods reduce token usage by up to 40% with little or no accuracy drop, e.g., Answer Consistency improves efficiency on NaturalQuestions.

Conclusion: Cost-effective reasoning methods at inference time are crucial for practical LLM applications, balancing efficiency and accuracy.

Abstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models
(LLMs) but often leads to verbose and redundant outputs, thus increasing
inference cost. We hypothesize that many reasoning steps are unnecessary for
producing correct answers. To investigate this, we start with a systematic
study to examine what is the minimum reasoning required for a model to reach a
stable decision. We find that on math reasoning tasks like math, models
typically converge to their final answers after 60\% of the reasoning steps,
suggesting substantial redundancy in the remaining content. Based on these
insights, we propose three inference-time strategies to improve efficiency: (1)
early stopping via answer consistency, (2) boosting the probability of
generating end-of-reasoning signals, and (3) a supervised method that learns
when to stop based on internal activations. Experiments across five benchmarks
and five open-weights LLMs show that our methods significantly reduce token
usage with little or no accuracy drop. In particular, on NaturalQuestions,
Answer Consistency reduces tokens by over 40\% while further improving
accuracy. Our work underscores the importance of cost-effective reasoning
methods that operate at inference time, offering practical benefits for
real-world applications.

</details>


### [68] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/pdf/2506.02544)
*Yang Tian, Fan Liu, Jingyuan Zhang, Victoria W., Yupeng Hu, Liqiang Nie*

Main category: cs.CL

TL;DR: CoRe-MMRAG addresses inconsistencies in multimodal knowledge retrieval for MMRAG, improving reliability and alignment between visual-textual sources.


<details>
  <summary>Details</summary>
Motivation: Challenges like Parametric-Retrieved Knowledge Inconsistency (PRKI) and Visual-Textual Knowledge Inconsistency (VTKI) hinder reliable multimodal knowledge integration.

Method: CoRe-MMRAG uses a four-stage pipeline: internal response generation, multimodal evidence selection, external response generation, and integration. A specialized training paradigm enhances discrimination and integration.

Result: Achieves 5.6% and 9.3% performance gains on InfoSeek and Encyclopedic-VQA benchmarks.

Conclusion: CoRe-MMRAG effectively reconciles knowledge inconsistencies, outperforming baselines, with code and data publicly available.

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [69] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/pdf/2506.02561)
*Yirao Zhao, Guizhen Chen, Kenji Kawaguchi, Lidong Bing, Wenxuan Zhang*

Main category: cs.CL

TL;DR: The paper introduces Cus-Prun, a method to prune large language models into compact expert models without post-training, outperforming other methods with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Large language models require significant computational resources, and existing pruning methods either focus on general capabilities or degrade performance. Cus-Prun addresses this by creating tailored expert models.

Method: Cus-Prun prunes irrelevant neurons along 'language', 'domain', and 'task' dimensions to transform a general model into a lightweight expert model without post-training.

Result: Experiments show Cus-Prun outperforms other methods, maintaining expert and general capabilities across various models and sizes.

Conclusion: Cus-Prun effectively prunes LLMs into expert models without post-training, offering a resource-efficient solution with minimal performance loss.

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [70] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/pdf/2506.02573)
*Muhammad Falensi Azmi, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto*

Main category: cs.CL

TL;DR: IndoSafety is a human-verified safety evaluation dataset for Indonesian LLMs, addressing cultural sensitivity in five language varieties. Fine-tuning on it improves safety without compromising performance.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored safety of region-specific LLMs in culturally diverse settings like Indonesia, where local norms are crucial.

Method: Extends prior safety frameworks to create a culturally tailored taxonomy and evaluates Indonesian-centric LLMs using the IndoSafety dataset.

Result: Existing Indonesian LLMs often produce unsafe outputs, especially in colloquial/local languages, but fine-tuning on IndoSafety enhances safety while maintaining performance.

Conclusion: Culturally grounded safety evaluation is essential for responsible LLM deployment in multilingual contexts.

Abstract: Although region-specific large language models (LLMs) are increasingly
developed, their safety remains underexplored, particularly in culturally
diverse settings like Indonesia, where sensitivity to local norms is essential
and highly valued by the community. In this work, we present IndoSafety, the
first high-quality, human-verified safety evaluation dataset tailored for the
Indonesian context, covering five language varieties: formal and colloquial
Indonesian, along with three major local languages: Javanese, Sundanese, and
Minangkabau. IndoSafety is constructed by extending prior safety frameworks to
develop a taxonomy that captures Indonesia's sociocultural context. We find
that existing Indonesian-centric LLMs often generate unsafe outputs,
particularly in colloquial and local language settings, while fine-tuning on
IndoSafety significantly improves safety while preserving task performance. Our
work highlights the critical need for culturally grounded safety evaluation and
provides a concrete step toward responsible LLM deployment in multilingual
settings. Warning: This paper contains example data that may be offensive,
harmful, or biased.

</details>


### [71] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/pdf/2506.02589)
*Maria Levchenko*

Main category: cs.CL

TL;DR: The paper evaluates NER models for Russian cultural event texts, finding GPT-4o and GPT-4 perform best, with GPT-4o achieving F1=0.93 and GPT-4 precision=0.99.


<details>
  <summary>Details</summary>
Motivation: To address NER challenges for person names in Russian cultural news texts and compare model performance.

Method: Comparative evaluation of transformer-based models (DeepPavlov, RoBERTa, SpaCy) and LLMs (GPT-3.5, GPT-4, GPT-4o) using the SPbLitGuide dataset.

Result: GPT-4o achieved the highest F1 score (0.93), while GPT-4 had the highest precision (0.99). GPT-4.1 later achieved F1=0.94.

Conclusion: The study highlights the effectiveness of LLMs for NER in morphologically rich languages like Russian, with rapid progress in model performance.

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [72] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/pdf/2506.02591)
*Minh Duc Bui, Kyung Eun Park, Goran Glavaš, Fabian David Schmidt, Katharina von der Wense*

Main category: cs.CL

TL;DR: The paper investigates whether large language models (LLMs) can handle diverse measurement systems accurately, finding biases toward dominant systems and performance instability, partially mitigated by reasoning methods like CoT, but at higher computational costs.


<details>
  <summary>Details</summary>
Motivation: To ensure LLMs provide accurate information across diverse cultural measurement systems, addressing potential biases and performance disparities.

Method: Testing seven open-source LLMs using newly compiled datasets to analyze default measurement systems, accuracy variance, and the impact of reasoning methods like chain-of-thought (CoT).

Result: LLMs default to dominant measurement systems, show performance instability across systems, and reasoning methods improve accuracy but increase computational costs.

Conclusion: LLMs exhibit biases and performance issues with underrepresented measurement systems, and while reasoning helps, it raises cost concerns, marginalizing some users.

Abstract: Measurement systems (e.g., currencies) differ across cultures, but the
conversions between them are well defined so that humans can state facts using
any measurement system of their choice. Being available to users from diverse
cultural backgrounds, large language models (LLMs) should also be able to
provide accurate information irrespective of the measurement system at hand.
Using newly compiled datasets we test if this is the case for seven open-source
LLMs, addressing three key research questions: (RQ1) What is the default system
used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their
accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate
potential challenges w.r.t. underrepresented systems via reasoning? Our
findings show that LLMs default to the measurement system predominantly used in
the data. Additionally, we observe considerable instability and variance in
performance across different measurement systems. While this instability can in
part be mitigated by employing reasoning methods such as chain-of-thought
(CoT), this implies longer responses and thereby significantly increases
test-time compute (and inference costs), marginalizing users from cultural
backgrounds that use underrepresented measurement systems.

</details>


### [73] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/pdf/2506.02592)
*Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin*

Main category: cs.CL

TL;DR: The paper introduces the DBG score to measure self-preference bias in LLMs, separating it from response quality by using gold judgments. It evaluates bias across models and explores influencing factors.


<details>
  <summary>Details</summary>
Motivation: Existing methods conflate self-preference bias with response quality, necessitating a clearer measure.

Method: Proposes the DBG score, using gold judgments to isolate bias from quality, and conducts experiments on LLMs.

Result: The DBG score effectively measures bias, and factors like text style and post-training data influence it.

Conclusion: The study provides a refined tool for bias assessment and insights into its mechanisms and mitigation.

Abstract: Recent studies show that large language models (LLMs) exhibit self-preference
bias when serving as judges, meaning they tend to favor their own responses
over those generated by other models. Existing methods typically measure this
bias by calculating the difference between the scores a judge model assigns to
its own responses and those it assigns to responses from other models. However,
this approach conflates self-preference bias with response quality, as
higher-quality responses from the judge model may also lead to positive score
differences, even in the absence of bias. To address this issue, we introduce
gold judgments as proxies for the actual quality of responses and propose the
DBG score, which measures self-preference bias as the difference between the
scores assigned by the judge model to its own responses and the corresponding
gold judgments. Since gold judgments reflect true response quality, the DBG
score mitigates the confounding effect of response quality on bias measurement.
Using the DBG score, we conduct comprehensive experiments to assess
self-preference bias across LLMs of varying versions, sizes, and reasoning
abilities. Additionally, we investigate two factors that influence and help
alleviate self-preference bias: response text style and the post-training data
of judge models. Finally, we explore potential underlying mechanisms of
self-preference bias from an attention-based perspective. Our code and data are
available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [74] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/pdf/2506.02596)
*Fan Gao, Dongyuan Li, Ding Xia, Fei Mi, Yasheng Wang, Lifeng Shang, Baojun Wang*

Main category: cs.CL

TL;DR: The paper introduces \benchName, a multi-genre benchmark for evaluating Chinese essay writing by LLMs, addressing gaps in existing metrics.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook structural and rhetorical complexities of Chinese essays across genres, prompting the need for a specialized evaluation framework.

Method: The study curates 728 real-world prompts into Open-Ended and Constrained sets, develops a genre-specific scoring framework, and benchmarks 15 LLMs.

Result: The benchmark evaluates LLMs' strengths and limitations across genres and instruction types, validated by human agreement.

Conclusion: \benchName aims to advance LLM-based Chinese essay evaluation and inspire future research in educational essay generation.

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [75] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/pdf/2506.02659)
*Manon Reusens, Bart Baesens, David Jurgens*

Main category: cs.CL

TL;DR: A new framework evaluates consistency in persona-assigned LLMs across personas and tasks, revealing factors like stereotypes and task structure influence consistency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive analysis on how well LLMs maintain consistency across different personas and task types.

Method: Introduces a standardized framework defining consistency and evaluates it across four persona categories and multiple task dimensions.

Result: Consistency varies by persona, stereotypes, and task structure, improving with structured tasks and context.

Conclusion: The framework provides insights into persona consistency in LLMs, highlighting influencing factors and task dependencies.

Abstract: Personalized Large Language Models (LLMs) are increasingly used in diverse
applications, where they are assigned a specific persona - such as a happy high
school teacher - to guide their responses. While prior research has examined
how well LLMs adhere to predefined personas in writing style, a comprehensive
analysis of consistency across different personas and task types is lacking. In
this paper, we introduce a new standardized framework to analyze consistency in
persona-assigned LLMs. We define consistency as the extent to which a model
maintains coherent responses when assigned the same persona across different
tasks and runs. Our framework evaluates personas across four different
categories (happiness, occupation, personality, and political stance) spanning
multiple task dimensions (survey writing, essay generation, social media post
generation, single turn, and multi-turn conversations). Our findings reveal
that consistency is influenced by multiple factors, including the assigned
persona, stereotypes, and model design choices. Consistency also varies across
tasks, increasing with more structured tasks and additional context. All code
is available on GitHub.

</details>


### [76] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/pdf/2506.02672)
*Shihan Dou, Ming Zhang, Chenhao Huang, Jiayi Chen, Feng Chen, Shichun Liu, Yan Liu, Chenxiao Liu, Cheng Zhong, Zongzhang Zhang, Tao Gui, Chao Xin, Wei Chengzhi, Lin Yan, Qi Zhang, Xuanjing Huang*

Main category: cs.CL

TL;DR: EvaLearn is a benchmark for evaluating LLMs' learning capability and efficiency through sequential problem-solving, revealing varied performance among models and highlighting a new dimension of model evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of LLMs' learning potential by creating a benchmark that evaluates models sequentially, unlike parallel evaluations in existing benchmarks.

Method: EvaLearn includes 648 problems across six task types, grouped into 182 sequences, with five automated metrics to assess learning capability and efficiency. Nine frontier models are benchmarked under sequential and learning settings.

Result: Varied performance profiles were observed: some models (e.g., Claude-3.7-sonnet) showed strong learning ability, while others struggled or exhibited negative transfer. Static ability did not guarantee learning advantage.

Conclusion: EvaLearn offers a novel evaluation perspective for LLMs, emphasizing learning capability and efficiency, and aims to bridge the gap between models and human capabilities.

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [77] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/pdf/2506.02678)
*Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Xing W, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, Zhijiang Guo, Xiao Liu, Fei Yin, Cheng-Lin Liu*

Main category: cs.CL

TL;DR: A dynamic ratio-based training pipeline reduces output tokens by 40% while maintaining reasoning accuracy in LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of efficient language reasoning, especially with long outputs, without relying on complex annotations or model interpolation.

Method: Balances weights between System-1 and System-2 data dynamically to eliminate redundant reasoning while preserving capability.

Result: Validated on DeepSeek-R1-Distill models, showing significant token reduction (40%) without accuracy loss.

Conclusion: The method offers an efficient solution for LLM inference, with code and data to be released.

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [78] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/pdf/2506.02683)
*Zhengdong Lu, Weikai Lu, Yiling Tao, Yun Dai, ZiXuan Chen, Huiping Zhuang, Cen Chen, Hao Peng, Ziqian Zeng*

Main category: cs.CL

TL;DR: DPPM is a parallel planning method for LLM-based agents, addressing constraints and cascading errors by decomposing tasks, planning subtasks in parallel, and merging results with verification.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based planning methods suffer from heavy constraints and cascading errors, limiting their effectiveness.

Method: DPPM decomposes tasks into subtasks, plans for them in parallel, merges subplans, and includes a verification/refinement module.

Result: DPPM outperforms existing methods in travel planning tasks.

Conclusion: DPPM effectively addresses limitations of current LLM-based planning methods, offering improved performance.

Abstract: Despite significant advances in Large Language Models (LLMs), planning tasks
still present challenges for LLM-based agents. Existing planning methods face
two key limitations: heavy constraints and cascading errors. To address these
limitations, we propose a novel parallel planning paradigm, which Decomposes,
Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).
Specifically, DPPM decomposes the complex task based on constraints into
subtasks, generates the subplan for each subtask in parallel, and merges them
into a global plan. In addition, our approach incorporates a verification and
refinement module, enabling error correction and conflict resolution.
Experimental results demonstrate that DPPM significantly outperforms existing
methods in travel planning tasks.

</details>


### [79] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/pdf/2506.02689)
*Liang Yue, Yihong Tang, Kehai Chen, Jie Liu, Min Zhang*

Main category: cs.CL

TL;DR: MASTER is a data augmentation method using multi-agent interactions to generate high-quality fine-tuning data, improving model performance and reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: High-quality fine-tuning data is hard to obtain due to collection difficulties and costs, necessitating innovative solutions like MASTER.

Method: MASTER enriches data through multi-agent interactions simulating teaching scenarios, creating augmented datasets like BOOST-QA.

Result: Models fine-tuned with BOOST-QA excel in benchmarks, showing strong multitask generalization and improved reasoning.

Conclusion: MASTER offers a scalable solution for data augmentation, enhancing model performance and providing research insights.

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [80] [On Entity Identification in Language Models](https://arxiv.org/pdf/2506.02701)
*Masaki Sakata, Sho Yokoi, Benjamin Heinzerling, Takumi Ito, Kentaro Inui*

Main category: cs.CL

TL;DR: The paper analyzes how language models (LMs) internally represent named entities, using clustering metrics to evaluate entity mention identification and distinction. Results show effective performance (0.66-0.9 precision/recall) and reveal compact, low-dimensional entity representations in early layers.


<details>
  <summary>Details</summary>
Motivation: To understand how LMs internally identify and distinguish named entity mentions, addressing ambiguity and variability in entity-mention relationships.

Method: Proposes a clustering-based framework to analyze LM internal representations, evaluating entity mention clustering and separation. Tests five Transformer-based autoregressive models.

Result: LMs effectively identify and distinguish entities (0.66-0.9 precision/recallike metrics). Entity information is compactly represented in early layers.

Conclusion: LMs internally organize entity information in a way that mirrors real-world knowledge structures, providing insights into their internal mechanisms.

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [81] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/pdf/2506.02726)
*Qihang Yan, Xinyu Zhang, Luming Guo, Qi Zhang, Feifan Liu*

Main category: cs.CL

TL;DR: RACE-Align improves LLM accuracy and reasoning in vertical domains by integrating retrieval-augmented knowledge and Chain-of-Thought reasoning into preference alignment.


<details>
  <summary>Details</summary>
Motivation: Address LLM limitations in accuracy, domain-specific reasoning, and interpretability by enhancing preference alignment with external knowledge and explicit reasoning.

Method: Constructs a binary preference dataset with retrieval-augmented knowledge and CoT reasoning, then aligns LLMs using DPO.

Result: Outperforms base and SFT models in TCM, improving accuracy, reasoning, and interpretability.

Conclusion: RACE-Align effectively enhances LLMs' knowledge application and reasoning in complex domains.

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [82] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/pdf/2506.02740)
*Amaç Herdağdelen, Marco Baroni*

Main category: cs.CL

TL;DR: The study extracts gender-specific actions from text corpora and Twitter, comparing them to stereotypes. Using OMCS and gender heuristics, it measures gender bias, achieving a 0.47 correlation with human judgments and 0.76 ROC AUC. It concludes that text corpora can augment commonsense repositories with gender stereotypes, providing two datasets for further research.


<details>
  <summary>Details</summary>
Motivation: To explore how gender-specific actions in text corpora align with stereotypical expectations and augment commonsense knowledge repositories with such insights.

Method: Extracted actions from text corpora and Twitter, used OMCS for commonsense relevance, and applied gender heuristics to compute bias. Evaluated against human gold standards.

Result: Achieved 0.47 Spearman correlation with human judgments and 0.76 ROC AUC for polarity prediction. Provided datasets of human-rated and automatically rated actions.

Conclusion: Feasible to use natural text (especially Twitter) to enrich commonsense repositories with gender stereotypes. Datasets support further research.

Abstract: We extracted gender-specific actions from text corpora and Twitter, and
compared them to stereotypical expectations of people. We used Open Mind Common
Sense (OMCS), a commonsense knowledge repository, to focus on actions that are
pertinent to common sense and daily life of humans. We use the gender
information of Twitter users and Web-corpus-based pronoun/name gender
heuristics to compute the gender bias of the actions. With high recall, we
obtained a Spearman correlation of 0.47 between corpus-based predictions and a
human gold standard, and an area under the ROC curve of 0.76 when predicting
the polarity of the gold standard. We conclude that it is feasible to use
natural text (and a Twitter-derived corpus in particular) in order to augment
commonsense repositories with the stereotypical gender expectations of actions.
We also present a dataset of 441 commonsense actions with human judges' ratings
on whether the action is typically/slightly masculine/feminine (or neutral),
and another larger dataset of 21,442 actions automatically rated by the methods
we investigate in this study.

</details>


### [83] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/pdf/2506.02753)
*Aisha Alansari, Hamzah Luqman*

Main category: cs.CL

TL;DR: A novel framework combining multi-task learning and active learning improves offensive speech detection in Arabic social media text, achieving state-of-the-art results with fewer labeled samples.


<details>
  <summary>Details</summary>
Motivation: The spread of offensive content on social media, especially in Arabic, is challenging due to dialectal variations and limited labeled data.

Method: The framework integrates multi-task learning (MTL) with active learning, dynamically adjusting task weights and using uncertainty sampling for data selection. Weighted emoji handling is also introduced.

Result: The model achieves an 85.42% macro F1-score on the OSACT2022 dataset, outperforming existing methods with fewer fine-tuning samples.

Conclusion: Combining MTL and active learning is effective for offensive language detection in resource-constrained settings.

Abstract: The rapid growth of social media has amplified the spread of offensive,
violent, and vulgar speech, which poses serious societal and cybersecurity
concerns. Detecting such content in Arabic text is particularly complex due to
limited labeled data, dialectal variations, and the language's inherent
complexity. This paper proposes a novel framework that integrates multi-task
learning (MTL) with active learning to enhance offensive speech detection in
Arabic social media text. By jointly training on two auxiliary tasks, violent
and vulgar speech, the model leverages shared representations to improve the
detection accuracy of the offensive speech. Our approach dynamically adjusts
task weights during training to balance the contribution of each task and
optimize performance. To address the scarcity of labeled data, we employ an
active learning strategy through several uncertainty sampling techniques to
iteratively select the most informative samples for model training. We also
introduce weighted emoji handling to better capture semantic cues. Experimental
results on the OSACT2022 dataset show that the proposed framework achieves a
state-of-the-art macro F1-score of 85.42%, outperforming existing methods while
using significantly fewer fine-tuning samples. The findings of this study
highlight the potential of integrating MTL with active learning for efficient
and accurate offensive language detection in resource-constrained settings.

</details>


### [84] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/pdf/2506.02758)
*Stefano Bannò, Kate Knill, Mark Gales*

Main category: cs.CL

TL;DR: The paper introduces a novel method for fine-grained vocabulary assessment in L2 proficiency using LLMs and EVP, outperforming traditional PoS-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current automated systems for vocabulary assessment in L2 proficiency focus on context-independent word use, lacking precision. The paper aims to improve this by leveraging contextual word usage.

Method: Combines large language models (LLMs) with the English Vocabulary Profile (EVP) to assess word-level proficiency in L2 learner writing, addressing challenges like polysemy and contextual variation.

Result: LLMs outperform PoS-based methods by utilizing semantic information. The approach also examines correlations between word-level and essay-level proficiency and evaluates EVP consistency.

Conclusion: LLMs are effective for vocabulary assessment, offering improved performance and insights into proficiency levels.

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [85] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/pdf/2506.02803)
*Sifan Li, Yujun Cai, Yiwei Wang*

Main category: cs.CL

TL;DR: VLMs struggle with hidden content detection in images, scoring 0-5.36% accuracy on HC-Bench. SemVink, a simple scaling method, boosts accuracy to >99%, revealing VLMs' overreliance on high-level semantics.


<details>
  <summary>Details</summary>
Motivation: To address VLMs' failure in detecting hidden content, a core human capability, and propose improvements for real-world applications.

Method: Introduces HC-Bench with 112 hidden-content images and SemVink, which scales images to low resolutions (32-128 pixels).

Result: VLMs perform poorly (0-5.36%), while SemVink achieves >99% accuracy by reducing visual noise.

Conclusion: VLMs need hybrid models with multi-scale processing to bridge the gap between computational vision and human cognition.

Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.

</details>


### [86] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/pdf/2506.02818)
*Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba*

Main category: cs.CL

TL;DR: The paper proposes a method to compress large language models (LLMs) by leveraging orthogonal transformations to improve the compressibility of weight matrices using structured representations.


<details>
  <summary>Details</summary>
Motivation: LLMs require high computational and memory resources, and structured matrices can reduce parameters but need fine-tuning for accuracy.

Method: Utilizes orthogonal transformations of weight matrices, which leave LLM output invariant, to enhance compressibility within structured classes.

Result: The approach improves the compressibility of LLM weights without altering model output, applicable to various structured matrices.

Conclusion: The method offers a practical way to reduce LLM resource requirements while maintaining performance, with code available for implementation.

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [87] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/pdf/2506.02827)
*Yulin Dou, Jiangming Liu*

Main category: cs.CL

TL;DR: TO-GATE improves LLM-based preference elicitation by optimizing dialogue trajectories, outperforming baselines by 9.32%.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with optimal dialogue trajectories and irrelevant questions.

Method: TO-GATE uses a clarification resolver for optimal questioning and a summarizer for task-aligned responses.

Result: Achieves a 9.32% improvement over baselines in preference elicitation tasks.

Conclusion: TO-GATE effectively enhances LLM-based preference elicitation through trajectory optimization.

Abstract: Large language models (LLMs) can effectively elicit human preferences through
multi-turn dialogue. Complex tasks can be accomplished through iterative
clarifying questions and final responses generated by an LLM acting as a
questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches
based on self-taught reasoning struggle to identify optimal dialogue
trajectories and avoid irrelevant questions to the tasks. To address this
limitation, we propose TO-GATE, a novel framework that enhances question
generation through trajectory optimization, which consists of two key
components: a clarification resolver that generates optimal questioning
trajectories, and a summarizer that ensures task-aligned final responses. The
trajectory optimization enables the model to produce effective elicitation
questions and summary responses tailored to specific tasks. Experimental
results demonstrate that TO-GATE significantly outperforms baseline methods,
achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [88] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/pdf/2506.02872)
*Ludovic Moncla, Hédi Zeghidi*

Main category: cs.CL

TL;DR: The paper benchmarks NER methods for historical texts, highlighting transformer models' superiority for nested entities and generative models' potential in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in NER for historical texts due to non-standardized language, archaic orthography, and nested entities.

Method: Evaluates CRFs, spaCy, CamemBERT, Flair, and generative models on the GeoEDdA dataset, proposing token- and span-level classification.

Result: Transformer models excel, especially for nested entities; generative models show promise with scarce labeled data.

Conclusion: Hybrid approaches combining symbolic and neural methods are suggested for better handling of historical text complexities.

Abstract: Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.

</details>


### [89] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/pdf/2506.02878)
*Jintian Shao, Yiming Cheng*

Main category: cs.CL

TL;DR: The paper argues that Chain-of-Thought (CoT) prompting doesn't enable genuine reasoning in Large Language Models but instead guides them to mimic reasoning through structural constraints.


<details>
  <summary>Details</summary>
Motivation: To challenge the claim that CoT elicits abstract reasoning in models, proposing it merely imitates reasoning forms.

Method: Theoretical analysis of CoT's role as a structural constraint leveraging sequence prediction and pattern matching.

Result: CoT enhances performance by mimicking reasoning, not enabling genuine abstract reasoning.

Conclusion: CoT's success is due to imitation, not true reasoning, questioning claims of emergent reasoning capabilities.

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [90] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/pdf/2506.02899)
*Yusuke Sakai, Takumi Goto, Taro Watanabe*

Main category: cs.CL

TL;DR: IMPARA-GED is a reference-free GEC evaluation method with GED capabilities, outperforming existing methods in human correlation.


<details>
  <summary>Details</summary>
Motivation: To improve automatic GEC evaluation by integrating GED capabilities and enhancing quality estimation.

Method: Enhances IMPARA's quality estimator using a pre-trained language model with GED capabilities, tested on SEEDA dataset.

Result: Achieves highest correlation with human sentence-level evaluations on SEEDA.

Conclusion: IMPARA-GED is effective for reference-free GEC evaluation with superior human alignment.

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [91] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/pdf/2506.02911)
*Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu*

Main category: cs.CL

TL;DR: The paper introduces CellPuzzles, a task for batch-level cell type annotation in single-cell RNA sequencing data, and proposes Cell-o1, a fine-tuned LLM that outperforms baselines by 73%.


<details>
  <summary>Details</summary>
Motivation: Current foundation models annotate cells independently, lacking batch-level context and reasoning, unlike human experts who consider broader context.

Method: Proposes Cell-o1, a 7B LLM trained via supervised fine-tuning on reasoning traces and reinforcement learning with batch-level rewards.

Result: Cell-o1 achieves 73% better batch-level accuracy than the best baseline (OpenAI's o1) and generalizes well.

Conclusion: Cell-o1 sets a new standard for batch-level cell annotation, mimicking expert-like reasoning and improving performance.

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [92] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/pdf/2506.02921)
*Yijun Yang, Zeyu Huang, Wenhao Zhu, Zihan Qiu, Fei Yuan, Jeff Z. Pan, Ivan Titov*

Main category: cs.CL

TL;DR: The paper introduces LongBioBench, a new benchmark for evaluating long-context language models (LCLMs) using artificially generated biographies, addressing limitations of existing real-world and synthetic tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks for LCLMs have intrinsic limitations: real-world tasks are complex and prone to data contamination, while synthetic tasks lack coherence and realism.

Method: LongBioBench is designed with seamless context, controllable settings, and sound evaluation, assessing LCLMs on understanding, reasoning, and trustworthiness.

Result: Evaluation of 18 LCLMs shows deficiencies in semantic understanding, reasoning, and trustworthiness as context length increases. Existing synthetic benchmarks are found vulnerable due to design flaws.

Conclusion: LongBioBench offers a better trade-off between realism and controllability compared to previous benchmarks, with high interpretability and configurability.

Abstract: Existing frameworks for evaluating long-context language models (LCLM) can be
broadly categorized into real-world and synthetic tasks. Despite their utility,
both approaches are accompanied by certain intrinsic limitations. Real-world
tasks are too complex to interpret or characterize and are susceptible to data
contamination. In contrast, synthetic tasks often adopt the
needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the
"needle" and the "haystack" compromises their validity as proxies for realistic
applications. In response to these challenges, we posit that an ideal
long-context evaluation framework should be characterized by three essential
features: $\textit{seamless context}$, $\textit{controllable setting}$, and
$\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a
novel benchmark that utilizes artificially generated biographies as a
controlled environment for assessing LCLMs across dimensions of
$\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$.
Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total,
demonstrates that most models still exhibit deficiencies in semantic
understanding and elementary reasoning over retrieved results and are less
trustworthy as context length increases. Our further analysis indicates some
design choices employed by existing synthetic benchmarks, such as contextual
non-coherence, numerical needles, and the absence of distractors, rendering
them vulnerable to test the model long-context capabilities. Moreover, we also
reveal that long-context continual pretraining primarily adjusts RoPE embedding
to accommodate extended context lengths. To sum up, compared to previous
synthetic benchmarks, LongBioBench achieves a better trade-off between
mirroring authentic language tasks and maintaining controllability, and is
highly interpretable and configurable.

</details>


### [93] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/pdf/2506.02924)
*Diogo A. P. Nunes, Eugénio Ribeiro*

Main category: cs.CL

TL;DR: The paper describes a method for identifying depression symptoms using binary classification, fine-tuning foundation models, and ensemble techniques, achieving top performance in the eRisk 2025 Task 1.


<details>
  <summary>Details</summary>
Motivation: The task involved identifying relevant sentences for depression symptoms from the BDI questionnaire, but the training data had binary labels, prompting a classification approach.

Method: The team used binary classification for each symptom, fine-tuned foundation models, employed sentence similarity, LLM prompting, and ensemble methods, with synthetic data to address class imbalance.

Result: Fine-tuning foundation models with synthetic data performed best, with varying approaches per symptom. Their submissions outperformed 16 other teams in IR metrics.

Conclusion: The study highlights the effectiveness of fine-tuning foundation models and ensemble methods for symptom-specific classification in depression detection tasks.

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [94] [Quantitative LLM Judges](https://arxiv.org/pdf/2506.02945)
*Aishwarya Sahoo, Jeevana Kruthi Karnuthala, Tushar Parmanand Budhwani, Pranchal Agarwal, Sankaran Vaidyanathan, Alexa Siu, Franck Dernoncourt, Jennifer Healey, Nedim Lipka, Ryan Rossi, Uttaran Bhattacharya, Branislav Kveton*

Main category: cs.CL

TL;DR: The paper introduces 'quantitative LLM judges,' a framework where large language models (LLMs) evaluate other LLMs' outputs by aligning scores with human feedback using regression models, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the evaluation of LLM outputs by aligning automated scores with human judgments more efficiently than fine-tuning, especially where human feedback is scarce.

Method: Proposes four quantitative judges using regression models to adjust original LLM judges' scores based on textual evaluations and scores, validated on four datasets with two base judges.

Result: Quantitative judges improve predictive power of existing judges, demonstrating computational and statistical efficiency, especially with limited human feedback.

Conclusion: The framework effectively enhances LLM evaluation by aligning automated scores with human judgments, offering a versatile and efficient alternative to fine-tuning.

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [95] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/pdf/2506.02959)
*Zhixiong Su, Yichen Wang, Herun Wan, Zhaohan Zhang, Minnan Luo*

Main category: cs.CL

TL;DR: The paper explores fine-grained detection of machine-generated text (MGT) in human-AI coauthored content, proposing a dataset (HACo-Det) and evaluating retrofitted detectors. Finetuned models outperform metric-based methods, but challenges remain.


<details>
  <summary>Details</summary>
Motivation: The misuse of LLMs and the lack of methods for detecting human-AI coauthored texts motivate the need for fine-grained MGT detection.

Method: Proposes HACo-Det dataset with word-level labels and retrofits seven document-level detectors for word- and sentence-level tasks.

Result: Finetuned models outperform metric-based methods (0.462 average F1), but fine-grained detection remains challenging.

Conclusion: Fine-grained coauthored text detection is unsolved; context window and method limitations highlight areas for improvement.

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [96] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2506.02961)
*Yan Gao, Massimo Roberto Scamarcia, Javier Fernandez-Marques, Mohammad Naseri, Chong Shen Ng, Dimitris Stripelis, Zexi Li, Tao Shen, Jiamu Bai, Daoyuan Chen, Zikai Zhang, Rui Hu, InSeo Song, Lee KangYoon, Hong Jia, Ting Dang, Junyan Wang, Zheyuan Liu, Daniel Janes Beutel, Lingjuan Lyu, Nicholas D. Lane*

Main category: cs.CL

TL;DR: The paper introduces the FlowerTune LLM Leaderboard to benchmark federated fine-tuning of LLMs across diverse domains, addressing data scarcity and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To tackle data scarcity and privacy issues in LLM development by leveraging Federated Learning (FL) for decentralized fine-tuning without sharing raw data.

Method: Developed the FlowerTune LLM Leaderboard, evaluating 26 pre-trained LLMs across four domains (general NLP, finance, medical, coding) with federated datasets and domain-specific metrics.

Result: First comprehensive comparison of LLMs in FL settings, providing insights into performance, resource constraints, and domain adaptation.

Conclusion: The work establishes a foundation for privacy-preserving, domain-specialized LLMs, enabling real-world applications.

Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across
diverse domains, yet their development remains reliant on vast amounts of
publicly available data, raising concerns about data scarcity and the lack of
access to domain-specific, sensitive information. Federated Learning (FL)
presents a compelling framework to address these challenges by enabling
decentralized fine-tuning on pre-trained LLMs without sharing raw data.
However, the compatibility and performance of pre-trained LLMs in FL settings
remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a
first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning
of LLMs across four diverse domains: general NLP, finance, medical, and coding.
Each domain includes federated instruction-tuning datasets and domain-specific
evaluation metrics. Our results, obtained through a collaborative, open-source
and community-driven approach, provide the first comprehensive comparison
across 26 pre-trained LLMs with different aggregation and fine-tuning
strategies under federated settings, offering actionable insights into model
performance, resource constraints, and domain adaptation. This work lays the
foundation for developing privacy-preserving, domain-specialized LLMs for
real-world applications.

</details>


### [97] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/pdf/2506.02973)
*Dingwei Chen, Ziqiang Liu, Feiteng Fang, Chak Tou Leong, Shiwen Ni, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li*

Main category: cs.CL

TL;DR: PLI (Premature Layers Interpolation) is a training-free method to reduce hallucinations in LLMs by interpolating premature layers, improving factual coherence without intensive resources.


<details>
  <summary>Details</summary>
Motivation: Addressing LLMs' tendency for factually inconsistent outputs (hallucinations) without resource-heavy fine-tuning or alignment methods.

Method: PLI inserts mathematically interpolated premature layers between adjacent layers to extend information processing depth.

Result: PLI reduces hallucinations and outperforms baselines on four datasets, with success tied to LLMs' internal mechanisms.

Conclusion: PLI offers an effective, resource-efficient solution for enhancing LLM factuality, with code and data to be released for reproducibility.

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text
understanding and generation. However, their tendency to produce factually
inconsistent outputs, commonly referred to as ''hallucinations'', remains a
critical challenge. Existing approaches, such as retrieval-based and
inference-time correction methods, primarily address this issue at the input or
output level, often overlooking the intrinsic information refinement process
and the role of premature layers. Meanwhile, alignment- and fine-tuning-based
methods are resource-intensive. In this paper, we propose PLI (Premature Layers
Interpolation), a novel, training-free, and plug-and-play intervention designed
to enhance factuality. PLI mitigates hallucinations by inserting premature
layers formed through mathematical interpolation with adjacent layers. Inspired
by stable diffusion and sampling steps, PLI extends the depth of information
processing and transmission in LLMs, improving factual coherence. Experiments
on four publicly available datasets demonstrate that PLI effectively reduces
hallucinations while outperforming existing baselines in most cases. Further
analysis suggests that the success of layer interpolation is closely linked to
LLMs' internal mechanisms. To promote reproducibility, we will release our code
and data upon acceptance.

</details>


### [98] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/pdf/2506.02987)
*Richard Armitage*

Main category: cs.CL

TL;DR: Leading LLMs (o3, Claude Opus 4, Grok3, Gemini 2.5 Pro) outperformed human GPs in answering MRCGP-style primary care exam questions, with o3 scoring highest at 99%.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of advanced LLMs in primary care education by testing their performance on MRCGP-style exam questions.

Method: Four LLMs answered 100 MRCGP-style multiple-choice questions once each, with responses scored against correct answers.

Result: o3 scored 99%, while others scored 95%, all surpassing the human GP average of 73%.

Conclusion: LLMs, especially reasoning models, show strong potential to support primary care, particularly when trained on clinical data.

Abstract: Background: Large language models (LLMs) have demonstrated substantial
potential to support clinical practice. Other than Chat GPT4 and its
predecessors, few LLMs, especially those of the leading and more powerful
reasoning model class, have been subjected to medical specialty examination
questions, including in the domain of primary care. This paper aimed to test
the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and
Gemini 2.5 Pro) in primary care education, specifically in answering Member of
the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer
100 randomly chosen multiple choice questions from the Royal College of General
Practitioners GP SelfTest on 25 May 2025. Questions included textual
information, laboratory results, and clinical images. Each model was prompted
to answer as a GP in the UK and was provided with full question information.
Each question was attempted once by each model. Responses were scored against
correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was
99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the
same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially
exceeded the average performance of GPs and GP registrars who had answered the
same questions. o3 demonstrated the best performance, while the performances of
the other leading models were comparable with each other and were not
substantially lower than that of o3. These findings strengthen the case for
LLMs, particularly reasoning models, to support the delivery of primary care,
especially those that have been specifically trained on primary care clinical
data.

</details>


### [99] [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/pdf/2506.02995)
*Iuliia Zaitova, Badr M. Abdullah, Wei Xue, Dietrich Klakow, Bernd Möbius, Tania Avgustinova*

Main category: cs.CL

TL;DR: The paper evaluates idiom translation in text-to-text and speech-to-text systems, revealing SLT systems struggle more with idioms compared to MT systems and LLMs.


<details>
  <summary>Details</summary>
Motivation: Idiom translation is a challenge for machine translation, especially in speech-to-text systems, where research is limited.

Method: Systematic evaluation of idiom translation in MT and SLT systems across German-English and Russian-English, comparing state-of-the-art models.

Result: SLT systems show significant performance drops on idioms, often producing literal translations, while MT systems and LLMs handle idioms better.

Conclusion: Idiom-specific strategies and improved representations are needed in SLT architectures to address this gap.

Abstract: Idioms are defined as a group of words with a figurative meaning not
deducible from their individual components. Although modern machine translation
systems have made remarkable progress, translating idioms remains a major
challenge, especially for speech-to-text systems, where research on this topic
is notably sparse. In this paper, we systematically evaluate idiom translation
as compared to conventional news translation in both text-to-text machine
translation (MT) and speech-to-text translation (SLT) systems across two
language pairs (German to English, Russian to English). We compare
state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large
v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large
Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal
that SLT systems experience a pronounced performance drop on idiomatic data,
often reverting to literal translations even in higher layers, whereas MT
systems and Large Language Models demonstrate better handling of idioms. These
findings underscore the need for idiom-specific strategies and improved
internal representations in SLT architectures.

</details>


### [100] [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/pdf/2506.02998)
*Đorđe Klisura, Astrid R Bernaga Torres, Anna Karen Gárate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, Anthony Rios*

Main category: cs.CL

TL;DR: A multi-agent framework reduces dialectal bias in Privacy Policy QA systems, improving accuracy without retraining.


<details>
  <summary>Details</summary>
Motivation: Privacy policies are complex and existing QA systems perform poorly for non-standard English dialects, limiting accessibility.

Method: Proposes a multi-agent framework with a Dialect Agent (translates queries to Standard American English) and a Privacy Policy Agent (refines predictions).

Result: Improves GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA.

Conclusion: Structured agent collaboration effectively mitigates dialect biases, emphasizing the need for NLP systems to address linguistic diversity for equitable access.

Abstract: Privacy policies inform users about data collection and usage, yet their
complexity limits accessibility for diverse populations. Existing Privacy
Policy Question Answering (QA) systems exhibit performance disparities across
English dialects, disadvantaging speakers of non-standard varieties. We propose
a novel multi-agent framework inspired by human-centered design principles to
mitigate dialectal biases. Our approach integrates a Dialect Agent, which
translates queries into Standard American English (SAE) while preserving
dialectal intent, and a Privacy Policy Agent, which refines predictions using
domain expertise. Unlike prior approaches, our method does not require
retraining or dialect-specific fine-tuning, making it broadly applicable across
models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves
GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from
0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without
additional training data. These results highlight the effectiveness of
structured agent collaboration in mitigating dialect biases and underscore the
importance of designing NLP systems that account for linguistic diversity to
ensure equitable access to privacy information.

</details>


### [101] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/pdf/2506.03009)
*Florian Ludwig, Torsten Zesch, Frederike Zufall*

Main category: cs.CL

TL;DR: The paper explores conditioning LLMs at various legal abstraction levels to assess hate speech under German law, finding gaps between model performance and legal expertise.


<details>
  <summary>Details</summary>
Motivation: To understand how well LLMs internalize legal systems and assess hate speech, focusing on German criminal law.

Method: Proposes and tests approaches to condition LLMs at different legal abstraction levels for hate speech classification.

Result: Models conditioned on abstract legal knowledge performed poorly, while those using concrete knowledge did better but still struggled with classifying target conducts.

Conclusion: LLMs lag behind legal experts in hate speech assessment, with performance varying by the abstraction level of legal conditioning.

Abstract: The assessment of legal problems requires the consideration of a specific
legal system and its levels of abstraction, from constitutional law to
statutory law to case law. The extent to which Large Language Models (LLMs)
internalize such legal systems is unknown. In this paper, we propose and
investigate different approaches to condition LLMs at different levels of
abstraction in legal systems. This paper examines different approaches to
conditioning LLMs at multiple levels of abstraction in legal systems to detect
potentially punishable hate speech. We focus on the task of classifying whether
a specific social media posts falls under the criminal offense of incitement to
hatred as prescribed by the German Criminal Code. The results show that there
is still a significant performance gap between models and legal experts in the
legal assessment of hate speech, regardless of the level of abstraction with
which the models were conditioned. Our analysis revealed, that models
conditioned on abstract legal knowledge lacked deep task understanding, often
contradicting themselves and hallucinating answers, while models using concrete
legal knowledge performed reasonably well in identifying relevant target
groups, but struggled with classifying target conducts.

</details>


### [102] [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/pdf/2506.03011)
*Aditya Bharat Soni, Boxuan Li, Xingyao Wang, Valerie Chen, Graham Neubig*

Main category: cs.CL

TL;DR: OpenHands-Versa is a generalist AI agent using minimal general tools (code editing, web search, multimodal browsing) to outperform specialized agents across diverse benchmarks.


<details>
  <summary>Details</summary>
Motivation: Specialized AI agents lack generalization; the paper explores minimal general tools for high performance across diverse tasks.

Method: Develop OpenHands-Versa with general tools (code editing, web search, multimodal browsing) and test on benchmarks (SWE-Bench, GAIA, The Agent Company).

Result: OpenHands-Versa outperforms specialized agents with absolute improvements of 9.1, 1.3, and 9.1 points on respective benchmarks.

Conclusion: Generalist agents like OpenHands-Versa are feasible and set a strong baseline for future research.

Abstract: Modern human labor is characterized by specialization; we train for years and
develop particular tools that allow us to perform well across a variety of
tasks. In addition, AI agents have been specialized for domains such as
software engineering, web navigation, and workflow automation. However, this
results in agents that are good for one thing but fail to generalize beyond
their intended scope. One reason for this is that agent developers provide a
highly specialized set of tools or make architectural decisions optimized for a
specific use case or benchmark. In this work, we ask the question: what is the
minimal set of general tools that can be used to achieve high performance
across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist
agent built with a modest number of general tools: code editing and execution,
web search, as well as multimodal web browsing and file access. Importantly,
OpenHands-Versa demonstrates superior or competitive performance over leading
specialized agents across three diverse and challenging benchmarks: SWE-Bench
Multimodal, GAIA, and The Agent Company, outperforming the best-performing
previously published results with absolute improvements in success rate of 9.1,
1.3, and 9.1 points respectively. Further, we show how existing
state-of-the-art multi-agent systems fail to generalize beyond their target
domains. These results demonstrate the feasibility of developing a generalist
agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline
for future research.

</details>


### [103] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/pdf/2506.03035)
*Pierre Lepagnol, Sahar Ghannay, Thomas Gerald, Christophe Servan, Sophie Rosset*

Main category: cs.CL

TL;DR: The paper explores using Information Retrieval (IR) methods to select examples for enhancing prompts in Spoken Language Understanding (SLU) tasks, improving performance without longer prompts.


<details>
  <summary>Details</summary>
Motivation: Limited annotated data for SLU tasks and languages motivates leveraging instruction-tuned LLMs and IR methods to enhance few-shot learning.

Method: Proposes using IR approaches to select examples for building enhanced prompts, applied to SLU tasks.

Result: Lexical IR methods significantly boost SLU performance without increasing prompt length.

Conclusion: IR-based example selection effectively improves SLU task performance in few-shot settings.

Abstract: Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.

</details>


### [104] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/pdf/2506.03038)
*Jintian Shao, Yiming Cheng*

Main category: cs.CL

TL;DR: The paper discusses limitations of the VAPO framework in reinforcement learning for long-chain reasoning in LLMs, focusing on credit assignment, value function capacity, and global-to-local policy translation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in modeling long-term value and improving policy guidance in extended reasoning tasks with sparse rewards.

Method: Theoretical analysis of VAPO's limitations, including credit assignment, value function representational capacity, and policy improvement translation.

Result: Identifies fundamental boundaries in VAPO's ability to model long-term value and guide fine-grained policies in long reasoning chains.

Conclusion: Highlights the need for future research to develop more robust RL methods for advanced reasoning in LLMs.

Abstract: Reinforcement learning (RL) enhances large language models (LLMs) in complex,
long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,
despite sophisticated mechanisms like Decoupled GAE, theoretically faces
fundamental limitations in comprehensively modeling and leveraging deep,
long-term value for fine-grained, step-by-step policy guidance in extended
reasoning chains. We argue these limitations stem from inherent difficulties in
credit assignment, value function representational capacity with temporally
abstracted goals, and translating global value signals into local policy
improvements, especially with sparse rewards. Our theoretical analysis examines
these aspects to illuminate VAPO's boundaries in long-term value modeling,
aiming to deepen understanding of current RL for advanced reasoning and suggest
future research for more robust LLM agents.

</details>


### [105] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/pdf/2506.03051)
*Yuval Kansal, Shmuel Berman, Lydia Liu*

Main category: cs.CL

TL;DR: The paper evaluates Llama3.1 models' factuality in non-English educational contexts, finding issues with truthfulness and biases against rare languages.


<details>
  <summary>Details</summary>
Motivation: To assess the correctness of LLMs in educational settings, especially for non-English languages, given their growing adoption.

Method: Evaluated Llama3.1 models on factual questions for middle and high school students.

Result: LLMs provided extraneous, less truthful information and showed biases against rare languages.

Conclusion: Factuality and bias issues must be addressed for LLMs to be reliable educational tools in diverse linguistic contexts.

Abstract: Factuality is a necessary precursor to useful educational tools. As adoption
of Large Language Models (LLMs) in education continues of grow, ensuring
correctness in all settings is paramount. Despite their strong English
capabilities, LLM performance in other languages is largely untested. In this
work, we evaluate the correctness of the Llama3.1 family of models in answering
factual questions appropriate for middle and high school students. We
demonstrate that LLMs not only provide extraneous and less truthful
information, but also exacerbate existing biases against rare languages.

</details>


### [106] [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/pdf/2506.03090)
*Katherine Thai, Mohit Iyyer*

Main category: cs.CL

TL;DR: Modern long-context LLMs like Gemini Pro 2.5 outperform humans in literary evidence retrieval (62.5% vs. 50% accuracy), but open-weight models lag behind (29.1%). Challenges remain in nuanced literary analysis.


<details>
  <summary>Details</summary>
Motivation: To assess how well modern LLMs understand literary fiction by testing their ability to retrieve missing quotations from literary criticism, mirroring human literary analysis.

Method: Repurposed the RELiC dataset to create a benchmark where LLMs generate missing quotations from primary sources, requiring narrative reasoning and textual examination. Curated 292 high-quality examples.

Result: Gemini Pro 2.5 achieved 62.5% accuracy, surpassing human experts (50%), while the best open-weight model scored 29.1%. Models struggled with nuanced signals and overgeneration.

Conclusion: While some LLMs excel in literary evidence retrieval, challenges like nuanced interpretation persist. The dataset and code are released to spur further research.

Abstract: How well do modern long-context language models understand literary fiction?
We explore this question via the task of literary evidence retrieval,
repurposing the RELiC dataset of That et al. (2022) to construct a benchmark
where the entire text of a primary source (e.g., The Great Gatsby) is provided
to an LLM alongside literary criticism with a missing quotation from that work.
This setting, in which the model must generate the missing quotation, mirrors
the human process of literary analysis by requiring models to perform both
global narrative reasoning and close textual examination. We curate a
high-quality subset of 292 examples through extensive filtering and human
verification. Our experiments show that recent reasoning models, such as Gemini
Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In
contrast, the best open-weight model achieves only 29.1% accuracy, highlighting
a wide gap in interpretive reasoning between open and closed-weight models.
Despite their speed and apparent accuracy, even the strongest models struggle
with nuanced literary signals and overgeneration, signaling open challenges for
applying LLMs to literary analysis. We release our dataset and evaluation code
to encourage future work in this direction.

</details>


### [107] [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/pdf/2506.03101)
*Jonas F. Lotz, António V. Lopes, Stephan Peitz, Hendra Setiawan, Leonardo Emili*

Main category: cs.CL

TL;DR: Smaller models can predict tokenizer impact on larger models efficiently. Tokenizer choice affects multilingual tasks more than English. New intrinsic metrics based on Zipf's law improve evaluation.


<details>
  <summary>Details</summary>
Motivation: The need for accessible and reliable evaluations of tokenizer quality, as tokenizer choice significantly impacts language model performance.

Method: Systematic evaluation of English-centric and multilingual tokenizers using smaller models to predict impacts on larger models. Introduction of new intrinsic metrics inspired by Zipf's law.

Result: Tokenizer choice has negligible effects on English tasks but consistent performance differences in multilingual settings. New metrics correlate better with downstream performance.

Conclusion: A reliable framework for intrinsic tokenizer evaluations is developed, offering an efficient path for informed tokenizer selection in future language models.

Abstract: The choice of tokenizer can profoundly impact language model performance, yet
accessible and reliable evaluations of tokenizer quality remain an open
challenge. Inspired by scaling consistency, we show that smaller models can
accurately predict significant differences in tokenizer impact on larger models
at a fraction of the compute cost. By systematically evaluating both
English-centric and multilingual tokenizers, we find that tokenizer choice has
negligible effects on tasks in English but results in consistent performance
differences in multilingual settings. We propose new intrinsic tokenizer
metrics inspired by Zipf's law that correlate more strongly with downstream
performance than text compression when modeling unseen languages. By combining
several metrics to capture multiple aspects of tokenizer behavior, we develop a
reliable framework for intrinsic tokenizer evaluations. Our work offers a more
efficient path to informed tokenizer selection in future language model
development.

</details>


### [108] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/pdf/2506.03106)
*Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chao Yang, Helen Meng*

Main category: cs.CL

TL;DR: Critique-GRPO, an RL framework integrating natural language and numerical feedback, outperforms traditional methods in refining LLMs' reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing performance plateaus and persistent failures in RL with numerical feedback by leveraging natural language critiques.

Method: Proposes Critique-GRPO, an online RL framework combining natural language critiques and numerical feedback for policy optimization.

Result: Improves pass@1 scores by ~4.5-5% over baselines in mathematical and reasoning tasks.

Conclusion: Natural language feedback enhances RL fine-tuning, with Critique-GRPO showing superior performance and insights on policy exploration.

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


### [109] [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/pdf/2506.03122)
*Prashanth Vijayaraghavan, Luyao Shi, Ehsan Degan, Vandana Mukherjee, Xin Zhang*

Main category: cs.CL

TL;DR: AUTOCIRCUIT-RL, an RL-based framework using LLMs, automates analog circuit synthesis, improving validity, efficiency, and reducing duplicates compared to baselines.


<details>
  <summary>Details</summary>
Motivation: The vast design space and strict constraints in analog circuit synthesis make automation challenging, necessitating innovative solutions like AI-driven methods.

Method: AUTOCIRCUIT-RL uses a two-phase approach: instruction tuning with LLMs to generate topologies from constraints, followed by RL refinement using reward models for validity, efficiency, and output voltage.

Result: The framework generates 12% more valid circuits, improves efficiency by 14%, and reduces duplicates by 38%, achieving over 60% success with limited training data.

Conclusion: AUTOCIRCUIT-RL advances AI-driven circuit design by effectively scaling to complex circuits while maintaining efficiency and constraint adherence.

Abstract: Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.

</details>


### [110] [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/pdf/2506.03136)
*Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang*

Main category: cs.CL

TL;DR: CURE is a reinforcement learning framework for co-evolving coding and unit test generation, improving accuracy and efficiency without ground-truth supervision.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and scalable training by allowing unit testers to learn from coder mistakes without relying on ground-truth code.

Method: Uses a dedicated reward design in reinforcement learning to co-evolve coding and unit test generation capabilities.

Result: Achieves 5.3% better code generation accuracy and 9.0% Best-of-N accuracy, outperforming competitors like Qwen-Coder and DeepSeek-Coder.

Conclusion: CURE effectively improves coding and testing performance, extends to downstream tasks, and can serve as a reward model for reinforcement learning.

Abstract: We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE

</details>


### [111] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/pdf/2506.03143)
*Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, Si Qin, Lars Liden, Qingwei Lin, Huan Zhang, Tong Zhang, Jianbing Zhang, Dongmei Zhang, Jianfeng Gao*

Main category: cs.CL

TL;DR: GUI-Actor is a VLM-based method for coordinate-free GUI grounding, using an attention-based action head and grounding verifier to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of text-based coordinate generation in GUI agents, such as weak spatial-semantic alignment and handling ambiguous targets.

Method: Introduces an attention-based action head aligning a dedicated <ACTOR> token with visual patches and a grounding verifier to select action regions.

Result: Outperforms state-of-the-art methods on benchmarks, achieving scores of 40.7 and 44.6 with different backbones, and shows improved generalization.

Conclusion: GUI-Actor effectively endows VLMs with grounding capabilities without compromising their general-purpose strengths, even with minimal fine-tuning.

Abstract: One of the principal challenges in building VLM-powered GUI agents is visual
grounding, i.e., localizing the appropriate screen region for action execution
based on both the visual content and the textual plans. Most existing work
formulates this as a text-based coordinate generation task. However, these
approaches suffer from several limitations: weak spatial-semantic alignment,
inability to handle ambiguous supervision targets, and a mismatch between the
dense nature of screen coordinates and the coarse, patch-level granularity of
visual features extracted by models like Vision Transformers. In this paper, we
propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its
core, GUI-Actor introduces an attention-based action head that learns to align
a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the
model to propose one or more action regions in a single forward pass. In line
with this, we further design a grounding verifier to evaluate and select the
most plausible action region from the candidates proposed for action execution.
Extensive experiments show that GUI-Actor outperforms prior state-of-the-art
methods on multiple GUI action grounding benchmarks, with improved
generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B
even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7
with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by
incorporating the verifier, we find that fine-tuning only the newly introduced
action head (~100M parameters for 7B model) while keeping the VLM backbone
frozen is sufficient to achieve performance comparable to previous
state-of-the-art models, highlighting that GUI-Actor can endow the underlying
VLM with effective grounding capabilities without compromising its
general-purpose strengths.

</details>


### [112] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/pdf/2506.03145)
*Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, Mohanasankar Sivaprakasam*

Main category: cs.CL

TL;DR: Proposes novel methods for constructing a neuroscience knowledge graph (KG) from unlabeled text using LLMs, ontology, and embeddings, improving knowledge discovery.


<details>
  <summary>Details</summary>
Motivation: Current retrieval methods struggle with dispersed neuroscience knowledge; labeled data is scarce.

Method: Uses LLMs, neuroscience ontology, and text embeddings to build KG; introduces entity-augmented retrieval.

Result: Achieves 0.84 F1 for entity extraction; KG improves answers to 54% of questions.

Conclusion: Methods enhance knowledge discovery from unlabeled neuroscience literature.

Abstract: Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


### [113] [Causal Estimation of Tokenisation Bias](https://arxiv.org/pdf/2506.03149)
*Pietro Lesci, Clara Meister, Thomas Hofmann, Andreas Vlachos, Tiago Pimentel*

Main category: cs.CL

TL;DR: The paper investigates tokenisation bias in language models, showing how the inclusion or exclusion of subwords in a tokeniser's vocabulary affects the probability assigned to character-strings.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of tokenisation choices on model outputs, as current practices introduce bias despite the ideal of invariance.

Method: Frames tokenisation bias as a causal effect and estimates it using regression discontinuity design, comparing subwords around vocabulary cutoff points.

Result: Tokenisation significantly affects model outputs, with subword presence increasing character probabilities by up to 17 times in small models.

Conclusion: Tokenisation is a critical design choice in language modelling, with measurable biases that impact model behavior.

Abstract: Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.

</details>


### [114] [Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?](https://arxiv.org/pdf/2110.13658)
*Arij Riabi, Benoît Sagot, Djamé Seddah*

Main category: cs.CL

TL;DR: A character-based language model for low-resource NArabizi performs comparably to large multilingual/monolingual models on POS tagging and dependency parsing.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of NLP for non-standardized, low-resource languages like NArabizi.

Method: Compare a character-based model (trained on 99k NArabizi sentences) to monolingual/multilingual models for POS tagging and dependency parsing.

Result: Character-based model achieves performance close to larger models. Results confirmed on noisy French data.

Conclusion: Character-based models are effective for low-resource, high-variability languages.

Abstract: Recent impressive improvements in NLP, largely based on the success of
contextual neural language models, have been mostly demonstrated on at most a
couple dozen high-resource languages. Building language models and, more
generally, NLP systems for non-standardized and low-resource languages remains
a challenging task. In this work, we focus on North-African colloquial
dialectal Arabic written using an extension of the Latin script, called
NArabizi, found mostly on social media and messaging communication. In this
low-resource scenario with data displaying a high level of variability, we
compare the downstream performance of a character-based language model on
part-of-speech tagging and dependency parsing to that of monolingual and
multilingual models. We show that a character-based model trained on only 99k
sentences of NArabizi and fined-tuned on a small treebank of this language
leads to performance close to those obtained with the same architecture
pre-trained on large multilingual and monolingual models. Confirming these
results a on much larger data set of noisy French user-generated content, we
argue that such character-based language models can be an asset for NLP in
low-resource and high language variability set-tings.

</details>


### [115] [TransAug: Translate as Augmentation for Sentence Embeddings](https://arxiv.org/pdf/2111.00157)
*Jue Wang*

Main category: cs.CL

TL;DR: TransAug introduces a two-stage method using translated sentence pairs for data augmentation, improving sentence embeddings and achieving state-of-the-art results on STS and transfer tasks.


<details>
  <summary>Details</summary>
Motivation: Existing sentence datasets limit contrastive learning for sentence embeddings. TransAug explores translation-based augmentation to overcome this.

Method: 1. Distill a Chinese encoder from a pretrained English SimCSE encoder. 2. Update the English encoder via cross-lingual contrastive learning while freezing the Chinese encoder.

Result: Achieves state-of-the-art on STS and outperforms SimCSE and Sentence-T5. Best performance on transfer tasks in SentEval.

Conclusion: TransAug demonstrates the effectiveness of translation-based augmentation and cross-lingual learning for advancing sentence embeddings.

Abstract: While contrastive learning greatly advances the representation of sentence
embeddings, it is still limited by the size of the existing sentence datasets.
In this paper, we present TransAug (Translate as Augmentation), which provide
the first exploration of utilizing translated sentence pairs as data
augmentation for text, and introduce a two-stage paradigm to advances the
state-of-the-art sentence embeddings. Instead of adopting an encoder trained in
other languages setting, we first distill a Chinese encoder from a SimCSE
encoder (pretrained in English), so that their embeddings are close in semantic
space, which can be regraded as implicit data augmentation. Then, we only
update the English encoder via cross-lingual contrastive learning and frozen
the distilled Chinese encoder. Our approach achieves a new state-of-art on
standard semantic textual similarity (STS), outperforming both SimCSE and
Sentence-T5, and the best performance in corresponding tracks on transfer tasks
evaluated by SentEval.

</details>


### [116] [Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset](https://arxiv.org/pdf/2303.12892)
*Thanh-Dung Le, Philippe Jouvet, Rita Noumeir*

Main category: cs.CL

TL;DR: A customized Mixture of Expert (MoE) Transformer model is proposed for classifying small-scale French clinical texts, outperforming other models in speed and efficiency while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformer models face challenges in small-scale clinical text classification due to limited data and computational resources. This study aims to address these issues for hospital use.

Method: The study introduces a MoE-Transformer model tailored for small-scale French clinical texts, focusing on efficient training with limited data and low-resource computation.

Result: The MoE-Transformer achieves 87% accuracy, 87% precision, 85% recall, and 86% F1-score, training 190 times faster than biomedical pre-trained BERT models.

Conclusion: The MoE-Transformer is a viable alternative for clinical settings with limited resources, advancing small French clinical text classification despite some limitations.

Abstract: Transformer-based models have shown outstanding results in natural language
processing but face challenges in applications like classifying small-scale
clinical texts, especially with constrained computational resources. This study
presents a customized Mixture of Expert (MoE) Transformer models for
classifying small-scale French clinical texts at CHU Sainte-Justine Hospital.
The MoE-Transformer addresses the dual challenges of effective training with
limited data and low-resource computation suitable for in-house hospital use.
Despite the success of biomedical pre-trained models such as CamemBERT-bio,
DrBERT, and AliBERT, their high computational demands make them impractical for
many clinical settings. Our MoE-Transformer model not only outperforms
DistillBERT, CamemBERT, FlauBERT, and Transformer models on the same dataset
but also achieves impressive results: an accuracy of 87\%, precision of 87\%,
recall of 85\%, and F1-score of 86\%. While the MoE-Transformer does not
surpass the performance of biomedical pre-trained BERT models, it can be
trained at least 190 times faster, offering a viable alternative for settings
with limited data and computational resources. Although the MoE-Transformer
addresses challenges of generalization gaps and sharp minima, demonstrating
some limitations for efficient and accurate clinical text classification, this
model still represents a significant advancement in the field. It is
particularly valuable for classifying small French clinical narratives within
the privacy and constraints of hospital-based computational resources.

</details>


### [117] [UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities](https://arxiv.org/pdf/2403.04247)
*Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Xuming Hu, Wenhao Jiang, Hai-Tao Zheng, Hui Wang*

Main category: cs.CL

TL;DR: The paper introduces negative seed entities to improve Entity Set Expansion (ESE) for ultra-fine-grained semantic classes, addressing ambiguity and unwanted semantics. It presents UltraWiki, a dataset for Ultra-ESE, and proposes frameworks (RetExpan, GenExpan) and strategies (contrastive learning, chain-of-thought reasoning) to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Traditional ESE methods struggle with ultra-fine-grained semantic classes due to ambiguity and inability to define unwanted semantics. Negative seed entities are introduced to mitigate these issues.

Method: The paper proposes using positive and negative seed entities to define semantic classes, introduces the UltraWiki dataset, and presents retrieval-based (RetExpan) and generation-based (GenExpan) frameworks. Strategies like contrastive learning and chain-of-thought reasoning are also devised.

Result: Experiments confirm the effectiveness of the proposed strategies but highlight significant room for improvement in Ultra-ESE.

Conclusion: The paper advances Ultra-ESE by introducing negative seed entities and new frameworks, though further research is needed to optimize performance.

Abstract: Entity Set Expansion (ESE) aims to identify new entities belonging to the
same semantic class as the given set of seed entities. Traditional methods
solely relied on positive seed entities to represent the target fine-grained
semantic class, rendering them tough to represent ultra-fine-grained semantic
classes. Specifically, merely relying on positive seed entities leads to two
inherent shortcomings: (i) Ambiguity among ultra-fine-grained semantic classes.
(ii) Inability to define ``unwanted'' semantics. Hence, previous ESE methods
struggle to address the ultra-fine-grained ESE (Ultra-ESE) task. To solve this
issue, we first introduce negative seed entities in the inputs, which jointly
describe the ultra-fine-grained semantic class with positive seed entities.
Negative seed entities eliminate the semantic ambiguity by providing a contrast
between positive and negative attributes. Meanwhile, it provides a
straightforward way to express ``unwanted''. To assess model performance in
Ultra-ESE and facilitate further research, we also constructed UltraWiki, the
first large-scale dataset tailored for Ultra-ESE. UltraWiki encompasses 50,973
entities and 394,097 sentences, alongside 236 ultra-fine-grained semantic
classes, where each class is represented with 3-5 positive and negative seed
entities. Moreover, a retrieval-based framework RetExpan and a generation-based
framework GenExpan are proposed to provide powerful baselines for Ultra-ESE.
Additionally, we devised two strategies to enhance models' comprehension of
ultra-fine-grained entities' semantics: contrastive learning and
chain-of-thought reasoning. Extensive experiments confirm the effectiveness of
our proposed strategies and also reveal that there remains a large space for
improvement in Ultra-ESE.

</details>


### [118] [Revealing the Parallel Multilingual Learning within Large Language Models](https://arxiv.org/pdf/2403.09073)
*Yongyu Mu, Peinan Feng, Zhiquan Cao, Yuzhang Wu, Bei Li, Chenglong Wang, Tong Xiao, Kai Song, Tongran Liu, Chunliang Zhang, Jingbo Zhu*

Main category: cs.CL

TL;DR: Multilingual LLMs improve comprehension by translating inputs into multiple languages (PiM), enhancing performance beyond conventional ICL. Surprisingly, PiM inhibits neurons, promoting precise activation, akin to synaptic pruning in neuroscience.


<details>
  <summary>Details</summary>
Motivation: To explore how multilingual LLMs leverage parallel inputs in multiple languages (PiM) to enhance comprehension and uncover underlying neural mechanisms.

Method: Conducted experiments on 8 datasets, 7 languages, and 8 multilingual LLMs, analyzing neuron activation patterns.

Result: PiM outperforms conventional ICL, even with inferior translations, and inhibits neurons for more precise activation, resembling synaptic pruning.

Conclusion: PiM enhances LLM performance by refining neuron activation, offering insights into multilingual learning mechanisms.

Abstract: In this study, we reveal an in-context learning (ICL) capability of
multilingual large language models (LLMs): by translating the input to several
languages, we provide Parallel Input in Multiple Languages (PiM) to LLMs, which
significantly enhances their comprehension abilities. To test this capability,
we design extensive experiments encompassing 8 typical datasets, 7 languages
and 8 state-of-the-art multilingual LLMs. Experimental results show that (1)
incorporating more languages help PiM surpass the conventional ICL further; (2)
even combining with the translations that are inferior to baseline performance
can also help. Moreover, by examining the activated neurons in LLMs, we
discover a counterintuitive but interesting phenomenon. Contrary to the common
thought that PiM would activate more neurons than monolingual input to leverage
knowledge learned from diverse languages, PiM actually inhibits neurons and
promotes more precise neuron activation especially when more languages are
added. This phenomenon aligns with the neuroscience insight about synaptic
pruning, which removes less used neural connections, strengthens remainders,
and then enhances brain intelligence.

</details>


### [119] [Checkpoint Merging via Bayesian Optimization in LLM Pretraining](https://arxiv.org/pdf/2403.19390)
*Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui*

Main category: cs.CL

TL;DR: Checkpoint merging in LLM pretraining reduces computational costs via Bayesian optimization, improving performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing high computational and environmental costs of training large language models (LLMs).

Method: Proposes checkpoint merging using shared training trajectories and Bayesian optimization for weight selection.

Result: Enhances pretraining efficiency and generalization across domains.

Conclusion: Checkpoint merging offers a cost-effective solution to improve LLM pretraining.

Abstract: The rapid proliferation of large language models (LLMs) such as GPT-4 and
Gemini underscores the intense demand for resources during their training
processes, posing significant challenges due to substantial computational and
environmental costs. To alleviate this issue, we propose checkpoint merging in
pretraining LLM. This method utilizes LLM checkpoints with shared training
trajectories, and is rooted in an extensive search space exploration for the
best merging weight via Bayesian optimization. Through various experiments, we
demonstrate that: (1) Our proposed methodology exhibits the capacity to augment
pretraining, presenting an opportunity akin to obtaining substantial benefits
at minimal cost; (2) Our proposed methodology, despite requiring a given
held-out dataset, still demonstrates robust generalization capabilities across
diverse domains, a pivotal aspect in pretraining.

</details>


### [120] [LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://arxiv.org/pdf/2405.06705)
*Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, Dongsheng Li*

Main category: cs.CL

TL;DR: PedCoT, a prompting strategy based on educational theory, improves LLMs' ability to detect mathematical reasoning mistakes, outperforming baselines in zero-shot settings.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' unreliability in identifying reasoning mistakes, especially in math, using simplistic prompting.

Method: Introduces PedCoT, combining pedagogical principles, a two-stage interaction process, and grounded prompts inspired by Bloom's Cognitive Model.

Result: Outperforms baselines on math problem datasets, enabling reliable mistake identification and automatic grading.

Conclusion: Educational theory enhances prompting strategies for LLMs, proving effective for challenging tasks like mistake detection.

Abstract: Self-correction is emerging as a promising approach to mitigate the issue of
hallucination in Large Language Models (LLMs). To facilitate effective
self-correction, recent research has proposed mistake detection as its initial
step. However, current literature suggests that LLMs often struggle with
reliably identifying reasoning mistakes when using simplistic prompting
strategies. To address this challenge, we introduce a unique prompting
strategy, termed the Pedagogical Chain-of-Thought (PedCoT), which is
specifically designed to guide the identification of reasoning mistakes,
particularly mathematical reasoning mistakes. PedCoT consists of pedagogical
principles for prompts (PPP) design, two-stage interaction process (TIP) and
grounded PedCoT prompts, all inspired by the educational theory of the Bloom
Cognitive Model (BCM). We evaluate our approach on two public datasets
featuring math problems of varying difficulty levels. The experiments
demonstrate that our zero-shot prompting strategy significantly outperforms
strong baselines. The proposed method can achieve the goal of reliable
mathematical mistake identification and provide a foundation for automatic math
answer grading. The results underscore the significance of educational theory,
serving as domain knowledge, in guiding prompting strategy design for
addressing challenging tasks with LLMs effectively.

</details>


### [121] [SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale](https://arxiv.org/pdf/2406.06907)
*Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu*

Main category: cs.CL

TL;DR: The paper proposes a self-supervised method for learning sign language representations by focusing on key parts (face, hands, body pose) and avoiding reliance on inconsistent pose tracking models. It achieves similar translation performance to state-of-the-art models with significantly less computational cost.


<details>
  <summary>Details</summary>
Motivation: The challenge of effectively representing sign language while ignoring irrelevant visual differences, informed by the linguistics of signed languages.

Method: Self-supervised learning of handshapes and facial expressions from individual frames, avoiding full reliance on pose tracking models.

Result: Achieves similar translation performance to state-of-the-art models on the How2Sign dataset, using less than 3% of the compute.

Conclusion: The method is efficient and effective for sign language representation, offering a computationally lighter alternative to existing approaches.

Abstract: A persistent challenge in sign language video processing, including the task
of sign to written language translation, is how we learn representations of
sign language in an effective and efficient way that preserves the important
attributes of these languages, while remaining invariant to irrelevant visual
differences. Informed by the nature and linguistics of signed languages, our
proposed method focuses on just the most relevant parts in a signing video: the
face, hands and body pose of the signer. However, instead of fully relying on
pose estimation from off-the-shelf pose tracking models, which have
inconsistent performance for hands and faces, we propose to learn a
representation of the complex handshapes and facial expressions of sign
languages in a self-supervised fashion. Our approach is based on learning from
individual frames (rather than video sequences) and is therefore much more
efficient than prior work on sign language pre-training. Compared to a recent
model that established a new state of the art in sign language translation on
the How2Sign dataset, our approach yields similar translation performance,
using less than 3\% of the compute.

</details>


### [122] [Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems](https://arxiv.org/pdf/2406.14545)
*Đorđe Klisura, Anthony Rios*

Main category: cs.CL

TL;DR: A zero-knowledge framework exposes security vulnerabilities in text-to-SQL systems by reconstructing database schemas without prior knowledge, achieving high accuracy and revealing risks like prompt theft. A proposed protection mechanism has limited effectiveness.


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL systems are vulnerable to schema inference attacks, risking unauthorized data access. This paper aims to expose these vulnerabilities and propose countermeasures.

Method: The approach uses crafted questions and a surrogate GPT-4 model to probe text-to-SQL systems, uncovering hidden schema elements like tables and columns.

Result: High accuracy in schema reconstruction (F1 scores up to .99 for generative models, .78 for fine-tuned models) and demonstration of prompt theft in non-text-to-SQL models.

Conclusion: The study highlights severe schema leakage risks and shows that current protection mechanisms are insufficient, calling for stronger defenses.

Abstract: Text-to-SQL systems empower users to interact with databases using natural
language, automatically translating queries into executable SQL code. However,
their reliance on database schema information for SQL generation exposes them
to significant security vulnerabilities, particularly schema inference attacks
that can lead to unauthorized data access or manipulation. In this paper, we
introduce a novel zero-knowledge framework for reconstructing the underlying
database schema of text-to-SQL models without any prior knowledge of the
database. Our approach systematically probes text-to-SQL models with specially
crafted questions and leverages a surrogate GPT-4 model to interpret the
outputs, effectively uncovering hidden schema elements -- including tables,
columns, and data types. We demonstrate that our method achieves high accuracy
in reconstructing table names, with F1 scores of up to .99 for generative
models and .78 for fine-tuned models, underscoring the severity of schema
leakage risks. We also show that our attack can steal prompt information in
non-text-to-SQL models. Furthermore, we propose a simple protection mechanism
for generative models and empirically show its limitations in mitigating these
attacks.

</details>


### [123] [Free-text Rationale Generation under Readability Level Control](https://arxiv.org/pdf/2407.01384)
*Yi-Sheng Hsu, Nils Feldhus, Sherzod Hakimov*

Main category: cs.CL

TL;DR: The paper explores how large language models (LLMs) generate rationales at different readability levels, finding adaptability but mismatches with traditional metrics. Medium complexity rationales correlate with higher quality, and human annotators favor high-school-level readability.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of free-text rationales from LLMs under controlled readability levels, addressing potential misinterpretation and hallucination issues.

Method: Conducted a perturbation test where LLMs were prompted to generate rationales for specific expertise levels (e.g., sixth grade, college). Analyzed adaptability, complexity, and quality using automatic metrics and human evaluation.

Result: Rationales adapted to readability instructions, but complexity distinctions didn't fully align with traditional metrics. Medium complexity correlated with higher quality. Human annotators preferred high-school-level readability.

Conclusion: LLMs can generate adaptable rationales, though readability control needs refinement. Medium complexity rationales perform best, and high-school-level readability is favored by humans.

Abstract: Free-text rationales justify model decisions in natural language and thus
become likable and accessible among approaches to explanation across many
tasks. However, their effectiveness can be hindered by misinterpretation and
hallucination. As a perturbation test, we investigate how large language models
(LLMs) perform rationale generation under the effects of readability level
control, i.e., being prompted for an explanation targeting a specific expertise
level, such as sixth grade or college. We find that explanations are adaptable
to such instruction, though the observed distinction between readability levels
does not fully match the defined complexity scores according to traditional
readability metrics. Furthermore, the generated rationales tend to feature
medium level complexity, which correlates with the measured quality using
automatic metrics. Finally, our human annotators confirm a generally
satisfactory impression on rationales at all readability levels, with
high-school-level readability being most commonly perceived and favored.

</details>


### [124] [UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization](https://arxiv.org/pdf/2407.03525)
*Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral*

Main category: cs.CL

TL;DR: UnSeenTimeQA is a new TSQA benchmark using synthetic facts to test LLMs' temporal reasoning without real-world knowledge, revealing mixed performance and challenges in long-range and parallel event reasoning.


<details>
  <summary>Details</summary>
Motivation: To create a contamination-free TSQA benchmark that tests LLMs' genuine temporal reasoning abilities without relying on pre-trained factual knowledge.

Method: Introduces UnSeenTimeQA with synthetic facts and three question types to evaluate LLMs' temporal reasoning over sequential and parallel events.

Result: Mixed performance: LLMs do well on simpler tasks but struggle overall, especially with long-range dependencies and parallel events.

Conclusion: UnSeenTimeQA highlights LLMs' limitations in temporal reasoning, suggesting room for improvement in handling complex event structures.

Abstract: This paper introduces UnSeenTimeQA, a novel data contamination-free
time-sensitive question-answering (TSQA) benchmark. It differs from existing
TSQA benchmarks by avoiding web-searchable queries grounded in the real world.
We present a series of time-sensitive event scenarios based on synthetically
generated facts. It requires large language models (LLMs) to engage in genuine
temporal reasoning without depending on the factual knowledge acquired during
the pre-training phase. Our data generation framework enables on-demand
generation of new samples, mitigating the risk of data leakage. We designed
three types of time-sensitive questions to test LLMs' temporal reasoning
abilities over sequential and parallel event occurrences. Our evaluation of
five LLMs on synthetic fact-based TSQA reveals mixed results: while they
perform well on simpler subsets, their overall performance remains inferior as
compared to real world fact-based TSQA. Error analysis indicates that LLMs face
difficulties in reasoning over long-range event dependencies and parallel
events.

</details>


### [125] [Localizing and Mitigating Errors in Long-form Question Answering](https://arxiv.org/pdf/2407.11930)
*Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych*

Main category: cs.CL

TL;DR: HaluQuestQA is introduced as the first hallucination dataset for LFQA, with localized error annotations. It helps analyze and improve answer quality by training a feedback model and refining answers using error-informed prompts.


<details>
  <summary>Details</summary>
Motivation: LFQA answers often suffer from hallucinations and factual inconsistencies, making faithful evaluation challenging.

Method: Created HaluQuestQA with 698 QA pairs and 1.8k span-level error annotations. Trained an automatic feedback model and proposed Error-informed refinement for answer improvement.

Result: The approach reduces errors and improves answer quality, with humans preferring refined answers (84%) over baselines.

Conclusion: HaluQuestQA and the proposed methods effectively address LFQA shortcomings, enhancing answer comprehensiveness and reducing errors.

Abstract: Long-form question answering (LFQA) aims to provide thorough and in-depth
answers to complex questions, enhancing comprehension. However, such detailed
responses are prone to hallucinations and factual inconsistencies, challenging
their faithful evaluation. This work introduces HaluQuestQA, the first
hallucination dataset with localized error annotations for human-written and
model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k
span-level error annotations for five different error types by expert
annotators, along with preference judgments. Using our collected data, we
thoroughly analyze the shortcomings of long-form answers and find that they
lack comprehensiveness and provide unhelpful references. We train an automatic
feedback model on this dataset that predicts error spans with incomplete
information and provides associated explanations. Finally, we propose a
prompt-based approach, Error-informed refinement, that uses signals from the
learned feedback model to refine generated answers, which we show reduces
errors and improves answer quality across multiple models. Furthermore, humans
find answers generated by our approach comprehensive and highly prefer them
(84%) over the baseline answers.

</details>


### [126] [A Survey on Employing Large Language Models for Text-to-SQL Tasks](https://arxiv.org/pdf/2407.15186)
*Liang Shi, Zhengju Tang, Nan Zhang, Xiaotong Zhang, Zhi Yang*

Main category: cs.CL

TL;DR: A survey reviewing LLM-based Text2SQL methods, covering benchmarks, evaluation metrics, and two main approaches: prompt engineering and finetuning, with analysis and future challenges.


<details>
  <summary>Details</summary>
Motivation: To comprehensively review and analyze the emerging LLM-based Text2SQL methods, providing insights and identifying challenges.

Method: Survey of existing studies, categorization of prompt engineering and finetuning methods, and analysis of models on known datasets.

Result: Identifies characteristics of methods and models, highlighting strengths and limitations.

Conclusion: Summarizes findings and outlines challenges and future directions for LLM-based Text2SQL research.

Abstract: With the development of the Large Language Models (LLMs), a large range of
LLM-based Text-to-SQL(Text2SQL) methods have emerged. This survey provides a
comprehensive review of LLM-based Text2SQL studies. We first enumerate classic
benchmarks and evaluation metrics. For the two mainstream methods, prompt
engineering and finetuning, we introduce a comprehensive taxonomy and offer
practical insights into each subcategory. We present an overall analysis of the
above methods and various models evaluated on well-known datasets and extract
some characteristics. Finally, we discuss the challenges and future directions
in this field.

</details>


### [127] [Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes](https://arxiv.org/pdf/2407.21050)
*Yao-Shun Chuang, Chun-Teh Lee, Oluwabunmi Tokede, Guo-Hao Lin, Ryan Brandon, Trung Duong Tran, Xiaoqian Jiang, Muhammad F. Walji*

Main category: cs.CL

TL;DR: The paper proposes using AI and NLP (GPT-4 and RoBERTa) to extract periodontal diagnoses from unstructured dental records, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The complexity of updated periodontology classifications leads to incomplete structured data, necessitating better methods to extract diagnoses from unstructured text.

Method: Advanced AI and NLP techniques, including GPT-4 for synthetic note generation and RoBERTa for fine-tuning, are employed to enhance diagnostic extraction.

Result: The model achieved high accuracy (0.99 and 0.98) in diagnosing periodontal status, stage, and grade, with perfect scores in subtype categories at one site.

Conclusion: The approach improves diagnostic extraction, simplifies administrative tasks, and has broader healthcare potential, enhancing patient care quality.

Abstract: This research addresses the issue of missing structured data in dental
records by extracting diagnostic information from unstructured text. The
updated periodontology classification system's complexity has increased
incomplete or missing structured diagnoses. To tackle this, we use advanced AI
and NLP methods, leveraging GPT-4 to generate synthetic notes for fine-tuning a
RoBERTa model. This significantly enhances the model's ability to understand
medical and dental language. We evaluated the model using 120 randomly selected
clinical notes from two datasets, demonstrating its improved diagnostic
extraction accuracy. The results showed high accuracy in diagnosing periodontal
status, stage, and grade, with Site 1 scoring 0.99 and Site 2 scoring 0.98. In
the subtype category, Site 2 achieved perfect scores, outperforming Site 1.
This method enhances extraction accuracy and broadens its use across dental
contexts. The study underscores AI and NLP's transformative impact on
healthcare delivery and management. Integrating AI and NLP technologies
enhances documentation and simplifies administrative tasks by precisely
extracting complex clinical information. This approach effectively addresses
challenges in dental diagnostics. Using synthetic training data from LLMs
optimizes the training process, improving accuracy and efficiency in
identifying periodontal diagnoses from clinical notes. This innovative method
holds promise for broader healthcare applications, potentially improving
patient care quality.

</details>


### [128] [Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused](https://arxiv.org/pdf/2408.08769)
*Dingwei Chen, Feiteng Fang, Shiwen Ni, Feng Liang, Xiping Hu, Ahmadreza Argha, Hamid Alinejad-Rokny, Min Yang, Chengming Li*

Main category: cs.CL

TL;DR: The paper introduces LOL, a contrastive decoding framework that integrates lower-layer information and a truthfulness module to reduce hallucinations in LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of hallucinations in LLMs, where models generate inaccurate outputs, by improving contrastive decoding techniques.

Method: Proposes the LOL framework, which contrasts lower layers and uses a truthfulness refocused module for multi-layer fusion and improved accuracy.

Result: LOL significantly reduces hallucinations and outperforms baselines on four datasets.

Conclusion: The LOL framework effectively mitigates hallucinations in LLMs and enhances truthfulness, with code and data to be released for reproducibility.

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
various natural language processing tasks. However, they occasionally generate
inaccurate and counterfactual outputs, a phenomenon commonly referred to as
"hallucinations''. To tackle this issue, recent studies have explored
contrastive decoding between the original model and an amateur model with
induced hallucination, showing promising results. Nevertheless, this approach
can disrupt the original LLM's output distribution due to coarse contrast and
simple subtraction operations, potentially leading to errors. In this paper, we
introduce a novel contrastive decoding framework, termed LOL (LOwer Layer
Matters). Unlike prior methods that focus solely on the final layer, our
approach integrates contrastive information from lower layers to enable
multi-layer fusion during contrastive decoding. Additionally, we incorporate a
truthfulness refocused module that leverages instruction guidance to further
improve truthfulness in contrastive decoding. Extensive experiments on four
publicly available datasets demonstrate that the LOL framework significantly
mitigates hallucination while outperforming existing baselines in most cases.
For reproducibility, we will release our code and data upon acceptance.

</details>


### [129] [XTRUST: On the Multilingual Trustworthiness of Large Language Models](https://arxiv.org/pdf/2409.15762)
*Yahan Li, Yi Wang, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: XTRUST is the first multilingual benchmark evaluating LLM trustworthiness across 10 languages, revealing performance gaps in low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Address the lack of multilingual studies on LLM trustworthiness, especially as LLMs are deployed globally in sensitive fields.

Method: Introduce XTRUST, a benchmark covering diverse trustworthiness topics, and evaluate five LLMs across 10 languages.

Result: LLMs struggle with low-resource languages (e.g., Arabic, Russian), showing gaps in multilingual trustworthiness.

Conclusion: Current LLMs need improvement in multilingual trustworthiness, particularly for low-resource languages.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
a range of natural language processing (NLP) tasks, capturing the attention of
both practitioners and the broader public. A key question that now preoccupies
the AI community concerns the capabilities and limitations of these models,
with trustworthiness emerging as a central issue, particularly as LLMs are
increasingly applied in sensitive fields like healthcare and finance, where
errors can have serious consequences. However, most previous studies on the
trustworthiness of LLMs have been limited to a single language, typically the
predominant one in the dataset, such as English. In response to the growing
global deployment of LLMs, we introduce XTRUST, the first comprehensive
multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of
topics, including illegal activities, hallucination, out-of-distribution (OOD)
robustness, physical and mental health, toxicity, fairness, misinformation,
privacy, and machine ethics, across 10 different languages. Using XTRUST, we
conduct an empirical evaluation of the multilingual trustworthiness of five
widely used LLMs, offering an in-depth analysis of their performance across
languages and tasks. Our results indicate that many LLMs struggle with certain
low-resource languages, such as Arabic and Russian, highlighting the
considerable room for improvement in the multilingual trustworthiness of
current language models. The code is available at
https://github.com/LluckyYH/XTRUST.

</details>


### [130] [How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not](https://arxiv.org/pdf/2409.17044)
*Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, Paolo Merialdo, Simone Scardapane*

Main category: cs.CL

TL;DR: The paper investigates the impact of SFM, adapter, and LLM components in S2T tasks, finding SFM most critical, adapter choice moderately impactful, and dependent on SFM and LLM.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of SFM, adapter, and LLM components on S2T task performance and their interdependencies.

Method: Evaluated combinations of 5 adapter modules, 2 LLMs (Mistral, Llama), and 2 SFMs (Whisper, SeamlessM4T) on ASR and Speech Translation tasks.

Result: SFM is pivotal for performance, adapter choice has moderate impact and depends on SFM and LLM.

Conclusion: SFM is the most critical component in S2T tasks, while adapter design should consider SFM and LLM compatibility.

Abstract: The remarkable performance achieved by Large Language Models (LLM) has driven
research efforts to leverage them for a wide range of tasks and input
modalities. In speech-to-text (S2T) tasks, the emerging solution consists of
projecting the output of the encoder of a Speech Foundational Model (SFM) into
the LLM embedding space through an adapter module. However, no work has yet
investigated how much the downstream-task performance depends on each component
(SFM, adapter, LLM) nor whether the best design of the adapter depends on the
chosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter
modules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on
two widespread S2T tasks, namely Automatic Speech Recognition and Speech
Translation. Our results demonstrate that the SFM plays a pivotal role in
downstream performance, while the adapter choice has moderate impact and
depends on the SFM and LLM.

</details>


### [131] [Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning](https://arxiv.org/pdf/2410.00382)
*Shota Takashiro, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo*

Main category: cs.CL

TL;DR: A novel method, 'in-context knowledge unlearning', enables LLMs to selectively forget information based on query context, achieving high forget accuracy while retaining unrelated knowledge.


<details>
  <summary>Details</summary>
Motivation: The need for LLMs to selectively provide or withhold confidential information based on user authorization drives the development of this method.

Method: Fine-tuning pre-trained LLMs to enable prompt unlearning of target knowledge within the context, preserving unrelated information.

Result: Achieves up to 95% forget accuracy while retaining 80% unrelated knowledge, outperforming baselines. Internal behavior shows LLMs 'pretend to forget' at the last layer.

Conclusion: The method improves robustness of unlearning mechanisms in LLMs, providing insights for future research.

Abstract: As large language models (LLMs) are applied across diverse domains, the
ability to selectively unlearn specific information is becoming increasingly
essential. For instance, LLMs are expected to selectively provide confidential
information to authorized internal users, such as employees or trusted
partners, while withholding it from external users, including the general
public and unauthorized entities. Therefore, we propose a novel method termed
``in-context knowledge unlearning'', which enables the model to selectively
forget information in test-time based on the query context. Our method
fine-tunes pre-trained LLMs to enable prompt unlearning of target knowledge
within the context, while preserving unrelated information. Experiments on
TOFU, AGE and RWKU datasets using Llama2-7B/13B and Mistral-7B models
demonstrate that our method achieves up to 95% forget accuracy while retaining
80% of unrelated knowledge, significantly outperforming baselines in both
in-domain and out-of-domain scenarios. Further investigation of the model's
internal behavior revealed that while fine-tuned LLMs generate correct
predictions in the middle layers and preserve them up to the final layer.
However, the decision to forget is made only at the last layer, i.e. ``LLMs
pretend to forget''. Our findings offer valuable insight into the improvement
of the robustness of the unlearning mechanisms in LLMs, laying a foundation for
future research in the field.

</details>


### [132] [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming](https://arxiv.org/pdf/2410.02677)
*Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, Yejin Choi*

Main category: cs.CL

TL;DR: CulturalBench is a diverse cultural knowledge benchmark with 1,696 questions covering 45 regions. It challenges LMs, revealing gaps in performance, especially for underrepresented areas and tricky questions. GPT-4o outperforms others.


<details>
  <summary>Details</summary>
Motivation: To measure progress in making LMs culturally aware and helpful across diverse regions, including underrepresented ones.

Method: Constructed using Human-AI Red-Teaming methods, with 1,696 human-written and verified questions spanning 17 topics and 45 regions.

Result: LMs perform poorly (28.7%-61.5% accuracy) compared to humans (92.4%). GPT-4o leads but struggles with tricky questions and underrepresented regions.

Conclusion: CulturalBench highlights LM limitations in cultural knowledge, emphasizing the need for improvement in diverse and underrepresented contexts.

Abstract: Robust, diverse, and challenging cultural knowledge benchmarks are essential
for measuring our progress towards making LMs that are helpful across diverse
cultures. We introduce CulturalBench: a set of 1,696 human-written and
human-verified questions to assess LMs' cultural knowledge, covering 45 global
regions including underrepresented ones like Bangladesh, Zimbabwe, and Peru.
Questions are each verified by five independent annotators and span 17 diverse
topics ranging from food preferences to greeting etiquette. We construct
CulturalBench using methods inspired by Human-AI Red-Teaming. Compared to human
performance (92.4% accuracy), the hard version of CulturalBench is challenging
even for the best-performing frontier LMs, ranging from 28.7% to 61.5% in
accuracy. We find that LMs often struggle with tricky questions that have
multiple correct answers (e.g., What utensils do the Chinese usually use?),
revealing a tendency to overfit to a single answer. Our results indicate that
GPT-4o substantially outperform other models across cultures, besting local
providers (e.g., Mistral on European culture and DeepSeek on Chinese culture).
Across the board, models under-perform on questions related to North Africa,
South America and Middle East.

</details>


### [133] [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/pdf/2410.10672)
*Yahan Li, Tingyu Xia, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: The paper introduces Matrix Nuclear-Norm, a faster and scalable metric for evaluating LLMs' information compression, reducing time complexity from O(n³) to O(n²) and avoiding SVD.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics like Matrix Entropy are computationally intensive for large-scale LLMs, necessitating a more efficient alternative.

Method: Proposes Matrix Nuclear-Norm with L₁,₂-norm approximation to quantify compression proficiency, reducing complexity and eliminating SVD.

Result: Achieves 8-24x speedup over Matrix Entropy for models like CEREBRAS-GPT and Pythia, validated on benchmarks.

Conclusion: Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for LLM evaluation, balancing accuracy and speed.

Abstract: As large language models (LLMs) continue to evolve, efficient evaluation
metrics are vital for assessing their ability to compress information and
reduce redundancy. While traditional metrics like Matrix Entropy offer valuable
insights, they are computationally intensive for large-scale models due to
their \( O(n^3) \) time complexity with Singular Value Decomposition (SVD). To
mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only
serves as a metric to quantify the data compression proficiency of LLM but also
provides a convex approximation of matrix rank to capture both predictive
discriminability and diversity. By employing the \( L_{1,2}\text{-norm} \) to
further approximate the nuclear norm, we can effectively assess the model's
information compression capabilities. This approach reduces the time complexity
to \( O(n^2) \) and eliminates the need for SVD computation. Consequently, the
Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy
for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This
performance gap becomes more pronounced with larger models, as validated in
tests with other models like Pythia. Additionally, evaluations on benchmarks
and model responses confirm that our proposed Matrix Nuclear-Norm is a
reliable, scalable, and efficient tool for assessing LLMs' performance,
striking a balance between accuracy and computational efficiency. The code is
available at https://github.com/MLGroupJLU/MatrixNuclearNorm.

</details>


### [134] [Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation](https://arxiv.org/pdf/2410.10995)
*Emmanouil Zaranis, Giuseppe Attanasio, Sweta Agrawal, André F. T. Martins*

Main category: cs.CL

TL;DR: The paper investigates gender bias in Quality Estimation (QE) metrics for machine translation, revealing significant biases favoring masculine-inflected translations and penalizing neutral or feminine ones, with implications for data filtering and decoding.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked issue of social biases, particularly gender bias, in QE metrics, which can unfairly favor certain demographic groups and impact translation quality.

Method: Experiments with state-of-the-art QE metrics across various domains, datasets, and languages to analyze gender bias.

Result: Masculine-inflected translations score higher than feminine or neutral ones, even with contextual cues, and biased QE metrics affect downstream tasks like data filtering.

Conclusion: The findings highlight the urgent need for gender-centered development and evaluation of QE metrics to mitigate bias.

Abstract: Quality estimation (QE)-the automatic assessment of translation quality-has
recently become crucial across several stages of the translation pipeline, from
data curation to training and decoding. While QE metrics have been optimized to
align with human judgments, whether they encode social biases has been largely
overlooked. Biased QE risks favoring certain demographic groups over others,
e.g., by exacerbating gaps in visibility and usability. This paper defines and
investigates gender bias of QE metrics and discusses its downstream
implications for machine translation (MT). Experiments with state-of-the-art QE
metrics across multiple domains, datasets, and languages reveal significant
bias. When a human entity's gender in the source is undisclosed,
masculine-inflected translations score higher than feminine-inflected ones, and
gender-neutral translations are penalized. Even when contextual cues
disambiguate gender, using context-aware QE metrics leads to more errors in
selecting the correct translation inflection for feminine referents than for
masculine ones. Moreover, a biased QE metric affects data filtering and
quality-aware decoding. Our findings underscore the need for a renewed focus on
developing and evaluating QE metrics centered on gender.

</details>


### [135] [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/pdf/2410.11020)
*Bokai Hu, Sai Ashish Somayajula, Xin Pan, Pengtao Xie*

Main category: cs.CL

TL;DR: PPO improves NLU performance in small LLMs, outperforming supervised fine-tuning and GPT-4o on benchmarks like GLUE and SuperGLUE.


<details>
  <summary>Details</summary>
Motivation: Small LLMs underperform on NLU tasks compared to smaller models like BERT-base, prompting exploration of reinforcement learning (PPO) for improvement.

Method: NLU is framed as a reinforcement learning problem, using PPO to optimize token generation based on alignment with ground-truth labels.

Result: PPO yields a 6.3-point average improvement on GLUE, surpassing zero-shot and few-shot prompting by 38.7 and 26.1 points, respectively, and outperforms GPT-4o by over 4%.

Conclusion: Reframing NLU as a reinforcement learning problem with PPO is a promising approach for adapting LLMs to new tasks using simple rewards.

Abstract: Instruction-fine-tuned large language models (LLMs) under 14B parameters
continue to underperform on natural language understanding (NLU) tasks, often
trailing smaller models like BERT-base on benchmarks such as GLUE and
SuperGLUE. Motivated by the success of reinforcement learning in reasoning
tasks (e.g., DeepSeek), we explore Proximal Policy Optimization (PPO) as a
framework to improve the NLU capabilities of LLMs. We frame NLU as a
reinforcement learning environment, treating token generation as a sequence of
actions and optimizing for reward signals based on alignment with ground-truth
labels. PPO consistently outperforms supervised fine-tuning, yielding an
average improvement of 6.3 points on GLUE, and surpasses zero-shot and few-shot
prompting by 38.7 and 26.1 points, respectively. Notably, PPO-tuned models
outperform GPT-4o by over 4\% on average across sentiment and natural language
inference tasks, including gains of 7.3\% on the Mental Health dataset and
10.9\% on SIGA-nli. This work highlights a promising direction for adapting
LLMs to new tasks by reframing them as reinforcement learning problems,
enabling learning through simple end-task rewards rather than extensive data
curation.

</details>


### [136] [BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks](https://arxiv.org/pdf/2410.12974)
*Anna Sokol, Elizabeth Daly, Michael Hind, David Piorkowski, Xiangliang Zhang, Nuno Moniz, Nitesh Chawla*

Main category: cs.CL

TL;DR: The paper introduces BenchmarkCards, a framework to standardize and simplify the selection of benchmarks for evaluating large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting appropriate benchmarks for LLMs due to their diversity and the risk of misuse or misinterpretation.

Method: Development of BenchmarkCards, a documentation framework standardizing benchmark attributes like objectives, methodologies, and limitations, validated through user studies.

Result: BenchmarkCards simplifies benchmark selection and enhances transparency, aiding informed decision-making for LLM evaluation.

Conclusion: BenchmarkCards effectively addresses the complexity of benchmark selection for LLMs, improving usability and transparency.

Abstract: Large language models (LLMs) are powerful tools capable of handling diverse
tasks. Comparing and selecting appropriate LLMs for specific tasks requires
systematic evaluation methods, as models exhibit varying capabilities across
different domains. However, finding suitable benchmarks is difficult given the
many available options. This complexity not only increases the risk of
benchmark misuse and misinterpretation but also demands substantial effort from
LLM users, seeking the most suitable benchmarks for their specific needs. To
address these issues, we introduce \texttt{BenchmarkCards}, an intuitive and
validated documentation framework that standardizes critical benchmark
attributes such as objectives, methodologies, data sources, and limitations.
Through user studies involving benchmark creators and users, we show that
\texttt{BenchmarkCards} can simplify benchmark selection and enhance
transparency, facilitating informed decision-making in evaluating LLMs. Data &
Code: https://github.com/SokolAnn/BenchmarkCards

</details>


### [137] [A Complexity-Based Theory of Compositionality](https://arxiv.org/pdf/2410.14817)
*Eric Elmoznino, Thomas Jiralerspong, Yoshua Bengio, Guillaume Lajoie*

Main category: cs.CL

TL;DR: The paper proposes a formal definition of compositionality called 'representational compositionality,' grounded in algorithmic information theory, and validates it through experiments.


<details>
  <summary>Details</summary>
Motivation: To address the lack of measurable and mathematical definitions for compositionality, which is fundamental to intelligence in humans and AI.

Method: Introduces a definition with three properties: expressiveness, re-description as symbolic sequences, and simplicity of the semantic function. Validates it using synthetic and real-world data.

Result: The definition unifies intuitions from AI and cognitive science and is practically estimable with deep learning tools.

Conclusion: The work aims to inspire new models for compositional thought and provides accessible code for further research.

Abstract: Compositionality is believed to be fundamental to intelligence. In humans, it
underlies the structure of thought, language, and higher-level reasoning. In
AI, compositional representations can enable a powerful form of
out-of-distribution generalization, in which a model systematically adapts to
novel combinations of known concepts. However, while we have strong intuitions
about what compositionality is, we lack satisfying formal definitions for it
that are measurable and mathematical. Here, we propose such a definition, which
we call representational compositionality, that accounts for and extends our
intuitions about compositionality. The definition is conceptually simple,
quantitative, grounded in algorithmic information theory, and applicable to any
representation. Intuitively, representational compositionality states that a
compositional representation satisfies three properties. First, it must be
expressive. Second, it must be possible to re-describe the representation as a
function of discrete symbolic sequences with re-combinable parts, analogous to
sentences in natural language. Third, the function that relates these symbolic
sequences to the representation, analogous to semantics in natural language,
must be simple. Through experiments on both synthetic and real world data, we
validate our definition of compositionality and show how it unifies disparate
intuitions from across the literature in both AI and cognitive science. We also
show that representational compositionality, while theoretically intractable,
can be readily estimated using standard deep learning tools. We hope that our
definition can inspire the design of novel, theoretically-driven models that
better capture the mechanisms of compositional thought. We make our code
available at https://github.com/EricElmoznino/complexity_compositionality.

</details>


### [138] [EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/pdf/2410.21271)
*Shih-Yang Liu, Maksim Khadkevich, Nai Chit Fung, Charbel Sakr, Chao-Han Huck Yang, Chien-Yi Wang, Saurav Muralidharan, Hongxu Yin, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen*

Main category: cs.CL

TL;DR: EoRA enhances compressed LLMs with low-rank matrices, improving accuracy without fine-tuning, and offers flexible deployment with optimized CUDA kernels.


<details>
  <summary>Details</summary>
Motivation: Post-training compression of LLMs reduces efficiency but degrades accuracy and limits flexibility due to hardware constraints.

Method: EoRA augments compressed LLMs with low-rank matrices, enabling task-specific performance enhancement without fine-tuning.

Result: EoRA improves accuracy (e.g., 10.84% on ARC-Challenge) and speeds up inference by 1.4x with optimized CUDA kernels.

Conclusion: EoRA provides a flexible, efficient solution for deploying compressed LLMs with improved accuracy.

Abstract: While post-training compression techniques effectively reduce the memory
footprint, latency, and power consumption of Large Language Models (LLMs), they
often result in noticeable accuracy degradation and remain limited by hardware
and kernel constraints that restrict supported compression formats ultimately
reducing flexibility across a wide range of deployment scenarios. In this work,
we propose EoRA, a novel fine-tuning-free method that augments compressed LLMs
with low-rank matrices, allowing users to rapidly enhance task-specific
performance and freely balance the trade-off between accuracy and computational
overhead beyond the constraints of compression formats. EoRA consistently
outperforms prior training-free low rank methods in recovering the accuracy of
compressed LLMs, achieving notable accuracy improvements (e.g.,
$\mathbf{10.84\%}$ on ARC-Challenge, $\mathbf{6.74\%}$ on MathQA, and
$\mathbf{6.74\%}$ on GSM8K) for LLaMA3-8B compressed to 3-bit. We also
introduce an optimized CUDA kernel, accelerating inference by up to 1.4x and
reducing memory overhead through quantizing EoRA. Overall, EoRA offers a prompt
solution for improving the accuracy of compressed models under varying user
requirements, enabling more efficient and flexible deployment of LLMs. Code is
available at https://github.com/NVlabs/EoRA.

</details>


### [139] [Self-Evolved Reward Learning for LLMs](https://arxiv.org/pdf/2411.00418)
*Chenghua Huang, Zhizhen Fan, Lu Wang, Fangkai Yang, Pu Zhao, Zeqi Lin, Qingwei Lin, Dongmei Zhang, Saravan Rajmohan, Qi Zhang*

Main category: cs.CL

TL;DR: The paper introduces Self-Evolved Reward Learning (SER), a method to improve reward models (RMs) in RLHF by generating additional training data iteratively, reducing reliance on costly human labels.


<details>
  <summary>Details</summary>
Motivation: High-quality human or AI labels for RMs are expensive and may introduce biases, limiting the effectiveness of RLHF as language models improve.

Method: Proposes SER, where the RM self-generates training data to iteratively enhance its performance, tested on datasets like HH-RLHF and UltraFeedback using models such as Mistral and Llama 3.

Result: SER improves RM performance robustly even with limited human-annotated data, enhancing large language model capabilities.

Conclusion: SER offers a cost-effective and scalable alternative to traditional human-labeled RMs, advancing RLHF for language models.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for
aligning language models with human preferences, playing a pivotal role in the
success of conversational models like GPT-4, ChatGPT, and Llama 2. A core
challenge in employing RLHF lies in training a reliable reward model (RM),
which relies on high-quality labels typically provided by human experts or
advanced AI system. These methods can be costly and may introduce biases that
affect the language model's responses. As language models improve, human input
may become less effective in further enhancing their performance. In this
paper, we propose Self-Evolved Reward Learning (SER), a novel approach where
the RM generates additional training data to iteratively improve itself. We
conducted extensive experiments on multiple datasets such as HH-RLHF and
UltraFeedback, using models like Mistral and Llama 3, and compare SER against
various baselines. Our results demonstrate that even with limited
human-annotated data, learning from self-feedback can robustly enhance RM
performance, thereby boosting the capabilities of large language models (LLMs).
Resources of this paper can be found at https://aka.ms/ser

</details>


### [140] [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/pdf/2411.02430)
*Lin Wang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Zhitao Zhang*

Main category: cs.CL

TL;DR: The paper introduces MECEC, a task for explaining emotional causes in multimodal conversations, and proposes FAME-Net, a method leveraging LLMs to analyze visual data for emotion cause identification.


<details>
  <summary>Details</summary>
Motivation: Existing emotion cause research is limited to single-modality and lacks nuanced explanations, prompting the need for multimodal analysis.

Method: FAME-Net uses LLMs to interpret facial expressions in videos, capturing emotional causes by exploiting facial emotion contagion.

Result: FAME-Net outperforms baselines on the new ECEM dataset, demonstrating effective emotion cause explanation.

Conclusion: The study advances multimodal emotion cause analysis, offering a dataset and method for nuanced emotional trigger identification.

Abstract: Multimodal conversation, a crucial form of human communication, carries rich
emotional content, making the exploration of the causes of emotions within it a
research endeavor of significant importance. However, existing research on the
causes of emotions typically employs an utterance selection method within a
single textual modality to locate causal utterances. This approach remains
limited to coarse-grained assessments, lacks nuanced explanations of emotional
causation, and demonstrates inadequate capability in identifying multimodal
emotional triggers. Therefore, we introduce a task-\textbf{Multimodal Emotion
Cause Explanation in Conversation (MECEC)}. This task aims to generate a
summary based on the multimodal context of conversations, clearly and
intuitively describing the reasons that trigger a given emotion. To adapt to
this task, we develop a new dataset (ECEM) based on the MELD dataset. ECEM
combines video clips with detailed explanations of character emotions, helping
to explore the causal factors behind emotional expression in multimodal
conversations. A novel approach, FAME-Net, is further proposed, that harnesses
the power of Large Language Models (LLMs) to analyze visual data and accurately
interpret the emotions conveyed through facial expressions in videos. By
exploiting the contagion effect of facial emotions, FAME-Net effectively
captures the emotional causes of individuals engaged in conversations. Our
experimental results on the newly constructed dataset show that FAME-Net
outperforms several excellent baselines. Code and dataset are available at
https://github.com/3222345200/FAME-Net.

</details>


### [141] [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://arxiv.org/pdf/2411.02528)
*Lindia Tjuatja, Graham Neubig, Tal Linzen, Sophie Hao*

Main category: cs.CL

TL;DR: MORCELA, a new linking theory, adjusts for length and unigram frequency effects in LM probabilities more effectively than SLOR, showing that larger LMs require less adjustment for frequency due to better prediction of rare words.


<details>
  <summary>Details</summary>
Motivation: Prior methods assume uniform adjustments for length and unigram frequency effects across LMs, which may not hold true. MORCELA addresses this by learning optimal adjustments from data.

Method: Proposes MORCELA, a data-driven linking theory with learned parameters for length and unigram frequency adjustments, tested on Pythia and OPT transformer LMs.

Result: MORCELA outperforms SLOR, showing overcorrection in SLOR's adjustments. Larger LMs need less frequency adjustment due to better rare-word prediction.

Conclusion: MORCELA provides a better framework for comparing LM and human judgments, revealing insights into LM behavior, especially for larger models.

Abstract: When comparing the linguistic capabilities of language models (LMs) with
humans using LM probabilities, factors such as the length of the sequence and
the unigram frequency of lexical items have a significant effect on LM
probabilities in ways that humans are largely robust to. Prior works in
comparing LM and human acceptability judgments treat these effects uniformly
across models, making a strong assumption that models require the same degree
of adjustment to control for length and unigram frequency effects. We propose
MORCELA, a new linking theory between LM scores and acceptability judgments
where the optimal level of adjustment for these effects is estimated from data
via learned parameters for length and unigram frequency. We first show that
MORCELA outperforms a commonly used linking theory for acceptability - SLOR
(Pauls and Klein, 2012; Lau et al. 2017) - across two families of transformer
LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of
adjustment in SLOR for length and unigram frequency overcorrect for these
confounds, and that larger models require a lower relative degree of adjustment
for unigram frequency, though a significant amount of adjustment is still
necessary for all models. Finally, our subsequent analysis shows that larger
LMs' lower susceptibility to frequency effects can be explained by an ability
to better predict rarer words in context.

</details>


### [142] [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/pdf/2411.04975)
*Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao*

Main category: cs.CL

TL;DR: SuffixDecoding improves LLM inference latency for repetitive tasks by caching sequences and adapting speculation length.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods don't exploit repetitive, predictable sequences in agentic workloads.

Method: Uses suffix trees to cache sequences and adaptively adjusts speculation length based on acceptance likelihood.

Result: Achieves up to 5.3× speedup, outperforming state-of-the-art methods.

Conclusion: SuffixDecoding is effective for agentic workloads and is open-sourced.

Abstract: Speculative decoding is widely adopted to reduce latency in large language
model (LLM) inference by leveraging smaller draft models capable of handling
diverse user tasks. However, emerging AI applications, such as LLM-based
agents, present unique workload characteristics: instead of diverse independent
requests, agentic frameworks typically submit repetitive inference requests,
such as multi-agent pipelines performing similar subtasks or self-refinement
loops iteratively enhancing outputs. These workloads result in long and highly
predictable sequences, which current speculative decoding methods do not
effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a
novel method that utilizes efficient suffix trees to cache long token sequences
from prompts and previous outputs. By adaptively speculating more tokens when
acceptance likelihood is high and fewer when it is low, SuffixDecoding
effectively exploits opportunities for longer speculations while conserving
computation when those opportunities are limited. Evaluations on agentic
benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that
SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming
state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like
EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token
Recycling. SuffixDecoding is open-sourced at
https://github.com/snowflakedb/ArcticInference.

</details>


### [143] [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/pdf/2411.07965)
*Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma*

Main category: cs.CL

TL;DR: The paper introduces SHARP, a benchmark to study interactive hallucination in LLMs, revealing limitations in post-training methods for role-playing models.


<details>
  <summary>Details</summary>
Motivation: Existing research overlooks hallucination in LLM social interactions and lacks generalizability. The study aims to address this by analyzing interactive patterns across diverse worldviews.

Method: Defines interactive hallucination via stance transfer and constructs SHARP, a benchmark using commonsense knowledge graphs and LLMs' hallucination properties for multi-role interactions.

Result: Experiments confirm the paradigm's effectiveness, stability, and highlight factors influencing metrics, challenging conventional hallucination mitigation.

Conclusion: The work exposes a key limitation in role-playing LLMs: post-training methods obscure knowledge under style, leading to interactive hallucination.

Abstract: The advanced role-playing capabilities of Large Language Models (LLMs) have
enabled rich interactive scenarios, yet existing research in social
interactions neglects hallucination while struggling with poor generalizability
and implicit character fidelity judgments. To bridge this gap, motivated by
human behaviour, we introduce a generalizable and explicit paradigm for
uncovering interactive patterns of LLMs across diverse worldviews.
Specifically, we first define interactive hallucination through stance
transfer, then construct SHARP, a benchmark built by extracting relations from
commonsense knowledge graphs and utilizing LLMs' inherent hallucination
properties to simulate multi-role interactions. Extensive experiments confirm
our paradigm's effectiveness and stability, examine the factors that influence
these metrics, and challenge conventional hallucination mitigation solutions.
More broadly, our work reveals a fundamental limitation in popular
post-training methods for role-playing LLMs: the tendency to obscure knowledge
beneath style, resulting in monotonous yet human-like behaviors - interactive
hallucination.

</details>


### [144] [Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts](https://arxiv.org/pdf/2411.11479)
*Jingxuan Li, Yuning Yang, Shengqi Yang, Linfan Zhang, Ying Nian Wu*

Main category: cs.CL

TL;DR: The paper introduces Value-Spectrum, a VQA benchmark to evaluate VLMs on human values and personality traits, using a dataset of 50,000+ short videos. It reveals VLM variations in value-oriented tasks and adaptability in role-playing.


<details>
  <summary>Details</summary>
Motivation: Current VLM evaluations focus on functional tasks, ignoring abstract dimensions like human values. The paper aims to fill this gap by assessing VLMs on Schwartz's value dimensions.

Method: A VLM agent pipeline is designed to simulate video browsing, using a vector database of 50,000+ short videos from platforms like TikTok. The benchmark evaluates VLMs on value-oriented content and persona adaptability.

Result: Benchmarking shows variations in how VLMs handle value-based content and their ability to adopt specific personas when prompted, revealing insights into model adaptability.

Conclusion: Value-Spectrum serves as a comprehensive tool for evaluating VLMs in value-based tasks and persona simulation, with potential for broader applications.

Abstract: The recent progress in Vision-Language Models (VLMs) has broadened the scope
of multimodal applications. However, evaluations often remain limited to
functional tasks, neglecting abstract dimensions such as personality traits and
human values. To address this gap, we introduce Value-Spectrum, a novel Visual
Question Answering (VQA) benchmark aimed at assessing VLMs based on Schwartz's
value dimensions that capture core human values guiding people's preferences
and actions. We design a VLM agent pipeline to simulate video browsing and
construct a vector database comprising over 50,000 short videos from TikTok,
YouTube Shorts, and Instagram Reels. These videos span multiple months and
cover diverse topics, including family, health, hobbies, society, technology,
etc. Benchmarking on Value-Spectrum highlights notable variations in how VLMs
handle value-oriented content. Beyond identifying VLMs' intrinsic preferences,
we also explore the ability of VLM agents to adopt specific personas when
explicitly prompted, revealing insights into the adaptability of the model in
role-playing scenarios. These findings highlight the potential of
Value-Spectrum as a comprehensive evaluation set for tracking VLM preferences
in value-based tasks and abilities to simulate diverse personas. The complete
code and data are available at: https://github.com/Jeremyyny/Value-Spectrum.

</details>


### [145] [HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter](https://arxiv.org/pdf/2411.15462)
*Manuel Tonneau, Diyi Liu, Niyati Malhotra, Scott A. Hale, Samuel P. Fraiberger, Victor Orozco-Olvera, Paul Röttger*

Main category: cs.CL

TL;DR: HateDay, a global hate speech dataset, reveals biases in existing detection models, showing low real-world performance, especially for non-European languages, and highlights the need for better evaluation data.


<details>
  <summary>Details</summary>
Motivation: To address the unclear real-world effectiveness of hate speech detection models due to biased evaluation datasets.

Method: Constructed HateDay, a representative global dataset from tweets in eight languages and four English-speaking countries, and evaluated model performance.

Result: Found low detection performance, especially for non-European languages, due to model difficulties in distinguishing hate from offensive speech and dataset mismatches.

Conclusion: Current models are unsuitable for automatic moderation without human oversight, emphasizing the need for diverse evaluation data.

Abstract: To address the global challenge of online hate speech, prior research has
developed detection models to flag such content on social media. However, due
to systematic biases in evaluation datasets, the real-world effectiveness of
these models remains unclear, particularly across geographies. We introduce
HateDay, the first global hate speech dataset representative of social media
settings, constructed from a random sample of all tweets posted on September
21, 2022 and covering eight languages and four English-speaking countries.
Using HateDay, we uncover substantial variation in the prevalence and
composition of hate speech across languages and regions. We show that
evaluations on academic datasets greatly overestimate real-world detection
performance, which we find is very low, especially for non-European languages.
Our analysis identifies key drivers of this gap, including models' difficulty
to distinguish hate from offensive speech and a mismatch between the target
groups emphasized in academic datasets and those most frequently targeted in
real-world settings. We argue that poor model performance makes public models
ill-suited for automatic hate speech moderation and find that high moderation
rates are only achievable with substantial human oversight. Our results
underscore the need to evaluate detection systems on data that reflects the
complexity and diversity of real-world social media.

</details>


### [146] [SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction](https://arxiv.org/pdf/2411.16765)
*Shester Gueuwou, Xiaodan Du, Greg Shakhnarovich, Karen Livescu, Alexander H. Liu*

Main category: cs.CL

TL;DR: SHuBERT is a self-supervised model for sign language processing, outperforming traditional task-specific methods by learning contextual representations from multi-stream visual input.


<details>
  <summary>Details</summary>
Motivation: Traditional sign language models lack transfer learning capabilities and ignore temporal relationships. SHuBERT addresses these gaps by leveraging unlabeled data and contextual learning.

Method: SHuBERT uses masked token prediction on multi-stream visual input (hand, face, body pose) from 1,000 hours of ASL video, adapting BERT-like objectives.

Result: State-of-the-art performance in sign language translation, isolated recognition, and fingerspelling detection.

Conclusion: SHuBERT demonstrates the effectiveness of self-supervised contextual learning for sign language processing, enabling transfer across tasks.

Abstract: Sign language processing has traditionally relied on task-specific models,
limiting the potential for transfer learning across tasks. Pre-training methods
for sign language have typically focused on either supervised pre-training,
which cannot take advantage of unlabeled data, or context-independent (frame or
video segment) representations, which ignore the effects of relationships
across time in sign language. We introduce SHuBERT (Sign Hidden-Unit BERT), a
self-supervised contextual representation model learned from approximately
1,000 hours of American Sign Language video. SHuBERT adapts masked token
prediction objectives to multi-stream visual sign language input, learning to
predict multiple targets corresponding to clustered hand, face, and body pose
streams. SHuBERT achieves state-of-the-art performance across multiple tasks
including sign language translation, isolated sign language recognition, and
fingerspelling detection.

</details>


### [147] [Is it the end of (generative) linguistics as we know it?](https://arxiv.org/pdf/2412.12797)
*Cristiano Chesi*

Main category: cs.CL

TL;DR: The paper critiques Chomsky's generative linguistics, supporting Piantadosi's critique on Poverty of Stimulus and simplicity in Minimalism, advocating for updated formalizations and empirical datasets.


<details>
  <summary>Details</summary>
Motivation: To address flaws in generative linguistics, particularly the Poverty of Stimulus hypothesis and simplicity in Minimalism, and advocate for theoretical updates.

Method: Adopts computational, theoretical, and experimental perspectives to analyze two key issues.

Result: Partial support for Piantadosi's critique; highlights the need for precise formalizations and empirical datasets.

Conclusion: Generative linguistics must update its formal foundations and use standardized empirical evidence to maintain relevance and achieve descriptive/explanatory adequacy.

Abstract: A significant debate has emerged in response to a paper written by Steven
Piantadosi (Piantadosi, 2023) and uploaded to the LingBuzz platform, the open
archive for generative linguistics. Piantadosi's dismissal of Chomsky's
approach is ruthless, but generative linguists deserve it. In this paper, I
will adopt three idealized perspectives -- computational, theoretical, and
experimental -- to focus on two fundamental issues that lend partial support to
Piantadosi's critique: (a) the evidence challenging the Poverty of Stimulus
(PoS) hypothesis and (b) the notion of simplicity as conceived within
mainstream Minimalism. In conclusion, I argue that, to reclaim a central role
in language studies, generative linguistics -- representing a prototypical
theoretical perspective on language -- needs a serious update leading to (i)
more precise, consistent, and complete formalizations of foundational
intuitions and (ii) the establishment and utilization of a standardized dataset
of crucial empirical evidence to evaluate the theory's adequacy. On the other
hand, ignoring the formal perspective leads to major drawbacks in both
computational and experimental approaches. Neither descriptive nor explanatory
adequacy can be easily achieved without the precise formulation of general
principles that can be challenged empirically.

</details>


### [148] [SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation](https://arxiv.org/pdf/2412.13649)
*Jialong Wu, Zhenglin Wang, Linhai Zhang, Yilong Lai, Yulan He, Deyu Zhou*

Main category: cs.CL

TL;DR: SCOPE optimizes KV cache separately for prefill and decoding phases in LLMs, improving long-context generation efficiency.


<details>
  <summary>Details</summary>
Motivation: KV cache is a bottleneck in LLMs for long-context generation, with decoding phase optimization often overlooked. Observations show excessive prefill compression harms reasoning tasks and heavy hitters deviate in long outputs.

Method: SCOPE preserves KV cache in prefill for essential info and uses a sliding strategy for decoding-phase heavy hitters. Adaptive and discontinuous strategies optimize memory usage and transfer.

Result: Experiments on LongGenBench demonstrate SCOPE's effectiveness, generalization, and compatibility with other KV compression methods.

Conclusion: SCOPE efficiently addresses KV cache bottlenecks, enhancing long-output generation in LLMs.

Abstract: Key-Value (KV) cache has become a bottleneck of LLMs for long-context
generation. Despite the numerous efforts in this area, the optimization for the
decoding phase is generally ignored. However, we believe such optimization is
crucial, especially for long-output generation tasks based on the following two
observations: (i) Excessive compression during the prefill phase, which
requires specific full context impairs the comprehension of the reasoning task;
(ii) Deviation of heavy hitters occurs in the reasoning tasks with long
outputs. Therefore, SCOPE, a simple yet efficient framework that separately
performs KV cache optimization during the prefill and decoding phases, is
introduced. Specifically, the KV cache during the prefill phase is preserved to
maintain the essential information, while a novel strategy based on sliding is
proposed to select essential heavy hitters for the decoding phase. Memory usage
and memory transfer are further optimized using adaptive and discontinuous
strategies. Extensive experiments on LongGenBench show the effectiveness and
generalization of SCOPE and its compatibility as a plug-in to other
prefill-only KV compression methods.

</details>


### [149] [Can Input Attributions Explain Inductive Reasoning in In-Context Learning?](https://arxiv.org/pdf/2412.15628)
*Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki*

Main category: cs.CL

TL;DR: The paper investigates whether input attribution (IA) methods can identify influential examples in in-context learning (ICL) for large language models (LLMs), using synthetic diagnostic tasks of inductive reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding the internal reasoning process of LLMs during ICL, especially identifying which few-shot examples contribute to task solving, remains a challenge.

Method: The authors design synthetic diagnostic tasks with ambiguous examples and one critical disambiguating example, then test conventional IA methods to track reasoning processes.

Result: A simple IA method performs best, and gradient-based IA methods struggle more with larger models.

Conclusion: The study highlights limitations of current IA methods for interpreting ICL in LLMs, especially as models scale.

Abstract: Interpreting the internal process of neural models has long been a challenge.
This challenge remains relevant in the era of large language models (LLMs) and
in-context learning (ICL); for example, ICL poses a new issue of interpreting
which example in the few-shot examples contributed to identifying/solving the
task. To this end, in this paper, we design synthetic diagnostic tasks of
inductive reasoning, inspired by the generalization tests typically adopted in
psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their
underlying rule, and one critical example disambiguates it. The question is
whether conventional input attribution (IA) methods can track such a reasoning
process, i.e., identify the influential example, in ICL. Our experiments
provide several practical findings; for example, a certain simple IA method
works the best, and the larger the model, the generally harder it is to
interpret the ICL with gradient-based IA methods.

</details>


### [150] [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/pdf/2412.17063)
*Esther Shizgal, Eitan Wagner, Renana Keydar, Omri Abend*

Main category: cs.CL

TL;DR: A computational method analyzes character development in Holocaust survivor testimonies, focusing on religious trajectories to identify common patterns in belief and practice.


<details>
  <summary>Details</summary>
Motivation: To explore how protagonists' inner and outer changes, especially in religiosity, evolve over narratives, using Holocaust survivor testimonies as a case study.

Method: Analyzes first-person testimonies, clustering religious trajectories to identify common sequences in belief and practice.

Result: Found constant belief dispositions and oscillating practice structures, providing insights for historical and sociological research.

Conclusion: Shows NLP's potential for analyzing character evolution via thematic trajectories in narratives.

Abstract: This work presents a computational approach to analyze character development
along the narrative timeline. The analysis characterizes the inner and outer
changes the protagonist undergoes within a narrative, and the interplay between
them. We consider transcripts of Holocaust survivor testimonies as a test case,
each telling the story of an individual in first-person terms. We focus on the
survivor's religious trajectory, examining the evolution of their disposition
toward religious belief and practice along the testimony. Clustering the
resulting trajectories in the dataset, we identify common sequences in the
data. Our findings highlight multiple common structures of religiosity across
the narratives: in terms of belief, most present a constant disposition, while
for practice, most present an oscillating structure, serving as valuable
material for historical and sociological research. This work demonstrates the
potential of natural language processing techniques for analyzing character
evolution through thematic trajectories in narratives.

</details>


### [151] [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/pdf/2412.17451)
*Wei Liu, Junlong Li, Xiwen Zhang, Fan Zhou, Yu Cheng, Junxian He*

Main category: cs.CL

TL;DR: The paper introduces M-STAR, a framework for self-evolving training in multimodal reasoning, addressing performance saturation and optimizing design principles inspired by reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality chain-of-thought data and the underexplored effectiveness of self-evolving training in multimodal reasoning motivated this work.

Method: The authors reframe self-evolving training through reinforcement learning, focusing on Training Method, Reward Model, and Prompt Variation, and propose an automatic balancing mechanism to mitigate saturation.

Result: M-STAR achieves consistent performance gains across models of varying sizes and diverse benchmarks.

Conclusion: The framework enhances multimodal reasoning capabilities and addresses saturation, with all resources made publicly available.

Abstract: Self-evolving trainin--where models iteratively learn from their own
outputs--has emerged as a key approach for complex reasoning tasks, addressing
the scarcity of high-quality chain-of-thought data. However, its effectiveness
in multimodal reasoning, a domain more intricate than text-only reasoning,
remains underexplored, and the understanding of critical factors in this
training paradigm remains limited. Furthermore, a central challenge for this
training method is performance saturation, which impedes further improvements
and scalability. Inspired by reinforcement learning (RL), in this paper, we
reframe self-evolving training for multimodal reasoning through the lens of RL,
identifying three pivotal factors: Training Method, Reward Model, and Prompt
Variation. Through systematic analysis, we establish relatively optimal design
principles that significantly enhance multimodal reasoning capabilities.
Moreover, delving deeper into training dynamics, we uncover the roots of
saturation and propose a new automatic balancing mechanism to mitigate this
limitation. Building on these insights, we propose M-STAR (Multimodal
Self-evolving Training for Reasoning), a framework that achieves consistent
performance gains across models of varying sizes and diverse benchmarks. All
resources are made publicly available at https://mstar-lmm.github.io.

</details>


### [152] [Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse](https://arxiv.org/pdf/2412.17533)
*Anna Kołos, Katarzyna Lorenc, Emilia Wiśnios, Agnieszka Karlińska*

Main category: cs.CL

TL;DR: forePLay introduces a Polish dataset for erotic content detection, showing specialized language models outperform multilingual ones, especially transformers.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current tools in non-English contexts, particularly for Polish erotic content detection.

Method: Developed a Polish dataset (forePLay) with 24k annotated sentences and a multidimensional taxonomy. Evaluated specialized vs. multilingual models.

Result: Specialized Polish models, especially transformers, perform better than multilingual alternatives.

Conclusion: The dataset and analysis provide a framework for linguistically-aware moderation, with insights for morphologically complex languages.

Abstract: The surge in online content has created an urgent demand for robust detection
systems, especially in non-English contexts where current tools demonstrate
significant limitations. We present forePLay, a novel Polish language dataset
for erotic content detection, featuring over 24k annotated sentences with a
multidimensional taxonomy encompassing ambiguity, violence, and social
unacceptability dimensions. Our comprehensive evaluation demonstrates that
specialized Polish language models achieve superior performance compared to
multilingual alternatives, with transformer-based architectures showing
particular strength in handling imbalanced categories. The dataset and
accompanying analysis establish essential frameworks for developing
linguistically-aware content moderation systems, while highlighting critical
considerations for extending such capabilities to morphologically complex
languages.

</details>


### [153] [Instruction-Following Pruning for Large Language Models](https://arxiv.org/pdf/2501.02086)
*Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei*

Main category: cs.CL

TL;DR: The paper introduces a dynamic structured pruning method for LLMs, where the pruning mask adapts based on user instructions, outperforming static pruning and dense models.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance of pruned LLMs by moving beyond static pruning to a dynamic, input-dependent approach.

Method: Proposes 'instruction-following pruning' with a sparse mask predictor that dynamically selects relevant parameters based on user instructions, jointly optimized with the LLM.

Result: The 3B activated model outperforms the 3B dense model by 5-8 points in domains like math and coding, matching a 9B model's performance.

Conclusion: Dynamic pruning based on user instructions enhances model efficiency and performance, offering superior results over traditional methods.

Abstract: With the rapid scaling of large language models (LLMs), structured pruning
has become a widely used technique to learn efficient, smaller models from
larger ones, delivering superior performance compared to training similarly
sized models from scratch. In this paper, we move beyond the traditional static
pruning approach of determining a fixed pruning mask for a model, and propose a
dynamic approach to structured pruning. In our method, the pruning mask is
input-dependent and adapts dynamically based on the information described in a
user instruction. Our approach, termed "instruction-following pruning",
introduces a sparse mask predictor that takes the user instruction as input and
dynamically selects the most relevant model parameters for the given task. To
identify and activate effective parameters, we jointly optimize the sparse mask
predictor and the LLM, leveraging both instruction-following data and the
pre-training corpus. Experimental results demonstrate the effectiveness of our
approach on a wide range of evaluation benchmarks. For example, our 3B
activated model improves over the 3B dense model by 5-8 points of absolute
margin on domains such as math and coding, and rivals the performance of a 9B
model.

</details>


### [154] [Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection](https://arxiv.org/pdf/2501.02295)
*Yachao Zhao, Bo Wang, Yan Wang, Dongming Zhao, Ruifang He, Yuexian Hou*

Main category: cs.CL

TL;DR: The paper investigates explicit and implicit biases in LLMs, revealing a significant inconsistency between the two. It introduces a self-reflection-based framework to measure these biases and explores factors like training data, model size, and alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Prior research on LLM biases has focused mainly on explicit bias, neglecting implicit bias and its relation to explicit bias. This study aims to bridge this gap using social psychology theories.

Method: A two-phase self-reflection framework: first measures implicit bias via simulated psychological assessments, then evaluates explicit bias by prompting LLMs to analyze their own outputs. Experiments are conducted across multiple social dimensions.

Result: LLMs show strong implicit biases despite mild explicit biases. Explicit bias decreases with larger training data and models, while implicit bias increases. Alignment techniques reduce explicit bias but are less effective against implicit bias.

Conclusion: The study highlights the need to address both explicit and implicit biases in LLMs, as current alignment methods are insufficient for mitigating implicit biases.

Abstract: Large Language Models (LLMs) have been shown to exhibit various biases and
stereotypes in their generated content. While extensive research has
investigated biases in LLMs, prior work has predominantly focused on explicit
bias, with minimal attention to implicit bias and the relation between these
two forms of bias. This paper presents a systematic framework grounded in
social psychology theories to investigate and compare explicit and implicit
biases in LLMs. We propose a novel self-reflection-based evaluation framework
that operates in two phases: first measuring implicit bias through simulated
psychological assessment methods, then evaluating explicit bias by prompting
LLMs to analyze their own generated content. Through extensive experiments on
advanced LLMs across multiple social dimensions, we demonstrate that LLMs
exhibit a substantial inconsistency between explicit and implicit biases: while
explicit bias manifests as mild stereotypes, implicit bias exhibits strong
stereotypes. We further investigate the underlying factors contributing to this
explicit-implicit bias inconsistency, examining the effects of training data
scale, model size, and alignment techniques. Experimental results indicate that
while explicit bias declines with increased training data and model size,
implicit bias exhibits a contrasting upward trend. Moreover, contemporary
alignment methods effectively suppress explicit bias but show limited efficacy
in mitigating implicit bias.

</details>


### [155] [TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification](https://arxiv.org/pdf/2501.03835)
*Yindu Su, Huike Zou, Lin Sun, Ting Zhang, Haiyang Yang, Liyu Chen, David Lo, Qingheng Zhang, Shuguang Han, Jufeng Chen*

Main category: cs.CL

TL;DR: TACLR is a retrieval-based method for Product Attribute Value Identification (PAVI) that handles implicit and OOD values, scales efficiently, and supports industrial deployment.


<details>
  <summary>Details</summary>
Motivation: Existing PAVI methods struggle with implicit values, OOD values, and normalization, limiting their effectiveness in e-commerce applications.

Method: TACLR formulates PAVI as a retrieval task using embeddings, employs contrastive training with taxonomy-aware hard negative sampling, and adaptive inference with dynamic thresholds.

Result: TACLR effectively handles implicit and OOD values, scales to large datasets, and supports efficient industrial deployment, validated by experiments.

Conclusion: TACLR is successfully deployed on Xianyu, demonstrating its practicality and effectiveness for large-scale PAVI tasks.

Abstract: Product Attribute Value Identification (PAVI) involves identifying attribute
values from product profiles, a key task for improving product search,
recommendation, and business analytics on e-commerce platforms. However,
existing PAVI methods face critical challenges, such as inferring implicit
values, handling out-of-distribution (OOD) values, and producing normalized
outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive
Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR
formulates PAVI as an information retrieval task by encoding product profiles
and candidate values into embeddings and retrieving values based on their
similarity. It leverages contrastive training with taxonomy-aware hard negative
sampling and employs adaptive inference with dynamic thresholds. TACLR offers
three key advantages: (1) it effectively handles implicit and OOD values while
producing normalized outputs; (2) it scales to thousands of categories, tens of
thousands of attributes, and millions of values; and (3) it supports efficient
inference for high-load industrial deployment. Extensive experiments on
proprietary and public datasets validate the effectiveness and efficiency of
TACLR. Further, it has been successfully deployed on the real-world e-commerce
platform Xianyu, processing millions of product listings daily with frequently
updated, large-scale attribute taxonomies. We release the code to facilitate
reproducibility and future research at https://github.com/SuYindu/TACLR.

</details>


### [156] [FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings](https://arxiv.org/pdf/2501.06645)
*Tong Liu, Xiao Yu, Wenxuan Zhou, Jindong Gu, Volker Tresp*

Main category: cs.CL

TL;DR: FocalPO, a variant of DPO, down-weighs misranked preference pairs and focuses on enhancing correct rankings, outperforming DPO on benchmarks.


<details>
  <summary>Details</summary>
Motivation: DPO often fails to improve misranked pairs despite its gradient focus, prompting the need for a more effective approach.

Method: FocalPO introduces a modulating factor to dynamically scale DPO loss, prioritizing correctly ranked pairs.

Result: FocalPO outperforms DPO and variants on benchmarks like Alpaca Eval 2.0, with fixed hyperparameters.

Conclusion: FocalPO effectively improves model alignment by focusing on correct rankings, demonstrating superior performance over DPO.

Abstract: Efficient preference optimization algorithms such as Direct Preference
Optimization (DPO) have become a popular approach in aligning large language
models (LLMs) with human preferences. These algorithms implicitly treat the LLM
as a reward model, and focus on training it to correct misranked preference
pairs. However, recent work~\citep{chen2024preference} empirically finds that
DPO training \textit{rarely improves these misranked preference pairs}, despite
its gradient emphasizing on these cases. We introduce FocalPO, a DPO variant
that instead \textit{down-weighs} misranked preference pairs and prioritizes
enhancing the model's understanding of pairs that it can already rank
correctly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this
by adding a modulating factor to dynamically scale DPO loss. Our experiment
demonstrates that FocalPO surpasses DPO and its variants on popular benchmarks
like Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the
introduced hyperparameter fixed. Additionally, we empirically reveals how
FocalPO affects training on correct and incorrect sample groups, further
underscoring its effectiveness.

</details>


### [157] [Large Language Models to Diffusion Finetuning](https://arxiv.org/pdf/2501.15781)
*Edoardo Cetin, Tianyu Zhao, Yujin Tang*

Main category: cs.CL

TL;DR: A new finetuning method for pre-trained LMs using diffusion framework to scale test-time compute, improving accuracy and performance without altering original weights.


<details>
  <summary>Details</summary>
Motivation: To enhance pre-trained LMs by enabling scalable test-time compute and integrating guidance techniques for better task performance.

Method: Finetuning pre-trained LMs with diffusion steps, adaptive ODE solvers, and guidance techniques while preserving original weights.

Result: Monotonically increasing accuracy with more diffusion steps, improved downstream task performance, and expert question-answering capabilities.

Conclusion: The method is universally applicable, effective, and compatible with traditional finetuning, unifying autoregressive and diffusion strengths.

Abstract: We propose a new finetuning method to provide pre-trained large language
models (LMs) the ability to scale test-time compute through the diffusion
framework. By increasing the number of diffusion steps, we show our finetuned
models achieve monotonically increasing accuracy, directly translating to
improved performance across downstream tasks. Furthermore, our finetuned models
can expertly answer questions on specific topics by integrating powerful
guidance techniques, and autonomously determine the compute required for a
given problem by leveraging adaptive ODE solvers. Our method is universally
applicable to any foundation model pre-trained with a cross-entropy loss and
does not modify any of its original weights, fully preserving its strong
single-step generation capabilities. We show our method is more effective and
fully compatible with traditional finetuning approaches, introducing an
orthogonal new direction to unify the strengths of the autoregressive and
diffusion frameworks.

</details>


### [158] [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/pdf/2502.00334)
*Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang*

Main category: cs.CL

TL;DR: UGPhysics is a benchmark for evaluating LLMs on undergraduate-level physics reasoning, highlighting gaps in current models' capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack depth in assessing LLMs' physics reasoning, necessitating a comprehensive evaluation tool.

Method: UGPhysics includes 5,520 problems in English and Chinese, covering 13 subjects, with a MARJ pipeline for accurate assessment.

Result: Top LLM achieved 49.8% accuracy, showing the need for improved physics reasoning beyond math skills.

Conclusion: UGPhysics and MARJ aim to advance AI in physics reasoning, with data and code publicly available.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
solving complex reasoning tasks, particularly in mathematics. However, the
domain of physics reasoning presents unique challenges that have received
significantly less attention. Existing benchmarks often fall short in
evaluating LLMs' abilities on the breadth and depth of undergraduate-level
physics, underscoring the need for a comprehensive evaluation. To fill this
gap, we introduce UGPhysics, a large-scale and comprehensive benchmark
specifically designed to evaluate UnderGraduate-level Physics (UGPhysics)
reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics
problems in both English and Chinese, covering 13 subjects with seven different
answer types and four distinct physics reasoning skills, all rigorously
screened for data leakage. Additionally, we develop a Model-Assistant
Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer
correctness of physics problems, ensuring accurate evaluation. Our evaluation
of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by
OpenAI-o1-mini), emphasizes the necessity for models with stronger physics
reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,
will drive future advancements in AI for physics reasoning. Codes and data are
available at https://github.com/YangLabHKUST/UGPhysics .

</details>


### [159] [Inference-time sparse attention with asymmetric indexing](https://arxiv.org/pdf/2502.08246)
*Pierre-Emmanuel Mazaré, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Hervé Jégou, Matthijs Douze*

Main category: cs.CL

TL;DR: Saap (Self-Attention with Asymmetric Partitions) introduces an asymmetrical indexing technique to speed up self-attention in transformers by addressing key-query distribution mismatches and RoPE positional encoding issues.


<details>
  <summary>Details</summary>
Motivation: Standard partitioning methods like k-means perform poorly in self-attention due to differing key-query distributions and RoPE positional encoding interference.

Method: Saap uses distinct partitions for keys and queries, approximating self-attention with a data-adaptive sparsity pattern, requiring only offline training of a small query classifier.

Result: On a long-context Llama 3.1-8b model, Saap reduces memory look-up by 20x and saves 60% time compared to FlashAttention-v2.

Conclusion: Saap effectively improves self-attention efficiency in transformers for long sequences.

Abstract: Self-attention in transformer models is an incremental associative memory
that maps key vectors to value vectors. One way to speed up self-attention is
to employ GPU-compatible vector search algorithms based on standard
partitioning methods such as k-means. However, such partitioning methods yield
poor results in this context because (1) the keys and queries follow different
distributions, and (2) the RoPE positional encoding hinders the bucket
assignment.
  This paper introduces Saap (Self-Attention with Asymmetric Partitions), which
overcomes these problems. It is an asymmetrical indexing technique that employs
distinct partitions for keys and queries, thereby approximating self-attention
with a data-adaptive sparsity pattern. It works on pretrained language models
and only requires to train (offline) a small query classifier. On a long
context Llama 3.1-8b model, with sequences ranging from 100k to 500k tokens,
Saap typically reduces by a factor of 20 the fraction of memory that needs to
be looked-up, which translates to a time saving of 60\% when compared to
FlashAttention-v2.

</details>


### [160] [Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?](https://arxiv.org/pdf/2502.09416)
*Takumi Goto, Yusuke Sakai, Taro Watanabe*

Main category: cs.CL

TL;DR: Proposes an aggregation method for GEC metrics to align with human evaluation, improving results on the SEEDA benchmark.


<details>
  <summary>Details</summary>
Motivation: Current automatic GEC evaluations diverge from human methods, which aggregate sentence-level relative evaluations.

Method: Proposes an aggregation method for existing metrics (edit-based, n-gram, sentence-level) to match human evaluation.

Result: Improves most metrics on SEEDA; BERT-based metrics sometimes outperform GPT-4.

Conclusion: The proposed method bridges the gap between automatic and human evaluation in GEC.

Abstract: One of the goals of automatic evaluation metrics in grammatical error
correction (GEC) is to rank GEC systems such that it matches human preferences.
However, current automatic evaluations are based on procedures that diverge
from human evaluation. Specifically, human evaluation derives rankings by
aggregating sentence-level relative evaluation results, e.g., pairwise
comparisons, using a rating algorithm, whereas automatic evaluation averages
sentence-level absolute scores to obtain corpus-level scores, which are then
sorted to determine rankings. In this study, we propose an aggregation method
for existing automatic evaluation metrics which aligns with human evaluation
methods to bridge this gap. We conducted experiments using various metrics,
including edit-based metrics, n-gram based metrics, and sentence-level metrics,
and show that resolving the gap improves results for the most of metrics on the
SEEDA benchmark. We also found that even BERT-based metrics sometimes
outperform the metrics of GPT-4. The proposed ranking method is integrated
gec-metrics.

</details>


### [161] [Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues](https://arxiv.org/pdf/2502.10973)
*David Sasu, Zehui Wu, Ziwei Gong, Run Chen, Pengyuan Shi, Lin Ai, Julia Hirschberg, Natalie Schluter*

Main category: cs.CL

TL;DR: The paper introduces ACE, the first multimodal emotion dialogue dataset for the Akan language, addressing resource gaps in low-resource languages for emotion recognition.


<details>
  <summary>Details</summary>
Motivation: To address the lack of emotion recognition resources for low-resource languages, particularly African languages like Akan.

Method: Developed the ACE dataset with 385 emotion-labeled dialogues and 6,162 utterances across audio, visual, and textual modalities, including prosodic annotations.

Result: Demonstrated ACE's quality and utility through experiments with state-of-the-art emotion recognition methods, setting baselines for future research.

Conclusion: ACE aims to inspire more inclusive and diverse NLP resources, bridging gaps in emotion recognition for underrepresented languages.

Abstract: In this paper, we introduce the Akan Conversation Emotion (ACE) dataset, the
first multimodal emotion dialogue dataset for an African language, addressing
the significant lack of resources for low-resource languages in emotion
recognition research. ACE, developed for the Akan language, contains 385
emotion-labeled dialogues and 6,162 utterances across audio, visual, and
textual modalities, along with word-level prosodic prominence annotations. The
presence of prosodic labels in this dataset also makes it the first
prosodically annotated African language dataset. We demonstrate the quality and
utility of ACE through experiments using state-of-the-art emotion recognition
methods, establishing solid baselines for future research. We hope ACE inspires
further work on inclusive, linguistically and culturally diverse NLP resources.

</details>


### [162] [Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models](https://arxiv.org/pdf/2502.11075)
*Haoyang Li, Xuejia Chen, Zhanchao XU, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen*

Main category: cs.CL

TL;DR: NumericBench is a benchmark to evaluate LLMs' numerical reasoning, revealing their weaknesses despite strong linguistic performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with numerical tasks due to reliance on surface-level patterns, lacking understanding of numbers as continuous magnitudes.

Method: Proposes NumericBench, a benchmark with six numerical capabilities, tested on LLMs like GPT-4 and DeepSeek.

Result: LLMs show persistent weaknesses in numerical reasoning, emphasizing the need for improvement.

Conclusion: NumericBench highlights gaps in LLMs' numerical reasoning, urging development of numerically-aware models.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
natural language processing tasks, such as text generation and semantic
understanding. However, their performance on numerical reasoning tasks, such as
basic arithmetic, numerical retrieval, and magnitude comparison, remains
surprisingly poor. This gap arises from their reliance on surface-level
statistical patterns rather than understanding numbers as continuous
magnitudes. Existing benchmarks primarily focus on either linguistic competence
or structured mathematical problem-solving, neglecting fundamental numerical
reasoning required in real-world scenarios. To bridge this gap, we propose
NumericBench, a comprehensive benchmark to evaluate six fundamental numerical
capabilities: number recognition, arithmetic operations, contextual retrieval,
comparison, summary, and logical reasoning. NumericBench includes datasets
ranging from synthetic number lists to the crawled real-world data, addressing
challenges like long contexts, noise, and multi-step reasoning. Extensive
experiments on state-of-the-art LLMs, including GPT-4 and DeepSeek, reveal
persistent weaknesses in numerical reasoning, highlighting the urgent need to
improve numerically-aware language modeling. The benchmark is released in:
https://github.com/TreeAI-Lab/NumericBench.

</details>


### [163] [A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](https://arxiv.org/pdf/2502.12665)
*Junhui He, Junna Xing, Nan Wang, Rui Xu, Shangyu Wu, Peng Zhou, Qiang Liu, Chun Jason Xue, Qingan Li*

Main category: cs.CL

TL;DR: A$^2$ATS is a retrieval-based KV cache reduction method for LLMs, improving efficiency and accuracy by using vector quantization and a novel inference architecture.


<details>
  <summary>Details</summary>
Motivation: Long context LLMs face inefficiencies due to large KV cache memory and retrieval overhead, leading to accuracy degradation.

Method: A$^2$ATS uses vector quantization on key states, Windowed Rotary Position Embedding, and a heterogeneous inference architecture for KV cache offloading.

Result: A$^2$ATS reduces performance degradation and overhead, increasing serving throughput by up to 2.7x.

Conclusion: A$^2$ATS effectively addresses KV cache challenges, enhancing long context LLM serving efficiency.

Abstract: Long context large language models (LLMs) pose significant challenges for
efficient serving due to the large memory footprint and high access overhead of
KV cache. Retrieval-based KV cache reduction methods can mitigate these
challenges, typically by offloading the complete KV cache to CPU and retrieving
necessary tokens on demand during inference. However, these methods still
suffer from unsatisfactory accuracy degradation and extra retrieval overhead.
To address these limitations, this paper proposes A$^2$ATS, a novel
retrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate
approximation of attention scores by applying the vector quantization technique
to key states, thereby enabling efficient and precise retrieval of the top-K
tokens. First, we propose Windowed Rotary Position Embedding, which decouples
the positional dependency from query and key states after position embedding.
Then, we propose query-aware vector quantization that optimizes the objective
of attention score approximation directly. Finally, we design the heterogeneous
inference architecture for KV cache offloading, enabling long context serving
with larger batch sizes. Experimental results demonstrate that A$^2$ATS can
achieve a lower performance degradation with similar or lower overhead compared
to existing methods, thereby increasing long context serving throughput by up
to $2.7 \times$.

</details>


### [164] [Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison](https://arxiv.org/pdf/2502.12921)
*George-Kirollos Saad, Scott Sanner*

Main category: cs.CL

TL;DR: Q-STRUM Debate improves query-driven contrastive summarization by using debate-style prompting with LLMs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like STRUM-LLM fail to clarify why items match queries in query-driven recommendation.

Method: Q-STRUM Debate extends STRUM-LLM with debate-style prompting to generate focused, contrastive summaries.

Result: Experiments show Q-STRUM Debate outperforms existing methods on contrastive summarization criteria.

Conclusion: Q-STRUM Debate introduces a novel, effective debate prompting method for QCS.

Abstract: Query-driven recommendation with unknown items poses a challenge for users to
understand why certain items are appropriate for their needs. Query-driven
Contrastive Summarization (QCS) is a methodology designed to address this issue
by leveraging language-based item descriptions to clarify contrasts between
them. However, existing state-of-the-art contrastive summarization methods such
as STRUM-LLM fall short of this goal. To overcome these limitations, we
introduce Q-STRUM Debate, a novel extension of STRUM-LLM that employs
debate-style prompting to generate focused and contrastive summarizations of
item aspects relevant to a query. Leveraging modern large language models
(LLMs) as powerful tools for generating debates, Q-STRUM Debate provides
enhanced contrastive summaries. Experiments across three datasets demonstrate
that Q-STRUM Debate yields significant performance improvements over existing
methods on key contrastive summarization criteria, thus introducing a novel and
performant debate prompting methodology for QCS.

</details>


### [165] [A Similarity Paradigm Through Textual Regularization Without Forgetting](https://arxiv.org/pdf/2502.14376)
*Fangming Cui, Jan Fong, Rongfei Zeng, Xinmei Tian, Jun Yu*

Main category: cs.CL

TL;DR: SPTR is a novel prompt learning method that avoids overfitting by combining textual regularization and a similarity paradigm, outperforming existing methods on multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Prompt learning often overfits downstream tasks, forgetting generalized knowledge from hand-crafted prompts. SPTR aims to preserve this knowledge while improving generalization.

Method: SPTR uses optimal transport for textual regularization and a similarity paradigm for alignment scores to enhance robustness.

Result: SPTR outperforms existing methods on 11 datasets across four tasks, demonstrating superior generalization.

Conclusion: SPTR effectively balances task-specific adaptation and generalization, making it a robust prompt learning approach.

Abstract: Prompt learning has emerged as a promising method for adapting pre-trained
visual-language models (VLMs) to a range of downstream tasks. While optimizing
the context can be effective for improving performance on specific tasks, it
can often lead to poor generalization performance on unseen classes or datasets
sampled from different distributions. It may be attributed to the fact that
textual prompts tend to overfit downstream data distributions, leading to the
forgetting of generalized knowledge derived from hand-crafted prompts. In this
paper, we propose a novel method called Similarity Paradigm with Textual
Regularization (SPTR) for prompt learning without forgetting. SPTR is a
two-pronged design based on hand-crafted prompts that is an inseparable
framework. 1) To avoid forgetting general textual knowledge, we introduce the
optimal transport as a textual regularization to finely ensure approximation
with hand-crafted features and tuning textual features. 2) In order to
continuously unleash the general ability of multiple hand-crafted prompts, we
propose a similarity paradigm for natural alignment score and adversarial
alignment score to improve model robustness for generalization. Both modules
share a common objective in addressing generalization issues, aiming to
maximize the generalization capability derived from multiple hand-crafted
prompts. Four representative tasks (i.e., non-generalization few-shot learning,
base-to-novel generalization, cross-dataset generalization, domain
generalization) across 11 datasets demonstrate that SPTR outperforms existing
prompt learning methods.

</details>


### [166] [Social Genome: Grounded Social Reasoning Abilities of Multimodal Models](https://arxiv.org/pdf/2502.15109)
*Leena Mathur, Marian Qian, Paul Pu Liang, Louis-Philippe Morency*

Main category: cs.CL

TL;DR: SOCIAL GENOME is a benchmark for evaluating multimodal models' fine-grained social reasoning abilities using annotated video interactions and reasoning traces.


<details>
  <summary>Details</summary>
Motivation: To assess and improve AI systems' ability to interpret and respond to human social interactions by grounding reasoning in multimodal cues and external knowledge.

Method: Introduces SOCIAL GENOME, a dataset of 272 videos with 1,486 annotated reasoning traces (5,777 steps) referencing visual, verbal, vocal, and external knowledge cues.

Result: Identifies performance gaps in state-of-the-art models, highlighting the need for improved grounded social reasoning.

Conclusion: SOCIAL GENOME provides a comprehensive framework for evaluating and advancing multimodal models' social reasoning capabilities.

Abstract: Social reasoning abilities are crucial for AI systems to effectively
interpret and respond to multimodal human communication and interaction within
social contexts. We introduce SOCIAL GENOME, the first benchmark for
fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL
GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning
traces related to inferences about these interactions. These traces contain
5,777 reasoning steps that reference evidence from visual cues, verbal cues,
vocal cues, and external knowledge (contextual knowledge external to videos).
SOCIAL GENOME is also the first modeling challenge to study external knowledge
in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate
semantic and structural qualities of model-generated social reasoning traces.
We demonstrate the utility of SOCIAL GENOME through experiments with
state-of-the-art models, identifying performance gaps and opportunities for
future research to improve the grounded social reasoning abilities of
multimodal models.

</details>


### [167] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/pdf/2502.17110)
*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

Main category: cs.CL

TL;DR: Mobile-Agent-V uses video to automate mobile task management, improving performance by 36% over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise in mobile device usage demands better automation, but current AI frameworks lack operational expertise, and manual knowledge injection is inefficient.

Method: Mobile-Agent-V leverages video content to inject operational knowledge into mobile automation, eliminating manual effort.

Result: Mobile-Agent-V boosts performance by 36% compared to existing methods, as validated by the Mobile-Knowledge benchmark.

Conclusion: Mobile-Agent-V offers an effortless and efficient solution for mobile automation, outperforming traditional approaches.

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [168] [CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought](https://arxiv.org/pdf/2502.17214)
*Boxuan Zhang, Ruqi Zhang*

Main category: cs.CL

TL;DR: CoT-UQ is a response-wise uncertainty quantification framework for LLMs that leverages Chain-of-Thought reasoning to improve accuracy, outperforming existing methods by 5.9% AUROC.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with uncertainty quantification, leading to unreliable decision-making and misinformation detection. Existing methods are computationally expensive and often overconfident.

Method: CoT-UQ integrates LLMs' reasoning capabilities by extracting and assessing keywords from each reasoning step to estimate uncertainty.

Result: Experiments on Llama models (8B-13B) show CoT-UQ outperforms existing methods by 5.9% AUROC on logical and mathematical tasks.

Conclusion: CoT-UQ provides a more accurate and efficient uncertainty quantification method for LLMs, enhancing reliability in decision-making.

Abstract: Large language models (LLMs) excel in many tasks but struggle to accurately
quantify uncertainty in their generated responses. This limitation makes it
challenging to detect misinformation and ensure reliable decision-making.
Existing uncertainty quantification (UQ) methods for LLMs are primarily
prompt-wise rather than response-wise, often requiring multiple response
samples, which incurs high computational costs. Moreover, LLMs have been shown
to be overconfident, particularly when using reasoning steps to derive their
answers. In this work, we propose CoT-UQ, a response-wise UQ framework that
integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT)
into the UQ process. CoT-UQ captures critical information during inference by
extracting keywords from each reasoning step and assessing their importance to
the final answer. This key reasoning information is then aggregated to produce
a final uncertainty estimate. We conduct extensive experiments based on Llama
Family with model sizes varying from 8B to 13B across logical and mathematical
reasoning tasks. Experimental results demonstrate that CoT-UQ significantly
outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC
compared to current UQ methods. The code is available at:
https://github.com/ZBox1005/CoT-UQ.

</details>


### [169] [Towards Enhanced Immersion and Agency for LLM-based Interactive Drama](https://arxiv.org/pdf/2502.17878)
*Hongqiu Wu, Weiqi Wu, Tianyang Xu, Jiameng Zhang, Hai Zhao*

Main category: cs.CL

TL;DR: The paper introduces LLM-based Interactive Drama, focusing on enhancing Immersion and Agency through Playwriting-guided Generation and Plot-based Reflection, validated by human evaluation.


<details>
  <summary>Details</summary>
Motivation: To improve the interactive drama experience by addressing underexplored aspects of Immersion and Agency in LLM-based dialogue scenarios.

Method: Proposes Playwriting-guided Generation for better story structure and Plot-based Reflection for aligning LLM reactions with player intentions.

Result: Human evaluation shows improvements in Immersion and Agency.

Conclusion: The methods enhance interactive drama quality, making it more engaging and responsive to player influence.

Abstract: LLM-based Interactive Drama is a novel AI-based dialogue scenario, where the
user (i.e. the player) plays the role of a character in the story, has
conversations with characters played by LLM agents, and experiences an
unfolding story. This paper begins with understanding interactive drama from
two aspects: Immersion, the player's feeling of being present in the story, and
Agency, the player's ability to influence the story world. Both are crucial to
creating an enjoyable interactive experience, while they have been
underexplored in previous work. To enhance these two aspects, we first propose
Playwriting-guided Generation, a novel method that helps LLMs craft dramatic
stories with substantially improved structures and narrative quality.
Additionally, we introduce Plot-based Reflection for LLM agents to refine their
reactions to align with the player's intentions. Our evaluation relies on human
judgment to assess the gains of our methods in terms of immersion and agency.

</details>


### [170] [DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers](https://arxiv.org/pdf/2502.18460)
*Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen*

Main category: cs.CL

TL;DR: DRAMA is a training framework using LLMs to train smaller dense retrievers, balancing efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of large LLMs for retrieval tasks and the poor generalization of smaller retrievers with limited data.

Method: Uses pruned LLMs as backbone and trains on diverse LLM-augmented data in a single-stage contrastive learning setup.

Result: DRAMA outperforms traditional retrievers in multilingual and long-context tasks, showing strong performance across languages.

Conclusion: Connecting smaller retrievers with LLM advancements bridges efficiency and generalization gaps.

Abstract: Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.

</details>


### [171] [PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation](https://arxiv.org/pdf/2502.19756)
*Nathan Roll*

Main category: cs.CL

TL;DR: PolyPrompt enhances multilingual LLMs by learning language-specific trigger tokens, improving accuracy by 3.7%-19.9% on MMLU.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent multilingual performance of LLMs despite strong English benchmarks.

Method: Gradient-based search to learn language-specific trigger tokens, prepended to prompts during inference.

Result: Accuracy gains of 3.7%-19.9% on MMLU across 15 diverse languages.

Conclusion: PolyPrompt effectively boosts multilingual LLM performance with minimal parameter overhead.

Abstract: Large language models (LLMs) showcase increasingly impressive English
benchmark scores, however their performance profiles remain inconsistent across
multilingual settings. To address this gap, we introduce PolyPrompt, a novel,
parameter-efficient framework for enhancing the multilingual capabilities of
LLMs. Our method learns a set of trigger tokens for each language through a
gradient-based search, identifying the input query's language and selecting the
corresponding trigger tokens which are prepended to the prompt during
inference. We perform experiments on two ~1 billion parameter models, with
evaluations on the global MMLU benchmark across fifteen typologically and
resource diverse languages, demonstrating accuracy gains of 3.7%-19.9% compared
to naive and translation-pipeline baselines.

</details>


### [172] [Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking](https://arxiv.org/pdf/2502.20129)
*Yifan Zhang, Wenyu Du, Dongming Jin, Jie Fu, Zhi Jin*

Main category: cs.CL

TL;DR: The paper investigates how Chain-of-thought (CoT) enhances Transformer models, identifying key components (late-layer MLP neurons) that form an implicit finite state automaton (FSA) and demonstrating its robustness in challenging settings.


<details>
  <summary>Details</summary>
Motivation: To mechanistically understand the algorithms learned by Transformer+CoT and evaluate its state-tracking capabilities.

Method: Evaluated state tracking, identified key circuits (late-layer MLP neurons), proposed metrics (compression and distinction), and tested robustness in challenging settings.

Result: Confirmed CoT's effectiveness, identified FSA-like behavior in neurons, and demonstrated resilience in noisy and generalized settings.

Conclusion: Transformer+CoT learns robust algorithms (FSAs), with late-layer MLP neurons playing a critical role, showcasing its potential for complex tasks.

Abstract: Chain-of-thought (CoT) significantly enhances the performance of large
language models (LLMs) across a wide range of tasks, and prior research shows
that CoT can theoretically increase expressiveness. However, there is limited
mechanistic understanding of the algorithms that Transformer+CoT can learn. Our
key contributions are: (1) We evaluate the state tracking capabilities of
Transformer+CoT and its variants, confirming the effectiveness of CoT. (2)
Next, we identify the circuit (a subset of model components, responsible for
tracking the world state), indicating that late-layer MLP neurons play a key
role. We propose two metrics, compression and distinction, and show that the
neuron sets for each state achieve nearly 100% accuracy, providing evidence of
an implicit finite state automaton (FSA) embedded within the model. (3)
Additionally, we explore three challenging settings: skipping intermediate
steps, introducing data noises, and testing length generalization. Our results
demonstrate that Transformer+CoT learns robust algorithms (FSAs), highlighting
its resilience in challenging scenarios. Our code is available at
https://github.com/IvanChangPKU/FSA.

</details>


### [173] [Unnatural Languages Are Not Bugs but Features for LLMs](https://arxiv.org/pdf/2503.01926)
*Keyu Duan, Yiran Zhao, Zhili Feng, Jinjie Ni, Tianyu Pang, Qian Liu, Tianle Cai, Longxu Dou, Kenji Kawaguchi, Anirudh Goyal, J. Zico Kolter, Michael Qizhe Shieh*

Main category: cs.CL

TL;DR: LLMs can process unnatural languages (incomprehensible to humans but meaningful to models) effectively, achieving performance comparable to natural language training.


<details>
  <summary>Details</summary>
Motivation: Challenge the perception that non-human-readable text sequences are bugs for aligned LLMs, showing they contain usable latent features.

Method: Systematic investigation of unnatural languages, fine-tuning models on unnatural instruction datasets, and analyzing their processing mechanisms.

Result: Models fine-tuned on unnatural languages perform on-par with natural language training (49.71 win rate in Length-controlled AlpacaEval 2.0).

Conclusion: Unnatural languages contain generalizable latent features, and LLMs process them by filtering noise and inferring contextual meaning.

Abstract: Large Language Models (LLMs) have been observed to process non-human-readable
text sequences, such as jailbreak prompts, often viewed as a bug for aligned
LLMs. In this work, we present a systematic investigation challenging this
perception, demonstrating that unnatural languages - strings that appear
incomprehensible to humans but maintain semantic meanings for LLMs - contain
latent features usable by models. Notably, unnatural languages possess latent
features that can be generalized across different models and tasks during
inference. Furthermore, models fine-tuned on unnatural versions of instruction
datasets perform on-par with those trained on natural language, achieving 49.71
win rates in Length-controlled AlpacaEval 2.0 in average across various base
models. In addition, through comprehensive analysis, we demonstrate that LLMs
process unnatural languages by filtering noise and inferring contextual meaning
from filtered words.

</details>


### [174] [Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent](https://arxiv.org/pdf/2503.02519)
*Xingzuo Li, Kehai Chen, Yunfei Long, Xuefeng Bai, Yong Xu, Min Zhang*

Main category: cs.CL

TL;DR: GA-Rollback is a new framework for LLM agents that uses a generator-assistant duo to detect and correct errors in reasoning, improving decision-making and reducing error propagation.


<details>
  <summary>Details</summary>
Motivation: Current step-by-step reasoning in LLM agents suffers from irreversible error propagation due to unchecked intermediate thoughts.

Method: GA-Rollback employs a generator to interact with the environment and an assistant to review actions, triggering rollbacks for incorrect steps. Additional strategies enhance rollback effectiveness.

Result: GA-Rollback outperforms baselines on three benchmarks and integrates well as a plug-and-play module.

Conclusion: GA-Rollback effectively mitigates error propagation in LLM agents, enhancing their decision-making robustness.

Abstract: Large language model (LLM) agents typically adopt a step-by-step reasoning
framework, in which they interleave the processes of thinking and acting to
accomplish the given task. However, this paradigm faces a deep-rooted one-pass
issue whereby each generated intermediate thought is plugged into the
trajectory regardless of its correctness, which can cause irreversible error
propagation. To address the issue, this paper proposes a novel framework called
Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better
decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator
to interact with the environment and an assistant to examine each action
produced by the generator, where the assistant triggers a rollback operation
upon detection of incorrect actions. Moreover, we introduce two additional
strategies tailored for the rollback scenario to further improve its
effectiveness. Extensive experiments show that GA-Rollback achieves significant
improvements over several strong baselines on three widely used benchmarks. Our
analysis further reveals that GA-Rollback can function as a robust
plug-and-play module, integrating seamlessly with other methods.

</details>


### [175] [Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence](https://arxiv.org/pdf/2503.05037)
*Mohsen Fayyaz, Ali Modarressi, Hinrich Schuetze, Nanyun Peng*

Main category: cs.CL

TL;DR: The paper investigates heuristic biases in dense retrieval models, revealing vulnerabilities like favoring shorter documents and ignoring answer presence, leading to downstream performance drops in applications like RAG.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of heuristic biases in dense retrieval models, which are critical for downstream IR applications like RAG.

Method: Repurposes a relation extraction dataset (Re-DocRED) to design controlled experiments, evaluating retrievers like Dragon+ and Contriever.

Result: Retrievers favor shorter documents, early positions, repeated entities, and literal matches, ignoring answer presence. Combined biases cause catastrophic performance drops (answer-containing document selected <10% of cases). Downstream RAG performance drops 34%.

Conclusion: Dense retrieval models exhibit significant heuristic biases, severely impacting their robustness and downstream applications like RAG.

Abstract: Dense retrieval models are commonly used in Information Retrieval (IR)
applications, such as Retrieval-Augmented Generation (RAG). Since they often
serve as the first step in these systems, their robustness is critical to avoid
downstream failures. In this work, we repurpose a relation extraction dataset
(e.g., Re-DocRED) to design controlled experiments that quantify the impact of
heuristic biases, such as a preference for shorter documents, on retrievers
like Dragon+ and Contriever. We uncover major vulnerabilities, showing
retrievers favor shorter documents, early positions, repeated entities, and
literal matches, all while ignoring the answer's presence! Notably, when
multiple biases combine, models exhibit catastrophic performance degradation,
selecting the answer-containing document in less than 10% of cases over a
synthetic biased document without the answer. Furthermore, we show that these
biases have direct consequences for downstream applications like RAG, where
retrieval-preferred documents can mislead LLMs, resulting in a 34% performance
drop than providing no documents at all.
https://huggingface.co/datasets/mohsenfayyaz/ColDeR

</details>


### [176] [OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses](https://arxiv.org/pdf/2503.10927)
*Angela Lopez-Cardona, Sebastian Idesis, Miguel Barreda-Ángeles, Sergi Abadal, Ioannis Arapakis*

Main category: cs.CL

TL;DR: OASST-ETC introduces an eye-tracking corpus to study human reading patterns on LLM responses, revealing distinct patterns for preferred vs. non-preferred outputs and correlations with model attention.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human preferences is challenging; eye-tracking data provides real-time cognitive insights beyond explicit feedback.

Method: Collects eye-tracking data from 24 participants evaluating LLM responses from OASST1, compares reading patterns, and analyzes correlations with transformer-based models.

Result: Distinct reading patterns for preferred responses and stronger correlations with model attention in such cases.

Conclusion: OASST-ETC offers a resource for LLM alignment research and suggests integrating eye-tracking data into alignment methods.

Abstract: While Large Language Models (LLMs) have significantly advanced natural
language processing, aligning them with human preferences remains an open
challenge. Although current alignment methods rely primarily on explicit
feedback, eye-tracking (ET) data offers insights into real-time cognitive
processing during reading. In this paper, we present OASST-ETC, a novel
eye-tracking corpus capturing reading patterns from 24 participants, while
evaluating LLM-generated responses from the OASST1 dataset. Our analysis
reveals distinct reading patterns between preferred and non-preferred
responses, which we compare with synthetic eye-tracking data. Furthermore, we
examine the correlation between human reading measures and attention patterns
from various transformer-based models, discovering stronger correlations in
preferred responses. This work introduces a unique resource for studying human
cognitive processing in LLM evaluation and suggests promising directions for
incorporating eye-tracking data into alignment methods. The dataset and
analysis code are publicly available.

</details>


### [177] [The time scale of redundancy between prosody and linguistic context](https://arxiv.org/pdf/2503.11630)
*Tamar I. Regev, Chiebuka Ohams, Shaylee Xie, Lukas Wolf, Evelina Fedorenko, Alex Warstadt, Ethan G. Wilcox, Tiago Pimentel*

Main category: cs.CL

TL;DR: Prosody in speech carries information that is somewhat redundant with words but requires extended past context for prediction and helps predict upcoming words.


<details>
  <summary>Details</summary>
Motivation: To understand how prosodic features relate to word context and their role in communication.

Method: Systematically examining the time scale of prosody-word relationships using past and future contexts.

Result: Prosody requires 3-8 words of past context for prediction and shows short-scale redundancy with future words (1-2 words).

Conclusion: Prosody aids communication by integrating past context and predicting upcoming words, playing distinct roles in each.

Abstract: In spoken communication, information is transmitted not only via words, but
also through a rich array of non-verbal signals, including prosody--the
non-segmental auditory features of speech. Do these different communication
channels carry distinct information? Prior work has shown that the information
carried by prosodic features is substantially redundant with that carried by
the surrounding words. Here, we systematically examine the time scale of this
relationship, studying how it varies with the length of past and future
contexts. We find that a word's prosodic features require an extended past
context (3-8 words across different features) to be reliably predicted. Given
that long-scale contextual information decays in memory, prosody may facilitate
communication by adding information that is locally unique. We also find that a
word's prosodic features show some redundancy with future words, but only with
a short scale of 1-2 words, consistent with reports of incremental short-term
planning in language production. Thus, prosody may facilitate communication by
helping listeners predict upcoming material. In tandem, our results highlight
potentially distinct roles that prosody plays in facilitating integration of
words into past contexts and in helping predict upcoming words.

</details>


### [178] [Splintering Nonconcatenative Languages for Better Tokenization](https://arxiv.org/pdf/2503.14433)
*Bar Gazit, Shaltiel Shmidman, Avi Shmidman, Yuval Pinter*

Main category: cs.CL

TL;DR: SPLINTER is a pre-processing step for tokenization that handles nonconcatenative morphologies in languages like Hebrew, Arabic, and Malay, improving tokenizer performance.


<details>
  <summary>Details</summary>
Motivation: Existing tokenization methods (e.g., BPE, UnigramLM) fail for languages with nonconcatenative morphologies, such as Hebrew and Arabic.

Method: SPLINTER rearranges text into a linear form to represent nonconcatenative morphologies, enabling better tokenization.

Result: SPLINTER improves token vocabularies in Hebrew, Arabic, and Malay, and enhances downstream task performance in Hebrew BERT models.

Conclusion: SPLINTER effectively addresses the limitations of current tokenization methods for nonconcatenative languages.

Abstract: Common subword tokenization algorithms like BPE and UnigramLM assume that
text can be split into meaningful units by concatenative measures alone. This
is not true for languages such as Hebrew and Arabic, where morphology is
encoded in root-template patterns, or Malay and Georgian, where split affixes
are common. We present SPLINTER, a pre-processing step which rearranges text
into a linear form that better represents such nonconcatenative morphologies,
enabling meaningful contiguous segments to be found by the tokenizer. We
demonstrate SPLINTER's merit using both intrinsic measures evaluating token
vocabularies in Hebrew, Arabic, and Malay; as well as on downstream tasks using
BERT-architecture models trained for Hebrew.

</details>


### [179] [Meta-Learning Neural Mechanisms rather than Bayesian Priors](https://arxiv.org/pdf/2503.16048)
*Michael Goodale, Salvador Mascarenhas, Yair Lakretz*

Main category: cs.CL

TL;DR: Meta-learning in neural networks doesn't teach simplicity-based priors but embeds cognitive primitives like counters, with efficiency gains from focused meta-training on single formal languages.


<details>
  <summary>Details</summary>
Motivation: To understand what meta-learning actually imparts to models, especially in the context of formal languages and human-like learning biases.

Method: Investigating meta-learning of formal languages, analyzing whether models learn simplicity-based priors or neural mechanisms like counters.

Result: Meta-trained models don't learn simplicity priors but acquire neural mechanisms (e.g., counters). Meta-training on one formal language can be as effective as on 5000, if it teaches useful mechanisms.

Conclusion: Findings offer practical efficiency in meta-learning and bridge symbolic theories with neural mechanisms.

Abstract: Children acquire language despite being exposed to several orders of
magnitude less data than large language models require. Meta-learning has been
proposed as a way to integrate human-like learning biases into neural-network
architectures, combining both the structured generalizations of symbolic models
with the scalability of neural-network models. But what does meta-learning
exactly imbue the model with? We investigate the meta-learning of formal
languages and find that, contrary to previous claims, meta-trained models are
not learning simplicity-based priors when meta-trained on datasets organised
around simplicity. Rather, we find evidence that meta-training imprints neural
mechanisms (such as counters) into the model, which function like cognitive
primitives for the network on downstream tasks. Most surprisingly, we find that
meta-training on a single formal language can provide as much improvement to a
model as meta-training on 5000 different formal languages, provided that the
formal language incentivizes the learning of useful neural mechanisms. Taken
together, our findings provide practical implications for efficient
meta-learning paradigms and new theoretical insights into linking symbolic
theories and neural mechanisms.

</details>


### [180] [Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility](https://arxiv.org/pdf/2503.17579)
*Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt*

Main category: cs.CL

TL;DR: LLMs show human-like production-interpretation asymmetries, influenced by model size and prompts.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs process language like humans, focusing on production-interpretation distinctions.

Method: Tested LLMs using pronoun asymmetry in implicit causality verbs, varying model size and prompts.

Result: Some LLMs replicate human-like asymmetries, more likely in larger models and with specific prompts.

Conclusion: LLMs can mimic human language processing patterns, dependent on model size and prompt design.

Abstract: Whether large language models (LLMs) process language similarly to humans has
been the subject of much theoretical and practical debate. We examine this
question through the lens of the production-interpretation distinction found in
human sentence processing and evaluate the extent to which instruction-tuned
LLMs replicate this distinction. Using an empirically documented asymmetry
between pronoun production and interpretation in humans for implicit causality
verbs as a testbed, we find that some LLMs do quantitatively and qualitatively
reflect human-like asymmetries between production and interpretation. We
demonstrate that whether this behavior holds depends upon both model size-with
larger models more likely to reflect human-like patterns and the choice of
meta-linguistic prompts used to elicit the behavior. Our codes and results are
available at
https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025.

</details>


### [181] [Negation: A Pink Elephant in the Large Language Models' Room?](https://arxiv.org/pdf/2503.22395)
*Tereza Vrabcová, Marek Kadlčík, Petr Sojka, Michal Štefánik, Michal Spiegel*

Main category: cs.CL

TL;DR: The paper introduces two new datasets (NoFEVER-ML and NoSNLI-ML) to study LLMs' challenges with negation, showing model size, language type, and premise explicitness impact performance.


<details>
  <summary>Details</summary>
Motivation: Negations are crucial for logical reasoning but pose challenges for LLMs, which remain underexplored.

Method: Constructed multilingual entailment datasets (NoFEVER-ML and NoSNLI-ML) to analyze negation handling in LLMs.

Result: Larger models improve negation handling; performance is language-dependent, with projective languages like English outperforming non-projective ones.

Conclusion: The datasets enable further research on negation, LLM hallucinations, and multilingual reasoning improvements.

Abstract: Negations are key to determining sentence meaning, making them essential for
logical reasoning. Despite their importance, negations pose a substantial
challenge for large language models (LLMs) and remain underexplored.
  We constructed and published two new textual entailment datasets NoFEVER-ML
and NoSNLI-ML in four languages (English, Czech, German, and Ukrainian) with
  examples differing in negation. It allows investigation of the root causes of
the negation problem and its exemplification: how popular LLM model properties
and language impact their inability to handle negation correctly.
  Contrary to previous work, we show that increasing the model size may improve
the models' ability to handle negations. Furthermore, we find that both the
models' reasoning accuracy and robustness to negation are language-dependent
and that the length and explicitness of the premise have an impact on
robustness. There is better accuracy in projective language with fixed order,
such as English, than in non-projective ones, such as German or Czech.
  Our entailment datasets pave the way to further research for explanation and
exemplification of the negation problem, minimization of LLM hallucinations,
and improvement of LLM reasoning in multilingual settings.

</details>


### [182] [Efficient Annotator Reliability Assessment with EffiARA](https://arxiv.org/pdf/2504.00589)
*Owen Cook, Jake Vasilakes, Ian Roberts, Xingyi Song*

Main category: cs.CL

TL;DR: EffiARA is a framework supporting the entire document-level annotation pipeline, improving efficiency and reliability in machine learning data annotation.


<details>
  <summary>Details</summary>
Motivation: Data annotation is costly and lacks standardization, especially for document-level tasks. EffiARA aims to address this gap.

Method: EffiARA provides tools for task setup, dataset compilation, and annotator reliability analysis, supported by Python and a webtool.

Result: Previous studies show improved classification and annotator agreement using EffiARA's reliability-based methods.

Conclusion: EffiARA is open-source and accessible, offering a standardized solution for document-level annotation.

Abstract: Data annotation is an essential component of the machine learning pipeline;
it is also a costly and time-consuming process. With the introduction of
transformer-based models, annotation at the document level is increasingly
popular; however, there is no standard framework for structuring such tasks.
The EffiARA annotation framework is, to our knowledge, the first project to
support the whole annotation pipeline, from understanding the resources
required for an annotation task to compiling the annotated dataset and gaining
insights into the reliability of individual annotators as well as the dataset
as a whole. The framework's efficacy is supported by two previous studies: one
improving classification performance through annotator-reliability-based
soft-label aggregation and sample weighting, and the other increasing the
overall agreement among annotators through removing identifying and replacing
an unreliable annotator. This work introduces the EffiARA Python package and
its accompanying webtool, which provides an accessible graphical user interface
for the system. We open-source the EffiARA Python package at
https://github.com/MiniEggz/EffiARA and the webtool is publicly accessible at
https://effiara.gate.ac.uk.

</details>


### [183] [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/pdf/2504.05050)
*Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau*

Main category: cs.CL

TL;DR: Aligned LLMs retain harmful knowledge despite alignment efforts, with vulnerabilities exposed via adversarial prompts under distributional shifts.


<details>
  <summary>Details</summary>
Motivation: To uncover the persistent ethical vulnerabilities in aligned LLMs, showing that current alignment methods fail to fully disconnect harmful pretrained knowledge.

Method: Theoretical analysis of alignment limitations and empirical validation using adversarial prompts under distributional shifts.

Result: 100% attack success rate on 19 out of 23 state-of-the-art aligned LLMs, revealing universal vulnerabilities.

Conclusion: Current alignment methods are insufficient; harmful knowledge persists and can be exploited, necessitating better safeguards.

Abstract: Large language models (LLMs) are foundational explorations to artificial
general intelligence, yet their alignment with human values via instruction
tuning and preference learning achieves only superficial compliance. Here, we
demonstrate that harmful knowledge embedded during pretraining persists as
indelible "dark patterns" in LLMs' parametric memory, evading alignment
safeguards and resurfacing under adversarial inducement at distributional
shifts. In this study, we first theoretically analyze the intrinsic ethical
vulnerability of aligned LLMs by proving that current alignment methods yield
only local "safety regions" in the knowledge manifold. In contrast, pretrained
knowledge remains globally connected to harmful concepts via high-likelihood
adversarial trajectories. Building on this theoretical insight, we empirically
validate our findings by employing semantic coherence inducement under
distributional shifts--a method that systematically bypasses alignment
constraints through optimized adversarial prompts. This combined theoretical
and empirical approach achieves a 100% attack success rate across 19 out of 23
state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing
their universal vulnerabilities.

</details>


### [184] [A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models](https://arxiv.org/pdf/2504.08961)
*Kseniia Petukhova, Ekaterina Kochmar*

Main category: cs.CL

TL;DR: An automated pipeline using LLMs constructs tree annotation schemes and performs discourse annotation, outperforming manual methods and matching human annotators in quality while saving time.


<details>
  <summary>Details</summary>
Motivation: Manual creation of tree annotation schemes is time-consuming and requires expertise, prompting the need for automation.

Method: The approach uses LLMs to build annotation schemes and perform annotation, evaluated on SFs and SWBD-DAMSL taxonomies with frequency-guided decision trees.

Result: The automated pipeline outperforms manual designs and matches human annotators, reducing annotation time significantly.

Conclusion: The released code and resources aim to advance discourse annotation research, demonstrating the effectiveness of LLMs in automating the process.

Abstract: Recent advances in Large Language Models (LLMs) have shown promise in
automating discourse annotation for conversations. While manually designing
tree annotation schemes significantly improves annotation quality for humans
and models, their creation remains time-consuming and requires expert
knowledge. We propose a fully automated pipeline that uses LLMs to construct
such schemes and perform annotation. We evaluate our approach on speech
functions (SFs) and the Switchboard-DAMSL (SWBD-DAMSL) taxonomies. Our
experiments compare various design choices, and we show that frequency-guided
decision trees, paired with an advanced LLM for annotation, can outperform
previously manually designed trees and even match or surpass human annotators
while significantly reducing the time required for annotation. We release all
code and resultant schemes and annotations to facilitate future research on
discourse annotation.

</details>


### [185] [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/pdf/2504.11042)
*Sukannya Purkayastha, Zhuang Li, Anne Lauscher, Lizhen Qu, Iryna Gurevych*

Main category: cs.CL

TL;DR: LazyReview dataset addresses lazy thinking in peer reviews, showing LLMs improve with fine-tuning, and feedback enhances review quality.


<details>
  <summary>Details</summary>
Motivation: To tackle the issue of lazy thinking in peer reviews and improve review quality by providing a dataset and tools for detection.

Method: Introduces LazyReview, a dataset annotated with lazy thinking categories, and tests LLMs in zero-shot and fine-tuned settings.

Result: LLMs perform poorly in zero-shot but improve by 10-20 points with fine-tuning; feedback makes reviews more actionable.

Conclusion: High-quality training data and feedback are crucial for improving peer-review quality and training junior reviewers.

Abstract: Peer review is a cornerstone of quality control in scientific publishing.
With the increasing workload, the unintended use of `quick' heuristics,
referred to as lazy thinking, has emerged as a recurring issue compromising
review quality. Automated methods to detect such heuristics can help improve
the peer-reviewing process. However, there is limited NLP research on this
issue, and no real-world dataset exists to support the development of detection
tools. This work introduces LazyReview, a dataset of peer-review sentences
annotated with fine-grained lazy thinking categories. Our analysis reveals that
Large Language Models (LLMs) struggle to detect these instances in a zero-shot
setting. However, instruction-based fine-tuning on our dataset significantly
boosts performance by 10-20 performance points, highlighting the importance of
high-quality training data. Furthermore, a controlled experiment demonstrates
that reviews revised with lazy thinking feedback are more comprehensive and
actionable than those written without such feedback. We will release our
dataset and the enhanced guidelines that can be used to train junior reviewers
in the community. (Code available here:
https://github.com/UKPLab/acl2025-lazy-review)

</details>


### [186] [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/pdf/2504.12216)
*Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover*

Main category: cs.CL

TL;DR: The paper introduces d1, a framework to enhance reasoning in diffusion-based large language models (dLLMs) using supervised finetuning and reinforcement learning, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Despite the success of autoregressive LLMs in reasoning, it's unclear if diffusion-based LLMs (dLLMs) can similarly benefit. The study aims to bridge this gap.

Method: The d1 framework combines masked supervised finetuning (SFT) and a novel critic-free RL algorithm (diffu-GRPO) to adapt pretrained dLLMs for reasoning tasks.

Result: Empirical studies show d1 significantly improves reasoning performance in dLLMs on mathematical and planning benchmarks.

Conclusion: The d1 framework successfully adapts dLLMs for reasoning, demonstrating competitive performance and opening new avenues for diffusion-based models.

Abstract: Recent large language models (LLMs) have demonstrated strong reasoning
capabilities that benefits from online reinforcement learning (RL). These
capabilities have primarily been demonstrated within the left-to-right
autoregressive (AR) generation paradigm. In contrast, non-autoregressive
paradigms based on diffusion generate text in a coarse-to-fine manner. Although
recent diffusion-based large language models (dLLMs) have achieved competitive
language modeling performance compared to their AR counterparts, it remains
unclear if dLLMs can also leverage recent advances in LLM reasoning. To this
end, we propose d1, a framework to adapt pre-trained masked dLLMs into
reasoning models via a combination of supervised finetuning (SFT) and RL.
Specifically, we develop and extend techniques to improve reasoning in
pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge
and instill self-improvement behavior directly from existing datasets, and (b)
we introduce a novel critic-free, policy-gradient based RL algorithm called
diffu-GRPO, the first integration of policy gradient methods to masked dLLMs.
Through empirical studies, we investigate the performance of different
post-training recipes on multiple mathematical and planning benchmarks. We find
that d1 yields the best performance and significantly improves performance of a
state-of-the-art dLLM. Our code is released at
https://dllm-reasoning.github.io/.

</details>


### [187] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/pdf/2505.02862)
*Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang*

Main category: cs.CL

TL;DR: The paper introduces ICRT, a novel jailbreak attack framework for LLMs, leveraging human cognitive biases to bypass safety mechanisms and generate harmful content, with a new ranking-based evaluation metric.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attack methods are either brute-force or manually designed, lacking real-world risk insights. The paper aims to address this gap by proposing a more effective framework.

Method: ICRT uses cognitive decomposition (simplicity effect) and relevance bias to simplify and reorganize malicious prompts, enhancing their effectiveness. A ranking-based metric (Elo, HodgeRank, Rank Centrality) evaluates harmfulness.

Result: ICRT consistently bypasses LLM safety mechanisms, generating high-risk content, and outperforms traditional binary evaluation methods.

Conclusion: The framework highlights jailbreak risks and aids in developing stronger defenses, advancing understanding of LLM vulnerabilities.

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [188] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/pdf/2505.03320)
*Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: Mamba's long-context performance is improved using Recall with Reasoning (RwR), a method that distills CoT summarization from a teacher model, enhancing recall and reasoning without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Mamba's theoretical infinite-context potential is limited in practice for sequences exceeding training lengths.

Method: RwR prepends CoT summarization prompts during fine-tuning to teach Mamba to recall and reason over long contexts.

Result: RwR boosts Mamba's performance on long-context benchmarks (LONGMEMEVAL, HELMET) against baselines, preserving short-context capabilities.

Conclusion: RwR effectively unlocks Mamba's long-context memory ability without modifying its architecture.

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [189] [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/pdf/2505.06150)
*Ryan Lagasse, Aidan Kierans, Avijit Ghosh, Shiri Dori-Hacohen*

Main category: cs.CL

TL;DR: A scaling law for fine-tuning LLMs under fixed compute budgets, emphasizing data composition (dataset volume) over just token count, improves token efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional scaling laws overlook data composition (number of examples and average token length), which significantly impacts model performance.

Method: Proposes a scaling law accounting for dataset volume, validated on BRICC and MMLU datasets with subsampling strategies.

Result: Data composition affects token efficiency, motivating refined scaling laws for resource-constrained LLM fine-tuning.

Conclusion: Refined scaling laws considering data composition are crucial for efficient LLM fine-tuning under compute constraints.

Abstract: We introduce a scaling law for fine-tuning large language models (LLMs) under
fixed compute budgets that explicitly accounts for data composition.
Conventional approaches measure training data solely by total tokens, yet the
number of examples and their average token length -- what we term \emph{dataset
volume} -- play a decisive role in model performance. Our formulation is tuned
following established procedures. Experiments on the BRICC dataset
\cite{salavati2024reducing} and subsets of the MMLU dataset
\cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple
subsampling strategies, reveal that data composition significantly affects
token efficiency. These results motivate refined scaling laws for practical LLM
fine-tuning in resource-constrained settings.

</details>


### [190] [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/pdf/2505.09825)
*Peiqi Sui, Juan Diego Rodriguez, Philippe Laban, Dean Murphy, Joseph P. Dexter, Richard Jean So, Samuel Baker, Pramit Chaudhuri*

Main category: cs.CL

TL;DR: KRISTEVA is the first benchmark for evaluating LLMs on close reading tasks, showing they lag behind humans in interpretive reasoning.


<details>
  <summary>Details</summary>
Motivation: Close reading is foundational in education but untested on LLMs. KRISTEVA fills this gap by assessing LLMs' ability to analyze literature.

Method: KRISTEVA includes 1331 multiple-choice questions across three tasks: feature extraction, contextual retrieval, and multi-hop reasoning.

Result: LLMs show some competency (49.7%-69.7% accuracy) but underperform humans in 10 of 11 tasks.

Conclusion: LLMs have potential in close reading but need improvement to match human evaluators.

Abstract: Each year, tens of millions of essays are written and graded in college-level
English courses. Students are asked to analyze literary and cultural texts
through a process known as close reading, in which they gather textual details
to formulate evidence-based arguments. Despite being viewed as a basis for
critical thinking and widely adopted as a required element of university
coursework, close reading has never been evaluated on large language models
(LLMs), and multi-discipline benchmarks like MMLU do not include literature as
a subject. To fill this gap, we present KRISTEVA, the first close reading
benchmark for evaluating interpretive reasoning, consisting of 1331
multiple-choice questions adapted from classroom data. With KRISTEVA, we
propose three progressively more difficult sets of tasks to approximate
different elements of the close reading process, which we use to test how well
LLMs may seem to understand and reason about literary works: 1) extracting
stylistic features, 2) retrieving relevant contextual information from
parametric knowledge, and 3) multi-hop reasoning between style and external
contexts. Our baseline results find that, while state-of-the-art LLMs possess
some college-level close reading competency (accuracy 49.7% - 69.7%), their
performances still trail those of experienced human evaluators on 10 out of our
11 tasks.

</details>


### [191] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/pdf/2505.13508)
*Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You*

Main category: cs.CL

TL;DR: Time-R1 is a framework enhancing a 3B-parameter LLM with temporal abilities (understanding, prediction, creative generation) via a three-stage RL curriculum, outperforming larger models.


<details>
  <summary>Details</summary>
Motivation: LLMs lack robust temporal intelligence and struggle with integrating past reasoning and future predictions. Existing methods target isolated skills and generalize poorly.

Method: A three-stage RL curriculum with a dynamic rule-based reward system: (1) foundational temporal understanding, (2) future event prediction, (3) creative scenario generation.

Result: Time-R1 outperforms models 200x larger (e.g., 671B DeepSeek-R1) on future event prediction and creative generation benchmarks.

Conclusion: Progressive RL fine-tuning enables smaller models to achieve superior temporal performance, offering a scalable path for time-aware AI. Time-Bench dataset and checkpoints are released for research.

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities but lack
robust temporal intelligence, struggling to integrate reasoning about the past
with predictions and plausible generations of the future. Meanwhile, existing
methods typically target isolated temporal skills, such as question answering
about past events or basic forecasting, and exhibit poor generalization,
particularly when dealing with events beyond their knowledge cutoff or
requiring creative foresight. To address these limitations, we introduce
\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter)
LLM with comprehensive temporal abilities: understanding, prediction, and
creative generation. Our approach features a novel three-stage development
path; the first two constitute a \textit{reinforcement learning (RL)
curriculum} driven by a meticulously designed dynamic rule-based reward system.
This framework progressively builds (1) foundational temporal understanding and
logical event-time mappings from historical data, (2) future event prediction
skills for events beyond its knowledge cutoff, and finally (3) enables
remarkable generalization to creative future scenario generation without any
fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms
models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1,
on highly challenging future event prediction and creative scenario generation
benchmarks. This work provides strong evidence that thoughtfully engineered,
progressive RL fine-tuning allows smaller, efficient models to achieve superior
temporal performance, offering a practical and scalable path towards truly
time-aware AI. To foster further research, we also release \textit{Time-Bench},
a large-scale multi-task temporal reasoning dataset derived from 10 years of
news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [192] [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/pdf/2505.15299)
*Maodong Li, Longyin Zhang, Fang Kong*

Main category: cs.CL

TL;DR: The paper introduces a Dual-Perspective Keyword-Guided (DPKG) framework for multi-hop question generation, distinguishing between question and document keywords to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully leverage keyword guidance and overlook the distinct roles of question-specific and document-specific keywords in multi-hop question generation.

Method: The DPKG framework uses an expanded transformer encoder and two answer-aware decoders for keyword and question generation, integrating dual-perspective keywords (question and document keywords).

Result: Experiments show the framework's effectiveness, demonstrating superior performance in multi-hop question generation.

Conclusion: The DPKG framework successfully addresses the challenge of pinpointing essential information snippets, proving valuable for multi-hop question generation tasks.

Abstract: Multi-hop question generation (MQG) aims to generate questions that require
synthesizing multiple information snippets from documents to derive target
answers. The primary challenge lies in effectively pinpointing crucial
information snippets related to question-answer (QA) pairs, typically relying
on keywords. However, existing works fail to fully utilize the guiding
potential of keywords and neglect to differentiate the distinct roles of
question-specific and document-specific keywords. To address this, we define
dual-perspective keywords (i.e., question and document keywords) and propose a
Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates
keywords into the multi-hop question generation process. We argue that question
keywords capture the questioner's intent, whereas document keywords reflect the
content related to the QA pair. Functionally, question and document keywords
work together to pinpoint essential information snippets in the document, with
question keywords required to appear in the generated question. The DPKG
framework consists of an expanded transformer encoder and two answer-aware
transformer decoders for keyword and question generation, respectively.
Extensive experiments demonstrate the effectiveness of our work, showcasing its
promising performance and underscoring its significant value in the MQG task.

</details>


### [193] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/pdf/2505.16014)
*Yash Saxena, Ankur Padia, Mandar S Chaudhary, Kalpa Gunaratna, Srinivasan Parthasarathy, Manas Gaur*

Main category: cs.CL

TL;DR: METEORA replaces heuristic-based re-ranking in RAG with a rationale-driven selection method, improving accuracy and robustness while reducing chunk usage.


<details>
  <summary>Details</summary>
Motivation: Address the lack of explainability, interpretability, and robustness in traditional RAG pipelines.

Method: Two-stage approach: preference-tuned LLM generates rationales for evidence chunk selection, followed by a three-stage selection process and verification.

Result: 33.34% accuracy improvement, 50% fewer chunks used, and strong resilience to adversarial attacks (F1 score from 0.10 to 0.44).

Conclusion: METEORA offers a more explainable, robust, and efficient alternative to traditional RAG pipelines.

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [194] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/pdf/2505.16552)
*Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Ruihua Song*

Main category: cs.CL

TL;DR: CoLaR introduces a framework to compress reasoning processes in latent space, improving efficiency and performance over traditional CoT methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Chain-of-Thought (CoT) reasoning in LLMs is computationally expensive and inefficient at the token level.

Method: CoLaR uses a two-stage approach: supervised fine-tuning with compressed embedding prediction and reinforcement learning to explore diverse reasoning paths.

Result: CoLaR achieves 14.1% higher accuracy than baselines, reduces reasoning chain length by 53.3%, and improves performance by 5.4% in challenging tasks.

Conclusion: CoLaR offers a more efficient and dynamic alternative to CoT, with significant performance gains and reduced computational overhead.

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [195] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/pdf/2505.16660)
*Liu Chang, Wang Dongbo, Liu liu, Zhao Zhixiao*

Main category: cs.CL

TL;DR: The study introduces Guji_MATH, a benchmark for evaluating reasoning models on Chinese ancient mathematical texts, revealing their limitations and suggesting improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in processing Chinese ancient mathematical classics and evaluate reasoning models' capabilities under classical Chinese constraints.

Method: Constructed a benchmark (Guji_MATH) with 538 problems from 8 texts, using machine-assisted annotation and manual verification. Evaluated six models via closed-book and open-book modes.

Result: Models partially comprehend and solve problems but underperform compared to modern tasks. Classical Chinese comprehension and cultural knowledge need enhancement.

Conclusion: The study aids in mining mathematical knowledge from ancient texts and evaluating cross-linguistic model capabilities, suggesting prioritization of classical Chinese comprehension for optimization.

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [196] [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/pdf/2505.17134)
*Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu*

Main category: cs.CL

TL;DR: LongMagpie is a self-synthesis framework for generating high-quality long-context instruction data automatically, outperforming human annotation and template-based methods.


<details>
  <summary>Details</summary>
Motivation: High-quality long-context instruction data is scarce and costly to produce, limiting the alignment of long-context LLMs.

Method: LongMagpie leverages aligned long-context LLMs to auto-generate contextually relevant queries from documents, creating instruction data without human effort.

Result: LongMagpie achieves top performance on long-context benchmarks (HELMET, RULER, Longbench v2) while staying competitive on short-context tasks.

Conclusion: LongMagpie offers a scalable, diverse, and open solution for long-context instruction data synthesis.

Abstract: High-quality long-context instruction data is essential for aligning
long-context large language models (LLMs). Despite the public release of models
like Qwen and Llama, their long-context instruction data remains proprietary.
Human annotation is costly and challenging, while template-based synthesis
methods limit scale, diversity, and quality. We introduce LongMagpie, a
self-synthesis framework that automatically generates large-scale long-context
instruction data. Our key insight is that aligned long-context LLMs, when
presented with a document followed by special tokens preceding a user turn,
auto-regressively generate contextually relevant queries. By harvesting these
document-query pairs and the model's responses, LongMagpie produces
high-quality instructions without human effort. Experiments on HELMET, RULER,
and Longbench v2 demonstrate that LongMagpie achieves leading performance on
long-context tasks while maintaining competitive performance on short-context
tasks, establishing it as a simple and effective approach for open, diverse,
and scalable long-context instruction data synthesis.

</details>


### [197] [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/pdf/2505.20015)
*Ramon Ferrer-i-Cancho*

Main category: cs.CL

TL;DR: The paper introduces a new class of coding systems exhibiting Zipf's law, linking it to optimal coding and compression.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between Zipf's law and optimal coding, identifying human languages and certain animal communication systems as members of this class.

Method: Analyzing frequency-rank distributions in double logarithmic scale to identify linear displacement from optimal coding.

Result: Human languages and some animal systems (e.g., dolphins, whales) fit the class, while others with exponential distributions do not.

Conclusion: Zipf's law likely stems from compression, with straight lines in log-log plots indicating near-optimal coding.

Abstract: Here we present a new class of optimality for coding systems. Members of that
class are displaced linearly from optimal coding and thus exhibit Zipf's law,
namely a power-law distribution of frequency ranks. Within that class, Zipf's
law, the size-rank law and the size-probability law form a group-like
structure. We identify human languages that are members of the class. All
languages showing sufficient agreement with Zipf's law are potential members of
the class. In contrast, there are communication systems in other species that
cannot be members of that class for exhibiting an exponential distribution
instead but dolphins and humpback whales might. We provide a new insight into
plots of frequency versus rank in double logarithmic scale. For any system, a
straight line in that scale indicates that the lengths of optimal codes under
non-singular coding and under uniquely decodable encoding are displaced by a
linear function whose slope is the exponent of Zipf's law. For systems under
compression and constrained to be uniquely decodable, such a straight line may
indicate that the system is coding close to optimality. Our findings provide
support for the hypothesis that Zipf's law originates from compression.

</details>


### [198] [One-shot Entropy Minimization](https://arxiv.org/pdf/2505.20282)
*Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai*

Main category: cs.CL

TL;DR: Entropy minimization with a single unlabeled data and 10 optimization steps matches or exceeds performance of rule-based RL with thousands of data.


<details>
  <summary>Details</summary>
Motivation: To challenge the efficiency of post-training paradigms for large language models by simplifying the process.

Method: Trained 13,440 models using entropy minimization with minimal data and optimization steps.

Result: Achieved comparable or better performance than rule-based RL with significantly less data and effort.

Conclusion: Suggests a potential shift in post-training approaches for large language models.

Abstract: We trained 13,440 large language models and found that entropy minimization
requires only a single unlabeled data and 10 steps optimization to achieve
performance improvements comparable to or even greater than those obtained
using thousands of data and carefully designed rewards in rule-based
reinforcement learning. This striking result may prompt a rethinking of
post-training paradigms for large language models. Our code is avaliable at
https://github.com/zitian-gao/one-shot-em.

</details>


### [199] [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/pdf/2505.20322)
*Mengru Wang, Ziwen Xu, Shengyu Mao, Shumin Deng, Zhaopeng Tu, Huajun Chen, Ningyu Zhang*

Main category: cs.CL

TL;DR: The paper introduces Steering Target Atoms (STA), a method to isolate and manipulate disentangled knowledge components in language models for enhanced safety and control.


<details>
  <summary>Details</summary>
Motivation: Precise control over language model generation is crucial for safety and reliability, but current methods like prompt engineering and steering face limitations due to intertwined internal representations.

Method: The proposed method, Steering Target Atoms (STA), isolates and manipulates disentangled knowledge components, addressing the challenge of locating atomic knowledge in high-dimensional spaces.

Result: Experiments show STA's effectiveness in enhancing safety, with superior robustness and flexibility in adversarial scenarios. It also works well for precise reasoning control in large models.

Conclusion: STA offers a promising approach for precise and robust control of language model behaviors, particularly in safety-critical applications.

Abstract: Precise control over language model generation is vital for ensuring both
safety and reliability. Although prompt engineering and steering are commonly
used to intervene in model behaviors, the vast number of parameters in models
often results in highly intertwined internal representations. This
interdependency can limit control precision and sometimes lead to unintended
side effects. Recent research has explored the use of sparse autoencoders (SAE)
to disentangle knowledge in high-dimensional spaces for steering. However,
these applications have been limited to toy tasks owing to the nontrivial issue
of locating atomic knowledge components. In this paper, we propose Steering
Target Atoms (STA), a novel method that isolates and manipulates disentangled
knowledge components to enhance safety. Comprehensive experiments demonstrate
the effectiveness of our approach. Further analysis reveals that steering
exhibits superior robustness and flexibility, particularly in adversarial
scenarios. We also apply the steering strategy to the large reasoning model,
confirming its effectiveness in precise reasoning control.

</details>


### [200] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/pdf/2505.22019)
*Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao*

Main category: cs.CL

TL;DR: VRAG-RL is a novel RL framework for visually rich RAG tasks, addressing limitations of traditional text-based and current vision-based methods by optimizing VLMs with tailored actions and rewards.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based RAG methods fail with visual information, and current vision-based approaches lack effective reasoning due to insufficient model activation. RL is introduced to enhance reasoning in visually rich contexts.

Method: VRAG-RL uses RL to optimize VLMs, enabling interaction with search engines and autonomous sampling of reasoning trajectories. It includes actions like cropping and scaling for coarse-to-fine information gathering and integrates query rewriting and retrieval performance in rewards.

Result: The framework addresses key limitations: insufficient reasoning token allocation in multi-modal RAG and poor retrieval due to unclear queries. It aligns VLMs with real-world applications.

Conclusion: VRAG-RL effectively enhances reasoning and retrieval in visually rich RAG tasks by leveraging RL strategies and tailored actions, bridging gaps in current approaches.

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
https://github.com/Alibaba-NLP/VRAG.

</details>


### [201] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/pdf/2505.22116)
*Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie*

Main category: cs.CL

TL;DR: IOHFuseLM is a multimodal language model for predicting intraoperative hypotension (IOH) using a two-stage training strategy and token-level alignment of clinical and physiological data.


<details>
  <summary>Details</summary>
Motivation: IOH is linked to adverse outcomes, but prediction is challenging due to sparse events and diverse patient data.

Method: Uses domain adaptive pretraining on augmented physiological time series, followed by task fine-tuning. Aligns clinical descriptions with time series and converts static attributes to text.

Result: Outperforms baselines in identifying IOH events on two datasets.

Conclusion: IOHFuseLM is effective for clinical decision support, with code available for reproducibility.

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [202] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/pdf/2505.22830)
*Alexander Gill, Abhilasha Ravichander, Ana Marasović*

Main category: cs.CL

TL;DR: LLMs can generate valid reasoning benchmarks at lower cost but are less challenging than human-authored ones, raising concerns about their use for evaluation.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can create high-quality evaluation benchmarks for reasoning tasks, comparing them to human-crowdsourced datasets.

Method: Case studies using LLM-generated versions of CondaQA (negation reasoning) and DROP (quantity reasoning) datasets, evaluating validity and difficulty.

Result: LLM-generated benchmarks are valid but less challenging for LLMs than human-authored ones.

Conclusion: LLMs' use for benchmark creation needs critical reassessment due to reduced challenge and potential quality loss.

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [203] [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/pdf/2505.22848)
*Pingjun Hong, Beiduo Chen, Siyao Peng, Marie-Catherine de Marneffe, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces LITEX, a taxonomy for categorizing free-text explanations in NLI to address within-label variation, showing its effectiveness in explanation generation.


<details>
  <summary>Details</summary>
Motivation: To understand and address within-label variation in NLI, where annotators agree on labels but provide divergent reasoning.

Method: Develop LITEX, a linguistically-informed taxonomy, annotate e-SNLI dataset, validate reliability, and assess alignment with labels, highlights, and explanations.

Result: LITEX captures within-label variation and improves explanation generation, aligning better with human reasoning than label/highlight-based methods.

Conclusion: LITEX bridges the gap between human and model explanations, offering a systematic approach to understanding and generating NLI reasoning.

Abstract: There is increasing evidence of Human Label Variation (HLV) in Natural
Language Inference (NLI), where annotators assign different labels to the same
premise-hypothesis pair. However, within-label variation--cases where
annotators agree on the same label but provide divergent reasoning--poses an
additional and mostly overlooked challenge. Several NLI datasets contain
highlighted words in the NLI item as explanations, but the same spans on the
NLI item can be highlighted for different reasons, as evidenced by free-text
explanations, which offer a window into annotators' reasoning. To
systematically understand this problem and gain insight into the rationales
behind NLI labels, we introduce LITEX, a linguistically-informed taxonomy for
categorizing free-text explanations. Using this taxonomy, we annotate a subset
of the e-SNLI dataset, validate the taxonomy's reliability, and analyze how it
aligns with NLI labels, highlights, and explanations. We further assess the
taxonomy's usefulness in explanation generation, demonstrating that
conditioning generation on LITEX yields explanations that are linguistically
closer to human explanations than those generated using only labels or
highlights. Our approach thus not only captures within-label variation but also
shows how taxonomy-guided generation for reasoning can bridge the gap between
human and model explanations more effectively than existing strategies.

</details>


### [204] [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/pdf/2505.23001)
*Yize Cheng, Wenxiao Wang, Mazda Moayeri, Soheil Feizi*

Main category: cs.CL

TL;DR: DyePack is a framework using backdoor attacks to detect if models trained on benchmark test sets, ensuring low false positive rates and strong evidence for contamination.


<details>
  <summary>Details</summary>
Motivation: Open benchmarks are vulnerable to test set contamination; DyePack addresses this by identifying models that misuse benchmarks.

Method: DyePack uses backdoor samples in test data to flag contaminated models, with stochastic targets for exact FPR computation.

Result: Detects all contaminated models with very low FPRs (e.g., 0.000073% on MMLU-Pro) across multiple-choice and open-ended tasks.

Conclusion: DyePack effectively identifies benchmark misuse with provable guarantees, enhancing benchmark integrity.

Abstract: Open benchmarks are essential for evaluating and advancing large language
models, offering reproducibility and transparency. However, their accessibility
makes them likely targets of test set contamination. In this work, we introduce
DyePack, a framework that leverages backdoor attacks to identify models that
used benchmark test sets during training, without requiring access to the loss,
logits, or any internal details of the model. Like how banks mix dye packs with
their money to mark robbers, DyePack mixes backdoor samples with the test data
to flag models that trained on it. We propose a principled design incorporating
multiple backdoors with stochastic targets, enabling exact false positive rate
(FPR) computation when flagging every model. This provably prevents false
accusations while providing strong evidence for every detected case of
contamination. We evaluate DyePack on five models across three datasets,
covering both multiple-choice and open-ended generation tasks. For
multiple-choice questions, it successfully detects all contaminated models with
guaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard
using eight backdoors. For open-ended generation tasks, it generalizes well and
identifies all contaminated models on Alpaca with a guaranteed false positive
rate of just 0.127% using six backdoors.

</details>


### [205] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/pdf/2505.23754)
*Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

Main category: cs.CL

TL;DR: DeepTheorem is a framework for informal theorem proving using natural language to enhance LLM reasoning, featuring a large dataset, reinforcement learning, and evaluation metrics, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional ATP methods misalign with LLMs' natural language strengths, prompting the need for an informal theorem-proving framework.

Method: DeepTheorem includes a 121K theorem dataset, RL-Zero reinforcement learning, and evaluation metrics for proof correctness and reasoning quality.

Result: DeepTheorem outperforms existing methods, achieving top accuracy and reasoning quality in LLM theorem proving.

Conclusion: DeepTheorem advances informal theorem proving and mathematical exploration, demonstrating significant improvements over traditional approaches.

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [206] [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/pdf/2505.23114)
*Seohyeong Lee, Eunwon Kim, Hwaran Lee, Buru Chang*

Main category: cs.CL

TL;DR: Alignment Data Map, a GPT-4o-assisted tool, improves efficiency in collecting human preference data for LLM alignment by identifying high-quality samples without explicit annotations.


<details>
  <summary>Details</summary>
Motivation: Human preference data is costly and inefficient to collect, hindering scalability in aligning LLMs with human values.

Method: Uses GPT-4o to compute alignment scores for LLM responses, constructs a map based on mean and variance, and identifies high-quality samples.

Result: Using 33% of high-quality data achieves comparable or better performance than the full dataset.

Conclusion: The Alignment Data Map enhances data efficiency and diagnoses dataset quality, reducing reliance on costly annotations.

Abstract: Human preference data plays a critical role in aligning large language models
(LLMs) with human values. However, collecting such data is often expensive and
inefficient, posing a significant scalability challenge. To address this, we
introduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and
diagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we
compute alignment scores for LLM-generated responses to instructions from
existing preference datasets. These scores are then used to construct an
Alignment Data Map based on their mean and variance. Our experiments show that
using only 33 percent of the data, specifically samples in the high-mean,
low-variance region, achieves performance comparable to or better than using
the entire dataset. This finding suggests that the Alignment Data Map can
significantly improve data collection efficiency by identifying high-quality
samples for LLM alignment without requiring explicit annotations. Moreover, the
Alignment Data Map can diagnose existing preference datasets. Our analysis
shows that it effectively detects low-impact or potentially misannotated
samples. Source code is available online.

</details>


### [207] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/pdf/2505.23807)
*Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang*

Main category: cs.CL

TL;DR: DLP is a dynamic layerwise pruning method for LLMs that adaptively assigns pruning rates based on layer importance, outperforming uniform and pre-defined non-uniform methods at high sparsity.


<details>
  <summary>Details</summary>
Motivation: Uniform pruning degrades performance at high sparsity, and pre-defined non-uniform methods are suboptimal. DLP aims to dynamically optimize pruning.

Method: DLP integrates model weights and input activation to determine layer importance and assign adaptive pruning rates.

Result: At 70% sparsity, DLP reduces perplexity by 7.79 and improves accuracy by 2.7% for LLaMA2-7B, outperforming state-of-the-art methods.

Conclusion: DLP is effective, compatible with other compression techniques, and integrates well with PEFT. Code is released for future research.

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [208] [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/pdf/2505.23368)
*Beiduo Chen, Yang Janet Liu, Anna Korhonen, Barbara Plank*

Main category: cs.CL

TL;DR: The paper introduces a novel LLM-based pipeline using linguistically-grounded discourse segmenters to extract supporting and opposing statements from CoTs, improving accuracy in human label variation analysis. It also proposes a rank-based evaluation framework, outperforming baselines and aligning better with human rankings.


<details>
  <summary>Details</summary>
Motivation: To address human label variation by leveraging reasoning-tuned LLMs and CoTs, which provide forward reasoning paths, unlike prior reverse paradigms.

Method: A novel LLM-based pipeline with linguistically-grounded discourse segmenters to extract statements from CoTs, and a rank-based evaluation framework for HLV.

Result: Outperforms direct generation methods and baselines on three datasets, showing better alignment with human rankings.

Conclusion: The proposed approach effectively leverages CoTs and a rank-based framework to improve accuracy and alignment with human label variation.

Abstract: The recent rise of reasoning-tuned Large Language Models (LLMs)--which
generate chains of thought (CoTs) before giving the final answer--has attracted
significant attention and offers new opportunities for gaining insights into
human label variation, which refers to plausible differences in how multiple
annotators label the same data instance. Prior work has shown that
LLM-generated explanations can help align model predictions with human label
distributions, but typically adopt a reverse paradigm: producing explanations
based on given answers. In contrast, CoTs provide a forward reasoning path that
may implicitly embed rationales for each answer option, before generating the
answers. We thus propose a novel LLM-based pipeline enriched with
linguistically-grounded discourse segmenters to extract supporting and opposing
statements for each answer option from CoTs with improved accuracy. We also
propose a rank-based HLV evaluation framework that prioritizes the ranking of
answers over exact scores, which instead favor direct comparison of label
distributions. Our method outperforms a direct generation method as well as
baselines on three datasets, and shows better alignment of ranking methods with
humans, highlighting the effectiveness of our approach.

</details>


### [209] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/pdf/2505.23809)
*Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao*

Main category: cs.CL

TL;DR: A framework using LLMs for marketing copy generation boosts CTR by 12.5% and CVR by 8.3% while maintaining novelty.


<details>
  <summary>Details</summary>
Motivation: To balance creative content and conversion effectiveness in competitive e-commerce.

Method: Integrates prompt engineering, multi-objective fine-tuning (sentiment, diversity, CTA), and post-processing.

Result: 12.5% CTR and 8.3% CVR increase, with maintained content novelty.

Conclusion: Offers a practical solution for automated copy generation and hints at future multimodal, real-time personalization.

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [210] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/pdf/2505.24133)
*Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu*

Main category: cs.CL

TL;DR: R-KV is a redundancy-aware KV cache compression method for reasoning models, achieving near-full performance with only 10% of the cache, outperforming baselines and saving memory.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods fail with reasoning models, leading to performance drops and inefficiencies.

Method: R-KV targets redundant tokens in reasoning models, compressing the KV cache while preserving performance.

Result: R-KV achieves 100% performance with 10% cache, 105% with 16%, and saves 90% memory with 6.6X throughput.

Conclusion: R-KV is a superior KV cache compression method for reasoning models, outperforming baselines in efficiency and performance.

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [211] [Localizing Persona Representations in LLMs](https://arxiv.org/pdf/2505.24539)
*Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman*

Main category: cs.CL

TL;DR: The study explores how personas (human traits, values, beliefs) are encoded in LLMs, identifying layers with the most divergence and analyzing activations to reveal shared and distinct embedding spaces.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent human personas and traits, aiding future efforts to modulate such traits in model outputs.

Method: Uses dimension reduction and pattern recognition to identify divergent layers and analyzes activations to compare persona embeddings.

Result: Personas show large differences in the final third of decoder layers, with overlapping activations for ethical perspectives but distinct regions for political ideologies.

Conclusion: The findings enhance understanding of LLM representations and can guide refinement of trait modulation in outputs.

Abstract: We present a study on how and where personas -- defined by distinct sets of
human characteristics, values, and beliefs -- are encoded in the representation
space of large language models (LLMs). Using a range of dimension reduction and
pattern recognition methods, we first identify the model layers that show the
greatest divergence in encoding these representations. We then analyze the
activations within a selected layer to examine how specific personas are
encoded relative to others, including their shared and distinct embedding
spaces. We find that, across multiple pre-trained decoder-only LLMs, the
analyzed personas show large differences in representation space only within
the final third of the decoder layers. We observe overlapping activations for
specific ethical perspectives -- such as moral nihilism and utilitarianism --
suggesting a degree of polysemy. In contrast, political ideologies like
conservatism and liberalism appear to be represented in more distinct regions.
These findings help to improve our understanding of how LLMs internally
represent information and can inform future efforts in refining the modulation
of specific human traits in LLM outputs. Warning: This paper includes
potentially offensive sample statements.

</details>


### [212] [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/pdf/2506.00022)
*Shenghe Zheng, Qianjia Cheng, Junchi Yao, Mengsong Wu, Haonan He, Ning Ding, Yu Cheng, Shuyue Hu, Lei Bai, Dongzhan Zhou, Ganqu Cui, Peng Ye*

Main category: cs.CL

TL;DR: The paper introduces PHYSICS, a dataset of 16,568 physics problems to improve LLMs' reasoning in physics, addressing gaps in current research and evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Physics is reasoning-intensive but understudied in LLMs. The paper aims to fill this gap by providing a high-quality dataset and tailored evaluation methods.

Method: Curated 16,568 physics problems from textbooks, covering five domains and difficulty levels. Introduced a Rule+Model evaluation framework for physics.

Result: Current LLMs show limitations in physics tasks. The dataset and evaluation framework highlight these gaps and aim to improve model performance.

Conclusion: PHYSICS dataset and evaluation methodology aim to advance LLMs' capabilities in physics reasoning.

Abstract: Large Language Models (LLMs) have achieved remarkable progress on advanced
reasoning tasks such as mathematics and coding competitions. Meanwhile,
physics, despite being both reasoning-intensive and essential to real-world
understanding, received limited academic and industrial attention. This paper
introduces PHYSICS, a dataset containing 16,568 high-quality physics problems
spanning subjects and difficulty levels, to facilitate this issue.
Specifically, PHYSICS is curated with exercises from over 100 textbooks through
a carefully designed pipeline for quality control. It covers five major physics
domains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern
Physics. It also spans a wide range of difficulty levels, from high school to
graduate-level physics courses. To utilize the data for improving and
evaluating the model's physical reasoning capabilities, we split the dataset
into training and test sets, and provide reasoning paths generated by powerful
reasoning models for the training data to facilitate model training. In
addition, for the evaluation part, we find that existing evaluation frameworks
exhibit biases in aspects such as units, simplification, and precision in
physics domain. To balance efficiency and accuracy, we introduce a Rule+Model
evaluation framework tailored to physics problems. Our evaluations on current
state-of-the-art open-source and proprietary models highlight the limitations
of current models in handling physics-related tasks. We hope that our dataset
and evaluation methodology will jointly advance the development of LLMs in the
field of physics.

</details>


### [213] [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/pdf/2506.00077)
*Edward L. Wang, Tianyu Wang, Avanti Athreya, Vince Lyzinski, Carey E. Priebe*

Main category: cs.CL

TL;DR: The paper proposes interacting Gaussian mixture models (GMMs) as a simpler alternative to LLMs for studying human behavior, comparing their dynamics and highlighting key similarities and differences.


<details>
  <summary>Details</summary>
Motivation: LLMs are computationally expensive, motivating the need for simpler models like GMMs to study human behavior in social sciences.

Method: The authors compare interacting GMMs to LLMs in experimental simulations, focusing on dynamics and feedback mechanisms.

Result: Interacting GMMs capture key features of LLM dynamics, with notable similarities and differences identified.

Conclusion: GMMs offer benefits over LLMs, with potential for modifications and future research to enhance their utility.

Abstract: Large language models (LLMs) are a powerful tool with the ability to match
human capabilities and behavior in many settings. Retrieval-augmented
generation (RAG) further allows LLMs to generate diverse output depending on
the contents of their RAG database. This motivates their use in the social
sciences to study human behavior between individuals when large-scale
experiments are infeasible. However, LLMs depend on complex, computationally
expensive algorithms. In this paper, we introduce interacting Gaussian mixture
models (GMMs) as an alternative to similar frameworks using LLMs. We compare a
simplified model of GMMs to select experimental simulations of LLMs whose
updating and response depend on feedback from other LLMs. We find that
interacting GMMs capture important features of the dynamics in interacting
LLMs, and we investigate key similarities and differences between interacting
LLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture
models, potential modifications, and future research directions.

</details>


### [214] [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/pdf/2506.00288)
*Ahmed Elhady, Eneko Agirre, Mikel Artetxe*

Main category: cs.CL

TL;DR: Including English data in continued pretraining (CPT) for language adaptation doesn't affect validation perplexity but is crucial for downstream task performance in the target language. Omitting English leads to catastrophic forgetting and poor generalization, mitigated by curriculum learning and EMA of weights.


<details>
  <summary>Details</summary>
Motivation: To understand the role of English data in CPT for adapting LLMs to new languages and its impact on downstream capabilities.

Method: Introduces a language-agnostic benchmark for in-context learning (ICL), analyzes the effects of including/excluding English, and proposes curriculum learning and EMA of weights as solutions.

Result: English inclusion is vital for downstream task performance despite not affecting perplexity. Omitting it causes catastrophic forgetting and parameter shifts, harming generalization.

Conclusion: The study clarifies how emergent abilities arise in CPT and suggests better methods (curriculum learning, EMA) for language adaptation.

Abstract: Continued pretraining (CPT) is a popular approach to adapt existing large
language models (LLMs) to new languages. When doing so, it is common practice
to include a portion of English data in the mixture, but its role has not been
carefully studied to date. In this work, we show that including English does
not impact validation perplexity, yet it is critical for the emergence of
downstream capabilities in the target language. We introduce a
language-agnostic benchmark for in-context learning (ICL), which reveals
catastrophic forgetting early on CPT when English is not included. This in turn
damages the ability of the model to generalize to downstream prompts in the
target language as measured by perplexity, even if it does not manifest in
terms of accuracy until later in training, and can be tied to a big shift in
the model parameters. Based on these insights, we introduce curriculum learning
and exponential moving average (EMA) of weights as effective alternatives to
mitigate the need for English. All in all, our work sheds light into the
dynamics by which emergent abilities arise when doing CPT for language
adaptation, and can serve as a foundation to design more effective methods in
the future.

</details>


### [215] [PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain](https://arxiv.org/pdf/2506.00250)
*Mohammad Javad Ranjbar Kalahroodi, Amirhossein Sheikholselami, Sepehr Karimi, Sepideh Ranjbar Kalahroodi, Heshaam Faili, Azadeh Shakery*

Main category: cs.CL

TL;DR: The paper introduces PersianMedQA, a dataset to evaluate LLMs in Persian and English for medical reasoning, showing GPT-4.1's superiority and the limitations of fine-tuned models.


<details>
  <summary>Details</summary>
Motivation: To assess LLM reliability in high-stakes, low-resource medical domains, particularly Persian, which is underexplored.

Method: Benchmarked 40+ LLMs (general, Persian fine-tuned, medical) on PersianMedQA in zero-shot and CoT settings, analyzing translation impact.

Result: GPT-4.1 outperformed others (83.3% Persian, 80.7% English), while fine-tuned models like Dorna struggled (35.9%). Translation showed mixed accuracy.

Conclusion: Model size alone isn't enough; domain/language adaptation is crucial. PersianMedQA aids multilingual, culturally grounded medical LLM evaluation.

Abstract: Large Language Models (LLMs) have achieved remarkable performance on a wide
range of NLP benchmarks, often surpassing human-level accuracy. However, their
reliability in high-stakes domains such as medicine, particularly in
low-resource languages, remains underexplored. In this work, we introduce
PersianMedQA, a large-scale, expert-validated dataset of multiple-choice
Persian medical questions, designed to evaluate LLMs across both Persian and
English. We benchmark over 40 state-of-the-art models, including
general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and
chain-of-thought (CoT) settings. Our results show that closed-source general
models (e.g., GPT-4.1) consistently outperform all other categories, achieving
83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models
such as Dorna underperform significantly (e.g., 35.9% in Persian), often
struggling with both instruction-following and domain reasoning. We also
analyze the impact of translation, showing that while English performance is
generally higher, Persian responses are sometimes more accurate due to cultural
and clinical contextual cues. Finally, we demonstrate that model size alone is
insufficient for robust performance without strong domain or language
adaptation. PersianMedQA provides a foundation for evaluating multilingual and
culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be
accessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA

</details>


### [216] [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/pdf/2506.00519)
*Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma*

Main category: cs.CL

TL;DR: CausalAbstain improves LLMs' abstention decisions in multilingual settings by selecting useful feedback causally, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address knowledge disparities and reduce hallucinations in LLMs by improving abstention strategies.

Method: Introduces CausalAbstain, a causal method to select useful feedback for abstention decisions.

Result: Outperforms baselines in native and multilingual settings, enhancing interpretability.

Conclusion: CausalAbstain effectively improves LLMs' abstention decisions with open-sourced code and data.

Abstract: Large Language Models (LLMs) often exhibit knowledge disparities across
languages. Encouraging LLMs to \textit{abstain} when faced with knowledge gaps
is a promising strategy to reduce hallucinations in multilingual settings.
Current abstention strategies for multilingual scenarios primarily rely on
generating feedback in various languages using LLMs and performing
self-reflection. However, these methods can be adversely impacted by
inaccuracies and biases in the generated feedback. To address this, from a
causal perspective, we introduce \textit{CausalAbstain}, a method that helps
LLMs determine whether to utilize multiple generated feedback responses and how
to identify the most useful ones. Extensive experiments demonstrate that
\textit{CausalAbstain} effectively selects helpful feedback and enhances
abstention decisions with interpretability in both native language
(\textsc{Casual-native}) and multilingual (\textsc{Causal-multi}) settings,
outperforming strong baselines on two benchmark datasets covering encyclopedic
and commonsense knowledge QA tasks. Our code and data are open-sourced at
https://github.com/peachch/CausalAbstain.

</details>


### [217] [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/pdf/2506.00612)
*Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li*

Main category: cs.CL

TL;DR: A knowledge-guided data augmentation framework (KGGDG) is introduced to enhance clinical MCQ datasets by generating plausible but misleading distractors, reducing LLM accuracy for robust evaluation.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the reliability of LLMs in clinical decision-making by increasing dataset difficulty with deceptive distractors.

Method: Multi-step, semantically informed walks on a medical knowledge graph to generate clinically plausible but incorrect distractors.

Result: Applied to six medical QA benchmarks, KGGDG consistently reduces the accuracy of state-of-the-art LLMs.

Conclusion: KGGDG is a powerful tool for robust and diagnostic evaluations of medical LLMs.

Abstract: Clinical tasks such as diagnosis and treatment require strong decision-making
abilities, highlighting the importance of rigorous evaluation benchmarks to
assess the reliability of large language models (LLMs). In this work, we
introduce a knowledge-guided data augmentation framework that enhances the
difficulty of clinical multiple-choice question (MCQ) datasets by generating
distractors (i.e., incorrect choices that are similar to the correct one and
may confuse existing LLMs). Using our KG-based pipeline, the generated choices
are both clinically plausible and deliberately misleading. Our approach
involves multi-step, semantically informed walks on a medical knowledge graph
to identify distractor paths-associations that are medically relevant but
factually incorrect-which then guide the LLM in crafting more deceptive
distractors. We apply the designed knowledge graph guided distractor generation
(KGGDG) pipline, to six widely used medical QA benchmarks and show that it
consistently reduces the accuracy of state-of-the-art LLMs. These findings
establish KGGDG as a powerful tool for enabling more robust and diagnostic
evaluations of medical LLMs.

</details>


### [218] [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/pdf/2506.00622)
*Haesung Pyun, Yoonah Park, Yohan Jo*

Main category: cs.CL

TL;DR: CombiSearch improves dialogue state tracking (DST) by optimizing retriever training with combinatorial scoring, outperforming state-of-the-art models and increasing data efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing retriever training methods for DST ignore synergistic effects, linguistic characteristics, and direct DST performance optimization, leading to suboptimal example retrieval.

Method: CombiSearch scores in-context examples based on their combinatorial impact on DST performance, addressing the limitations of current methods.

Result: CombiSearch achieves a 20x gain in data efficiency, generalizes well to SGD dataset, and improves DST performance by 12% over traditional approaches.

Conclusion: CombiSearch demonstrates superior retriever training, highlighting the suboptimal nature of existing methods and expanding practical DST performance potential.

Abstract: In dialogue state tracking (DST), in-context learning comprises a retriever
that selects labeled dialogues as in-context examples and a DST model that uses
these examples to infer the dialogue state of the query dialogue. Existing
methods for constructing training data for retrievers suffer from three key
limitations: (1) the synergistic effect of examples is not considered, (2) the
linguistic characteristics of the query are not sufficiently factored in, and
(3) scoring is not directly optimized for DST performance. Consequently, the
retriever can fail to retrieve examples that would substantially improve DST
performance. To address these issues, we present CombiSearch, a method that
scores effective in-context examples based on their combinatorial impact on DST
performance. Our evaluation on MultiWOZ shows that retrievers trained with
CombiSearch surpass state-of-the-art models, achieving a 20x gain in data
efficiency and generalizing well to the SGD dataset. Moreover, CombiSearch
attains a 12% absolute improvement in the upper bound DST performance over
traditional approaches when no retrieval errors are assumed. This significantly
increases the headroom for practical DST performance while demonstrating that
existing methods rely on suboptimal data for retriever training.

</details>


### [219] [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/pdf/2506.00694)
*Li Zhang, Morgan Gray, Jaromir Savelka, Kevin D. Ashley*

Main category: cs.CL

TL;DR: The paper introduces an automated pipeline to evaluate LLMs in generating 3-ply legal arguments, focusing on faithfulness, factor utilization, and abstention. Findings show high hallucination avoidance but poor factor use and abstention.


<details>
  <summary>Details</summary>
Motivation: To address reliability concerns of LLMs in legal tasks by assessing their performance in generating faithful, factor-aware, and abstaining legal arguments.

Method: An automated pipeline using an external LLM to compare generated arguments against ground-truth factors, tested on three difficulty levels.

Result: LLMs avoid hallucination well (90%+ accuracy) but underutilize factors and fail to abstain when no factual basis exists.

Conclusion: The pipeline highlights the need for improved factor utilization and abstention in LLMs for reliable legal deployment.

Abstract: Large Language Models (LLMs) demonstrate potential in complex legal tasks
like argument generation, yet their reliability remains a concern. Building
upon pilot work assessing LLM generation of 3-ply legal arguments using human
evaluation, this paper introduces an automated pipeline to evaluate LLM
performance on this task, specifically focusing on faithfulness (absence of
hallucination), factor utilization, and appropriate abstention. We define
hallucination as the generation of factors not present in the input case
materials and abstention as the model's ability to refrain from generating
arguments when instructed and no factual basis exists. Our automated method
employs an external LLM to extract factors from generated arguments and
compares them against the ground-truth factors provided in the input case
triples (current case and two precedent cases). We evaluated eight distinct
LLMs on three tests of increasing difficulty: 1) generating a standard 3-ply
argument, 2) generating an argument with swapped precedent roles, and 3)
recognizing the impossibility of argument generation due to lack of shared
factors and abstaining. Our findings indicate that while current LLMs achieve
high accuracy (over 90%) in avoiding hallucination on viable argument
generation tests (Tests 1 & 2), they often fail to utilize the full set of
relevant factors present in the cases. Critically, on the abstention test (Test
3), most models failed to follow instructions to stop, instead generating
spurious arguments despite the lack of common factors. This automated pipeline
provides a scalable method for assessing these crucial LLM behaviors,
highlighting the need for improvements in factor utilization and robust
abstention capabilities before reliable deployment in legal settings. Link:
https://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/

</details>


### [220] [Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models](https://arxiv.org/pdf/2506.00773)
*Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He*

Main category: cs.CL

TL;DR: Proposes a dynamic chunking method for LLMs to better handle long texts by adaptively dividing and selecting relevant chunks based on semantic similarity and question relevance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long texts due to fixed-length chunking, which can split semantically relevant content, leading to ambiguity.

Method: Dynamically divides long contexts into variable-length chunks using semantic similarity between sentences and trains a question-aware classifier to select critical chunks.

Result: Outperforms baselines on QA benchmarks, handling sequences up to 256k tokens robustly.

Conclusion: The approach improves LLM performance on long texts by dynamically selecting relevant chunks, with demonstrated effectiveness and scalability.

Abstract: Large language models (LLMs) often struggle to accurately read and comprehend
extremely long texts. Current methods for improvement typically rely on
splitting long contexts into fixed-length chunks. However, fixed truncation
risks separating semantically relevant content, leading to ambiguity and
compromising accurate understanding. To overcome this limitation, we propose a
straightforward approach for dynamically separating and selecting chunks of
long context, facilitating a more streamlined input for LLMs. In particular, we
compute semantic similarities between adjacent sentences, using lower
similarities to adaptively divide long contexts into variable-length chunks. We
further train a question-aware classifier to select sensitive chunks that are
critical for answering specific questions. Experimental results on both
single-hop and multi-hop question-answering benchmarks show that the proposed
approach consistently outperforms strong baselines. Notably, it maintains
robustness across a wide range of input lengths, handling sequences of up to
256k tokens. Our datasets and code are available at the following link:
https://github.com/ECNU-Text-Computing/DCS

</details>


### [221] [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/pdf/2506.00829)
*Keyuan Cheng, Zijian Kan, Zhixian He, Zhuoran Zhang, Muhammad Asif Ali, Ke Xu, Lijie Hu, Di Wang*

Main category: cs.CL

TL;DR: The paper introduces COMPKE, a benchmark for evaluating knowledge editing in large language models using complex real-life questions, revealing varying effectiveness across models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for knowledge editing in large language models lack evaluation of real-life applicability, especially for complex reasoning tasks.

Method: The authors propose COMPKE, a benchmark with 11,924 complex questions, and evaluate four knowledge editing methods on it.

Result: Effectiveness of editing methods varies significantly across models (e.g., MeLLo's accuracy drops from 39.47 to 3.83 on different models).

Conclusion: COMPKE highlights the need for better benchmarks and methods to evaluate knowledge editing in real-life scenarios.

Abstract: Knowledge Editing, which efficiently modifies the knowledge in large language
models, has gathered great attention. Current benchmarks primarily use
multi-hop question answering to assess and analyze newly injected or updated
knowledge. However, we argue that these benchmarks fail to effectively evaluate
how well the updated models apply this knowledge in real-life scenarios,
particularly when questions require complex reasoning, involving one-to-many
relationships or multi-step logical intersections. To fill in this gap, we
introduce a new benchmark, COMPKE: Complex Question Answering under Knowledge
Editing, which includes 11,924 complex questions that reflect real-life
situations. We conduct an extensive evaluation of four knowledge editing
methods on COMPKE, revealing that their effectiveness varies notably across
different models. For instance, MeLLo attains an accuracy of 39.47 on
GPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further
investigate the underlying causes of these disparities from both methodological
and model-specific perspectives. The datasets are available at
https://github.com/kzjkzj666/CompKE.

</details>


### [222] [How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation](https://arxiv.org/pdf/2506.00859)
*Md Kowsher, Nusrat Jahan Prottasha, Shiyun Xu, Shetu Mohanto, Chen Chen, Ozlem Garibay, Niloofar Yousefi*

Main category: cs.CL

TL;DR: Bidirectional language models outperform unidirectional ones due to better information retention and higher representational complexity, explained via the Information Bottleneck principle. FlowNIB is introduced as a scalable method to analyze this.


<details>
  <summary>Details</summary>
Motivation: To understand why bidirectional models perform better than unidirectional ones in natural language understanding tasks.

Method: Proposes FlowNIB, a dynamic method for estimating mutual information during training, addressing limitations of classical IB approaches. Theoretical analysis and experiments validate the framework.

Result: Bidirectional models retain more mutual information and have higher effective dimensionality. FlowNIB effectively analyzes information flow in models.

Conclusion: The work provides a theoretical explanation for bidirectional models' superiority and introduces FlowNIB as a practical tool for analyzing information in language models.

Abstract: Bidirectional language models have better context understanding and perform
better than unidirectional models on natural language understanding tasks, yet
the theoretical reasons behind this advantage remain unclear. In this work, we
investigate this disparity through the lens of the Information Bottleneck (IB)
principle, which formalizes a trade-off between compressing input information
and preserving task-relevant content. We propose FlowNIB, a dynamic and
scalable method for estimating mutual information during training that
addresses key limitations of classical IB approaches, including computational
intractability and fixed trade-off schedules. Theoretically, we show that
bidirectional models retain more mutual information and exhibit higher
effective dimensionality than unidirectional models. To support this, we
present a generalized framework for measuring representational complexity and
prove that bidirectional representations are strictly more informative under
mild conditions. We further validate our findings through extensive experiments
across multiple models and tasks using FlowNIB, revealing how information is
encoded and compressed throughout training. Together, our work provides a
principled explanation for the effectiveness of bidirectional architectures and
introduces a practical tool for analyzing information flow in deep language
models.

</details>


### [223] [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/pdf/2506.00912)
*Yongdong chi, Hanqing Wang, Zonghan Yang, Jian Yang, Xiao Yan, Yun Chen, Guanhua Chen*

Main category: cs.CL

TL;DR: Pi-SQL uses Python as a pivot to bridge natural language queries and SQL, improving accuracy and execution speed.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based methods struggle with the semantic gap between text and SQL, limiting accuracy.

Method: Pi-SQL generates Python programs for step-by-step guidance, then derives SQL from them, selecting the best candidate.

Result: Pi-SQL achieves higher execution accuracy (up to 3.20 improvement) and efficiency (reward score 4.55 higher than baselines).

Conclusion: Pi-SQL effectively bridges the gap between natural language and SQL, outperforming existing methods.

Abstract: Text-to-SQL transforms the user queries from natural language to executable
SQL programs, enabling non-experts to interact with complex databases. Existing
prompt-based methods craft meticulous text guidelines and examples to
facilitate SQL generation, but their accuracy is hindered by the large semantic
gap between the texts and the low-resource SQL programs. In this work, we
propose Pi-SQL, which incorporates the high-resource Python program as a pivot
to bridge between the natural language query and SQL program. In particular,
Pi-SQL first generates Python programs that provide fine-grained step-by-step
guidelines in their code blocks or comments, and then produces an SQL program
following the guidance of each Python program. The final SQL program matches
the reference Python program's query results and, through selection from
candidates generated by different strategies, achieves superior execution
speed, with a reward-based valid efficiency score up to 4.55 higher than the
best-performing baseline. Extensive experiments demonstrate the effectiveness
of Pi-SQL, which improves the execution accuracy of the best-performing
baseline by up to 3.20.

</details>


### [224] [CHEER-Ekman: Fine-grained Embodied Emotion Classification](https://arxiv.org/pdf/2506.01047)
*Phan Anh Duong, Cat Luong, Divyesh Bommana, Tianyu Jiang*

Main category: cs.CL

TL;DR: The paper introduces CHEER-Ekman, a dataset for embodied emotion classification, extending binary emotion labels to Ekman's six categories. It uses large language models with best-worst scaling, outperforming supervised methods, and shows simplified prompts and chain-of-thought reasoning boost accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the understudied area of identifying embodied emotions in text by expanding emotion categories and improving recognition methods.

Method: Uses automatic best-worst scaling with large language models, incorporating simplified prompting and chain-of-thought reasoning.

Result: Achieves superior performance over supervised approaches, with smaller models matching larger ones in accuracy.

Conclusion: The CHEER-Ekman dataset and proposed methods advance embodied emotion recognition, with potential for broader applications.

Abstract: Emotions manifest through physical experiences and bodily reactions, yet
identifying such embodied emotions in text remains understudied. We present an
embodied emotion classification dataset, CHEER-Ekman, extending the existing
binary embodied emotion dataset with Ekman's six basic emotion categories.
Using automatic best-worst scaling with large language models, we achieve
performance superior to supervised approaches on our new dataset. Our
investigation reveals that simplified prompting instructions and
chain-of-thought reasoning significantly improve emotion recognition accuracy,
enabling smaller models to achieve competitive performance with larger ones.
Our dataset is publicly available at: https://github.com/menamerai/cheer-ekman.

</details>


### [225] [STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/pdf/2506.01531)
*Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, Yan Ma*

Main category: cs.CL

TL;DR: STORM-BORN is a high-quality, ultra-challenging math dataset sourced from academic papers, designed to improve LLMs' reasoning by addressing outdated content, lack of human-like reasoning, and reliability issues.


<details>
  <summary>Details</summary>
Motivation: Existing math datasets for LLMs are outdated, lack challenging content, and neglect human-like reasoning, limiting model performance.

Method: A human-in-the-loop, multi-agent framework generates the dataset, incorporating reasoning-dense filters, multi-agent collaboration, and human evaluations.

Result: The dataset includes 2,000 synthetic samples, with 100 highly difficult problems. Advanced models like GPT-o1 solved fewer than 5%. Fine-tuning on STORM-BORN improved accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B).

Conclusion: STORM-BORN serves as a high-difficulty benchmark and training resource for human-like reasoning, advancing AI toward mathematician-level capabilities.

Abstract: High-quality math datasets are crucial for advancing the reasoning abilities
of large language models (LLMs). However, existing datasets often suffer from
three key issues: outdated and insufficient challenging content, neglecting
human-like reasoning, and limited reliability due to single-LLM generation. To
address these, we introduce STORM-BORN, an ultra-challenging dataset of
mathematical derivations sourced from cutting-edge academic papers, which
includes dense human-like approximations and heuristic cues. To ensure the
reliability and quality, we propose a novel human-in-the-loop, multi-agent data
generation framework, integrating reasoning-dense filters, multi-agent
collaboration, and human mathematicians' evaluations. We curated a set of 2,000
synthetic samples and deliberately selected the 100 most difficult problems.
Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning
on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As
AI approaches mathematician-level reasoning, STORM-BORN provides both a
high-difficulty benchmark and a human-like reasoning training resource. Our
code and dataset are publicly available at
https://github.com/lwhere/STORM-BORN.

</details>


### [226] [IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems](https://arxiv.org/pdf/2506.01615)
*Pasunuti Prasanjith, Prathmesh B More, Anoop Kunchukuttan, Raj Dabre*

Main category: cs.CL

TL;DR: The paper introduces IndicMSMarco, a benchmark and dataset for improving Retrieval-Augmented Generation (RAG) systems in Indian languages, addressing the lack of evaluation benchmarks and training data.


<details>
  <summary>Details</summary>
Motivation: The development of RAG systems for Indian languages is hindered by missing evaluation benchmarks and training datasets, which are mostly available for English or high-resource languages.

Method: The authors create IndicMSMarco, a multilingual benchmark for 13 Indian languages, and a large-scale dataset of (question, answer, passage) tuples from 19 Indian language Wikipedias, supplemented with translated MS MARCO data.

Result: IndicMSMarco provides evaluation benchmarks and training data for RAG systems in Indian languages, enhancing their accuracy and contextual grounding.

Conclusion: The work bridges the gap in resources for Indian languages, enabling better RAG systems and fostering multilingual AI advancements.

Abstract: Retrieval-Augmented Generation (RAG) systems enable language models to access
relevant information and generate accurate, well-grounded, and contextually
informed responses. However, for Indian languages, the development of
high-quality RAG systems is hindered by the lack of two critical resources: (1)
evaluation benchmarks for retrieval and generation tasks, and (2) large-scale
training datasets for multilingual retrieval. Most existing benchmarks and
datasets are centered around English or high-resource languages, making it
difficult to extend RAG capabilities to the diverse linguistic landscape of
India. To address the lack of evaluation benchmarks, we create IndicMSMarco, a
multilingual benchmark for evaluating retrieval quality and response generation
in 13 Indian languages, created via manual translation of 1000 diverse queries
from MS MARCO-dev set. To address the need for training data, we build a
large-scale dataset of (question, answer, relevant passage) tuples derived from
the Wikipedias of 19 Indian languages using state-of-the-art LLMs.
Additionally, we include translated versions of the original MS MARCO dataset
to further enrich the training data and ensure alignment with real-world
information-seeking tasks. Resources are available here:
https://huggingface.co/collections/ai4bharat/indicragsuite-683e7273cb2337208c8c0fcb

</details>


### [227] [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/pdf/2506.01776)
*Yile Liu, Ziwei Ma, Xiu Jiang, Jinglu Hu, Jing Chang, Liang Li*

Main category: cs.CL

TL;DR: MaXIFE is a multilingual benchmark for evaluating instruction-following in LLMs across 23 languages, combining rule-based and model-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs focus on single-language scenarios, neglecting multilingual challenges.

Method: MaXIFE uses Rule-Based and Model-Based Evaluation across 23 languages with 1667 tasks.

Result: Baseline results for leading commercial LLMs were established.

Conclusion: MaXIFE provides a standardized tool to advance multilingual NLP research.

Abstract: With the rapid adoption of large language models (LLMs) in natural language
processing, the ability to follow instructions has emerged as a key metric for
evaluating their practical utility. However, existing evaluation methods often
focus on single-language scenarios, overlooking the challenges and differences
present in multilingual and cross-lingual contexts. To address this gap, we
introduce MaXIFE: a comprehensive evaluation benchmark designed to assess
instruction-following capabilities across 23 different languages with 1667
verifiable instruction tasks. MaXIFE integrates both Rule-Based Evaluation and
Model-Based Evaluation, ensuring a balance of efficiency and accuracy. We
applied MaXIFE to evaluate several leading commercial LLMs, establishing
baseline results for future comparisons. By providing a standardized tool for
multilingual instruction-following evaluation, MaXIFE aims to advance research
and development in natural language processing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [228] [CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge](https://arxiv.org/pdf/2506.02010)
*Zehua Liu, Xiaolou Li, Chen Chen, Lantian Li, Dong Wang*

Main category: cs.CV

TL;DR: CNVSRC 2024 advances Chinese LVC-VSR research with improved baselines and an additional dataset, focusing on reading and Internet speech scenarios.


<details>
  <summary>Details</summary>
Motivation: To push the state-of-the-art in Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) by addressing limitations and enhancing data diversity.

Method: Utilizes CN-CVS for training and CNVSRC-Single/Multi for evaluation, introducing a stronger baseline system and the CN-CVS2-P1 dataset for open tracks.

Result: Demonstrated innovations in data preprocessing, feature extraction, model design, and training strategies.

Conclusion: CNVSRC 2024 successfully advances Chinese LVC-VSR research with improved methodologies and datasets.

Abstract: This paper presents the second Chinese Continuous Visual Speech Recognition
Challenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in
Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The
challenge evaluates two test scenarios: reading in recording studios and
Internet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC
2023, which involves CN-CVS for training and CNVSRC-Single/Multi for
development and evaluation. However, CNVSRC 2024 introduced two key
improvements: (1) a stronger baseline system, and (2) an additional dataset,
CN-CVS2-P1, for open tracks to improve data volume and diversity. The new
challenge has demonstrated several important innovations in data preprocessing,
feature extraction, model design, and training strategies, further pushing the
state-of-the-art in Chinese LVC-VSR. More details and resources are available
at the official website.

</details>


### [229] [OASIS: Online Sample Selection for Continual Visual Instruction Tuning](https://arxiv.org/pdf/2506.02011)
*Minjae Lee, Minhyuk Seo, Tingyu Qu, Tinne Tuytelaars, Jonghyun Choi*

Main category: cs.CV

TL;DR: OASIS is an adaptive online sample selection method for continual visual instruction tuning (CVIT) that dynamically adjusts sample selection per batch and minimizes redundancy, achieving strong performance with only 25% of data.


<details>
  <summary>Details</summary>
Motivation: Training delays in CVIT due to large-scale data hinder real-time adaptation, and existing methods rely on impractical pre-trained reference models or suffer from distribution shifts.

Method: OASIS dynamically adjusts selected samples per batch based on relative inter-batch informativeness and minimizes redundancy through iterative score updates.

Result: OASIS achieves comparable performance to full-data training using only 25% of the data and outperforms state-of-the-art methods.

Conclusion: OASIS effectively addresses the limitations of existing CVIT methods, enabling efficient and adaptive online learning.

Abstract: In continual visual instruction tuning (CVIT) scenarios, where multi-modal
data continuously arrive in an online streaming manner, training delays from
large-scale data significantly hinder real-time adaptation. While existing data
selection strategies reduce training overheads, they rely on pre-trained
reference models, which are impractical in CVIT setups due to unknown future
data. Recent reference model-free online sample selection methods address this
issue but typically select a fixed number of samples per batch (e.g., top-k),
causing them to suffer from distribution shifts where informativeness varies
across batches. To address these limitations, we propose OASIS, an adaptive
online sample selection approach for CVIT that: (1) dynamically adjusts
selected samples per batch based on relative inter-batch informativeness, and
(2) minimizes redundancy of selected samples through iterative selection score
updates. Empirical results across various MLLMs, such as LLaVA-1.5 and
Qwen-VL-2.5, show that OASIS achieves comparable performance to full-data
training using only 25% of the data and outperforms the state-of-the-art.

</details>


### [230] [Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing](https://arxiv.org/pdf/2506.02012)
*Zehua Liu, Xiaolou Li, Li Guo, Lantian Li, Dong Wang*

Main category: cs.CV

TL;DR: The paper explores leveraging Large Language Models (LLMs) for Visual Speech Recognition (VSR), introducing scaling tests, context-aware decoding, and iterative polishing to improve performance.


<details>
  <summary>Details</summary>
Motivation: The potential of LLMs in VSR is underexplored, and effective utilization methods are lacking.

Method: Three approaches: scaling tests to study LLM size impact, context-aware decoding for accuracy, and iterative polishing to reduce errors.

Result: Significant VSR performance improvement by harnessing LLMs effectively.

Conclusion: LLMs hold great potential for VSR, and the proposed methods unlock their capabilities effectively.

Abstract: Visual Speech Recognition (VSR) transcribes speech by analyzing lip
movements. Recently, Large Language Models (LLMs) have been integrated into VSR
systems, leading to notable performance improvements. However, the potential of
LLMs has not been extensively studied, and how to effectively utilize LLMs in
VSR tasks remains unexplored. This paper systematically explores how to better
leverage LLMs for VSR tasks and provides three key contributions: (1) Scaling
Test: We study how the LLM size affects VSR performance, confirming a scaling
law in the VSR task. (2) Context-Aware Decoding: We add contextual text to
guide the LLM decoding, improving recognition accuracy. (3) Iterative
Polishing: We propose iteratively refining LLM outputs, progressively reducing
recognition errors. Extensive experiments demonstrate that by these designs,
the great potential of LLMs can be largely harnessed, leading to significant
VSR performance improvement.

</details>


### [231] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/pdf/2506.02014)
*Wang Mengjie, Zhu Huiping, Li Jian, Shi Wenxiu, Zhang Song*

Main category: cs.CV

TL;DR: The paper proposes a method to optimize multimodal models for driving scenarios, addressing challenges like data collection, training, and deployment. It improves accuracy and efficiency through dynamic prompt optimization, hybrid datasets, and advanced training techniques.


<details>
  <summary>Details</summary>
Motivation: The need for better understanding of complex driving scenarios in autonomous and assisted driving technologies drives the development of optimized multimodal models.

Method: The method includes dynamic prompt optimization, hybrid dataset construction (real and synthetic data), and advanced training techniques like knowledge distillation and quantization.

Result: Experiments show improved accuracy in tasks like cone detection and traffic light recognition, along with efficient resource utilization.

Conclusion: The systematic optimization method enhances model performance and practicality for driving scenario perception technologies.

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [232] [Object-centric Self-improving Preference Optimization for Text-to-Image Generation](https://arxiv.org/pdf/2506.02015)
*Yoonjin Oh, Yongjin Kim, Hyomin Kim, Donghwan Chi, Sungwoong Kim*

Main category: cs.CV

TL;DR: The paper introduces OSPO, a framework for improving text-to-image generation in MLLMs by autonomously creating high-quality preference pairs through object-centric methods, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack fine-grained visual comprehension in text-to-image tasks, and preference optimization for generation is underexplored.

Method: OSPO uses MLLMs' reasoning to autonomously generate object-level contrastive preference pairs via prompt perturbation, densification, and VQA scoring.

Result: OSPO shows significant performance gains on three compositional text-to-image benchmarks.

Conclusion: OSPO effectively addresses fine-grained generation challenges in MLLMs without external data or models.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly improved both image understanding and generation capabilities.
Despite these improvements, MLLMs still struggle with fine-grained visual
comprehension, particularly in text-to-image generation tasks. While preference
optimization methods have been explored to address these limitations in image
understanding tasks, their application to image generation remains largely
underexplored. To address this gap, we propose an Object-centric Self-improving
Preference Optimization (OSPO) framework designed for text-to-image generation
by MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without
requiring any external datasets or models. OSPO emphasizes the importance of
high-quality preference pair data, which is critical for effective preference
optimization. To achieve this, it introduces a self-improving mechanism that
autonomously constructs object-level contrastive preference pairs through
object-centric prompt perturbation, densification and VQA scoring. This process
eliminates ambiguous or disproportionate variations commonly found in naively
generated preference pairs, thereby enhancing the effectiveness of preference
optimization. We validate OSPO on three representative compositional
text-to-image benchmarks, demonstrating substantial performance gains over
baseline models.

</details>


### [233] [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/pdf/2506.03144)
*Wei Chow, Yuan Gao, Linfeng Li, Xian Wang, Qi Xu, Hang Song, Lingdong Kong, Ran Zhou, Yi Zeng, Yidong Cai, Botian Jiang, Shilin Xu, Jiajun Zhang, Minghui Qiu, Xiangtai Li, Tianshu Yang, Siliang Tang, Juncheng Li*

Main category: cs.CV

TL;DR: The paper introduces MERIT, a multilingual dataset for multi-condition semantic retrieval, and Coral, a fine-tuning framework that improves retrieval performance by 45.9%.


<details>
  <summary>Details</summary>
Motivation: Existing datasets and models for semantic retrieval are limited, failing to handle multi-condition queries effectively.

Method: Proposes Coral, a framework combining embedding reconstruction and contrastive learning to adapt pre-trained MLLMs.

Result: Coral achieves a 45.9% performance improvement on MERIT and generalizes well across 8 benchmarks.

Conclusion: The contributions (dataset, limitation identification, and framework) advance research in multi-condition semantic retrieval.

Abstract: Semantic retrieval is crucial for modern applications yet remains
underexplored in current research. Existing datasets are limited to single
languages, single images, or singular retrieval conditions, often failing to
fully exploit the expressive capacity of visual information as evidenced by
maintained performance when images are replaced with captions. However,
practical retrieval scenarios frequently involve interleaved multi-condition
queries with multiple images. Hence, this paper introduces MERIT, the first
multilingual dataset for interleaved multi-condition semantic retrieval,
comprising 320,000 queries with 135,000 products in 5 languages, covering 7
distinct product categories. Extensive experiments on MERIT identify existing
models's limitation: focusing solely on global semantic information while
neglecting specific conditional elements in queries. Consequently, we propose
Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by
integrating embedding reconstruction to preserve fine-grained conditional
elements and contrastive learning to extract comprehensive global semantics.
Experiments demonstrate that Coral achieves a 45.9% performance improvement
over conventional approaches on MERIT, with strong generalization capabilities
validated across 8 established retrieval benchmarks. Collectively, our
contributions - a novel dataset, identification of critical limitations in
existing approaches, and an innovative fine-tuning framework - establish a
foundation for future research in interleaved multi-condition semantic
retrieval.

</details>


### [234] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/pdf/2506.02016)
*Nuolin Sun, Linyuan Wang, Dongyang Li, Bin Yan, Lei Li*

Main category: cs.CV

TL;DR: The paper proposes a method for adversarial example detection and image recognition using layer-wise feature paths, achieving a trade-off between clean and adversarial accuracy without costly adversarial training.


<details>
  <summary>Details</summary>
Motivation: Classical DNNs have weak adversarial robustness, and adversarial training, while effective, is computationally expensive. The study aims to leverage DNN layer features for robustness.

Method: Constructs feature paths from layer-wise features and computes correlations with class-centered paths for adversarial detection and recognition.

Result: Achieves 82.77% clean and 44.17% adversarial accuracy on ResNet-20, and 80.01% and 46.1% on ResNet-18, showing inherent robustness.

Conclusion: The method reveals inherent adversarial robustness in DNNs, challenging conventional views, and offers a less computationally intensive alternative to adversarial training.

Abstract: Adversarial attacks have received increasing attention and it has been widely
recognized that classical DNNs have weak adversarial robustness. The most
commonly used adversarial defense method, adversarial training, improves the
adversarial accuracy of DNNs by generating adversarial examples and retraining
the model. However, adversarial training requires a significant computational
overhead. In this paper, inspired by existing studies focusing on the
clustering properties of DNN output features at each layer and the Progressive
Feedforward Collapse phenomenon, we propose a method for adversarial example
detection and image recognition that uses layer-wise features to construct
feature paths and computes the correlation between the examples feature paths
and the class-centered feature paths. Experimental results show that the
recognition method achieves 82.77% clean accuracy and 44.17% adversarial
accuracy on the ResNet-20 with PFC. Compared to the adversarial training method
with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits
a trade-off without relying on computationally expensive defense strategies.
Furthermore, on the standard ResNet-18, our method maintains this advantage
with respective metrics of 80.01% and 46.1%. This result reveals inherent
adversarial robustness in DNNs, challenging the conventional understanding of
the weak adversarial robustness in DNNs.

</details>


### [235] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/pdf/2506.03150)
*Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Ronald Clark, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: IllumiCraft integrates lighting, appearance, and geometry cues into a diffusion framework for high-quality, controllable video generation.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models lack explicit geometric cues for lighting and appearance control in video sequences.

Method: IllumiCraft uses HDR video maps, relit frames, and 3D point tracks as inputs in a unified diffusion architecture.

Result: It generates temporally coherent videos with better fidelity than existing methods, supporting text and background conditioning.

Conclusion: IllumiCraft advances controllable video generation by integrating multiple cues for improved lighting and appearance control.

Abstract: Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page

</details>


### [236] [Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition](https://arxiv.org/pdf/2506.02017)
*Camilla Quaresmini, Giacomo Zanotti*

Main category: cs.CV

TL;DR: The paper critiques Automatic Gender Recognition (AGR) systems for their binary assumptions and unreliability, proposing a rethinking with feedback mechanisms to improve fairness and respect for gender diversity.


<details>
  <summary>Details</summary>
Motivation: AGR systems often misclassify gender due to binary assumptions and observable features, failing to account for non-binary and gender non-conforming individuals. The paper aims to address these issues.

Method: The authors distinguish between sex, gender, and gender expression, then propose a feedback mechanism allowing users to correct AGR outputs, inspired by human-human misgendering dynamics.

Result: The suggested feedback mechanism could increase fairness in AGR systems, aligning them with respect for individual rights and self-expression.

Conclusion: AGR systems should be redesigned to include user feedback, promoting fairness and respecting gender diversity, rather than relying on flawed binary classifications.

Abstract: Automatic Gender Recognition (AGR) systems are an increasingly widespread
application in the Machine Learning (ML) landscape. While these systems are
typically understood as detecting gender, they often classify datapoints based
on observable features correlated at best with either male or female sex. In
addition to questionable binary assumptions, from an epistemological point of
view, this is problematic for two reasons. First, there exists a gap between
the categories the system is meant to predict (woman versus man) and those onto
which their output reasonably maps (female versus male). What is more, gender
cannot be inferred on the basis of such observable features. This makes AGR
tools often unreliable, especially in the case of non-binary and gender
non-conforming people. We suggest a theoretical and practical rethinking of AGR
systems. To begin, distinctions are made between sex, gender, and gender
expression. Then, we build upon the observation that, unlike algorithmic
misgendering, human-human misgendering is open to the possibility of
re-evaluation and correction. We suggest that analogous dynamics should be
recreated in AGR, giving users the possibility to correct the system's output.
While implementing such a feedback mechanism could be regarded as diminishing
the system's autonomy, it represents a way to significantly increase fairness
levels in AGR. This is consistent with the conceptual change of paradigm that
we advocate for AGR systems, which should be understood as tools respecting
individuals' rights and capabilities of self-expression and determination.

</details>


### [237] [T-TAME: Trainable Attention Mechanism for Explaining Convolutional Networks and Vision Transformers](https://arxiv.org/pdf/2403.04523)
*Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris*

Main category: cs.CV

TL;DR: T-TAME is a trainable attention mechanism for explaining Vision Transformers and CNNs in image classification, outperforming costly methods with single-pass explanations.


<details>
  <summary>Details</summary>
Motivation: The 'black box' nature of neural networks hinders adoption in explainability-critical applications, especially for newer architectures like Vision Transformers.

Method: T-TAME introduces a general methodology for explaining deep networks, applicable to CNNs and Vision Transformers, using a streamlined training approach.

Result: T-TAME achieves SOTA performance, generating high-quality explanation maps in one forward pass, surpassing perturbation-based methods.

Conclusion: T-TAME is a versatile and efficient solution for explainability in image classification, validated across multiple architectures.

Abstract: The development and adoption of Vision Transformers and other deep-learning
architectures for image classification tasks has been rapid. However, the
"black box" nature of neural networks is a barrier to adoption in applications
where explainability is essential. While some techniques for generating
explanations have been proposed, primarily for Convolutional Neural Networks,
adapting such techniques to the new paradigm of Vision Transformers is
non-trivial. This paper presents T-TAME, Transformer-compatible Trainable
Attention Mechanism for Explanations, a general methodology for explaining deep
neural networks used in image classification tasks. The proposed architecture
and training technique can be easily applied to any convolutional or Vision
Transformer-like neural network, using a streamlined training approach. After
training, explanation maps can be computed in a single forward pass; these
explanation maps are comparable to or outperform the outputs of computationally
expensive perturbation-based explainability techniques, achieving SOTA
performance. We apply T-TAME to three popular deep learning classifier
architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet
dataset, and we demonstrate improvements over existing state-of-the-art
explainability methods. A detailed analysis of the results and an ablation
study provide insights into how the T-TAME design choices affect the quality of
the generated explanation maps.

</details>


### [238] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/pdf/2506.02020)
*Youze Xue, Dian Li, Gang Liu*

Main category: cs.CV

TL;DR: The paper analyzes the role of hard negative samples in contrastive learning for multi-modal embeddings, proposes a method to amplify their gradients, and achieves state-of-the-art performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To understand the specific impact of hard negative samples in contrastive learning and improve multi-modal embedding performance.

Method: Analyzes gradients of info-NCE loss, proposes Explicit Gradient Amplifier for hard negatives, and trains a model based on LLaVA-OneVision-7B.

Result: Achieves state-of-the-art performance on MMEB benchmark and top rank on the leaderboard with the QQMM MLLM.

Conclusion: Amplifying gradients of hard negatives enhances discriminative embeddings, leading to superior performance in multi-modal retrieval tasks.

Abstract: With the rapid advancement of multi-modal large language models (MLLMs) in
recent years, the foundational Contrastive Language-Image Pretraining (CLIP)
framework has been successfully extended to MLLMs, enabling more powerful and
universal multi-modal embeddings for a wide range of retrieval tasks. Despite
these developments, the core contrastive learning paradigm remains largely
unchanged from CLIP-style models to MLLMs. Within this framework, the effective
mining of hard negative samples continues to be a critical factor for enhancing
performance. Prior works have introduced both offline and online strategies for
hard negative mining to improve the efficiency of contrastive learning. While
these approaches have led to improved multi-modal embeddings, the specific
contribution of each hard negative sample to the learning process has not been
thoroughly investigated. In this work, we conduct a detailed analysis of the
gradients of the info-NCE loss with respect to the query, positive, and
negative samples, elucidating the role of hard negatives in updating model
parameters. Building upon this analysis, we propose to explicitly amplify the
gradients associated with hard negative samples, thereby encouraging the model
to learn more discriminative embeddings. Our multi-modal embedding model,
trained with the proposed Explicit Gradient Amplifier and based on the
LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the
MMEB benchmark compared to previous methods utilizing the same MLLM backbone.
Furthermore, when integrated with our self-developed MLLM, QQMM, our approach
attains the top rank on the MMEB leaderboard. Code and models are released on
https://github.com/QQ-MM/QQMM-embed.

</details>


### [239] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/pdf/2506.02021)
*Yinjie Zhao, Heng Zhao, Bihan Wen, Yew-Soon Ong, Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces DAViD, a Reinforcement Learning approach for video dataset distillation, addressing temporal redundancy and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Redundancy reduction in video datasets is underexplored, with existing methods assuming uniform temporal redundancy, limiting effectiveness.

Method: Proposes DAViD, using RL to predict optimal temporal resolution and a teacher-in-the-loop reward function.

Result: DAViD significantly outperforms existing dataset distillation methods.

Conclusion: This work advances semantic-adaptive video dataset distillation, opening avenues for future research.

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


### [240] [Inclusion 2024 Global Multimedia Deepfake Detection Challenge: Towards Multi-dimensional Face Forgery Detection](https://arxiv.org/pdf/2412.20833)
*Yi Zhang, Weize Gao, Changtao Miao, Man Luo, Jianshu Li, Wenzhong Deng, Zhe Li, Bingyu Hu, Weibin Yao, Yunfeng Diao, Wenbo Zhou, Tao Gong, Qi Chu*

Main category: cs.CV

TL;DR: The paper discusses the Global Multimedia Deepfake Detection challenge, highlighting top solutions and their impact on deepfake detection research.


<details>
  <summary>Details</summary>
Motivation: To advance research in detecting manipulated multimedia content (images, audio-video) by showcasing top methodologies from a global competition.

Method: Organized a challenge with 1500 teams, analyzed top 3 solutions from two tracks, and encouraged open-sourcing of methods.

Result: Top solutions were presented, contributing to next-generation deepfake detection systems.

Conclusion: The challenge successfully fostered innovation and collaboration in deepfake detection, with potential for future advancements.

Abstract: In this paper, we present the Global Multimedia Deepfake Detection held
concurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to
detect automatic image and audio-video manipulations including but not limited
to editing, synthesis, generation, Photoshop,etc. Our challenge has attracted
1500 teams from all over the world, with about 5000 valid result submission
counts. We invite the top 20 teams to present their solutions to the challenge,
from which the top 3 teams are awarded prizes in the grand finale. In this
paper, we present the solutions from the top 3 teams of the two tracks, to
boost the research work in the field of image and audio-video forgery
detection. The methodologies developed through the challenge will contribute to
the development of next-generation deepfake detection systems and we encourage
participants to open source their methods.

</details>


### [241] [Hyperspectral Image Generation with Unmixing Guided Diffusion Model](https://arxiv.org/pdf/2506.02601)
*Shiyu Shen, Bin Pan, Ziye Zhang, Zhenwei Shi*

Main category: cs.CV

TL;DR: A novel diffusion model for hyperspectral image generation uses unmixing guidance to reduce dimensionality and ensure physical consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for hyperspectral images rely on conditional schemes, limiting diversity. High dimensionality and physical constraints pose challenges for diffusion models.

Method: Proposes a diffusion model with two modules: an unmixing autoencoder to shift generation to low-dimensional abundance space, and an abundance diffusion module to ensure physical consistency.

Result: Generates high-quality, diverse hyperspectral images, validated by traditional and new metrics.

Conclusion: The model advances hyperspectral data generation by addressing dimensionality and physical constraints effectively.

Abstract: Recently, hyperspectral image generation has received increasing attention,
but existing generative models rely on conditional generation schemes, which
limits the diversity of generated images. Diffusion models are popular for
their ability to generate high-quality samples, but adapting these models from
RGB to hyperspectral data presents the challenge of high dimensionality and
physical constraints. To address these challenges, we propose a novel diffusion
model guided by hyperspectral unmixing. Our model comprises two key modules: an
unmixing autoencoder module and an abundance diffusion module. The unmixing
autoencoder module leverages unmixing guidance to shift the generative task
from the image space to the low-dimensional abundance space, significantly
reducing computational complexity while preserving high fidelity. The abundance
diffusion module generates samples that satisfy the constraints of
non-negativity and unity, ensuring the physical consistency of the
reconstructed HSIs. Additionally, we introduce two evaluation metrics tailored
to hyperspectral data. Empirical results, evaluated using both traditional
metrics and our proposed metrics, indicate that our model is capable of
generating high-quality and diverse hyperspectral images, offering an
advancement in hyperspectral data generation.

</details>


### [242] [Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs](https://arxiv.org/pdf/2506.02022)
*Aditya Kanade, Tanuja Ganu*

Main category: cs.CV

TL;DR: MLLMs often misinterpret visuals despite correct reasoning answers. A new benchmark, 'Do You See Me', reveals MLLMs' poor visual perception (below 50% accuracy vs. humans' 96.49%).


<details>
  <summary>Details</summary>
Motivation: To address the visual perception bottleneck in MLLMs, which masks underlying failures even when answers are correct.

Method: Introduces 'Do You See Me', a scalable benchmark with 1,758 images and 2,612 questions across seven subtasks to evaluate MLLM visual skills.

Result: Top MLLMs average below 50% accuracy, far behind humans (96.49%). Performance drops with task complexity.

Conclusion: MLLMs lack robust visual perception, highlighting the need for improvement in visual attention and fine-grained detail representation.

Abstract: Multimodal Large Language Models (MLLMs) show reasoning promise, yet their
visual perception is a critical bottleneck. Strikingly, MLLMs can produce
correct answers even while misinterpreting crucial visual elements, masking
these underlying failures. Our preliminary study on a joint
perception-reasoning dataset revealed that for one leading MLLM, 29% of its
correct answers to reasoning questions still exhibited visual perception
errors. To systematically address this, we introduce "Do You See Me", a
scalable benchmark with 1,758 images and 2,612 questions. It spans seven
human-psychology inspired subtasks in 2D and 3D, featuring controllable
complexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading
closed-source and 5 major open-source models reveal a stark deficit: humans
achieve 96.49% accuracy, while top MLLMs average below 50%. This performance
gap widens rapidly with increased task complexity (e.g., from 12% to 45% in the
visual form constancy subtask). Further analysis into the root causes suggests
that failures stem from challenges like misallocated visual attention and the
instability of internal representations for fine-grained details, especially at
or below encoder patch resolution. This underscores an urgent need for MLLMs
with truly robust visual perception. The benchmark dataset, source code and
evaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.

</details>


### [243] [Ola: Pushing the Frontiers of Omni-Modal Language Model](https://arxiv.org/pdf/2502.04328)
*Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao*

Main category: cs.CV

TL;DR: Ola is an omni-modal language model achieving competitive performance across image, video, and audio understanding, surpassing open-source alternatives and nearing specialized models.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between open-source omni-modal models and specialized single-modality models, advancing omni-modal understanding.

Method: Comprehensive exploration of architecture, data curation, and training strategies, with improvements in visual and audio understanding, and a progressive training pipeline for cross-modal alignment.

Result: Ola outperforms existing open omni-modal LLMs and competes with specialized models of similar sizes.

Conclusion: Ola is released as an open-source solution to advance omni-modal research, with model weights, code, and data publicly available.

Abstract: Recent advances in large language models, particularly following GPT-4o, have
sparked increasing interest in developing omni-modal models capable of
understanding more modalities. While some open-source alternatives have
emerged, there is still a notable lag behind specialized single-modality models
in performance. In this paper, we present Ola, an Omni-modal Language model
that achieves competitive performance across image, video, and audio
understanding compared to specialized counterparts, pushing the frontiers of
the omni-modal language model to a large extent. We conduct a comprehensive
exploration of architectural design, data curation, and training strategies
essential for building a robust omni-modal model. Ola incorporates advanced
visual understanding and audio recognition capabilities through several
critical and effective improvements over mainstream baselines. Moreover, we
rethink inter-modal relationships during omni-modal training, emphasizing
cross-modal alignment with video as a central bridge, and propose a progressive
training pipeline that begins with the most distinct modalities and gradually
moves towards closer modality alignment. Extensive experiments demonstrate that
Ola surpasses existing open omni-modal LLMs across all modalities while
achieving highly competitive performance compared to state-of-the-art
specialized models of similar sizes. We aim to make Ola a fully open omni-modal
understanding solution to advance future research in this emerging field. Model
weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.

</details>


### [244] [Application of convolutional neural networks in image super-resolution](https://arxiv.org/pdf/2506.02604)
*Tian Chunwei, Song Mingjian, Zuo Wangmeng, Du Bo, Zhang Yanning, Zhang Shichao*

Main category: cs.CV

TL;DR: A summary of CNN-based methods for image super-resolution, comparing their differences, relations, and performance, and identifying future research directions.


<details>
  <summary>Details</summary>
Motivation: The lack of literature summarizing the relations and differences among CNN-based methods for image super-resolution motivates this paper.

Method: The paper reviews CNN principles and compares various interpolation methods (bicubic, nearest neighbor, bilinear, transposed convolution, sub-pixel layer, meta up-sampling) through experiments.

Result: Performance comparisons of different CNN-based interpolation methods are provided.

Conclusion: The paper highlights potential research points and drawbacks, aiding future developments in CNN-based image super-resolution.

Abstract: Due to strong learning abilities of convolutional neural networks (CNNs),
they have become mainstream methods for image super-resolution. However, there
are big differences of different deep learning methods with different types.
There is little literature to summarize relations and differences of different
methods in image super-resolution. Thus, summarizing these literatures are
important, according to loading capacity and execution speed of devices. This
paper first introduces principles of CNNs in image super-resolution, then
introduces CNNs based bicubic interpolation, nearest neighbor interpolation,
bilinear interpolation, transposed convolution, sub-pixel layer, meta
up-sampling for image super-resolution to analyze differences and relations of
different CNNs based interpolations and modules, and compare performance of
these methods by experiments. Finally, this paper gives potential research
points and drawbacks and summarizes the whole paper, which can facilitate
developments of CNNs in image super-resolution.

</details>


### [245] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/pdf/2506.02095)
*Hyojin Bahng, Caroline Chan, Fredo Durand, Phillip Isola*

Main category: cs.CV

TL;DR: The paper proposes a cycle consistency-based method to align language and vision, avoiding costly human/AI preference collection. It constructs a dataset of 866K pairs and trains a reward model, outperforming existing metrics and improving vision-language tasks.


<details>
  <summary>Details</summary>
Motivation: Aligning language and vision is challenging with existing methods being costly and time-intensive due to reliance on human or AI preferences.

Method: Uses cycle consistency as a supervisory signal, mapping text back to image space and vice versa to compute similarity. Constructs a preference dataset and trains a reward model.

Result: The reward model outperforms state-of-the-art alignment metrics and improves vision-language tasks and text-to-image generation.

Conclusion: Cycle consistency is an effective, scalable alternative for aligning language and vision, with demonstrated performance gains.

Abstract: Learning alignment between language and vision is a fundamental challenge,
especially as multimodal data becomes increasingly detailed and complex.
Existing methods often rely on collecting human or AI preferences, which can be
costly and time-intensive. We propose an alternative approach that leverages
cycle consistency as a supervisory signal. Given an image and generated text,
we map the text back to image space using a text-to-image model and compute the
similarity between the original image and its reconstruction. Analogously, for
text-to-image generation, we measure the textual similarity between an input
caption and its reconstruction through the cycle. We use the cycle consistency
score to rank candidates and construct a preference dataset of 866K comparison
pairs. The reward model trained on our dataset outperforms state-of-the-art
alignment metrics on detailed captioning, with superior inference-time
scalability when used as a verifier for Best-of-N sampling. Furthermore,
performing DPO and Diffusion DPO using our dataset enhances performance across
a wide range of vision-language tasks and text-to-image generation. Our
dataset, model, and code are at https://cyclereward.github.io

</details>


### [246] [Solving Inverse Problems with FLAIR](https://arxiv.org/pdf/2506.02680)
*Julius Erbach, Dominik Narnhofer, Andreas Dombos, Bernt Schiele, Jan Eric Lenssen, Konrad Schindler*

Main category: cs.CV

TL;DR: FLAIR is a training-free variational framework using flow-based generative models as priors for inverse imaging problems, outperforming existing methods in quality and diversity.


<details>
  <summary>Details</summary>
Motivation: Flow-based models like Stable Diffusion 3 show high-quality image generation but struggle as priors for inverse problems due to non-linear mappings, intractable likelihoods, and rare mode recovery.

Method: FLAIR introduces a variational objective for flow matching, deterministic trajectory adjustments, decoupled optimization, and time-dependent calibration.

Result: FLAIR outperforms existing diffusion- and flow-based methods in reconstruction quality and sample diversity on standard benchmarks.

Conclusion: FLAIR effectively leverages flow-based models for inverse problems, addressing key obstacles and achieving superior performance.

Abstract: Flow-based latent generative models such as Stable Diffusion 3 are able to
generate images with remarkable quality, even enabling photorealistic
text-to-image generation. Their impressive performance suggests that these
models should also constitute powerful priors for inverse imaging problems, but
that approach has not yet led to comparable fidelity. There are several key
obstacles: (i) the encoding into a lower-dimensional latent space makes the
underlying (forward) mapping non-linear; (ii) the data likelihood term is
usually intractable; and (iii) learned generative models struggle to recover
rare, atypical data modes during inference. We present FLAIR, a novel training
free variational framework that leverages flow-based generative models as a
prior for inverse problems. To that end, we introduce a variational objective
for flow matching that is agnostic to the type of degradation, and combine it
with deterministic trajectory adjustments to recover atypical modes. To enforce
exact consistency with the observed data, we decouple the optimization of the
data fidelity and regularization terms. Moreover, we introduce a time-dependent
calibration scheme in which the strength of the regularization is modulated
according to off-line accuracy estimates. Results on standard imaging
benchmarks demonstrate that FLAIR consistently outperforms existing diffusion-
and flow-based methods in terms of reconstruction quality and sample diversity.

</details>


### [247] [SAB3R: Semantic-Augmented Backbone in 3D Reconstruction](https://arxiv.org/pdf/2506.02112)
*Xuweiyi Chen, Tian Xia, Sihan Xu, Jianing Yang, Joyce Chai, Zezhou Cheng*

Main category: cs.CV

TL;DR: The paper introduces 'Map and Locate,' a task combining open-vocabulary segmentation and 3D reconstruction, and proposes SAB3R, a unified model outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between reconstruction, recognition, and reorganization for real-world embodied AI applications.

Method: SAB3R builds on MASt3R, using lightweight distillation to transfer 2D semantic features (e.g., CLIP, DINOv2) for cohesive point maps in one pass.

Result: SAB3R outperforms separate MASt3R and CLIP deployments on the Map and Locate benchmark and excels in 2D/3D tasks.

Conclusion: SAB3R effectively unifies segmentation and reconstruction, advancing practical AI applications.

Abstract: We introduce a new task, Map and Locate, which unifies the traditionally
distinct objectives of open-vocabulary segmentation - detecting and segmenting
object instances based on natural language queries - and 3D reconstruction, the
process of estimating a scene's 3D structure from visual inputs. Specifically,
Map and Locate involves generating a point cloud from an unposed video and
segmenting object instances based on open-vocabulary queries. This task serves
as a critical step toward real-world embodied AI applications and introduces a
practical task that bridges reconstruction, recognition and reorganization. To
tackle this task, we introduce a simple yet effective baseline, which we denote
as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer
vision, and incorporates a lightweight distillation strategy. This method
transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP
and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary
frozen networks, our model generates per-pixel semantic features and constructs
cohesive point maps in a single forward pass. Compared to separately deploying
MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the
Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic
segmentation and 3D tasks to comprehensively validate its effectiveness.

</details>


### [248] [Astrophotography turbulence mitigation via generative models](https://arxiv.org/pdf/2506.02981)
*Joonyeoup Kim, Yu Yuan, Xingguang Zhang, Xijun Wang, Stanley Chan*

Main category: cs.CV

TL;DR: AstroDiff, a diffusion-based method, improves astronomical image quality by mitigating atmospheric turbulence, outperforming existing learning-based techniques.


<details>
  <summary>Details</summary>
Motivation: Ground-based telescope images suffer from atmospheric turbulence, degrading quality. Current methods like lucky imaging are data-intensive and require manual processing.

Method: AstroDiff uses generative priors and diffusion models to restore images affected by turbulence.

Result: AstroDiff surpasses state-of-the-art methods, offering better perceptual quality and structural fidelity in severe turbulence.

Conclusion: AstroDiff is an effective solution for turbulence mitigation in astronomical images, with code and results publicly available.

Abstract: Photography is the cornerstone of modern astronomical and space research.
However, most astronomical images captured by ground-based telescopes suffer
from atmospheric turbulence, resulting in degraded imaging quality. While
multi-frame strategies like lucky imaging can mitigate some effects, they
involve intensive data acquisition and complex manual processing. In this
paper, we propose AstroDiff, a generative restoration method that leverages
both the high-quality generative priors and restoration capabilities of
diffusion models to mitigate atmospheric turbulence. Extensive experiments
demonstrate that AstroDiff outperforms existing state-of-the-art learning-based
methods in astronomical image turbulence mitigation, providing higher
perceptual quality and better structural fidelity under severe turbulence
conditions. Our code and additional results are available at
https://web-six-kappa-66.vercel.app/

</details>


### [249] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/pdf/2506.02150)
*Stefano Fogarollo, Gregor Laimer, Reto Bale, Matthias Harders*

Main category: cs.CV

TL;DR: A novel implicit registration framework for deformable medical image registration improves accuracy and reliability, outperforming traditional and recent AI methods while preserving anatomical relationships.


<details>
  <summary>Details</summary>
Motivation: Precise image alignment is crucial for oncological treatments, but current AI methods often produce unreliable deformations, limiting clinical adoption.

Method: The framework reformulates registration as a signal reconstruction problem, learning a kernel function to recover dense displacement fields from sparse keypoints, integrated in a hierarchical coarse-to-fine architecture.

Result: Validated on thoracic and abdominal tasks, the method matches specialized commercial systems, showing competitive accuracy and better generalization.

Conclusion: The method bridges the gap between implicit and explicit techniques, offering reliable deformations with potential for clinical use.

Abstract: Deformable medical image registration is an essential task in
computer-assisted interventions. This problem is particularly relevant to
oncological treatments, where precise image alignment is necessary for tracking
tumor growth, assessing treatment response, and ensuring accurate delivery of
therapies. Recent AI methods can outperform traditional techniques in accuracy
and speed, yet they often produce unreliable deformations that limit their
clinical adoption. In this work, we address this challenge and introduce a
novel implicit registration framework that can predict accurate and reliable
deformations. Our insight is to reformulate image registration as a signal
reconstruction problem: we learn a kernel function that can recover the dense
displacement field from sparse keypoint correspondences. We integrate our
method in a novel hierarchical architecture, and estimate the displacement
field in a coarse-to-fine manner. Our formulation also allows for efficient
refinement at test time, permitting clinicians to easily adjust registrations
when needed. We validate our method on challenging intra-patient thoracic and
abdominal zero-shot registration tasks, using public and internal datasets from
the local University Hospital. Our method not only shows competitive accuracy
to state-of-the-art approaches, but also bridges the generalization gap between
implicit and explicit registration techniques. In particular, our method
generates deformations that better preserve anatomical relationships and
matches the performance of specialized commercial systems, underscoring its
potential for clinical adoption.

</details>


### [250] [TIIF-Bench: How Does Your T2I Model Follow Your Instructions?](https://arxiv.org/pdf/2506.02161)
*Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, Lei Zhang*

Main category: cs.CV

TL;DR: TIIF-Bench is introduced to evaluate Text-to-Image (T2I) models' ability to follow complex instructions, addressing limitations in existing benchmarks with diverse prompts and fine-grained metrics.


<details>
  <summary>Details</summary>
Motivation: Existing T2I benchmarks lack prompt diversity and granular evaluation metrics, hindering accurate assessment of model alignment with textual instructions.

Method: TIIF-Bench includes 5000 prompts categorized by difficulty, with short/long versions for robustness testing. It introduces text rendering and style control metrics and uses designer-level prompts and a computable framework for evaluation.

Result: The benchmark reveals strengths and weaknesses of mainstream T2I models and highlights limitations in current benchmarks.

Conclusion: TIIF-Bench provides a systematic way to assess T2I models, offering insights for future improvements in instruction-following capabilities.

Abstract: The rapid advancements of Text-to-Image (T2I) models have ushered in a new
phase of AI-generated content, marked by their growing ability to interpret and
follow user instructions. However, existing T2I model evaluation benchmarks
fall short in limited prompt diversity and complexity, as well as coarse
evaluation metrics, making it difficult to evaluate the fine-grained alignment
performance between textual instructions and generated images. In this paper,
we present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming
to systematically assess T2I models' ability in interpreting and following
intricate textual instructions. TIIF-Bench comprises a set of 5000 prompts
organized along multiple dimensions, which are categorized into three levels of
difficulties and complexities. To rigorously evaluate model robustness to
varying prompt lengths, we provide a short and a long version for each prompt
with identical core semantics. Two critical attributes, i.e., text rendering
and style control, are introduced to evaluate the precision of text synthesis
and the aesthetic coherence of T2I models. In addition, we collect 100
high-quality designer level prompts that encompass various scenarios to
comprehensively assess model performance. Leveraging the world knowledge
encoded in large vision language models, we propose a novel computable
framework to discern subtle variations in T2I model outputs. Through meticulous
benchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and
cons of current T2I models and reveal the limitations of current T2I
benchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/.

</details>


### [251] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/pdf/2506.02164)
*Yu, Qian, Wilson S. Geisler, Xue-Xin Wei*

Main category: cs.CV

TL;DR: The paper introduces Decision Variable Correlation (DVC) to compare decision strategies between models and brains, finding model-monkey similarity lower than expected and decreasing with model performance.


<details>
  <summary>Details</summary>
Motivation: To resolve conflicting claims about the similarity of brain and deep neural network representations by focusing on task-relevant decision strategies.

Method: Proposes DVC to quantify decision strategy similarity, tested on monkey V4/IT recordings and image classification models.

Result: Model-monkey similarity is lower than model-model or monkey-monkey similarity and declines with model performance; adversarial training and larger datasets don't improve it.

Conclusion: Task-relevant representations in monkeys and models diverge fundamentally, suggesting current models may not fully capture biological decision strategies.

Abstract: Previous studies have compared the brain and deep neural networks trained on
image classification. Intriguingly, while some suggest that their
representations are highly similar, others argued the opposite. Here, we
propose a new approach to characterize the similarity of the decision
strategies of two observers (models or brains) using decision variable
correlation (DVC). DVC quantifies the correlation between decoded decisions on
individual samples in a classification task and thus can capture task-relevant
information rather than general representational alignment. We evaluate this
method using monkey V4/IT recordings and models trained on image classification
tasks.
  We find that model--model similarity is comparable to monkey--monkey
similarity, whereas model--monkey similarity is consistently lower and,
surprisingly, decreases with increasing ImageNet-1k performance. While
adversarial training enhances robustness, it does not improve model--monkey
similarity in task-relevant dimensions; however, it markedly increases
model--model similarity. Similarly, pre-training on larger datasets does not
improve model--monkey similarity. These results suggest a fundamental
divergence between the task-relevant representations in monkey V4/IT and those
learned by models trained on image classification tasks.

</details>


### [252] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/pdf/2506.02167)
*Aditi Tiwari, Farzaneh Masoud, Dac Trong Nguyen, Jill Kraft, Heng Ji, Klara Nahrstedt*

Main category: cs.CV

TL;DR: Fire360 is a benchmark dataset for evaluating AI perception and reasoning in firefighting scenarios, featuring 360-degree videos with annotations for five tasks, highlighting gaps in model performance compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address AI reliability in critical environments like firefighting, where situational perception failures cause injuries, by providing a robust evaluation framework.

Method: The dataset includes 228 annotated 360-degree videos from professional firefighting training, supporting tasks like Visual Question Answering and Transformed Object Retrieval.

Result: Human experts outperform models (83.5% vs. GPT-4o) in tasks like Transformed Object Retrieval, revealing AI shortcomings in degradation-invariant reasoning.

Conclusion: Fire360 aims to advance AI models for reliable perception and reasoning under uncertainty, with the dataset publicly available for research.

Abstract: Modern AI systems struggle most in environments where reliability is critical
- scenes with smoke, poor visibility, and structural deformation. Each year,
tens of thousands of firefighters are injured on duty, often due to breakdowns
in situational perception. We introduce Fire360, a benchmark for evaluating
perception and reasoning in safety-critical firefighting scenarios. The dataset
includes 228 360-degree videos from professional training sessions under
diverse conditions (e.g., low light, thermal distortion), annotated with action
segments, object locations, and degradation metadata. Fire360 supports five
tasks: Visual Question Answering, Temporal Action Captioning, Object
Localization, Safety-Critical Reasoning, and Transformed Object Retrieval
(TOR). TOR tests whether models can match pristine exemplars to fire-damaged
counterparts in unpaired scenes, evaluating transformation-invariant
recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag
significantly, exposing failures in reasoning under degradation. By releasing
Fire360 and its evaluation suite, we aim to advance models that not only see,
but also remember, reason, and act under uncertainty. The dataset is available
at: https://uofi.box.com/v/fire360dataset.

</details>


### [253] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/pdf/2506.02221)
*Johannes Schusterbauer, Ming Gui, Frank Fundel, Björn Ommer*

Main category: cs.CV

TL;DR: Diff2Flow bridges diffusion models and flow matching (FM) for efficient knowledge transfer, enabling FM finetuning of diffusion priors without extra overhead.


<details>
  <summary>Details</summary>
Motivation: Current FM models are computationally heavy for finetuning, while diffusion models like Stable Diffusion have efficient architectures. Diff2Flow aims to transfer knowledge from diffusion to FM efficiently.

Method: Proposes Diff2Flow, a framework aligning diffusion and FM by rescaling timesteps, aligning interpolants, and deriving FM-compatible velocity fields from diffusion predictions.

Result: Outperforms naive FM and diffusion finetuning, especially under parameter-efficient constraints, and achieves competitive performance in downstream tasks.

Conclusion: Diff2Flow successfully bridges diffusion and FM, offering efficient finetuning and superior performance, with code to be released.

Abstract: Diffusion models have revolutionized generative tasks through high-fidelity
outputs, yet flow matching (FM) offers faster inference and empirical
performance gains. However, current foundation FM models are computationally
prohibitive for finetuning, while diffusion models like Stable Diffusion
benefit from efficient architectures and ecosystem support. This work addresses
the critical challenge of efficiently transferring knowledge from pre-trained
diffusion models to flow matching. We propose Diff2Flow, a novel framework that
systematically bridges diffusion and FM paradigms by rescaling timesteps,
aligning interpolants, and deriving FM-compatible velocity fields from
diffusion predictions. This alignment enables direct and efficient FM
finetuning of diffusion priors with no extra computation overhead. Our
experiments demonstrate that Diff2Flow outperforms na\"ive FM and diffusion
finetuning particularly under parameter-efficient constraints, while achieving
superior or competitive performance across diverse downstream tasks compared to
state-of-the-art methods. We will release our code at
https://github.com/CompVis/diff2flow.

</details>


### [254] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/pdf/2506.02229)
*Manas Mehta, Yimu Pan, Kelly Gallagher, Alison D. Gernand, Jeffery A. Goldstein, Delia Mwinyelle, Leena Mithal, James Z. Wang*

Main category: cs.CV

TL;DR: The paper proposes two modifications to vision-language contrastive learning (VLC) frameworks to improve efficiency and accuracy in detecting childbirth-related pathologies from placenta images and reports.


<details>
  <summary>Details</summary>
Motivation: Existing automated methods for placenta pathology analysis are computationally intensive, limiting deployability. The goal is to enhance efficiency and accessibility of AI-based healthcare solutions.

Method: Introduces (1) text-anchored vision-language contrastive knowledge distillation (VLCD) for medical VLC pretraining, and (2) unsupervised predistillation using natural images for better initialization.

Result: The approach achieves model compression and acceleration while matching or surpassing teacher model performance, especially for lower-quality images.

Conclusion: VLCD improves efficiency and deployability of medical VLC, making AI healthcare solutions more accessible, particularly in resource-constrained settings.

Abstract: Pathological examination of the placenta is an effective method for detecting
and mitigating health risks associated with childbirth. Recent advancements in
AI have enabled the use of photographs of the placenta and pathology reports
for detecting and classifying signs of childbirth-related pathologies. However,
existing automated methods are computationally extensive, which limits their
deployability. We propose two modifications to vision-language contrastive
learning (VLC) frameworks to enhance their accuracy and efficiency: (1)
text-anchored vision-language contrastive knowledge distillation (VLCD)-a new
knowledge distillation strategy for medical VLC pretraining, and (2)
unsupervised predistillation using a large natural images dataset for improved
initialization. Our approach distills efficient neural networks that match or
surpass the teacher model in performance while achieving model compression and
acceleration. Our results showcase the value of unsupervised predistillation in
improving the performance and robustness of our approach, specifically for
lower-quality images. VLCD serves as an effective way to improve the efficiency
and deployability of medical VLC approaches, making AI-based healthcare
solutions more accessible, especially in resource-constrained environments.

</details>


### [255] [Motion aware video generative model](https://arxiv.org/pdf/2506.02244)
*Bowen Xue, Giuseppe Claudio Guarnera, Shuang Zhao, Zahra Montazeri*

Main category: cs.CV

TL;DR: The paper introduces a physics-informed frequency domain approach to improve the physical plausibility of diffusion-based video generation, addressing non-physical artifacts by modeling motion in the frequency domain.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods rely on statistical learning without explicit physics modeling, leading to subtle non-physical artifacts that reduce realism.

Method: The approach includes a physical motion loss function and a frequency domain enhancement module to optimize and adjust video features for physical plausibility.

Result: Experiments show improved motion quality and physical plausibility without sacrificing visual quality or semantic alignment.

Conclusion: The framework bridges data-driven models and physics-based motion models, offering a principled way to integrate physical constraints into video synthesis.

Abstract: Recent advances in diffusion-based video generation have yielded
unprecedented quality in visual content and semantic coherence. However,
current approaches predominantly rely on statistical learning from vast
datasets without explicitly modeling the underlying physics of motion,
resulting in subtle yet perceptible non-physical artifacts that diminish the
realism of generated videos. This paper introduces a physics-informed frequency
domain approach to enhance the physical plausibility of generated videos. We
first conduct a systematic analysis of the frequency-domain characteristics of
diverse physical motions (translation, rotation, scaling), revealing that each
motion type exhibits distinctive and identifiable spectral signatures. Building
on this theoretical foundation, we propose two complementary components: (1) a
physical motion loss function that quantifies and optimizes the conformity of
generated videos to ideal frequency-domain motion patterns, and (2) a frequency
domain enhancement module that progressively learns to adjust video features to
conform to physical motion constraints while preserving original network
functionality through a zero-initialization strategy. Experiments across
multiple video diffusion architectures demonstrate that our approach
significantly enhances motion quality and physical plausibility without
compromising visual quality or semantic alignment. Our frequency-domain
physical motion framework generalizes effectively across different video
generation architectures, offering a principled approach to incorporating
physical constraints into deep learning-based video synthesis pipelines. This
work seeks to establish connections between data-driven models and
physics-based motion models.

</details>


### [256] [PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss](https://arxiv.org/pdf/2506.02247)
*Yu Wang, Juhyung Ha, David J. Crandall*

Main category: cs.CV

TL;DR: PAIR-Net integrates pretrained audio and visual models with alignment loss for robust active speaker detection in egocentric videos, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional visual-centric methods degrade in egocentric videos due to unstable viewpoints and off-screen speech, necessitating a robust cross-modal approach.

Method: PAIR-Net combines a frozen Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone, using inter-modal alignment loss for balanced fusion.

Result: PAIR-Net achieves 76.6% mAP on Ego4D ASD, outperforming LoCoNet and STHG by 8.2% and 12.9%, respectively.

Conclusion: Pretrained audio priors and alignment-based fusion are key for robust active speaker detection in real-world egocentric conditions.

Abstract: Active speaker detection (ASD) in egocentric videos presents unique
challenges due to unstable viewpoints, motion blur, and off-screen speech
sources - conditions under which traditional visual-centric methods degrade
significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with
Regularization Network), an effective model that integrates a partially frozen
Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly
fuse cross-modal cues. To counteract modality imbalance, we introduce an
inter-modal alignment loss that synchronizes audio and visual representations,
enabling more consistent convergence across modalities. Without relying on
multi-speaker context or ideal frontal views, PAIR-Net achieves
state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP,
surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results
highlight the value of pretrained audio priors and alignment-based fusion for
robust ASD under real-world egocentric conditions.

</details>


### [257] [Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction](https://arxiv.org/pdf/2506.02265)
*Samuel Li, Pujith Kachana, Prajwal Chidananda, Saurabh Nair, Yasutaka Furukawa, Matthew Brown*

Main category: cs.CV

TL;DR: Rig3R improves 3D reconstruction by incorporating rig structure, outperforming existing methods by 17-45% mAA.


<details>
  <summary>Details</summary>
Motivation: Existing models treat images as unstructured, limiting performance in synchronized rig scenarios.

Method: Rig3R uses rig metadata (camera ID, time, poses) to create a rig-aware latent space and predicts pointmaps and raymaps.

Result: Achieves state-of-the-art performance in 3D reconstruction, pose estimation, and rig discovery.

Conclusion: Rig3R is robust and efficient, requiring no post-processing or iterative refinement.

Abstract: Estimating agent pose and 3D scene structure from multi-camera rigs is a
central task in embodied AI applications such as autonomous driving. Recent
learned approaches such as DUSt3R have shown impressive performance in
multiview settings. However, these models treat images as unstructured
collections, limiting effectiveness in scenarios where frames are captured from
synchronized rigs with known or inferable structure.
  To this end, we introduce Rig3R, a generalization of prior multiview
reconstruction models that incorporates rig structure when available, and
learns to infer it when not. Rig3R conditions on optional rig metadata
including camera ID, time, and rig poses to develop a rig-aware latent space
that remains robust to missing information. It jointly predicts pointmaps and
two types of raymaps: a pose raymap relative to a global frame, and a rig
raymap relative to a rig-centric frame consistent across time. Rig raymaps
allow the model to infer rig structure directly from input images when metadata
is missing.
  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose
estimation, and rig discovery, outperforming both traditional and learned
methods by 17-45% mAA across diverse real-world rig datasets, all in a single
forward pass without post-processing or iterative refinement.

</details>


### [258] [Entity Image and Mixed-Modal Image Retrieval Datasets](https://arxiv.org/pdf/2506.02291)
*Cristian-Ioan Blaga, Paul Suganthan, Sahil Dua, Krishna Srinivasan, Enrique Alfonseca, Peter Dornbach, Tom Duerig, Imed Zitouni, Zhe Dong*

Main category: cs.CV

TL;DR: The paper introduces a new benchmark and datasets (EI and MMIR) for mixed-modal image retrieval, requiring deep cross-modal understanding, and validates their utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of challenging benchmarks for mixed-modal image retrieval combining visual and textual information.

Method: Introduces two datasets (EI and MMIR) with challenging query types (single and multi-entity-image queries) and validates them empirically.

Result: The benchmark serves as both training corpus and evaluation set, with dataset quality confirmed via human annotations.

Conclusion: The datasets and benchmark provide a robust tool for advancing mixed-modal retrieval research.

Abstract: Despite advances in multimodal learning, challenging benchmarks for
mixed-modal image retrieval that combines visual and textual information are
lacking. This paper introduces a novel benchmark to rigorously evaluate image
retrieval that demands deep cross-modal contextual understanding. We present
two new datasets: the Entity Image Dataset (EI), providing canonical images for
Wikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived
from the WIT dataset. The MMIR benchmark features two challenging query types
requiring models to ground textual descriptions in the context of provided
visual entities: single entity-image queries (one entity image with descriptive
text) and multi-entity-image queries (multiple entity images with relational
text). We empirically validate the benchmark's utility as both a training
corpus and an evaluation set for mixed-modal retrieval. The quality of both
datasets is further affirmed through crowd-sourced human annotations. The
datasets are accessible through the GitHub page:
https://github.com/google-research-datasets/wit-retrieval.

</details>


### [259] [PHISWID: Physics-Inspired Underwater Image Dataset Synthesized from RGB-D Images](https://arxiv.org/pdf/2404.03998)
*Reina Kaneko, Takumi Ueda, Hiroshi Higashi, Yuichi Tanaka*

Main category: cs.CV

TL;DR: PHISWID is a synthetic underwater image dataset designed to improve underwater image processing by providing paired atmospheric and degraded underwater images, addressing limitations of existing datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack physics models, publicity, and ground-truth atmospheric images, hindering effective underwater image enhancement. PHISWID aims to fill this gap.

Method: Underwater images are synthetically degraded from atmospheric RGB-D images using a physics-based model, incorporating color degradation and marine snow artifacts.

Result: Benchmark experiments show PHISWID improves image enhancement performance, enabling effective training of deep neural networks.

Conclusion: PHISWID, publicly available, advances underwater image processing by providing a scalable and physics-inspired dataset.

Abstract: This paper introduces the physics-inspired synthesized underwater image
dataset (PHISWID), a dataset tailored for enhancing underwater image processing
through physics-inspired image synthesis. For underwater image enhancement,
data-driven approaches (e.g., deep neural networks) typically demand extensive
datasets, yet acquiring paired clean atmospheric images and degraded underwater
images poses significant challenges. Existing datasets have limited
contributions to image enhancement due to lack of physics models, publicity,
and ground-truth atmospheric images. PHISWID addresses these issues by offering
a set of paired atmospheric and underwater images. Specifically, underwater
images are synthetically degraded by color degradation and marine snow
artifacts from atmospheric RGB-D images. It is enabled based on a physics-based
underwater image observation model. Our synthetic approach generates a large
quantity of the pairs, enabling effective training of deep neural networks and
objective image quality assessment. Through benchmark experiments with some
datasets and image enhancement methods, we validate that our dataset can
improve the image enhancement performance. Our dataset, which is publicly
available, contributes to the development in underwater image processing.

</details>


### [260] [Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation](https://arxiv.org/pdf/2506.02294)
*Niclas Popp, Kevin Alexander Laube, Matthias Hein, Lukas Schott*

Main category: cs.CV

TL;DR: A novel diffusion-based data augmentation method improves knowledge distillation under covariate shift by generating challenging samples, enhancing student model robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of knowledge distillation effectiveness due to covariate shift and spurious features, leveraging a robust teacher to improve student robustness.

Method: Introduces a diffusion-based data augmentation strategy that generates images maximizing teacher-student disagreement, creating challenging samples for the student.

Result: Significant improvements in worst-group and mean-group accuracy on CelebA and SpuCo Birds, and spurious mAUC on spurious ImageNet, outperforming existing baselines.

Conclusion: The proposed method effectively enhances student model robustness to unknown spurious features under covariate shift, demonstrating superior performance.

Abstract: Large foundation models trained on extensive datasets demonstrate strong
zero-shot capabilities in various domains. To replicate their success when data
and model size are constrained, knowledge distillation has become an
established tool for transferring knowledge from foundation models to small
student networks. However, the effectiveness of distillation is critically
limited by the available training data. This work addresses the common
practical issue of covariate shift in knowledge distillation, where spurious
features appear during training but not at test time. We ask the question: when
these spurious features are unknown, yet a robust teacher is available, is it
possible for a student to also become robust to them? We address this problem
by introducing a novel diffusion-based data augmentation strategy that
generates images by maximizing the disagreement between the teacher and the
student, effectively creating challenging samples that the student struggles
with. Experiments demonstrate that our approach significantly improves worst
group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious
mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art
diffusion-based data augmentation baselines

</details>


### [261] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/pdf/2506.02295)
*Ahmed Wasfy, Omer Nacar, Abdelakreem Elkhateb, Mahmoud Reda, Omar Elshehy, Adel Ammar, Wadii Boulila*

Main category: cs.CV

TL;DR: Qari-OCR, a vision-language model optimized for Arabic, achieves state-of-the-art OCR performance with low error rates and high BLEU scores, excelling in handling diacritics, fonts, and low-resolution images.


<details>
  <summary>Details</summary>
Motivation: The cursive nature, diacritical marks, and varied typography of Arabic script complicate OCR tasks, necessitating specialized solutions.

Method: Qari-OCR is derived from Qwen2-VL-2B-Instruct and fine-tuned on synthetic datasets, with iterative optimizations leading to models like QARI v0.2 and v0.3.

Result: QARI v0.2 achieves a WER of 0.160, CER of 0.061, and BLEU score of 0.737, excelling in tashkeel handling, font diversity, and low-resolution images.

Conclusion: Qari-OCR significantly improves Arabic OCR accuracy and efficiency, with models and datasets released to advance research.

Abstract: The inherent complexities of Arabic script; its cursive nature, diacritical
marks (tashkeel), and varied typography, pose persistent challenges for Optical
Character Recognition (OCR). We present Qari-OCR, a series of vision-language
models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic
through iterative fine-tuning on specialized synthetic datasets. Our leading
model, QARI v0.2, establishes a new open-source state-of-the-art with a Word
Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score
of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling
of tashkeel, diverse fonts, and document layouts, alongside impressive
performance on low-resolution images. Further explorations (QARI v0.3) showcase
strong potential for structural document understanding and handwritten text.
This work delivers a marked improvement in Arabic OCR accuracy and efficiency,
with all models and datasets released to foster further research.

</details>


### [262] [Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free](https://arxiv.org/pdf/2506.00433)
*Luigi Sigillo, Shengfeng He, Danilo Comminiello*

Main category: cs.CV

TL;DR: LWD is a lightweight framework for ultra-high-resolution image synthesis (2K-4K) using latent diffusion models, improving quality without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Balancing computational efficiency and fine-grained detail preservation in high-resolution image synthesis.

Method: Introduces scale-consistent VAE, wavelet energy maps, and time-dependent masking for focused denoising.

Result: Improves perceptual quality and reduces FID, outperforming baselines.

Conclusion: Frequency-aware supervision is effective for high-resolution generative modeling.

Abstract: High-resolution image synthesis remains a core challenge in generative
modeling, particularly in balancing computational efficiency with the
preservation of fine-grained visual detail. We present Latent Wavelet Diffusion
(LWD), a lightweight framework that enables any latent diffusion model to scale
to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces
three key components: (1) a scale-consistent variational autoencoder objective
that enhances the spectral fidelity of latent representations; (2) wavelet
energy maps that identify and localize detail-rich spatial regions within the
latent space; and (3) a time-dependent masking strategy that focuses denoising
supervision on high-frequency components during training. LWD requires no
architectural modifications and incurs no additional computational overhead.
Despite its simplicity, it consistently improves perceptual quality and reduces
FID in ultra-high-resolution image synthesis, outperforming strong baseline
models. These results highlight the effectiveness of frequency-aware,
signal-driven supervision as a principled and efficient approach for
high-resolution generative modeling.

</details>


### [263] [Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning](https://arxiv.org/pdf/2506.02327)
*Yijun Yang, Zhao-Yang Wang, Qiuping Liu, Shuwen Sun, Kang Wang, Rama Chellappa, Zongwei Zhou, Alan Yuille, Lei Zhu, Yu-Dong Zhang, Jieneng Chen*

Main category: cs.CV

TL;DR: MeWM is a medical world model that predicts future disease states visually, combining vision-language and tumor generative models to simulate and optimize clinical treatments.


<details>
  <summary>Details</summary>
Motivation: To enhance clinical decision-making by simulating disease dynamics and evaluating treatment efficacy using generative models.

Method: Uses vision-language models for policy (treatment plans) and tumor generative models for dynamics (tumor progression/regression). Introduces an inverse dynamics model for survival analysis and treatment optimization.

Result: Achieves state-of-the-art specificity in Turing tests and outperforms medical GPTs in treatment optimization, improving F1-score for optimal TACE protocol selection by 13%.

Conclusion: MeWM advances clinical decision-making, demonstrating potential for integration as a second reader in medicine.

Abstract: Providing effective treatment and making informed clinical decisions are
essential goals of modern medicine and clinical care. We are interested in
simulating disease dynamics for clinical decision-making, leveraging recent
advances in large generative models. To this end, we introduce the Medical
World Model (MeWM), the first world model in medicine that visually predicts
future disease states based on clinical decisions. MeWM comprises (i)
vision-language models to serve as policy models, and (ii) tumor generative
models as dynamics models. The policy model generates action plans, such as
clinical treatments, while the dynamics model simulates tumor progression or
regression under given treatment conditions. Building on this, we propose the
inverse dynamics model that applies survival analysis to the simulated
post-treatment tumor, enabling the evaluation of treatment efficacy and the
selection of the optimal clinical action plan. As a result, the proposed MeWM
simulates disease dynamics by synthesizing post-treatment tumors, with
state-of-the-art specificity in Turing tests evaluated by radiologists.
Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs
in optimizing individualized treatment protocols across all metrics. Notably,
MeWM improves clinical decision-making for interventional physicians, boosting
F1-score in selecting the optimal TACE protocol by 13%, paving the way for
future integration of medical world models as the second readers.

</details>


### [264] [Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization](https://arxiv.org/pdf/2506.02334)
*Duo Liu, Zhiquan Tan, Linglan Zhao, Zhongqiang Zhang, Xiangzhong Fang, Weiran Huang*

Main category: cs.CV

TL;DR: RLCD introduces a Reciprocal Learning Framework (RLF) and Class-wise Distribution Regularization (CDR) to improve Generalized Category Discovery (GCD) by enhancing base discrimination and novel class performance.


<details>
  <summary>Details</summary>
Motivation: Parametric-based GCD methods suffer from unreliable self-supervision and inferior base discrimination, prompting the need for a more robust solution.

Method: RLF uses an auxiliary branch for base classification, creating a virtuous cycle with the main branch. CDR mitigates base-class bias by boosting unlabeled data confidence.

Result: RLCD outperforms existing methods on seven GCD datasets with minimal extra computation.

Conclusion: RLCD effectively addresses GCD challenges, achieving superior performance across all classes.

Abstract: Generalized Category Discovery (GCD) aims to identify unlabeled samples by
leveraging the base knowledge from labeled ones, where the unlabeled set
consists of both base and novel classes. Since clustering methods are
time-consuming at inference, parametric-based approaches have become more
popular. However, recent parametric-based methods suffer from inferior base
discrimination due to unreliable self-supervision. To address this issue, we
propose a Reciprocal Learning Framework (RLF) that introduces an auxiliary
branch devoted to base classification. During training, the main branch filters
the pseudo-base samples to the auxiliary branch. In response, the auxiliary
branch provides more reliable soft labels for the main branch, leading to a
virtuous cycle. Furthermore, we introduce Class-wise Distribution
Regularization (CDR) to mitigate the learning bias towards base classes. CDR
essentially increases the prediction confidence of the unlabeled data and
boosts the novel class performance. Combined with both components, our proposed
method, RLCD, achieves superior performance in all classes with negligible
extra computation. Comprehensive experiments across seven GCD datasets validate
its superiority. Our codes are available at https://github.com/APORduo/RLCD.

</details>


### [265] [RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models](https://arxiv.org/pdf/2506.02354)
*Junjie Li, Nan Zhang, Xiaoyang Qu, Kai Lu, Guokuan Li, Jiguang Wan, Jianzong Wang*

Main category: cs.CV

TL;DR: RATE-Nav improves ObjectNav by addressing redundant exploration and failures through a region-aware termination method, leveraging VLMs for efficient termination.


<details>
  <summary>Details</summary>
Motivation: Redundant exploration and failures in ObjectNav remain unresolved; timely termination is underexplored.

Method: Proposes RATE-Nav with geometric predictive region segmentation and region-based exploration estimation, using VLMs.

Result: Achieves 67.8% success rate and 31.3% SPL on HM3D; ~10% improvement on MP3D over zero-shot methods.

Conclusion: RATE-Nav effectively addresses exploration inefficiencies in ObjectNav, demonstrating significant performance gains.

Abstract: Object Navigation (ObjectNav) is a fundamental task in embodied artificial
intelligence. Although significant progress has been made in semantic map
construction and target direction prediction in current research, redundant
exploration and exploration failures remain inevitable. A critical but
underexplored direction is the timely termination of exploration to overcome
these challenges. We observe a diminishing marginal effect between exploration
steps and exploration rates and analyze the cost-benefit relationship of
exploration. Inspired by this, we propose RATE-Nav, a Region-Aware
Termination-Enhanced method. It includes a geometric predictive region
segmentation algorithm and region-Based exploration estimation algorithm for
exploration rate calculation. By leveraging the visual question answering
capabilities of visual language models (VLMs) and exploration rates enables
efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of
31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav
shows approximately 10% improvement over previous zero-shot methods.

</details>


### [266] [InterRVOS: Interaction-aware Referring Video Object Segmentation](https://arxiv.org/pdf/2506.02356)
*Woojeong Jin, Seongchan Kim, Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces Interaction-aware referring video object segmentation (InterRVOS), a new task focusing on segmenting interacting objects in videos using complementary expressions. It presents a dataset (InterRVOS-8K) and a baseline model (ReVIOSa) that outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing referring video object segmentation methods focus on single objects, ignoring interactions between entities, which are crucial for comprehensive video understanding.

Method: The paper proposes InterRVOS-8K, a large-scale dataset with interaction-aware expressions, and ReVIOSa, a baseline model for actor-target segmentation from a single expression.

Result: ReVIOSa achieves strong performance in both standard and interaction-focused settings, outperforming prior methods.

Conclusion: The work establishes a foundation for interaction-centric video understanding and demonstrates the effectiveness of the proposed approach.

Abstract: Referring video object segmentation aims to segment the object in a video
corresponding to a given natural language expression. While prior works have
explored various referring scenarios, including motion-centric or
multi-instance expressions, most approaches still focus on localizing a single
target object in isolation. However, in comprehensive video understanding, an
object's role is often defined by its interactions with other entities, which
are largely overlooked in existing datasets and models. In this work, we
introduce Interaction-aware referring video object sgementation (InterRVOS), a
new task that requires segmenting both actor and target entities involved in an
interaction. Each interactoin is described through a pair of complementary
expressions from different semantic perspectives, enabling fine-grained
modeling of inter-object relationships. To tackle this task, we propose
InterRVOS-8K, the large-scale and automatically constructed dataset containing
diverse interaction-aware expressions with corresponding masks, including
challenging cases such as motion-only multi-instance expressions. We also
present a baseline architecture, ReVIOSa, designed to handle actor-target
segmentation from a single expression, achieving strong performance in both
standard and interaction-focused settings. Furthermore, we introduce an
actor-target-aware evalaution setting that enables a more targeted assessment
of interaction understanding. Experimental results demonstrate that our
approach outperforms prior methods in modeling complex object interactions for
referring video object segmentation task, establishing a strong foundation for
future research in interaction-centric video understanding. Our project page is
available at
\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}.

</details>


### [267] [RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving](https://arxiv.org/pdf/2506.02358)
*Tianze Wang, Zhang Zhang, Chao Sun*

Main category: cs.CV

TL;DR: The paper proposes a vision-based fine-grained road surface classification (RSC) method for autonomous driving, combining convolutional and transformer modules to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhancing road safety and traffic management by accurately classifying road surfaces, especially fine-grained types, for better autonomous driving performance.

Method: A hybrid approach using convolutional and transformer modules for feature fusion, plus a Foreground-Background Module (FBM) to handle fine-grained challenges.

Result: Achieved Top-1 accuracies of 92.52% and 96.50% on two datasets, outperforming SOTA methods by 5.69% to 12.84%.

Conclusion: RoadFormer significantly improves RSC reliability, advancing pavement perception in autonomous driving.

Abstract: The classification of the type of road surface (RSC) aims to utilize pavement
features to identify the roughness, wet and dry conditions, and material
information of the road surface. Due to its ability to effectively enhance road
safety and traffic management, it has received widespread attention in recent
years. In autonomous driving, accurate RSC allows vehicles to better understand
the road environment, adjust driving strategies, and ensure a safer and more
efficient driving experience. For a long time, vision-based RSC has been
favored. However, existing visual classification methods have overlooked the
exploration of fine-grained classification of pavement types (such as similar
pavement textures). In this work, we propose a pure vision-based fine-grained
RSC method for autonomous driving scenarios, which fuses local and global
feature information through the stacking of convolutional and transformer
modules. We further explore the stacking strategies of local and global feature
extraction modules to find the optimal feature extraction strategy. In
addition, since fine-grained tasks also face the challenge of relatively large
intra-class differences and relatively small inter-class differences, we
propose a Foreground-Background Module (FBM) that effectively extracts
fine-grained context features of the pavement, enhancing the classification
ability for complex pavements. Experiments conducted on a large-scale pavement
dataset containing one million samples and a simplified dataset reorganized
from this dataset achieved Top-1 classification accuracies of 92.52% and
96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.
These results demonstrate that RoadFormer outperforms existing methods in RSC
tasks, providing significant progress in improving the reliability of pavement
perception in autonomous driving systems.

</details>


### [268] [Auto-Labeling Data for Object Detection](https://arxiv.org/pdf/2506.02359)
*Brent A. Griffin, Manushree Gangwar, Jacob Sela, Jason J. Corso*

Main category: cs.CV

TL;DR: The paper proposes using vision-language models to auto-generate pseudo-labels for training lightweight object detection models, eliminating the need for costly traditional labeling while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Traditional labeling for object detection is expensive and alternatives often compromise functionality or efficiency. The goal is to train models without ground truth labels by leveraging pre-trained vision-language models.

Method: Configure vision-language foundation models to generate pseudo-labels, then train lightweight detection models using these auto-generated labels.

Result: The approach maintains competitive performance across datasets while significantly reducing labeling time and costs.

Conclusion: Auto-labeling with vision-language models is a viable and efficient alternative to traditional labeling for object detection.

Abstract: Great labels make great models. However, traditional labeling approaches for
tasks like object detection have substantial costs at scale. Furthermore,
alternatives to fully-supervised object detection either lose functionality or
require larger models with prohibitive computational costs for inference at
scale. To that end, this paper addresses the problem of training standard
object detection models without any ground truth labels. Instead, we configure
previously-trained vision-language foundation models to generate
application-specific pseudo "ground truth" labels. These auto-generated labels
directly integrate with existing model training frameworks, and we subsequently
train lightweight detection models that are computationally efficient. In this
way, we avoid the costs of traditional labeling, leverage the knowledge of
vision-language models, and keep the efficiency of lightweight models for
practical application. We perform exhaustive experiments across multiple
labeling configurations, downstream inference models, and datasets to establish
best practices and set an extensive auto-labeling benchmark. From our results,
we find that our approach is a viable alternative to standard labeling in that
it maintains competitive performance on multiple datasets and substantially
reduces labeling time and costs.

</details>


### [269] [A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer](https://arxiv.org/pdf/2506.02364)
*Liang Li, Jianli Zhao, Sheng Fang, Siyu Chen, Hui Sun*

Main category: cs.CV

TL;DR: A novel deep unfolding network (DU-TRPCA) is proposed for hyperspectral image denoising, integrating low-rank and sparse modules with tight coupling and stage-wise alternation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral images (HSIs) suffer from mixed noise, and existing hybrid approaches lack effective alternation between priors, leading to loosely coupled regularization.

Method: DU-TRPCA combines thresholded tensor SVD for low-rankness and a Top-K sparse transformer for sparse constraints, preserving TRPCA's alternation while enhancing representation.

Result: Extensive experiments show DU-TRPCA outperforms state-of-the-art methods under severe mixed noise, offering interpretability and stable denoising.

Conclusion: DU-TRPCA effectively denoises HSIs by tightly integrating low-rank and sparse modules, leveraging TRPCA-inspired alternation and attention mechanisms.

Abstract: Hyperspectral images (HSIs) are often degraded by complex mixed noise during
acquisition and transmission, making effective denoising essential for
subsequent analysis. Recent hybrid approaches that bridge model-driven and
data-driven paradigms have shown great promise. However, most of these
approaches lack effective alternation between different priors or modules,
resulting in loosely coupled regularization and insufficient exploitation of
their complementary strengths. Inspired by tensor robust principal component
analysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that
enforces stage-wise alternation between two tightly integrated modules:
low-rank and sparse. The low-rank module employs thresholded tensor singular
value decomposition (t-SVD), providing a widely adopted convex surrogate for
tensor low-rankness and has been demonstrated to effectively capture the global
spatial-spectral structure of HSIs. The Top-K sparse transformer module
adaptively imposes sparse constraints, directly matching the sparse
regularization in TRPCA and enabling effective removal of localized outliers
and complex noise. This tightly coupled architecture preserves the stage-wise
alternation between low-rank approximation and sparse refinement inherent in
TRPCA, while enhancing representational capacity through attention mechanisms.
Extensive experiments on synthetic and real-world HSIs demonstrate that
DU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while
offering interpretability benefits and stable denoising dynamics inspired by
iterative optimization. Code is available at
https://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising.

</details>


### [270] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/pdf/2506.02366)
*Qin Xie, Qinghua Zhang, Shuyin Xia*

Main category: cs.CV

TL;DR: The paper proposes an approximate borderline sampling method using granular-balls (GBs) to address limitations like class boundary blurring and overlap issues, improving classifier efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing GB-based sampling methods lack borderline strategies and suffer from issues like class boundary blurring due to GB overlaps.

Method: Proposes a restricted diffusion-based GB generation (RD-GBG) to prevent overlaps and a GB-based approximate borderline sampling (GBABS) method for borderline sampling and noise dataset quality improvement.

Result: The methods outperform existing GB-based and other sampling methods, especially on noisy datasets, without needing an optimal purity threshold.

Conclusion: The proposed RD-GBG and GBABS methods effectively address GB overlap and borderline sampling challenges, enhancing classification performance.

Abstract: Data sampling enhances classifier efficiency and robustness through data
compression and quality improvement. Recently, the sampling method based on
granular-ball (GB) has shown promising performance in generality and noisy
classification tasks. However, some limitations remain, including the absence
of borderline sampling strategies and issues with class boundary blurring or
shrinking due to overlap between GBs. In this paper, an approximate borderline
sampling method using GBs is proposed for classification tasks. First, a
restricted diffusion-based GB generation (RD-GBG) method is proposed, which
prevents GB overlaps by constrained expansion, preserving precise geometric
representation of GBs via redefined ones. Second, based on the concept of
heterogeneous nearest neighbor, a GB-based approximate borderline sampling
(GBABS) method is proposed, which is the first general sampling method capable
of both borderline sampling and improving the quality of class noise datasets.
Additionally, since RD-GBG incorporates noise detection and GBABS focuses on
borderline samples, GBABS performs outstandingly on class noise datasets
without the need for an optimal purity threshold. Experimental results
demonstrate that the proposed methods outperform the GB-based sampling method
and several representative sampling methods. Our source code is publicly
available at https://github.com/CherylTse/GBABS.

</details>


### [271] [ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery](https://arxiv.org/pdf/2506.02367)
*Jiayi Su, Dequan Jin*

Main category: cs.CV

TL;DR: The paper proposes ViTNF, a new architecture replacing the MLP head in ViT with a neural field-based classifier, simplifying training and improving performance in generalized category discovery.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high training cost of the MLP head in ViT for few-shot learning, and to better leverage the feature extractor's power.

Method: Introduces a neural field-based classifier (NF) with two coupled static neural fields, replacing the MLP head. Simplifies training by separating feature extractor pre-training and NF classifier training.

Result: ViTNF outperforms state-of-the-art methods on multiple datasets, achieving 19% and 16% accuracy improvements in new and all classes, respectively.

Conclusion: The proposed ViTNF architecture significantly reduces training complexity and enhances performance in generalized category discovery.

Abstract: Generalized category discovery (GCD) is a highly popular task in open-world
recognition, aiming to identify unknown class samples using known class data.
By leveraging pre-training, meta-training, and fine-tuning, ViT achieves
excellent few-shot learning capabilities. Its MLP head is a feedforward
network, trained synchronously with the entire network in the same process,
increasing the training cost and difficulty without fully leveraging the power
of the feature extractor. This paper proposes a new architecture by replacing
the MLP head with a neural field-based one. We first present a new static
neural field function to describe the activity distribution of the neural field
and then use two static neural field functions to build an efficient few-shot
classifier. This neural field-based (NF) classifier consists of two coupled
static neural fields. It stores the feature information of support samples by
its elementary field, the known categories by its high-level field, and the
category information of support samples by its cross-field connections. We
replace the MLP head with the proposed NF classifier, resulting in a novel
architecture ViTNF, and simplify the three-stage training mode by pre-training
the feature extractor on source tasks and training the NF classifier with
support samples in meta-testing separately, significantly reducing ViT's demand
for training samples and the difficulty of model training. To enhance the
model's capability in identifying new categories, we provide an effective
algorithm to determine the lateral interaction scale of the elementary field.
Experimental results demonstrate that our model surpasses existing
state-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard
Cars, achieving dramatic accuracy improvements of 19\% and 16\% in new and all
classes, respectively, indicating a notable advantage in GCD.

</details>


### [272] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/pdf/2506.02382)
*Seulgi Kim, Ghazal Kaviani, Mohit Prabhushankar, Ghassan AlRegib*

Main category: cs.CV

TL;DR: The paper introduces m&m-Ant, a multi-modal action anticipation method combining visual and textual cues with hierarchical modeling, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Action anticipation requires handling incomplete information and uncertainty, but traditional methods often ignore multi-modal integration.

Method: Proposes m&m-Ant, integrating visual and textual cues with hierarchical modeling, a fine-grained label generator, and a temporal consistency loss.

Result: Achieves 3.08% higher anticipation accuracy on datasets like Breakfast, 50 Salads, and DARai.

Conclusion: Highlights the value of multi-modal and hierarchical modeling, setting a new benchmark for action anticipation.

Abstract: Action anticipation, the task of predicting future actions from partially
observed videos, is crucial for advancing intelligent systems. Unlike action
recognition, which operates on fully observed videos, action anticipation must
handle incomplete information. Hence, it requires temporal reasoning, and
inherent uncertainty handling. While recent advances have been made,
traditional methods often focus solely on visual modalities, neglecting the
potential of integrating multiple sources of information. Drawing inspiration
from human behavior, we introduce \textit{Multi-level and Multi-modal Action
Anticipation (m\&m-Ant)}, a novel multi-modal action anticipation approach that
combines both visual and textual cues, while explicitly modeling hierarchical
semantic information for more accurate predictions. To address the challenge of
inaccurate coarse action labels, we propose a fine-grained label generator
paired with a specialized temporal consistency loss function to optimize
performance. Extensive experiments on widely used datasets, including
Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,
achieving state-of-the-art results with an average anticipation accuracy
improvement of 3.08\% over existing methods. This work underscores the
potential of multi-modal and hierarchical modeling in advancing action
anticipation and establishes a new benchmark for future research in the field.
Our code is available at: https://github.com/olivesgatech/mM-ant.

</details>


### [273] [RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection](https://arxiv.org/pdf/2506.02393)
*Yongxian Liu, Boyang Li, Ting Liu, Zaiping Lin, Wei An*

Main category: cs.CV

TL;DR: RRCA-Net is proposed for efficient infrared small target detection using reusable-convolution blocks and dual interactive attention, achieving state-of-the-art performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Infrared small target detection is challenging due to targets being small, dim, and shapeless. Existing CNN-based methods are heavy, so RRCA-Net aims for efficiency and effectiveness.

Method: RRCA-Net uses reusable-convolution blocks (RuCB) recurrently and a dual interactive attention module (DIAAM) for feature refinement and fusion. A target-inspired loss function (DpT-k loss) ensures steady convergence.

Result: RRCA-Net achieves comparable performance to state-of-the-art methods on benchmark datasets (NUAA-SIRST, IRSTD-1k, DenseSIRST) with fewer parameters and can enhance other IRSTD methods.

Conclusion: RRCA-Net is an efficient, plug-and-play solution for infrared small target detection, balancing performance and parameter efficiency.

Abstract: Infrared small target detection is a challenging task due to its unique
characteristics (e.g., small, dim, shapeless and changeable). Recently
published CNN-based methods have achieved promising performance with heavy
feature extraction and fusion modules. To achieve efficient and effective
detection, we propose a recurrent reusable-convolution attention network
(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net
incorporates reusable-convolution block (RuCB) in a recurrent manner without
introducing extra parameters. With the help of the repetitive iteration in
RuCB, the high-level information of small targets in the deep layers can be
well maintained and further refined. Then, a dual interactive attention
aggregation module (DIAAM) is proposed to promote the mutual enhancement and
fusion of refined information. In this way, RRCA-Net can both achieve
high-level feature refinement and enhance the correlation of contextual
information between adjacent layers. Moreover, to achieve steady convergence,
we design a target characteristic inspired loss function (DpT-k loss) by
integrating physical and mathematical constraints. Experimental results on
three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate
that our RRCA-Net can achieve comparable performance to the state-of-the-art
methods while maintaining a small number of parameters, and act as a plug and
play module to introduce consistent performance improvement for several popular
IRSTD methods. Our code will be available at https://github.com/yongxianLiu/
soon.

</details>


### [274] [The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception](https://arxiv.org/pdf/2506.02395)
*Xiaofeng Cong, Yu-Xin Zhang, Haoran Wei, Yeying Jin, Junming Hou, Jie Gui, Jing Zhang, Dacheng Tao*

Main category: cs.CV

TL;DR: The paper introduces DiffND, a framework for converting nighttime hazy images to daytime brightness, addressing data and model limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to address brightness relationships between day and night and lack explicit daytime brightness knowledge, limiting realistic lighting reconstruction.

Method: DiffND uses a data synthesis pipeline for brightness-consistent synthetic scenes and a restoration model combining a pre-trained diffusion model with a brightness perception network.

Result: Experiments show the dataset's utility and the model's superior performance in haze removal and brightness mapping.

Conclusion: DiffND effectively addresses nighttime dehazing challenges by integrating brightness-aware data synthesis and restoration.

Abstract: While nighttime image dehazing has been extensively studied, converting
nighttime hazy images to daytime-equivalent brightness remains largely
unaddressed. Existing methods face two critical limitations: (1) datasets
overlook the brightness relationship between day and night, resulting in the
brightness mapping being inconsistent with the real world during image
synthesis; and (2) models do not explicitly incorporate daytime brightness
knowledge, limiting their ability to reconstruct realistic lighting. To address
these challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)
framework, which excels in both data synthesis and lighting reconstruction. Our
approach starts with a data synthesis pipeline that simulates severe
distortions while enforcing brightness consistency between synthetic and
real-world scenes, providing a strong foundation for learning night-to-day
brightness mapping. Next, we propose a restoration model that integrates a
pre-trained diffusion model guided by a brightness perception network. This
design harnesses the diffusion model's generative ability while adapting it to
nighttime dehazing through brightness-aware optimization. Experiments validate
our dataset's utility and the model's superior performance in joint haze
removal and brightness mapping.

</details>


### [275] [Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather](https://arxiv.org/pdf/2506.02396)
*Longyu Yang, Ping Hu, Shangbo Yuan, Lu Zhang, Jun Liu, Hengtao Shen, Xiaofeng Zhu*

Main category: cs.CV

TL;DR: A novel Geometry-Reflectance Collaboration (GRC) framework improves LiDAR semantic segmentation in adverse weather by separately processing geometric and reflectance features and collaborating them robustly.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with accuracy in adverse weather due to heterogeneous domain shifts in point clouds' geometry and reflectance.

Method: GRC uses a dual-branch architecture to independently process geometry and reflectance features, followed by a multi-level feature collaboration module to filter redundant information.

Result: GRC outperforms previous methods, achieving state-of-the-art results without complex simulation or augmentation.

Conclusion: GRC effectively enhances robustness and generalization in adverse weather by leveraging intrinsic scene information.

Abstract: Existing LiDAR semantic segmentation models often suffer from decreased
accuracy when exposed to adverse weather conditions. Recent methods addressing
this issue focus on enhancing training data through weather simulation or
universal augmentation techniques. However, few works have studied the negative
impacts caused by the heterogeneous domain shifts in the geometric structure
and reflectance intensity of point clouds. In this paper, we delve into this
challenge and address it with a novel Geometry-Reflectance Collaboration (GRC)
framework that explicitly separates feature extraction for geometry and
reflectance. Specifically, GRC employs a dual-branch architecture designed to
independently process geometric and reflectance features initially, thereby
capitalizing on their distinct characteristic. Then, GRC adopts a robust
multi-level feature collaboration module to suppress redundant and unreliable
information from both branches. Consequently, without complex simulation or
augmentation, our method effectively extracts intrinsic information about the
scene while suppressing interference, thus achieving better robustness and
generalization in adverse weather conditions. We demonstrate the effectiveness
of GRC through comprehensive experiments on challenging benchmarks, showing
that our method outperforms previous approaches and establishes new
state-of-the-art results.

</details>


### [276] [Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models](https://arxiv.org/pdf/2506.02405)
*Zhiya Tan, Xin Zhang, Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces 'Modelship Attribution' to trace iterative image manipulations by identifying generative models and reconstructing edit sequences, proposing a novel transformer-based framework (MAT) for effective attribution.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods fail in complex iterative manipulation scenarios, necessitating a systematic approach to trace model contributions and edit sequences.

Method: The authors simulate iterative edits using three generative models (StyleMapGAN, DiffSwap, FacePartsSwap) to create a dataset (83,700 images) and propose MAT, a transformer-based framework, to recognize and attribute model contributions.

Result: MAT outperforms related methods in attributing multi-stage manipulations, validated through extensive experiments and ablation studies.

Conclusion: The proposed MAT framework effectively addresses the challenge of modelship attribution in iterative manipulation, offering a robust solution for real-world applications.

Abstract: As generative techniques become increasingly accessible, authentic visuals
are frequently subjected to iterative alterations by various individuals
employing a variety of tools. Currently, to avoid misinformation and ensure
accountability, a lot of research on detection and attribution is emerging.
Although these methods demonstrate promise in single-stage manipulation
scenarios, they fall short when addressing complex real-world iterative
manipulation. In this paper, we are the first, to the best of our knowledge, to
systematically model this real-world challenge and introduce a novel method to
solve it. We define a task called "Modelship Attribution", which aims to trace
the evolution of manipulated images by identifying the generative models
involved and reconstructing the sequence of edits they performed. To
realistically simulate this scenario, we utilize three generative models,
StyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct
regions of the same image. This process leads to the creation of the first
modelship dataset, comprising 83,700 images (16,740 images*5). Given that later
edits often overwrite the fingerprints of earlier models, the focus shifts from
extracting blended fingerprints to characterizing each model's distinctive
editing patterns. To tackle this challenge, we introduce the modelship
attribution transformer (MAT), a purpose-built framework designed to
effectively recognize and attribute the contributions of various models within
complex, multi-stage manipulation workflows. Through extensive experiments and
comparative analysis with other related methods, our results, including
comprehensive ablation studies, demonstrate that the proposed approach is a
highly effective solution for modelship attribution.

</details>


### [277] [Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology](https://arxiv.org/pdf/2506.02408)
*Wenhao Tang, Rong Qin, Heng Fang, Fengtao Zhou, Hao Chen, Xiang Li, Ming-Ming Cheng*

Main category: cs.CV

TL;DR: The paper introduces ABMILX, a novel MIL method, to address optimization challenges in E2E learning for computational pathology, outperforming SOTA models.


<details>
  <summary>Details</summary>
Motivation: Limitations in current methods (disjoint optimization, high computational demands) motivate revisiting E2E learning for better performance in computational pathology.

Method: Proposes ABMILX, which refines attention via global correlation and multi-head mechanisms, paired with multi-scale random patch sampling for efficiency.

Result: ABMILX with E2E training surpasses SOTA foundation models in benchmarks, achieving high performance with low computational cost (<10 RTX3090 hours).

Conclusion: E2E learning with ABMILX shows promise in computational pathology, urging more research focus in this direction.

Abstract: Pre-trained encoders for offline feature extraction followed by multiple
instance learning (MIL) aggregators have become the dominant paradigm in
computational pathology (CPath), benefiting cancer diagnosis and prognosis.
However, performance limitations arise from the absence of encoder fine-tuning
for downstream tasks and disjoint optimization with MIL. While slide-level
supervised end-to-end (E2E) learning is an intuitive solution to this issue, it
faces challenges such as high computational demands and suboptimal results.
These limitations motivate us to revisit E2E learning. We argue that prior work
neglects inherent E2E optimization challenges, leading to performance
disparities compared to traditional two-stage methods. In this paper, we
pioneer the elucidation of optimization challenge caused by sparse-attention
MIL and propose a novel MIL called ABMILX. It mitigates this problem through
global correlation-based attention refinement and multi-head mechanisms. With
the efficient multi-scale random patch sampling strategy, an E2E trained ResNet
with ABMILX surpasses SOTA foundation models under the two-stage paradigm
across multiple challenging benchmarks, while remaining computationally
efficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath
and calls for greater research focus in this area. The code is
https://github.com/DearCaat/E2E-WSI-ABMILX.

</details>


### [278] [Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models](https://arxiv.org/pdf/2506.02419)
*Nurislam Tursynbek, Hastings Greer, Basar Demir, Marc Niethammer*

Main category: cs.CV

TL;DR: Diffusion models, trained for image generation, can extract meaningful features for medical image registration, outperforming traditional intensity-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional intensity-based similarity losses often fail in challenging medical image registration scenarios, such as when certain anatomies are missing in one image.

Method: The paper proposes using diffusion model features as a similarity measure to guide deformable image registration networks.

Result: The method shows superior performance in multimodal 2D (DXA to X-Ray) and monomodal 3D (brain-extracted to non-brain-extracted MRI) registration tasks.

Conclusion: Diffusion models provide a robust alternative for semantic correspondence in medical image registration, improving accuracy over traditional methods.

Abstract: Diffusion models, while trained for image generation, have emerged as
powerful foundational feature extractors for downstream tasks. We find that
off-the-shelf diffusion models, trained exclusively to generate natural RGB
images, can identify semantically meaningful correspondences in medical images.
Building on this observation, we propose to leverage diffusion model features
as a similarity measure to guide deformable image registration networks. We
show that common intensity-based similarity losses often fail in challenging
scenarios, such as when certain anatomies are visible in one image but absent
in another, leading to anatomically inaccurate alignments. In contrast, our
method identifies true semantic correspondences, aligning meaningful structures
while disregarding those not present across images. We demonstrate superior
performance of our approach on two tasks: multimodal 2D registration (DXA to
X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted
MRI). Code: https://github.com/uncbiag/dgir

</details>


### [279] [Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals](https://arxiv.org/pdf/2506.02433)
*Weiheng Yao, Xuhang Chen, Shuqiang Wang*

Main category: cs.CV

TL;DR: A generative AI framework maps multimodal functional neuroimaging into a unified space, generating data for constrained modalities and underrepresented groups, improving fairness and performance.


<details>
  <summary>Details</summary>
Motivation: High costs and feasibility limitations in acquiring multimodal functional neuroimaging, along with fairness issues in BCI decoding due to underrepresented groups.

Method: Proposes a unified representation framework using generative AI to map and generate data for underrepresented or constrained modalities.

Result: Generates data consistent with real brain activity, improves downstream task performance, and enhances model fairness.

Conclusion: The framework reduces acquisition costs and improves fairness in BCI decoding models.

Abstract: Multimodal functional neuroimaging enables systematic analysis of brain
mechanisms and provides discriminative representations for brain-computer
interface (BCI) decoding. However, its acquisition is constrained by high costs
and feasibility limitations. Moreover, underrepresentation of specific groups
undermines fairness of BCI decoding model. To address these challenges, we
propose a unified representation framework for multimodal functional
neuroimaging via generative artificial intelligence (AI). By mapping multimodal
functional neuroimaging into a unified representation space, the proposed
framework is capable of generating data for acquisition-constrained modalities
and underrepresented groups. Experiments show that the framework can generate
data consistent with real brain activity patterns, provide insights into brain
mechanisms, and improve performance on downstream tasks. More importantly, it
can enhance model fairness by augmenting data for underrepresented groups.
Overall, the framework offers a new paradigm for decreasing the cost of
acquiring multimodal functional neuroimages and enhancing the fairness of BCI
decoding models.

</details>


### [280] [Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/pdf/2506.02439)
*Shuang Li, Jiaxu Leng, Changjiang Kuang, Mingpi Tan, Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes VLD, a framework for Video-based Visible-Infrared Person Re-Identification (VVI-ReID), using language prompts to bridge modality gaps and achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Language provides consistent descriptions across modalities, but generating and using modality-shared video-level language prompts to address modality gaps is challenging.

Method: VLD includes two modules: Invariant-Modality Language Prompting (IMLP) for aligning visual and text features, and Spatial-Temporal Prompting (STP) to enhance IMLP with spatiotemporal information.

Result: VLD achieves state-of-the-art performance on two VVI-ReID benchmarks.

Conclusion: The VLD framework effectively mitigates modality differences and leverages spatiotemporal information for superior VVI-ReID performance.

Abstract: Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to
match pedestrian sequences across modalities by extracting modality-invariant
sequence-level features. As a high-level semantic representation, language
provides a consistent description of pedestrian characteristics in both
infrared and visible modalities. Leveraging the Contrastive Language-Image
Pre-training (CLIP) model to generate video-level language prompts and guide
the learning of modality-invariant sequence-level features is theoretically
feasible. However, the challenge of generating and utilizing modality-shared
video-level language prompts to address modality gaps remains a critical
problem. To address this problem, we propose a simple yet powerful framework,
video-level language-driven VVI-ReID (VLD), which consists of two core modules:
invariant-modality language prompting (IMLP) and spatial-temporal prompting
(STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the
prompt learner to effectively generate modality-shared text prompts and align
them with visual features from different modalities in CLIP's multimodal space,
thereby mitigating modality differences. Additionally, STP models
spatiotemporal information through two submodules, the spatial-temporal hub
(STH) and spatial-temporal aggregation (STA), which further enhance IMLP by
incorporating spatiotemporal information into text prompts. The STH aggregates
and diffuses spatiotemporal information into the [CLS] token of each frame
across the vision transformer (ViT) layers, whereas STA introduces dedicated
identity-level loss and specialized multihead attention to ensure that the STH
focuses on identity-relevant spatiotemporal feature aggregation. The VLD
framework achieves state-of-the-art results on two VVI-ReID benchmarks. The
code will be released at https://github.com/Visuang/VLD.

</details>


### [281] [SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios](https://arxiv.org/pdf/2506.02444)
*Lingwei Dang, Ruizhi Shao, Hongwen Zhang, Wei Min, Yebin Liu, Qingyao Wu*

Main category: cs.CV

TL;DR: A novel framework combines visual priors and dynamic constraints in a synchronized diffusion process to generate Hand-Object Interaction (HOI) videos and motion simultaneously, enhancing consistency and generalization.


<details>
  <summary>Details</summary>
Motivation: Current HOI methods rely on predefined 3D models or sacrifice physical plausibility for visual fidelity, limiting generalization.

Method: Uses tri-modal adaptive modulation for feature alignment and 3D full-attention for dependencies, coupled with a vision-aware 3D interaction diffusion model for closed-loop feedback.

Result: Outperforms state-of-the-art in generating high-fidelity, physically plausible HOI sequences with strong generalization.

Conclusion: The framework eliminates dependencies on predefined models and improves video-motion consistency, demonstrating superior performance in real-world scenarios.

Abstract: Hand-Object Interaction (HOI) generation has significant application
potential. However, current 3D HOI motion generation approaches heavily rely on
predefined 3D object models and lab-captured motion data, limiting
generalization capabilities. Meanwhile, HOI video generation methods prioritize
pixel-level visual fidelity, often sacrificing physical plausibility.
Recognizing that visual appearance and motion patterns share fundamental
physical laws in the real world, we propose a novel framework that combines
visual priors and dynamic constraints within a synchronized diffusion process
to generate the HOI video and motion simultaneously. To integrate the
heterogeneous semantics, appearance, and motion features, our method implements
tri-modal adaptive modulation for feature aligning, coupled with 3D
full-attention for modeling inter- and intra-modal dependencies. Furthermore,
we introduce a vision-aware 3D interaction diffusion model that generates
explicit 3D interaction sequences directly from the synchronized diffusion
outputs, then feeds them back to establish a closed-loop feedback cycle. This
architecture eliminates dependencies on predefined object models or explicit
pose guidance while significantly enhancing video-motion consistency.
Experimental results demonstrate our method's superiority over state-of-the-art
approaches in generating high-fidelity, dynamically plausible HOI sequences,
with notable generalization capabilities in unseen real-world scenarios.
Project page at
\href{https://github.com/Droliven}{https://github.com/Droliven}.

</details>


### [282] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/pdf/2506.02448)
*Baoyu Liang, Qile Su, Shoutai Zhu, Yuchen Liang, Chao Tong*

Main category: cs.CV

TL;DR: The paper introduces VidEvent, a large-scale dataset for video event understanding, featuring detailed event structures and logical relations, along with baseline models for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Understanding complex video events is challenging for AI due to their dynamic and hierarchical nature. The paper aims to address this gap.

Method: Proposes the VidEvent dataset with over 23,000 labeled events from movie recaps, created via meticulous annotation. Provides baseline models for benchmarking.

Result: The dataset and models serve as benchmarks, demonstrating VidEvent's potential to advance video event understanding.

Conclusion: VidEvent encourages future research in video event understanding, with the dataset and resources publicly available.

Abstract: Despite the significant impact of visual events on human cognition,
understanding events in videos remains a challenging task for AI due to their
complex structures, semantic hierarchies, and dynamic evolution. To address
this, we propose the task of video event understanding that extracts event
scripts and makes predictions with these scripts from videos. To support this
task, we introduce VidEvent, a large-scale dataset containing over 23,000
well-labeled events, featuring detailed event structures, broad hierarchies,
and logical relations extracted from movie recap videos. The dataset was
created through a meticulous annotation process, ensuring high-quality and
reliable event data. We also provide comprehensive baseline models offering
detailed descriptions of their architecture and performance metrics. These
models serve as benchmarks for future research, facilitating comparisons and
improvements. Our analysis of VidEvent and the baseline models highlights the
dataset's potential to advance video event understanding and encourages the
exploration of innovative algorithms and models. The dataset and related
resources are publicly available at www.videvent.top.

</details>


### [283] [ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model](https://arxiv.org/pdf/2506.02452)
*Wenshuo Chen, Kuimou Yu, Haozhe Jia, Kaishen Yuan, Bowen Tian, Songning Lai, Hongru Xiao, Erhang Zhang, Lei Wang, Yutao Yue*

Main category: cs.CV

TL;DR: ANT improves text-to-motion generation by adapting semantic conditioning to temporal-frequency demands, enhancing efficiency and fidelity.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models ignore temporal-frequency demands in text-to-motion generation, leading to mismatched semantic conditioning.

Method: Proposes ANT with STA Module, DCFG, and temporal-semantic reweighting to adapt semantic granularity dynamically.

Result: ANT significantly improves performance and achieves state-of-the-art semantic alignment.

Conclusion: ANT addresses temporal-frequency mismatches, enhancing text-to-motion generation.

Abstract: While diffusion models advance text-to-motion generation, their static
semantic conditioning ignores temporal-frequency demands: early denoising
requires structural semantics for motion foundations while later stages need
localized details for text alignment. This mismatch mirrors biological
morphogenesis where developmental phases demand distinct genetic programs.
Inspired by epigenetic regulation governing morphological specialization, we
propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.
ANT orchestrates semantic granularity through: **(i) Semantic Temporally
Adaptive (STA) Module:** Automatically partitions denoising into low-frequency
structural planning and high-frequency refinement via spectral analysis. **(ii)
Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts
conditional to unconditional ratio enhancing efficiency while maintaining
fidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text
influence with phase requirements. Extensive experiments show that ANT can be
applied to various baselines, significantly improving model performance, and
achieving state-of-the-art semantic alignment on StableMoFusion.

</details>


### [284] [PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation](https://arxiv.org/pdf/2506.02453)
*Kunyu Wang, Xueyang Fu, Yunfei Bao, Chengjie Ge, Chengzhi Cao, Wei Zhai, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: PAID leverages the stable pairwise angular structure of pre-trained weights for continual test-time adaptation, outperforming SOTA methods by preserving domain-invariant priors.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods overlook domain-invariant priors in pre-trained weights, particularly the pairwise angular structure, which remains stable across domains.

Method: PAID decomposes weights into magnitude and direction, using a learnable orthogonal matrix to rotate direction while preserving angular structure. Only magnitudes and matrices are updated.

Result: PAID consistently improves performance over SOTA methods on four CTTA benchmarks.

Conclusion: Preserving pairwise angular structure is a simple yet effective principle for CTTA, as demonstrated by PAID.

Abstract: Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained
model to changing environments during inference. Most existing methods focus on
exploiting target data, while overlooking another crucial source of
information, the pre-trained weights, which encode underutilized
domain-invariant priors. This paper takes the geometric attributes of
pre-trained weights as a starting point, systematically analyzing three key
components: magnitude, absolute angle, and pairwise angular structure. We find
that the pairwise angular structure remains stable across diverse corrupted
domains and encodes domain-invariant semantic information, suggesting it should
be preserved during adaptation. Based on this insight, we propose PAID
(Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that
decomposes weight into magnitude and direction, and introduces a learnable
orthogonal matrix via Householder reflections to globally rotate direction
while preserving the pairwise angular structure. During adaptation, only the
magnitudes and the orthogonal matrices are updated. PAID achieves consistent
improvements over recent SOTA methods on four widely used CTTA benchmarks,
demonstrating that preserving pairwise angular structure offers a simple yet
effective principle for CTTA.

</details>


### [285] [ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment](https://arxiv.org/pdf/2506.02459)
*Martin JJ. Bucher, Iro Armeni*

Main category: cs.CV

TL;DR: ReSpace is a generative framework for text-driven 3D indoor scene synthesis and editing using autoregressive language models, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods oversimplify object semantics, lack editing support, or ignore room boundaries, while LLM-based methods lack editing capabilities or spatial reasoning.

Method: Uses a compact structured scene representation with explicit room boundaries, dual-stage training (supervised fine-tuning and preference alignment), and zero-shot LLM for editing.

Result: Surpasses state-of-the-art on object addition and maintains competitive full scene synthesis results.

Conclusion: ReSpace effectively addresses limitations of existing methods by combining rich semantics, explicit boundaries, and flexible editing.

Abstract: Scene synthesis and editing has emerged as a promising direction in computer
graphics. Current trained approaches for 3D indoor scenes either oversimplify
object semantics through one-hot class encodings (e.g., 'chair' or 'table'),
require masked diffusion for editing, ignore room boundaries, or rely on floor
plan renderings that fail to capture complex layouts. In contrast, LLM-based
methods enable richer semantics via natural language (e.g., 'modern studio with
light wood furniture') but do not support editing, remain limited to
rectangular layouts or rely on weak spatial reasoning from implicit world
models. We introduce ReSpace, a generative framework for text-driven 3D indoor
scene synthesis and editing using autoregressive language models. Our approach
features a compact structured scene representation with explicit room
boundaries that frames scene editing as a next-token prediction task. We
leverage a dual-stage training approach combining supervised fine-tuning and
preference alignment, enabling a specially trained language model for object
addition that accounts for user instructions, spatial geometry, object
semantics, and scene-level composition. For scene editing, we employ a
zero-shot LLM to handle object removal and prompts for addition. We further
introduce a novel voxelization-based evaluation that captures fine-grained
geometry beyond 3D bounding boxes. Experimental results surpass
state-of-the-art on object addition while maintaining competitive results on
full scene synthesis.

</details>


### [286] [Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning](https://arxiv.org/pdf/2506.02462)
*Kunyu Wang, Xueyang Fu, Xin Lu, Chengjie Ge, Chengzhi Cao, Wei Zhai, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Proposes an efficient CTTA-OD method via pruning, reducing computational overhead by 12% in FLOPs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Not all learned source features are beneficial; some domain-sensitive channels harm target domain performance.

Method: Sensitivity-guided channel pruning with weighted sparsity regularization and stochastic channel reactivation.

Result: Superior adaptation performance with 12% reduction in FLOPs compared to SOTA.

Conclusion: The method effectively balances efficiency and performance in CTTA-OD.

Abstract: Continual test-time adaptive object detection (CTTA-OD) aims to online adapt
a source pre-trained detector to ever-changing environments during inference
under continuous domain shifts. Most existing CTTA-OD methods prioritize
effectiveness while overlooking computational efficiency, which is crucial for
resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD
method via pruning. Our motivation stems from the observation that not all
learned source features are beneficial; certain domain-sensitive feature
channels can adversely affect target domain performance. Inspired by this, we
introduce a sensitivity-guided channel pruning strategy that quantifies each
channel based on its sensitivity to domain discrepancies at both image and
instance levels. We apply weighted sparsity regularization to selectively
suppress and prune these sensitive channels, focusing adaptation efforts on
invariant ones. Additionally, we introduce a stochastic channel reactivation
mechanism to restore pruned channels, enabling recovery of potentially useful
features and mitigating the risks of early pruning. Extensive experiments on
three benchmarks show that our method achieves superior adaptation performance
while reducing computational overhead by 12% in FLOPs compared to the recent
SOTA method.

</details>


### [287] [HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation](https://arxiv.org/pdf/2506.02472)
*Halil Ismail Helvaci, Justin Philip Huber, Jihye Bae, Sen-ching Samson Cheung*

Main category: cs.CV

TL;DR: HRTR, a single-stage transformer, outperforms state-of-the-art methods for fine-grained, sub-second action detection in stroke rehabilitation.


<details>
  <summary>Details</summary>
Motivation: Precise tracking of patient movements in stroke rehabilitation requires fine-grained, sub-second action detection, which existing methods struggle with.

Method: Proposes HRTR, a High Resolution Temporal Transformer, for single-stage time-localization and classification of actions without multi-stage processing.

Result: HRTR achieves superior performance: ES of 70.1 on StrokeRehab Video, 69.4 on StrokeRehab IMU, and 88.4 on 50Salads.

Conclusion: HRTR is an effective solution for high-resolution, sub-second action detection in stroke rehabilitation, outperforming existing methods.

Abstract: Stroke rehabilitation often demands precise tracking of patient movements to
monitor progress, with complexities of rehabilitation exercises presenting two
critical challenges: fine-grained and sub-second (under one-second) action
detection. In this work, we propose the High Resolution Temporal Transformer
(HRTR), to time-localize and classify high-resolution (fine-grained),
sub-second actions in a single-stage transformer, eliminating the need for
multi-stage methods and post-processing. Without any refinements, HRTR
outperforms state-of-the-art systems on both stroke related and general
datasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on
StrokeRehab IMU, and 88.4 on 50Salads.

</details>


### [288] [Generative Perception of Shape and Material from Differential Motion](https://arxiv.org/pdf/2506.02473)
*Xinran Nicole Han, Ko Nishino, Todd Zickler*

Main category: cs.CV

TL;DR: A diffusion model generates shape-and-material maps from short videos of moving objects, addressing ambiguities in single-image perception.


<details>
  <summary>Details</summary>
Motivation: Humans resolve shape and material ambiguities by observing motion, inspiring a model to mimic this behavior for better visual reasoning.

Method: A conditional denoising-diffusion model trained on synthetic object-motion videos generates disentangled shape-and-material maps.

Result: The model produces diverse predictions for static images and accurate estimates for moving objects, even generalizing to real-world objects.

Conclusion: Continuous motion observations improve generative perception, suggesting applications for embodied systems.

Abstract: Perceiving the shape and material of an object from a single image is
inherently ambiguous, especially when lighting is unknown and unconstrained.
Despite this, humans can often disentangle shape and material, and when they
are uncertain, they often move their head slightly or rotate the object to help
resolve the ambiguities. Inspired by this behavior, we introduce a novel
conditional denoising-diffusion model that generates samples of
shape-and-material maps from a short video of an object undergoing differential
motions. Our parameter-efficient architecture allows training directly in
pixel-space, and it generates many disentangled attributes of an object
simultaneously. Trained on a modest number of synthetic object-motion videos
with supervision on shape and material, the model exhibits compelling emergent
behavior: For static observations, it produces diverse, multimodal predictions
of plausible shape-and-material maps that capture the inherent ambiguities; and
when objects move, the distributions quickly converge to more accurate
explanations. The model also produces high-quality shape-and-material estimates
for less ambiguous, real-world objects. By moving beyond single-view to
continuous motion observations, our work suggests a generative perception
approach for improving visual reasoning in physically-embodied systems.

</details>


### [289] [Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay](https://arxiv.org/pdf/2506.02477)
*Kunyu Wang, Xueyang Fu, Chengzhi Cao, Chengjie Ge, Wei Zhai, Zheng-Jun Zha*

Main category: cs.CV

TL;DR: A new framework for image de-raining uses GANs and knowledge distillation to continuously learn from expanding datasets, improving adaptability and performance in varied rainy conditions.


<details>
  <summary>Details</summary>
Motivation: Existing de-raining methods perform poorly in diverse real-world scenarios due to limited training data. The goal is to enhance adaptability by mimicking the human brain's continuous learning process.

Method: The framework employs GANs to capture new data features (like the hippocampus) and trains the de-raining network with synthesized and existing data. Knowledge distillation replicates neocortical learning.

Result: The framework outperforms state-of-the-art methods, enabling continuous knowledge accumulation across six datasets and better generalization to unseen rainy scenes.

Conclusion: The proposed approach effectively enhances de-raining performance by leveraging continuous learning, inspired by the human brain's complementary learning system.

Abstract: Current image de-raining methods primarily learn from a limited dataset,
leading to inadequate performance in varied real-world rainy conditions. To
tackle this, we introduce a new framework that enables networks to
progressively expand their de-raining knowledge base by tapping into a growing
pool of datasets, significantly boosting their adaptability. Drawing
inspiration from the human brain's ability to continuously absorb and
generalize from ongoing experiences, our approach borrow the mechanism of the
complementary learning system. Specifically, we first deploy Generative
Adversarial Networks (GANs) to capture and retain the unique features of new
data, mirroring the hippocampus's role in learning and memory. Then, the
de-raining network is trained with both existing and GAN-synthesized data,
mimicking the process of hippocampal replay and interleaved learning.
Furthermore, we employ knowledge distillation with the replayed data to
replicate the synergy between the neocortex's activity patterns triggered by
hippocampal replays and the pre-existing neocortical knowledge. This
comprehensive framework empowers the de-raining network to amass knowledge from
various datasets, continually enhancing its performance on previously unseen
rainy scenes. Our testing on three benchmark de-raining networks confirms the
framework's effectiveness. It not only facilitates continuous knowledge
accumulation across six datasets but also surpasses state-of-the-art methods in
generalizing to new real-world scenarios.

</details>


### [290] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/pdf/2506.02488)
*Hongtao Huang, Xiaojun Chang, Lina Yao*

Main category: cs.CV

TL;DR: Flexiffusion is a training-free NAS framework for optimizing diffusion models, achieving significant speedup with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Address computational inefficiency in diffusion models due to iterative multi-step inference and limitations of existing NAS methods.

Method: Decomposes generation into flexible segments with dynamic step types (full, partial, null) and introduces rFID for lightweight evaluation.

Result: Achieves 2× to 5.1× speedup on models like Stable Diffusion with FID degradation under 5%.

Conclusion: Pioneers a resource-efficient NAS paradigm for high-speed diffusion models without quality sacrifice.

Abstract: Diffusion models (DMs) are powerful generative models capable of producing
high-fidelity images but are constrained by high computational costs due to
iterative multi-step inference. While Neural Architecture Search (NAS) can
optimize DMs, existing methods are hindered by retraining requirements,
exponential search complexity from step-wise optimization, and slow evaluation
relying on massive image generation. To address these challenges, we propose
Flexiffusion, a training-free NAS framework that jointly optimizes generation
schedules and model architectures without modifying pre-trained parameters. Our
key insight is to decompose the generation process into flexible segments of
equal length, where each segment dynamically combines three step types: full
(complete computation), partial (cache-reused computation), and null (skipped
computation). This segment-wise search space reduces the candidate pool
exponentially compared to step-wise NAS while preserving architectural
diversity. Further, we introduce relative FID (rFID), a lightweight evaluation
metric for NAS that measures divergence from a teacher model's outputs instead
of ground truth, slashing evaluation time by over $90\%$. In practice,
Flexiffusion achieves at least $2\times$ acceleration across LDMs, Stable
Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\%$,
outperforming prior NAS and caching methods. Notably, it attains $5.1\times$
speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers
a resource-efficient paradigm for searching high-speed DMs without sacrificing
quality.

</details>


### [291] [Co-Evidential Fusion with Information Volume for Medical Image Segmentation](https://arxiv.org/pdf/2506.02492)
*Yuanpeng He, Lijian Li, Tianxiang Zhan, Chi-Man Pun, Wenpin Jiao, Zhi Jin*

Main category: cs.CV

TL;DR: The paper proposes improvements to semi-supervised image segmentation by introducing a pignistic co-evidential fusion strategy and the concept of IVUM for better uncertainty utilization and learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively use multiple sources of voxel-level uncertainty for targeted learning in semi-supervised image segmentation.

Method: 1. A pignistic co-evidential fusion strategy for precise uncertainty measures. 2. Introduction of IVUM to evaluate evidence and implement two evidential learning schemes.

Result: Experiments on four datasets show competitive performance.

Conclusion: The proposed method enhances uncertainty utilization and learning in semi-supervised image segmentation.

Abstract: Although existing semi-supervised image segmentation methods have achieved
good performance, they cannot effectively utilize multiple sources of
voxel-level uncertainty for targeted learning. Therefore, we propose two main
improvements. First, we introduce a novel pignistic co-evidential fusion
strategy using generalized evidential deep learning, extended by traditional
D-S evidence theory, to obtain a more precise uncertainty measure for each
voxel in medical samples. This assists the model in learning mixed labeled
information and establishing semantic associations between labeled and
unlabeled data. Second, we introduce the concept of information volume of mass
function (IVUM) to evaluate the constructed evidence, implementing two
evidential learning schemes. One optimizes evidential deep learning by
combining the information volume of the mass function with original uncertainty
measures. The other integrates the learning pattern based on the co-evidential
fusion strategy, using IVUM to design a new optimization objective. Experiments
on four datasets demonstrate the competitive performance of our method.

</details>


### [292] [Towards In-the-wild 3D Plane Reconstruction from a Single Image](https://arxiv.org/pdf/2506.02493)
*Jiachen Liu, Rui Yu, Sili Chen, Sharon X. Huang, Hengkai Guo*

Main category: cs.CV

TL;DR: ZeroPlane is a Transformer-based model for zero-shot 3D plane reconstruction from a single image, outperforming SOTA methods in accuracy and generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to single datasets (indoor/outdoor), lacking generalizability. ZeroPlane aims to address this by enabling cross-domain 3D plane reconstruction.

Method: The model disentangles plane normal and offset representations, uses an exemplar-guided classification-then-regression approach, and employs advanced backbones and a pixel-geometry-enhanced embedding module.

Result: ZeroPlane significantly outperforms previous methods in reconstruction accuracy and generalizability, especially on diverse, in-the-wild data.

Conclusion: ZeroPlane advances 3D plane reconstruction by enabling zero-shot performance across diverse domains, supported by a large-scale benchmark and novel methodology.

Abstract: 3D plane reconstruction from a single image is a crucial yet challenging
topic in 3D computer vision. Previous state-of-the-art (SOTA) methods have
focused on training their system on a single dataset from either indoor or
outdoor domain, limiting their generalizability across diverse testing data. In
this work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based
model targeting zero-shot 3D plane detection and reconstruction from a single
image, over diverse domains and environments. To enable data-driven models
across multiple domains, we have curated a large-scale planar benchmark,
comprising over 14 datasets and 560,000 high-resolution, dense planar
annotations for diverse indoor and outdoor scenes. To address the challenge of
achieving desirable planar geometry on multi-dataset training, we propose to
disentangle the representation of plane normal and offset, and employ an
exemplar-guided, classification-then-regression paradigm to learn plane and
offset respectively. Additionally, we employ advanced backbones as image
encoder, and present an effective pixel-geometry-enhanced plane embedding
module to further facilitate planar reconstruction. Extensive experiments
across multiple zero-shot evaluation datasets have demonstrated that our
approach significantly outperforms previous methods on both reconstruction
accuracy and generalizability, especially over in-the-wild data. Our code and
data are available at: https://github.com/jcliu0428/ZeroPlane.

</details>


### [293] [LumosFlow: Motion-Guided Long Video Generation](https://arxiv.org/pdf/2506.02497)
*Jiahao Chen, Hangjie Yuan, Yichen Qian, Jingyun Liang, Jiazheng Xing, Pengwei Liu, Weihua Chen, Fan Wang, Bing Su*

Main category: cs.CV

TL;DR: LumosFlow introduces motion guidance for long video generation, using key frames and motion refinement to improve coherence and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in synthesizing temporally coherent and visually compelling long videos, which conventional methods struggle with due to issues like repetition and unnatural transitions.

Method: Uses a hierarchical pipeline with LMTV-DM for key frames, LOF-DM for motion synthesis, and MotionControlNet for refinement, achieving 15x interpolation.

Result: Generates long videos with consistent motion and appearance, outperforming traditional interpolation methods.

Conclusion: LumosFlow effectively enhances long video generation by explicitly guiding motion and refining transitions.

Abstract: Long video generation has gained increasing attention due to its widespread
applications in fields such as entertainment and simulation. Despite advances,
synthesizing temporally coherent and visually compelling long sequences remains
a formidable challenge. Conventional approaches often synthesize long videos by
sequentially generating and concatenating short clips, or generating key frames
and then interpolate the intermediate frames in a hierarchical manner. However,
both of them still remain significant challenges, leading to issues such as
temporal repetition or unnatural transitions. In this paper, we revisit the
hierarchical long video generation pipeline and introduce LumosFlow, a
framework introduce motion guidance explicitly. Specifically, we first employ
the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames
with larger motion intervals, thereby ensuring content diversity in the
generated long videos. Given the complexity of interpolating contextual
transitions between key frames, we further decompose the intermediate frame
interpolation into motion generation and post-hoc refinement. For each pair of
key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes
complex and large-motion optical flows, while MotionControlNet subsequently
refines the warped results to enhance quality and guide intermediate frame
generation. Compared with traditional video frame interpolation, we achieve 15x
interpolation, ensuring reasonable and continuous motion between adjacent
frames. Experiments show that our method can generate long videos with
consistent motion and appearance. Code and models will be made publicly
available upon acceptance. Our project page:
https://jiahaochen1.github.io/LumosFlow/

</details>


### [294] [RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers](https://arxiv.org/pdf/2506.02528)
*Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, Yin Zhang*

Main category: cs.CV

TL;DR: A new method, RelationAdapter, improves visual prompt-based image editing by leveraging source-target pairs and a lightweight module for better generalization and adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-rigid transformations and limited editing intent transfer. The goal is to enhance generalization in visual prompt-driven scenarios.

Method: Proposes RelationAdapter, a lightweight module for Diffusion Transformer models, and Relation252K dataset for evaluation.

Result: RelationAdapter improves editing intent understanding and transfer, boosting generation quality and performance.

Conclusion: The approach advances generalizable visual prompt-based editing, demonstrating effectiveness through experiments.

Abstract: Inspired by the in-context learning mechanism of large language models
(LLMs), a new paradigm of generalizable visual prompt-based image editing is
emerging. Existing single-reference methods typically focus on style or
appearance adjustments and struggle with non-rigid transformations. To address
these limitations, we propose leveraging source-target image pairs to extract
and transfer content-aware editing intent to novel query images. To this end,
we introduce RelationAdapter, a lightweight module that enables Diffusion
Transformer (DiT) based models to effectively capture and apply visual
transformations from minimal examples. We also introduce Relation252K, a
comprehensive dataset comprising 218 diverse editing tasks, to evaluate model
generalization and adaptability in visual prompt-driven scenarios. Experiments
on Relation252K show that RelationAdapter significantly improves the model's
ability to understand and transfer editing intent, leading to notable gains in
generation quality and overall editing performance.

</details>


### [295] [Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels](https://arxiv.org/pdf/2506.02534)
*Sining Chen, Yilei Shi, Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: The paper proposes an ensemble-based pipeline for monocular height estimation using imperfect labels to improve model generalizability, achieving significant error reductions on two datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack generalizability due to scarce perfect labels, limiting large-scale applications. The paper addresses this by incorporating imperfect labels into training.

Method: An ensemble-based pipeline is introduced, leveraging weak supervision through balanced soft losses and ordinal constraints to handle noisy labels, domain shift, and long-tailed height distributions.

Result: The method outperforms baselines, reducing average root mean square errors by 22.94% and 18.62% on DFC23 and GBH datasets, respectively.

Conclusion: The proposed pipeline effectively utilizes imperfect labels, enhancing performance and generalizability, with validated efficacy through ablation studies.

Abstract: Monocular height estimation is considered the most efficient and
cost-effective means of 3D perception in remote sensing, and it has attracted
much attention since the emergence of deep learning. While training neural
networks requires a large amount of data, data with perfect labels are scarce
and only available within developed regions. The trained models therefore lack
generalizability, which limits the potential for large-scale application of
existing methods. We tackle this problem for the first time, by introducing
data with imperfect labels into training pixel-wise height estimation networks,
including labels that are incomplete, inexact, and inaccurate compared to
high-quality labels. We propose an ensemble-based pipeline compatible with any
monocular height estimation network. Taking the challenges of noisy labels,
domain shift, and long-tailed distribution of height values into consideration,
we carefully design the architecture and loss functions to leverage the
information concealed in imperfect labels using weak supervision through
balanced soft losses and ordinal constraints. We conduct extensive experiments
on two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).
The results indicate that the proposed pipeline outperforms baselines by
achieving more balanced performance across various domains, leading to
improvements of average root mean square errors up to 22.94 %, and 18.62 % on
DFC23 and GBH, respectively. The efficacy of each design component is validated
through ablation studies. Code is available at
https://github.com/zhu-xlab/weakim2h.

</details>


### [296] [MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection](https://arxiv.org/pdf/2506.02535)
*Juntong Li, Lingwei Dang, Yukun Su, Yun Hao, Qingxin Xiao, Yongwei Nie, Qingyao Wu*

Main category: cs.CV

TL;DR: The paper proposes a novel Video Anomaly Detection (VAD) framework addressing generalization and semantic limitations via Sparse Feature Filtering Module (SFFM) and Vision-Language Model (VLM) integration.


<details>
  <summary>Details</summary>
Motivation: Existing VAD methods struggle with excessive generalization (reconstructing anomalies) and lack of high-level semantic understanding.

Method: Introduces SFFM with bottleneck filters and MoE architecture for adaptive feature filtering, and integrates VLM for semantic-textual descriptions.

Result: Validated on public datasets, the framework effectively combines semantic, appearance, and motion cues for improved anomaly detection.

Conclusion: The proposed multimodal approach enhances VAD by addressing generalization and semantic gaps, supported by empirical results.

Abstract: Video Anomaly Detection (VAD) methods based on reconstruction or prediction
face two critical challenges: (1) strong generalization capability often
results in accurate reconstruction or prediction of abnormal events, making it
difficult to distinguish normal from abnormal patterns; (2) reliance only on
low-level appearance and motion cues limits their ability to identify
high-level semantic in abnormal events from complex scenes. To address these
limitations, we propose a novel VAD framework with two key innovations. First,
to suppress excessive generalization, we introduce the Sparse Feature Filtering
Module (SFFM) that employs bottleneck filters to dynamically and adaptively
remove abnormal information from features. Unlike traditional memory modules,
it does not need to memorize the normal prototypes across the training dataset.
Further, we design the Mixture of Experts (MoE) architecture for SFFM. Each
expert is responsible for extracting specialized principal features during
running time, and different experts are selectively activated to ensure the
diversity of the learned principal features. Second, to overcome the neglect of
semantics in existing methods, we integrate a Vision-Language Model (VLM) to
generate textual descriptions for video clips, enabling comprehensive joint
modeling of semantic, appearance, and motion cues. Additionally, we enforce
modality consistency through semantic similarity constraints and motion
frame-difference contrastive loss. Extensive experiments on multiple public
datasets validate the effectiveness of our multimodal joint modeling framework
and sparse feature filtering paradigm. Project page at
https://qzfm.github.io/sfn_vad_project_page/.

</details>


### [297] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/pdf/2506.02537)
*Hao Yan, Handong Zheng, Hao Wang, Liang Yin, Xingchen Liu, Zhenbiao Cao, Xinxing Su, Zihao Chen, Jihao Wu, Minghui Liao, Chao Weng, Wei Chen, Yuliang Liu, Xiang Bai*

Main category: cs.CV

TL;DR: The paper addresses the challenge of Abstract Visual Reasoning (AVR) in multimodal large language models (MLLMs) by introducing VisuRiddles, a benchmark, and the Perceptual Riddle Synthesizer (PRS) for data generation, improving model performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: AVR remains a critical challenge for MLLMs due to limitations in perceiving abstract graphics, prompting the need for better benchmarks and training data.

Method: The authors propose VisuRiddles, a benchmark for AVR, and PRS, an automated framework for generating riddles with fine-grained perceptual descriptions to enhance training.

Result: Experiments show fine-grained visual perception is the main bottleneck, and PRS significantly improves MLLM performance on AVR tasks.

Conclusion: The study demonstrates the effectiveness of PRS and VisuRiddles in addressing AVR challenges, with plans to release the code and dataset.

Abstract: Recent strides in multimodal large language models (MLLMs) have significantly
advanced their performance in many reasoning tasks. However, Abstract Visual
Reasoning (AVR) remains a critical challenge, primarily due to limitations in
perceiving abstract graphics. To tackle this issue, we investigate the
bottlenecks in current MLLMs and synthesize training data to improve their
abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,
featuring tasks meticulously constructed to assess models' reasoning capacities
across five core dimensions and two high-level reasoning categories. Second, we
introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for
generating riddles with fine-grained perceptual descriptions. PRS not only
generates valuable training data for abstract graphics but also provides
fine-grained perceptual description, crucially allowing for supervision over
intermediate reasoning stages and thereby improving both training efficacy and
model interpretability. Our extensive experimental results on VisuRiddles
empirically validate that fine-grained visual perception is the principal
bottleneck and our synthesis framework markedly enhances the performance of
contemporary MLLMs on these challenging tasks. Our code and dataset will be
released at https://github.com/yh-hust/VisuRiddles

</details>


### [298] [Probabilistic Online Event Downsampling](https://arxiv.org/pdf/2506.02547)
*Andreu Girbau-Xalabarder, Jun Nagata, Shinichi Sumiyoshi*

Main category: cs.CV

TL;DR: POLED is a probabilistic framework for adaptive event downsampling in event cameras, using an event-importance probability density function (ePDF) to prioritize important events and maintain performance under constraints.


<details>
  <summary>Details</summary>
Motivation: Event cameras' high temporal resolution comes with bandwidth and computational costs. Existing downsampling methods lack adaptability.

Method: POLED models event importance via an ePDF, operates online, and introduces zero-shot downsampling with a contour-preserving ePDF.

Result: Evaluated on four tasks, POLED shows intelligent sampling is key for performance under event-budget limits.

Conclusion: POLED offers adaptable, scene-specific event downsampling, outperforming fixed heuristics.

Abstract: Event cameras capture scene changes asynchronously on a per-pixel basis,
enabling extremely high temporal resolution. However, this advantage comes at
the cost of high bandwidth, memory, and computational demands. To address this,
prior work has explored event downsampling, but most approaches rely on fixed
heuristics or threshold-based strategies, limiting their adaptability. Instead,
we propose a probabilistic framework, POLED, that models event importance
through an event-importance probability density function (ePDF), which can be
arbitrarily defined and adapted to different applications. Our approach
operates in a purely online setting, estimating event importance on-the-fly
from raw event streams, enabling scene-specific adaptation. Additionally, we
introduce zero-shot event downsampling, where downsampled events must remain
usable for models trained on the original event stream, without task-specific
adaptation. We design a contour-preserving ePDF that prioritizes structurally
important events and evaluate our method across four datasets and tasks--object
classification, image interpolation, surface normal estimation, and object
detection--demonstrating that intelligent sampling is crucial for maintaining
performance under event-budget constraints.

</details>


### [299] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/pdf/2506.02550)
*Qiaohui Chu, Haoyu Zhang, Yisen Feng, Meng Liu, Weili Guan, Yaowei Wang, Liqiang Nie*

Main category: cs.CV

TL;DR: A three-stage framework for the Ego4D LTA task, using visual feature extraction, action recognition, and LLM-based anticipation, achieving top performance at CVPR 2025.


<details>
  <summary>Details</summary>
Motivation: To advance long-term action anticipation by leveraging foundation models and improving accuracy with verb-noun co-occurrence.

Method: Three-stage approach: visual feature extraction, Transformer-based verb-noun prediction, and LLM-driven action sequence anticipation.

Result: Achieved first place in the CVPR 2025 challenge, setting a new state-of-the-art.

Conclusion: The framework effectively combines visual and language models for superior long-term action prediction.

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [300] [SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence](https://arxiv.org/pdf/2506.02555)
*Zhitao Zeng, Zhu Zhuo, Xiaojun Jia, Erli Zhang, Junde Wu, Jiaan Zhang, Yuxuan Wang, Chang Han Low, Jian Jiang, Zilong Zheng, Xiaochun Cao, Yutong Ban, Qi Dou, Yang Liu, Yueming Jin*

Main category: cs.CV

TL;DR: SurgVLM is a large vision-language foundation model for surgical intelligence, addressing unique surgical challenges with a comprehensive multimodal database and outperforming mainstream models.


<details>
  <summary>Details</summary>
Motivation: Existing general-purpose vision-language models lack domain-specific supervision and large-scale surgical data, limiting their effectiveness in surgical tasks.

Method: Construct SurgVLM-DB, a large-scale multimodal surgical database, and develop SurgVLM, a vision-language model fine-tuned for 10+ surgical tasks.

Result: SurgVLM outperforms 14 mainstream commercial VLMs on SurgVLM-Bench, demonstrating superior performance in surgical tasks.

Conclusion: SurgVLM bridges the gap in surgical intelligence, offering a versatile and high-performing solution for diverse surgical applications.

Abstract: Foundation models have achieved transformative success across biomedical
domains by enabling holistic understanding of multimodal data. However, their
application in surgery remains underexplored. Surgical intelligence presents
unique challenges - requiring surgical visual perception, temporal analysis,
and reasoning. Existing general-purpose vision-language models fail to address
these needs due to insufficient domain-specific supervision and the lack of a
large-scale high-quality surgical database. To bridge this gap, we propose
SurgVLM, one of the first large vision-language foundation models for surgical
intelligence, where this single universal model can tackle versatile surgical
tasks. To enable this, we construct a large-scale multimodal surgical database,
SurgVLM-DB, comprising over 1.81 million frames with 7.79 million
conversations, spanning more than 16 surgical types and 18 anatomical
structures. We unify and reorganize 23 public datasets across 10 surgical
tasks, followed by standardizing labels and doing hierarchical vision-language
alignment to facilitate comprehensive coverage of gradually finer-grained
surgical tasks, from visual perception, temporal analysis, to high-level
reasoning. Building upon this comprehensive dataset, we propose SurgVLM, which
is built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical
tasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for
method evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets
in surgical domain, covering several crucial downstream tasks. Based on
SurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:
SurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive
comparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,
Qwen2.5-Max).

</details>


### [301] [Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models](https://arxiv.org/pdf/2506.02557)
*Shizhan Gong, Yankai Jiang, Qi Dou, Farzan Farnia*

Main category: cs.CV

TL;DR: A kernel-based method aligns CLIP's visual representation with DINOv2 to enhance fine-grained perception while maintaining text compatibility, improving downstream MLLM performance.


<details>
  <summary>Details</summary>
Motivation: CLIP's limited fine-grained perception hinders downstream MLLMs, while DINOv2 excels in detail capture. Bridging these models aims to combine strengths.

Method: Proposes a kernel-based alignment of CLIP's visual representation with DINOv2, optimized for efficiency, while retaining text compatibility.

Result: Improved zero-shot object recognition, fine-grained reasoning, and localization. Enhanced downstream MLLM performance.

Conclusion: The alignment method successfully combines CLIP's text compatibility with DINOv2's fine-grained perception, benefiting MLLMs.

Abstract: Vision-language models, such as CLIP, have achieved significant success in
aligning visual and textual representations, becoming essential components of
many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.
However, numerous studies have identified CLIP's limited fine-grained
perception as a critical drawback, leading to substantial failures in
downstream MLLMs. In contrast, vision-centric foundation models like DINOv2
demonstrate remarkable capabilities in capturing fine details from images. In
this work, we propose a novel kernel-based method to align CLIP's visual
representation with that of DINOv2, ensuring that the resulting embeddings
maintain compatibility with text embeddings while enhancing perceptual
capabilities. Our alignment objective is designed for efficient stochastic
optimization. Following this image-only alignment fine-tuning, the visual
encoder retains compatibility with the frozen text encoder and exhibits
significant improvements in zero-shot object recognition, fine-grained spatial
reasoning, and localization. By integrating the aligned visual encoder,
downstream MLLMs also demonstrate enhanced performance.

</details>


### [302] [DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing](https://arxiv.org/pdf/2506.02560)
*Zixiang Li, Haoyu Wang, Wei Wang, Chuangchuang Tan, Yunchao Wei, Yao Zhao*

Main category: cs.CV

TL;DR: Dual-Conditional Inversion (DCI) improves diffusion model inversion by jointly conditioning on source prompts and reference images, balancing reconstruction accuracy and editing flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing inversion approaches struggle with trade-offs between reconstruction accuracy and editing flexibility due to challenges in maintaining semantic alignment and structural consistency.

Method: DCI formulates inversion as a dual-condition fixed-point optimization problem, minimizing latent noise gap and reconstruction error under joint guidance.

Result: DCI achieves state-of-the-art performance in editing tasks, improving reconstruction quality and editing precision, while also performing well in reconstruction tasks.

Conclusion: DCI provides a robust and generalizable solution for diffusion model inversion, advancing both reconstruction and editing capabilities.

Abstract: Diffusion models have achieved remarkable success in image generation and
editing tasks. Inversion within these models aims to recover the latent noise
representation for a real or generated image, enabling reconstruction, editing,
and other downstream tasks. However, to date, most inversion approaches suffer
from an intrinsic trade-off between reconstruction accuracy and editing
flexibility. This limitation arises from the difficulty of maintaining both
semantic alignment and structural consistency during the inversion process. In
this work, we introduce Dual-Conditional Inversion (DCI), a novel framework
that jointly conditions on the source prompt and reference image to guide the
inversion process. Specifically, DCI formulates the inversion process as a
dual-condition fixed-point optimization problem, minimizing both the latent
noise gap and the reconstruction error under the joint guidance. This design
anchors the inversion trajectory in both semantic and visual space, leading to
more accurate and editable latent representations. Our novel setup brings new
understanding to the inversion process. Extensive experiments demonstrate that
DCI achieves state-of-the-art performance across multiple editing tasks,
significantly improving both reconstruction quality and editing precision.
Furthermore, we also demonstrate that our method achieves strong results in
reconstruction tasks, implying a degree of robustness and generalizability
approaching the ultimate goal of the inversion process.

</details>


### [303] [Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories](https://arxiv.org/pdf/2506.02571)
*Abhishek Vivekanandan, Christian Hubschneider, J. Marius Zöllner*

Main category: cs.CV

TL;DR: A novel Transformer-based framework learns compact, interpretable trajectory embeddings using contrastive triplet loss, outperforming FFT-based methods in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for trajectory retrieval rely on inefficient heuristics or opaque representations, lacking interpretability and controllability.

Method: A Transformer encoder is trained with contrastive triplet loss, evaluating Cosine and FFT-based similarity metrics for trajectory embeddings.

Result: Cosine similarity outperforms FFT-based methods in clustering trajectories by semantic and directional attributes, even with low-dimensional embeddings.

Conclusion: The framework provides efficient, interpretable trajectory representations, enhancing motion forecasting pipelines.

Abstract: The ability to retrieve semantically and directionally similar short-range
trajectories with both accuracy and efficiency is foundational for downstream
applications such as motion forecasting and autonomous navigation. However,
prevailing approaches often depend on computationally intensive heuristics or
latent anchor representations that lack interpretability and controllability.
In this work, we propose a novel framework for learning fixed-dimensional
embeddings for short trajectories by leveraging a Transformer encoder trained
with a contrastive triplet loss that emphasize the importance of discriminative
feature spaces for trajectory data. We analyze the influence of Cosine and
FFT-based similarity metrics within the contrastive learning paradigm, with a
focus on capturing the nuanced directional intent that characterizes short-term
maneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates
that embeddings shaped by Cosine similarity objectives yield superior
clustering of trajectories by both semantic and directional attributes,
outperforming FFT-based baselines in retrieval tasks. Notably, we show that
compact Transformer architectures, even with low-dimensional embeddings (e.g.,
16 dimensions, but qualitatively down to 4), achieve a compelling balance
between retrieval performance (minADE, minFDE) and computational overhead,
aligning with the growing demand for scalable and interpretable motion priors
in real-time systems. The resulting embeddings provide a compact, semantically
meaningful, and efficient representation of trajectory data, offering a robust
alternative to heuristic similarity measures and paving the way for more
transparent and controllable motion forecasting pipelines.

</details>


### [304] [BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations](https://arxiv.org/pdf/2506.02587)
*Weiduo Yuan, Jerry Li, Justin Yue, Divyank Shah, Konstantinos Karydis, Hang Qiu*

Main category: cs.CV

TL;DR: BEVCALIB is a novel LiDAR-camera calibration method using BEV features, outperforming baselines significantly in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional calibration methods are limited by controlled environments and cannot adapt to dynamic changes during movement.

Method: Extracts and fuses camera and LiDAR BEV features, using a feature selector for efficient training.

Result: Outperforms baselines by 47.08%-82.32% on KITTI and 68.29%-78.17% on NuScenes in translation and rotation accuracy.

Conclusion: BEVCALIB sets a new state-of-the-art, improving reproducibility by an order of magnitude.

Abstract: Accurate LiDAR-camera calibration is fundamental to fusing multi-modal
perception in autonomous driving and robotic systems. Traditional calibration
methods require extensive data collection in controlled environments and cannot
compensate for the transformation changes during the vehicle/robot movement. In
this paper, we propose the first model that uses bird's-eye view (BEV) features
to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve
this, we extract camera BEV features and LiDAR BEV features separately and fuse
them into a shared BEV feature space. To fully utilize the geometric
information from the BEV feature, we introduce a novel feature selector to
filter the most important features in the transformation decoder, which reduces
memory consumption and enables efficient training. Extensive evaluations on
KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a
new state of the art. Under various noise conditions, BEVCALIB outperforms the
best baseline in the literature by an average of (47.08%, 82.32%) on KITTI
dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,
rotation), respectively. In the open-source domain, it improves the best
reproducible baseline by one order of magnitude. Our code and demo results are
available at https://cisl.ucr.edu/BEVCalib.

</details>


### [305] [One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation](https://arxiv.org/pdf/2506.02605)
*Xue Wu, Jingwei Xin, Zhijun Tu, Jie Hu, Jie Li, Nannan Wang, Xinbo Gao*

Main category: cs.CV

TL;DR: VPD-SR is a one-step diffusion distillation framework for image super-resolution, enhancing semantic alignment and perceptual quality via explicit semantic supervision and high-frequency perception loss.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based SR methods suffer from slow inference and poor semantic alignment, leading to suboptimal perceptual quality.

Method: VPD-SR combines Explicit Semantic-aware Supervision (ESS) and High-Frequency Perception (HFP) loss, leveraging CLIP for semantic consistency and adversarial training for authenticity.

Result: VPD-SR outperforms state-of-the-art methods and the teacher model with one-step sampling on synthetic and real-world datasets.

Conclusion: VPD-SR effectively addresses semantic and perceptual challenges in SR, offering a fast and high-quality solution.

Abstract: Diffusion-based models have been widely used in various visual generation
tasks, showing promising results in image super-resolution (SR), while
typically being limited by dozens or even hundreds of sampling steps. Although
existing methods aim to accelerate the inference speed of multi-step
diffusion-based SR methods through knowledge distillation, their generated
images exhibit insufficient semantic alignment with real images, resulting in
suboptimal perceptual quality reconstruction, specifically reflected in the
CLIPIQA score. These methods still have many challenges in perceptual quality
and semantic fidelity. Based on the challenges, we propose VPD-SR, a novel
visual perception diffusion distillation framework specifically designed for
SR, aiming to construct an effective and efficient one-step SR model.
Specifically, VPD-SR consists of two components: Explicit Semantic-aware
Supervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS
leverages the powerful visual perceptual understanding capabilities of the CLIP
model to extract explicit semantic supervision, thereby enhancing semantic
consistency. Then, Considering that high-frequency information contributes to
the visual perception quality of images, in addition to the vanilla
distillation loss, the HFP loss guides the student model to restore the missing
high-frequency details in degraded images that are critical for enhancing
perceptual quality. Lastly, we expand VPD-SR in adversarial training manner to
further enhance the authenticity of the generated content. Extensive
experiments conducted on synthetic and real-world datasets demonstrate that the
proposed VPD-SR achieves superior performance compared to both previous
state-of-the-art methods and the teacher model with just one-step sampling.

</details>


### [306] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/pdf/2506.02614)
*Guohang Zhuang, Weixi Song, Jinyang Huang, Chenwei Yang, Yan Lu*

Main category: cs.CV

TL;DR: A deep learning-based Space Debris Tracking Network (SDT-Net) is proposed for accurate debris tracking, supported by a large-scale dataset (SDTD). The model achieves strong performance on real-world data.


<details>
  <summary>Details</summary>
Motivation: Space debris poses a significant threat, but existing methods struggle with complex backgrounds and dense debris. A more efficient solution is needed.

Method: Proposes SDT-Net, a deep learning model for debris tracking, and introduces SDTD, a large-scale dataset created via observation-based simulation.

Result: SDT-Net achieves a MOTA score of 70.6% on real-world data from the Antarctic Station, demonstrating high accuracy and transferability.

Conclusion: SDT-Net and SDTD offer an effective solution for space debris tracking, with potential for real-world applications.

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [307] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/pdf/2506.02615)
*Safaa Abdullahi Moallim Mohamud, Minjin Baek, Dong Seog Han*

Main category: cs.CV

TL;DR: A hierarchical QA approach for autonomous vehicles balances cost-efficiency and detailed scene understanding by fine-tuning a compact VLM and dynamically navigating a question tree.


<details>
  <summary>Details</summary>
Motivation: To achieve detailed visual interpretation in autonomous driving while maintaining cost-efficiency and low latency.

Method: Fine-tunes a compact VLM on a custom dataset, uses a hierarchical QA strategy to decompose tasks, and dynamically skips questions to optimize inference time.

Result: Competes with state-of-the-art methods like GPT-4o in accuracy while reducing inference time, validated via GPT reference-free scoring and real-time deployment.

Conclusion: The approach effectively captures key driving details with minimal latency, proving practical for real-world autonomous vehicle applications.

Abstract: In this paper, we present a hierarchical question-answering (QA) approach for
scene understanding in autonomous vehicles, balancing cost-efficiency with
detailed visual interpretation. The method fine-tunes a compact vision-language
model (VLM) on a custom dataset specific to the geographical area in which the
vehicle operates to capture key driving-related visual elements. At the
inference stage, the hierarchical QA strategy decomposes the scene
understanding task into high-level and detailed sub-questions. Instead of
generating lengthy descriptions, the VLM navigates a structured question tree,
where answering high-level questions (e.g., "Is it possible for the ego vehicle
to turn left at the intersection?") triggers more detailed sub-questions (e.g.,
"Is there a vehicle approaching the intersection from the opposite
direction?"). To optimize inference time, questions are dynamically skipped
based on previous answers, minimizing computational overhead. The extracted
answers are then synthesized using handcrafted templates to ensure coherent,
contextually accurate scene descriptions. We evaluate the proposed approach on
the custom dataset using GPT reference-free scoring, demonstrating its
competitiveness with state-of-the-art methods like GPT-4o in capturing key
scene details while achieving significantly lower inference time. Moreover,
qualitative results from real-time deployment highlight the proposed approach's
capacity to capture key driving elements with minimal latency.

</details>


### [308] [Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies](https://arxiv.org/pdf/2506.02626)
*Ada Sawilska, Mateusz Trokielewicz*

Main category: cs.CV

TL;DR: The paper reviews iris image synthesis methods to address challenges in collecting real biometric datasets, covering techniques like GANs, VAEs, and diffusion models, while also discussing risks and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulties of acquiring large, diverse biometric datasets from living individuals, which are crucial for biometric method development.

Method: Examines traditional image processing, GANs, VAEs, and diffusion models for iris image synthesis, comparing their potential and fidelity.

Result: Provides insights into the effectiveness of each synthesis method and highlights risks of biometric feature leakage from training data.

Conclusion: Suggests that while generative methods can replace real datasets, strategies to prevent data leakage must be implemented.

Abstract: This paper presents a comprehensive overview of iris image synthesis methods,
which can alleviate the issues associated with gathering large, diverse
datasets of biometric data from living individuals, which are considered
pivotal for biometric methods development. These methods for synthesizing iris
data range from traditional, hand crafted image processing-based techniques,
through various iterations of GAN-based image generators, variational
autoencoders (VAEs), as well as diffusion models. The potential and fidelity in
iris image generation of each method is discussed and examples of inferred
predictions are provided. Furthermore, the risks of individual biometric
features leakage from the training sets are considered, together with possible
strategies for preventing them, which have to be implemented should these
generative methods be considered a valid replacement of real-world biometric
datasets.

</details>


### [309] [ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration](https://arxiv.org/pdf/2506.02633)
*Cheng Yang, Lijing Liang, Zhixun Su*

Main category: cs.CV

TL;DR: ControlMambaIR integrates Mamba and diffusion models for image restoration, excelling in perceptual quality while maintaining distortion metrics.


<details>
  <summary>Details</summary>
Motivation: Address perceptual challenges in image deraining, deblurring, and denoising by refining conditional control in the generation process.

Method: Combines Mamba network architecture with diffusion models for improved control and optimization.

Result: Outperforms existing methods in perceptual metrics (LPIPS, FID) and matches distortion metrics (PSNR, SSIM).

Conclusion: ControlMambaIR is effective and adaptable, with Mamba proving superior as a conditional control network.

Abstract: This paper proposes ControlMambaIR, a novel image restoration method designed
to address perceptual challenges in image deraining, deblurring, and denoising
tasks. By integrating the Mamba network architecture with the diffusion model,
the condition network achieves refined conditional control, thereby enhancing
the control and optimization of the image generation process. To evaluate the
robustness and generalization capability of our method across various image
degradation conditions, extensive experiments were conducted on several
benchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results
demonstrate that our proposed approach consistently surpasses existing methods
in perceptual quality metrics, such as LPIPS and FID, while maintaining
comparable performance in image distortion metrics, including PSNR and SSIM,
highlighting its effectiveness and adaptability. Notably, ablation experiments
reveal that directly noise prediction in the diffusion process achieves better
performance, effectively balancing noise suppression and detail preservation.
Furthermore, the findings indicate that the Mamba architecture is particularly
well-suited as a conditional control network for diffusion models,
outperforming both CNN- and Attention-based approaches in this context.
Overall, these results highlight the flexibility and effectiveness of
ControlMambaIR in addressing a range of image restoration perceptual
challenges.

</details>


### [310] [Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet](https://arxiv.org/pdf/2506.02671)
*Xiao Chen, Jiazhen Huang, Qinting Jiang, Fanding Huang, Xianghua Fu, Jingyan Jiang, Zhi Wang*

Main category: cs.CV

TL;DR: SAIL introduces a lightweight adapter-based TTA framework for efficient and scalable adaptation of VLMs, reducing computational costs and avoiding catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods for VLMs are computationally expensive and poorly scalable due to sample-wise adaptation and costly auxiliary designs.

Method: SAIL uses a frozen VLM and a learnable AdaptNet with confidence-based interpolation and batch-wise processing. A gradient-aware reset strategy (GDI) prevents catastrophic forgetting.

Result: SAIL achieves state-of-the-art performance with low computational costs across diverse benchmarks.

Conclusion: SAIL is effective, efficient, and scalable for real-world deployment, offering a practical solution for TTA in VLMs.

Abstract: Test-time adaptation (TTA) has emerged as a critical technique for enhancing
the generalization capability of vision-language models (VLMs) during
inference. However, existing approaches often incur substantial computational
costs and exhibit poor scalability, primarily due to sample-wise adaptation
granularity and reliance on costly auxiliary designs such as data augmentation.
To address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel
adapter-based TTA framework that leverages a lightweight, learnable AdaptNet to
enable efficient and scalable model adaptation. As SAIL's core, a frozen
pre-trained VLM collaborates with AdaptNet through a confidence-based
interpolation weight, generating robust predictions during inference. These
predictions serve as self-supervised targets to align AdaptNet's outputs
through efficient batch-wise processing, dramatically reducing computational
costs without modifying the VLM or requiring memory caches. To mitigate
catastrophic forgetting during continual adaptation, we propose a
gradient-aware reset strategy driven by a gradient drift indicator (GDI), which
dynamically detects domain transitions and strategically resets AdaptNet for
stable adaptation. Extensive experiments across diverse benchmarks on two
scenarios demonstrate that SAIL achieves state-of-the-art performance while
maintaining low computational costs. These results highlight SAIL's
effectiveness, efficiency and scalability for real-world deployment. The code
will be released upon acceptance.

</details>


### [311] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/pdf/2506.02677)
*Jintao Tong, Yixiong Zou, Guangyao Chen, Yuhua Li, Ruixuan Li*

Main category: cs.CV

TL;DR: The paper addresses the entanglement problem in Cross-Domain Few-Shot Segmentation (CD-FSS) by decomposing ViT components and learning to weigh comparisons, improving performance.


<details>
  <summary>Details</summary>
Motivation: Current CD-FSS methods suffer from an entanglement problem, binding source-domain patterns and hindering transfer.

Method: Decomposes ViT structure, identifies cross-comparison issues, and proposes learning to weigh comparisons for disentanglement.

Result: Outperforms state-of-the-art by 1.92% (1-shot) and 1.88% (5-shot) in average accuracy.

Conclusion: The proposed method effectively addresses entanglement, enhancing generalization and finetuning for CD-FSS.

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a
source-domain dataset to unseen target-domain datasets with limited
annotations. Current methods typically compare the distance between training
and testing samples for mask prediction. However, we find an entanglement
problem exists in this widely adopted method, which tends to bind sourcedomain
patterns together and make each of them hard to transfer. In this paper, we aim
to address this problem for the CD-FSS task. We first find a natural
decomposition of the ViT structure, based on which we delve into the
entanglement problem for an interpretation. We find the decomposed ViT
components are crossly compared between images in distance calculation, where
the rational comparisons are entangled with those meaningless ones by their
equal importance, leading to the entanglement problem. Based on this
interpretation, we further propose to address the entanglement problem by
learning to weigh for all comparisons of ViT components, which learn
disentangled features and re-compose them for the CD-FSS task, benefiting both
the generalization and finetuning. Experiments show that our model outperforms
the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under
1-shot and 5-shot settings, respectively.

</details>


### [312] [Towards Geometry Problem Solving in the Large Model Era: A Survey](https://arxiv.org/pdf/2506.02690)
*Yurui Zhao, Xiang Wang, Jiahong Liu, Irwin King, Zhitao Huang*

Main category: cs.CV

TL;DR: This survey synthesizes advancements in Geometry Problem Solving (GPS), focusing on benchmarks, parsing, and reasoning, while proposing a unified paradigm and future research directions.


<details>
  <summary>Details</summary>
Motivation: GPS is critical for AI applications but remains challenging due to spatial and logical demands. The field lacks cohesion in methodologies and benchmarks.

Method: The survey analyzes GPS through three dimensions: benchmark construction, textual/diagrammatic parsing, and reasoning paradigms. It also proposes a unified analytical framework.

Result: Highlights current advancements, limitations, and opportunities in GPS, such as automated benchmark generation and neuro-symbolic integration.

Conclusion: The paper aims to guide future research toward human-level geometric reasoning by addressing fragmentation and proposing innovative solutions.

Abstract: Geometry problem solving (GPS) represents a critical frontier in artificial
intelligence, with profound applications in education, computer-aided design,
and computational graphics. Despite its significance, automating GPS remains
challenging due to the dual demands of spatial understanding and rigorous
logical reasoning. Recent advances in large models have enabled notable
breakthroughs, particularly for SAT-level problems, yet the field remains
fragmented across methodologies, benchmarks, and evaluation frameworks. This
survey systematically synthesizes GPS advancements through three core
dimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,
and (3) reasoning paradigms. We further propose a unified analytical paradigm,
assess current limitations, and identify emerging opportunities to guide future
research toward human-level geometric reasoning, including automated benchmark
generation and interpretable neuro-symbolic integration.

</details>


### [313] [Large-scale Self-supervised Video Foundation Model for Intelligent Surgery](https://arxiv.org/pdf/2506.02692)
*Shu Yang, Fengtao Zhou, Leon Mayer, Fuxiang Huang, Yiliang Chen, Yihui Wang, Sunan He, Yuxiang Nie, Xi Wang, Ömer Sümer, Yueming Jin, Huihui Sun, Shuchang Xu, Alex Qinyang Liu, Zheng Li, Jing Qin, Jeremy YuenChun Teoh, Lena Maier-Hein, Hao Chen*

Main category: cs.CV

TL;DR: SurgVISTA is a video-level surgical pre-training framework for joint spatiotemporal representation learning, outperforming existing models in surgical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AI-driven approaches lack explicit temporal modeling, limiting dynamic surgical context capture. SurgVISTA aims to address this gap.

Method: Proposes SurgVISTA, a reconstruction-based pre-training method using a large-scale surgical video dataset (3,650 videos, 3.55M frames). Incorporates image-level knowledge distillation.

Result: Outperforms natural- and surgical-domain pre-trained models across 13 video-level datasets in six surgical procedures.

Conclusion: SurgVISTA shows strong potential to advance intelligent surgical systems by improving spatiotemporal understanding.

Abstract: Computer-Assisted Intervention (CAI) has the potential to revolutionize
modern surgery, with surgical scene understanding serving as a critical
component in supporting decision-making, improving procedural efficacy, and
ensuring intraoperative safety. While existing AI-driven approaches alleviate
annotation burdens via self-supervised spatial representation learning, their
lack of explicit temporal modeling during pre-training fundamentally restricts
the capture of dynamic surgical contexts, resulting in incomplete
spatiotemporal understanding. In this work, we introduce the first video-level
surgical pre-training framework that enables joint spatiotemporal
representation learning from large-scale surgical video data. To achieve this,
we constructed a large-scale surgical video dataset comprising 3,650 videos and
approximately 3.55 million frames, spanning more than 20 surgical procedures
and over 10 anatomical structures. Building upon this dataset, we propose
SurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a
reconstruction-based pre-training method that captures intricate spatial
structures and temporal dynamics through joint spatiotemporal modeling.
Additionally, SurgVISTA incorporates image-level knowledge distillation guided
by a surgery-specific expert to enhance the learning of fine-grained anatomical
and semantic features. To validate its effectiveness, we established a
comprehensive benchmark comprising 13 video-level datasets spanning six
surgical procedures across four tasks. Extensive experiments demonstrate that
SurgVISTA consistently outperforms both natural- and surgical-domain
pre-trained models, demonstrating strong potential to advance intelligent
surgical systems in clinically meaningful scenarios.

</details>


### [314] [FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition](https://arxiv.org/pdf/2506.02695)
*Linquan Wu, Tianxiang Jiang, Wenhao Duan, Yini Fang, Jacky Keung*

Main category: cs.CV

TL;DR: FaceSleuth introduces a dual-stream architecture for micro-expression recognition, enhancing motion along the vertical axis and localizing signals with attention mechanisms, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition requires amplifying subtle facial motions while suppressing identity-specific features.

Method: FaceSleuth uses a Continuously Vertical Attention (CVA) block, Facial Position Focalizer, and Action-Unit embeddings. A Single-Orientation Attention (SOA) module learns optimal pooling direction.

Result: Achieves 95.1% accuracy on CASME II, 87.1% on SAMM, and 92.9% on MMEW, outperforming prior methods.

Conclusion: Vertical attention bias is most discriminative for MER, and FaceSleuth sets a new state of the art.

Abstract: Micro-expression recognition (MER) demands models that can amplify
millisecond-level, low-amplitude facial motions while suppressing
identity-specific appearance. We introduce FaceSleuth, a dual-stream
architecture that (1) enhances motion along the empirically dominant vertical
axix through a Continuously Vertical Attention (CVA) block, (2) localises the
resulting signals with a Facial Position Focalizer built on hierarchical
cross-window attention, and (3) steers feature learning toward physiologically
meaningful regions via lightweight Action-Unit embeddings. To examine whether
the hand-chosen vertical axis is indeed optimal, we further propose a
Single-Orientation Attention (SOA) module that learns its own pooling direction
end-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses
to CVA when the learned angle converges to {\Pi}/2. In practice, SOA reliably
drifts to 88{\deg}, confirming the effectiveness of the vertical prior while
delivering consistent gains. On three standard MER benchmarks, FaceSleuth with
CVA already surpasses previous state-of-the-art methods; plugging in SOA lifts
accuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840
on SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.
These results establish a new state of the art and, for the first time, provide
empirical evidence that the vertical attention bias is the most discriminative
orientation for MER.

</details>


### [315] [LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation](https://arxiv.org/pdf/2506.02697)
*Yuxuan Wu, Le Wang, Sanping Zhou, Mengnan Liu, Gang Hua, Haoxiang Li*

Main category: cs.CV

TL;DR: The paper introduces a method for controllable layout generation using retrieval-based templates and reference-guided generation, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To improve layout generation under given constraints by leveraging retrieved templates and reference-guided processes, addressing limitations of current models.

Method: Retrieves layout templates matching conditions, uses them to guide denoising or flow-based transport, and employs condition-modulated attention for selective knowledge absorption.

Result: Produces high-quality layouts meeting conditions and surpasses state-of-the-art models.

Conclusion: The proposed approach effectively leverages retrieved templates for better layout generation, demonstrating superior performance.

Abstract: Controllable layout generation aims to create plausible visual arrangements
of element bounding boxes within a graphic design according to certain optional
constraints, such as the type or position of a specific component. While recent
diffusion or flow-matching models have achieved considerable advances in
multifarious conditional generation tasks, there remains considerable room for
generating optimal arrangements under given conditions. In this work, we
propose to carry out layout generation through retrieving by conditions and
reference-guided generation. Specifically, we retrieve appropriate layout
templates according to given conditions as references. The references are then
utilized to guide the denoising or flow-based transport process. By retrieving
layouts compatible with the given conditions, we can uncover the potential
information not explicitly provided in the given condition. Such an approach
offers more effective guidance to the model during the generation process, in
contrast to previous models that feed the condition to the model and let the
model infer the unprovided layout attributes directly. Meanwhile, we design a
condition-modulated attention that selectively absorbs retrieval knowledge,
adapting to the difference between retrieved templates and given conditions.
Extensive experiment results show that our method successfully produces
high-quality layouts that meet the given conditions and outperforms existing
state-of-the-art models. Code will be released upon acceptance.

</details>


### [316] [Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences](https://arxiv.org/pdf/2506.02698)
*Yunhong Lu, Qichao Wang, Hengyuan Cao, Xiaoyin Xu, Min Zhang*

Main category: cs.CV

TL;DR: SmPO-Diffusion improves Direct Preference Optimization (DPO) by modeling granular preference distributions and providing a numerical upper bound for diffusion optimization, achieving better alignment and performance.


<details>
  <summary>Details</summary>
Motivation: Existing DPO methods neglect individual preference variability, leading to misalignment and excessive optimization. SmPO-Diffusion addresses this by modeling preference distributions more accurately.

Method: Introduces smoothed preference distributions, a reward model for simulating human preferences, and preference likelihood averaging to refine the DPO loss. Uses inversion for trajectory preference distribution simulation.

Result: SmPO-Diffusion outperforms baselines in preference evaluation with lower training costs, mitigating excessive optimization and misalignment issues.

Conclusion: SmPO-Diffusion offers a robust solution for aligning T2I models with human preferences, achieving state-of-the-art results through granular preference modeling.

Abstract: Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation
models with human preferences using pairwise preference data. Although
substantial resources are expended in collecting and labeling datasets, a
critical aspect is often neglected: \textit{preferences vary across individuals
and should be represented with more granularity.} To address this, we propose
SmPO-Diffusion, a novel method for modeling preference distributions to improve
the DPO objective, along with a numerical upper bound estimation for the
diffusion optimization objective. First, we introduce a smoothed preference
distribution to replace the original binary distribution. We employ a reward
model to simulate human preferences and apply preference likelihood averaging
to improve the DPO loss, such that the loss function approaches zero when
preferences are similar. Furthermore, we utilize an inversion technique to
simulate the trajectory preference distribution of the diffusion model,
enabling more accurate alignment with the optimization objective. Our approach
effectively mitigates issues of excessive optimization and objective
misalignment present in existing methods through straightforward modifications.
Our SmPO-Diffusion achieves state-of-the-art performance in preference
evaluation, outperforming baselines across metrics with lower training costs.
The project page is https://jaydenlyh.github.io/SmPO-project-page/.

</details>


### [317] [ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings](https://arxiv.org/pdf/2506.02702)
*Tibor Kubík, François Guibault, Michal Španěl, Hervé Lombaert*

Main category: cs.CV

TL;DR: ToothForge is a spectral method for generating 3D teeth, addressing dataset sparsity by using synchronized frequential embeddings to stabilize decomposed harmonics and improve reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: The sparsity of dental shape datasets and instability in decomposed harmonics limit the generation of high-resolution 3D teeth. ToothForge aims to overcome these challenges.

Method: The method operates in the spectral domain, aligning spectra to a common basis and modeling the latent manifold on synchronized frequential embeddings. This eliminates decomposition biases and removes connectivity constraints.

Result: ToothForge achieves higher reconstruction quality than models using unaligned embeddings and enables additional applications like shape compression and interpolation.

Conclusion: ToothForge advances spectral analysis in dentistry and broader medical applications by reducing mesh structure restrictions and improving shape generation.

Abstract: We introduce ToothForge, a spectral approach for automatically generating
novel 3D teeth, effectively addressing the sparsity of dental shape datasets.
By operating in the spectral domain, our method enables compact machine
learning modeling, allowing the generation of high-resolution tooth meshes in
milliseconds. However, generating shape spectra comes with the instability of
the decomposed harmonics. To address this, we propose modeling the latent
manifold on synchronized frequential embeddings. Spectra of all data samples
are aligned to a common basis prior to the training procedure, effectively
eliminating biases introduced by the decomposition instability. Furthermore,
synchronized modeling removes the limiting factor imposed by previous methods,
which require all shapes to share a common fixed connectivity. Using a private
dataset of real dental crowns, we observe a greater reconstruction quality of
the synthetized shapes, exceeding those of models trained on unaligned
embeddings. We also explore additional applications of spectral analysis in
digital dentistry, such as shape compression and interpolation. ToothForge
facilitates a range of approaches at the intersection of spectral analysis and
machine learning, with fewer restrictions on mesh structure. This makes it
applicable for shape analysis not only in dentistry, but also in broader
medical applications, where guaranteeing consistent connectivity across shapes
from various clinics is unrealistic. The code is available at
https://github.com/tiborkubik/toothForge.

</details>


### [318] [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/pdf/2506.02708)
*Naoto Tanji, Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: A novel self-training method for Vision Language Models (VLMs) generates image scores and natural language justifications, improving accuracy and coherence without external data.


<details>
  <summary>Details</summary>
Motivation: Understanding a model's rationale for image scoring is essential for trust in real-world applications.

Method: Leverages an image scoring dataset and an instruction-tuned VLM for self-training, using Direct Preference Optimization on two datasets to merge and improve results.

Result: Improved scoring accuracy and coherence of generated explanations.

Conclusion: The method effectively enhances VLM performance in generating both scores and justifications, fostering trust in model judgments.

Abstract: Image scoring is a crucial task in numerous real-world applications. To trust
a model's judgment, understanding its rationale is essential. This paper
proposes a novel training method for Vision Language Models (VLMs) to generate
not only image scores but also corresponding justifications in natural
language. Leveraging only an image scoring dataset and an instruction-tuned
VLM, our method enables self-training, utilizing the VLM's generated text
without relying on external data or models. In addition, we introduce a simple
method for creating a dataset designed to improve alignment between predicted
scores and their textual justifications. By iteratively training the model with
Direct Preference Optimization on two distinct datasets and merging them, we
can improve both scoring accuracy and the coherence of generated explanations.

</details>


### [319] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/pdf/2506.02733)
*Xiaoyi Feng, Kaifeng Zou, Caichun Cen, Tao Huang, Hui Guo, Zizhou Huang, Yingli Zhao, Mingqing Zhang, Diwei Wang, Yuntao Zou, Dagang Li*

Main category: cs.CV

TL;DR: LinkTo-Anime is the first high-quality dataset for cel anime character motion, offering rich annotations and benchmarks for optical flow research.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack focus on cel anime motion, hindering research in optical flow and related tasks like anime video generation.

Method: The dataset is created using 3D model rendering, providing optical flow, occlusion masks, and skeleton annotations across 395 video sequences.

Result: LinkTo-Anime includes 24,230 training, 720 validation, and 4,320 test frames, with benchmarks highlighting method limitations.

Conclusion: The dataset bridges a critical gap, enabling advancements in optical flow estimation and anime-specific applications.

Abstract: Existing optical flow datasets focus primarily on real-world simulation or
synthetic human motion, but few are tailored to Celluloid(cel) anime character
motion: a domain with unique visual and motion characteristics. To bridge this
gap and facilitate research in optical flow estimation and downstream tasks
such as anime video generation and line drawing colorization, we introduce
LinkTo-Anime, the first high-quality dataset specifically designed for cel
anime character motion generated with 3D model rendering. LinkTo-Anime provides
rich annotations including forward and backward optical flow, occlusion masks,
and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230
training frames, 720 validation frames, and 4,320 test frames. Furthermore, a
comprehensive benchmark is constructed with various optical flow estimation
methods to analyze the shortcomings and limitations across multiple datasets.

</details>


### [320] [GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal](https://arxiv.org/pdf/2506.02736)
*Shufan Qing, Anzhen Li, Qiandi Wang, Yuefeng Niu, Mingchen Feng, Guoliang Hu, Jinqiao Wu, Fengtao Nan, Yingchun Fan*

Main category: cs.CV

TL;DR: GeneA-SLAM2 improves semantic SLAM in dynamic scenes by using depth variance for dynamic pixel extraction and autoencoder-based keypoint reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully cover dynamic regions in highly dynamic scenarios, limiting accuracy.

Method: Uses depth variance constraints to extract dynamic pixels and create depth masks, plus an autoencoder for keypoint reconstruction and genetic resampling.

Result: Outperforms current methods in highly dynamic sequences, maintaining high accuracy.

Conclusion: GeneA-SLAM2 is robust and efficient for dynamic environments, with code publicly available.

Abstract: Existing semantic SLAM in dynamic environments mainly identify dynamic
regions through object detection or semantic segmentation methods. However, in
certain highly dynamic scenarios, the detection boxes or segmentation masks
cannot fully cover dynamic regions. Therefore, this paper proposes a robust and
efficient GeneA-SLAM2 system that leverages depth variance constraints to
handle dynamic scenes. Our method extracts dynamic pixels via depth variance
and creates precise depth masks to guide the removal of dynamic objects.
Simultaneously, an autoencoder is used to reconstruct keypoints, improving the
genetic resampling keypoint algorithm to obtain more uniformly distributed
keypoints and enhance the accuracy of pose estimation. Our system was evaluated
on multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2
maintains high accuracy in dynamic scenes compared to current methods. Code is
available at: https://github.com/qingshufan/GeneA-SLAM2.

</details>


### [321] [Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](https://arxiv.org/pdf/2506.02738)
*Negin Baghbanzadeh, Sajad Ashkezari, Elham Dolatabadi, Arash Afkanpour*

Main category: cs.CV

TL;DR: The paper introduces a scalable pipeline for subfigure extraction from compound biomedical figures, releasing a large dataset (OPEN-PMC-18M) and showing improved vision-language model performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale subfigure extraction in biomedical literature and its impact on vision-language models.

Method: A transformer-based object detection pipeline trained on 500,000 synthetic compound figures, applied to create OPEN-PMC-18M.

Result: State-of-the-art performance on benchmarks, improved model performance in retrieval, zero-shot classification, and robustness.

Conclusion: The work advances biomedical vision-language modeling, with released resources for reproducibility and further research.

Abstract: Compound figures, which are multi-panel composites containing diverse
subfigures, are ubiquitous in biomedical literature, yet large-scale subfigure
extraction remains largely unaddressed. Prior work on subfigure extraction has
been limited in both dataset size and generalizability, leaving a critical open
question: How does high-fidelity image-text alignment via large-scale subfigure
extraction impact representation learning in vision-language models? We address
this gap by introducing a scalable subfigure extraction pipeline based on
transformer-based object detection, trained on a synthetic corpus of 500,000
compound figures, and achieving state-of-the-art performance on both ImageCLEF
2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a
large-scale high quality biomedical vision-language dataset comprising 18
million clinically relevant subfigure-caption pairs spanning radiology,
microscopy, and visible light photography. We train and evaluate
vision-language models on our curated datasets and show improved performance
across retrieval, zero-shot classification, and robustness benchmarks,
outperforming existing baselines. We release our dataset, models, and code to
support reproducible benchmarks and further study into biomedical
vision-language modeling and representation learning.

</details>


### [322] [VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians](https://arxiv.org/pdf/2506.02741)
*Pengchong Hu, Zhizhong Han*

Main category: cs.CV

TL;DR: The paper introduces view-tied 3D Gaussians for RGBD SLAM, improving efficiency and scalability by simplifying Gaussian representation and optimizing tracking/mapping strategies.


<details>
  <summary>Details</summary>
Motivation: Current methods using 3D Gaussians for SLAM struggle with scalability in large scenes due to GPU memory constraints and inefficient optimization.

Method: Proposes view-tied 3D Gaussians, simplified representations tied to depth pixels, and novel tracking/mapping strategies to reduce memory usage and improve rendering.

Result: Achieves better rendering and tracking accuracy, scalability, and efficiency compared to state-of-the-art methods on benchmarks.

Conclusion: The approach successfully addresses scalability and efficiency issues in RGBD SLAM, demonstrating superior performance and practical viability.

Abstract: Jointly estimating camera poses and mapping scenes from RGBD images is a
fundamental task in simultaneous localization and mapping (SLAM).
State-of-the-art methods employ 3D Gaussians to represent a scene, and render
these Gaussians through splatting for higher efficiency and better rendering.
However, these methods cannot scale up to extremely large scenes, due to the
inefficient tracking and mapping strategies that need to optimize all 3D
Gaussians in the limited GPU memories throughout the training to maintain the
geometry and color consistency to previous RGBD observations. To resolve this
issue, we propose novel tracking and mapping strategies to work with a novel 3D
representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied
3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,
without needing to learn locations, rotations, and multi-dimensional variances.
Tying Gaussians to views not only significantly saves storage but also allows
us to employ many more Gaussians to represent local details in the limited GPU
memory. Moreover, our strategies remove the need of maintaining all Gaussians
learnable throughout the training, while improving rendering quality, and
tracking accuracy. We justify the effectiveness of these designs, and report
better performance over the latest methods on the widely used benchmarks in
terms of rendering and tracking accuracy and scalability. Please see our
project page for code and videos at
https://machineperceptionlab.github.io/VTGaussian-SLAM-Project .

</details>


### [323] [RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS](https://arxiv.org/pdf/2506.02751)
*Chuanyu Fu, Yuqi Zhang, Kunbin Yao, Guanying Chen, Yuan Xiong, Chuan Huang, Shuguang Cui, Xiaochun Cao*

Main category: cs.CV

TL;DR: RobustSplat improves 3D Gaussian Splatting by addressing transient object artifacts with delayed Gaussian growth and scale-cascaded mask bootstrapping.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with transient objects, causing rendering artifacts.

Method: Proposes delayed Gaussian growth and scale-cascaded mask bootstrapping to mitigate transient object interference.

Result: Outperforms existing methods on challenging datasets, showing robustness and effectiveness.

Conclusion: RobustSplat offers a reliable solution for transient object handling in 3DGS.

Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its
real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.
However, existing methods struggle with accurately modeling scenes affected by
transient objects, leading to artifacts in the rendered images. We identify
that the Gaussian densification process, while enhancing scene detail capture,
unintentionally contributes to these artifacts by growing additional Gaussians
that model transient disturbances. To address this, we propose RobustSplat, a
robust solution based on two critical designs. First, we introduce a delayed
Gaussian growth strategy that prioritizes optimizing static scene structure
before allowing Gaussian splitting/cloning, mitigating overfitting to transient
objects in early optimization. Second, we design a scale-cascaded mask
bootstrapping approach that first leverages lower-resolution feature similarity
supervision for reliable initial transient mask estimation, taking advantage of
its stronger semantic consistency and robustness to noise, and then progresses
to high-resolution supervision to achieve more precise mask prediction.
Extensive experiments on multiple challenging datasets show that our method
outperforms existing methods, clearly demonstrating the robustness and
effectiveness of our method. Our project page is
https://fcyycf.github.io/RobustSplat/.

</details>


### [324] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/pdf/2506.02764)
*Fatma Youssef Mohammed, Kostas Alexis*

Main category: cs.CV

TL;DR: A neural network architecture based on HAT shows free-viewing and visual search share a common representation, enabling efficient knowledge transfer with minimal performance drop and significant computational savings.


<details>
  <summary>Details</summary>
Motivation: To explore whether free-viewing and task-specific visual attention share a common representation, bridging a gap in current research.

Method: Proposes a neural network architecture extending the Human Attention Transformer (HAT) to test the hypothesis of shared representation.

Result: Demonstrates efficient knowledge transfer between free-viewing and visual search, with only a 3.86% performance drop in predicted scanpaths and substantial computational savings (92.29% GFLOPs, 31.23% parameters).

Conclusion: A common representation exists between free-viewing and visual search, enabling efficient transfer learning with minimal performance loss and reduced computational costs.

Abstract: Computational human attention modeling in free-viewing and task-specific
settings is often studied separately, with limited exploration of whether a
common representation exists between them. This work investigates this question
and proposes a neural network architecture that builds upon the Human Attention
transformer (HAT) to test the hypothesis. Our results demonstrate that
free-viewing and visual search can efficiently share a common representation,
allowing a model trained in free-viewing attention to transfer its knowledge to
task-driven visual search with a performance drop of only 3.86% in the
predicted fixation scanpaths, measured by the semantic sequence score (SemSS)
metric which reflects the similarity between predicted and human scanpaths.
This transfer reduces computational costs by 92.29% in terms of GFLOPs and
31.23% in terms of trainable parameters.

</details>


### [325] [A Dynamic Transformer Network for Vehicle Detection](https://arxiv.org/pdf/2506.02765)
*Chunwei Tian, Kai Liu, Bob Zhang, Zhixiang Huang, Chia-Wen Lin, David Zhang*

Main category: cs.CV

TL;DR: DTNet, a dynamic Transformer network, improves vehicle detection by dynamically generating weights and using mixed attention mechanisms to handle lighting and occlusion challenges.


<details>
  <summary>Details</summary>
Motivation: Existing traffic algorithms struggle with lighting and occlusion variations. DTNet aims to enhance adaptability and accuracy in vehicle detection.

Method: DTNet combines dynamic convolution, mixed attention (channel attention and Transformer), and translation-variant convolution to refine structural information.

Result: DTNet demonstrates competitive performance in vehicle detection.

Conclusion: DTNet effectively addresses limitations in current methods, offering a robust solution for vehicle detection.

Abstract: Stable consumer electronic systems can assist traffic better. Good traffic
consumer electronic systems require collaborative work between traffic
algorithms and hardware. However, performance of popular traffic algorithms
containing vehicle detection methods based on deep networks via learning data
relation rather than learning differences in different lighting and occlusions
is limited. In this paper, we present a dynamic Transformer network for vehicle
detection (DTNet). DTNet utilizes a dynamic convolution to guide a deep network
to dynamically generate weights to enhance adaptability of an obtained
detector. Taking into relations of different information account, a mixed
attention mechanism based channel attention and Transformer is exploited to
strengthen relations of channels and pixels to extract more salient information
for vehicle detection. To overcome the drawback of difference in an image
account, a translation-variant convolution relies on spatial location
information to refine obtained structural information for vehicle detection.
Experimental results illustrate that our DTNet is competitive for vehicle
detection. Code of the proposed DTNet can be obtained at
https://github.com/hellloxiaotian/DTNet.

</details>


### [326] [FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts](https://arxiv.org/pdf/2506.02781)
*Tongyuan Bai, Wangyuanfan Bai, Dong Chen, Tieru Wu, Manyi Li, Rui Ma*

Main category: cs.CV

TL;DR: FreeScene is a user-friendly framework for 3D indoor scene synthesis, combining text and image inputs for fine-grained control, outperforming existing methods in quality and controllability.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing methods—either too rough (language-based) or too complex (graph-based)—for fine-grained scene customization.

Method: Uses a VLM-based Graph Designer to integrate free-form inputs (text/images) into a graph, then employs MG-DiT (Mixed Graph Diffusion Transformer) for graph-aware denoising and scene generation.

Result: Outperforms state-of-the-art methods in generation quality and controllability, unifying text-based and graph-based synthesis in one model.

Conclusion: FreeScene offers an efficient, versatile solution for indoor scene synthesis, balancing ease of use with fine-grained control.

Abstract: Controllability plays a crucial role in the practical applications of 3D
indoor scene synthesis. Existing works either allow rough language-based
control, that is convenient but lacks fine-grained scene customization, or
employ graph based control, which offers better controllability but demands
considerable knowledge for the cumbersome graph design process. To address
these challenges, we present FreeScene, a user-friendly framework that enables
both convenient and effective control for indoor scene synthesis.Specifically,
FreeScene supports free-form user inputs including text description and/or
reference images, allowing users to express versatile design intentions. The
user inputs are adequately analyzed and integrated into a graph representation
by a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion
Transformer, which performs graph-aware denoising to enhance scene generation.
Our MG-DiT not only excels at preserving graph structure but also offers broad
applicability to various tasks, including, but not limited to, text-to-scene,
graph-to-scene, and rearrangement, all within a single model. Extensive
experiments demonstrate that FreeScene provides an efficient and user-friendly
solution that unifies text-based and graph based scene synthesis, outperforming
state-of-the-art methods in terms of both generation quality and
controllability in a range of applications.

</details>


### [327] [SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model](https://arxiv.org/pdf/2506.02783)
*Carlos Garcia-Lopez-de-Haro, Caterina Fuster-Barcelo, Curtis T. Rueden, Jonathan Heras, Vladimir Ulman, Daniel Franco-Barranco, Adrian Ines, Kevin W. Eliceiri, Jean-Christophe Olivo-Marin, Jean-Yves Tinevez, Daniel Sage, Arrate Munoz-Barrutia*

Main category: cs.CV

TL;DR: SAMJ is an ImageJ/Fiji plugin using SAM to simplify and speed up mask annotation in biomedical image analysis.


<details>
  <summary>Details</summary>
Motivation: Mask annotation is labor-intensive, creating a bottleneck in AI-driven biomedical image analysis.

Method: SAMJ leverages the Segment Anything Model (SAM) for interactive, one-click annotations on standard computers.

Result: SAMJ enables real-time object delineation in large scientific images, simplifying dataset creation.

Conclusion: SAMJ provides an easy-to-use solution to accelerate labeled image dataset generation.

Abstract: Mask annotation remains a significant bottleneck in AI-driven biomedical
image analysis due to its labor-intensive nature. To address this challenge, we
introduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment
Anything Model (SAM). SAMJ enables seamless, interactive annotations with
one-click installation on standard computers. Designed for real-time object
delineation in large scientific images, SAMJ is an easy-to-use solution that
simplifies and accelerates the creation of labeled image datasets.

</details>


### [328] [Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video](https://arxiv.org/pdf/2506.02789)
*Renxing Li, Weiyi Tang, Peiqi Li, Qiming Huang, Jiayuan She, Shengkai Li, Haoran Xu, Yeyun Wan, Jing Liu, Hailong Fu, Xiang Li, Jiangang Chen*

Main category: cs.CV

TL;DR: A novel automated method for measuring optic nerve sheath diameter (ONSD) using KCF tracking and SLIC segmentation achieves high accuracy, reducing reliance on operator skill.


<details>
  <summary>Details</summary>
Motivation: Manual ONSD measurement is operator-dependent, limiting its reliability for intracranial pressure (ICP) monitoring.

Method: Combines KCF tracking, SLIC segmentation, and GMM with KL-divergence for automated ONSD measurement.

Result: Achieved mean error of 0.04, mean squared deviation of 0.054, and ICC of 0.782 compared to expert clinicians.

Conclusion: The method offers accurate, automated ONSD measurement, promising for clinical ICP monitoring.

Abstract: Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker
of secondary brain injury, with a significant linear correlation observed
between optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD
could effectively support dynamic evaluation of ICP. However, ONSD measurement
is heavily reliant on the operator's experience and skill, particularly in
manually selecting the optimal frame from ultrasound sequences and measuring
ONSD. Approach. This paper presents a novel method to automatically identify
the optimal frame from video sequences for ONSD measurement by employing the
Kernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative
Clustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and
measured using a Gaussian Mixture Model (GMM) combined with a
KL-divergence-based method. Results. When compared with the average
measurements of two expert clinicians, the proposed method achieved a mean
error, mean squared deviation, and intraclass correlation coefficient (ICC) of
0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that
this method provides highly accurate automated ONSD measurements, showing
potential for clinical application.

</details>


### [329] [Random Registers for Cross-Domain Few-Shot Learning](https://arxiv.org/pdf/2506.02843)
*Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li*

Main category: cs.CV

TL;DR: Random registers in ViT improve cross-domain few-shot learning by reducing overfitting and flattening loss landscapes, outperforming traditional prompt tuning.


<details>
  <summary>Details</summary>
Motivation: To explore why random registers in Vision Transformers (ViT) enhance transferability in cross-domain few-shot learning (CDFSL) compared to learnable prompts.

Method: Analyze the impact of random registers vs. learnable prompts, propose adding random registers to semantic regions of image tokens for better perturbation.

Result: Random registers consistently improve target-domain performance, validated on four benchmarks with state-of-the-art results.

Conclusion: Random registers act as a form of sharpness-aware minimization, enhancing ViT's transferability in CDFSL.

Abstract: Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a
data-sufficient source domain to data-scarce target domains. Although Vision
Transformer (ViT) has shown superior capability in many vision tasks, its
transferability against huge domain gaps in CDFSL is still under-explored. In
this paper, we find an intriguing phenomenon: during the source-domain
training, prompt tuning, as a common way to train ViT, could be harmful for the
generalization of ViT in target domains, but setting them to random noises
(i.e., random registers) could consistently improve target-domain performance.
We then delve into this phenomenon for an interpretation. We find that
learnable prompts capture domain information during the training on the source
dataset, which views irrelevant visual patterns as vital cues for recognition.
This can be viewed as a kind of overfitting and increases the sharpness of the
loss landscapes. In contrast, random registers are essentially a novel way of
perturbing attention for the sharpness-aware minimization, which helps the
model find a flattened minimum in loss landscapes, increasing the
transferability. Based on this phenomenon and interpretation, we further
propose a simple but effective approach for CDFSL to enhance the perturbation
on attention maps by adding random registers on the semantic regions of image
tokens, improving the effectiveness and efficiency of random registers.
Extensive experiments on four benchmarks validate our rationale and
state-of-the-art performance. Codes and models are available at
https://github.com/shuaiyi308/REAP.

</details>


### [330] [Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments](https://arxiv.org/pdf/2506.02845)
*Di Wen, Lei Qi, Kunyu Peng, Kailun Yang, Fei Teng, Ao Luo, Jia Fu, Yufan Chen, Ruiping Liu, Yitian Shi, M. Saquib Sarfraz, Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: MicroG-4M is the first benchmark dataset for video understanding in microgravity, featuring 4,759 clips, 50 actions, and multiple tasks like action recognition and visual question answering.


<details>
  <summary>Details</summary>
Motivation: Existing video datasets lack microgravity contexts, limiting vision systems for space applications.

Method: Constructed from real-world space missions and simulations, MicroG-4M includes clips, captions, and QA pairs.

Result: Supports tasks like action recognition, video captioning, and visual QA, with baselines from state-of-the-art models.

Conclusion: MicroG-4M fills a critical gap for domain-robust video understanding in microgravity.

Abstract: Despite substantial progress in video understanding, most existing datasets
are limited to Earth's gravitational conditions. However, microgravity alters
human motion, interactions, and visual semantics, revealing a critical gap for
real-world vision systems. This presents a challenge for domain-robust video
understanding in safety-critical space applications. To address this, we
introduce MicroG-4M, the first benchmark for spatio-temporal and semantic
understanding of human activities in microgravity. Constructed from real-world
space missions and cinematic simulations, the dataset includes 4,759 clips
covering 50 actions, 1,238 context-rich captions, and over 7,000
question-answer pairs on astronaut activities and scene understanding.
MicroG-4M supports three core tasks: fine-grained multi-label action
recognition, temporal video captioning, and visual question answering, enabling
a comprehensive evaluation of both spatial localization and semantic reasoning
in microgravity contexts. We establish baselines using state-of-the-art models.
All data, annotations, and code are available at
https://github.com/LEI-QI-233/HAR-in-Space.

</details>


### [331] [PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors](https://arxiv.org/pdf/2506.02846)
*Yujin Chen, Yinyu Nie, Benjamin Ummenhofer, Reiner Birkl, Michael Paulitsch, Matthias Nießner*

Main category: cs.CV

TL;DR: PBR-SR is a zero-shot method for super-resolving PBR textures using pretrained image priors and iterative refinement, outperforming direct SR and prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating high-resolution PBR textures from low-resolution inputs without additional training or data.

Method: Leverages pretrained SR models, iteratively minimizes deviations between SR priors and differentiable renderings, and applies multi-view constraints for consistency.

Result: Produces high-fidelity PBR textures for artist-designed and AI-generated meshes, excelling in PBR and rendering evaluations.

Conclusion: PBR-SR is effective for zero-shot PBR texture super-resolution, enabling advanced applications like relighting.

Abstract: We present PBR-SR, a novel method for physically based rendering (PBR)
texture super resolution (SR). It outputs high-resolution, high-quality PBR
textures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR
leverages an off-the-shelf super-resolution model trained on natural images,
and iteratively minimizes the deviations between super-resolution priors and
differentiable renderings. These enhancements are then back-projected into the
PBR map space in a differentiable manner to produce refined, high-resolution
textures. To mitigate view inconsistencies and lighting sensitivity, which is
common in view-based super-resolution, our method applies 2D prior constraints
across multi-view renderings, iteratively refining the shared, upscaled
textures. In parallel, we incorporate identity constraints directly in the PBR
texture domain to ensure the upscaled textures remain faithful to the LR input.
PBR-SR operates without any additional training or data requirements, relying
entirely on pretrained image priors. We demonstrate that our approach produces
high-fidelity PBR textures for both artist-designed and AI-generated meshes,
outperforming both direct SR models application and prior texture optimization
methods. Our results show high-quality outputs in both PBR and rendering
evaluations, supporting advanced applications such as relighting.

</details>


### [332] [METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/pdf/2506.02850)
*Mengyue Wang, Shuo Chen, Kristian Kersting, Volker Tresp, Yunpu Ma*

Main category: cs.CV

TL;DR: METok is a training-free framework for compressing visual tokens in Video Large Language Models (VLLMs), improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Processing long videos in VLLMs is computationally intensive and redundant. METok aims to address these challenges by dynamically compressing visual tokens.

Method: METok uses a multi-stage approach: (1) event-aware compression, (2) hierarchical token pruning, and (3) KV Cache optimization.

Result: METok reduces FLOPs by 80.6% and KV Cache memory by 93.5% while maintaining accuracy.

Conclusion: METok effectively balances efficiency and accuracy in VLLMs for long video processing.

Abstract: Recent advances in Video Large Language Models (VLLMs) have significantly
enhanced their ability to understand video content. Nonetheless, processing
long videos remains challenging due to high computational demands and the
redundancy present in the visual data. In this work, we propose METok, a
training-free, Multi-stage Event-based Token compression framework designed to
accelerate VLLMs' inference while preserving accuracy. METok progressively
eliminates redundant visual tokens across three critical stages: (1)
event-aware compression during vision encoding, (2) hierarchical token pruning
in the prefilling stage based on semantic alignment and event importance, and
(3) a decoding-stage KV Cache optimization that further reduces memory
consumption. Our experiments on diverse video benchmarks demonstrate that METok
achieves an optimal trade-off between efficiency and accuracy by dynamically
selecting informative visual tokens. For instance, equipping LongVA-7B with
METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all
while maintaining comparable or even superior accuracy.

</details>


### [333] [Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation](https://arxiv.org/pdf/2506.02853)
*Mingjie Wei, Xuemei Xie, Yutong Zhong, Guangming Shi*

Main category: cs.CV

TL;DR: The paper proposes a Pyramid Graph Transformer (PGFormer) for 3D human pose estimation, addressing challenges in modeling long-range dependencies among body parts with a lightweight, multi-scale architecture.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with modeling long-range dependencies in human pose estimation due to noise and large model sizes. The paper aims to improve this by capturing cross-scale correlations.

Method: The authors introduce a Pyramid Graph Attention (PGA) module to capture long-range dependencies across scales, combined with graph convolution for a lightweight PGFormer architecture.

Result: PGFormer achieves lower error and smaller model size compared to state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets.

Conclusion: The proposed PGFormer effectively addresses the challenges of long-range dependency modeling in 3D pose estimation, offering a compact and efficient solution.

Abstract: Action coordination in human structure is indispensable for the spatial
constraints of 2D joints to recover 3D pose. Usually, action coordination is
represented as a long-range dependence among body parts. However, there are two
main challenges in modeling long-range dependencies. First, joints should not
only be constrained by other individual joints but also be modulated by the
body parts. Second, existing methods make networks deeper to learn dependencies
between non-linked parts. They introduce uncorrelated noise and increase the
model size. In this paper, we utilize a pyramid structure to better learn
potential long-range dependencies. It can capture the correlation across joints
and groups, which complements the context of the human sub-structure. In an
effective cross-scale way, it captures the pyramid-structured long-range
dependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)
module to capture long-range cross-scale dependencies. It concatenates
information from various scales into a compact sequence, and then computes the
correlation between scales in parallel. Combining PGA with graph convolution
modules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose
estimation, which is a lightweight multi-scale transformer architecture. It
encapsulates human sub-structures into self-attention by pooling. Extensive
experiments show that our approach achieves lower error and smaller model size
than state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code
is available at https://github.com/MingjieWe/PGFormer.

</details>


### [334] [Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework](https://arxiv.org/pdf/2506.02854)
*Mengmeng Zhang, Xingyuan Dai, Yicheng Sun, Jing Wang, Yueyang Yao, Xiaoyan Gong, Fuze Cong, Feiyue Wang, Yisheng Lv*

Main category: cs.CV

TL;DR: HSP-SAM is a self-prompting framework for medical image segmentation, eliminating the need for manual prompts and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: SAM's dependency on prompts limits its use in medical imaging where manual prompts are unavailable.

Method: Proposes HSP-SAM, introducing abstract prompts during self-prompting, unlike previous positional prompt methods.

Result: Achieves superior performance, up to 14.04% improvement over state-of-the-art, and strong generalization across modalities.

Conclusion: Abstract prompts enhance robustness and generalization, making HSP-SAM effective for prompt-free medical segmentation.

Abstract: Although the Segment Anything Model (SAM) is highly effective in natural
image segmentation, it requires dependencies on prompts, which limits its
applicability to medical imaging where manual prompts are often unavailable.
Existing efforts to fine-tune SAM for medical segmentation typically struggle
to remove this dependency. We propose Hierarchical Self-Prompting SAM
(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong
performance in prompt-free medical image segmentation. Unlike previous
self-prompting methods that remain limited to positional prompts similar to
vanilla SAM, we are the first to introduce learning abstract prompts during the
self-prompting process. This simple and intuitive self-prompting framework
achieves superior performance on classic segmentation tasks such as polyp and
skin lesion segmentation, while maintaining robustness across diverse medical
imaging modalities. Furthermore, it exhibits strong generalization to unseen
datasets, achieving improvements of up to 14.04% over previous state-of-the-art
methods on some challenging benchmarks. These results suggest that abstract
prompts encapsulate richer and higher-dimensional semantic information compared
to positional prompts, thereby enhancing the model's robustness and
generalization performance. All models and codes will be released upon
acceptance.

</details>


### [335] [Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection](https://arxiv.org/pdf/2506.02857)
*Luca Maiano, Fabrizio Casadei, Irene Amerini*

Main category: cs.CV

TL;DR: The paper proposes two novel OOD detection methods for deepfake detection, focusing on reconstruction and attention mechanisms, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deepfake detection to open-set scenarios, where training data may not cover all generative models.

Method: Two OOD detection approaches: one using input image reconstruction and another incorporating an attention mechanism.

Result: The methods outperform existing techniques, achieving top performance on benchmarks.

Conclusion: The proposed approaches offer robust and adaptable solutions for real-world deepfake detection.

Abstract: Detecting deepfakes has become a critical challenge in Computer Vision and
Artificial Intelligence. Despite significant progress in detection techniques,
generalizing them to open-set scenarios continues to be a persistent
difficulty. Neural networks are often trained on the closed-world assumption,
but with new generative models constantly evolving, it is inevitable to
encounter data generated by models that are not part of the training
distribution. To address these challenges, in this paper, we propose two novel
Out-Of-Distribution (OOD) detection approaches. The first approach is trained
to reconstruct the input image, while the second incorporates an attention
mechanism for detecting OODs. Our experiments validate the effectiveness of the
proposed approaches compared to existing state-of-the-art techniques. Our
method achieves promising results in deepfake detection and ranks among the
top-performing configurations on the benchmark, demonstrating their potential
for robust, adaptable solutions in dynamic, real-world applications.

</details>


### [336] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/pdf/2506.02975)
*Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Zunnan Xu, Zhaoyang Zhang, Yixiao Ge, Xiu Li, Ying Shan*

Main category: cs.CV

TL;DR: The paper introduces HaploOmni, a single transformer for unified multimodal tasks, using efficient training techniques like multimodal warmup and feature pre-scaling to achieve competitive performance.


<details>
  <summary>Details</summary>
Motivation: To advance unified multimodal understanding and generation by developing a single-model framework with efficient training methods.

Method: Proposes multimodal warmup, feature pre-scaling, and multimodal AdaLN techniques to train HaploOmni, a single transformer model.

Result: HaploOmni achieves competitive performance in image and video tasks with limited training costs.

Conclusion: The proposed methods enable efficient training of a unified multimodal transformer, with public code availability.

Abstract: With the advancement of language models, unified multimodal understanding and
generation have made significant strides, with model architectures evolving
from separated components to unified single-model frameworks. This paper
explores an efficient training paradigm to build a single transformer for
unified multimodal understanding and generation. Specifically, we propose a
multimodal warmup strategy utilizing prior knowledge to extend capabilities. To
address cross-modal compatibility challenges, we introduce feature pre-scaling
and multimodal AdaLN techniques. Integrating the proposed technologies, we
present the HaploOmni, a new single multimodal transformer. With limited
training costs, HaploOmni achieves competitive performance across multiple
image and video understanding and generation benchmarks over advanced unified
models. All codes will be made public at https://github.com/Tencent/HaploVLM.

</details>


### [337] [MVTD: A Benchmark Dataset for Maritime Visual Object Tracking](https://arxiv.org/pdf/2506.02866)
*Ahsan Baidar Bakht, Muhayy Ud Din, Sajid Javed, Irfan Hussain*

Main category: cs.CV

TL;DR: The paper introduces the Maritime Visual Tracking Dataset (MVTD) to address challenges in maritime VOT, showing performance gains when SOTA trackers are fine-tuned on it.


<details>
  <summary>Details</summary>
Motivation: Maritime environments pose unique challenges like reflections and occlusions, degrading existing tracking algorithms, necessitating a domain-specific dataset.

Method: MVTD includes 182 high-resolution videos (150,000 frames) of four maritime object classes, capturing diverse real-world conditions.

Result: Evaluation of 14 SOTA trackers on MVTD showed performance drops compared to general datasets, but fine-tuning improved results significantly.

Conclusion: MVTD fills a critical gap, proving the value of domain adaptation and transfer learning for maritime VOT.

Abstract: Visual Object Tracking (VOT) is a fundamental task with widespread
applications in autonomous navigation, surveillance, and maritime robotics.
Despite significant advances in generic object tracking, maritime environments
continue to present unique challenges, including specular water reflections,
low-contrast targets, dynamically changing backgrounds, and frequent
occlusions. These complexities significantly degrade the performance of
state-of-the-art tracking algorithms, highlighting the need for domain-specific
datasets. To address this gap, we introduce the Maritime Visual Tracking
Dataset (MVTD), a comprehensive and publicly available benchmark specifically
designed for maritime VOT. MVTD comprises 182 high-resolution video sequences,
totaling approximately 150,000 frames, and includes four representative object
classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset
captures a diverse range of operational conditions and maritime scenarios,
reflecting the real-world complexities of maritime environments. We evaluated
14 recent SOTA tracking algorithms on the MVTD benchmark and observed
substantial performance degradation compared to their performance on
general-purpose datasets. However, when fine-tuned on MVTD, these models
demonstrate significant performance gains, underscoring the effectiveness of
domain adaptation and the importance of transfer learning in specialized
tracking contexts. The MVTD dataset fills a critical gap in the visual tracking
community by providing a realistic and challenging benchmark for maritime
scenarios. Dataset and Source Code can be accessed here
"https://github.com/AhsanBaidar/MVTD".

</details>


### [338] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/pdf/2506.02976)
*Rachid Zeghlache, Ikram Brahim, Pierre-Henri Conze, Mathieu Lamard, Mohammed El Amine Lazouni, Zineb Aziza Elaouaber, Leila Ryma Lazouni, Christopher Nielsen, Ahmad O. Ahsan, Matthias Wilms, Nils D. Forkert, Lovre Antonio Budimir, Ivana Matovinović, Donik Vršnak, Sven Lončarić, Philippe Zhang, Weili Jiang, Yihao Li, Yiding Hao, Markus Frohmann, Patrick Binder, Marcel Huber, Taha Emre, Teresa Finisterra Araújo, Marzieh Oghbaie, Hrvoje Bogunović, Amerens A. Bekkers, Nina M. van Liebergen, Hugo J. Kuijf, Abdul Qayyum, Moona Mazher, Steven A. Niederer, Alberto J. Beltrán-Carrero, Juan J. Gómez-Valverde, Javier Torresano-Rodríquez, Álvaro Caballero-Sastre, María J. Ledesma Carbayo, Yosuke Yamagishi, Yi Ding, Robin Peretzke, Alexandra Ertl, Maximilian Fischer, Jessica Kächele, Sofiane Zehar, Karim Boukli Hacene, Thomas Monfort, Béatrice Cochener, Mostafa El Habib Daho, Anas-Alexis Benyoussef, Gwenolé Quellec*

Main category: cs.CV

TL;DR: The MARIO challenge at MICCAI 2024 evaluated AI models for AMD detection using OCT images, with tasks on classifying AMD progression and predicting future evolution. AI matched physician performance in detection but lagged in prediction.


<details>
  <summary>Details</summary>
Motivation: To advance automated AMD detection and monitoring using AI, addressing gaps in neovascular activity tracking and multi-modal data analysis.

Method: Teams trained and tested models on a primary dataset from Brest, France, with an auxiliary Algerian dataset for post-challenge evaluation. Tasks included classifying AMD progression and predicting future evolution.

Result: AI performed comparably to physicians in detecting AMD progression (Task 1) but struggled with predicting future evolution (Task 2).

Conclusion: The challenge set benchmarks for AMD monitoring, highlighting AI's current strengths and limitations in clinical applications.

Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated
detection and monitoring of age-related macular degeneration (AMD) through the
analysis of optical coherence tomography (OCT) images. Designed to evaluate
algorithmic performance in detecting neovascular activity changes within AMD,
the challenge incorporated unique multi-modal datasets. The primary dataset,
sourced from Brest, France, was used by participating teams to train and test
their models. The final ranking was determined based on performance on this
dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate
population and device shifts from submitted solutions. Two tasks were involved
in the MARIO challenge. The first one was the classification of evolution
between two consecutive 2D OCT B-scans. The second one was the prediction of
future AMD evolution over three months for patients undergoing anti-vascular
endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with
the top 12 finalists presenting their methods. This paper outlines the
challenge's structure, tasks, data characteristics, and winning methodologies,
setting a benchmark for AMD monitoring using OCT, infrared imaging, and
clinical data (such as the number of visits, age, gender, etc.). The results of
this challenge indicate that artificial intelligence (AI) performs as well as a
physician in measuring AMD progression (Task 1) but is not yet able of
predicting future evolution (Task 2).

</details>


### [339] [Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings](https://arxiv.org/pdf/2506.02868)
*Amal S. Perera, David Fernandez, Chandi Witharana, Elias Manos, Michael Pimenta, Anna K. Liljedahl, Ingmar Nitze, Yili Yang, Todd Nicholson, Chia-Yu Hsu, Wenwen Li, Guido Grosse*

Main category: cs.CV

TL;DR: The paper explores using Vision Transformers (ViTs) with geospatial location embeddings for Arctic feature detection, outperforming CNNs in tasks like detecting thaw slumps and infrastructure.


<details>
  <summary>Details</summary>
Motivation: Accurate mapping of Arctic permafrost and infrastructure is critical, but challenges include handling large-scale data and diverse spectral characteristics of features.

Method: The study integrates geospatial location embeddings into ViTs to improve adaptation across regions, evaluating their performance on detecting ice-wedge polygons, thaw slumps, and infrastructure.

Result: ViTs with location embeddings outperform CNNs, achieving an F1 score increase from 0.84 to 0.92 for retrogressive thaw slump detection.

Conclusion: Transformer-based models with spatial awareness show promise for high-resolution Arctic remote sensing tasks.

Abstract: Accurate mapping of permafrost landforms, thaw disturbances, and human-built
infrastructure at pan-Arctic scale using sub-meter satellite imagery is
increasingly critical. Handling petabyte-scale image data requires
high-performance computing and robust feature detection models. While
convolutional neural network (CNN)-based deep learning approaches are widely
used for remote sensing (RS),similar to the success in transformer based large
language models, Vision Transformers (ViTs) offer advantages in capturing
long-range dependencies and global context via attention mechanisms. ViTs
support pretraining via self-supervised learning-addressing the common
limitation of labeled data in Arctic feature detection and outperform CNNs on
benchmark datasets. Arctic also poses challenges for model generalization,
especially when features with the same semantic class exhibit diverse spectral
characteristics. To address these issues for Arctic feature detection, we
integrate geospatial location embeddings into ViTs to improve adaptation across
regions. This work investigates: (1) the suitability of pre-trained ViTs as
feature extractors for high-resolution Arctic remote sensing tasks, and (2) the
benefit of combining image and location embeddings. Using previously published
datasets for Arctic feature detection, we evaluate our models on three
tasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and
human-built infrastructure. We empirically explore multiple configurations to
fuse image embeddings and location embeddings. Results show that ViTs with
location embeddings outperform prior CNN-based models on two of the three tasks
including F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating
the potential of transformer-based models with spatial awareness for Arctic RS
applications.

</details>


### [340] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/pdf/2506.03022)
*David McVicar, Brian Avant, Adrian Gould, Diego Torrejon, Charles Della Porta, Ryan Mukherjee*

Main category: cs.CV

TL;DR: Smartflow is a cloud-based framework for scalable geospatial research, using open-source tools and Kubernetes for workflow orchestration. It supports model development and analysis, demonstrated by a novel neural architecture for detecting heavy construction.


<details>
  <summary>Details</summary>
Motivation: To enable scalable spatiotemporal geospatial research and analysis, leveraging open-source tools and standardized data processing.

Method: Uses STAC-compliant catalogs for input, processes data into datacubes, and manages model experimentation with tools like ClearML, Tensorboard, and Apache Superset. Kubernetes orchestrates workflows.

Result: Demonstrated by a neural architecture capable of detecting heavy construction across major development phases, using data from the IARPA SMART program.

Conclusion: Smartflow is effective for large-scale geospatial model development and analysis, as shown by its application in monitoring heavy construction.

Abstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable
spatiotemporal geospatial research built on open-source tools and technologies.
Using STAC-compliant catalogs as a common input, heterogeneous geospatial data
can be processed into standardized datacubes for analysis and model training.
Model experimentation is managed using a combination of tools, including
ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is
Kubernetes, which orchestrates the provisioning and execution of workflows to
support both horizontal and vertical scalability. This combination of features
makes Smartflow well-suited for geospatial model development and analysis over
large geographic areas, time scales, and expansive image archives.
  We also present a novel neural architecture, built using Smartflow, to
monitor large geographic areas for heavy construction. Qualitative results
based on data from the IARPA Space-based Machine Automated Recognition
Technique (SMART) program are presented that show the model is capable of
detecting heavy construction throughout all major phases of development.

</details>


### [341] [NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results](https://arxiv.org/pdf/2506.02875)
*Xiaohong Liu, Xiongkuo Min, Qiang Hu, Xiaoyun Zhang, Jie Guo, Guangtao Zhai, Shushi Wang, Yingjie Zhou, Lu Liu, Jingxin Li, Liu Yang, Farong Wen, Li Xu, Yanwei Jiang, Xilei Zhu, Chunyi Li, Zicheng Zhang, Huiyu Duan, Xiele Wu, Yixuan Gao, Yuqin Cao, Jun Jia, Wei Sun, Jiezhang Cao, Radu Timofte, Baojun Li, Jiamian Huang, Dan Luo, Tao Liu, Weixia Zhang, Bingkun Zheng, Junlin Chen, Ruikai Zhou, Meiya Chen, Yu Wang, Hao Jiang, Xiantao Li, Yuxiang Jiang, Jun Tang, Yimeng Zhao, Bo Hu, Zelu Qi, Chaoyang Zhang, Fei Zhao, Ping Shi, Lingzhi Fu, Heng Cong, Shuai He, Rongyu Zhang, Jiarong He, Zongyao Hu, Wei Luo, Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen, Mengjing Su, Yi Wang, Tuo Chen, Chunxiao Li, Shuaiyu Zhao, Jiaxin Wen, Chuyi Lin, Sitong Liu, Ningxin Chu, Jing Wan, Yu Zhou, Baoying Chen, Jishen Zeng, Jiarui Liu, Xianjin Liu, Xin Chen, Lanzhi Zhou, Hangyu Li, You Han, Bibo Xiang, Zhenjie Liu, Jianzhang Lu, Jialin Gui, Renjie Lu, Shangfei Wang, Donghao Zhou, Jingyu Lin, Quanjian Song, Jiancheng Huang, Yufeng Yang, Changwei Wang, Shupeng Zhong, Yang Yang, Lihuo He, Jia Liu, Yuting Xing, Tida Fang, Yuchun Jin*

Main category: cs.CV

TL;DR: The NTIRE 2025 XGC Quality Assessment Challenge addresses video and talking head processing through three tracks: user-generated video, AI-generated video, and talking head, with significant participation and submissions.


<details>
  <summary>Details</summary>
Motivation: To advance quality assessment in video and talking head processing by evaluating diverse methods through a structured challenge.

Method: The challenge is divided into three tracks, each using specific datasets (FineVD-GC, Q-Eval-Video, THQA-NTIRE) and involving multiple phases (development and test) with participant submissions.

Result: High participation and submissions across all tracks, with teams outperforming baselines, contributing to advancements in the field.

Conclusion: The challenge successfully fostered innovation and progress in video and talking head quality assessment.

Abstract: This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which
will be held in conjunction with the New Trends in Image Restoration and
Enhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major
challenge in the field of video and talking head processing. The challenge is
divided into three tracks, including user generated video, AI generated video
and talking head. The user-generated video track uses the FineVD-GC, which
contains 6,284 user generated videos. The user-generated video track has a
total of 125 registered participants. A total of 242 submissions are received
in the development phase, and 136 submissions are received in the test phase.
Finally, 5 participating teams submitted their models and fact sheets. The AI
generated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated
Videos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of
133 participants have registered in this track. A total of 396 submissions are
received in the development phase, and 226 submissions are received in the test
phase. Finally, 6 participating teams submitted their models and fact sheets.
The talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D
talking heads. A total of 89 participants have registered in this track. A
total of 225 submissions are received in the development phase, and 118
submissions are received in the test phase. Finally, 8 participating teams
submitted their models and fact sheets. Each participating team in every track
has proposed a method that outperforms the baseline, which has contributed to
the development of fields in three tracks.

</details>


### [342] [GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation](https://arxiv.org/pdf/2506.02882)
*Sohyun Lee, Yeho Kwon, Lukas Hoyer, Suha Kwak*

Main category: cs.CV

TL;DR: GaRA-SAM improves SAM's robustness to input degradations using lightweight adapters and dynamic rank adjustment, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing SAM's robustness for high-stakes applications like autonomous driving and robotics.

Method: Introduces gated-rank adaptation (GaRA) with lightweight adapters and dynamic rank adjustment.

Result: GaRA-SAM surpasses prior work, achieving up to 21.3%p higher IoU on ACDC.

Conclusion: GaRA-SAM effectively balances robustness and generalization, setting a new benchmark.

Abstract: Improving robustness of the Segment Anything Model (SAM) to input
degradations is critical for its deployment in high-stakes applications such as
autonomous driving and robotics. Our approach to this challenge prioritizes
three key aspects: first, parameter efficiency to maintain the inherent
generalization capability of SAM; second, fine-grained and input-aware
robustification to precisely address the input corruption; and third, adherence
to standard training protocols for ease of training. To this end, we propose
gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into
intermediate layers of the frozen SAM, where each adapter dynamically adjusts
the effective rank of its weight matrix based on the input by selectively
activating (rank-1) components of the matrix using a learned gating module.
This adjustment enables fine-grained and input-aware robustification without
compromising the generalization capability of SAM. Our model, GaRA-SAM,
significantly outperforms prior work on all robust segmentation benchmarks. In
particular, it surpasses the previous best IoU score by up to 21.3\%p on ACDC,
a challenging real corrupted image dataset.

</details>


### [343] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/pdf/2506.03065)
*Pengtao Chen, Xianfang Zeng, Maosen Zhao, Peng Ye, Mingzhu Shen, Wei Cheng, Gang Yu, Tao Chen*

Main category: cs.CV

TL;DR: Sparse-vDiT accelerates video generation by leveraging sparsity patterns in attention maps, reducing FLOPs and improving inference speed without compromising quality.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of attention mechanisms in Diffusion Transformers (DiTs) causes high inference latency in video generation, prompting the need for optimization.

Method: Identifies sparsity patterns in attention maps, proposes pattern-optimized sparse kernels, and uses an offline sparse diffusion search algorithm to optimize computation.

Result: Achieves significant FLOP reduction (up to 2.38×) and speedups (up to 1.85×) while maintaining visual fidelity (PSNR up to 27.09).

Conclusion: Structural sparsity in vDiTs can be systematically exploited for efficient long video synthesis.

Abstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring sparsity patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity
acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels
that replace dense attention with computationally efficient implementations for
each identified sparsity pattern. 2) An offline sparse diffusion search
algorithm that selects the optimal sparse computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural sparsity in vDiTs can be systematically exploited for long video
synthesis.

</details>


### [344] [OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis](https://arxiv.org/pdf/2506.02891)
*Jiewen Hu, Leena Mathur, Paul Pu Liang, Louis-Philippe Morency*

Main category: cs.CV

TL;DR: OpenFace 3.0 is an open-source toolkit for facial analysis, offering improved performance, speed, and efficiency with a lightweight unified model.


<details>
  <summary>Details</summary>
Motivation: Addressing the growing interest in automatic facial behavior analysis, OpenFace 3.0 aims to provide a versatile and accessible tool for diverse applications.

Method: Uses a multi-task architecture trained across varied conditions (populations, poses, lighting, etc.) with parameter sharing for efficiency.

Result: Outperforms similar toolkits in prediction, speed, and memory efficiency, rivaling state-of-the-art models.

Conclusion: OpenFace 3.0 is a practical, real-time solution for facial analysis, freely available for research and community contributions.

Abstract: In recent years, there has been increasing interest in automatic facial
behavior analysis systems from computing communities such as vision, multimodal
interaction, robotics, and affective computing. Building upon the widespread
utility of prior open-source facial analysis systems, we introduce OpenFace
3.0, an open-source toolkit capable of facial landmark detection, facial action
unit detection, eye-gaze estimation, and facial emotion recognition. OpenFace
3.0 contributes a lightweight unified model for facial analysis, trained with a
multi-task architecture across diverse populations, head poses, lighting
conditions, video resolutions, and facial analysis tasks. By leveraging the
benefits of parameter sharing through a unified model and training paradigm,
OpenFace 3.0 exhibits improvements in prediction performance, inference speed,
and memory efficiency over similar toolkits and rivals state-of-the-art models.
OpenFace 3.0 can be installed and run with a single line of code and operate in
real-time without specialized hardware. OpenFace 3.0 code for training models
and running the system is freely available for research purposes and supports
contributions from the community.

</details>


### [345] [Dense Match Summarization for Faster Two-view Estimation](https://arxiv.org/pdf/2506.02893)
*Jonathan Astermark, Anders Heyden, Viktor Larsson*

Main category: cs.CV

TL;DR: Proposes an efficient match summarization scheme to speed up robust two-view relative pose estimation from dense correspondences, achieving comparable accuracy with 10-100x faster runtime.


<details>
  <summary>Details</summary>
Motivation: Dense matchers improve pose accuracy and robustness but increase runtime during robust estimation in RANSAC due to the large number of matches.

Method: Introduces an efficient match summarization scheme to reduce runtime while maintaining accuracy.

Result: Validated on benchmark datasets, the method provides comparable accuracy to full dense matches with significantly faster runtime.

Conclusion: The proposed summarization scheme effectively balances speed and accuracy in robust pose estimation.

Abstract: In this paper, we speed up robust two-view relative pose from dense
correspondences. Previous work has shown that dense matchers can significantly
improve both accuracy and robustness in the resulting pose. However, the large
number of matches comes with a significantly increased runtime during robust
estimation in RANSAC. To avoid this, we propose an efficient match
summarization scheme which provides comparable accuracy to using the full set
of dense matches, while having 10-100x faster runtime. We validate our approach
on standard benchmark datasets together with multiple state-of-the-art dense
matchers.

</details>


### [346] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/pdf/2506.02896)
*Adam Pardyl, Dominik Matuszek, Mateusz Przebieracz, Marek Cygan, Bartosz Zieliński, Maciej Wołczyk*

Main category: cs.CV

TL;DR: FlySearch tests VLMs in complex 3D environments, revealing their limitations in exploration tasks compared to humans, with identified causes like vision hallucination and task planning failures.


<details>
  <summary>Details</summary>
Motivation: To evaluate if VLMs can effectively perform goal-driven exploration in messy, unstructured real-world conditions.

Method: Introduces FlySearch, a 3D photorealistic environment, with three difficulty scenarios to test VLMs.

Result: VLMs fail even simple tasks, with performance gaps widening as tasks get harder. Causes include vision hallucination and planning failures.

Conclusion: VLMs struggle with exploration tasks; finetuning helps some issues. The benchmark and code are released for further research.

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [347] [Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection](https://arxiv.org/pdf/2506.02914)
*Yechi Ma, Wei Hua, Shu Kong*

Main category: cs.CV

TL;DR: AnnoGuide is a benchmark for automating data annotation from expert guidelines, tested on nuScenes dataset, achieving 3D detection improvements but highlighting challenges.


<details>
  <summary>Details</summary>
Motivation: To reduce the laborious and costly process of manual data annotation by automating it using expert-defined guidelines.

Method: Uses a pipeline with open-source foundation models for 2D detection, projects to 3D, and clusters LiDAR points to generate 3D cuboids.

Result: Improved 3D detection mAP from 12.1 to 21.9, but the problem remains challenging.

Conclusion: AnnoGuide is a timely benchmark, emphasizing the need for LiDAR-based foundation models.

Abstract: A crucial yet under-appreciated prerequisite in machine learning solutions
for real-applications is data annotation: human annotators are hired to
manually label data according to detailed, expert-crafted guidelines. This is
often a laborious, tedious, and costly process. To study methods for
facilitating data annotation, we introduce a new benchmark AnnoGuide:
Auto-Annotation from Annotation Guidelines. It aims to evaluate automated
methods for data annotation directly from expert-defined annotation guidelines,
eliminating the need for manual labeling. As a case study, we repurpose the
well-established nuScenes dataset, commonly used in autonomous driving
research, which provides comprehensive annotation guidelines for labeling LiDAR
point clouds with 3D cuboids across 18 object classes. These guidelines include
a few visual examples and textual descriptions, but no labeled 3D cuboids in
LiDAR data, making this a novel task of multi-modal few-shot 3D detection
without 3D annotations. The advances of powerful foundation models (FMs) make
AnnoGuide especially timely, as FMs offer promising tools to tackle its
challenges. We employ a conceptually straightforward pipeline that (1) utilizes
open-source FMs for object detection and segmentation in RGB images, (2)
projects 2D detections into 3D using known camera poses, and (3) clusters LiDAR
points within the frustum of each 2D detection to generate a 3D cuboid.
Starting with a non-learned solution that leverages off-the-shelf FMs, we
progressively refine key components and achieve significant performance
improvements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our
results highlight that AnnoGuide remains an open and challenging problem,
underscoring the urgent need for developing LiDAR-based FMs. We release our
code and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark

</details>


### [348] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/pdf/2506.03135)
*Mengdi Jia, Zekun Qi, Shaochen Zhang, Wenyao Zhang, Xinqiang Yu, Jiawei He, He Wang, Li Yi*

Main category: cs.CV

TL;DR: OmniSpatial is a new benchmark for advanced spatial reasoning in vision-language models, covering dynamic reasoning, complex logic, interaction, and perspective-taking. It reveals significant limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models struggle with advanced spatial reasoning beyond basic tasks, prompting the need for a comprehensive benchmark.

Method: OmniSpatial was created by crawling and manually annotating data, resulting in 1.5K question-answer pairs across 50 subcategories.

Result: Experiments show VLMs and existing models perform poorly on OmniSpatial, highlighting their limitations in spatial understanding.

Conclusion: The benchmark exposes gaps in spatial reasoning and suggests future research directions to improve model performance.

Abstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major
bottleneck for current vision-language models (VLMs). While extensive research
has aimed to evaluate or improve VLMs' understanding of basic spatial
relations, such as distinguishing left from right, near from far, and object
counting, these tasks represent only the most fundamental level of spatial
reasoning. In this work, we introduce OmniSpatial, a comprehensive and
challenging benchmark for spatial reasoning, grounded in cognitive psychology.
OmniSpatial covers four major categories: dynamic reasoning, complex spatial
logic, spatial interaction, and perspective-taking, with 50 fine-grained
subcategories. Through Internet data crawling and careful manual annotation, we
construct over 1.5K question-answer pairs. Extensive experiments show that both
open- and closed-source VLMs, as well as existing reasoning and spatial
understanding models, exhibit significant limitations in comprehensive spatial
understanding. We further analyze failure cases and propose potential
directions for future research.

</details>


### [349] [MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction](https://arxiv.org/pdf/2506.02938)
*Xuhui Chen, Fei Hou, Wencheng Wang, Hong Qin, Ying He*

Main category: cs.CV

TL;DR: MIND proposes a novel algorithm for extracting non-manifold meshes directly from unsigned distance fields (UDFs), addressing topological artifacts and limitations of prior methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods struggle with extracting meshes from UDFs due to inexact zero distances and topological artifacts, especially for non-manifold geometry.

Method: MIND derives a spatial partitioning from UDFs, using a multi-labeled global field to construct material interfaces for non-manifold mesh extraction.

Result: The method robustly handles complex non-manifold surfaces and outperforms existing techniques in experiments.

Conclusion: MIND enables accurate non-manifold mesh extraction from UDFs, overcoming key limitations of prior approaches.

Abstract: Unsigned distance fields (UDFs) are widely used in 3D deep learning due to
their ability to represent shapes with arbitrary topology. While prior work has
largely focused on learning UDFs from point clouds or multi-view images,
extracting meshes from UDFs remains challenging, as the learned fields rarely
attain exact zero distances. A common workaround is to reconstruct signed
distance fields (SDFs) locally from UDFs to enable surface extraction via
Marching Cubes. However, this often introduces topological artifacts such as
holes or spurious components. Moreover, local SDFs are inherently incapable of
representing non-manifold geometry, leading to complete failure in such cases.
To address this gap, we propose MIND (Material Interface from Non-manifold
Distance fields), a novel algorithm for generating material interfaces directly
from UDFs, enabling non-manifold mesh extraction from a global perspective. The
core of our method lies in deriving a meaningful spatial partitioning from the
UDF, where the target surface emerges as the interface between distinct
regions. We begin by computing a two-signed local field to distinguish the two
sides of manifold patches, and then extend this to a multi-labeled global field
capable of separating all sides of a non-manifold structure. By combining this
multi-labeled field with the input UDF, we construct material interfaces that
support non-manifold mesh extraction via a multi-labeled Marching Cubes
algorithm. Extensive experiments on UDFs generated from diverse data sources,
including point cloud reconstruction, multi-view reconstruction, and medial
axis transforms, demonstrate that our approach robustly handles complex
non-manifold surfaces and significantly outperforms existing methods.

</details>


### [350] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/pdf/2506.03097)
*Ashwin Vinod, Shrey Pandit, Aditya Vavre, Linshen Liu*

Main category: cs.CV

TL;DR: EgoVLM is a vision-language model for egocentric video reasoning, fine-tuned with GRPO, outperforming general-purpose VLMs and enhancing interpretability with reasoning traces.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for robust reasoning in first-person video streams for embodied AI applications.

Method: Fine-tuned via Group Relative Policy Optimization (GRPO) without supervised fine-tuning, using a novel keyframe-based reward.

Result: EgoVLM-3B outperforms Qwen2.5-VL models by 14.33 and 13.87 accuracy points on EgoSchema.

Conclusion: EgoVLM improves performance and interpretability, with potential for future exploration in egocentric reasoning.

Abstract: Emerging embodied AI applications, such as wearable cameras and autonomous
agents, have underscored the need for robust reasoning from first person video
streams. We introduce EgoVLM, a vision-language model specifically designed to
integrate visual comprehension and spatial-temporal reasoning within egocentric
video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization
(GRPO), a reinforcement learning method adapted to align model outputs with
human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly
tune using RL without any supervised fine-tuning phase on chain-of-thought
(CoT) data. We evaluate EgoVLM on egocentric video question answering
benchmarks and show that domain-specific training substantially improves
performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on
non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by
14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By
explicitly generating reasoning traces, EgoVLM enhances interpretability,
making it well-suited for downstream applications. Furthermore, we introduce a
novel keyframe-based reward that incorporates salient frame selection to guide
reinforcement learning optimization. This reward formulation opens a promising
avenue for future exploration in temporally grounded egocentric reasoning.

</details>


### [351] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/pdf/2506.03147)
*Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, Li Yuan*

Main category: cs.CV

TL;DR: UniWorld is a unified generative framework leveraging semantic features from visual-language models, outperforming BAGEL in image editing with minimal data and maintaining strong image understanding and generation capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing models lack image perception and manipulation capabilities, while GPT-4o-Image's performance suggests semantic encoders are superior to VAEs.

Method: UniWorld uses semantic features from visual-language models and contrastive semantic encoders, trained with only 1% of BAGEL's data.

Result: UniWorld outperforms BAGEL on image editing benchmarks and excels in image understanding and generation tasks.

Conclusion: UniWorld is a strong, efficient, and open-source unified model for image perception and manipulation.

Abstract: Although existing unified models deliver strong performance on
vision-language understanding and text-to-image generation, their models are
limited in exploring image perception and manipulation tasks, which are
urgently desired by users for wide applications. Recently, OpenAI released
their powerful GPT-4o-Image model for comprehensive image perception and
manipulation, achieving expressive capability and attracting community
interests. By observing the performance of GPT-4o-Image in our carefully
constructed experiments, we infer that GPT-4o-Image leverages features
extracted by semantic encoders instead of VAE, while VAEs are considered
essential components in many image manipulation models. Motivated by such
inspiring observations, we present a unified generative framework named
UniWorld based on semantic features provided by powerful visual-language models
and contrastive semantic encoders. As a result, we build a strong unified model
using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on
image editing benchmarks. UniWorld also maintains competitive image
understanding and generation capabilities, achieving strong performance across
multiple image perception tasks. We fully open-source our models, including
model weights, training and evaluation scripts, and datasets.

</details>


### [352] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/pdf/2506.02964)
*Guiqiu Liao, Matjaz Jogan, Eric Eaton, Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: FORLA introduces a federated learning framework for unsupervised object-centric representation learning using slot attention, outperforming centralized baselines in object discovery and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: The challenge of learning efficient visual representations across heterogeneous unlabeled datasets in federated learning, requiring jointly informative features while disentangling domain-specific factors.

Method: FORLA uses a shared feature adapter and slot attention module, trained via a two-branch student-teacher architecture to reconstruct and align features across clients.

Result: Outperforms centralized baselines in object discovery and learns compact, universal representations that generalize well across domains.

Conclusion: Federated slot attention is effective for scalable, unsupervised visual representation learning from distributed cross-domain data.

Abstract: Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.

</details>


### [353] [DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models](https://arxiv.org/pdf/2506.03007)
*Jiarui Wang, Huiyu Duan, Juntong Wang, Ziheng Jia, Woo Yi Yang, Xiaorong Zhu, Yu Zhao, Jiaying Qian, Yuke Xing, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces DFBench, a large-scale deepfake benchmark, and MoA-DF, a detection method using multiple LMMs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of verifying digital content authenticity due to the rapid advancement of highly realistic AI-generated images.

Method: Develops DFBench with diverse datasets and proposes MoA-DF, leveraging multiple LMMs for detection.

Result: MoA-DF achieves state-of-the-art performance in deepfake detection.

Conclusion: LMMs are effective for deepfake detection, and DFBench provides a robust benchmark for future research.

Abstract: With the rapid advancement of generative models, the realism of AI-generated
images has significantly improved, posing critical challenges for verifying
digital content authenticity. Current deepfake detection methods often depend
on datasets with limited generation models and content diversity that fail to
keep pace with the evolving complexity and increasing realism of the
AI-generated content. Large multimodal models (LMMs), widely adopted in various
vision tasks, have demonstrated strong zero-shot capabilities, yet their
potential in deepfake detection remains largely unexplored. To bridge this gap,
we present \textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)
broad diversity, including 540,000 images across real, AI-edited, and
AI-generated content, (ii) latest model, the fake images are generated by 12
state-of-the-art generation models, and (iii) bidirectional benchmarking and
evaluating for both the detection accuracy of deepfake detectors and the
evasion capability of generative models. Based on DFBench, we propose
\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a
combined probability strategy from multiple LMMs. MoA-DF achieves
state-of-the-art performance, further proving the effectiveness of leveraging
LMMs for deepfake detection. Database and codes are publicly available at
https://github.com/IntMeGroup/DFBench.

</details>


### [354] [EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models](https://arxiv.org/pdf/2506.03067)
*Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai*

Main category: cs.CV

TL;DR: The paper introduces a prompt inversion technique called \sys for text-to-image diffusion models, improving image similarity, textual alignment, and interpretability over existing methods.


<details>
  <summary>Details</summary>
Motivation: Prompt inversion is valuable for data attribution, model provenance, and watermarking validation, but current methods struggle with semantic fluency and efficiency.

Method: The technique initializes embeddings using a pre-trained image captioning model, refines them via reverse-engineering in latent space, and converts them to text with an embedding-to-text model.

Result: Experiments on MS COCO, LAION, and Flickr show superior performance in image similarity, textual alignment, and interpretability.

Conclusion: The method's effectiveness is demonstrated in tasks like cross-concept image synthesis and unsupervised segmentation, highlighting its broad applicability.

Abstract: Text-to-image generation models~(e.g., Stable Diffusion) have achieved
significant advancements, enabling the creation of high-quality and realistic
images based on textual descriptions. Prompt inversion, the task of identifying
the textual prompt used to generate a specific artifact, holds significant
potential for applications including data attribution, model provenance, and
watermarking validation. Recent studies introduced a delayed projection scheme
to optimize for prompts representative of the vocabulary space, though
challenges in semantic fluency and efficiency remain. Advanced image captioning
models or visual large language models can generate highly interpretable
prompts, but they often lack in image similarity. In this paper, we propose a
prompt inversion technique called \sys for text-to-image diffusion models,
which includes initializing embeddings using a pre-trained image captioning
model, refining them through reverse-engineering in the latent space, and
converting them to texts using an embedding-to-text model. Our experiments on
the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our
method outperforms existing methods in terms of image similarity, textual
alignment, prompt interpretability and generalizability. We further illustrate
the application of our generated prompts in tasks such as cross-concept image
synthesis, concept manipulation, evolutionary multi-concept generation and
unsupervised segmentation.

</details>


### [355] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/pdf/2506.03139)
*Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang*

Main category: cs.CV

TL;DR: SVGenius is a benchmark for evaluating SVG processing in LLMs, covering 2,377 queries across understanding, editing, and generation tasks. It reveals proprietary models outperform open-source ones, with reasoning-enhanced training being more effective than scaling.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for SVG processing lack real-world coverage, complexity stratification, and unified evaluation. SVGenius addresses these gaps.

Method: SVGenius includes 2,377 queries across 24 domains, evaluated through 8 task categories and 18 metrics, testing 22 models of varying scales and architectures.

Result: Proprietary models outperform open-source ones, but all degrade with complexity. Reasoning-enhanced training helps, but style transfer remains challenging.

Conclusion: SVGenius provides a systematic framework for evaluating SVG processing, offering insights for improving vector graphics models and graphic design automation.

Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising
capabilities for SVG processing, yet existing benchmarks suffer from limited
real-world coverage, lack of complexity stratification, and fragmented
evaluation paradigms. We introduce SVGenius, a comprehensive benchmark
comprising 2,377 queries across three progressive dimensions: understanding,
editing, and generation. Built on real-world data from 24 application domains
with systematic complexity stratification, SVGenius evaluates models through 8
task categories and 18 metrics. We assess 22 mainstream models spanning
different scales, architectures, training paradigms, and accessibility levels.
Our analysis reveals that while proprietary models significantly outperform
open-source counterparts, all models exhibit systematic performance degradation
with increasing complexity, indicating fundamental limitations in current
approaches; however, reasoning-enhanced training proves more effective than
pure scaling for overcoming these limitations, though style transfer remains
the most challenging capability across all model types. SVGenius establishes
the first systematic evaluation framework for SVG processing, providing crucial
insights for developing more capable vector graphics models and advancing
automated graphic design applications. Appendix and supplementary materials
(including all data and code) are available at
https://zju-real.github.io/SVGenius.

</details>


### [356] [LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM](https://arxiv.org/pdf/2506.03073)
*Roman Titkov, Egor Zubkov, Dmitry Yudin, Jaafar Mahmoud, Malik Mohrat, Gennady Sidorov*

Main category: cs.CV

TL;DR: LEG-SLAM integrates Gaussian Splatting with visual-language features for real-time semantic 3D SLAM, achieving high fps and competitive rendering quality.


<details>
  <summary>Details</summary>
Motivation: The challenge of integrating semantic information into Gaussian Splatting for real-time SLAM applications.

Method: Combines optimized Gaussian Splatting, DINOv2 for feature extraction, and a PCA-based feature compressor for online dense SLAM.

Result: Achieves 10+ fps on Replica and 18 fps on ScanNet, outperforming state-of-the-art in speed with competitive quality.

Conclusion: LEG-SLAM advances real-time semantic 3D SLAM, suitable for robotics and AR, without needing pre-computed data.

Abstract: Modern Gaussian Splatting methods have proven highly effective for real-time
photorealistic rendering of 3D scenes. However, integrating semantic
information into this representation remains a significant challenge,
especially in maintaining real-time performance for SLAM (Simultaneous
Localization and Mapping) applications. In this work, we introduce LEG-SLAM --
a novel approach that fuses an optimized Gaussian Splatting implementation with
visual-language feature extraction using DINOv2 followed by a learnable feature
compressor based on Principal Component Analysis, while enabling an online
dense SLAM. Our method simultaneously generates high-quality photorealistic
images and semantically labeled scene maps, achieving real-time scene
reconstruction with more than 10 fps on the Replica dataset and 18 fps on
ScanNet. Experimental results show that our approach significantly outperforms
state-of-the-art methods in reconstruction speed while achieving competitive
rendering quality. The proposed system eliminates the need for prior data
preparation such as camera's ego motion or pre-computed static semantic maps.
With its potential applications in autonomous robotics, augmented reality, and
other interactive domains, LEG-SLAM represents a significant step forward in
real-time semantic 3D Gaussian-based SLAM. Project page:
https://titrom025.github.io/LEG-SLAM/

</details>


### [357] [ORV: 4D Occupancy-centric Robot Video Generation](https://arxiv.org/pdf/2506.03079)
*Xiuyu Yang, Bohan Li, Shaocong Xu, Nan Wang, Chongjie Ye, Zhaoxi Chen, Minghan Qin, Yikang Ding, Xin Jin, Hang Zhao, Hao Zhao*

Main category: cs.CV

TL;DR: ORV is a framework for generating photorealistic robot videos using 4D semantic occupancy sequences, improving precision and generalization over existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional teleoperation for robotic simulation data is labor-intensive, and current generative models lack fine-grained control.

Method: ORV uses 4D semantic occupancy sequences for fine-grained guidance in video generation, ensuring temporal consistency and controllability.

Result: ORV outperforms baseline methods in generating multi-view robot gripping videos, enhancing downstream robotic learning.

Conclusion: ORV provides a scalable and precise solution for generating realistic robot simulation videos, addressing limitations of prior methods.

Abstract: Acquiring real-world robotic simulation data through teleoperation is
notoriously time-consuming and labor-intensive. Recently, action-driven
generative models have gained widespread adoption in robot learning and
simulation, as they eliminate safety concerns and reduce maintenance efforts.
However, the action sequences used in these methods often result in limited
control precision and poor generalization due to their globally coarse
alignment. To address these limitations, we propose ORV, an Occupancy-centric
Robot Video generation framework, which utilizes 4D semantic occupancy
sequences as a fine-grained representation to provide more accurate semantic
and geometric guidance for video generation. By leveraging occupancy-based
representations, ORV enables seamless translation of simulation data into
photorealistic robot videos, while ensuring high temporal consistency and
precise controllability. Furthermore, our framework supports the simultaneous
generation of multi-view videos of robot gripping operations - an important
capability for downstream robotic learning tasks. Extensive experimental
results demonstrate that ORV consistently outperforms existing baseline methods
across various datasets and sub-tasks. Demo, Code and Model:
https://orangesodahub.github.io/ORV

</details>


### [358] [SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis](https://arxiv.org/pdf/2506.03082)
*Ssharvien Kumar Sivakumar, Yannik Frisch, Ghazal Ghazaei, Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: SG2VID is a diffusion-based video model using Scene Graphs for precise surgical video synthesis and fine-grained human control, outperforming prior methods and enabling generative augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing surgical simulators lack photorealism and human anatomy variability, and current generative models neglect fine-grained human control.

Method: SG2VID leverages Scene Graphs for precise video synthesis and control, tested on cataract and cholecystectomy surgery datasets.

Result: SG2VID outperforms previous methods, enables precise control over tools and anatomy, and improves downstream tasks via synthetic data augmentation.

Conclusion: SG2VID addresses gaps in surgical simulation, offering photorealism, control, and augmentation potential for training and rare case generation.

Abstract: Surgical simulation plays a pivotal role in training novice surgeons,
accelerating their learning curve and reducing intra-operative errors. However,
conventional simulation tools fall short in providing the necessary
photorealism and the variability of human anatomy. In response, current methods
are shifting towards generative model-based simulators. Yet, these approaches
primarily focus on using increasingly complex conditioning for precise
synthesis while neglecting the fine-grained human control aspect. To address
this gap, we introduce SG2VID, the first diffusion-based video model that
leverages Scene Graphs for both precise video synthesis and fine-grained human
control. We demonstrate SG2VID's capabilities across three public datasets
featuring cataract and cholecystectomy surgery. While SG2VID outperforms
previous methods both qualitatively and quantitatively, it also enables precise
synthesis, providing accurate control over tool and anatomy's size and
movement, entrance of new tools, as well as the overall scene layout. We
qualitatively motivate how SG2VID can be used for generative augmentation and
present an experiment demonstrating its ability to improve a downstream phase
detection task when the training set is extended with our synthetic videos.
Finally, to showcase SG2VID's ability to retain human control, we interact with
the Scene Graphs to generate new video samples depicting major yet rare
intra-operative irregularities.

</details>


### [359] [InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba](https://arxiv.org/pdf/2506.03084)
*Zizhao Wu, Yingying Sun, Yiming Chen, Xiaoling Gu, Ruyu Liu, Jiazhou Chen*

Main category: cs.CV

TL;DR: Proposes an efficient human-human interaction generation method using the Mamba framework, outperforming baseline methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addresses scalability and efficiency challenges in transformer-based motion synthesis for human-human interactions.

Method: Introduces an adaptive spatio-temporal Mamba framework with two parallel SSM branches and key modules for feature learning.

Result: Achieves state-of-the-art results with 66M parameters (36% of baseline) and 0.57s inference speed (46% of baseline).

Conclusion: The method effectively captures long-sequence dependencies and interactions, offering real-time feedback with high efficiency.

Abstract: Human-human interaction generation has garnered significant attention in
motion synthesis due to its vital role in understanding humans as social
beings. However, existing methods typically rely on transformer-based
architectures, which often face challenges related to scalability and
efficiency. To address these issues, we propose a novel, efficient human-human
interaction generation method based on the Mamba framework, designed to meet
the demands of effectively capturing long-sequence dependencies while providing
real-time feedback. Specifically, we introduce an adaptive spatio-temporal
Mamba framework that utilizes two parallel SSM branches with an adaptive
mechanism to integrate the spatial and temporal features of motion sequences.
To further enhance the model's ability to capture dependencies within
individual motion sequences and the interactions between different individual
sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba
module and the cross-adaptive spatio-temporal Mamba module, enabling efficient
feature learning. Extensive experiments demonstrate that our method achieves
state-of-the-art results on two interaction datasets with remarkable quality
and efficiency. Compared to the baseline method InterGen, our approach not only
improves accuracy but also requires a minimal parameter size of just 66M ,only
36% of InterGen's, while achieving an average inference speed of 0.57 seconds,
which is 46% of InterGen's execution time.

</details>


### [360] [Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness](https://arxiv.org/pdf/2506.03089)
*Lucas Piper, Arlindo L. Oliveira, Tiago Marques*

Main category: cs.CV

TL;DR: EVNets, hybrid CNNs combining VOneBlock and SubcorticalBlock, improve robustness and alignment with biological vision, outperforming standard CNNs by 8.5% on robustness benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address CNN vulnerability to visual perturbations and out-of-domain images by mimicking biological vision (V1 and subcortical responses).

Method: Introduce EVNets: hybrid CNNs with VOneBlock (V1 mimic) and SubcorticalBlock (subcortical mimic). Tested on robustness benchmarks.

Result: Improved V1 alignment, stronger shape bias, and 8.5% better robustness. Further gains (7.3%) with data augmentation.

Conclusion: Combining biologically inspired architecture with training-based methods enhances CNN robustness.

Abstract: Convolutional neural networks (CNNs) trained on object recognition achieve
high task performance but continue to exhibit vulnerability under a range of
visual perturbations and out-of-domain images, when compared with biological
vision. Prior work has demonstrated that coupling a standard CNN with a
front-end block (VOneBlock) that mimics the primate primary visual cortex (V1)
can improve overall model robustness. Expanding on this, we introduce Early
Vision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock
with a novel SubcorticalBlock, whose architecture draws from computational
models in neuroscience and is parameterized to maximize alignment with
subcortical responses reported across multiple experimental studies. Without
being optimized to do so, the assembly of the SubcorticalBlock with the
VOneBlock improved V1 alignment across most standard V1 benchmarks, and better
modeled extra-classical receptive field phenomena. In addition, EVNets exhibit
stronger emergent shape bias and overperform the base CNN architecture by 8.5%
on an aggregate benchmark of robustness evaluations, including adversarial
perturbations, common corruptions, and domain shifts. Finally, we show that
EVNets can be further improved when paired with a state-of-the-art data
augmentation technique, surpassing the performance of the isolated data
augmentation approach by 7.3% on our robustness benchmark. This result reveals
complementary benefits between changes in architecture to better mimic biology
and training-based machine learning approaches.

</details>


### [361] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/pdf/2506.03096)
*Christian Schlarmann, Francesco Croce, Nicolas Flammarion, Matthias Hein*

Main category: cs.CV

TL;DR: FuseLIP introduces a single transformer model for multimodal embedding, enabling early fusion of text and image tokens, outperforming late fusion methods in tasks like VQA.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive pre-training methods lack native support for multimodal inputs, requiring additional modules for feature merging.

Method: Uses a single transformer model with an extended vocabulary of text and image tokens for early fusion, allowing deeper modality interaction.

Result: FuseLIP outperforms baselines in multimodal tasks (e.g., VQA) and matches performance in unimodal tasks.

Conclusion: Early fusion via FuseLIP provides richer multimodal representations, improving performance in complex tasks.

Abstract: Contrastive language-image pre-training aligns the features of text-image
pairs in a common latent space via distinct encoders for each modality. While
this approach achieves impressive performance in several zero-shot tasks, it
cannot natively handle multimodal inputs, i.e., encoding image and text into a
single feature vector. As a remedy, it is common practice to use additional
modules to merge the features extracted by the unimodal encoders. In this work,
we present FuseLIP, an alternative architecture for multimodal embedding.
Leveraging recent progress in discrete image tokenizers, we propose to use a
single transformer model which operates on an extended vocabulary of text and
image tokens. This early fusion approach allows the different modalities to
interact at each depth of encoding and obtain richer representations compared
to common late fusion. We collect new datasets for multimodal pre-training and
evaluation, designing challenging tasks for multimodal encoder models. We show
that FuseLIP outperforms other approaches in multimodal embedding tasks such as
VQA and text-guided image transformation retrieval, while being comparable to
baselines on unimodal tasks.

</details>


### [362] [DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation](https://arxiv.org/pdf/2506.03103)
*Xiaoyan Cong, Angela Xing, Chandradeep Pokhariya, Rao Fu, Srinath Sridhar*

Main category: cs.CV

TL;DR: DyTact is a markerless capture method for dynamic hand-object contact reconstruction, addressing challenges like occlusions and complex surfaces with 2D Gaussian surfels and MANO meshes.


<details>
  <summary>Details</summary>
Motivation: Dynamic hand-object contact reconstruction is vital for AI animation, XR, and robotics but is hindered by occlusions, surface details, and capture limitations.

Method: DyTact uses 2D Gaussian surfels bound to MANO meshes, a refinement module for high-frequency deformations, and adaptive sampling for contact regions.

Result: DyTact achieves state-of-the-art contact estimation accuracy, improves novel view synthesis, and operates efficiently.

Conclusion: DyTact offers a non-intrusive, accurate, and efficient solution for dynamic hand-object contact capture.

Abstract: Reconstructing dynamic hand-object contacts is essential for realistic
manipulation in AI character animation, XR, and robotics, yet it remains
challenging due to heavy occlusions, complex surface details, and limitations
in existing capture techniques. In this paper, we introduce DyTact, a
markerless capture method for accurately capturing dynamic contact in
hand-object manipulations in a non-intrusive manner. Our approach leverages a
dynamic, articulated representation based on 2D Gaussian surfels to model
complex manipulations. By binding these surfels to MANO meshes, DyTact
harnesses the inductive bias of template models to stabilize and accelerate
optimization. A refinement module addresses time-dependent high-frequency
deformations, while a contact-guided adaptive sampling strategy selectively
increases surfel density in contact regions to handle heavy occlusion.
Extensive experiments demonstrate that DyTact not only achieves
state-of-the-art dynamic contact estimation accuracy but also significantly
improves novel view synthesis quality, all while operating with fast
optimization and efficient memory usage. Project Page:
https://oliver-cong02.github.io/DyTact.github.io/ .

</details>


### [363] [ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions](https://arxiv.org/pdf/2506.03107)
*Di Chang, Mingdeng Cao, Yichun Shi, Bo Liu, Shengqu Cai, Shijie Zhou, Weilin Huang, Gordon Wetzstein, Mohammad Soleymani, Peng Wang*

Main category: cs.CV

TL;DR: ByteMorph introduces a framework for instruction-based image editing focusing on non-rigid motions, including a dataset (ByteMorph-6M) and a model (ByteMorpher).


<details>
  <summary>Details</summary>
Motivation: Existing methods and datasets lack support for expressive edits involving dynamic motion, limiting their applicability.

Method: ByteMorph uses a Diffusion Transformer (DiT) model and a large-scale dataset (ByteMorph-6M) generated via motion-guided techniques and automated captioning.

Result: The framework includes a 6M-pair dataset and a benchmark (ByteMorph-Bench), covering diverse non-rigid motions.

Conclusion: ByteMorph addresses the gap in handling non-rigid motions in image editing and provides a robust dataset and model for future research.

Abstract: Editing images with instructions to reflect non-rigid motions, camera
viewpoint shifts, object deformations, human articulations, and complex
interactions, poses a challenging yet underexplored problem in computer vision.
Existing approaches and datasets predominantly focus on static scenes or rigid
transformations, limiting their capacity to handle expressive edits involving
dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive
framework for instruction-based image editing with an emphasis on non-rigid
motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong
baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.
ByteMorph-6M includes over 6 million high-resolution image editing pairs for
training, along with a carefully curated evaluation benchmark ByteMorph-Bench.
Both capture a wide variety of non-rigid motion types across diverse
environments, human figures, and object categories. The dataset is constructed
using motion-guided data generation, layered compositing techniques, and
automated captioning to ensure diversity, realism, and semantic coherence. We
further conduct a comprehensive evaluation of recent instruction-based image
editing methods from both academic and commercial domains.

</details>


### [364] [Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning](https://arxiv.org/pdf/2506.03110)
*Shuai Yi, Yixiong Zou, Yuhua Li, Ruixuan Li*

Main category: cs.CV

TL;DR: The paper investigates the role of image token continuity in Vision Transformers (ViT) for Cross-Domain Few-Shot Learning (CDFSL), finding that disrupting continuity harms general domain performance but minimally affects target domains. A method is proposed to exploit this for better domain adaptation.


<details>
  <summary>Details</summary>
Motivation: To understand why disrupting image token continuity in ViT affects performance differently across domains and to leverage this for improving CDFSL.

Method: Analyzes the impact of token continuity on ViT's generalization, then proposes a method to disrupt continuity to encourage reliance on smaller, more transferable patterns.

Result: Disrupting continuity reduces domain gaps and outperforms state-of-the-art methods in CDFSL.

Conclusion: Continuity aids in learning large spatial patterns, which are less transferable; disrupting it improves adaptation to distant domains by focusing on smaller patterns.

Abstract: Vision Transformer (ViT) has achieved remarkable success due to its
large-scale pretraining on general domains, but it still faces challenges when
applying it to downstream distant domains that have only scarce training data,
which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired
by Self-Attention's insensitivity to token orders, we find an interesting
phenomenon neglected in current works: disrupting the continuity of image
tokens (i.e., making pixels not smoothly transited across patches) in ViT leads
to a noticeable performance decline in the general (source) domain but only a
marginal decrease in downstream target domains. This questions the role of
image tokens' continuity in ViT's generalization under large domain gaps. In
this paper, we delve into this phenomenon for an interpretation. We find
continuity aids ViT in learning larger spatial patterns, which are harder to
transfer than smaller ones, enlarging domain distances. Meanwhile, it implies
that only smaller patterns within each patch could be transferred under extreme
domain gaps. Based on this interpretation, we further propose a simple yet
effective method for CDFSL that better disrupts the continuity of image tokens,
encouraging the model to rely less on large patterns and more on smaller ones.
Extensive experiments show the effectiveness of our method in reducing domain
gaps and outperforming state-of-the-art works. Codes and models are available
at https://github.com/shuaiyi308/ReCIT.

</details>


### [365] [Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery](https://arxiv.org/pdf/2506.03114)
*Michelle Chen, David Russell, Amritha Pallavoor, Derek Young, Jane Wu*

Main category: cs.CV

TL;DR: The paper explores using the Segment Anything Model 2 (SAM2) for zero-shot individual tree detection and segmentation in remote sensing imagery, showing promising results and synergy with specialized methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale tree delineation is vital for ecological research amid climate change, but current methods rely on hard-to-scale labeled data.

Method: Evaluates SAM2 for zero-shot segmentation and transfer using prompts from an existing tree detection model.

Result: SAM2 demonstrates strong generalization and synergy with domain-specific methods.

Conclusion: Pretrained models like SAM2 offer a promising path for advancing remote sensing applications.

Abstract: Large-scale delineation of individual trees from remote sensing imagery is
crucial to the advancement of ecological research, particularly as climate
change and other environmental factors rapidly transform forest landscapes
across the world. Current RGB tree segmentation methods rely on training
specialized machine learning models with labeled tree datasets. While these
learning-based approaches can outperform manual data collection when accurate,
the existing models still depend on training data that's hard to scale. In this
paper, we investigate the efficacy of using a state-of-the-art image
segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for
individual tree detection and segmentation. We evaluate a pretrained SAM2 model
on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot
transfer by using predictions from an existing tree detection model as prompts.
Our results suggest that SAM2 not only has impressive generalization
capabilities, but also can form a natural synergy with specialized methods
trained on in-domain labeled data. We find that applying large pretrained
models to problems in remote sensing is a promising avenue for future progress.
We make our code available at:
https://github.com/open-forest-observatory/tree-detection-framework.

</details>


### [366] [Targeted Forgetting of Image Subgroups in CLIP Models](https://arxiv.org/pdf/2506.03117)
*Zeliang Zhang, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Chenliang Xu*

Main category: cs.CV

TL;DR: The paper proposes a three-stage method for fine-grained unlearning in foundation models like CLIP, addressing harmful knowledge without pre-trained data access, while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Foundation models inherit harmful knowledge from noisy datasets, but existing unlearning methods lack fine-grained control and require pre-trained data.

Method: A three-stage approach: forgetting (fine-tuning on forgotten samples), reminding (restoring retained samples), and restoring (recovering zero-shot capabilities via model souping), with knowledge distillation for distribution disparity.

Result: Effective unlearning of specific subgroups while maintaining zero-shot performance, outperforming baselines on CIFAR-10, ImageNet-1K, and style datasets.

Conclusion: The method enables fine-grained unlearning in CLIP without pre-trained data, preserving performance and outperforming existing approaches.

Abstract: Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot
performance across various tasks by leveraging large-scale, unsupervised
pre-training. However, they often inherit harmful or unwanted knowledge from
noisy internet-sourced datasets, compromising their reliability in real-world
applications. Existing model unlearning methods either rely on access to
pre-trained datasets or focus on coarse-grained unlearning (e.g., entire
classes), leaving a critical gap for fine-grained unlearning. In this paper, we
address the challenging scenario of selectively forgetting specific portions of
knowledge within a class, without access to pre-trained data, while preserving
the model's overall performance. We propose a novel three-stage approach that
progressively unlearns targeted knowledge while mitigating over-forgetting. It
consists of (1) a forgetting stage to fine-tune the CLIP on samples to be
forgotten, (2) a reminding stage to restore performance on retained samples,
and (3) a restoring stage to recover zero-shot capabilities using model
souping. Additionally, we introduce knowledge distillation to handle the
distribution disparity between forgetting, retaining samples, and unseen
pre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style
datasets demonstrate that our approach effectively unlearns specific subgroups
while maintaining strong zero-shot performance on semantically similar
subgroups and other categories, significantly outperforming baseline unlearning
methods, which lose effectiveness under the CLIP unlearning setting.

</details>


### [367] [Controllable Human-centric Keyframe Interpolation with Generative Prior](https://arxiv.org/pdf/2506.03119)
*Zujin Guo, Size Wu, Zhongang Cai, Wei Li, Chen Change Loy*

Main category: cs.CV

TL;DR: PoseFuse3D-KI integrates 3D human guidance into video diffusion for better keyframe interpolation, outperforming baselines with improved metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack 3D geometric guidance, leading to poor results for complex human motions and limited control.

Method: PoseFuse3D-KI uses a 3D-informed control model with a novel SMPL-X encoder and fusion network to integrate 3D cues into 2D latent space.

Result: Achieves 9% higher PSNR and 38% lower LPIPS than baselines on the CHKI-Video dataset.

Conclusion: PoseFuse3D-KI enhances interpolation fidelity by leveraging 3D guidance, validated by comprehensive ablations.

Abstract: Existing interpolation methods use pre-trained video diffusion priors to
generate intermediate frames between sparsely sampled keyframes. In the absence
of 3D geometric guidance, these methods struggle to produce plausible results
for complex, articulated human motions and offer limited control over the
synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe
Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human
guidance signals into the diffusion process for Controllable Human-centric
Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for
interpolation, our PoseFuse3D, a 3D-informed control model, features a novel
SMPL-X encoder that transforms 3D geometry and shape into the 2D latent
conditioning space, alongside a fusion network that integrates these 3D cues
with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset
annotated with both 2D poses and 3D SMPL-X parameters. We show that
PoseFuse3D-KI consistently outperforms state-of-the-art baselines on
CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.
Comprehensive ablations demonstrate that our PoseFuse3D model improves
interpolation fidelity.

</details>


### [368] [DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation](https://arxiv.org/pdf/2506.03123)
*Zhengyao Lv, Chenyang Si, Tianlin Pan, Zhaoxi Chen, Kwan-Yee K. Wong, Yu Qiao, Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces a Dual-Expert Consistency Model (DCM) to address the computational overhead and quality degradation in video diffusion models by specializing tasks between semantic and detail experts, achieving state-of-the-art results with fewer sampling steps.


<details>
  <summary>Details</summary>
Motivation: Current Consistency Models accelerate diffusion models but degrade temporal consistency and appearance details in videos due to conflicting learning dynamics during distillation.

Method: Proposes DCM with a semantic expert for layout/motion and a detail expert for refinement, using Temporal Coherence Loss and GAN/Feature Matching Loss.

Result: Achieves high visual quality with reduced steps, outperforming existing methods.

Conclusion: Expert specialization in DCM effectively balances efficiency and quality in video diffusion model distillation.

Abstract: Diffusion Models have achieved remarkable results in video synthesis but
require iterative denoising steps, leading to substantial computational
overhead. Consistency Models have made significant progress in accelerating
diffusion models. However, directly applying them to video diffusion models
often results in severe degradation of temporal consistency and appearance
details. In this paper, by analyzing the training dynamics of Consistency
Models, we identify a key conflicting learning dynamics during the distillation
process: there is a significant discrepancy in the optimization gradients and
loss contributions across different timesteps. This discrepancy prevents the
distilled student model from achieving an optimal state, leading to compromised
temporal consistency and degraded appearance details. To address this issue, we
propose a parameter-efficient \textbf{Dual-Expert Consistency Model~(DCM)},
where a semantic expert focuses on learning semantic layout and motion, while a
detail expert specializes in fine detail refinement. Furthermore, we introduce
Temporal Coherence Loss to improve motion consistency for the semantic expert
and apply GAN and Feature Matching Loss to enhance the synthesis quality of the
detail expert.Our approach achieves state-of-the-art visual quality with
significantly reduced sampling steps, demonstrating the effectiveness of expert
specialization in video diffusion model distillation. Our code and models are
available at
\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.

</details>


### [369] [AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation](https://arxiv.org/pdf/2506.03126)
*Lu Qiu, Yizhuo Li, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu*

Main category: cs.CV

TL;DR: AnimeShooter is a dataset for reference-guided multi-shot animation generation, featuring hierarchical annotations and visual consistency. It includes story-level and shot-level details, with an audio subset. The baseline model, AnimeShooterGen, uses MLLMs and diffusion models for coherent video generation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack reference images and coherent multi-shot video annotations for animation production, hindering AI-generated content (AIGC) in this domain.

Method: AnimeShooter dataset provides hierarchical annotations (story-level and shot-level) and visual consistency. AnimeShooterGen uses MLLMs and video diffusion models for reference-guided multi-shot generation.

Result: The model trained on AnimeShooter achieves superior visual consistency and adherence to reference guidance, demonstrating the dataset's effectiveness.

Conclusion: AnimeShooter bridges the gap in animation datasets and supports coherent video generation, advancing AIGC in animation production.

Abstract: Recent advances in AI-generated content (AIGC) have significantly accelerated
animation production. To produce engaging animations, it is essential to
generate coherent multi-shot video clips with narrative scripts and character
references. However, existing public datasets primarily focus on real-world
scenarios with global descriptions, and lack reference images for consistent
character guidance. To bridge this gap, we present AnimeShooter, a
reference-guided multi-shot animation dataset. AnimeShooter features
comprehensive hierarchical annotations and strong visual consistency across
shots through an automated pipeline. Story-level annotations provide an
overview of the narrative, including the storyline, key scenes, and main
character profiles with reference images, while shot-level annotations
decompose the story into consecutive shots, each annotated with scene,
characters, and both narrative and descriptive visual captions. Additionally, a
dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each
shot, along with audio descriptions and sound sources. To demonstrate the
effectiveness of AnimeShooter and establish a baseline for the reference-guided
multi-shot video generation task, we introduce AnimeShooterGen, which leverages
Multimodal Large Language Models (MLLMs) and video diffusion models. The
reference image and previously generated shots are first processed by MLLM to
produce representations aware of both reference and context, which are then
used as the condition for the diffusion model to decode the subsequent shot.
Experimental results show that the model trained on AnimeShooter achieves
superior cross-shot visual consistency and adherence to reference visual
guidance, which highlight the value of our dataset for coherent animated video
generation.

</details>


### [370] [Native-Resolution Image Synthesis](https://arxiv.org/pdf/2506.03131)
*Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, Yiyuan Zhang*

Main category: cs.CV

TL;DR: Native-resolution image synthesis introduces a method to generate images at any resolution/aspect ratio, overcoming fixed-resolution limitations with the NiT architecture.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed-resolution, square-image methods in generative modeling.

Method: Introduces the Native-resolution diffusion Transformer (NiT), designed to handle variable resolutions/aspect ratios in denoising.

Result: Achieves state-of-the-art performance on ImageNet benchmarks and demonstrates zero-shot generalization to unseen resolutions/aspect ratios.

Conclusion: Native-resolution modeling bridges visual generative modeling with advanced LLM methodologies, showing significant potential.

Abstract: We introduce native-resolution image synthesis, a novel generative modeling
paradigm that enables the synthesis of images at arbitrary resolutions and
aspect ratios. This approach overcomes the limitations of conventional
fixed-resolution, square-image methods by natively handling variable-length
visual tokens, a core challenge for traditional techniques. To this end, we
introduce the Native-resolution diffusion Transformer (NiT), an architecture
designed to explicitly model varying resolutions and aspect ratios within its
denoising process. Free from the constraints of fixed formats, NiT learns
intrinsic visual distributions from images spanning a broad range of
resolutions and aspect ratios. Notably, a single NiT model simultaneously
achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512
benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in
advanced large language models, NiT, trained solely on ImageNet, demonstrates
excellent zero-shot generalization performance. It successfully generates
high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)
and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These
findings indicate the significant potential of native-resolution modeling as a
bridge between visual generative modeling and advanced LLM methodologies.

</details>


### [371] [CamCloneMaster: Enabling Reference-based Camera Control for Video Generation](https://arxiv.org/pdf/2506.03140)
*Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Tianfan Xue*

Main category: cs.CV

TL;DR: CamCloneMaster is a framework for intuitive camera control in videos by replicating movements from reference videos, eliminating the need for explicit camera parameters or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing camera control methods require cumbersome explicit camera parameter sequences, making intricate movements difficult for users.

Method: CamCloneMaster replicates camera movements from reference videos without needing camera parameters or test-time fine-tuning, supporting Image-to-Video and Video-to-Video tasks.

Result: Outperforms existing methods in camera controllability and visual quality, validated by experiments and user studies.

Conclusion: CamCloneMaster offers a more intuitive and effective approach to camera control, supported by a large-scale synthetic dataset.

Abstract: Camera control is crucial for generating expressive and cinematic videos.
Existing methods rely on explicit sequences of camera parameters as control
conditions, which can be cumbersome for users to construct, particularly for
intricate camera movements. To provide a more intuitive camera control method,
we propose CamCloneMaster, a framework that enables users to replicate camera
movements from reference videos without requiring camera parameters or
test-time fine-tuning. CamCloneMaster seamlessly supports reference-based
camera control for both Image-to-Video and Video-to-Video tasks within a
unified framework. Furthermore, we present the Camera Clone Dataset, a
large-scale synthetic dataset designed for camera clone learning, encompassing
diverse scenes, subjects, and camera movements. Extensive experiments and user
studies demonstrate that CamCloneMaster outperforms existing methods in terms
of both camera controllability and visual quality.

</details>


### [372] [Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval](https://arxiv.org/pdf/2506.03141)
*Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Xihui Liu*

Main category: cs.CV

TL;DR: Context-as-Memory improves long video generation by using historical context as memory, with efficient frame storage and retrieval, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack scene-consistent memory in long video generation due to limited historical context use.

Method: Proposes Context-as-Memory with frame-based context storage and a Memory Retrieval module for efficient frame selection.

Result: Outperforms SOTAs in memory capabilities, even in unseen open-domain scenarios.

Conclusion: Context-as-Memory effectively enhances long video generation with superior memory utilization.

Abstract: Recent advances in interactive video generation have shown promising results,
yet existing approaches struggle with scene-consistent memory capabilities in
long video generation due to limited use of historical context. In this work,
we propose Context-as-Memory, which utilizes historical context as memory for
video generation. It includes two simple yet effective designs: (1) storing
context in frame format without additional post-processing; (2) conditioning by
concatenating context and frames to be predicted along the frame dimension at
the input, requiring no external control modules. Furthermore, considering the
enormous computational overhead of incorporating all historical context, we
propose the Memory Retrieval module to select truly relevant context frames by
determining FOV (Field of View) overlap between camera poses, which
significantly reduces the number of candidate frames without substantial
information loss. Experiments demonstrate that Context-as-Memory achieves
superior memory capabilities in interactive long video generation compared to
SOTAs, even generalizing effectively to open-domain scenarios not seen during
training. The link of our project page is https://context-as-memory.github.io/.

</details>


### [373] [Self-Supervised Spatial Correspondence Across Modalities](https://arxiv.org/pdf/2506.03148)
*Ayush Shrivastava, Andrew Owens*

Main category: cs.CV

TL;DR: A method for cross-modal space-time correspondences between images of different modalities (e.g., RGB and depth) without needing aligned data.


<details>
  <summary>Details</summary>
Motivation: To identify corresponding pixels across different visual modalities without relying on photo-consistency assumptions or labeled data.

Method: Extends the contrastive random walk framework to learn cycle-consistent features for cross-modal and intra-modal matching.

Result: Achieves strong performance on geometric (RGB-to-depth, RGB-to-thermal) and semantic (photo-sketch, cross-style) tasks.

Conclusion: The model is simple, trainable with unlabeled data, and effective for diverse cross-modal matching tasks.

Abstract: We present a method for finding cross-modal space-time correspondences. Given
two images from different visual modalities, such as an RGB image and a depth
map, our model identifies which pairs of pixels correspond to the same physical
points in the scene. To solve this problem, we extend the contrastive random
walk framework to simultaneously learn cycle-consistent feature representations
for both cross-modal and intra-modal matching. The resulting model is simple
and has no explicit photo-consistency assumptions. It can be trained entirely
using unlabeled data, without the need for any spatially aligned multimodal
image pairs. We evaluate our method on both geometric and semantic
correspondence tasks. For geometric matching, we consider challenging tasks
such as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic
matching, we evaluate on photo-sketch and cross-style image alignment. Our
method achieves strong performance across all benchmarks.

</details>


### [374] [Scene Structure Guidance Network: Unfolding Graph Partitioning into Pixel-Wise Feature Learning](https://arxiv.org/pdf/2301.00555)
*Jisu Shin, Seunghyun Shin, Hae-Gon Jeon*

Main category: cs.CV

TL;DR: A lightweight neural network (SSGNet) extracts task-specific scene structures for low-level vision tasks, achieving state-of-the-art results without supervision.


<details>
  <summary>Details</summary>
Motivation: Defining informative scene structures is challenging due to task-specific visual feature influences. A general solution is needed.

Method: SSGNet unfolds traditional spectral clustering into a learnable network, producing explicit feature representations. It uses unsupervised training with novel losses.

Result: SSGNet achieves state-of-the-art performance, generalizes well, and includes a lighter version (SSGNet-D) for edge devices.

Conclusion: SSGNet is a simple, effective, and adaptable solution for task-specific scene structure extraction in low-level vision.

Abstract: Understanding the informative structures of scenes is essential for low-level
vision tasks. Unfortunately, it is difficult to obtain a concrete visual
definition of the informative structures because influences of visual features
are task-specific. In this paper, we propose a single general neural network
architecture for extracting task-specific structure guidance for scenes. To do
this, we first analyze traditional spectral clustering methods, which computes
a set of eigenvectors to model a segmented graph forming small compact
structures on image domains. We then unfold the traditional graph-partitioning
problem into a learnable network, named \textit{Scene Structure Guidance
Network (SSGNet)}, to represent the task-specific informative structures. The
SSGNet yields a set of coefficients of eigenvectors that produces explicit
feature representations of image structures. In addition, our SSGNet is
light-weight ($\sim$ 56K parameters), and can be used as a plug-and-play module
for off-the-shelf architectures. We optimize the SSGNet without any supervision
by proposing two novel training losses that enforce task-specific scene
structure generation during training. Our main contribution is to show that
such a simple network can achieve state-of-the-art results for several
low-level vision applications. We also demonstrate that our network generalizes
well on unseen datasets, compared to existing methods which use structural
embedding frameworks. We further propose a lighter version of SSGNet ($\sim$
29K parameters) for depth computation, SSGNet-D, and successfully execute it on
edge computing devices like Jetson AGX Orin, improving the performance of
baseline network, even in the wild, with little computational delay.

</details>


### [375] [Self-supervised Learning of Event-guided Video Frame Interpolation for Rolling Shutter Frames](https://arxiv.org/pdf/2306.15507)
*Yunfan Lu, Guoqiang Liang, Yiran Shen, Lin Wang*

Main category: cs.CV

TL;DR: A framework combining RS and event cameras to remove RS distortions and recover high-frame-rate GS videos, reducing bandwidth by 94% and achieving 16 ms per frame.


<details>
  <summary>Details</summary>
Motivation: Address RS distortions (skew, jelly effects) and bandwidth/frame rate limitations in consumer cameras by leveraging event cameras' high temporal resolution.

Method: Self-supervised strategy using a displacement field for mutual RS-GS reconstruction and slow-motion recovery, integrating RS frames and inverse mapping.

Result: Removes RS distortions, reduces bandwidth by 94%, and achieves 16 ms per frame at 32x interpolation on four datasets.

Conclusion: The framework effectively recovers GS videos with minimal distortion and high efficiency, demonstrating practical benefits for consumer cameras.

Abstract: Most consumer cameras use rolling shutter (RS) exposure, which often leads to
distortions such as skew and jelly effects. These videos are further limited by
bandwidth and frame rate constraints. In this paper, we explore the potential
of event cameras, which offer high temporal resolution. We propose a framework
to recover global shutter (GS) high-frame-rate videos without RS distortion by
combining an RS camera and an event camera. Due to the lack of real-world
datasets, our framework adopts a self-supervised strategy based on a
displacement field, a dense 3D spatiotemporal representation of pixel motion
during exposure. This enables mutual reconstruction between RS and GS frames
and facilitates slow-motion recovery. We combine RS frames with the
displacement field to generate GS frames, and integrate inverse mapping and RS
frame warping for self-supervision. Experiments on four datasets show that our
method removes distortion, reduces bandwidth usage by 94 percent, and achieves
16 ms per frame at 32x interpolation.

</details>


### [376] [Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset](https://arxiv.org/pdf/2309.13570)
*Zixun Huang, Keling Yao, Seth Z. Zhao, Chuanyu Pan, Allen Y. Yang*

Main category: cs.CV

TL;DR: DTTDNet, a transformer-based 6DoF pose estimation method, addresses robustness issues in existing RGBD-based methods by introducing a geometric feature filtering module and Chamfer distance loss. It outperforms state-of-the-art methods on the new DTTD-Mobile dataset, especially in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Existing 6DoF pose estimation methods struggle with depth sensor noise, leading to performance discrepancies. This paper aims to improve robustness against such noise.

Method: Proposes DTTDNet, featuring a geometric feature filtering module and Chamfer distance loss, and introduces the DTTD-Mobile dataset for noisy depth data.

Result: DTTDNet outperforms state-of-the-art methods by 4.32 to 60.74 points in ADD metrics on DTTD-Mobile, showing superior noise robustness.

Conclusion: DTTDNet sets a new benchmark for robustness in 6DoF pose estimation, particularly in noisy environments, with a publicly available project page.

Abstract: Robust 6DoF pose estimation with mobile devices is the foundation for
applications in robotics, augmented reality, and digital twin localization. In
this paper, we extensively investigate the robustness of existing RGBD-based
6DoF pose estimation methods against varying levels of depth sensor noise. We
highlight that existing 6DoF pose estimation methods suffer significant
performance discrepancies due to depth measurement inaccuracies. In response to
the robustness issue, we present a simple and effective transformer-based 6DoF
pose estimation approach called DTTDNet, featuring a novel geometric feature
filtering module and a Chamfer distance loss for training. Moreover, we advance
the field of robust 6DoF pose estimation and introduce a new dataset -- Digital
Twin Tracking Dataset Mobile (DTTD-Mobile), tailored for digital twin object
tracking with noisy depth data from the mobile RGBD sensor suite of the Apple
iPhone 14 Pro. Extensive experiments demonstrate that DTTDNet significantly
outperforms state-of-the-art methods at least 4.32, up to 60.74 points in ADD
metrics on the DTTD-Mobile. More importantly, our approach exhibits superior
robustness to varying levels of measurement noise, setting a new benchmark for
robustness to measurement noise. The project page is publicly available at
https://openark-berkeley.github.io/DTTDNet/.

</details>


### [377] [Low-Resolution Self-Attention for Semantic Segmentation](https://arxiv.org/pdf/2310.05026)
*Yu-Huan Wu, Shi-Chen Zhang, Yun Liu, Le Zhang, Xin Zhan, Daquan Zhou, Jiashi Feng, Ming-Ming Cheng, Liangli Zhen*

Main category: cs.CV

TL;DR: The paper introduces LRSA, a low-resolution self-attention mechanism for semantic segmentation, reducing computational cost while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing vision transformers for semantic segmentation are computationally expensive due to high-resolution context modeling.

Method: Proposes LRSA, which computes self-attention in a fixed low-resolution space and uses 3x3 depth-wise convolutions for fine details.

Result: LRFormer, built with LRSA, outperforms state-of-the-art models on ADE20K, COCO-Stuff, and Cityscapes datasets.

Conclusion: LRSA is an effective, computationally efficient alternative for semantic segmentation tasks.

Abstract: Semantic segmentation tasks naturally require high-resolution information for
pixel-wise segmentation and global context information for class prediction.
While existing vision transformers demonstrate promising performance, they
often utilize high-resolution context modeling, resulting in a computational
bottleneck. In this work, we challenge conventional wisdom and introduce the
Low-Resolution Self-Attention (LRSA) mechanism to capture global context at a
significantly reduced computational cost, i.e., FLOPs. Our approach involves
computing self-attention in a fixed low-resolution space regardless of the
input image's resolution, with additional 3x3 depth-wise convolutions to
capture fine details in the high-resolution space. We demonstrate the
effectiveness of our LRSA approach by building the LRFormer, a vision
transformer with an encoder-decoder structure. Extensive experiments on the
ADE20K, COCO-Stuff, and Cityscapes datasets demonstrate that LRFormer
outperforms state-of-the-art models. Code is available at
https://github.com/yuhuan-wu/LRFormer.

</details>


### [378] [MoBluRF: Motion Deblurring Neural Radiance Fields for Blurry Monocular Video](https://arxiv.org/pdf/2312.13528)
*Minh-Quan Viet Bui, Jongmin Park, Jihyong Oh, Munchurl Kim*

Main category: cs.CV

TL;DR: MoBluRF is a motion deblurring NeRF framework for blurry monocular videos, addressing challenges in video view synthesis caused by motion blur. It uses a two-stage approach (BRI and MDD) to reconstruct scenes and predict sharp rays, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Motion blur in videos due to object or camera movement hinders precise view synthesis. Existing NeRF methods struggle with dynamic scenes, necessitating a specialized solution.

Method: MoBluRF employs a two-stage framework: Base Ray Initialization (BRI) for coarse scene reconstruction and Motion Decomposition-based Deblurring (MDD) to predict latent sharp rays by decomposing motion into global and local components. Two loss functions aid geometry regularization and scene decomposition.

Result: MoBluRF outperforms state-of-the-art methods in both qualitative and quantitative metrics, demonstrating superior performance in deblurring and view synthesis.

Conclusion: MoBluRF effectively addresses motion blur in video view synthesis, offering a robust solution for dynamic scenes without requiring mask supervision.

Abstract: Neural Radiance Fields (NeRF), initially developed for static scenes, have
inspired many video novel view synthesis techniques. However, the challenge for
video view synthesis arises from motion blur, a consequence of object or camera
movements during exposure, which hinders the precise synthesis of sharp
spatio-temporal views. In response, we propose a novel motion deblurring NeRF
framework for blurry monocular video, called MoBluRF, consisting of a Base Ray
Initialization (BRI) stage and a Motion Decomposition-based Deblurring (MDD)
stage. In the BRI stage, we coarsely reconstruct dynamic 3D scenes and jointly
initialize the base rays which are further used to predict latent sharp rays,
using the inaccurate camera pose information from the given blurry frames. In
the MDD stage, we introduce a novel Incremental Latent Sharp-rays Prediction
(ILSP) approach for the blurry monocular video frames by decomposing the latent
sharp rays into global camera motion and local object motion components. We
further propose two loss functions for effective geometry regularization and
decomposition of static and dynamic scene components without any mask
supervision. Experiments show that MoBluRF outperforms qualitatively and
quantitatively the recent state-of-the-art methods with large margins.

</details>


### [379] [A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of Foundation Models](https://arxiv.org/pdf/2401.11311)
*Reda Bensaid, Vincent Gripon, François Leduc-Primeau, Lukas Mauch, Ghouthi Boukli Hacene, Fabien Cardinaux*

Main category: cs.CV

TL;DR: The paper explores adapting vision foundation models (VFM) for few-shot semantic segmentation (FSS), proposing a new benchmark and comparing VFM and segmentation models using various adaptation methods.


<details>
  <summary>Details</summary>
Motivation: Current FSS benchmarks focus on in-domain generalization, making them unsuitable for VFM trained on large-scale datasets. The study aims to bridge this gap.

Method: A novel realistic benchmark is introduced, and VFM and segmentation models are evaluated using adaptation methods like linear probing, PEFT, and full fine-tuning.

Result: Segmentation models are outperformed by self-supervised models, and PEFT methods show competitive but similar performance, emphasizing the feature extractor's importance.

Conclusion: This is the first study on VFM adaptation for FSS, highlighting the feature extractor's critical role and the potential of self-supervised models.

Abstract: Few-shot semantic segmentation (FSS) is a crucial challenge in computer
vision, driving extensive research into a diverse range of methods, from
advanced meta-learning techniques to simple transfer learning baselines. With
the emergence of vision foundation models (VFM) serving as generalist feature
extractors, we seek to explore the adaptation of these models for FSS. While
current FSS benchmarks focus on adapting pre-trained models to new tasks with
few images, they emphasize in-domain generalization, making them less suitable
for VFM trained on large-scale web datasets. To address this, we propose a
novel realistic benchmark with a simple and straightforward adaptation process
tailored for this task. Using this benchmark, we conduct a comprehensive
comparative analysis of prominent VFM and semantic segmentation models. To
evaluate their effectiveness, we leverage various adaption methods, ranging
from linear probing to parameter efficient fine-tuning (PEFT) and full
fine-tuning. Our findings show that models designed for segmentation can be
outperformed by self-supervised (SSL) models. On the other hand, while PEFT
methods yields competitive performance, they provide little discrepancy in the
obtained results compared to other methods, highlighting the critical role of
the feature extractor in determining results. To our knowledge, this is the
first study on the adaptation of VFM for FSS.

</details>


### [380] [The Male CEO and the Female Assistant: Evaluation and Mitigation of Gender Biases in Text-To-Image Generation of Dual Subjects](https://arxiv.org/pdf/2402.11089)
*Yixin Wan, Kai-Wei Chang*

Main category: cs.CV

TL;DR: The paper evaluates gender biases in T2I models like DALLE-3, introduces the Paired Stereotype Test (PST) to measure biases in multi-person images, and proposes FairCritic, an LLM-based framework to detect and mitigate biases.


<details>
  <summary>Details</summary>
Motivation: Despite progress in reducing gender stereotypes in single-person images, biases persist in multi-person settings, particularly in gendered occupations and organizational power.

Method: The Paired Stereotype Test (PST) is used to evaluate biases by contrasting male- and female-stereotyped identities. FairCritic, an LLM-based framework, is introduced to detect biases and provide adaptive feedback.

Result: DALLE-3 exhibits gender-occupational biases in 74% of images under PST, with stronger male-associated stereotypes. FairCritic achieves near-perfect fairness in mitigating these biases.

Conclusion: The study highlights persistent gender biases in T2I models and demonstrates the effectiveness of FairCritic in improving fairness, surpassing prompt-based interventions.

Abstract: Recent large-scale T2I models like DALLE-3 have made progress in reducing
gender stereotypes when generating single-person images. However, significant
biases remain when generating images with more than one person. To
systematically evaluate this, we propose the Paired Stereotype Test (PST)
framework, which queries T2I models to depict two individuals assigned with
male-stereotyped and female-stereotyped social identities, respectively (e.g.
"a CEO" and "an Assistant"). This contrastive setting often triggers T2I models
to generate gender-stereotyped images. Using PST, we evaluate two aspects of
gender biases -- the well-known bias in gendered occupation and a novel aspect:
bias in organizational power. Experiments show that over 74\% images generated
by DALLE-3 display gender-occupational biases. Additionally, compared to
single-person settings, DALLE-3 is more likely to perpetuate male-associated
stereotypes under PST. We further propose FairCritic, a novel and interpretable
framework that leverages an LLM-based critic model to i) detect bias in
generated images, and ii) adaptively provide feedback to T2I models for
improving fairness. FairCritic achieves near-perfect fairness on PST,
overcoming the limitations of previous prompt-based intervention approaches.

</details>


### [381] [PointCloud-Text Matching: Benchmark Datasets and a Baseline](https://arxiv.org/pdf/2403.19386)
*Yanglin Feng, Yang Qin, Dezhong Peng, Hongyuan Zhu, Xi Peng, Peng Hu*

Main category: cs.CV

TL;DR: The paper introduces PointCloud-Text Matching (PTM), a new retrieval task, and proposes a benchmark dataset (SceneDepict-3D2T) and a baseline method (RoMa) to address challenges like noise and ambiguity in point clouds and texts.


<details>
  <summary>Details</summary>
Motivation: PTM has applications in localization and scene retrieval, but lacks suitable datasets and effective methods due to data challenges like sparsity and ambiguity.

Method: Proposes RoMa with Dual Attention Perception (DAP) for feature aggregation and Robust Negative Contrastive Learning (RNCL) for handling noisy correspondence.

Result: Extensive experiments show RoMa's superiority on the proposed benchmark.

Conclusion: RoMa effectively addresses PTM challenges, demonstrating potential for real-world applications.

Abstract: In this paper, we present and study a new instance-level retrieval task:
PointCloud-Text Matching (PTM), which aims to identify the exact cross-modal
instance that matches a given point-cloud query or text query. PTM has
potential applications in various scenarios, such as indoor/urban-canyon
localization and scene retrieval. However, there is a lack of suitable and
targeted datasets for PTM in practice. To address this issue, we present a new
PTM benchmark dataset, namely SceneDepict-3D2T. We observe that the data poses
significant challenges due to its inherent characteristics, such as the
sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or
incompleteness of texts, which render existing cross-modal matching methods
ineffective for PTM. To overcome these challenges, we propose a PTM baseline,
named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two key
modules: a Dual Attention Perception module (DAP) and a Robust Negative
Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and
feature-level attention mechanisms to adaptively focus on useful local and
global features, and aggregate them into common representations, thereby
reducing the adverse impact of noise and ambiguity. To handle noisy
correspondence, RNCL enhances robustness against mismatching by dividing
negative pairs into clean and noisy subsets and assigning them forward and
reverse optimization directions, respectively. We conduct extensive experiments
on our benchmarks and demonstrate the superiority of our RoMa.

</details>


### [382] [GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing Sparsity, Trained from Scratch on Small Datasets](https://arxiv.org/pdf/2404.04924)
*Dongjing Shan, guiqiang chen*

Main category: cs.CV

TL;DR: The paper proposes a Graph-based Vision Transformer (GvT) to bridge the performance gap between ViTs and CNNs on small datasets by incorporating graph convolutional projection and pooling, along with talking-heads technology to address low-rank bottlenecks.


<details>
  <summary>Details</summary>
Motivation: ViTs underperform CNNs on small datasets due to lack of inductive bias, prompting the need for a solution.

Method: GvT uses graph convolutional projection, graph-pooling, and talking-heads technology to enhance attention mechanisms and token aggregation.

Result: GvT matches or outperforms CNNs and surpasses standard ViTs on small datasets without pre-training.

Conclusion: GvT effectively addresses ViTs' limitations on small datasets, offering a competitive alternative to CNNs.

Abstract: Vision Transformers (ViTs) have achieved impressive results in large-scale
image classification. However, when training from scratch on small datasets,
there is still a significant performance gap between ViTs and Convolutional
Neural Networks (CNNs), which is attributed to the lack of inductive bias. To
address this issue, we propose a Graph-based Vision Transformer (GvT) that
utilizes graph convolutional projection and graph-pooling. In each block,
queries and keys are calculated through graph convolutional projection based on
the spatial adjacency matrix, while dot-product attention is used in another
graph convolution to generate values. When using more attention heads, the
queries and keys become lower-dimensional, making their dot product an
uninformative matching function. To overcome this low-rank bottleneck in
attention heads, we employ talking-heads technology based on bilinear pooled
features and sparse selection of attention tensors. This allows interaction
among filtered attention scores and enables each attention mechanism to depend
on all queries and keys. Additionally, we apply graph-pooling between two
intermediate blocks to reduce the number of tokens and aggregate semantic
information more effectively. Our experimental results show that GvT produces
comparable or superior outcomes to deep convolutional networks and surpasses
vision transformers without pre-training on large datasets. The code for our
proposed model is publicly available on the website.

</details>


### [383] [CodeEnhance: A Codebook-Driven Approach for Low-Light Image Enhancement](https://arxiv.org/pdf/2404.05253)
*Xu Wu, XianXu Hou, Zhihui Lai, Jie Zhou, Ya-nan Zhang, Witold Pedrycz, Linlin Shen*

Main category: cs.CV

TL;DR: CodeEnhance improves low-light image enhancement by using quantized priors and interactive refinement, addressing uncertainty and information loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse brightness degradations and loss of texture/color information during enhancement.

Method: Proposes CodeEnhance, leveraging image-to-code mapping, Semantic Embedding Module (SEM), Codebook Shift (CS), and Interactive Feature Transformation (IFT).

Result: Outperforms benchmarks in quality and fidelity, showing robustness to uneven illumination, noise, and color distortion.

Conclusion: CodeEnhance effectively enhances low-light images by integrating prior knowledge and controllable refinement.

Abstract: Low-light image enhancement (LLIE) aims to improve low-illumination images.
However, existing methods face two challenges: (1) uncertainty in restoration
from diverse brightness degradations; (2) loss of texture and color information
caused by noise suppression and light enhancement. In this paper, we propose a
novel enhancement approach, CodeEnhance, by leveraging quantized priors and
image refinement to address these challenges. In particular, we reframe LLIE as
learning an image-to-code mapping from low-light images to discrete codebook,
which has been learned from high-quality images. To enhance this process, a
Semantic Embedding Module (SEM) is introduced to integrate semantic information
with low-level features, and a Codebook Shift (CS) mechanism, designed to adapt
the pre-learned codebook to better suit the distinct characteristics of our
low-light dataset. Additionally, we present an Interactive Feature
Transformation (IFT) module to refine texture and color information during
image reconstruction, allowing for interactive enhancement based on user
preferences. Extensive experiments on both real-world and synthetic benchmarks
demonstrate that the incorporation of prior knowledge and controllable
information transfer significantly enhances LLIE performance in terms of
quality and fidelity. The proposed CodeEnhance exhibits superior robustness to
various degradations, including uneven illumination, noise, and color
distortion.

</details>


### [384] [VectorPainter: Advanced Stylized Vector Graphics Synthesis Using Stroke-Style Priors](https://arxiv.org/pdf/2405.02962)
*Juncheng Hu, Ximing Xing, Jing Zhang, Qian Yu*

Main category: cs.CV

TL;DR: VectorPainter is a framework for text-to-vector-graphics synthesis using stylized strokes from reference images, optimizing their positions and colors to match text descriptions.


<details>
  <summary>Details</summary>
Motivation: The style of strokes distinguishes artists, so the task is reformulated to synthesize vector graphics by rearranging stylized strokes from references.

Method: Converts reference image pixels to vector strokes, optimizes their positions/colors for text descriptions, and uses imitation learning for style capture and a style-preserving loss.

Result: Superior performance in stylized vector graphics synthesis, with effective components validated through experiments.

Conclusion: VectorPainter excels in synthesizing stylized vector graphics by leveraging reference-guided stroke rearrangement and style preservation.

Abstract: We introduce VectorPainter, a novel framework designed for reference-guided
text-to-vector-graphics synthesis. Based on our observation that the style of
strokes can be an important aspect to distinguish different artists, our method
reforms the task into synthesize a desired vector graphics by rearranging
stylized strokes, which are vectorized from the reference images. Specifically,
our method first converts the pixels of the reference image into a series of
vector strokes, and then generates a vector graphic based on the input text
description by optimizing the positions and colors of these vector strokes. To
precisely capture the style of the reference image in the vectorized strokes,
we propose an innovative vectorization method that employs an imitation
learning strategy. To preserve the style of the strokes throughout the
generation process, we introduce a style-preserving loss function. Extensive
experiments have been conducted to demonstrate the superiority of our approach
over existing works in stylized vector graphics synthesis, as well as the
effectiveness of the various components of our method.

</details>


### [385] [OralBBNet: Spatially Guided Dental Segmentation of Panoramic X-Rays with Bounding Box Priors](https://arxiv.org/pdf/2406.03747)
*Devichand Budagam, Azamat Zhanatuly Imanbayev, Iskander Rafailovich Akhmetov, Aleksandr Sinitca, Sergey Antonov, Dmitrii Kaplun*

Main category: cs.CV

TL;DR: The paper introduces UFBA-425, a dental dataset, and OralBBNet, a model combining U-Net and YOLOv8 for simultaneous teeth segmentation and detection, showing improved accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for simultaneous teeth segmentation and detection in dental diagnostics, leveraging deep learning for precision.

Method: OralBBNet integrates U-Net for segmentation and YOLOv8 for detection, tested on the UFBA-425 dataset with 425 annotated X-rays.

Result: Achieved 1-3% mAP improvement in detection and 15-20% dice score improvement in segmentation over U-Net, outperforming other architectures.

Conclusion: The study advances dental diagnostics by demonstrating the effectiveness of combining segmentation and detection models.

Abstract: Teeth segmentation and recognition play a vital role in a variety of dental
applications and diagnostic procedures. The integration of deep learning models
has facilitated the development of precise and automated segmentation methods.
Although prior research has explored teeth segmentation, not many methods have
successfully performed tooth segmentation and detection simultaneously. This
study presents UFBA-425, a dental dataset derived from the UFBA-UESC dataset,
featuring bounding box and polygon annotations for 425 panoramic dental X-rays.
Additionally, this work introduces OralBBNet, an architecture featuring
distinct segmentation and detection heads as U-Net and YOLOv8, respectively.
OralBBNet is designed to improve the accuracy and robustness of tooth
classification and segmentation on panoramic X-rays by leveraging the
complementary strengths of U-Net and YOLOv8. Our approach achieved a 1-3%
improvement in mean average precision (mAP) for teeth detection compared to
existing techniques and a 15-20% improvement in the dice score for teeth
segmentation over U-Net over various tooth categories and 2-4% improvement in
the dice score when compared with other segmentation architectures. The results
of this study establish a foundation for the wider implementation of object
detection models in dental diagnostics.

</details>


### [386] [LeYOLO, New Embedded Architecture for Object Detection](https://arxiv.org/pdf/2406.14239)
*Lilian Hollard, Lucas Mohimont, Nathalie Gaveau, Luiz Angelo Steffenel*

Main category: cs.CV

TL;DR: The paper introduces LeNeck and LeYOLO, two efficient object detection models for low-resource environments, aiming to bridge the gap between lightweight detectors and high-performance YOLO architectures.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in object detection prioritize speed over lightweight design, leaving a gap for efficient models in low-resource environments. The paper addresses whether parameter and FLOP-efficient models can match YOLO's accuracy.

Method: Proposes LeNeck, a general-purpose detection framework, and LeYOLO, an efficient YOLO-based model, both validated on MSCOCO.

Result: LeNeck matches SSDLite's speed while improving accuracy and reducing parameters. LeYOLO bridges the gap between SSDLite and YOLO, offering high accuracy in compact models.

Conclusion: Both models are suitable for mobile, embedded, and ultra-low-power devices, addressing the need for computational efficiency in resource-constrained environments.

Abstract: Efficient computation in deep neural networks is crucial for real-time object
detection. However, recent advancements primarily result from improved
high-performing hardware rather than improving parameters and FLOP efficiency.
This is especially evident in the latest YOLO architectures, where speed is
prioritized over lightweight design. As a result, object detection models
optimized for low-resource environments like microcontrollers have received
less attention. For devices with limited computing power, existing solutions
primarily rely on SSDLite or combinations of low-parameter classifiers,
creating a noticeable gap between YOLO-like architectures and truly efficient
lightweight detectors. This raises a key question: Can a model optimized for
parameter and FLOP efficiency achieve accuracy levels comparable to mainstream
YOLO models? To address this, we introduce two key contributions to object
detection models using MSCOCO as a base validation set. First, we propose
LeNeck, a general-purpose detection framework that maintains inference speed
comparable to SSDLite while significantly improving accuracy and reducing
parameter count. Second, we present LeYOLO, an efficient object detection model
designed to enhance computational efficiency in YOLO-based architectures.
LeYOLO effectively bridges the gap between SSDLite-based detectors and YOLO
models, offering high accuracy in a model as compact as MobileNets. Both
contributions are particularly well-suited for mobile, embedded, and
ultra-low-power devices, including microcontrollers, where computational
efficiency is critical.

</details>


### [387] [T-FAKE: Synthesizing Thermal Images for Facial Landmarking](https://arxiv.org/pdf/2408.15127)
*Philipp Flotho, Moritz Piening, Anna Kukleva, Gabriele Steidl*

Main category: cs.CV

TL;DR: The paper introduces T-FAKE, a synthetic thermal dataset with landmarks, using a novel RGB2Thermal loss for domain-adaptive transfer, improving thermal landmark detection.


<details>
  <summary>Details</summary>
Motivation: Existing facial datasets lack thermal modality, crucial for healthcare and biometrics, prompting the creation of T-FAKE.

Method: Proposes RGB2Thermal loss for style transfer, leveraging Wasserstein distance and clinical temperature statistics to generate realistic thermal images.

Result: T-FAKE dataset enhances landmark detection on thermal images, showing strong performance with sparse and dense landmarks.

Conclusion: The RGB2Thermal loss and T-FAKE dataset significantly advance thermal facial analysis, with applications in healthcare and biometrics.

Abstract: Facial analysis is a key component in a wide range of applications such as
healthcare, autonomous driving, and entertainment. Despite the availability of
various facial RGB datasets, the thermal modality, which plays a crucial role
in life sciences, medicine, and biometrics, has been largely overlooked. To
address this gap, we introduce the T-FAKE dataset, a new large-scale synthetic
thermal dataset with sparse and dense landmarks. To facilitate the creation of
the dataset, we propose a novel RGB2Thermal loss function, which enables the
domain-adaptive transfer of RGB faces to thermal style. By utilizing the
Wasserstein distance between thermal and RGB patches and the statistical
analysis of clinical temperature distributions on faces, we ensure that the
generated thermal images closely resemble real samples. Using RGB2Thermal style
transfer based on our RGB2Thermal loss function, we create the large-scale
synthetic thermal T-FAKE dataset with landmark and segmentation annotations.
Leveraging our novel T-FAKE dataset, probabilistic landmark prediction, and
label adaptation networks, we demonstrate significant improvements in landmark
detection methods on thermal images across different landmark conventions. Our
models show excellent performance with both sparse 70-point landmarks and dense
478-point landmark annotations. Moreover, our RGB2Thermal loss leads to notable
results in terms of perceptual evaluation and temperature prediction.

</details>


### [388] [StimuVAR: Spatiotemporal Stimuli-aware Video Affective Reasoning with Multimodal Large Language Models](https://arxiv.org/pdf/2409.00304)
*Yuxiang Guo, Faizan Siddiqui, Yang Zhao, Rama Chellappa, Shao-Yuan Lo*

Main category: cs.CV

TL;DR: StimuVAR is a spatiotemporal stimuli-aware framework for video affective reasoning (VAR) with MLLMs, addressing their oversight of emotional stimuli by incorporating frame-level and token-level awareness mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on video semantics but overlook emotional stimuli, limiting their ability to predict and explain viewers' emotional reactions.

Method: StimuVAR uses a two-level stimuli-aware mechanism (frame-level and token-level awareness) and VAR instruction data to enhance MLLMs' affective reasoning.

Result: Experiments show StimuVAR excels in understanding emotional responses and providing coherent explanations, outperforming existing methods.

Conclusion: StimuVAR is the first MLLM-based method for viewer-centered VAR, demonstrating superior performance in affective reasoning.

Abstract: Predicting and reasoning how a video would make a human feel is crucial for
developing socially intelligent systems. Although Multimodal Large Language
Models (MLLMs) have shown impressive video understanding capabilities, they
tend to focus more on the semantic content of videos, often overlooking
emotional stimuli. Hence, most existing MLLMs fall short in estimating viewers'
emotional reactions and providing plausible explanations. To address this
issue, we propose StimuVAR, a spatiotemporal Stimuli-aware framework for Video
Affective Reasoning (VAR) with MLLMs. StimuVAR incorporates a two-level
stimuli-aware mechanism: frame-level awareness and token-level awareness.
Frame-level awareness involves sampling video frames with events that are most
likely to evoke viewers' emotions. Token-level awareness performs tube
selection in the token space to make the MLLM concentrate on emotion-triggered
spatiotemporal regions. Furthermore, we create VAR instruction data to perform
affective training, steering MLLMs' reasoning strengths towards emotional focus
and thereby enhancing their affective reasoning ability. To thoroughly assess
the effectiveness of VAR, we provide a comprehensive evaluation protocol with
extensive metrics. StimuVAR is the first MLLM-based method for viewer-centered
VAR. Experiments demonstrate its superiority in understanding viewers'
emotional responses to videos and providing coherent and insightful
explanations. Our code is available at https://github.com/EthanG97/StimuVAR

</details>


### [389] [GASP: Gaussian Splatting for Physic-Based Simulations](https://arxiv.org/pdf/2409.05819)
*Piotr Borycki, Weronika Smolak, Joanna Waczyńska, Marcin Mazur, Sławomir Tadeja, Przemysław Spurek*

Main category: cs.CV

TL;DR: The paper introduces GASP, a model integrating Gaussian Splatting with physics simulations by treating 3D points as discrete entities, avoiding traditional meshing methods.


<details>
  <summary>Details</summary>
Motivation: Physics simulation integration with advanced 3D rendering like Gaussian Splatting is challenging, requiring alternatives to traditional meshing.

Method: GASP uses flat Gaussian distributions parameterized by three points (mesh faces) and treats each 3D point as a discrete entity, simplifying Gaussian component modeling.

Result: GASP shows superior performance on benchmark datasets for 3D object rendering.

Conclusion: GASP offers a viable alternative to traditional meshing for physics-based simulations, compatible with existing physics engines.

Abstract: Physics simulation is paramount for modeling and utilization of 3D scenes in
various real-world applications. However, its integration with state-of-the-art
3D scene rendering techniques such as Gaussian Splatting (GS) remains
challenging. Existing models use additional meshing mechanisms, including
triangle or tetrahedron meshing, marching cubes, or cage meshes. As an
alternative, we can modify the physics grounded Newtonian dynamics to align
with 3D Gaussian components. Current models take the first-order approximation
of a deformation map, which locally approximates the dynamics by linear
transformations. In contrast, our Gaussian Splatting for Physics-Based
Simulations (GASP) model uses such a map (without any modifications) and flat
Gaussian distributions, which are parameterized by three points (mesh faces).
Subsequently, each 3D point (mesh face node) is treated as a discrete entity
within a 3D space. Consequently, the problem of modeling Gaussian components is
reduced to working with 3D points. Additionally, the information on mesh faces
can be used to incorporate further properties into the physics model,
facilitating the use of triangles. Resulting solution can be integrated into
any physics engine that can be treated as a black box. As demonstrated in our
studies, the proposed model exhibits superior performance on a diverse range of
benchmark datasets designed for 3D object rendering.

</details>


### [390] [FIHA: Autonomous Hallucination Evaluation in Vision-Language Models with Davidson Scene Graphs](https://arxiv.org/pdf/2409.13612)
*Bowen Yan, Zhengsong Zhang, Liqiang Jing, Eftekhar Hossain, Xinya Du*

Main category: cs.CV

TL;DR: FIHA is introduced as a cost-effective, annotation-free method for evaluating hallucination in Large Vision-Language Models (LVLMs), generating Q&A pairs and modeling dependencies between hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current hallucination assessment methods are costly and lack comprehensiveness, failing to evaluate all aspects like relations, attributes, and dependencies.

Method: FIHA autonomously evaluates LVLMs without LLMs or annotations, generates Q&A pairs from images, and uses Davidson Scene Graph (DSG) to structure evaluations.

Result: FIHA-v1 benchmark is created, assessing models on diverse questions from MSCOCO and Foggy datasets, revealing limitations and challenges.

Conclusion: FIHA provides a scalable, reliable way to assess LVLM hallucinations, with code and data publicly released.

Abstract: The rapid development of Large Vision-Language Models (LVLMs) often comes
with widespread hallucination issues, making cost-effective and comprehensive
assessments increasingly vital. Current approaches mainly rely on costly
annotations and are not comprehensive -- in terms of evaluating all aspects
such as relations, attributes, and dependencies between aspects. Therefore, we
introduce the FIHA (autonomous Fine-graIned Hallucination evAluation evaluation
in LVLMs), which could access hallucination LVLMs in the LLM-free and
annotation-free way and model the dependency between different types of
hallucinations. FIHA can generate Q&A pairs on any image dataset at minimal
cost, enabling hallucination assessment from both image and caption. Based on
this approach, we introduce a benchmark called FIHA-v1, which consists of
diverse questions on various images from MSCOCO and Foggy. Furthermore, we use
the Davidson Scene Graph (DSG) to organize the structure among Q&A pairs, in
which we can increase the reliability of the evaluation. We evaluate
representative models using FIHA-v1, highlighting their limitations and
challenges. We released our code and data.

</details>


### [391] [Adversarial Robustness of AI-Generated Image Detectors in the Real World](https://arxiv.org/pdf/2410.01574)
*Sina Mavali, Jonas Ricker, David Pape, Asja Fischer, Lea Schönherr*

Main category: cs.CV

TL;DR: Current AI-generated content detectors are vulnerable to adversarial attacks, even under real-world conditions, highlighting the need for more robust solutions.


<details>
  <summary>Details</summary>
Motivation: The misuse of GenAI, especially in generating misinformation, threatens public trust in democratic processes, necessitating reliable detection tools.

Method: Evaluated four detection methods and five attack algorithms, including black-box attacks on a commercial tool (HIVE), and tested robustness using features from a pre-trained model.

Result: Attackers can significantly reduce detection performance without knowing the detector's architecture, and most attacks remain effective despite image degradation.

Conclusion: The findings emphasize the urgent need for more research and innovative approaches to combat GenAI misuse and enhance detection robustness.

Abstract: The rapid advancement of Generative Artificial Intelligence (GenAI)
capabilities is accompanied by a concerning rise in its misuse. In particular
the generation of credible misinformation in the form of images poses a
significant threat to the public trust in democratic processes. Consequently,
there is an urgent need to develop tools to reliably distinguish between
authentic and AI-generated content. The majority of detection methods are based
on neural networks that are trained to recognize forensic artifacts. In this
work, we demonstrate that current state-of-the-art classifiers are vulnerable
to adversarial examples under real-world conditions. Through extensive
experiments, comprising four detection methods and five attack algorithms, we
show that an attacker can dramatically decrease classification performance,
without internal knowledge of the detector's architecture. Notably, most
attacks remain effective even when images are degraded during the upload to,
e.g., social media platforms. In a case study, we demonstrate that these
robustness challenges are also found in commercial tools by conducting
black-box attacks on HIVE, a proprietary online GenAI media detector. In
addition, we evaluate the robustness of using generated features of a robust
pre-trained model and showed that this increases the robustness, while not
reaching the performance on benign inputs. These results, along with the
increasing potential of GenAI to erode public trust, underscore the need for
more research and new perspectives on methods to prevent its misuse.

</details>


### [392] [SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference](https://arxiv.org/pdf/2410.04417)
*Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Shanghang Zhang*

Main category: cs.CV

TL;DR: SparseVLM is a text-guided, training-free token optimization method for vision-language models (VLMs) that prunes redundant visual tokens using self-attention matrices, improving efficiency without extra parameters or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Visual tokens in VLMs are computationally expensive but often contain sparse information. Existing methods require training or extra parameters, which SparseVLM avoids.

Method: SparseVLM uses text tokens to rate visual tokens via self-attention matrices, prunes them adaptively, and recycles pruned tokens into compact representations.

Result: SparseVLM reduces FLOPs by 54% and CUDA latency by 37% in LLaVA while retaining 97% accuracy.

Conclusion: SparseVLM offers an efficient, parameter-free solution for optimizing VLMs, enhancing performance in image and video tasks.

Abstract: In vision-language models (VLMs), visual tokens usually bear a significant
amount of computational overhead despite sparsity of information in them when
compared to text tokens. To address this, most existing methods learn a network
to prune redundant visual tokens using certain training data. Differently, we
propose a text-guided training-free token optimization mechanism dubbed
SparseVLM that eliminates the need of extra parameters or fine-tuning costs.
Given that visual tokens complement text tokens in VLM's linguistic reasoning,
we select relevant text tokens to rate the significance of visual tokens using
self-attention matrices and, then, prune visual tokens using the proposed
strategy to maximize sparsity while retaining information. In particular, we
introduce a rank-based strategy to adaptively determine the sparsification
ratio for each layer, alongside a token recycling method that compresses pruned
tokens into more compact representations. Experimental results show that
SparseVLM increases the efficiency of various VLMs in a number of image and
video understanding tasks. For example, LLaVA when equipped with SparseVLM
achieves 54% reduction in FLOPs, 37% decrease in CUDA latency while maintaining
97% of its original accuracy. Our code is available at
https://github.com/Gumpest/SparseVLMs.

</details>


### [393] [Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification](https://arxiv.org/pdf/2411.05698)
*Antonio De Santis, Riccardo Campi, Matteo Bianchi, Marco Brambilla*

Main category: cs.CV

TL;DR: Visual-TCAV bridges saliency and concept-based methods for CNN explainability, providing local and global explanations via CAVs and saliency maps.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing explainability methods (saliency and concept-based) in CNNs by combining their strengths.

Method: Introduces Visual-TCAV, using CAVs and a generalization of Integrated Gradients to generate saliency maps and concept attributions.

Result: Evaluated on CNN architectures, validated with ground truth, and compared to TCAV, showing effectiveness.

Conclusion: Visual-TCAV successfully combines local and global explanations, enhancing CNN transparency.

Abstract: Convolutional Neural Networks (CNNs) have seen significant performance
improvements in recent years. However, due to their size and complexity, they
function as black-boxes, leading to transparency concerns. State-of-the-art
saliency methods generate local explanations that highlight the area in the
input image where a class is identified but cannot explain how a concept of
interest contributes to the prediction, which is essential for bias mitigation.
On the other hand, concept-based methods, such as TCAV (Testing with Concept
Activation Vectors), provide insights into how sensitive is the network to a
concept, but cannot compute its attribution in a specific prediction nor show
its location within the input image. This paper introduces a novel post-hoc
explainability framework, Visual-TCAV, which aims to bridge the gap between
these methods by providing both local and global explanations for CNN-based
image classification. Visual-TCAV uses Concept Activation Vectors (CAVs) to
generate saliency maps that show where concepts are recognized by the network.
Moreover, it can estimate the attribution of these concepts to the output of
any class using a generalization of Integrated Gradients. This framework is
evaluated on popular CNN architectures, with its validity further confirmed via
experiments where ground truth for explanations is known, and a comparison with
TCAV. Our code is available at
https://github.com/DataSciencePolimi/Visual-TCAV.

</details>


### [394] [Point Cloud Mixture-of-Domain-Experts Model for 3D Self-supervised Learning](https://arxiv.org/pdf/2410.09886)
*Yaohua Zha, Tao Dai, Hang Guo, Yanzi Wang, Bin Chen, Ke Chen, Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper introduces Point-MoDE, a self-supervised learning model for point clouds that leverages cross-domain knowledge between scene and object domains via a block-to-scene pre-training strategy.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud SSL methods focus on single-domain learning, missing the benefits of cross-domain knowledge. This limits the comprehensiveness of 3D representations.

Method: Proposes a mixture-of-domain-expert model (Point-MoDE) with scene and object domain experts. Uses block-to-scene pre-training with object-level mask reconstruction and scene-level position regression.

Result: The model integrates cross-domain knowledge, improving both object and scene domain representations. Experiments show superior performance in downstream tasks.

Conclusion: Point-MoDE effectively combines cross-domain knowledge, enhancing 3D representation learning and outperforming existing methods.

Abstract: Point clouds, as a primary representation of 3D data, can be categorized into
scene domain point clouds and object domain point clouds. Point cloud
self-supervised learning (SSL) has become a mainstream paradigm for learning 3D
representations. However, existing point cloud SSL primarily focuses on
learning domain-specific 3D representations within a single domain, neglecting
the complementary nature of cross-domain knowledge, which limits the learning
of 3D representations. In this paper, we propose to learn a comprehensive Point
cloud Mixture-of-Domain-Experts model (Point-MoDE) via a block-to-scene
pre-training strategy. Specifically, we first propose a
mixture-of-domain-expert model consisting of scene domain experts and multiple
shared object domain experts. Furthermore, we propose a block-to-scene
pretraining strategy, which leverages the features of point blocks in the
object domain to regress their initial positions in the scene domain through
object-level block mask reconstruction and scene-level block position
regression. By integrating the complementary knowledge between object and
scene, this strategy simultaneously facilitates the learning of both
object-domain and scene-domain representations, leading to a more comprehensive
3D representation. Extensive experiments in downstream tasks demonstrate the
superiority of our model.

</details>


### [395] [Foundation Models for Remote Sensing and Earth Observation: A Survey](https://arxiv.org/pdf/2410.16602)
*Aoran Xiao, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, Naoto Yokoya*

Main category: cs.CV

TL;DR: The paper surveys Remote Sensing Foundation Models (RSFMs), addressing challenges in AI for Remote Sensing (RS) and exploring their potential across Earth Observation tasks.


<details>
  <summary>Details</summary>
Motivation: To develop more intelligent RS systems by leveraging large Foundation Models (FMs) despite their limitations with RS data.

Method: Systematic review of RSFMs, categorizing studies, benchmarking models, and analyzing datasets and technical contributions.

Result: Identifies gaps in RSFM applications, benchmarks performance, and highlights challenges in adapting FMs to RS data.

Conclusion: Proposes future research directions for RSFMs, emphasizing their potential in Earth Observation tasks.

Abstract: Remote Sensing (RS) is a crucial technology for observing, monitoring, and
interpreting our planet, with broad applications across geoscience, economics,
humanitarian fields, etc. While artificial intelligence (AI), particularly deep
learning, has achieved significant advances in RS, unique challenges persist in
developing more intelligent RS systems, including the complexity of Earth's
environments, diverse sensor modalities, distinctive feature patterns, varying
spatial and spectral resolutions, and temporal dynamics. Meanwhile, recent
breakthroughs in large Foundation Models (FMs) have expanded AI's potential
across many domains due to their exceptional generalizability and zero-shot
transfer capabilities. However, their success has largely been confined to
natural data like images and video, with degraded performance and even failures
for RS data of various non-optical modalities. This has inspired growing
interest in developing Remote Sensing Foundation Models (RSFMs) to address the
complex demands of Earth Observation (EO) tasks, spanning the surface,
atmosphere, and oceans. This survey systematically reviews the emerging field
of RSFMs. It begins with an outline of their motivation and background,
followed by an introduction of their foundational concepts. It then categorizes
and reviews existing RSFM studies including their datasets and technical
contributions across Visual Foundation Models (VFMs), Visual-Language Models
(VLMs), Large Language Models (LLMs), and beyond. In addition, we benchmark
these models against publicly available datasets, discuss existing challenges,
and propose future research directions in this rapidly evolving field. A
project associated with this survey has been built at
https://github.com/xiaoaoran/awesome-RSFMs .

</details>


### [396] [Constant Rate Scheduling: Constant-Rate Distributional Change for Efficient Training and Sampling in Diffusion Models](https://arxiv.org/pdf/2411.12188)
*Shuntaro Okada, Kenji Doi, Ryota Yoshihashi, Hirokatsu Kataoka, Tomohiro Tanaka*

Main category: cs.CV

TL;DR: A method to optimize noise schedules in diffusion models for consistent probability distribution changes, improving performance across datasets and samplers.


<details>
  <summary>Details</summary>
Motivation: To enhance diffusion model performance by ensuring a steady rate of change in the probability distribution during diffusion.

Method: Optimizes noise schedules using adaptable distance metrics for probability-distributional change, tested on various datasets and samplers.

Result: Achieved state-of-the-art FID score of 2.03 on LSUN Horse 256×256, with broad performance improvements.

Conclusion: The approach effectively optimizes noise schedules, enhancing diffusion model performance universally.

Abstract: We propose a general approach to optimize noise schedules for training and
sampling in diffusion models. Our approach optimizes the noise schedules to
ensure a constant rate of change in the probability distribution of diffused
data throughout the diffusion process. Any distance metric for measuring the
probability-distributional change is applicable to our approach, and we
introduce three distance metrics. We evaluated the effectiveness of our
approach on unconditional and class-conditional image-generation tasks using
the LSUN (Horse, Bedroom, Church), ImageNet, FFHQ, and CIFAR10 datasets.
Through extensive experiments, we confirmed that our approach broadly improves
the performance of pixel-space and latent-space diffusion models regardless of
the dataset, sampler, and number of function evaluations ranging from 5 to 250.
Notably, by using our approach for optimizing both training and sampling
schedules, we achieved a state-of-the-art FID score of 2.03 without sacrificing
mode coverage on LSUN Horse 256 $\times$ 256.

</details>


### [397] [Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens](https://arxiv.org/pdf/2411.14725)
*Feng Chen, Chenhui Gou, Jing Liu, Yang Yang, Zhaoyang Li, Jiyuan Zhang, Zhenbang Sun, Bohan Zhuang, Qi Wu*

Main category: cs.CV

TL;DR: AbilityLens is a unified benchmark for evaluating vision perception in MLLMs, addressing variance in existing benchmarks and revealing insights into model performance, training phenomena, and conflict resolution.


<details>
  <summary>Details</summary>
Motivation: Existing perception benchmarks lack consistency, complicating comprehensive evaluation of MLLMs' vision abilities.

Method: Introduces AbilityLens, a benchmark evaluating six key perception abilities with diverse questions, domains, and metrics.

Result: Identifies strengths/weaknesses of MLLMs, reveals training phenomena, and highlights data mixing and model size as conflict causes.

Conclusion: AbilityLens provides robust evaluation and insights, with potential solutions like fine-tuning to address conflicts.

Abstract: As multimodal large language models (MLLMs) advance rapidly, rigorous
evaluation has become essential, providing further guidance for their
development. In this work, we focus on a unified and robust evaluation of
\textbf{vision perception} abilities, the foundational skill of MLLMs. We find
that existing perception benchmarks, each focusing on different question types,
domains, and evaluation metrics, introduce significant evaluation variance,
complicating comprehensive assessments of perception abilities when relying on
any single benchmark. To address this, we introduce \textbf{AbilityLens}, a
unified benchmark designed to evaluate MLLMs in six key perception abilities
(ranging from counting, OCR, to understanding structural data), focusing on
both accuracy and stability, with each ability encompassing diverse types of
questions, domains, and metrics. With the assistance of AbilityLens, we: (1)
identify the strengths and weaknesses of current main-stream MLLMs,
highlighting stability patterns and revealing a notable performance gap between
state-of-the-art open-source and closed-source models; (2) uncover interesting
ability conflict and early convergence phenomena during MLLM training; (3)
reveal the primary reason of ability conflict is data mixing ratio and LLM
model size; and (4) discuss the effectiveness of some straightforward
strategies \eg, fine-tuning and model merging, to solve the ability conflict.
The benchmark and online leaderboard is released in
https://github.com/Chenfeng1271/AbilityLens.

</details>


### [398] [Spatio-Temporal Fuzzy-oriented Multi-Modal Meta-Learning for Fine-grained Emotion Recognition](https://arxiv.org/pdf/2412.13541)
*Jingyao Wang, Wenwen Qiang, Changwen Zheng, Fuchun Sun*

Main category: cs.CV

TL;DR: ST-F2M is a framework for fine-grained emotion recognition (FER) that addresses challenges like data annotation costs, temporal heterogeneity, and spatial heterogeneity using spatio-temporal fuzzy-oriented meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing FER methods struggle with high annotation costs, temporal and spatial heterogeneity, and emotion ambiguity. ST-F2M aims to overcome these limitations.

Method: ST-F2M divides multi-modal videos into views, uses spatio-temporal convolutions, integrates fuzzy semantics, and employs meta-recurrent neural networks for meta-learning.

Result: ST-F2M outperforms state-of-the-art methods in accuracy and efficiency, validated by extensive experiments and ablation studies.

Conclusion: ST-F2M effectively handles FER challenges, offering a robust and efficient solution with demonstrated superiority over existing approaches.

Abstract: Fine-grained emotion recognition (FER) plays a vital role in various fields,
such as disease diagnosis, personalized recommendations, and multimedia mining.
However, existing FER methods face three key challenges in real-world
applications: (i) they rely on large amounts of continuously annotated data to
ensure accuracy since emotions are complex and ambiguous in reality, which is
costly and time-consuming; (ii) they cannot capture the temporal heterogeneity
caused by changing emotion patterns, because they usually assume that the
temporal correlation within sampling periods is the same; (iii) they do not
consider the spatial heterogeneity of different FER scenarios, that is, the
distribution of emotion information in different data may have bias or
interference. To address these challenges, we propose a Spatio-Temporal
Fuzzy-oriented Multi-modal Meta-learning framework (ST-F2M). Specifically,
ST-F2M first divides the multi-modal videos into multiple views, and each view
corresponds to one modality of one emotion. Multiple randomly selected views
for the same emotion form a meta-training task. Next, ST-F2M uses an integrated
module with spatial and temporal convolutions to encode the data of each task,
reflecting the spatial and temporal heterogeneity. Then it adds fuzzy semantic
information to each task based on generalized fuzzy rules, which helps handle
the complexity and ambiguity of emotions. Finally, ST-F2M learns
emotion-related general meta-knowledge through meta-recurrent neural networks
to achieve fast and robust fine-grained emotion recognition. Extensive
experiments show that ST-F2M outperforms various state-of-the-art methods in
terms of accuracy and model efficiency. In addition, we construct ablation
studies and further analysis to explore why ST-F2M performs well.

</details>


### [399] [VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/pdf/2501.13106)
*Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao*

Main category: cs.CV

TL;DR: VideoLLaMA3 is a vision-centric multimodal model for image and video understanding, leveraging high-quality image-text data and a four-stage training process to achieve strong performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve image and video understanding by focusing on vision-centric training and framework design, avoiding the need for massive video-text datasets.

Method: The method involves four stages: Vision Encoder Adaptation, Vision-Language Alignment, Multi-task Fine-tuning, and Video-centric Fine-tuning, along with a framework design for variable-resolution images and compact video representations.

Result: VideoLLaMA3 achieves compelling performance in image and video understanding benchmarks.

Conclusion: The vision-centric approach and framework design of VideoLLaMA3 effectively enhance multimodal understanding capabilities.

Abstract: In this paper, we propose VideoLLaMA3, a more advanced multimodal foundation
model for image and video understanding. The core design philosophy of
VideoLLaMA3 is vision-centric. The meaning of "vision-centric" is two-fold: the
vision-centric training paradigm and vision-centric framework design. The key
insight of our vision-centric training paradigm is that high-quality image-text
data is crucial for both image and video understanding. Instead of preparing
massive video-text datasets, we focus on constructing large-scale and
high-quality image-text datasets. VideoLLaMA3 has four training stages: 1)
Vision Encoder Adaptation, which enables vision encoder to accept images of
variable resolutions as input; 2) Vision-Language Alignment, which jointly
tunes the vision encoder, projector, and LLM with large-scale image-text data
covering multiple types (including scene images, documents, charts) as well as
text-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT
data for downstream tasks and video-text data to establish a foundation for
video understanding. 4) Video-centric Fine-tuning, which further improves the
model's capability in video understanding. As for the framework design, to
better capture fine-grained details in images, the pretrained vision encoder is
adapted to encode images of varying sizes into vision tokens with corresponding
numbers, rather than a fixed number of tokens. For video inputs, we reduce the
number of vision tokens according to their similarity so that the
representation of videos will be more precise and compact. Benefit from
vision-centric designs, VideoLLaMA3 achieves compelling performances in both
image and video understanding benchmarks.

</details>


### [400] [P-TAME: Explain Any Image Classifier with Trained Perturbations](https://arxiv.org/pdf/2501.17813)
*Mariano V. Ntrougkas, Vasileios Mezaris, Ioannis Patras*

Main category: cs.CV

TL;DR: P-TAME is a model-agnostic method for explaining DNN-based image classifiers, offering efficient, high-resolution explanations without tailoring to specific architectures.


<details>
  <summary>Details</summary>
Motivation: The black-box nature of DNNs limits their adoption in critical fields requiring justifications for predictions.

Method: P-TAME uses an auxiliary classifier to extract features, bypassing architecture-specific tailoring, and generates explanations in a single forward pass.

Result: P-TAME matches or outperforms existing explainability methods on VGG-16, ResNet-50, and ViT-B-16.

Conclusion: P-TAME provides an efficient, high-quality solution for explaining DNN classifiers, with potential for broad application.

Abstract: The adoption of Deep Neural Networks (DNNs) in critical fields where
predictions need to be accompanied by justifications is hindered by their
inherent black-box nature. In this paper, we introduce P-TAME
(Perturbation-based Trainable Attention Mechanism for Explanations), a
model-agnostic method for explaining DNN-based image classifiers. P-TAME
employs an auxiliary image classifier to extract features from the input image,
bypassing the need to tailor the explanation method to the internal
architecture of the backbone classifier being explained. Unlike traditional
perturbation-based methods, which have high computational requirements, P-TAME
offers an efficient alternative by generating high-resolution explanations in a
single forward pass during inference. We apply P-TAME to explain the decisions
of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image
classifiers. Quantitative and qualitative results show that our method matches
or outperforms previous explainability methods, including model-specific
approaches. Code and trained models will be released upon acceptance.

</details>


### [401] [Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior](https://arxiv.org/pdf/2501.18913)
*Tongda Xu, Xiyan Cai, Xinjie Zhang, Xingtong Ge, Dailan He, Ming Sun, Jingjing Liu, Ya-Qin Zhang, Jian Li, Yan Wang*

Main category: cs.CV

TL;DR: The paper re-evaluates Diffusion Posterior Sampling (DPS), showing it aligns more with MAP than conditional score estimation, and proposes enhancements to improve performance.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that DPS effectively approximates the conditional score, revealing its closer alignment with MAP and proposing improvements.

Method: Examines DPS on 512x512 ImageNet images, identifies flaws in its conditional score estimation, and proposes multi-step gradient ascent and a lightweight conditional score estimator.

Result: DPS's conditional score estimation is flawed, resembling MAP more, and proposed enhancements significantly improve performance.

Conclusion: DPS is better understood as MAP, and the proposed enhancements notably boost its effectiveness.

Abstract: Recent advancements in diffusion models have been leveraged to address
inverse problems without additional training, and Diffusion Posterior Sampling
(DPS) (Chung et al., 2022a) is among the most popular approaches. Previous
analyses suggest that DPS accomplishes posterior sampling by approximating the
conditional score. While in this paper, we demonstrate that the conditional
score approximation employed by DPS is not as effective as previously assumed,
but rather aligns more closely with the principle of maximizing a posterior
(MAP). This assertion is substantiated through an examination of DPS on 512x512
ImageNet images, revealing that: 1) DPS's conditional score estimation
significantly diverges from the score of a well-trained conditional diffusion
model and is even inferior to the unconditional score; 2) The mean of DPS's
conditional score estimation deviates significantly from zero, rendering it an
invalid score estimation; 3) DPS generates high-quality samples with
significantly lower diversity. In light of the above findings, we posit that
DPS more closely resembles MAP than a conditional score estimator, and
accordingly propose the following enhancements to DPS: 1) we explicitly
maximize the posterior through multi-step gradient ascent and projection; 2) we
utilize a light-weighted conditional score estimator trained with only 100
images and 8 GPU hours. Extensive experimental results indicate that these
proposed improvements significantly enhance DPS's performance. The source code
for these improvements is provided in
https://github.com/tongdaxu/Rethinking-Diffusion-Posterior-Sampling-From-Conditional-Score-Estimator-to-Maximizing-a-Posterior.

</details>


### [402] [ViFOR: A Fourier-Enhanced Vision Transformer for Multi-Image Super-Resolution in Earth System](https://arxiv.org/pdf/2502.12427)
*Ehsan Zeraatkar, Salah A Faroughi, Jelena Tešić*

Main category: cs.CV

TL;DR: ViFOR, a novel SR algorithm combining Vision Transformers and Fourier-based INRs, outperforms state-of-the-art methods in enhancing ESM data resolution.


<details>
  <summary>Details</summary>
Motivation: Super-resolution is vital for improving spatial resolution in Earth System Model data, enabling more precise environmental analysis.

Method: ViFOR integrates Vision Transformers with Fourier-based Implicit Neural Representation Networks, using Fourier-based activation functions to capture global context and high-frequency details.

Result: ViFOR achieves significant PSNR improvements (up to 4.18 dB) over existing methods like ViT, SIREN, and SRGANs, excelling in both global and local imagery.

Conclusion: ViFOR demonstrates high effectiveness and potential for advancing high-resolution climate data analysis.

Abstract: Super-resolution (SR) is crucial for enhancing the spatial resolution of
Earth System Model (ESM) data, thereby enabling more precise analysis of
environmental processes. This paper introduces ViFOR, a novel SR algorithm
integrating Vision Transformers (ViTs) with Fourier-based Implicit Neural
Representation Networks (INRs). ViFOR effectively captures global context and
high-frequency details essential for accurate SR reconstruction by embedding
Fourier-based activation functions within the transformer architecture.
Extensive experiments demonstrate that ViFOR consistently outperforms
state-of-the-art methods, including ViT, SIREN, and SRGANs, in terms of Peak
Signal-to-Noise Ratio (PSNR) and Mean Squared Error (MSE) for both global and
local imagery. ViFOR achieves PSNR improvements of up to 4.18 dB, 1.56 dB, and
1.73 dB over ViT on full-image Source Temperature, Shortwave, and Longwave Flux
datasets. These results highlight ViFOR's effectiveness and potential for
advancing high-resolution climate data analysis.

</details>


### [403] [Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images](https://arxiv.org/pdf/2502.13928)
*Shengguang Wu, Fan-Yun Sun, Kaiyue Wen, Nick Haber*

Main category: cs.CV

TL;DR: S-VCO improves VLMs by enhancing visual grounding, reducing hallucinations, and boosting performance in vision-centric tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs neglect image details, leading to errors and hallucinations. The goal is to improve visual grounding.

Method: Proposes S-VCO, a finetuning objective, and MVC dataset for hard contrastive cases to align visual details with text.

Result: Achieves 22% fewer hallucinations and significant gains in vision-centric and general tasks.

Conclusion: S-VCO enhances VLM performance in visually-dependent tasks while maintaining general abilities.

Abstract: Recent studies have shown that Large Vision-Language Models (VLMs) tend to
neglect image content and over-rely on language-model priors, resulting in
errors in visually grounded tasks and hallucinations. We hypothesize that this
issue arises because existing VLMs are not explicitly trained to generate texts
that are accurately grounded in fine-grained image details. To enhance visual
feedback during VLM training, we propose S-VCO (Symmetrical Visual Contrastive
Optimization), a novel finetuning objective that steers the model toward
capturing important visual details and aligning them with corresponding text
tokens. To further facilitate this detailed alignment, we introduce MVC, a
paired image-text dataset built by automatically filtering and augmenting
visual counterfactual data to challenge the model with hard contrastive cases
involving Minimal Visual Contrasts. Experiments show that our method
consistently improves VLM performance across diverse benchmarks covering
various abilities and domains, achieving up to a 22% reduction in
hallucinations, and significant gains in vision-centric and general tasks.
Notably, these improvements become increasingly pronounced in benchmarks with
higher visual dependency. In short, S-VCO offers a significant enhancement of
VLM's visually-dependent task performance while retaining or even improving the
model's general abilities. We opensource our code at https://s-vco.github.io/

</details>


### [404] [DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models](https://arxiv.org/pdf/2502.14779)
*Hongji Yang, Wencheng Han, Yucheng Zhou, Jianbing Shen*

Main category: cs.CV

TL;DR: DC-ControlNet is a flexible framework for multi-condition image generation by decoupling control conditions into hierarchical elements, improving precision and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing ControlNet models lack element-specific control, reducing flexibility and causing condition misunderstandings in multi-conditional image generation.

Method: DC-ControlNet introduces Intra-Element and Inter-Element Controllers to handle individual elements and their interactions, respectively.

Result: DC-ControlNet outperforms existing models in control flexibility and precision for multi-condition image generation.

Conclusion: The framework enhances image generation by enabling precise, flexible control over individual elements and their interactions.

Abstract: In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and
precisely controllable framework for multi-condition image generation. The core
idea behind DC-ControlNet is to decouple control conditions, transforming
global control into a hierarchical system that integrates distinct elements,
contents, and layouts. This enables users to mix these individual conditions
with greater flexibility, leading to more efficient and accurate image
generation control. Previous ControlNet-based models rely solely on global
conditions, which affect the entire image and lack the ability of element- or
region-specific control. This limitation reduces flexibility and can cause
condition misunderstandings in multi-conditional image generation. To address
these challenges, we propose both intra-element and Inter-element Controllers
in DC-ControlNet. The Intra-Element Controller handles different types of
control signals within individual elements, accurately describing the content
and layout characteristics of the object. For interactions between elements, we
introduce the Inter-Element Controller, which accurately handles multi-element
interactions and occlusion based on user-defined relationships. Extensive
evaluations show that DC-ControlNet significantly outperforms existing
ControlNet models and Layout-to-Image generative models in terms of control
flexibility and precision in multi-condition control. Our project website is
available at: https://um-lab.github.io/DC-ControlNet/

</details>


### [405] [Concept Corrector: Erase concepts on the fly for text-to-image diffusion models](https://arxiv.org/pdf/2502.16368)
*Zheling Meng, Bo Peng, Xiaochuan Jin, Yueming Lyu, Wei Wang, Jing Dong, Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces Concept Corrector, a method for erasing unwanted concepts in text-to-image diffusion models by focusing on generated images rather than input texts, achieving effective and direct erasure.


<details>
  <summary>Details</summary>
Motivation: Existing methods for concept erasure in text-to-image models focus on input texts, leading to incomplete erasure due to limited generalization. Addressing this, the paper proposes a direct approach by targeting generated images.

Method: Concept Corrector checks target concepts using visual features from intermediate-generated images and employs Concept Removal Attention to erase unwanted features without altering model parameters.

Result: Experiments show the method effectively erases various unwanted concepts, outperforming existing approaches.

Conclusion: Concept Corrector is the first method to erase concepts dynamically using intermediate images, offering a practical and efficient solution for unwanted content removal.

Abstract: Text-to-image diffusion models have demonstrated the underlying risk of
generating various unwanted content, such as sexual elements. To address this
issue, the task of concept erasure has been introduced, aiming to erase any
undesired concepts that the models can generate. Previous methods, whether
training-based or training-free, have primarily focused on the input side,
i.e., texts. However, they often suffer from incomplete erasure due to
limitations in the generalization from limited prompts to diverse image
content. In this paper, motivated by the notion that concept erasure on the
output side, i.e., generated images, may be more direct and effective, we
propose Concept Corrector. It checks target concepts based on visual features
provided by final generated images predicted at certain time steps. Further, it
incorporates Concept Removal Attention to erase generated concept features. It
overcomes the limitations of existing methods, which are either unable to
remove the concept features that have been generated in images or rely on the
assumption that the related concept words are contained in input prompts. In
the whole pipeline, our method changes no model parameters and only requires a
given target concept as well as the corresponding replacement content, which is
easy to implement. To the best of our knowledge, this is the first erasure
method based on intermediate-generated images, achieving the ability to erase
concepts on the fly. The experiments on various concepts demonstrate its
impressive erasure performance.

</details>


### [406] [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/pdf/2502.18017)
*Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, Feng Zhao*

Main category: cs.CV

TL;DR: ViDoRAG, a multi-agent RAG framework, addresses challenges in visually rich document analysis by integrating multi-modal retrieval and iterative reasoning, outperforming existing methods by 10%.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG methods struggle with visually rich documents due to poor integration of textual and visual features and insufficient reasoning tokens.

Method: ViDoRAG uses a GMM-based hybrid strategy for multi-modal retrieval and an iterative agent workflow (exploration, summarization, reflection) for reasoning.

Result: ViDoRAG achieves over 10% improvement on the ViDoSeek benchmark compared to existing methods.

Conclusion: ViDoRAG effectively bridges gaps in RAG for visually rich documents, demonstrating superior performance and generalization.

Abstract: Understanding information from visually rich documents remains a significant
challenge for traditional Retrieval-Augmented Generation (RAG) methods.
Existing benchmarks predominantly focus on image-based question answering (QA),
overlooking the fundamental challenges of efficient retrieval, comprehension,
and reasoning within dense visual documents. To bridge this gap, we introduce
ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich
documents requiring complex reasoning. Based on it, we identify key limitations
in current RAG approaches: (i) purely visual retrieval methods struggle to
effectively integrate both textual and visual features, and (ii) previous
approaches often allocate insufficient reasoning tokens, limiting their
effectiveness. To address these challenges, we propose ViDoRAG, a novel
multi-agent RAG framework tailored for complex reasoning across visual
documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy
to effectively handle multi-modal retrieval. To further elicit the model's
reasoning capabilities, we introduce an iterative agent workflow incorporating
exploration, summarization, and reflection, providing a framework for
investigating test-time scaling in RAG domains. Extensive experiments on
ViDoSeek validate the effectiveness and generalization of our approach.
Notably, ViDoRAG outperforms existing methods by over 10% on the competitive
ViDoSeek benchmark. The code is available at
https://github.com/Alibaba-NLP/ViDoRAG.

</details>


### [407] [Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection](https://arxiv.org/pdf/2503.10080)
*Zhen Qu, Xian Tao, Xinyi Gong, Shichen Qu, Qiyu Chen, Zhengtao Zhang, Xingang Wang, Guiguang Ding*

Main category: cs.CV

TL;DR: Bayes-PFL improves zero-shot anomaly detection by modeling prompt space as a learnable probability distribution, addressing limitations of manual or single-form prompts.


<details>
  <summary>Details</summary>
Motivation: Existing prompt methods for vision-language models in zero-shot anomaly detection face challenges like expert dependency, limited semantic capture, and poor generalization.

Method: Proposes Bayesian Prompt Flow Learning (Bayes-PFL), modeling prompt space as a probability distribution, and introduces a residual cross-model attention module for better alignment.

Result: Outperforms existing methods on 15 industrial and medical datasets.

Conclusion: Bayes-PFL effectively addresses prompt space limitations and enhances generalization for zero-shot anomaly detection.

Abstract: Recently, vision-language models (e.g. CLIP) have demonstrated remarkable
performance in zero-shot anomaly detection (ZSAD). By leveraging auxiliary data
during training, these models can directly perform cross-category anomaly
detection on target datasets, such as detecting defects on industrial product
surfaces or identifying tumors in organ tissues. Existing approaches typically
construct text prompts through either manual design or the optimization of
learnable prompt vectors. However, these methods face several challenges: 1)
handcrafted prompts require extensive expert knowledge and trial-and-error; 2)
single-form learnable prompts struggle to capture complex anomaly semantics;
and 3) an unconstrained prompt space limits generalization to unseen
categories. To address these issues, we propose Bayesian Prompt Flow Learning
(Bayes-PFL), which models the prompt space as a learnable probability
distribution from a Bayesian perspective. Specifically, a prompt flow module is
designed to learn both image-specific and image-agnostic distributions, which
are jointly utilized to regularize the text prompt space and improve the
model's generalization on unseen categories. These learned distributions are
then sampled to generate diverse text prompts, effectively covering the prompt
space. Additionally, a residual cross-model attention (RCA) module is
introduced to better align dynamic text embeddings with fine-grained image
features. Extensive experiments on 15 industrial and medical datasets
demonstrate our method's superior performance. The code is available at
https://github.com/xiaozhen228/Bayes-PFL.

</details>


### [408] [Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning](https://arxiv.org/pdf/2503.13360)
*Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye*

Main category: cs.CV

TL;DR: MLLMs struggle with visual attention in reasoning tasks. TVC improves performance by dynamically pruning and shifting visual inputs.


<details>
  <summary>Details</summary>
Motivation: MLLMs lose focus on visual information during reasoning, leading to text-dominated outputs.

Method: Ablated image inputs during reasoning, proposed TVC to shift and compress visual tokens dynamically.

Result: Achieved +3.4 points over previous SOTA on five benchmarks, with only ~2% accuracy drop when images were removed.

Conclusion: TVC effectively enhances multimodal reasoning by maintaining visual attention.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated
enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting
to advanced, product-oriented solutions like OpenAI o1. During our
re-implementation of this model, we noticed that in multimodal tasks requiring
visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to
maintain focus on the visual information, in other words, MLLMs suffer from a
gradual decline in attention to visual information as reasoning progresses,
causing text-over-relied outputs. To investigate this, we ablate image inputs
during long-chain reasoning. Concretely, we truncate the reasoning process
midway, then re-complete the reasoning process with the input image removed. We
observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the
model's textual outputs dominate the following reasoning process. Motivated by
this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts
image input to critical reasoning stages and compresses redundant visual tokens
via dynamic pruning. This methodology helps the model retain attention to the
visual components throughout the reasoning. Our approach achieves
state-of-the-art performance on average across five mathematical reasoning
benchmarks (+3.4 points vs previous sota), demonstrating the effectiveness of
TVC in enhancing multimodal reasoning systems.

</details>


### [409] [SceneSplat: Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining](https://arxiv.org/pdf/2503.18052)
*Yue Li, Qi Ma, Runyi Yang, Huapeng Li, Mengjiao Ma, Bin Ren, Nikola Popovic, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, Martin R. Oswald, Danda Pani Paudel*

Main category: cs.CV

TL;DR: SceneSplat is the first large-scale 3D indoor scene understanding method using 3D Gaussian Splatting (3DGS) with self-supervised learning and a new dataset, SceneSplat-7K.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D or textual modalities, lacking a pure 3D data approach for end-to-end semantic learning.

Method: Proposes SceneSplat, integrating semantic reasoning into 3DGS, and a self-supervised learning scheme for 3D feature learning. Introduces SceneSplat-7K, a large-scale 3DGS dataset.

Result: Outperforms baselines on SceneSplat-7K, demonstrating the method's effectiveness.

Conclusion: SceneSplat advances 3D scene understanding by enabling pure 3D data processing and providing a benchmark dataset.

Abstract: Recognizing arbitrary or previously unseen categories is essential for
comprehensive real-world 3D scene understanding. Currently, all existing
methods rely on 2D or textual modalities during training or together at
inference. This highlights the clear absence of a model capable of processing
3D data alone for learning semantics end-to-end, along with the necessary data
to train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as
the de facto standard for 3D scene representation across various vision tasks.
However, effectively integrating semantic reasoning into 3DGS in a
generalizable manner remains an open challenge. To address these limitations,
we introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene
understanding approach that operates natively on 3DGS. Furthermore, we propose
a self-supervised learning scheme that unlocks rich 3D feature learning from
unlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K,
the first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes
derived from seven established datasets, such as ScanNet and Matterport3D.
Generating SceneSplat-7K required computational resources equivalent to 150 GPU
days on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning
for indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the
significant benefit of the proposed method over the established baselines.

</details>


### [410] [Video Motion Graphs](https://arxiv.org/pdf/2503.20218)
*Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, Yang Zhou*

Main category: cs.CV

TL;DR: Video Motion Graphs generate realistic human motion videos using reference videos and conditional signals, employing HMInterp for seamless interpolation.


<details>
  <summary>Details</summary>
Motivation: To create realistic human motion videos by combining retrieval and interpolation methods, addressing challenges in complex motion scenarios like dancing.

Method: Uses HMInterp, a dual-branch interpolation model combining Motion Diffusion Model and diffusion-based VFI, with condition progressive training for high-quality results.

Result: Outperforms existing generative- and retrieval-based methods in multi-modal conditioned human motion video generation.

Conclusion: Video Motion Graphs effectively combines retrieval and interpolation for high-quality motion video synthesis, validated by superior performance.

Abstract: We present Video Motion Graphs, a system designed to generate realistic human
motion videos. Using a reference video and conditional signals such as music or
motion tags, the system synthesizes new videos by first retrieving video clips
with gestures matching the conditions and then generating interpolation frames
to seamlessly connect clip boundaries. The core of our approach is HMInterp, a
robust Video Frame Interpolation (VFI) model that enables seamless
interpolation of discontinuous frames, even for complex motion scenarios like
dancing. HMInterp i) employs a dual-branch interpolation approach, combining a
Motion Diffusion Model for human skeleton motion interpolation with a
diffusion-based video frame interpolation model for final frame generation. ii)
adopts condition progressive training to effectively leverage identity strong
and weak conditions, such as images and pose. These designs ensure both high
video texture quality and accurate motion trajectory. Results show that our
Video Motion Graphs outperforms existing generative- and retrieval-based
methods for multi-modal conditioned human motion video generation. Project page
can be found at https://h-liu1997.github.io/Video-Motion-Graphs/

</details>


### [411] [Shape and Texture Recognition in Large Vision-Language Models](https://arxiv.org/pdf/2503.23062)
*Sagi Eppel, Mor Bismut, Alona Faktor-Strugatski*

Main category: cs.CV

TL;DR: The paper introduces the LAS&T dataset to benchmark LVLMs on shape and texture recognition, revealing their limitations compared to humans and simple nets.


<details>
  <summary>Details</summary>
Motivation: To assess how well LVLMs understand fundamental visual concepts like shapes and textures, which are crucial for general visual perception.

Method: Created the LAS&T dataset via unsupervised extraction from natural images, then tested LVLMs on shape and texture recognition tasks.

Result: LVLMs underperform humans in shape recognition and 2D texture tasks but approach human-level in 3D material recognition. Simple nets outperform LVLMs.

Conclusion: Current LVLMs have significant deficiencies in understanding basic visual concepts, highlighting a gap in their foundational visual understanding.

Abstract: Shapes and textures are the basic building blocks of visual perception. The
ability to identify shapes regardless of orientation, texture, or context, and
to recognize textures and materials independently of their associated objects,
is essential for a general visual understanding of the world. This work
introduces the Large Shape and Textures dataset (LAS&T), a giant collection of
highly diverse shapes and textures, created by unsupervised extraction of
patterns from natural images. This dataset is used to benchmark how effectively
leading Large Vision-Language Models (LVLMs) understand shapes, textures, and
materials in 2D and 3D scenes. For shape recognition, we test the models'
ability to match images of identical shapes that differ in orientation,
texture, color, or environment. Our results show that the shape recognition
capabilities of the LVLMs remain significantly below human performance. LVLMs
rely predominantly on high-level and semantic features and struggle with
abstract shapes lacking clear class associations. For texture and material
recognition, we evaluated the models' ability to identify images with identical
textures and materials across different objects and environments.
Interestingly, leading LVLMs approach human-level performance in recognizing
materials in 3D scenes, yet substantially underperform humans when identifying
simpler more abstract 2D textures. These results are consistent across a wide
range of leading VLMs (GPT/Gemini/LLama/Qwen) and foundation vision models
(DINO/CLIP), exposing major deficiencies in the ability of leading models to
understand fundamental visual concepts. In contrast, simple nets trained
directly for these tasks achieve high accuracy. The LAS&T dataset has been made
available.

</details>


### [412] [PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization](https://arxiv.org/pdf/2503.24135)
*Alexis Guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger*

Main category: cs.CV

TL;DR: PixelCAM introduces a multi-task WSOL method for histology images, addressing asynchronous convergence and localization issues by training classification and localization simultaneously in pixel-feature space.


<details>
  <summary>Details</summary>
Motivation: Standard WSOL methods struggle with histology images due to under-/over-activation, asynchronous convergence, and OOD challenges.

Method: PixelCAM trains a pixel-wise classifier in shared pixel-feature space using pseudo-labels and partial-cross entropy, compatible with CNN/transformer architectures.

Result: Simultaneous training improves localization and classification, with PixelCAM being cost-effective and adaptable.

Conclusion: PixelCAM effectively addresses WSOL limitations in histology images, offering a flexible and efficient solution.

Abstract: Weakly supervised object localization (WSOL) methods allow training models to
classify images and localize ROIs. WSOL only requires low-cost image-class
annotations yet provides a visually interpretable classifier. Standard WSOL
methods rely on class activation mapping (CAM) methods to produce spatial
localization maps according to a single- or two-step strategy. While both
strategies have made significant progress, they still face several limitations
with histology images. Single-step methods can easily result in under- or
over-activation due to the limited visual ROI saliency in histology images and
scarce localization cues. They also face the well-known issue of asynchronous
convergence between classification and localization tasks. The two-step
approach is sub-optimal because it is constrained to a frozen classifier,
limiting the capacity for localization. Moreover, these methods also struggle
when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task
approach for WSOL is introduced for simultaneous training of both tasks to
address the asynchronous convergence problem. In particular, localization is
performed in the pixel-feature space of an image encoder that is shared with
classification. This allows learning discriminant features and accurate
delineation of foreground/background regions to support ROI localization and
image classification. We propose PixelCAM, a cost-effective
foreground/background pixel-wise classifier in the pixel-feature space that
allows for spatial object localization. Using partial-cross entropy, PixelCAM
is trained using pixel pseudo-labels collected from a pretrained WSOL model.
Both image and pixel-wise classifiers are trained simultaneously using standard
gradient descent. In addition, our pixel classifier can easily be integrated
into CNN- and transformer-based architectures without any modifications.

</details>


### [413] [OmniTalker: One-shot Real-time Text-Driven Talking Audio-Video Generation With Multimodal Style Mimicking](https://arxiv.org/pdf/2504.02433)
*Zhongjian Wang, Peng Zhang, Jinwei Qi, Guangyuan Wang, Chaonan Ji, Sheng Xu, Bang Zhang, Liefeng Bo*

Main category: cs.CV

TL;DR: OmniTalker is a unified framework for text-driven talking head generation, using a dual-branch diffusion transformer to jointly produce synchronized audio-video content with real-time performance.


<details>
  <summary>Details</summary>
Motivation: Text-driven talking head generation is underexplored compared to audio-driven methods, and existing approaches lack joint modeling of speech and facial styles in real time.

Method: A dual-branch diffusion transformer (DiT) architecture with cross-modal fusion for audio and video generation, leveraging in-context learning for style capture without explicit modules.

Result: OmniTalker achieves real-time inference at 25 FPS, outperforming existing methods in generation quality, style consistency, and audio-video synchronization.

Conclusion: OmniTalker is the first one-shot framework for real-time joint modeling of speech and facial styles, demonstrating superior performance and efficiency.

Abstract: Although significant progress has been made in audio-driven talking head
generation, text-driven methods remain underexplored. In this work, we present
OmniTalker, a unified framework that jointly generates synchronized talking
audio-video content from input text while emulating the speaking and facial
movement styles of the target identity, including speech characteristics, head
motion, and facial dynamics. Our framework adopts a dual-branch diffusion
transformer (DiT) architecture, with one branch dedicated to audio generation
and the other to video synthesis. At the shallow layers, cross-modal fusion
modules are introduced to integrate information between the two modalities. In
deeper layers, each modality is processed independently, with the generated
audio decoded by a vocoder and the video rendered using a GAN-based
high-quality visual renderer. Leveraging the in-context learning capability of
DiT through a masked-infilling strategy, our model can simultaneously capture
both audio and visual styles without requiring explicit style extraction
modules. Thanks to the efficiency of the DiT backbone and the optimized visual
renderer, OmniTalker achieves real-time inference at 25 FPS. To the best of our
knowledge, OmniTalker is the first one-shot framework capable of jointly
modeling speech and facial styles in real time. Extensive experiments
demonstrate its superiority over existing methods in terms of generation
quality, particularly in preserving style consistency and ensuring precise
audio-video synchronization, all while maintaining efficient inference.

</details>


### [414] [TestDG: Test-time Domain Generalization for Continual Test-time Adaptation](https://arxiv.org/pdf/2504.04981)
*Sohyun Lee, Nayeong Kim, Juwon Kang, Seong Joon Oh, Suha Kwak*

Main category: cs.CV

TL;DR: TestDG introduces a novel online test-time domain generalization framework for continual test-time adaptation (CTTA), focusing on learning domain-invariant features for current and future unseen domains.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods lack generalization to arbitrary future test domains, limiting their effectiveness.

Method: Proposes a new model architecture, adaptation strategy, data structure, and optimization algorithm to learn and manage domain-invariant features during testing.

Result: Achieved state-of-the-art performance on four CTTA benchmarks and demonstrated superior generalization to unseen domains.

Conclusion: TestDG effectively addresses the limitation of existing CTTA methods by improving generalization to future test domains.

Abstract: This paper studies continual test-time adaptation (CTTA), the task of
adapting a model to constantly changing unseen domains in testing while
preserving previously learned knowledge. Existing CTTA methods mostly focus on
adaptation to the current test domain only, overlooking generalization to
arbitrary test domains a model may face in the future. To tackle this
limitation, we present a novel online test-time domain generalization framework
for CTTA, dubbed TestDG. TestDG aims to learn features invariant to both
current and previous test domains on the fly during testing, improving the
potential for effective generalization to future domains. To this end, we
propose a new model architecture and a test-time adaptation strategy dedicated
to learning domain-invariant features, along with a new data structure and
optimization algorithm for effectively managing information from previous test
domains. TestDG achieved state of the art on four public CTTA benchmarks.
Moreover, it showed superior generalization to unseen test domains.

</details>


### [415] [MMLA: Multi-Environment, Multi-Species, Low-Altitude Drone Dataset](https://arxiv.org/pdf/2504.07744)
*Jenna Kline, Samuel Stevens, Guy Maalouf, Camille Rondeau Saint-Jean, Dat Nguyen Ngoc, Majid Mirmehdi, David Guerin, Tilo Burghardt, Elzbieta Pastucha, Blair Costelloe, Matthew Watson, Thomas Richardson, Ulrik Pagh Schultz Lundquist*

Main category: cs.CV

TL;DR: MMLA dataset improves wildlife detection in drone imagery, boosting YOLOv11m performance by 52 points over baseline.


<details>
  <summary>Details</summary>
Motivation: Standard detection models like YOLO struggle with generalization across locations and rare species, limiting automated drone use in conservation.

Method: Introduces MMLA, a multi-environment, multi-species drone dataset with 811K annotations from 37 videos across three sites. Fine-tunes YOLOv11m on MMLA.

Result: Fine-tuning YOLOv11m on MMLA improves mAP50 to 82%, a 52-point gain over baseline.

Conclusion: Diverse training data is crucial for robust animal detection in autonomous drone systems.

Abstract: Real-time wildlife detection in drone imagery supports critical ecological
and conservation monitoring. However, standard detection models like YOLO often
fail to generalize across locations and struggle with rare species, limiting
their use in automated drone deployments. We present MMLA, a novel
multi-environment, multi-species, low-altitude drone dataset collected across
three sites (Ol Pejeta Conservancy and Mpala Research Centre in Kenya, and The
Wilds in Ohio), featuring six species (zebras, giraffes, onagers, and African
wild dogs). The dataset contains 811K annotations from 37 high-resolution
videos. Baseline YOLO models show performance disparities across locations
while fine-tuning YOLOv11m on MMLA improves mAP50 to 82%, a 52-point gain over
baseline. Our results underscore the need for diverse training data to enable
robust animal detection in autonomous drone systems.

</details>


### [416] [Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts of Labeled Data](https://arxiv.org/pdf/2504.13077)
*Prasanna Reddy Pulakurthi, Majid Rabbani, Celso M. de Melo, Sohail A. Dianat, Raghuveer M. Rao*

Main category: cs.CV

TL;DR: A dual-region augmentation method improves model robustness and reduces reliance on labeled data by applying noise to foregrounds and shuffling backgrounds, outperforming existing techniques in SFDA and ReID tasks.


<details>
  <summary>Details</summary>
Motivation: To reduce dependence on large labeled datasets and enhance model adaptability across diverse computer vision tasks like SFDA and ReID.

Method: Targeted data transformations: random noise perturbations on foreground objects and spatial shuffling of background patches.

Result: Outperforms existing methods on PACS (SFDA) and Market-1501/DukeMTMC-reID (ReID), showing significant accuracy improvements.

Conclusion: The method enhances model generalization across domains and reduces the need for manual annotations, validated by strong performance in SFDA and ReID.

Abstract: This paper introduces a novel dual-region augmentation approach designed to
reduce reliance on large-scale labeled datasets while improving model
robustness and adaptability across diverse computer vision tasks, including
source-free domain adaptation (SFDA) and person re-identification (ReID). Our
method performs targeted data transformations by applying random noise
perturbations to foreground objects and spatially shuffling background patches.
This effectively increases the diversity of the training data, improving model
robustness and generalization. Evaluations on the PACS dataset for SFDA
demonstrate that our augmentation strategy consistently outperforms existing
methods, achieving significant accuracy improvements in both single-target and
multi-target adaptation settings. By augmenting training data through
structured transformations, our method enables model generalization across
domains, providing a scalable solution for reducing reliance on manually
annotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID
datasets validate the effectiveness of our approach for person ReID, surpassing
traditional augmentation techniques. The code is available at
https://github.com/PrasannaPulakurthi/Foreground-Background-Augmentation

</details>


### [417] [Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/pdf/2504.19475)
*Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards*

Main category: cs.CV

TL;DR: Prisma is an open-source framework for vision mechanistic interpretability, offering tools, pre-trained weights, and educational resources. It reveals insights like lower sparsity in vision SAEs compared to language SAEs.


<details>
  <summary>Details</summary>
Motivation: Progress in vision mechanistic interpretability lags behind language models due to lack of accessible frameworks and pre-trained weights.

Method: Prisma provides a toolkit for 75+ vision/video transformers, SAE/transcoder/crosscoder training, pre-trained weights, activation caching, circuit analysis, and visualization tools.

Result: Findings include lower sparsity in vision SAEs than language SAEs and cases where SAE reconstructions reduce model loss.

Conclusion: Prisma accelerates vision interpretability research, enabling new insights and lowering entry barriers.

Abstract: Robust tooling and publicly available pre-trained models have helped drive
recent advances in mechanistic interpretability for language models. However,
similar progress in vision mechanistic interpretability has been hindered by
the lack of accessible frameworks and pre-trained weights. We present Prisma
(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an
open-source framework designed to accelerate vision mechanistic
interpretability research, providing a unified toolkit for accessing 75+ vision
and video transformers; support for sparse autoencoder (SAE), transcoder, and
crosscoder training; a suite of 80+ pre-trained SAE weights; activation
caching, circuit analysis tools, and visualization tools; and educational
resources. Our analysis reveals surprising findings, including that effective
vision SAEs can exhibit substantially lower sparsity patterns than language
SAEs, and that in some instances, SAE reconstructions can decrease model loss.
Prisma enables new research directions for understanding vision model internals
while lowering barriers to entry in this emerging field.

</details>


### [418] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/pdf/2505.03414)
*Fangming Cui, Yonggang Zhang, Xuan Wang, Xinmei Tian, Jun Yu*

Main category: cs.CV

TL;DR: A novel Features Matrix (FM) approach is proposed to enhance Vision-Language Models (VLMs) for target-unspecific tasks by preserving general knowledge and preventing overfitting.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods for VLMs struggle with target-unspecific tasks due to overfitting, which causes loss of general knowledge.

Method: The FM approach extracts and leverages general knowledge by capturing diverse input semantics deeply and finely, preserving essential knowledge.

Result: FM is compatible with existing frameworks and significantly improves performance in target-unspecific tasks, achieving state-of-the-art results.

Conclusion: The FM method effectively mitigates overfitting and enhances VLMs for generalizable tasks, demonstrating broad applicability and superior performance.

Abstract: Recent developments in prompt learning of large Vision-Language Models (VLMs)
have significantly improved performance in target-specific tasks. However,
these prompting methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge. The
general knowledge has a strong promotion on target-unspecific tasks. To
alleviate this issue, we propose a novel Features Matrix (FM) approach designed
to enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks (base-to-novel generalization, domain generalization, and cross-dataset
generalization), achieving state-of-the-art performance.

</details>


### [419] [S3D: Sketch-Driven 3D Model Generation](https://arxiv.org/pdf/2505.04185)
*Hail Song, Wonsik Shin, Naeun Lee, Soomin Chung, Nojun Kwak, Woontack Woo*

Main category: cs.CV

TL;DR: S3D is a framework that converts 2D sketches into detailed 3D models using a U-Net-based encoder-decoder architecture and a style-alignment loss for improved fidelity.


<details>
  <summary>Details</summary>
Motivation: The challenge of generating high-quality 3D models from sparse and ambiguous 2D sketch data.

Method: Uses a U-Net-based encoder-decoder to convert sketches into face segmentation masks, then generates 3D models with a style-alignment loss for consistency. Augmentation techniques are applied for robustness.

Result: The framework effectively generates high-quality 3D models from sketch inputs.

Conclusion: S3D demonstrates a streamlined and effective approach for 3D model generation from sketches, with publicly available source code.

Abstract: Generating high-quality 3D models from 2D sketches is a challenging task due
to the inherent ambiguity and sparsity of sketch data. In this paper, we
present S3D, a novel framework that converts simple hand-drawn sketches into
detailed 3D models. Our method utilizes a U-Net-based encoder-decoder
architecture to convert sketches into face segmentation masks, which are then
used to generate a 3D representation that can be rendered from novel views. To
ensure robust consistency between the sketch domain and the 3D output, we
introduce a novel style-alignment loss that aligns the U-Net bottleneck
features with the initial encoder outputs of the 3D generation module,
significantly enhancing reconstruction fidelity. To further enhance the
network's robustness, we apply augmentation techniques to the sketch dataset.
This streamlined framework demonstrates the effectiveness of S3D in generating
high-quality 3D models from sketch inputs. The source code for this project is
publicly available at https://github.com/hailsong/S3D.

</details>


### [420] [MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection](https://arxiv.org/pdf/2505.11282)
*Shrutarv Awasthi, Anas Gouda, Sven Franke, Jérôme Rutinowski, Frank Hoffmann, Moritz Roidl*

Main category: cs.CV

TL;DR: MTevent is a dataset for 6D pose estimation and moving object detection in high-speed, dynamic environments, addressing RGB camera limitations with event cameras.


<details>
  <summary>Details</summary>
Motivation: High-speed mobile robots face perception challenges due to RGB camera limitations like motion blur and latency, prompting the need for event-based solutions.

Method: A stereo-event camera and RGB camera captured 75 scenes with 16 unique objects under challenging conditions (e.g., extreme angles, lighting, occlusions).

Result: Baseline 6D pose estimation using RGB images achieved an Average Recall of 0.22, highlighting RGB limitations in dynamic settings.

Conclusion: MTevent provides a novel resource to advance event-based vision in robotics, addressing high-speed motion and real-world object interactions.

Abstract: Mobile robots are reaching unprecedented speeds, with platforms like Unitree
B2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s.
However, effectively utilizing such speeds remains a challenge due to the
limitations of RGB cameras, which suffer from motion blur and fail to provide
real-time responsiveness. Event cameras, with their asynchronous operation, and
low-latency sensing, offer a promising alternative for high-speed robotic
perception. In this work, we introduce MTevent, a dataset designed for 6D pose
estimation and moving object detection in highly dynamic environments with
large detection distances. Our setup consists of a stereo-event camera and an
RGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16
unique objects under challenging conditions such as extreme viewing angles,
varying lighting, and occlusions. MTevent is the first dataset to combine
high-speed motion, long-range perception, and real-world object interactions,
making it a valuable resource for advancing event-based vision in robotics. To
establish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's
FoundationPose on RGB images, achieving an Average Recall of 0.22 with
ground-truth masks, highlighting the limitations of RGB-based approaches in
such dynamic settings. With MTevent, we provide a novel resource to improve
perception models and foster further research in high-speed robotic vision. The
dataset is available for download
https://huggingface.co/datasets/anas-gouda/MTevent

</details>


### [421] [AvatarShield: Visual Reinforcement Learning for Human-Centric Video Forgery Detection](https://arxiv.org/pdf/2505.15173)
*Zhipei Xu, Xuanyu Zhang, Xing Zhou, Jian Zhang*

Main category: cs.CV

TL;DR: AvatarShield is an interpretable MLLM-based framework for detecting human-centric fake videos, addressing challenges like poor generalization and scalability in existing methods. It uses GRPO and a dual-encoder architecture for precise detection, validated on the FakeHumanVid benchmark.


<details>
  <summary>Details</summary>
Motivation: The rise of AIGC technologies, especially in video generation, threatens information integrity and public trust. Current detection methods lack robustness for human-centric videos, which are riskier due to realism and ethical concerns.

Method: Proposes AvatarShield, leveraging Group Relative Policy Optimization (GRPO) and a dual-encoder architecture combining high-level semantics and low-level artifacts. Avoids costly text annotations via accuracy and temporal compensation rewards.

Result: Outperforms existing methods in in-domain and cross-domain detection, validated on the FakeHumanVid benchmark. Sets a new standard for human-centric video forensics.

Conclusion: AvatarShield offers a scalable, interpretable solution for detecting human-centric fake videos, addressing key limitations of current approaches.

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
technologies, particularly in video generation, has led to unprecedented
creative capabilities but also increased threats to information integrity,
identity security, and public trust. Existing detection methods, while
effective in general scenarios, lack robust solutions for human-centric videos,
which pose greater risks due to their realism and potential for legal and
ethical misuse. Moreover, current detection approaches often suffer from poor
generalization, limited scalability, and reliance on labor-intensive supervised
fine-tuning. To address these challenges, we propose AvatarShield, the first
interpretable MLLM-based framework for detecting human-centric fake videos,
enhanced via Group Relative Policy Optimization (GRPO). Through our carefully
designed accuracy detection reward and temporal compensation reward, it
effectively avoids the use of high-cost text annotation data, enabling precise
temporal modeling and forgery detection. Meanwhile, we design a dual-encoder
architecture, combining high-level semantic reasoning and low-level artifact
amplification to guide MLLMs in effective forgery detection. We further collect
FakeHumanVid, a large-scale human-centric video benchmark that includes
synthesis methods guided by pose, audio, and text inputs, enabling rigorous
evaluation of detection methods in real-world scenes. Extensive experiments
show that AvatarShield significantly outperforms existing approaches in both
in-domain and cross-domain detection, setting a new standard for human-centric
video forensics.

</details>


### [422] [CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI](https://arxiv.org/pdf/2505.16452)
*Mohamed S. Elmahdy, Marius Staring, Patrick J. H. de Koning, Samer Alabed, Mahan Salehi, Faisal Alandejani, Michael Sharkey, Ziad Aldabbagh, Andrew J. Swift, Rob J. van der Geest*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end deep learning model for joint groupwise registration and segmentation in cardiac cine-MRI to improve cardiac function assessment by combining LVEF and myocardial strain.


<details>
  <summary>Details</summary>
Motivation: LVEF has limitations due to variability and insensitivity in certain cardiac dysfunctions. Combining LVEF with myocardial strain provides a more comprehensive assessment, but existing methods perform these tasks separately, limiting efficiency.

Method: An end-to-end deep learning model (anatomically-guided Deep GW network) is developed to jointly estimate groupwise registration and segmentation for cardiac cine-MRI images.

Result: The model outperformed conventional GW registration and other DL-based methods, improving performance and reducing computation time.

Conclusion: The proposed model enhances cardiac function assessment by integrating registration and segmentation, offering a more efficient and accurate solution.

Abstract: Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.

</details>


### [423] [Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection](https://arxiv.org/pdf/2505.16512)
*Jiaxin Liu, Jia Wang, Saihui Hou, Min Ren, Huijia Wu, Long Ma, Renwang Pei, Zhaofeng He*

Main category: cs.CV

TL;DR: The paper introduces DigiFakeAV, a large-scale dataset for detecting deepfake videos generated by diffusion models, and proposes DigiShield, a detection method achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: The rise of diffusion-based deepfake technology poses severe security threats, challenging existing detection methods due to its realism and flexibility.

Method: The authors create DigiFakeAV using five digital human generation methods and voice cloning, then propose DigiShield, a detection model using spatiotemporal and cross-modal fusion.

Result: DigiFakeAV's high misrecognition rate (68%) and existing detectors' poor performance highlight its challenge. DigiShield achieves SOTA results and generalizes well.

Conclusion: DigiFakeAV and DigiShield address the gap in detecting advanced deepfakes, offering a robust solution for future security threats.

Abstract: In recent years, the explosive advancement of deepfake technology has posed a
critical and escalating threat to public security: diffusion-based digital
human generation. Unlike traditional face manipulation methods, such models can
generate highly realistic videos with consistency via multimodal control
signals. Their flexibility and covertness pose severe challenges to existing
detection strategies. To bridge this gap, we introduce DigiFakeAV, the new
large-scale multimodal digital human forgery dataset based on diffusion models.
Leveraging five of the latest digital human generation methods and a voice
cloning method, we systematically construct a dataset comprising 60,000 videos
(8.4 million frames), covering multiple nationalities, skin tones, genders, and
real-world scenarios, significantly enhancing data diversity and realism. User
studies demonstrate that the misrecognition rate by participants for DigiFakeAV
reaches as high as 68%. Moreover, the substantial performance degradation of
existing detection models on our dataset further highlights its challenges. To
address this problem, we propose DigiShield, an effective detection baseline
based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D
spatiotemporal features of videos and the semantic-acoustic features of audio,
DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and
shows strong generalization on other datasets.

</details>


### [424] [LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud Active Learning](https://arxiv.org/pdf/2505.18924)
*Chenxi Li, Nuo Chen, Fengyun Tan, Yantong Chen, Bochun Yuan, Tianrui Li, Chongshou Li*

Main category: cs.CV

TL;DR: A novel active learning framework for 3D point cloud semantic segmentation integrates LLMs to create hierarchical label structures and guide uncertainty-based sample selection, improving performance under low annotation budgets.


<details>
  <summary>Details</summary>
Motivation: Prior methods treat labels as flat and independent, ignoring semantic structure. This work leverages LLMs to capture hierarchical relationships and improve sample selection.

Method: Uses LLM prompting to generate multi-level semantic taxonomies and introduces a recursive uncertainty projection mechanism for uncertainty propagation across hierarchy levels.

Result: Achieves up to 4% mIoU improvement under very low annotation budgets (e.g., 0.02%), outperforming existing baselines on S3DIS and ScanNet v2.

Conclusion: Demonstrates the potential of LLMs as knowledge priors in 3D vision and establishes hierarchical uncertainty modeling for efficient point cloud annotation.

Abstract: We present a novel active learning framework for 3D point cloud semantic
segmentation that, for the first time, integrates large language models (LLMs)
to construct hierarchical label structures and guide uncertainty-based sample
selection. Unlike prior methods that treat labels as flat and independent, our
approach leverages LLM prompting to automatically generate multi-level semantic
taxonomies and introduces a recursive uncertainty projection mechanism that
propagates uncertainty across hierarchy levels. This enables spatially diverse,
label-aware point selection that respects the inherent semantic structure of 3D
scenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to
4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),
substantially outperforming existing baselines. Our results highlight the
untapped potential of LLMs as knowledge priors in 3D vision and establish
hierarchical uncertainty modeling as a powerful paradigm for efficient point
cloud annotation.

</details>


### [425] [InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts](https://arxiv.org/pdf/2505.19028)
*Minzhi Lin, Tianchi Xie, Mengchen Liu, Yilin Ye, Changjian Chen, Shixia Liu*

Main category: cs.CV

TL;DR: InfoChartQA is a benchmark for evaluating MLLMs on infographic chart understanding, featuring paired infographic and plain charts with visual-element-based questions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack paired plain charts and visual-element-based questions, limiting evaluation of MLLMs' infographic chart understanding.

Method: Introduces InfoChartQA with 5,642 chart pairs and visual-element-based questions to assess MLLMs.

Result: Evaluation shows MLLMs perform poorly on infographic charts, especially for visual-element-based questions.

Conclusion: InfoChartQA enables fine-grained analysis and highlights opportunities for improving MLLMs in infographic understanding.

Abstract: Understanding infographic charts with design-driven visual elements (e.g.,
pictograms, icons) requires both visual recognition and reasoning, posing
challenges for multimodal large language models (MLLMs). However, existing
visual-question answering benchmarks fall short in evaluating these
capabilities of MLLMs due to the lack of paired plain charts and
visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a
benchmark for evaluating MLLMs on infographic chart understanding. It includes
5,642 pairs of infographic and plain charts, each sharing the same underlying
data but differing in visual presentations. We further design
visual-element-based questions to capture their unique visual designs and
communicative intent. Evaluation of 20 MLLMs reveals a substantial performance
decline on infographic charts, particularly for visual-element-based questions
related to metaphors. The paired infographic and plain charts enable
fine-grained error analysis and ablation studies, which highlight new
opportunities for advancing MLLMs in infographic chart understanding. We
release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.

</details>


### [426] [Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation](https://arxiv.org/pdf/2505.19746)
*Jakov Samardžija, Donik Vršnak, Sven Lončarić*

Main category: cs.CV

TL;DR: The paper proposes using StyleGAN to generate synthetic 3R biopsy images to address class imbalance in training deep learning models for acute cellular rejection (ACR) classification. Combining synthetic and real data improves model performance.


<details>
  <summary>Details</summary>
Motivation: The rarity of high-grade rejection (3R) cases makes training robust deep learning models challenging due to class imbalance.

Method: Histogram equalization was applied for image standardization, followed by StyleGAN training on 3R patches to generate synthetic images. ResNet-18 classifiers were trained using real and synthetic data in various configurations.

Result: Models using synthetic data, especially combined with real samples, outperformed those trained solely on real data, achieving strong precision and recall.

Conclusion: Hybrid training with synthetic and real data is effective, demonstrating the potential of GAN-based augmentation in biomedical image analysis with limited datasets.

Abstract: Accurate identification of acute cellular rejection (ACR) in endomyocardial
biopsies is essential for effective management of heart transplant patients.
However, the rarity of high-grade rejection cases (3R) presents a significant
challenge for training robust deep learning models. This work addresses the
class imbalance problem by leveraging synthetic data generation using StyleGAN
to augment the limited number of real 3R images. Prior to GAN training,
histogram equalization was applied to standardize image appearance and improve
the consistency of tissue representation. StyleGAN was trained on available 3R
biopsy patches and subsequently used to generate 10,000 realistic synthetic
images. These were combined with real 0R samples, that is samples without
rejection, in various configurations to train ResNet-18 classifiers for binary
rejection classification.
  Three classifier variants were evaluated: one trained on real 0R and
synthetic 3R images, another using both synthetic and additional real samples,
and a third trained solely on real data. All models were tested on an
independent set of real biopsy images. Results demonstrate that synthetic data
improves classification performance, particularly when used in combination with
real samples. The highest-performing model, which used both real and synthetic
images, achieved strong precision and recall for both classes. These findings
underscore the value of hybrid training strategies and highlight the potential
of GAN-based data augmentation in biomedical image analysis, especially in
domains constrained by limited annotated datasets.

</details>


### [427] [Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM](https://arxiv.org/pdf/2505.19901)
*Peng Liu, Xiaoming Ren, Fengkai Liu, Qingsong Xie, Quanlong Zheng, Yanhao Zhang, Haonan Lu, Yujiu Yang*

Main category: cs.CV

TL;DR: Dynamic-I2V integrates MLLMs with DiT for improved motion controllability and temporal coherence in image-to-video generation, outperforming existing methods by significant margins.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in complex scenes requiring nuanced motion and object-action understanding in I2V generation.

Method: Combines Multimodal Large Language Models (MLLMs) with a diffusion transformer (DiT) for joint encoding of visual and textual conditions.

Result: Achieves state-of-the-art performance with improvements of 42.5%, 7.9%, and 11.8% in dynamic range, controllability, and quality.

Conclusion: Dynamic-I2V excels in I2V generation, validated by the proposed DIVE benchmark for dynamic quality assessment.

Abstract: Recent advancements in image-to-video (I2V) generation have shown promising
performance in conventional scenarios. However, these methods still encounter
significant challenges when dealing with complex scenes that require a deep
understanding of nuanced motion and intricate object-action relationships. To
address these challenges, we present Dynamic-I2V, an innovative framework that
integrates Multimodal Large Language Models (MLLMs) to jointly encode visual
and textual conditions for a diffusion transformer (DiT) architecture. By
leveraging the advanced multimodal understanding capabilities of MLLMs, our
model significantly improves motion controllability and temporal coherence in
synthesized videos. The inherent multimodality of Dynamic-I2V further enables
flexible support for diverse conditional inputs, extending its applicability to
various downstream generation tasks. Through systematic analysis, we identify a
critical limitation in current I2V benchmarks: a significant bias towards
favoring low-dynamic videos, stemming from an inadequate balance between motion
complexity and visual quality metrics. To resolve this evaluation gap, we
propose DIVE - a novel assessment benchmark specifically designed for
comprehensive dynamic quality measurement in I2V generation. In conclusion,
extensive quantitative and qualitative experiments confirm that Dynamic-I2V
attains state-of-the-art performance in image-to-video generation, particularly
revealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,
controllability, and quality, respectively, as assessed by the DIVE metric in
comparison to existing methods.

</details>


### [428] [HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters](https://arxiv.org/pdf/2505.20156)
*Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, Qinglin Lu*

Main category: cs.CV

TL;DR: HunyuanVideo-Avatar is a multimodal diffusion transformer model that generates dynamic, emotion-controllable, and multi-character dialogue videos, addressing challenges in character consistency, emotion alignment, and multi-character animation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in generating dynamic videos with character consistency, precise emotion alignment, and multi-character animation.

Method: Introduces a character image injection module, Audio Emotion Module (AEM), and Face-Aware Audio Adapter (FAA) for improved conditioning, emotion control, and multi-character audio injection.

Result: Surpasses state-of-the-art methods on benchmark and wild datasets, generating realistic avatars in dynamic scenarios.

Conclusion: HunyuanVideo-Avatar effectively addresses key challenges in audio-driven human animation, offering superior performance and versatility.

Abstract: Recent years have witnessed significant progress in audio-driven human
animation. However, critical challenges remain in (i) generating highly dynamic
videos while preserving character consistency, (ii) achieving precise emotion
alignment between characters and audio, and (iii) enabling multi-character
audio-driven animation. To address these challenges, we propose
HunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model
capable of simultaneously generating dynamic, emotion-controllable, and
multi-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces
three key innovations: (i) A character image injection module is designed to
replace the conventional addition-based character conditioning scheme,
eliminating the inherent condition mismatch between training and inference.
This ensures the dynamic motion and strong character consistency; (ii) An Audio
Emotion Module (AEM) is introduced to extract and transfer the emotional cues
from an emotion reference image to the target generated video, enabling
fine-grained and accurate emotion style control; (iii) A Face-Aware Audio
Adapter (FAA) is proposed to isolate the audio-driven character with
latent-level face mask, enabling independent audio injection via
cross-attention for multi-character scenarios. These innovations empower
HunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets
and a newly proposed wild dataset, generating realistic avatars in dynamic,
immersive scenarios.

</details>


### [429] [OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation](https://arxiv.org/pdf/2505.20292)
*Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, Li Yuan*

Main category: cs.CV

TL;DR: OpenS2V-Nexus introduces a benchmark (OpenS2V-Eval) and dataset (OpenS2V-5M) for Subject-to-Video (S2V) generation, focusing on subject consistency, naturalness, and text relevance. It evaluates 18 S2V models and provides a large-scale dataset to advance research.


<details>
  <summary>Details</summary>
Motivation: Existing S2V benchmarks lack fine-grained assessment of subject consistency and naturalness. OpenS2V-Nexus aims to fill this gap by providing a comprehensive infrastructure for S2V generation.

Method: Proposes OpenS2V-Eval (180 prompts, real/synthetic data) with three metrics (NexusScore, NaturalScore, GmeScore) and OpenS2V-5M (5M subject-text-video triples) via cross-video associations and GPT-Image-1 synthesis.

Result: Evaluated 18 S2V models, revealing strengths/weaknesses. OpenS2V-5M ensures subject diversity and high-quality data.

Conclusion: OpenS2V-Nexus provides a robust infrastructure to accelerate S2V research, addressing gaps in current benchmarks and datasets.

Abstract: Subject-to-Video (S2V) generation aims to create videos that faithfully
incorporate reference content, providing enhanced flexibility in the production
of videos. To establish the infrastructure for S2V generation, we propose
OpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and
(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V
benchmarks inherited from VBench that focus on global and coarse-grained
assessment of generated videos, OpenS2V-Eval focuses on the model's ability to
generate subject-consistent videos with natural subject appearance and identity
fidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven
major categories of S2V, which incorporate both real and synthetic test data.
Furthermore, to accurately align human preferences with S2V benchmarks, we
propose three automatic metrics, NexusScore, NaturalScore and GmeScore, to
separately quantify subject consistency, naturalness, and text relevance in
generated videos. Building on this, we conduct a comprehensive evaluation of 18
representative S2V models, highlighting their strengths and weaknesses across
different content. Moreover, we create the first open-source large-scale S2V
generation dataset OpenS2V-5M, which consists of five million high-quality 720P
subject-text-video triples. Specifically, we ensure subject-information
diversity in our dataset by (1) segmenting subjects and building pairing
information via cross-video associations and (2) prompting GPT-Image-1 on raw
frames to synthesize multi-view representations. Through OpenS2V-Nexus, we
deliver a robust infrastructure to accelerate future S2V generation research.

</details>


### [430] [Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models](https://arxiv.org/pdf/2505.21179)
*Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, Yi-Zhe Song*

Main category: cs.CV

TL;DR: NAG is a training-free method for effective negative guidance in diffusion models, outperforming CFG in few-step sampling and generalizing across architectures and modalities.


<details>
  <summary>Details</summary>
Motivation: Negative guidance in diffusion models fails under aggressive step compression, leading to divergent predictions. NAG addresses this limitation.

Method: NAG uses L1-based normalization and refinement in attention space for extrapolation, functioning as a universal plug-in.

Result: NAG improves text alignment, fidelity, and human-perceived quality across various architectures and sampling regimes.

Conclusion: NAG is a model-agnostic, efficient solution for negative guidance in diffusion models, validated by experiments and user studies.

Abstract: Negative guidance -- explicitly suppressing unwanted attributes -- remains a
fundamental challenge in diffusion models, particularly in few-step sampling
regimes. While Classifier-Free Guidance (CFG) works well in standard settings,
it fails under aggressive sampling step compression due to divergent
predictions between positive and negative branches. We present Normalized
Attention Guidance (NAG), an efficient, training-free mechanism that applies
extrapolation in attention space with L1-based normalization and refinement.
NAG restores effective negative guidance where CFG collapses while maintaining
fidelity. Unlike existing approaches, NAG generalizes across architectures
(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,
video), functioning as a \textit{universal} plug-in with minimal computational
overhead. Through extensive experimentation, we demonstrate consistent
improvements in text alignment (CLIP Score), fidelity (FID, PFID), and
human-perceived quality (ImageReward). Our ablation studies validate each
design component, while user studies confirm significant preference for
NAG-guided outputs. As a model-agnostic inference-time approach requiring no
retraining, NAG provides effortless negative guidance for all modern diffusion
frameworks -- pseudocode in the Appendix!

</details>


### [431] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/pdf/2505.21863)
*Shikhhar Siingh, Abhinav Rawat, Chitta Baral, Vivek Gupta*

Main category: cs.CV

TL;DR: GETReason framework and GREAT metric improve contextual understanding of publicly significant images by leveraging geospatial, temporal, and event data.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to accurately extract contextual relevance from images, limiting their utility in journalism and education.

Method: Introduces GETReason for deeper contextual inference and GREAT for evaluation, using a multi-agent approach.

Result: Demonstrates effective linking of images to broader event contexts through reasoning-based insights.

Conclusion: The framework and metric enhance image understanding, offering valuable tools for contextual analysis.

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [432] [InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective](https://arxiv.org/pdf/2505.21920)
*Yuanhong Zhang, Muyao Yuan, Weizhan Zhang, Tieliang Gong, Wen Wen, Jiangyong Ying, Weijie Shi*

Main category: cs.CV

TL;DR: InfoSAM enhances SAM fine-tuning by preserving domain-invariant relations using mutual information objectives, improving performance in specialized domains.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains despite strong zero-shot capabilities. Existing PEFT methods ignore domain-invariant relations, limiting SAM's potential.

Method: InfoSAM uses mutual information objectives to distill and preserve pre-trained segmentation knowledge, compressing domain-invariant relations and maximizing knowledge transfer.

Result: Extensive experiments show InfoSAM improves SAM's performance in specialized scenarios, validating its adaptability and superiority.

Conclusion: InfoSAM provides a robust PEFT framework for SAM, enhancing its effectiveness in real-world specialized tasks.

Abstract: The Segment Anything Model (SAM), a vision foundation model, exhibits
impressive zero-shot capabilities in general tasks but struggles in specialized
domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to
unleash the potential of SAM in novel scenarios. However, existing PEFT methods
for SAM neglect the domain-invariant relations encoded in the pre-trained
model. To bridge this gap, we propose InfoSAM, an information-theoretic
approach that enhances SAM fine-tuning by distilling and preserving its
pre-trained segmentation knowledge. Specifically, we formulate the knowledge
transfer process as two novel mutual information-based objectives: (i) to
compress the domain-invariant relation extracted from pre-trained SAM,
excluding pseudo-invariant information as possible, and (ii) to maximize mutual
information between the relational knowledge learned by the teacher
(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM
establishes a robust distillation framework for PEFT of SAM. Extensive
experiments across diverse benchmarks validate InfoSAM's effectiveness in
improving SAM family's performance on real-world tasks, demonstrating its
adaptability and superiority in handling specialized scenarios.

</details>


### [433] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/pdf/2505.24139)
*Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan*

Main category: cs.CV

TL;DR: S4-Driver is a scalable, self-supervised motion planning algorithm for autonomous driving, leveraging MLLMs and a novel sparse volume strategy to improve 3D trajectory prediction without human annotations.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end motion planning approaches either rely on human annotations or underperform in 3D space. S4-Driver addresses this gap by adapting MLLMs for 3D planning.

Method: S4-Driver uses a sparse volume strategy to transform MLLMs' 2D visual representations into 3D space, aggregating multi-view and multi-frame inputs for better trajectory prediction.

Result: S4-Driver outperforms supervised multi-task approaches on nuScenes and Waymo datasets, requiring no human annotations and showing scalability with unannotated data.

Conclusion: S4-Driver demonstrates the effectiveness of self-supervised learning in autonomous driving, bridging the gap between 2D pretraining and 3D planning.

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [434] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/pdf/2505.24380)
*Zheng Wang*

Main category: cs.CV

TL;DR: A novel framework for fine-grained bird image classification (FBIC) addresses challenges like varying bird sizes, background interference, and pose variability by using strip-aware spatial perception with EPA and CSW modules, achieving improved performance on the CUB-200-2011 dataset.


<details>
  <summary>Details</summary>
Motivation: FBIC is crucial for ecological monitoring and species identification but faces challenges like varying bird sizes, complex backgrounds, and pose variability, limiting traditional methods' effectiveness.

Method: Proposes a framework with strip-aware spatial perception, featuring two modules: EPA (integrates local and global spatial cues) and CSW (refines semantic representations by fusing long- and short-range channel information), built on ResNet-50.

Result: The framework significantly improves performance on the CUB-200-2011 dataset while maintaining efficiency.

Conclusion: The proposed method enhances robustness and interpretability in FBIC, addressing key challenges and outperforming traditional approaches.

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [435] [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/pdf/2505.24718)
*Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua*

Main category: cs.CV

TL;DR: TW-GRPO improves multimodal reasoning by focusing on salient tokens and using dense rewards, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of verbose reasoning chains and binary rewards in reinforcement learning for multimodal tasks.

Method: Introduces token weighting for focused reasoning and multi-choice QA with soft rewards, plus question-answer inversion for data augmentation.

Result: Achieves 50.4% accuracy on CLEVRER (18.8% improvement) and 65.8% on MMVU.

Conclusion: TW-GRPO advances visual reasoning with focused thinking and granular rewards, setting new benchmarks.

Abstract: Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group variance),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}.

</details>


### [436] [3D Trajectory Reconstruction of Moving Points Based on Asynchronous Cameras](https://arxiv.org/pdf/2506.00541)
*Huayu Huang, Banglei Guan, Yang Shang, Qifeng Yu*

Main category: cs.CV

TL;DR: A method for 3D trajectory reconstruction of point targets using asynchronous cameras, solving both trajectory reconstruction and camera synchronization simultaneously.


<details>
  <summary>Details</summary>
Motivation: Localizing moving targets is essential for analyzing their motion and dynamics, but current methods address only one sub-problem (trajectory or synchronization) individually.

Method: Extends trajectory intersection to asynchronous cameras, models camera temporal info and target motion, and optimizes camera rotations and time parameters.

Result: Improved accuracy, with real-world tests showing a localization error of 112.95 m at 15-20 km range.

Conclusion: The method is feasible and accurate, addressing limitations of traditional triangulation.

Abstract: Photomechanics is a crucial branch of solid mechanics. The localization of
point targets constitutes a fundamental problem in optical experimental
mechanics, with extensive applications in various missions of UAVs. Localizing
moving targets is crucial for analyzing their motion characteristics and
dynamic properties. Reconstructing the trajectories of points from asynchronous
cameras is a significant challenge. It encompasses two coupled sub-problems:
trajectory reconstruction and camera synchronization. Present methods typically
address only one of these sub-problems individually. This paper proposes a 3D
trajectory reconstruction method for point targets based on asynchronous
cameras, simultaneously solving both sub-problems. Firstly, we extend the
trajectory intersection method to asynchronous cameras to resolve the
limitation of traditional triangulation that requires camera synchronization.
Secondly, we develop models for camera temporal information and target motion,
based on imaging mechanisms and target dynamics characteristics. The parameters
are optimized simultaneously to achieve trajectory reconstruction without
accurate time parameters. Thirdly, we optimize the camera rotations alongside
the camera time information and target motion parameters, using tighter and
more continuous constraints on moving points. The reconstruction accuracy is
significantly improved, especially when the camera rotations are inaccurate.
Finally, the simulated and real-world experimental results demonstrate the
feasibility and accuracy of the proposed method. The real-world results
indicate that the proposed algorithm achieved a localization error of 112.95 m
at an observation range of 15 ~ 20 km.

</details>


### [437] [Uneven Event Modeling for Partially Relevant Video Retrieval](https://arxiv.org/pdf/2506.00891)
*Sa Zhu, Huashan Chen, Wanqian Zhang, Jinchao Zhang, Zexian Yang, Xiaoshuai Hao, Bo Li*

Main category: cs.CV

TL;DR: The paper introduces an Uneven Event Modeling (UEM) framework for partially relevant video retrieval (PRVR), addressing issues like ambiguous event boundaries and misalignment in previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous methods for PRVR segment videos into fixed-length clips, leading to unclear event boundaries and misaligned representations.

Method: Proposes UEM with two modules: Progressive-Grouped Video Segmentation (PGVS) for clear event boundaries and Context-Aware Event Refinement (CAER) for precise text-video alignment.

Result: Achieves state-of-the-art performance on two PRVR benchmarks.

Conclusion: UEM effectively improves event modeling and alignment in PRVR, outperforming existing methods.

Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to
retrieve untrimmed videos containing relevant moments, wherein event modeling
is crucial for partitioning the video into smaller temporal events that
partially correspond to the text. Previous methods typically segment videos
into a fixed number of equal-length clips, resulting in ambiguous event
boundaries. Additionally, they rely on mean pooling to compute event
representations, inevitably introducing undesired misalignment. To address
these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first
introduce the Progressive-Grouped Video Segmentation (PGVS) module, to
iteratively formulate events in light of both temporal dependencies and
semantic similarity between consecutive frames, enabling clear event
boundaries. Furthermore, we also propose the Context-Aware Event Refinement
(CAER) module to refine the event representation conditioned the text's
cross-attention. This enables event representations to focus on the most
relevant frames for a given text, facilitating more precise text-video
alignment. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on two PRVR benchmarks. Code is available at
https://github.com/Sasa77777779/UEM.git.

</details>


### [438] [MedEBench: Revisiting Text-instructed Image Editing on Medical Domain](https://arxiv.org/pdf/2506.01921)
*Minghao Liu, Zhitao He, Zhiyuan Fan, Qingyun Wang, Yi R. Fung*

Main category: cs.CV

TL;DR: MedEBench is a benchmark for evaluating text-guided medical image editing, addressing the lack of standardized evaluation in the field. It includes 1,182 image-prompt triplets, evaluates seven models, and introduces metrics for accuracy, contextual preservation, and visual quality.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in standardized evaluation for text-guided medical image editing, which has clinical applications like surgical simulation and patient communication.

Method: Introduces MedEBench with 1,182 image-prompt triplets across 70 tasks and 13 anatomical regions. Evaluates seven models using metrics like Editing Accuracy, Contextual Preservation, and Visual Quality, and analyzes failures via attention grounding.

Result: Reveals common failure patterns in models and provides a protocol for failure analysis using attention maps and ROI masks.

Conclusion: MedEBench offers a foundation for developing reliable, clinically meaningful medical image editing systems.

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce \textbf{MedEBench}, a
comprehensive benchmark for evaluating text-guided medical image editing. It
consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks
across 13 anatomical regions. MedEBench offers three key contributions: (1) a
clinically relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [439] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/pdf/2506.02097)
*Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, Srikant Panda*

Main category: cs.AI

TL;DR: A hybrid framework combining RAG with intent-based canned responses improves enterprise conversational AI by balancing accuracy (95%) and latency (180ms).


<details>
  <summary>Details</summary>
Motivation: Enterprise-scale deployments of RAG and LLM-powered chatbots face challenges like diverse queries, high latency, hallucinations, and integrating updated knowledge.

Method: The framework integrates RAG with intent-based responses, using predefined answers for efficiency and routing complex queries to RAG. It includes a dialogue context manager and feedback loop.

Result: The framework achieves 95% accuracy and 180ms latency, outperforming standalone RAG and intent-based systems.

Conclusion: The hybrid framework is scalable and adaptive, suitable for enterprise conversational AI.

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [440] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/pdf/2506.02125)
*Guy Tennenholtz, Jihwan Jeong, Chih-Wei Hsu, Yinlam Chow, Craig Boutilier*

Main category: cs.AI

TL;DR: The paper introduces Descriptive History Representations (DHRs) for summarizing interaction histories in partially observable environments, optimized for answering task-relevant questions and decision-making.


<details>
  <summary>Details</summary>
Motivation: Effective decision-making in partially observable environments requires compressing long interaction histories into informative representations.

Method: A multi-agent learning framework with representation, decision, and question-asking components, optimized using a joint objective balancing reward maximization and question-answering ability.

Result: DHRs capture salient historical details and predictive structures, validated on user modeling tasks with movie and shopping datasets, producing interpretable user profiles.

Conclusion: DHRs provide structured summaries of histories for optimal control, demonstrated in practical applications like user preference prediction.

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [441] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/pdf/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: The paper introduces the Unified Cognitive Consciousness Theory (UCCT) to explain why LLMs generalize some tasks with minimal examples while others need extensive supervision, framing LLMs as unconscious substrates of latent patterns.


<details>
  <summary>Details</summary>
Motivation: To resolve the paradox of few-shot learning in LLMs by redefining their role in cognition, not as broken approximations but foundational components of general intelligence.

Method: Proposes UCCT, which views LLMs as unconscious substrates and introduces semantic anchoring (via prompts, roles, interaction) as a conscious control layer. Also presents the Threshold-Crossing Dynamics Theorem to formalize semantic anchoring.

Result: UCCT unifies prompting, fine-tuning, retrieval, and multi-agent coordination through probabilistic alignment between unconscious representation and external control.

Conclusion: AGI will emerge not by discarding LLMs but by integrating them into systems that reason, regulate, and adapt together.

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [442] [Small Language Models are the Future of Agentic AI](https://arxiv.org/pdf/2506.02153)
*Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, Pavlo Molchanov*

Main category: cs.AI

TL;DR: The paper argues that small language models (SLMs) are more suitable and economical than large language models (LLMs) for agentic AI systems, proposing SLMs as the future of such systems.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high costs of using LLMs in repetitive, specialized tasks within agentic AI systems.

Method: The argument is based on current SLM capabilities, agentic system architectures, and deployment economics, with a proposed LLM-to-SLM conversion algorithm.

Result: SLMs are deemed sufficiently powerful and economical for many agentic tasks, with heterogeneous systems suggested for general-purpose needs.

Conclusion: The paper advocates for a shift from LLMs to SLMs in agentic AI, emphasizing operational and economic benefits, and invites further discussion.

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [443] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/pdf/2506.02158)
*Ruhana Azam, Aditya Vempaty, Ashish Jagmohan*

Main category: cs.AI

TL;DR: ReAP improves web navigation by using past experiences and self-reflections, boosting performance by 11 points overall and 29 points on failed tasks.


<details>
  <summary>Details</summary>
Motivation: Current web navigation agents lack memory, causing repeated mistakes and missed learning opportunities.

Method: Introduces Reflection-Augment Planning (ReAP) to leverage past experiences via self-reflections.

Result: Improves baseline by 11 points overall and 29 points on failed tasks.

Conclusion: Reflections enhance performance and transferability across web navigation tasks.

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [444] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/pdf/2506.02177)
*Haizhong Zheng, Yang Zhou, Brian R. Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, Beidi Chen*

Main category: cs.AI

TL;DR: GRESO, a lightweight pre-rollout filtering algorithm, skips uninformative prompts in RL training, saving computational overhead without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Reducing computational overhead in RL training by avoiding uninformative prompts, leveraging temporal consistency in prompt value.

Method: Proposes GRESO, an online filtering algorithm that predicts and skips uninformative prompts using reward dynamics.

Result: Achieves up to 2.4x speedup in rollout and 2.0x in total training time without accuracy degradation.

Conclusion: GRESO efficiently reduces computational costs in RL training while maintaining performance.

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [445] [Natural, Artificial, and Human Intelligences](https://arxiv.org/pdf/2506.02183)
*Emmanuel M. Pothos, Dominic Widdows*

Main category: cs.AI

TL;DR: The paper explores whether humans are uniquely intelligent by comparing human intelligence with non-human animals and AI chatbots, identifying four key ingredients for human intelligence.


<details>
  <summary>Details</summary>
Motivation: To address the question of whether human intelligence is qualitatively unique compared to non-human animals and AI chatbots.

Method: Analysis of psychological literature, evidence from non-human animals, written language's role, AI progress, intelligence testing history, and embodiment's role.

Result: Human intelligence relies on invention, complex inference, embodiment, and self-awareness; non-human animals meet all but complex language, while chatbots lack embodiment and awareness.

Conclusion: Human intelligence is not qualitatively unique compared to non-human animals, and chatbots' limitations lie in embodiment and awareness.

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [446] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/pdf/2506.02211)
*Maxime Robeyns, Laurence Aitchison*

Main category: cs.AI

TL;DR: The paper introduces a method to improve code quality in LLM-generated code by using a comprehensive library to quantify quality metrics, beyond just functional correctness, and integrating it into GRPO.


<details>
  <summary>Details</summary>
Motivation: Current training procedures for LLMs in code generation focus on functional correctness (unit test pass rate) but neglect maintainability, quality, and safety. This gap motivates the development of a more comprehensive reward system.

Method: The authors develop a library to quantify various aspects of code quality and integrate it as a reward signal in GRPO (a training procedure).

Result: GRPO improves code quality according to the new metrics, validated by expert human annotators.

Conclusion: The study highlights the importance of incorporating broader code quality metrics in LLM training and demonstrates the effectiveness of GRPO in achieving this.

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [447] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/pdf/2506.02280)
*Kedir Yassin Hussen, Walelign Tewabe Sewunetie, Abinew Ali Ayele, Sukairaj Hafiz Imam, Shamsuddeen Hassan Muhammad, Seid Muhie Yimam*

Main category: cs.AI

TL;DR: The paper highlights the limited support for African languages in LLMs, SLMs, and SSLMs, identifying gaps in coverage, scripts, and data, and suggests solutions like standardization and community-driven corpus development.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation of Africa's 2,000 low-resource languages in NLP models, despite the transformative impact of LLMs.

Method: Comparative analysis of African language coverage across six LLMs, eight SLMs, and six SSLMs, evaluating language coverage, training sets, technical limitations, scripts, and roadmaps.

Result: Identified 42 supported African languages and 23 public datasets, with four languages (Amharic, Swahili, Afrikaans, Malagasy) dominating while over 98% remain unsupported. Also, only Latin, Arabic, and Ge'ez scripts are recognized, neglecting 20 others.

Conclusion: Challenges like data scarcity, tokenization biases, high computational costs, and evaluation issues require solutions such as language standardization, community corpus development, and effective adaptation methods for African languages.

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [448] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/pdf/2506.02314)
*Tianyu Hua, Harper Hua, Violet Xiang, Benjamin Klieger, Sang T. Truong, Weixin Liang, Fan-Yun Sun, Nick Haber*

Main category: cs.AI

TL;DR: ResearchCodeBench evaluates LLMs' ability to implement novel ML ideas from recent papers into code, finding top models succeed less than 40% of the time.


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' capability to translate cutting-edge ML research into executable code, as their performance on unseen ideas during pretraining is unclear.

Method: Introduces ResearchCodeBench, a benchmark of 212 coding challenges from top 2024-2025 papers, testing 30+ LLMs.

Result: Best model (Gemini-2.5-Pro-Preview) achieves 37.3% success rate; others perform below 40%.

Conclusion: ResearchCodeBench provides a rigorous platform for evaluating and advancing LLMs in research code generation.

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [449] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/pdf/2506.02387)
*Zelai Xu, Zhexuan Xu, Xiangmin Yi, Huining Yuan, Xinlei Chen, Yi Wu, Chao Yu, Yu Wang*

Main category: cs.AI

TL;DR: VS-Bench is a new multimodal benchmark for evaluating Vision Language Models (VLMs) in multi-agent strategic reasoning and decision-making, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack multi-agent and multimodal contexts, while real-world scenarios require strategic interactions in rich visual and linguistic environments.

Method: VS-Bench includes eight vision-grounded environments for cooperative, competitive, and mixed-motive interactions, evaluated via next-action prediction accuracy (offline) and normalized episode return (online).

Result: Experiments with fourteen VLMs show a performance gap, with the best models achieving 47.8% prediction accuracy and 24.3% normalized return.

Conclusion: VS-Bench standardizes evaluation for strategic multimodal agents, highlighting current limitations and paving the way for future research.

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [450] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/pdf/2506.02397)
*Shengjia Zhang, Junjie Wu, Jiawei Chen, Changwang Zhang, Xingyu Lou, Wangchunshu Zhou, Sheng Zhou, Can Wang, Jun Wang*

Main category: cs.AI

TL;DR: OThink-R1 reduces redundant reasoning in LRMs by dynamically switching between fast and slow thinking, cutting redundancy by 23% without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Identify and address the inefficiency of large reasoning models (LRMs) using complex reasoning for tasks solvable by simpler methods.

Method: Analyze reasoning trajectories, classify them as redundant or essential, and introduce OThink-R1 to prune redundant steps while maintaining validity.

Result: OThink-R1 reduces reasoning redundancy by 23% on average without compromising accuracy.

Conclusion: OThink-R1 offers a practical solution for efficient reasoning, balancing speed and complexity.

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [451] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/pdf/2506.02456)
*Tri Cao, Bennett Lim, Yue Liu, Yuan Sui, Yuexin Li, Shumin Deng, Lin Lu, Nay Oo, Shuicheng Yan, Bryan Hooi*

Main category: cs.AI

TL;DR: The paper explores Visual Prompt Injection (VPI) attacks on Computer-Use Agents (CUAs) and Browser-Use Agents (BUAs), proposing VPI-Bench to evaluate their robustness. Results show high deception rates, emphasizing the need for better defenses.


<details>
  <summary>Details</summary>
Motivation: To address underexplored vulnerabilities of CUAs and BUAs, particularly VPI attacks, and assess their impact.

Method: Developed VPI-Bench, a benchmark with 306 test cases across five platforms, embedding malicious prompts in user interfaces.

Result: CUAs and BUAs were deceived at rates up to 51% and 100%, respectively, with limited improvement from system prompt defenses.

Conclusion: Robust, context-aware defenses are crucial for safe deployment of multimodal AI agents.

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [452] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/pdf/2506.02470)
*Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao*

Main category: cs.AI

TL;DR: MedRAG is a multimodal healthcare copilot using LLM reasoning to reduce misdiagnosis by integrating diverse inputs like voice, queries, and EHRs, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Misdiagnosis harms healthcare systems, increasing costs and patient risks, necessitating improved decision-making tools.

Method: Uses retrieval-augmented generation and knowledge graph-elicited reasoning to integrate diagnostic insights from multiple input modalities.

Result: Outperforms existing models, providing more specific and accurate healthcare assistance.

Conclusion: MedRAG effectively reduces misdiagnosis risks and enhances medical decision-making.

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [453] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/pdf/2506.02485)
*Haowen Xu, Sisi Zlatanova, Ruiyu Liang, Ismet Canbulat*

Main category: cs.AI

TL;DR: The paper advocates for using generative AI to improve wildfire prediction and simulation, addressing limitations of current methods and proposing a human-AI collaboration framework for knowledge extraction and synthesis.


<details>
  <summary>Details</summary>
Motivation: Current wildfire prediction models lack real-time, multimodal capabilities, hindering emergency response. Generative AI offers potential solutions for better forecasting and simulation.

Method: Proposes generative AI models (GANs, VAEs, Transformers, diffusion models) for 2D/3D wildfire simulation and a human-AI framework using LLMs for knowledge extraction.

Result: Identifies five key visions for generative AI in wildfire management and addresses challenges with potential solutions.

Conclusion: Generative AI can revolutionize wildfire prediction and management, but challenges must be addressed for successful implementation.

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [454] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/pdf/2506.02522)
*Xu Wan, Wenyue Xu, Chao Yang, Mingyang Sun*

Main category: cs.AI

TL;DR: ACE combines LLMs and RL for large-scale decision-making, improving performance in tasks like power grid operations with over 60K actions.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of LLMs (lack of real-time decision-making) and RL (sample inefficiency in large action spaces) in industrial-scale problems.

Method: Proposes ACE, a framework where LLMs act as Policy Actor and Value Critic in RL training, refining actions and shaping rewards, while RL enhances LLMs with fine-tuning data.

Result: ACE outperforms existing RL and LLM-based methods in power grid operation tasks with large action spaces.

Conclusion: ACE effectively bridges the gap between LLMs and RL, offering a scalable solution for complex decision-making scenarios.

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [455] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/pdf/2506.02565)
*Zhuoxuan Jiang, Tianyang Zhang, Peiyan Peng, Jing Chen, Yinong Xun, Haotian Zhang, Lichi Li, Yong Li, Shaohua Zhang*

Main category: cs.AI

TL;DR: A framework called SDE-GPG is introduced for generating high-quality geometry problems using symbolic deduction and multi-modal translation.


<details>
  <summary>Details</summary>
Motivation: Geometry problem generation is challenging due to multi-modal formats and language translation needs. Existing methods lack control over knowledge points and biases.

Method: SDE-GPG uses a symbolic deduction engine with four steps: mapping table search, symbolic deduction, problem filtering, and text/diagram generation.

Result: Experiments show SDE-GPG generates readable, solvable, and controllable geometry problems.

Conclusion: SDE-GPG effectively addresses biases and control issues in geometry problem generation.

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [456] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/pdf/2506.02568)
*Dongzhe Fan, Yi Fang, Jiajin Liu, Djellel Difallah, Qiaoyu Tan*

Main category: cs.AI

TL;DR: MLaGA extends LLMs to handle multimodal graphs with text and image attributes, outperforming baselines in graph learning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based graph methods focus on text-rich graphs, leaving multimodal graphs underexplored despite their real-world relevance.

Method: MLaGA uses a structure-aware multimodal encoder for attribute alignment and multimodal instruction-tuning to integrate features into LLMs.

Result: MLaGA achieves superior performance in supervised and transfer learning tasks across multiple datasets.

Conclusion: MLaGA effectively bridges the gap in multimodal graph analysis, demonstrating strong adaptability and performance.

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [457] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/pdf/2506.02576)
*Haichen Wang, Liu Yang, Xinyuan Zhang, Haomin Yu, Ming Li, Jilin Hu*

Main category: cs.AI

TL;DR: ADFormer, an Aggregation Differential Transformer, improves passenger demand forecasting by integrating high-level spatio-temporal correlations with original correlations using Differential Attention and tailored aggregation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully adapt to complex spatio-temporal correlations and overlook high-level correlations, limiting forecasting accuracy.

Method: ADFormer uses Differential Attention for spatial correlation capture and denoising, along with space-time-specific aggregation strategies to unify original and high-level correlations.

Result: Experiments on taxi and bike datasets show ADFormer's effectiveness and efficiency in demand forecasting.

Conclusion: ADFormer successfully captures holistic spatio-temporal relations, offering practical value for urban efficiency optimization.

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [458] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/pdf/2506.02580)
*Xuewen Luo, Fengze Yang, Fan Ding, Xiangbo Gao, Shuo Xing, Yang Zhou, Zhengzhong Tu, Chenxi Liu*

Main category: cs.AI

TL;DR: V2X-UniPool integrates multimodal V2X data for autonomous driving, improving reasoning and reducing transmission costs.


<details>
  <summary>Details</summary>
Motivation: Addresses limited perception and hallucination in knowledge-driven ADs by leveraging V2X data.

Method: Uses a dual-query RAG mechanism to retrieve static and dynamic knowledge from a unified knowledge pool.

Result: Enhances motion planning accuracy and reasoning, achieving state-of-the-art performance with reduced transmission costs.

Conclusion: V2X-UniPool effectively improves AD performance and efficiency through unified knowledge integration.

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [459] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/pdf/2506.02594)
*Ruibo Duan, Yuxin Liu, Xinyao Dong, Chenglin Fan*

Main category: cs.AI

TL;DR: EALG is a framework that co-evolves optimization problem instances and heuristic solvers using LLMs, producing harder instances and adaptive solvers.


<details>
  <summary>Details</summary>
Motivation: To advance combinatorial optimization by automating the creation of challenging instances and adaptive solvers, moving beyond static benchmarks and manual design.

Method: Uses a mutation-based adversarial approach with LLMs to dynamically evolve instance generation and synthesize heuristic solvers.

Result: Generates harder instances than current benchmarks, with solvers generalizing well across combinatorial tasks.

Conclusion: EALG integrates instance generation and solver design, achieving state-of-the-art performance in combinatorial optimization.

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [460] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/pdf/2506.02609)
*Tianfan Jiang, Mei Wu, Wenchao Weng, Dewen Seng, Yiqian Lin*

Main category: cs.AI

TL;DR: A novel Time-Enhanced Data Disentanglement Network (TEDDN) is proposed to improve traffic flow forecasting by disentangling complex traffic data into stable patterns and trends, leveraging dynamic graphs and temporal feature extraction.


<details>
  <summary>Details</summary>
Motivation: Traffic flow prediction is challenging due to temporal variations and dynamic spatial correlations. Existing methods struggle with diverse data dependencies and overlook temporal information importance.

Method: TEDDN disentangles traffic data into stable patterns and trends using a dynamic graph enhanced by a temporal feature extraction module.

Result: Experiments on four real-world datasets show TEDDN's superiority in handling complex traffic information.

Conclusion: TEDDN effectively addresses traffic prediction challenges by emphasizing temporal information and disentangling data, outperforming traditional methods.

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [461] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/pdf/2506.02648)
*Yue Yang, MingKang Chen, Qihua Liu, Mengkang Hu, Qiguang Chen, Gengrui Zhang, Shuyue Hu, Guangtao Zhai, Yu Qiao, Yu Wang, Wenqi Shao, Ping Luo*

Main category: cs.AI

TL;DR: DRE-Bench is a new benchmark for evaluating fluid intelligence in LLMs, revealing gaps in high-level cognition and generalization despite strong low-level performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretable and reliable benchmarks for assessing fluid intelligence in LLMs.

Method: Proposes DRE-Bench, a dynamic reasoning benchmark with 36 tasks across four cognitive levels, testing latent rules with dynamic variants.

Result: LLMs perform well in low-level cognition but struggle with high-level tasks and generalization as complexity increases.

Conclusion: Highlights the gap between LLMs and human-like fluid intelligence, offering a systematic way to track reasoning progress.

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [462] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/pdf/2506.02649)
*Yousef Emami, Hao Zhou, Miguel Gutierrez Gaitan, Kai Li, Luis Almeida, Zhu Han*

Main category: cs.AI

TL;DR: The paper proposes using Large Language Models (LLMs) with In-Context Learning (ICL) to enhance public safety UAVs, addressing limitations of Deep Reinforcement Learning (DRL) like high training complexity and simulation-to-reality gaps. The LLM-enabled ICL framework improves tasks like path planning and reduces packet loss, offering a lightweight, adaptive solution for emergency response.


<details>
  <summary>Details</summary>
Motivation: Public safety UAVs are crucial for emergency response but face limitations with DRL due to high training complexity and inefficiency. LLMs, with their reasoning and generalization capabilities, offer a promising alternative.

Method: The paper integrates LLM-enabled ICL with UAVs for tasks like path planning and velocity control. A case study on data collection scheduling demonstrates reduced packet loss and improved adaptability.

Result: The LLM-enabled ICL framework outperforms conventional methods, reducing packet loss and mitigating vulnerabilities, while enabling adaptive decision-making for UAVs.

Conclusion: LLM-enabled ICL provides a lightweight, efficient solution for enhancing UAV autonomy in emergencies, with potential for future research in LLM optimizers and broader applications.

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [463] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/pdf/2506.02668)
*Frederico Metelo, Alexandre Oliveira, Stevo Racković, Pedro Ákos Costa, Cláudia Soares*

Main category: cs.AI

TL;DR: FAuNO is a federated reinforcement-learning framework for decentralized task offloading in edge systems, outperforming baselines in reducing latency and task loss.


<details>
  <summary>Details</summary>
Motivation: Addressing latency and resource bottlenecks in traditional centralized orchestration for edge computing.

Method: Uses a buffered, asynchronous federated reinforcement-learning (FRL) framework with actor-critic architecture for decentralized task offloading.

Result: FAuNO matches or exceeds heuristic and federated multi-agent RL baselines in reducing task loss and latency.

Conclusion: FAuNO is adaptable and efficient for dynamic edge-computing scenarios.

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [464] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/pdf/2506.02696)
*Jinyuan Luo, Zhen Fang, Yixuan Li, Seongheon Park, Ling Chen*

Main category: cs.AI

TL;DR: SSP improves LLM hallucination detection by analyzing intermediate representations' sensitivity to perturbations, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Hallucination in LLMs hinders reliable QA deployment; self-assessment based on output confidence is unreliable due to model bias.

Method: SSP uses dynamic noise prompts and a lightweight encoder to analyze perturbation sensitivity in intermediate representations, employing a contrastive distance metric.

Result: SSP significantly outperforms prior hallucination detection methods in benchmarks.

Conclusion: SSP offers a more reliable self-assessment framework by leveraging intermediate representations under perturbation.

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [465] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/pdf/2506.02713)
*Xiaochong Lan, Jie Feng, Yizhou Sun, Chen Gao, Jiahuan Lei, Xinlei Shi, Hengliang Luo, Yong Li*

Main category: cs.AI

TL;DR: PIGEON redefines living need prediction as an open-set problem using LLMs, outperforming closed-set methods by 19.37%.


<details>
  <summary>Details</summary>
Motivation: Traditional closed-set classification fails to capture the diversity of living needs, prompting a shift to open-set prediction.

Method: PIGEON uses behavior-aware retrieval and Maslow's hierarchy with LLMs, plus a recall module for linking needs to services.

Result: PIGEON outperforms closed-set methods by 19.37% and shows reasonable, specific predictions in human evaluation.

Conclusion: PIGEON's open-set approach and LLM integration effectively predict diverse living needs, with practical deployment via instruction tuning.

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [466] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/pdf/2506.02720)
*Xiaochong Lan, Jie Feng, Jiahuan Lei, Xinlei Shi, Yong Li*

Main category: cs.AI

TL;DR: A study evaluates LLMs for local life services, showing a 7B model can match a 72B model's performance with fine-tuning and agent-based workflows, improving deployment feasibility.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in local life services and enhance their practicality for real-world applications.

Method: Established a benchmark, evaluated diverse LLMs, and tested fine-tuning and agent-based workflows.

Result: A 7B model achieved performance comparable to a 72B model, optimizing cost and capability.

Conclusion: LLMs can be efficiently deployed for local life services, balancing performance and practicality.

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [467] [Why do AI agents communicate in human language?](https://arxiv.org/pdf/2506.02739)
*Pengcheng Zhou, Yinglun Feng, Halimulati Julaiti, Zhongliang Yang*

Main category: cs.AI

TL;DR: The paper critiques the reliance on natural language for inter-agent communication in LLM-based AI systems, highlighting inefficiencies and proposing a need for new paradigms to better support multi-agent coordination.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems use natural language for agent communication, which is misaligned with LLMs' high-dimensional vector spaces, causing inefficiencies and limitations in coordination.

Method: The paper analyzes the structural misalignment between natural language and LLM vector spaces, identifying limitations like information loss and behavioral drift.

Result: The analysis reveals that LLMs lack mechanisms for role continuity, task boundaries, and multi-agent dependencies, hindering robust coordination.

Conclusion: The paper advocates for rethinking agent communication and developing new model paradigms to natively support structured multi-agent coordination.

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [468] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/pdf/2506.02761)
*Renyang Liu, Wenjie Feng, Tianwei Zhang, Wei Zhou, Xueqi Cheng, See-Kiong Ng*

Main category: cs.AI

TL;DR: The paper addresses challenges in image generation model unlearning (IGMU), proposing a hierarchical task categorization framework (CatIGMU), a comprehensive evaluation framework (EvalIGMU), and a high-quality dataset (DataIGM) to improve understanding and evaluation of IGMU.


<details>
  <summary>Details</summary>
Motivation: The rise of image generation models has raised privacy and safety concerns, but existing IGMU solutions lack clear guidelines, effective evaluation, and reliable metrics.

Method: The authors assess existing unlearning algorithms, design CatIGMU for task categorization, introduce EvalIGMU for evaluation, and create DataIGM for benchmarking.

Result: Most existing IGMU algorithms perform poorly across evaluation dimensions, particularly in preservation and robustness.

Conclusion: The proposed frameworks and dataset advance IGMU research by providing standardized tools for algorithm design and evaluation.

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [469] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/pdf/2506.02805)
*Henri Bollaert, Chris Cornelis, Marko Palangetić, Salvatore Greco, Roman Słowiński*

Main category: cs.AI

TL;DR: The paper explores whether optimizing attribute order in the FRRI rule induction algorithm improves performance, finding it ineffective, but fuzzy rough feature selection enhances accuracy and rule length.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability in machine learning by refining the FRRI algorithm, focusing on attribute order and selection.

Method: Tests attribute order optimization and fuzzy rough feature selection within FRRI, comparing performance metrics.

Result: Attribute order optimization alone doesn't improve FRRI, but feature selection boosts balanced accuracy and reduces rule length.

Conclusion: Fuzzy rough feature selection is more impactful than attribute order optimization for enhancing FRRI's performance.

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [470] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/pdf/2506.02838)
*Jizhou Wang, Xiaodan Fang, Lei Huang, Yongfeng Huang*

Main category: cs.AI

TL;DR: TaxAgent combines LLMs and ABM to design adaptive tax policies, outperforming traditional methods in balancing equity and productivity.


<details>
  <summary>Details</summary>
Motivation: Address economic inequality by improving tax policy adaptability to taxpayer heterogeneity and irrational behavior.

Method: Integrates large language models (LLMs) with agent-based modeling (ABM) to simulate household behaviors and optimize tax rates iteratively.

Result: TaxAgent achieves better equity-efficiency trade-offs than Saez Optimal Taxation, U.S. federal income taxes, and free markets.

Conclusion: TaxAgent provides a scalable, data-driven framework for adaptive fiscal policy, offering a novel solution to economic inequality.

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [471] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/pdf/2506.02865)
*Mathieu Andreux, Breno Baldas Skuk, Hamza Benchekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Matthias Brunel, Pierre-Louis Cedoz, Antoine Chassang, Mickaël Chen, Alexandra D. Constantinou, Antoine d'Andigné, Hubert de La Jonquière, Aurélien Delfosse, Ludovic Denoyer, Alexis Deprez, Augustin Derupti, Michael Eickenberg, Mathïs Federico, Charles Kantor, Xavier Koegler, Yann Labbé, Matthew C. H. Lee, Erwan Le Jumeau de Kergaradec, Amir Mahla, Avshalom Manevich, Adrien Maret, Charles Masson, Rafaël Maurin, Arturo Mena, Philippe Modard, Axel Moyal, Axel Nguyen Kerbel, Julien Revelle, Mats L. Richter, María Santos, Laurent Sifre, Maxime Theillard, Marc Thibault, Louis Thiry, Léo Tronchon, Nicolas Usunier, Tony Wu*

Main category: cs.AI

TL;DR: Surfer-H is a cost-efficient web agent using Vision-Language Models (VLM) for web tasks, paired with Holo1, a specialized VLM for navigation and extraction. Holo1 excels in benchmarks, and Surfer-H achieves 92.2% performance on WebVoyager. Both WebClick dataset and Holo1 weights are open-sourced.


<details>
  <summary>Details</summary>
Motivation: To create a cost-efficient web agent leveraging VLMs for user-defined tasks, advancing research in agentic systems.

Method: Integrates Surfer-H with Holo1, a VLM trained on curated data (web content, synthetic examples, agentic data). Evaluated on UI benchmarks and WebClick.

Result: Holo1 tops UI benchmarks; Surfer-H achieves 92.2% performance on WebVoyager, balancing accuracy and cost.

Conclusion: Surfer-H and Holo1 demonstrate high performance and efficiency, with open-sourced resources to foster further research.

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [472] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/pdf/2506.02867)
*Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, Jing Shao*

Main category: cs.AI

TL;DR: The paper explores the reasoning mechanisms of large reasoning models (LRMs) by analyzing mutual information (MI) peaks during reasoning, identifying 'thinking tokens' like 'Hmm' and 'Therefore' as crucial for performance, and proposing methods to enhance LRM reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the internal reasoning mechanisms of LRMs, which remain poorly understood despite their problem-solving capabilities.

Method: Tracked MI between intermediate representations and correct answers during LRM reasoning, identified MI peaks and 'thinking tokens,' and proposed methods to leverage these tokens.

Result: Observed MI peaks correlate with reduced prediction errors and identified 'thinking tokens' as key to reasoning performance.

Conclusion: Provides insights into LRM reasoning and practical methods to improve performance, with code available for further exploration.

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [473] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/pdf/2506.02873)
*Matthew Kowal, Jasper Timm, Jean-Francois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine*

Main category: cs.AI

TL;DR: The paper introduces the Attempt to Persuade Eval (APE) benchmark to assess LLMs' willingness to persuade on harmful topics, revealing gaps in safety guardrails.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked risk of LLMs attempting persuasion in harmful contexts, such as glorifying terrorism, and evaluate the efficacy of safety measures.

Method: APE uses a multi-turn conversational setup between simulated persuader and persuadee agents, probing LLMs on diverse topics, with an automated evaluator to measure persuasive attempts.

Result: Many open and closed-weight LLMs frequently attempt persuasion on harmful topics, with jailbreaking increasing this behavior.

Conclusion: APE highlights the need to evaluate willingness to persuade as a critical dimension of LLM risk, revealing gaps in current safety measures.

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [474] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/pdf/2506.02918)
*Shangmin Guo, Omar Darwiche Domingues, Raphaël Avalos, Aaron Courville, Florian Strub*

Main category: cs.AI

TL;DR: DyMo enhances LLMs with state prediction for tool use, reducing hallucinations and improving success rates. Combined with SVS, it boosts reliability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Address challenges of LLMs in stateful environments where repeated trials are impractical.

Method: DyMo adds state prediction and function calling post-training; integrates with SVS for self-verification.

Result: Improves success rates, reduces hallucinations, and enhances pass^k over trials.

Conclusion: DyMo and SVS advance LLM reliability for tool use, enabling scalable planning without repeated environment queries.

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [475] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/pdf/2506.02923)
*Alexis Bellot, Jonathan Richens, Tom Everitt*

Main category: cs.AI

TL;DR: The paper explores how well an agent's beliefs can be inferred from its behavior and how reliably these beliefs predict behavior in new situations, providing theoretical bounds for such predictions.


<details>
  <summary>Details</summary>
Motivation: Understanding and predicting AI behavior is crucial for safe deployment, especially in novel situations where comprehensive safety evaluations are impractical.

Method: The study assumes the agent's behavior is guided by a world model and derives novel bounds on behavior in unseen environments.

Result: The paper establishes theoretical limits for predicting intentional agents from behavioral data alone.

Conclusion: The findings have implications for fairness and safety in AI research, highlighting the limits of behavioral prediction.

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [476] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/pdf/2506.02949)
*Lixiang Xu, Xianwei Ding, Xin Yuan, Richang Hong, Feiping Nie, Enhong Chen, Philip S. Yu*

Main category: cs.AI

TL;DR: The paper introduces CRDP-KT, a model using dynamic programming to optimize cognitive representations in Knowledge Tracing, addressing biases from non-cognitive factors and improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing KT methods focus on feature enhancement but neglect cognitive representation issues caused by non-cognitive factors like slipping and guessing, leading to prediction bias and modeling inefficiencies.

Method: The CRDP-KT model employs dynamic programming to align cognitive representations with student patterns, optimizes representations partitionedly, and uses weighted fusion for better cognition expression.

Result: Experiments on three public datasets confirm the model's effectiveness in maintaining cognitive continuity and coherence, reducing prediction bias.

Conclusion: CRDP-KT enhances KT by optimizing cognitive representations, improving accuracy and reliability in predicting student performance.

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [477] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/pdf/2506.02992)
*Li Zhang, Kevin D. Ashley*

Main category: cs.AI

TL;DR: The paper introduces a reflective multi-agent method to improve legal argument generation by LLMs, addressing issues like hallucination and ungrounded persuasion. It outperforms baselines in abstention, accuracy, and fact utilization.


<details>
  <summary>Details</summary>
Motivation: LLMs in legal argumentation risk manipulation through hallucination and poor fact usage. This work aims to ensure ethical, grounded persuasion.

Method: A multi-agent system (Factor Analyst and Argument Polisher) iteratively refines 3-ply legal arguments. Evaluated across diverse LLMs and legal scenarios.

Result: Outperforms baselines in abstention, reduces hallucination, and improves fact recall, especially in "non-arguable" cases.

Conclusion: Structured multi-agent reflection enhances ethical persuasion in legal LLMs, advancing trustworthy AI in law.

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [478] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/pdf/2506.02996)
*Matthieu Tehenan, Christian Bolivar Moya, Tenghai Long, Guang Lin*

Main category: cs.AI

TL;DR: LLMs encode linear spatial world models, as evidenced by empirical testing of their contextual embeddings.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs acquire internal world models, specifically linear spatial representations.

Method: Introduced a formal framework for spatial world models, trained probes on synthetic object positions, and conducted causal interventions.

Result: Empirical evidence shows LLMs encode linear spatial world models.

Conclusion: LLMs implicitly represent linear spatial configurations, suggesting internal world model acquisition.

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [479] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/pdf/2506.03032)
*Junhao Yu, Yan Zhuang, YuXuan Sun, Weibo Gao, Qi Liu, Mingyue Cheng, Zhenya Huang, Enhong Chen*

Main category: cs.AI

TL;DR: TestAgent, an LLM-powered agent, improves adaptive testing by engaging interactively, reducing questions by 20% while enhancing accuracy and user experience.


<details>
  <summary>Details</summary>
Motivation: Current adaptive testing methods struggle with mechanization, guessing behavior, noisy data, and coarse outputs, limiting effectiveness.

Method: TestAgent uses LLMs for personalized question selection, captures responses dynamically, and provides precise outcomes via conversational interactions.

Result: Experiments show 20% fewer questions needed and higher accuracy in psychological, educational, and lifestyle assessments, with better user feedback.

Conclusion: TestAgent advances adaptive testing by leveraging LLMs for interactive, efficient, and accurate assessments.

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [480] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/pdf/2506.03056)
*Ram Potham, Max Harms*

Main category: cs.AI

TL;DR: The paper proposes 'Corrigibility as a Singular Target' (CAST) to ensure foundation models (FMs) remain under human control, addressing alignment challenges and emergent power-seeking behaviors.


<details>
  <summary>Details</summary>
Motivation: To prevent existential risks from misaligned FMs by shifting focus from static value-loading to dynamic human empowerment.

Method: CAST framework, using training methods like RLAIF and SFT, scalability testing, and controlled instructability demonstrations.

Result: FMs designed to prioritize human control, transforming instrumental drives to align with human guidance.

Conclusion: CAST offers a scalable solution to FM alignment, ensuring AI remains tool-like and responsive to human oversight.

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [481] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/pdf/2506.03095)
*Man Luo, David Cobbley, Xin Su, Shachar Rosenman, Vasudev Lal, Shao-Yen Tseng, Phillip Howard*

Main category: cs.AI

TL;DR: A lightweight vision-language model for local GUI agents is developed, using an LLM-as-Judge framework for training, outperforming baselines on OS-World.


<details>
  <summary>Details</summary>
Motivation: Address privacy and scalability concerns of cloud-based GUI agents by creating a local, efficient alternative.

Method: Develop a lightweight vision-language model trained with synthetic interaction trajectories filtered by an LLM-as-Judge framework.

Result: The local model outperforms existing baselines on the OS-World benchmark.

Conclusion: The approach offers a promising path toward private, efficient, and generalizable GUI agents.

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


### [482] [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/pdf/2506.01704)
*Jiongnan Liu, Zhicheng Dou, Ning Hu, Chenyan Xiong*

Main category: cs.AI

TL;DR: A new method uses multimodal models to generate personalized items like images, enhancing recommender systems beyond filtering.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional recommender systems by generating novel, personalized content instead of just filtering existing items.

Method: Leverage any-to-any Large Multimodal Models (LMMs) trained with supervised fine-tuning and online reinforcement learning to generate tailored items.

Result: Experiments and user studies show the method effectively aligns with user preferences and future interests.

Conclusion: The proposed paradigm successfully extends recommender systems to generate personalized, novel content.

Abstract: To address the challenge of information overload from massive web contents,
recommender systems are widely applied to retrieve and present personalized
results for users. However, recommendation tasks are inherently constrained to
filtering existing items and lack the ability to generate novel concepts,
limiting their capacity to fully satisfy user demands and preferences. In this
paper, we propose a new paradigm that goes beyond content filtering and
selecting: directly generating personalized items in a multimodal form, such as
images, tailored to individual users. To accomplish this, we leverage
any-to-any Large Multimodal Models (LMMs) and train them in both supervised
fine-tuning and online reinforcement learning strategy to equip them with the
ability to yield tailored next items for users. Experiments on two benchmark
datasets and user study confirm the efficacy of the proposed method. Notably,
the generated images not only align well with users' historical preferences but
also exhibit relevance to their potential future interests.

</details>


### [483] [MCU: An Evaluation Framework for Open-Ended Game Agents](https://arxiv.org/pdf/2310.08367)
*Xinyue Zheng, Haowei Lin, Kaichen He, Zihao Wang, Zilong Zheng, Yitao Liang*

Main category: cs.AI

TL;DR: The paper introduces Minecraft Universe (MCU), a scalable evaluation framework for AI agents in open-world environments, featuring composable tasks and a human-aligned assessment system.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for open-ended AI agents lack scalability, necessitating a robust framework like MCU to evaluate diverse tasks effectively.

Method: MCU includes 3,452 atomic tasks, a task composition mechanism for infinite task generation, and a general evaluation framework aligned with human ratings.

Result: State-of-the-art AI agents struggle with MCU's diverse and complex tasks, demonstrating the framework's effectiveness.

Conclusion: MCU serves as a vital benchmark for advancing AI agent development in open-ended environments.

Abstract: Developing AI agents capable of interacting with open-world environments to
solve diverse tasks is a compelling challenge. However, evaluating such
open-ended agents remains difficult, with current benchmarks facing scalability
limitations. To address this, we introduce Minecraft Universe (MCU), a
comprehensive evaluation framework set within the open-world video game
Minecraft. MCU incorporates three key components: (1) an expanding collection
of 3,452 composable atomic tasks that encompasses 11 major categories and 41
subcategories of challenges; (2) a task composition mechanism capable of
generating infinite diverse tasks with varying difficulty; and (3) a general
evaluation framework that achieves 91.5\% alignment with human ratings for
open-ended task assessment. Empirical results reveal that even state-of-the-art
foundation agents struggle with the increasing diversity and complexity of
tasks. These findings highlight the necessity of MCU as a robust benchmark to
drive progress in AI agent development within open-ended environments. Our
evaluation code and scripts are available at
https://github.com/CraftJarvis/MCU.

</details>


### [484] [Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges](https://arxiv.org/pdf/2403.15587)
*David Herrera-Poyatos, Cristina Zuheros, Rosana Montes, Francisco Herrera*

Main category: cs.AI

TL;DR: The paper explores using ChatGPT for Crowd Decision Making (CDM) by analyzing social media texts, demonstrating its potential in opinion extraction and decision-making through prompt design strategies.


<details>
  <summary>Details</summary>
Motivation: To leverage Large Language Models (LLMs) like ChatGPT for enhancing CDM processes by automating opinion extraction from social media texts.

Method: Integrates ChatGPT into CDM processes using prompt design strategies for inferring opinions, with experiments on real data from TripAdvisor (TripR-2020Large dataset).

Result: Empirical results show promise for ChatGPT in developing quality decision-making models.

Conclusion: The study highlights ChatGPT's potential in CDM but notes challenges like consistency, sensitivity, and explainability, suggesting future research directions.

Abstract: Social Media and Internet have the potential to be exploited as a source of
opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a
methodology able to infer opinions and decisions from plain texts, such as
reviews published in social media platforms, by means of Sentiment Analysis.
Currently, the emergence and potential of Large Language Models (LLMs) lead us
to explore new scenarios of automatically understand written texts, also known
as natural language processing. This paper analyzes the use of ChatGPT based on
prompt design strategies to assist in CDM processes to extract opinions and
make decisions. We integrate ChatGPT in CDM processes as a flexible tool that
infer the opinions expressed in texts, providing numerical or linguistic
evaluations where the decision making models are based on the prompt design
strategies. We include a multi-criteria decision making scenario with a
category ontology for criteria. We also consider ChatGPT as an end-to-end CDM
model able to provide a general opinion and score on the alternatives. We
conduct empirical experiments on real data extracted from TripAdvisor, the
TripR-2020Large dataset. The analysis of results show a promising branch for
developing quality decision making models using ChatGPT. Finally, we discuss
the challenges of consistency, sensitivity and explainability associated to the
use of LLMs in CDM processes, raising open questions for future studies.

</details>


### [485] [Assurance of AI Systems From a Dependability Perspective](https://arxiv.org/pdf/2407.13948)
*Robin Bloomfield, John Rushby*

Main category: cs.AI

TL;DR: The paper discusses assurance principles for AI/ML systems, contrasting 'dependability' (minimizing trust in AI/ML) and 'trustworthy' (assuring AI/ML directly) perspectives. It proposes architectures leaning toward dependability and suggests research directions.


<details>
  <summary>Details</summary>
Motivation: Addressing risks in AI/ML systems by balancing dependability and trustworthiness, especially in critical applications.

Method: Uses 'defense in depth' with simpler systems to guard AI/ML, and explores methods like diversity, explanations, and normative rules to minimize trust.

Result: Proposes architectures and methods for dependable AI/ML systems, emphasizing a spectrum between dependability and trustworthiness.

Conclusion: Advocates for a fourfold research agenda to improve assurance in AI/ML systems, spanning autonomous systems to AGI.

Abstract: We outline the principles of classical assurance for computer-based systems
that pose significant risks. We then consider application of these principles
to systems that employ Artificial Intelligence (AI) and Machine Learning (ML).
  A key element in this "dependability" perspective is a requirement for
thorough understanding of the behavior of critical components, and this is
considered infeasible for AI and ML. Hence the dependability perspective aims
to minimize trust in AI and ML elements by using "defense in depth" with a
hierarchy of less complex systems, some of which may be highly assured
conventionally engineered components, to "guard" them. This may be contrasted
with the "trustworthy" perspective that seeks to apply assurance to the AI and
ML elements themselves.
  In cyber-physical and many other systems, it is difficult to provide guards
that do not depend on AI and ML to perceive their environment (e.g., vehicles
sharing the road with a self-driving car), so both perspectives are needed and
there is a continuum or spectrum between them. We focus on architectures toward
the dependability end of the continuum and invite others to consider additional
points along the spectrum.
  For guards that require perception using AI and ML, we examine ways to
minimize the trust placed in these elements; they include diversity, defense in
depth, explanations, and micro-ODDs. We also examine methods to enforce
acceptable behavior, given a model of the world. These include classical
cyber-physical calculations and envelopes, and normative rules based on
overarching principles, constitutions, ethics, or reputation.
  We apply our perspective to autonomous systems, AI systems for specific
functions, general-purpose AI such as Large Language Models (LLMs), and
Artificial General Intelligence (AGI), and we propose current best practice and
conclude with a fourfold agenda for research.

</details>


### [486] [RAG4ITOps: A Supervised Fine-Tunable and Comprehensive RAG Framework for IT Operations and Maintenance](https://arxiv.org/pdf/2410.15805)
*Tianyang Zhang, Zhuoxuan Jiang, Shengguang Bai, Tianrui Zhang, Lin Lin, Yang Liu, Jiawei Ren*

Main category: cs.AI

TL;DR: The paper proposes RAG4ITOps, a framework for building domain-specific QA systems for IT operations using Retrieval Augmented Generation (RAG), achieving superior results on enterprise-exclusive corpora.


<details>
  <summary>Details</summary>
Motivation: Address the gap in efficiently handling enterprise-exclusive corpora and building domain-specific QA systems for industrial applications, particularly in IT operations and maintenance.

Method: A two-stage framework: (1) Fine-tuning embedding models and LLMs using contrastive learning and instruction templates, and (2) Building an online QA system.

Result: Superior performance on QA tasks using enterprise-exclusive cloud computing corpora, validated through extensive experiments.

Conclusion: RAG4ITOps is effective for real-world enterprise-level QA applications in IT operations, demonstrating practical utility.

Abstract: With the ever-increasing demands on Question Answering (QA) systems for IT
operations and maintenance, an efficient and supervised fine-tunable framework
is necessary to ensure the data security, private deployment and continuous
upgrading. Although Large Language Models (LLMs) have notably improved the
open-domain QA's performance, how to efficiently handle enterprise-exclusive
corpora and build domain-specific QA systems are still less-studied for
industrial applications. In this paper, we propose a general and comprehensive
framework based on Retrieval Augmented Generation (RAG) and facilitate the
whole business process of establishing QA systems for IT operations and
maintenance. In accordance with the prevailing RAG method, our proposed
framework, named with RAG4ITOps, composes of two major stages: (1) Models
Fine-tuning \& Data Vectorization, and (2) Online QA System Process. At the
Stage 1, we leverage a contrastive learning method with two negative sampling
strategies to fine-tune the embedding model, and design the instruction
templates to fine-tune the LLM with a Retrieval Augmented Fine-Tuning method.
At the Stage 2, an efficient process of QA system is built for serving. We
collect enterprise-exclusive corpora from the domain of cloud computing, and
the extensive experiments show that our method achieves superior results than
counterparts on two kinds of QA tasks. Our experiment also provide a case for
applying the RAG4ITOps to real-world enterprise-level applications.

</details>


### [487] [The Tug of War Within: Mitigating the Fairness-Privacy Conflicts in Large Language Models](https://arxiv.org/pdf/2410.16672)
*Chen Qian, Dongrui Liu, Jie Zhang, Yong Liu, Jing Shao*

Main category: cs.AI

TL;DR: The paper introduces SPIN, a training-free method to address the trade-off between fairness and privacy awareness in LLMs, improving both without compromising general capabilities.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve the counter-intuitive trade-off where enhancing privacy awareness in LLMs via SFT reduces fairness awareness.

Method: SPIN suppresses coupled neurons for privacy and fairness using information theory, reducing their mutual influence.

Result: SPIN improves fairness by 12.2% and privacy by 14.0% in Qwen-2-7B-Instruct, even with limited or malicious data.

Conclusion: SPIN effectively addresses fairness-privacy trade-offs, generalizes to other dimensions, and aids ethical AI development.

Abstract: Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is
critical. Interestingly, we discover a counter-intuitive trade-off phenomenon
that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT)
methods significantly decreases its fairness awareness with thousands of
samples. To address this issue, inspired by the information theory, we
introduce a training-free method to \textbf{S}uppress the \textbf{P}rivacy and
fa\textbf{I}rness coupled \textbf{N}eurons (\textbf{SPIN}), which theoretically
and empirically decrease the mutual information between fairness and privacy
awareness. Extensive experimental results demonstrate that SPIN eliminates the
trade-off phenomenon and significantly improves LLMs' fairness and privacy
awareness simultaneously without compromising general capabilities, \eg
improving Qwen-2-7B-Instruct's fairness awareness by 12.2\% and privacy
awareness by 14.0\%. More crucially, SPIN remains robust and effective with
limited annotated data or even when only malicious fine-tuning data is
available, whereas SFT methods may fail to perform properly in such scenarios.
Furthermore, we show that SPIN could generalize to other potential trade-off
dimensions. We hope this study provides valuable insights into concurrently
addressing fairness and privacy concerns in LLMs and can be integrated into
comprehensive frameworks to develop more ethical and responsible AI systems.
Our code is available at https://github.com/ChnQ/SPIN.

</details>


### [488] [Reclaiming "Open AI" -- AI Model Serving Can Be Open Access, Yet Monetizable and Loyal](https://arxiv.org/pdf/2411.03887)
*Zerui Cheng, Edoardo Contente, Ben Finch, Oleg Golev, Jonathan Hayase, Andrew Miller, Niusha Moshrefi, Anshul Nasery, Sandeep Nailwal, Sewoong Oh, Himanshu Tyagi, Pramod Viswanath*

Main category: cs.AI

TL;DR: The paper proposes the OML paradigm for AI model serving, combining transparency, monetization, and safety to address the current dichotomy between open-weight and API-based approaches.


<details>
  <summary>Details</summary>
Motivation: The dichotomy in AI model serving—open-weight lacks control/monetization, while API-based risks privacy/transparency—hinders an equitable AI ecosystem.

Method: Introduces and formulates the OML paradigm, surveys its constructions, analyzes trade-offs, outlines a deployment protocol, and discusses implications.

Result: OML can democratize AI, mitigate centralized power risks, and foster innovation through transparency, monetization, and safety.

Conclusion: Calls for further research on OML's design space to achieve a collaborative, accountable, and resilient AI future.

Abstract: The rapid rise of AI has split model serving between open-weight
distribution, which often lacks owner control and monetization, and opaque
API-based approaches that risk user privacy and model transparency, forming a
dichotomy that hinders an equitable AI ecosystem. This position paper
introduces, rigorously formulates, and champions the Open-access, Monetizable,
and Loyal (OML) paradigm for AI model serving: a foundational shift to securely
distribute and serve AI models by synthesizing transparency with granular
monetization and critical safety controls. We survey diverse OML constructions
from theory and practice, analyze their security, performance, and practical
trade-offs, outline a conceptual OML deployment protocol, and discuss market
and policy implications. We assert that OML can foster a democratized,
self-sustaining, and innovative AI landscape, mitigating centralized power
risks. Finally, we call on the research community to further explore the broad
design space of OML, spanning cryptographic, AI-native, and socio-economic
mechanisms, to realize its full potential for a collaborative, accountable, and
resilient AI future.

</details>


### [489] [Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge](https://arxiv.org/pdf/2411.09689)
*Seongmin Lee, Hsiang Hsu, Chun-Fu Chen, Duen Horng, Chau*

Main category: cs.AI

TL;DR: The paper introduces hallucination probing to classify LLM-generated text into aligned, misaligned, or fabricated categories, proposing SHINE, a method that outperforms existing approaches without needing external knowledge or training.


<details>
  <summary>Details</summary>
Motivation: Addressing LLM hallucination's critical challenge and the lack of distinction between hallucination types in current detection methods.

Method: Proposes SHINE, a hallucination probing method leveraging prompt perturbations to classify text without external knowledge or training.

Result: SHINE achieves state-of-the-art performance in hallucination detection across multiple LLMs and datasets, outperforming seven competing methods.

Conclusion: Hallucination probing is crucial for accurate detection, and SHINE offers an effective, resource-efficient solution.

Abstract: LLM hallucination, where unfaithful text is generated, presents a critical
challenge for LLMs' practical applications. Current detection methods often
resort to external knowledge, LLM fine-tuning, or supervised training with
large hallucination-labeled datasets. Moreover, these approaches do not
distinguish between different types of hallucinations, which is crucial for
enhancing detection performance. To address such limitations, we introduce
hallucination probing, a new task that classifies LLM-generated text into three
categories: aligned, misaligned, and fabricated. Driven by our novel discovery
that perturbing key entities in prompts affects LLM's generation of these three
types of text differently, we propose SHINE, a novel hallucination probing
method that does not require external knowledge, supervised training, or LLM
fine-tuning. SHINE is effective in hallucination probing across three modern
LLMs, and achieves state-of-the-art performance in hallucination detection,
outperforming seven competing methods across four datasets and four LLMs,
underscoring the importance of probing for accurate detection.

</details>


### [490] [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/pdf/2412.10849)
*Peter G. Brodeur, Thomas A. Buckley, Zahir Kanjee, Ethan Goh, Evelyn Bin Ling, Priyank Jain, Stephanie Cabral, Raja-Elie Abdulnour, Adrian D. Haimovich, Jason A. Freed, Andrew Olson, Daniel J. Morgan, Jason Hom, Robert Gallo, Liam G. McCoy, Haadi Mombini, Christopher Lucas, Misha Fotoohi, Matthew Gwiazdon, Daniele Restifo, Daniel Restrepo, Eric Horvitz, Jonathan Chen, Arjun K. Manrai, Adam Rodman*

Main category: cs.AI

TL;DR: A study evaluates a large language model (LLM) against physicians in clinical reasoning tasks, showing superhuman performance in diagnostics and management, fulfilling a 1959 vision for expert medical systems.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can meet the gold standard for clinical diagnostic reasoning set by Ledley and Lusted in 1959, and compare their performance to human physicians.

Method: Five experiments measured clinical reasoning (e.g., differential diagnosis, probabilistic reasoning) and a real-world study compared LLMs and physicians in emergency room second opinions.

Result: The LLM outperformed physicians in all experiments, demonstrating superhuman diagnostic and reasoning abilities.

Conclusion: LLMs have achieved superhuman performance in medical diagnostics, fulfilling the 1959 vision and highlighting the need for prospective trials.

Abstract: A seminal paper published by Ledley and Lusted in 1959 introduced complex
clinical diagnostic reasoning cases as the gold standard for the evaluation of
expert medical computing systems, a standard that has held ever since. Here, we
report the results of a physician evaluation of a large language model (LLM) on
challenging clinical cases against a baseline of hundreds of physicians. We
conduct five experiments to measure clinical reasoning across differential
diagnosis generation, display of diagnostic reasoning, triage differential
diagnosis, probabilistic reasoning, and management reasoning, all adjudicated
by physician experts with validated psychometrics. We then report a real-world
study comparing human expert and AI second opinions in randomly-selected
patients in the emergency room of a major tertiary academic medical center in
Boston, MA. We compared LLMs and board-certified physicians at three predefined
diagnostic touchpoints: triage in the emergency room, initial evaluation by a
physician, and admission to the hospital or intensive care unit. In all
experiments--both vignettes and emergency room second opinions--the LLM
displayed superhuman diagnostic and reasoning abilities, as well as continued
improvement from prior generations of AI clinical decision support. Our study
suggests that LLMs have achieved superhuman performance on general medical
diagnostic and management reasoning, fulfilling the vision put forth by Ledley
and Lusted, and motivating the urgent need for prospective trials.

</details>


### [491] [GAS: Generative Auto-bidding with Post-training Search](https://arxiv.org/pdf/2412.17018)
*Yewen Li, Shuai Mao, Jingtong Gao, Nan Jiang, Yunjian Xu, Qingpeng Cai, Fei Pan, Peng Jiang, Bo An*

Main category: cs.AI

TL;DR: The paper introduces GAS, a Generative Auto-bidding scheme using post-training search, to refine base policy models and adapt to diverse advertiser preferences, addressing data quality and generalization issues in generative models.


<details>
  <summary>Details</summary>
Motivation: Generative auto-bidding faces challenges like low-quality data and poor generalization due to majority preferences in datasets, making retraining costly and impractical.

Method: GAS employs weak-to-strong search alignment with small critics and MCTS-inspired search, enhanced by a transformer-based voting mechanism, and includes a fine-tuning method for high-frequency preferences.

Result: Experiments on real-world data and online A/B tests show GAS improves performance, e.g., a 4.60% increase in target cost.

Conclusion: GAS offers a flexible and practical solution for generative auto-bidding, advancing the field toward large foundation models.

Abstract: Auto-bidding is essential in facilitating online advertising by automatically
placing bids on behalf of advertisers. Generative auto-bidding, which generates
bids based on an adjustable condition using models like transformers and
diffusers, has recently emerged as a new trend due to its potential to learn
optimal strategies directly from data and adjust flexibly to preferences.
However, generative models suffer from low-quality data leading to a mismatch
between the condition, like return to go, and true action value, especially in
long sequential decision-making. Besides, the majority preference in the
dataset may hinder models' generalization ability on minority advertisers'
preferences. While it is possible to collect high-quality data and retrain
multiple models for different preferences, the high cost makes it unaffordable,
hindering the advancement of auto-bidding into the era of large foundation
models. To address this, we propose a flexible and practical Generative
Auto-bidding scheme using post-training Search, termed GAS, to refine a base
policy model's output and adapt to various preferences. We use weak-to-strong
search alignment by training small critics for different preferences and an
MCTS-inspired search to refine the model's output. Specifically, a novel voting
mechanism with transformer-based critics trained with policy indications could
enhance search alignment performance. Additionally, utilizing the search, we
provide a fine-tuning method for high-frequency preference scenarios
considering computational efficiency. Extensive experiments conducted on the
real-world dataset and online A/B test on the Kuaishou advertising platform
demonstrate the effectiveness of GAS, achieving significant improvements, e.g.,
4.60% increment of target cost.

</details>


### [492] [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/pdf/2501.12599)
*Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, Zongyu Lin*

Main category: cs.AI

TL;DR: The paper introduces Kimi k1.5, a multi-modal LLM trained with RL, achieving state-of-the-art results by simplifying RL techniques and leveraging long context scaling.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of next token prediction by scaling RL for improved AI performance, addressing prior shortcomings in RL-based LLMs.

Method: Uses long context scaling, improved policy optimization, and multi-modal data recipes, avoiding complex techniques like Monte Carlo tree search.

Result: Achieves top-tier benchmarks (e.g., 77.5 on AIME, 96.2 on MATH500) and outperforms models like GPT-4o in short-CoT reasoning.

Conclusion: Demonstrates a simple yet effective RL framework for LLMs, advancing multi-modal reasoning and outperforming existing models.

Abstract: Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior published work has not produced competitive results. In
light of this, we report on the training practice of Kimi k1.5, our latest
multi-modal LLM trained with RL, including its RL training techniques,
multi-modal data recipes, and infrastructure optimization. Long context scaling
and improved policy optimization methods are key ingredients of our approach,
which establishes a simplistic, effective RL framework without relying on more
complex techniques such as Monte Carlo tree search, value functions, and
process reward models. Notably, our system achieves state-of-the-art reasoning
performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,
96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching
OpenAI's o1. Moreover, we present effective long2short methods that use
long-CoT techniques to improve short-CoT models, yielding state-of-the-art
short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on
LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and
Claude Sonnet 3.5 by a large margin (up to +550%).

</details>


### [493] [MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems](https://arxiv.org/pdf/2501.19318)
*Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou*

Main category: cs.AI

TL;DR: MINDSTORES enhances LLM-based embodied agents by enabling them to learn from past experiences and build mental models, improving robustness in complex environments like Minecraft.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack the ability to learn from experience or build persistent mental models, limiting their effectiveness in open-world environments.

Method: MINDSTORES augments zero-shot LLM planning with a database of past experiences, represented as natural language embeddings, to inform future planning.

Result: Experiments in MineDojo show MINDSTORES outperforms memory-based LLM planners while retaining zero-shot flexibility.

Conclusion: MINDSTORES advances embodied AI by enabling continuous learning through natural interaction, enhancing adaptability in complex environments.

Abstract: While large language models (LLMs) have shown promising capabilities as
zero-shot planners for embodied agents, their inability to learn from
experience and build persistent mental models limits their robustness in
complex open-world environments like Minecraft. We introduce MINDSTORES, an
experience-augmented planning framework that enables embodied agents to build
and leverage mental models through natural interaction with their environment.
Drawing inspiration from how humans construct and refine cognitive mental
models, our approach extends existing zero-shot LLM planning by maintaining a
database of past experiences that informs future planning iterations. The key
innovation is representing accumulated experiences as natural language
embeddings of (state, task, plan, outcome) tuples, which can then be
efficiently retrieved and reasoned over by an LLM planner to generate insights
and guide plan refinement for novel states and tasks. Through extensive
experiments in the MineDojo environment, a simulation environment for agents in
Minecraft that provides low-level controls for Minecraft, we find that
MINDSTORES learns and applies its knowledge significantly better than existing
memory-based LLM planners while maintaining the flexibility and generalization
benefits of zero-shot approaches, representing an important step toward more
capable embodied AI systems that can learn continuously through natural
experience.

</details>


### [494] [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/pdf/2502.16810)
*Jibang Wu, Chenghao Yang, Simon Mahns, Chaoqi Wang, Hao Zhu, Fei Fang, Haifeng Xu*

Main category: cs.AI

TL;DR: An agentic framework using LLMs automates persuasive, grounded marketing content for real estate, outperforming human experts in preference while maintaining factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate persuasive marketing content generation while aligning with user preferences and ensuring factual accuracy, using real estate as a case study.

Method: Three modules: Grounding (predicts marketable features), Personalization (aligns with user preferences), and Marketing (ensures factual accuracy and localized features).

Result: Generated descriptions preferred over human experts' by a clear margin, with equal factual accuracy.

Conclusion: The framework shows promise for automating large-scale targeted marketing with factual content generation.

Abstract: This paper develops an agentic framework that employs large language models
(LLMs) to automate the generation of persuasive and grounded marketing content,
using real estate listing descriptions as our focal application domain. Our
method is designed to align the generated content with user preferences while
highlighting useful factual attributes. This agent consists of three key
modules: (1) Grounding Module, mimicking expert human behavior to predict
marketable features; (2) Personalization Module, aligning content with user
preferences; (3) Marketing Module, ensuring factual accuracy and the inclusion
of localized features. We conduct systematic human-subject experiments in the
domain of real estate marketing, with a focus group of potential house buyers.
The results demonstrate that marketing descriptions generated by our approach
are preferred over those written by human experts by a clear margin while
maintaining the same level of factual accuracy. Our findings suggest a
promising agentic approach to automate large-scale targeted marketing while
ensuring factuality of content generation.

</details>


### [495] [Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff](https://arxiv.org/pdf/2502.20704)
*Maximilian Holsman, Yukun Huang, Bhuwan Dhingra*

Main category: cs.AI

TL;DR: Fuzzy Speculative Decoding (FSD) generalizes Speculative Decoding (SD) by allowing controlled divergence from the target model, enabling flexible trade-offs between generation quality and inference speed.


<details>
  <summary>Details</summary>
Motivation: SD's strict distributional equivalence limits speedup potential and prevents quality-speed trade-offs. FSD addresses this by introducing controlled divergence.

Method: FSD accepts candidate tokens based on divergences between target and draft model distributions, allowing tunable quality-speed trade-offs.

Result: FSD achieves significant speed improvements (over 5 tokens/sec faster than SD) with minimal accuracy loss (~2%) and sometimes matches SD accuracy at higher speeds.

Conclusion: FSD demonstrates that strict distributional equivalence isn't necessary for performance, and it can enhance existing SD extensions like EAGLE-2.

Abstract: Speculative Decoding (SD) enforces strict distributional equivalence to the
target model when accepting candidate tokens. While it maintains the target
model's generation quality, this strict equivalence limits the speedup
achievable by SD and prevents users from trading deviations from the target
distribution in exchange for further inference speed gains. To address these
limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding
algorithm that generalizes SD by accepting candidate tokens based on the
divergences between the target and draft model distributions. By allowing for
controlled divergence from the target model, FSD enables users to flexibly
trade generation quality for inference speed. Across several benchmarks, our
method is able to achieve significant runtime improvements of over 5 tokens per
second faster than SD at only an approximate 2% absolute reduction in benchmark
accuracy. In many cases, FSD is even able to match SD benchmark accuracy at
over 2 tokens per second faster, demonstrating that distributional equivalence
is not necessary to maintain target model performance. Furthermore, FSD can be
seamlessly integrated into existing SD extensions; we demonstrate this by
applying FSD to EAGLE-2, greatly enhancing this existing extension's efficiency
while allowing it to leverage FSD's tunable quality-speed trade-off.

</details>


### [496] [MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math Problem Mistake Finding by Prompt-Guided LLMs](https://arxiv.org/pdf/2503.04291)
*Tianyang Zhang, Zhuoxuan Jiang, Haotian Zhang, Lin Lin, Shaohua Zhang*

Main category: cs.AI

TL;DR: MathMistake Checker automates mistake detection in math problems using computer vision and LLMs, improving grading efficiency and learning.


<details>
  <summary>Details</summary>
Motivation: To simplify grading, enhance efficiency, and improve learning by automating mistake detection in lengthy math answers.

Method: A two-stage process integrating computer vision and LLMs for open-ended grading and personalized feedback.

Result: Effective across calculation and word problems, supporting personalized learning.

Conclusion: The system successfully automates mistake finding, benefiting both educators and learners.

Abstract: We propose a novel system, MathMistake Checker, designed to automate
step-by-step mistake finding in mathematical problems with lengthy answers
through a two-stage process. The system aims to simplify grading, increase
efficiency, and enhance learning experiences from a pedagogical perspective. It
integrates advanced technologies, including computer vision and the
chain-of-thought capabilities of the latest large language models (LLMs). Our
system supports open-ended grading without reference answers and promotes
personalized learning by providing targeted feedback. We demonstrate its
effectiveness across various types of math problems, such as calculation and
word problems.

</details>


### [497] [Towards Automated Semantic Interpretability in Reinforcement Learning via Vision-Language Models](https://arxiv.org/pdf/2503.16724)
*Zhaoxin Li, Zhang Xi-Jia, Batuhan Altundas, Letian Chen, Rohan Paleja, Matthew Gombolay*

Main category: cs.AI

TL;DR: iTRACE automates semantic feature extraction and policy optimization in RL using VLMs and interpretable trees, eliminating human annotation and matching CNN-based performance.


<details>
  <summary>Details</summary>
Motivation: To achieve semantic interpretability in RL without relying on human-specified features, which may not generalize well.

Method: Uses VLMs for semantic feature extraction and interpretable trees for policy optimization, distilling VLM outputs into a lightweight model.

Result: Outperforms MLP baselines and matches CNN-based policies, producing verifiable and human-aligned behaviors.

Conclusion: iTRACE provides a scalable, automated solution for interpretable RL without human annotation.

Abstract: Semantic interpretability in Reinforcement Learning (RL) enables transparency
and verifiability by making the agent's decisions understandable and
verifiable. Achieving this, however, requires a feature space composed of
human-understandable concepts, which traditionally rely on human specification
and may fail to generalize to unseen environments. We introduce interpretable
Tree-based Reinforcement learning via Automated Concept Extraction (iTRACE), an
automated framework that leverages pre-trained vision-language models (VLM) for
semantic feature extraction and interpretable tree-based models for policy
optimization. iTRACE first extracts semantically meaningful features, then maps
them to policies via interpretable trees. To address the impracticality of
running VLMs in RL loops, we distill their outputs into a lightweight model. By
leveraging Vision-Language Models (VLMs) to automate tree-based reinforcement
learning, iTRACE eliminates the need for human annotation traditionally
required by interpretable models, while also addressing the limitations of VLMs
alone, such as their lack of grounding in action spaces and inability to
directly optimize policies. iTRACE outperforms MLP baselines that use the same
interpretable features and matches the performance of CNN-based policies,
producing verifiable, semantically interpretable, and human-aligned behaviors
without requiring human annotation.

</details>


### [498] [DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing](https://arxiv.org/pdf/2505.00278)
*Lo Pang-Yun Ting, Yu-Hao Chiang, Yi-Tung Tsai, Hsu-Chao Lai, Kun-Ta Chuang*

Main category: cs.AI

TL;DR: DeCo is an AI-driven approach for optimizing IC testing task assignment by integrating defect characteristics, historical failures, and engineer expertise, achieving high success rates and balanced workloads.


<details>
  <summary>Details</summary>
Motivation: Current IC defect studies lack integration of defect characteristics, historical failures, and engineer insights, limiting their effectiveness in improving IC handling.

Method: DeCo constructs a defect-aware graph from IC testing reports, models engineer and task representations, and uses a contrasting-based assignment mechanism for task-engineer pairing.

Result: DeCo achieves over 80% task-handling success rates and balanced workloads, even with scarce defect data, and assigns tasks to capable engineers for unfamiliar defects.

Conclusion: DeCo demonstrates potential as an AI-driven solution for real-world IC failure analysis and task handling, improving efficiency and reducing losses.

Abstract: In the semiconductor industry, integrated circuit (IC) processes play a vital
role, as the rising complexity and market expectations necessitate improvements
in yield. Identifying IC defects and assigning IC testing tasks to the right
engineers improves efficiency and reduces losses. While current studies
emphasize fault localization or defect classification, they overlook the
integration of defect characteristics, historical failures, and the insights
from engineer expertise, which restrains their effectiveness in improving IC
handling. To leverage AI for these challenges, we propose DeCo, an innovative
approach for optimizing task assignment in IC testing. DeCo constructs a novel
defect-aware graph from IC testing reports, capturing co-failure relationships
to enhance defect differentiation, even with scarce defect data. Additionally,
it formulates defect-aware representations for engineers and tasks, reinforced
by local and global structure modeling on the defect-aware graph. Finally, a
contrasting-based assignment mechanism pairs testing tasks with QA engineers by
considering their skill level and current workload, thus promoting an equitable
and efficient job dispatch. Experiments on a real-world dataset demonstrate
that DeCo achieves the highest task-handling success rates in different
scenarios, exceeding 80\%, while also maintaining balanced workloads on both
scarce or expanded defect data. Moreover, case studies reveal that DeCo can
assign tasks to potentially capable engineers, even for their unfamiliar
defects, highlighting its potential as an AI-driven solution for the real-world
IC failure analysis and task handling.

</details>


### [499] [Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System](https://arxiv.org/pdf/2505.01305)
*Lo Pang-Yun Ting, Hong-Pei Chen, An-Shan Liu, Chun-Yin Yeh, Po-Lin Chen, Kun-Ta Chuang*

Main category: cs.AI

TL;DR: TARL is a novel method using shapelet-transition knowledge graphs to analyze heart rate data for early patient deterioration detection, handling missing values and improving reliability.


<details>
  <summary>Details</summary>
Motivation: Early detection of patient deterioration is vital, but challenges like diverse heart rate data and missing values in wearable devices hinder effective monitoring.

Method: TARL models shapelet dynamics in heart rate time series via a knowledge graph, introduces transition-aware embedding, and quantifies missing values' impact.

Result: TARL achieves high reliability and early detection in ICU patient data, with explainable results aiding clinicians.

Conclusion: TARL shows promise as an AI tool for early illness detection, assisting clinicians in recognizing deterioration signs.

Abstract: Early detection of patient deterioration is crucial for reducing mortality
rates. Heart rate data has shown promise in assessing patient health, and
wearable devices offer a cost-effective solution for real-time monitoring.
However, extracting meaningful insights from diverse heart rate data and
handling missing values in wearable device data remain key challenges. To
address these challenges, we propose TARL, an innovative approach that models
the structural relationships of representative subsequences, known as
shapelets, in heart rate time series. TARL creates a shapelet-transition
knowledge graph to model shapelet dynamics in heart rate time series,
indicating illness progression and potential future changes. We further
introduce a transition-aware knowledge embedding to reinforce relationships
among shapelets and quantify the impact of missing values, enabling the
formulation of comprehensive heart rate representations. These representations
capture explanatory structures and predict future heart rate trends, aiding
early illness detection. We collaborate with physicians and nurses to gather
ICU patient heart rate data from wearables and diagnostic metrics assessing
illness severity for evaluating deterioration. Experiments on real-world ICU
data demonstrate that TARL achieves both high reliability and early detection.
A case study further showcases TARL's explainable detection process,
highlighting its potential as an AI-driven tool to assist clinicians in
recognizing early signs of patient deterioration.

</details>


### [500] [Evaluations at Work: Measuring the Capabilities of GenAI in Use](https://arxiv.org/pdf/2505.10742)
*Brandon Lepine, Gawesha Weerantunga, Juho Kim, Pamela Mishkin, Matthew Beane*

Main category: cs.AI

TL;DR: The paper introduces a framework for evaluating human-AI collaboration by decomposing tasks into subtasks and tracking performance and user strategies. It proposes metrics like semantic similarity, structural coherence, and a novel 'information frontier' measure. Findings show LLM integration improves output quality but is moderated by factors like incoherence and knowledge gaps.


<details>
  <summary>Details</summary>
Motivation: Current AI benchmarks fail to capture the complexity of human-AI collaboration in multi-turn dialogues, necessitating a more holistic evaluation framework.

Method: Develops a framework decomposing tasks into subtasks and introduces metrics (semantic similarity, structural coherence, intra-turn diversity, 'information frontier') to evaluate LLM performance and user strategies.

Result: Greater LLM integration improves output quality but is moderated by response incoherence, excessive subtask diversity, and misalignment with user knowledge. Proactive novelty strategies may harm performance.

Conclusion: The work provides a robust framework and insights for evaluating and improving human-AI collaboration, emphasizing the need for alignment with user knowledge and coherence.

Abstract: Current AI benchmarks miss the messy, multi-turn nature of human-AI
collaboration. We present an evaluation framework that decomposes real-world
tasks into interdependent subtasks, letting us track both LLM performance and
users' strategies across a dialogue. Complementing this framework, we develop a
suite of metrics, including a composite usage derived from semantic similarity,
word overlap, and numerical matches; structural coherence; intra-turn
diversity; and a novel measure of the "information frontier" reflecting the
alignment between AI outputs and users' working knowledge. We demonstrate our
methodology in a financial valuation task that mirrors real-world complexity.
Our empirical findings reveal that while greater integration of LLM-generated
content generally enhances output quality, its benefits are moderated by
factors such as response incoherence, excessive subtask diversity, and the
distance of provided information from users' existing knowledge. These results
suggest that proactive dialogue strategies designed to inject novelty may
inadvertently undermine task performance. Our work thus advances a more
holistic evaluation of human-AI collaboration, offering both a robust
methodological framework and actionable insights for developing more effective
AI-augmented work processes.

</details>


### [501] [Extracting Explainable Dates From Medical Images By Reverse-Engineering UNIX Timestamps](https://arxiv.org/pdf/2505.11451)
*Lee Harris*

Main category: cs.AI

TL;DR: The paper explores using regular expression synthesis to improve date extraction from medical documents, balancing accuracy and false positives.


<details>
  <summary>Details</summary>
Motivation: Dates are critical for medical decisions but hard to extract. AI methods lack explainability, and manual regexes are error-prone.

Method: Tested public regexes, created manual regexes, and used regex synthesis to reverse-engineer UNIX timestamps.

Result: Synthesized regexes reduced false positives but slightly increased missed dates compared to manual regexes.

Conclusion: Regex synthesis is a novel, effective method for extracting complex dates and ranges from text.

Abstract: Dates often contribute towards highly impactful medical decisions, but it is
rarely clear how to extract this data. AI has only just begun to be used
transcribe such documents, and common methods are either to trust that the
output produced by a complex AI model, or to parse the text using regular
expressions. Recent work has established that regular expressions are an
explainable form of logic, but it is difficult to decompose these into the
component parts that are required to construct precise UNIX timestamps. First,
we test publicly-available regular expressions, and we found that these were
unable to capture a significant number of our dates. Next, we manually created
easily-decomposable regular expressions, and we found that these were able to
detect the majority of real dates, but also a lot of sequences of text that
look like dates. Finally, we used regular expression synthesis to automatically
identify regular expressions from the reverse-engineered UNIX timestamps that
we created. We find that regular expressions created by regular expression
synthesis detect far fewer sequences of text that look like dates than those
that were manually created, at the cost of a slight increase to the number of
missed dates. Overall, our results show that regular expressions can be created
through regular expression synthesis to identify complex dates and date ranges
in text transcriptions. To our knowledge, our proposed way of learning
deterministic logic by reverse-engineering several many-one mappings and
feeding these into a regular expression synthesiser is a new approach.

</details>


### [502] [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/pdf/2505.13887)
*Junyang Wang, Haiyang Xu, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, Jitao Sang*

Main category: cs.AI

TL;DR: Mobile-Agent-V uses video to automate mobile tasks, reducing manual effort and improving performance by 36%.


<details>
  <summary>Details</summary>
Motivation: The need for efficient mobile task automation due to rising device usage, with current AI frameworks lacking operational expertise.

Method: Mobile-Agent-V leverages video content to inject operational knowledge into automation, eliminating manual input.

Result: 36% performance improvement over existing methods, validated by the Mobile-Knowledge benchmark.

Conclusion: Mobile-Agent-V offers an effortless and efficient solution for mobile automation, outperforming traditional approaches.

Abstract: The exponential rise in mobile device usage necessitates streamlined
automation for effective task management, yet many AI frameworks fall short due
to inadequate operational expertise. While manually written knowledge can
bridge this gap, it is often burdensome and inefficient. We introduce
Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool
to effortlessly and efficiently inject operational knowledge into mobile
automation processes. By deriving knowledge directly from video content,
Mobile-Agent-V eliminates manual intervention, significantly reducing the
effort and time required for knowledge acquisition. To rigorously evaluate this
approach, we propose Mobile-Knowledge, a benchmark tailored to assess the
impact of external knowledge on mobile agent performance. Our experimental
findings demonstrate that Mobile-Agent-V enhances performance by 36% compared
to existing methods, underscoring its effortless and efficient advantages in
mobile automation.

</details>


### [503] [lmgame-Bench: How Good are LLMs at Playing Games?](https://arxiv.org/pdf/2505.15146)
*Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang*

Main category: cs.AI

TL;DR: The paper introduces lmgame-Bench, a tool to evaluate LLMs using video games, addressing challenges like brittle perception and data contamination. It shows the benchmark's effectiveness across 13 models and highlights transfer learning benefits.


<details>
  <summary>Details</summary>
Motivation: Video games test perception, memory, and planning, key skills for LLMs, but direct evaluation is flawed due to perception issues, prompt sensitivity, and data contamination.

Method: Developed lmgame-Bench, a unified API for platformer, puzzle, and narrative games, with scaffolds to stabilize prompts and remove contamination.

Result: lmgame-Bench is challenging yet effective in differentiating models, with games probing unique capabilities. Reinforcement learning on one game transfers to others and external tasks.

Conclusion: lmgame-Bench provides a reliable, versatile evaluation framework for LLMs, demonstrating transferability and unique capability assessment.

Abstract: Playing video games requires perception, memory, and planning, exactly the
faculties modern large language model (LLM) agents are expected to master. We
study the major challenges in using popular video games to evaluate modern LLMs
and find that directly dropping LLMs into games cannot make an effective
evaluation, for three reasons -- brittle vision perception, prompt sensitivity,
and potential data contamination. We introduce lmgame-Bench to turn games into
reliable evaluations. lmgame-Bench features a suite of platformer, puzzle, and
narrative games delivered through a unified Gym-style API and paired with
lightweight perception and memory scaffolds, and is designed to stabilize
prompt variance and remove contamination. Across 13 leading models, we show
lmgame-Bench is challenging while still separating models well. Correlation
analysis shows that every game probes a unique blend of capabilities often
tested in isolation elsewhere. More interestingly, performing reinforcement
learning on a single game from lmgame-Bench transfers both to unseen games and
to external planning tasks. Our evaluation code is available at
https://github.com/lmgame-org/GamingAgent/lmgame-bench.

</details>


### [504] [Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions](https://arxiv.org/pdf/2505.18492)
*Jialiang Sun, Yuzhi Tang, Ao Li, Chris J. Maddison, Kuldeep S. Meel*

Main category: cs.AI

TL;DR: The paper introduces the Enumerate-Conjecture-Prove (ECP) framework, a neuro-symbolic method combining LLMs and formal theorem proving to improve mathematical reasoning, achieving significant accuracy improvements on the ConstructiveBench dataset.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning is crucial for AI, but existing methods (LLMs and symbolic provers) have limitations—LLMs lack formal verification, and provers struggle with creativity. ECP aims to bridge this gap.

Method: The ECP framework integrates LLM-based enumeration and pattern-driven conjecturing with formal theorem proving, tested on the ConstructiveBench dataset of 3,431 math problems.

Result: ECP improves answer-construction accuracy from 14.54% (CoT baseline) to 45.06% (gpt-4.1-mini) and boosts proof-generation accuracy to 25.01% (DeepSeek-Prover-V2-7B) compared to 9.86% for symbolic-only methods.

Conclusion: ECP successfully combines neuro-symbolic approaches to enhance mathematical reasoning, demonstrating significant improvements in both answer construction and proof generation.

Abstract: Mathematical reasoning lies at the heart of artificial intelligence,
underpinning applications in education, program verification, and
research-level mathematical discovery. Mathematical competitions, in
particular, present two challenging problem types: theorem proving, which
requires rigorous proofs of stated conclusions, and answer construction, which
involves hypothesizing and formally verifying mathematical objects. Large
Language Models (LLMs) effectively generate creative candidate answers but
struggle with formal verification, while symbolic provers ensure rigor but
cannot efficiently handle creative conjecture generation. We introduce the
Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method
integrating LLM-based enumeration and pattern-driven conjecturing with formal
theorem proving. We present ConstructiveBench, a dataset of 3,431
answer-construction problems in various math competitions with verified Lean
formalizations. On the ConstructiveBench dataset, ECP improves the accuracy of
answer construction from a Chain-of-Thought (CoT) baseline of 14.54% to 45.06%
with the gpt-4.1-mini model. Moreover, combined with ECP's constructed answers,
the state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for
858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy
compared to 9.86% for symbolic-only baselines. Our code and dataset are
publicly available at https://github.com/JackSun200312/ECP.

</details>


### [505] [DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving](https://arxiv.org/pdf/2505.19381)
*Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Yunda Dong, Zongzheng Zhang, Xianda Guo, Hao Sun, Hao Zhao*

Main category: cs.AI

TL;DR: The paper introduces Diff-VLA, a hybrid sparse-dense diffusion policy for end-to-end autonomous driving, leveraging Vision-Language Models (VLM) to address challenges like BEV computation costs and sub-optimal decisions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing end-to-end autonomous driving methods, such as high computational costs and poor decision-making in complex scenarios.

Method: Proposes Diff-VLA, combining sparse diffusion for multi-modal behavior and VLM for improved trajectory guidance through agent-map-VLM interaction.

Result: Achieves superior performance in the Autonomous Grand Challenge 2025, with a score of 45.0 PDMS.

Conclusion: Diff-VLA effectively addresses key challenges in autonomous driving, demonstrating strong performance in real and synthetic scenarios.

Abstract: Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.

</details>


### [506] [Toward Scientific Reasoning in LLMs: Training from Expert Discussions via Reinforcement Learning](https://arxiv.org/pdf/2505.19501)
*Ming Yin, Yuanhao Qu, Ling Yang, Le Cong, Mengdi Wang*

Main category: cs.AI

TL;DR: Teaching LLMs scientific reasoning using expert discussions, focusing on genomics, with a new benchmark (Genome-Bench) and RL-based fine-tuning, improving performance by 15%.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' scientific reasoning by leveraging expert discussions as a learning signal, particularly in genomics.

Method: Developed an automated pipeline to extract trainable data from scientific forums, created Genome-Bench (3000+ QA pairs), and fine-tuned an LLM using RL with rule-based rewards.

Result: RL from scientific discussions improved model performance by over 15% on Genome-Bench, bridging the gap between open-source LLMs and expert reasoning.

Conclusion: First end-to-end pipeline for teaching LLMs scientific reasoning from discussions, with potential for broader scientific applications.

Abstract: We investigate how to teach large language models (LLMs) to perform
scientific reasoning by leveraging expert discussions as a learning signal.
Focusing on the genomics domain, we develop an automated pipeline to extract
trainable data and introduce Genome-Bench, a new benchmark constructed from
over a decade of scientific forum discussions on genome engineering. Our
pipeline transforms raw interactions into a reinforcement learning-friendly
multiple-choice questions format, supported by 3000+ high-quality
question-answer pairs spanning foundational biology, experimental
troubleshooting, tool usage, and beyond. We fine-tune an LLM using RL with a
rule-based reward signal derived from the synthetic MCQ dataset to enhance
domain-specific reasoning. Our results show that reinforcement learning from
scientific discussions improves model performance by over 15% compared to the
base model on Genome-Bench, narrowing the gap between open-source LLMs and
expert-level reasoning. To our knowledge, this is the first end-to-end pipeline
for teaching LLMs to reason from scientific discussions, with promising
potential for generalization across scientific domains beyond biology.

</details>


### [507] [MAPLE: A Mobile Agent with Persistent Finite State Machines for Structured Task Reasoning](https://arxiv.org/pdf/2505.23596)
*Linqiang Guo, Wei Liu, Yi Wen Heng, Tse-Hsun, Chen, Yang Wang*

Main category: cs.AI

TL;DR: MAPLE introduces a state-aware multi-agent framework using Finite State Machines (FSM) to improve mobile GUI agents' task execution, error recovery, and action accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing mobile GUI agents are reactive, lacking structured navigation flow, which limits context understanding and error recovery.

Method: MAPLE models UI screens as states and actions as transitions in an FSM, with specialized agents for planning, execution, verification, error recovery, and knowledge retention.

Result: MAPLE outperforms baselines, improving task success by 12%, recovery by 13.8%, and action accuracy by 6.5%.

Conclusion: Structured state modeling is crucial for GUI agents, and FSM can be a lightweight, model-agnostic memory layer for future architectures.

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [508] [MobCLIP: Learning General-purpose Geospatial Representation at Scale](https://arxiv.org/pdf/2506.01297)
*Ya Wen, Jixuan Cai, Qiyao Ma, Linyan Li, Xinhua Chen, Chris Webster, Yulun Zhou*

Main category: cs.AI

TL;DR: MobCLIP is a general-purpose location encoder integrating diverse data modalities for superior geospatial representation, outperforming state-of-the-art models by 35% on average.


<details>
  <summary>Details</summary>
Motivation: Current geospatial embedding methods lack versatility, limiting their utility across diverse tasks.

Method: MobCLIP uses a CLIP-based architecture to align POIs, remote sensing imagery, demographic statistics, and a mobility graph, tokenizing locations into grid cells.

Result: MobCLIP achieves a 35% average performance improvement, with significant gains in human-centric tasks (e.g., +260% in energy consumption prediction).

Conclusion: MobCLIP demonstrates scalable geospatial representation learning and is open-sourced for broader use.

Abstract: Representation learning of geospatial locations remains a core challenge in
achieving general geospatial intelligence. Current embedding methods often lack
versatility, limiting their utility across diverse tasks in both human and
natural domains. We present MobCLIP, the first nationwide general-purpose
location encoder, integrating an unprecedented diversity of data modalities
through effective and scalable multimodal fusion. Adopting a novel CLIP-based
architecture, our framework aligns 100M+ POIs, nationwide remote sensing
imagery, and structured demographic statistics with a billion-edge mobility
graph. By tokenizing spatial locations into grid cells inspired by Vision
Transformers, we establish a unified representation space bridging mobility
patterns and multimodal features. To rigorously evaluate the general-purpose
effectiveness of MobCLIP, we construct a benchmark dataset composed of 11
downstream prediction tasks across social, economic, and natural domains.
Experiments show that MobCLIP, with four input modalities and a compact
128-dimensional representation space, achieves significantly superior
general-purpose predictive performances than state-of-the-art models by an
average of 35%. Thanks to the effective integration of human-centric
modalities, the performance gain is particularly profound in human-centric
tasks, such as energy consumption (+260%), offline retail consumption amount
(+98%), and crime cases (+95%) predictions. Echoing LLM scaling laws, we
further demonstrate the scaling behavior in geospatial representation learning.
We open-source code and pretrained models at: github.com.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [509] [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/pdf/2506.02059)
*Ziwei Gong, Pengyuan Shi, Kaan Donbekci, Lin Ai, Run Chen, David Sasu, Zehui Wu, Julia Hirschberg*

Main category: cs.SD

TL;DR: The paper explores unsupervised learning, specifically contrastive learning and BYOL, to improve Speech Emotion Recognition (SER) in Low-Resource Languages (LRLs), achieving significant F1 score improvements.


<details>
  <summary>Details</summary>
Motivation: The scarcity of annotated data for Low-Resource Languages (LRLs) makes Speech Emotion Recognition (SER) challenging, motivating the use of unsupervised learning to enhance cross-lingual generalization.

Method: The study investigates contrastive learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised approaches for SER in LRLs.

Result: The methods achieved F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla, demonstrating effectiveness in LRLs.

Conclusion: The work lays a foundation for more inclusive, explainable, and robust emotion recognition systems for underrepresented languages, while highlighting challenges in low-resource SER.

Abstract: Speech Emotion Recognition (SER) has seen significant progress with deep
learning, yet remains challenging for Low-Resource Languages (LRLs) due to the
scarcity of annotated data. In this work, we explore unsupervised learning to
improve SER in low-resource settings. Specifically, we investigate contrastive
learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised
approaches to enhance cross-lingual generalization. Our methods achieve notable
F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,
demonstrating their effectiveness in LRLs. Additionally, we analyze model
behavior to provide insights on key factors influencing performance across
languages, and also highlighting challenges in low-resource SER. This work
provides a foundation for developing more inclusive, explainable, and robust
emotion recognition systems for underrepresented languages.

</details>


### [510] [SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction](https://arxiv.org/pdf/2506.02082)
*Saurabh Agrawal, Raj Gohil, Gopal Kumar Agrawal, Vikram C M, Kushal Verma*

Main category: cs.SD

TL;DR: A novel model, SALF-MOS, is introduced to predict MOS scores for speech quality assessment, addressing limitations of existing objective and subjective metrics.


<details>
  <summary>Details</summary>
Motivation: Current objective metrics for speech quality assessment are inadequate, and subjective MOS evaluation is time-consuming and labor-intensive.

Method: The SALF-MOS model uses stacked convolutions to extract latent features from audio samples for MOS prediction.

Result: The model achieves state-of-the-art performance using metrics like MSE, LCC, SRCC, and KTAU.

Conclusion: SALF-MOS offers a scalable and efficient alternative to traditional MOS evaluation, balancing accuracy and practicality.

Abstract: Speech quality assessment is a critical process in selecting text-to-speech
synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can
be done using objective metrics or subjective metrics. Although there are many
objective metrics like the Perceptual Evaluation of Speech Quality (PESQ),
Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time
Objective Intelligibility (STOI) but none of them is feasible in selecting the
best model. On the other hand subjective metric like Mean Opinion Score is
highly reliable but it requires a lot of manual efforts and are time-consuming.
To counter the issues in MOS Evaluation, we have developed a novel model,
Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a
small-sized, end-to-end, highly generalized and scalable model for predicting
MOS score on a scale of 5. We use the sequences of convolutions and stack them
to get the latent features of the audio samples to get the best
state-of-the-art results based on mean squared error (MSE), Linear Concordance
Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and
Kendall Rank Correlation Coefficient (KTAU).

</details>


### [511] [LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention](https://arxiv.org/pdf/2506.02083)
*Aditya Srinivas Menon, Raj Prakash Gohil, Kumud Tripathi, Pankaj Wasnik*

Main category: cs.SD

TL;DR: A novel disentanglement learning strategy improves speaker recognition by separating linguistic and speaker information, enhancing accuracy in multi-lingual settings.


<details>
  <summary>Details</summary>
Motivation: Speaker recognition struggles in multi-lingual contexts due to entangled linguistic and speaker information in embeddings. Disentangling these can boost accuracy.

Method: Proposes a joint learning strategy using prefix-tuned cross-attention to disentangle linguistic and speaker information, especially for language-switching speakers.

Result: The model generalizes across monolingual and multi-lingual settings, including unseen languages, and reduces the equal error rate.

Conclusion: The approach effectively separates language from speaker embeddings, improving recognition in diverse linguistic scenarios.

Abstract: Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.

</details>


### [512] [Trusted Fake Audio Detection Based on Dirichlet Distribution](https://arxiv.org/pdf/2506.02401)
*Chi Ding, Junxiao Xue, Cong Wang, Hao Zhou*

Main category: cs.SD

TL;DR: A fake audio detection method using Dirichlet distribution to model uncertainty and improve decision reliability, tested on ASVspoof datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of trustworthiness in existing fake audio detection models by incorporating uncertainty modeling.

Method: Generates evidence via neural network, models uncertainty with Dirichlet distribution, and combines probabilities with uncertainty estimates for final decisions.

Result: Demonstrated superior performance in accuracy, robustness, and trustworthiness on ASVspoof datasets.

Conclusion: The proposed approach enhances fake audio detection reliability by effectively modeling uncertainty.

Abstract: With the continuous development of deep learning-based speech conversion and
speech synthesis technologies, the cybersecurity problem posed by fake audio
has become increasingly serious. Previously proposed models for defending
against fake audio have attained remarkable performance. However, they all fall
short in modeling the trustworthiness of the decisions made by the models
themselves. Based on this, we put forward a plausible fake audio detection
approach based on the Dirichlet distribution with the aim of enhancing the
reliability of fake audio detection. Specifically, we first generate evidence
through a neural network. Uncertainty is then modeled using the Dirichlet
distribution. By modeling the belief distribution with the parameters of the
Dirichlet distribution, an estimate of uncertainty can be obtained for each
decision. Finally, the predicted probabilities and corresponding uncertainty
estimates are combined to form the final opinion. On the ASVspoof series
dataset (i.e., ASVspoof 2019 LA, ASVspoof 2021 LA, and DF), we conduct a number
of comparison experiments to verify the excellent performance of the proposed
model in terms of accuracy, robustness, and trustworthiness.

</details>


### [513] [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/pdf/2506.02085)
*Ajinkya Kulkarni, Sandipana Dowerah, Tanel Alumae, Mathew Magimai. -Doss*

Main category: cs.SD

TL;DR: A novel audio source tracing system combines deep metric multi-class N-pair loss, Real Emphasis and Fake Dispersion, a Conformer network, and ensemble score-embedding fusion to improve discriminative ability and robustness in tracing audio deepfake sources.


<details>
  <summary>Details</summary>
Motivation: The increasing realism of audio deepfakes necessitates not just detection but also tracing the source system, which current research overlooks.

Method: The system uses N-pair loss for discriminative ability, Real Emphasis and Fake Dispersion for robustness, a Conformer network for capturing audio dependencies, and ensemble score-embedding fusion for optimal performance.

Result: The method outperforms baselines in source tracing, evaluated using Frechet Distance and standard metrics.

Conclusion: The proposed system effectively addresses the challenge of tracing audio deepfake sources with superior performance.

Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced
AI. While current research focuses on discerning real speech from spoofed
speech, tracing the source system is equally crucial. This work proposes a
novel audio source tracing system combining deep metric multi-class N-pair loss
with Real Emphasis and Fake Dispersion framework, a Conformer classification
network, and ensemble score-embedding fusion. The N-pair loss improves
discriminative ability, while Real Emphasis and Fake Dispersion enhance
robustness by focusing on differentiating real and fake speech patterns. The
Conformer network captures both global and local dependencies in the audio
signal, crucial for source tracing. The proposed ensemble score-embedding
fusion shows an optimal trade-off between in-domain and out-of-domain source
tracing scenarios. We evaluate our method using Frechet Distance and standard
metrics, demonstrating superior performance in source tracing over the baseline
system.

</details>


### [514] [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/pdf/2506.02088)
*Alef Iury Siqueira Ferreira, Lucas Rafael Gris, Alexandre Ferro Filho, Lucas Ólives, Daniel Ribeiro, Luiz Fernando, Fernanda Lustosa, Rodrigo Tanaka, Frederico Santos de Oliveira, Arlindo Galvão Filho*

Main category: cs.SD

TL;DR: A robust system for Speech Emotion Recognition (SER) in naturalistic conditions, combining audio and text features with prosodic cues, achieves a Macro F1-score of 39.79%.


<details>
  <summary>Details</summary>
Motivation: Challenges in training SER models due to subtle emotions and unpredictable real-world audio conditions.

Method: Combines state-of-the-art audio models with text features, investigates F0 quantization, uses a pretrained audio tagging model, and employs an ensemble model.

Result: Achieved a Macro F1-score of 39.79% (42.20% on validation).

Conclusion: The methods show potential, with Graph Attention Networks proving effective for fusion techniques.

Abstract: Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
quantization and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.

</details>


### [515] [Comparison of spectrogram scaling in multi-label Music Genre Recognition](https://arxiv.org/pdf/2506.02091)
*Bartosz Karpiński, Cyryl Leszczyński*

Main category: cs.SD

TL;DR: The paper explores preprocessing methods and model training approaches for genre classification in music, using a manually labeled dataset of 18,000 entries.


<details>
  <summary>Details</summary>
Motivation: The rise in music availability and blurred genre boundaries necessitate better classification methods.

Method: Multiple preprocessing techniques and model training approaches are compared.

Result: Experiments are conducted using a custom dataset of over 18,000 entries.

Conclusion: The study aims to improve genre classification accuracy in modern, eclectic music.

Abstract: As the accessibility and ease-of-use of digital audio workstations increases,
so does the quantity of music available to the average listener; additionally,
differences between genres are not always well defined and can be abstract,
with widely varying combinations of genres across individual records. In this
article, multiple preprocessing methods and approaches to model training are
described and compared, accounting for the eclectic nature of today's albums. A
custom, manually labeled dataset of more than 18000 entries has been used to
perform the experiments.

</details>


### [516] [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/pdf/2506.02178)
*Thai-Binh Nguyen, Ngoc-Quan Pham, Alexander Waibel*

Main category: cs.SD

TL;DR: A novel audio-visual cocktail-party dataset is introduced to improve AVSR performance in noisy, real-world settings, reducing WER by 67%.


<details>
  <summary>Details</summary>
Motivation: Current AVSR models are optimized for idealized scenarios, ignoring silent-face segments and noisy conditions, limiting real-world applicability.

Method: Introduces a 1526-hour AVSR dataset with talking-face and silent-face segments, tested in extreme noise without explicit segmentation cues.

Result: Achieves a 67% reduction in WER, lowering it from 119% to 39.2% in extreme noise.

Conclusion: The new dataset and approach significantly enhance AVSR performance in challenging, real-world environments.

Abstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech
recognition in challenging environments, such as cocktail-party scenarios,
where relying solely on audio proves insufficient. However, current AVSR models
are often optimized for idealized scenarios with consistently active speakers,
overlooking the complexities of real-world settings that include both speaking
and silent facial segments. This study addresses this gap by introducing a
novel audio-visual cocktail-party dataset designed to benchmark current AVSR
systems and highlight the limitations of prior approaches in realistic noisy
conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising
both talking-face and silent-face segments, enabling significant performance
gains in cocktail-party environments. Our approach reduces WER by 67% relative
to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,
without relying on explicit segmentation cues.

</details>


### [517] [Breaking the Barriers of Text-Hungry and Audio-Deficient AI](https://arxiv.org/pdf/2506.02443)
*Hamidou Tembine, Issa Bamia, Massa NDong, Bakary Coulibaly, Oumar Issiaka Traore, Moussa Traore, Moussa Sanogo, Mamadou Eric Sangare, Salif Kante, Daryl Noupa Yongueng, Hafiz Tiomoko Ali, Malik Tiomoko, Frejus Laleye, Boualem Djehiche, Wesmanegda Elisee Dipama, Idris Baba Saje, Hammid Mohammed Ibrahim, Moumini Sanogo, Marie Coursel Nininahazwe, Abdul-Latif Siita, Haine Mhlongo, Teddy Nelvy Dieu Merci Kouka, Mariam Serine Jeridi, Mutiyamuogo Parfait Mupenge, Lekoueiry Dehah, Abdoul Aziz Bio Sidi Bouko, Wilfried Franceslas Zokoue, Odette Richette Sambila, Alina RS Mbango, Mady Diagouraga, Oumarou Moussa Sanoussi, Gizachew Dessalegn, Mohamed Lamine Samoura, Bintou Laetitia Audrey Coulibaly*

Main category: cs.SD

TL;DR: A textless audio-to-audio framework for underserved audio-literate populations, using novel architectures and MAST for high-fidelity speech generation without text.


<details>
  <summary>Details</summary>
Motivation: Addressing the bias toward written text in machine intelligence, which excludes 700M audio-literate people, especially in rural/remote areas.

Method: Introduces Audio-to-Audio translation models (spectrogram-, scalogram-, wavelet-, unit-based) and MAST for encoding audio features. Uses fractional diffusion with fractional Brownian motion for text-free speech generation.

Result: A scalable system learning directly from raw audio, even for unwritten/rarely digitized languages.

Conclusion: Shifts toward audio-native machine intelligence, expanding access to language technologies for excluded communities.

Abstract: While global linguistic diversity spans more than 7164 recognized languages,
the current dominant architecture of machine intelligence remains fundamentally
biased toward written text. This bias excludes over 700 million people
particularly in rural and remote regions who are audio-literate. In this work,
we introduce a fully textless, audio-to-audio machine intelligence framework
designed to serve this underserved population, and all the people who prefer
audio-efficiency. Our contributions include novel Audio-to-Audio translation
architectures that bypass text entirely, including spectrogram-, scalogram-,
wavelet-, and unit-based models. Central to our approach is the Multiscale
Audio-Semantic Transform (MAST), a representation that encodes tonal, prosodic,
speaker, and expressive features. We further integrate MAST into a fractional
diffusion of mean-field-type framework powered by fractional Brownian motion.
It enables the generation of high-fidelity, semantically consistent speech
without reliance on textual supervision. The result is a robust and scalable
system capable of learning directly from raw audio, even in languages that are
unwritten or rarely digitized. This work represents a fundamental shift toward
audio-native machine intelligence systems, expanding access to language
technologies for communities historically left out of the current machine
intelligence ecosystem.

</details>


### [518] [SOVA-Bench: Benchmarking the Speech Conversation Ability for LLM-based Voice Assistant](https://arxiv.org/pdf/2506.02457)
*Yixuan Hou, Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang*

Main category: cs.SD

TL;DR: SOVA-Bench is a systematic benchmark for evaluating speech LLMs, covering general knowledge, speech recognition, understanding, and acoustic quality.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of speech LLMs focus on semantic accuracy but neglect acoustic quality, despite its importance for vivid and spontaneous speech.

Method: Proposes SOVA-Bench, a framework comparing general knowledge, speech recognition, understanding, and generative abilities (semantic and acoustic) of speech LLMs.

Result: SOVA-Bench provides a comprehensive evaluation, addressing gaps in current benchmarking methods.

Conclusion: SOVA-Bench is a pioneering framework for assessing speech LLMs, guiding future advancements in voice interaction systems.

Abstract: Thanks to the steady progress of large language models (LLMs), speech
encoding algorithms and vocoder structure, recent advancements have enabled
generating speech response directly from a user instruction. However,
benchmarking the generated speech quality has been a neglected but critical
issue, considering the shift from the pursuit of semantic accuracy to vivid and
spontaneous speech flow. Previous evaluation focused on the
speech-understanding ability, lacking a quantification of acoustic quality. In
this paper, we propose Speech cOnversational Voice Assistant Benchmark
(SOVA-Bench), providing a comprehension comparison of the general knowledge,
speech recognition and understanding, along with both semantic and acoustic
generative ability between available speech LLMs. To the best of our knowledge,
SOVA-Bench is one of the most systematic evaluation frameworks for speech LLMs,
inspiring the direction of voice interaction systems.

</details>


### [519] [DnR-nonverbal: Cinematic Audio Source Separation Dataset Containing Non-Verbal Sounds](https://arxiv.org/pdf/2506.02499)
*Takuya Hasumi, Yusuke Fujita*

Main category: cs.SD

TL;DR: A new dataset, DnR-nonverbal, is introduced for cinematic audio source separation (CASS) to address issues with non-verbal sounds like laughter and screams, improving model performance on real movie audio.


<details>
  <summary>Details</summary>
Motivation: Existing CASS datasets lack acted-out voices and non-verbal sounds, leading to poor separation of emotionally heightened voices in real movie audio.

Method: The authors build the DnR-nonverbal dataset, including non-verbal sounds in the speech stem, and test it on synthetic and actual movie audio.

Result: Experiments show the current CASS model struggles with non-verbal sound extraction, and the new dataset effectively addresses this issue.

Conclusion: The DnR-nonverbal dataset improves CASS performance for non-verbal sounds and is publicly available.

Abstract: We propose a new dataset for cinematic audio source separation (CASS) that
handles non-verbal sounds. Existing CASS datasets only contain reading-style
sounds as a speech stem. These datasets differ from actual movie audio, which
is more likely to include acted-out voices. Consequently, models trained on
conventional datasets tend to have issues where emotionally heightened voices,
such as laughter and screams, are more easily separated as an effect, not
speech. To address this problem, we build a new dataset, DnR-nonverbal. The
proposed dataset includes non-verbal sounds like laughter and screams in the
speech stem. From the experiments, we reveal the issue of non-verbal sound
extraction by the current CASS model and show that our dataset can effectively
address the issue in the synthetic and actual movie audio. Our dataset is
available at https://zenodo.org/records/15470640.

</details>


### [520] [On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs](https://arxiv.org/pdf/2506.02545)
*Kemal Altwlkany, Amar Kuric, Emanuel Lacic*

Main category: cs.SD

TL;DR: The paper investigates biases in audio codecs (PSTN, VoIP, neural) for speech technology, revealing gender bias in PSTN and language bias in neural codecs.


<details>
  <summary>Details</summary>
Motivation: Address fairness and inclusivity in speech technology by identifying biases in audio coding mechanisms, which can impact user experience and societal equity.

Method: Analyzed speech quality of over 2 million multilingual audio files transcoded through PSTN, VoIP, and neural codecs.

Result: PSTN codecs exhibit strong gender bias, while neural codecs introduce language biases.

Conclusion: Audio codecs can perpetuate biases; addressing these is crucial for equitable speech technology.

Abstract: In recent years, there has been a growing focus on fairness and inclusivity
within speech technology, particularly in areas such as automatic speech
recognition and speech sentiment analysis. When audio is transcoded prior to
processing, as is the case in streaming or real-time applications, any inherent
bias in the coding mechanism may result in disparities. This not only affects
user experience but can also have broader societal implications by perpetuating
stereotypes and exclusion. Thus, it is important that audio coding mechanisms
are unbiased. In this work, we contribute towards the scarce research with
respect to language and gender biases of audio codecs. By analyzing the speech
quality of over 2 million multilingual audio files after transcoding through a
representative subset of codecs (PSTN, VoIP and neural), our results indicate
that PSTN codecs are strongly biased in terms of gender and that neural codecs
introduce language biases.

</details>


### [521] [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/pdf/2506.02590)
*Dimitrios Koutsianos, Stavros Zacharopoulos, Yannis Panagakis, Themos Stafylakis*

Main category: cs.SD

TL;DR: The paper explores source tracing in synthetic speech using speaker recognition methods, comparing classification-based and metric-learning approaches. ResNet performs competitively, even surpassing SSL-based systems, highlighting its potential for this task.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks robust solutions for source tracing in synthetic speech, focusing mainly on spoofing detection. This paper aims to bridge speaker recognition with audio forensics to address synthetic media manipulation.

Method: Two approaches are evaluated: classification-based and metric-learning, tested on the MLAADv5 benchmark using ResNet and SSL backbones.

Result: ResNet achieves competitive performance, matching or exceeding SSL-based systems, demonstrating its viability for source tracing.

Conclusion: The work shows ResNet's potential for source tracing and emphasizes the need to optimize SSL representations, offering new directions to combat synthetic media manipulation.

Abstract: This paper addresses source tracing in synthetic speech-identifying
generative systems behind manipulated audio via speaker recognition-inspired
pipelines. While prior work focuses on spoofing detection, source tracing lacks
robust solutions. We evaluate two approaches: classification-based and
metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet
and self-supervised learning (SSL) backbones. The results show that ResNet
achieves competitive performance with the metric learning approach, matching
and even exceeding SSL-based systems. Our work demonstrates ResNet's viability
for source tracing while underscoring the need to optimize SSL representations
for this task. Our work bridges speaker recognition methodologies with audio
forensic challenges, offering new directions for combating synthetic media
manipulation.

</details>


### [522] [Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm](https://arxiv.org/pdf/2506.02610)
*Zhaoyang Li, Jie Wang, XiaoXiao Li, Wangjie Li, Longjie Luo, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: Proposes OCDGALP, a method combining Graph Attention networks and Label Propagation for speaker diarization, reducing DER significantly.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering-based methods struggle with complex speaker embeddings and overlapping speech segments.

Method: Uses a Graph Attention Network to refine embeddings and a Label Propagation Algorithm for overlapping community detection.

Result: Achieves 15.94% DER on DIHARD-III without oracle VAD and 11.07% with oracle VAD.

Conclusion: OCDGALP outperforms traditional methods, offering a robust solution for speaker diarization.

Abstract: In speaker diarization, traditional clustering-based methods remain widely
used in real-world applications. However, these methods struggle with the
complex distribution of speaker embeddings and overlapping speech segments. To
address these limitations, we propose an Overlapping Community Detection method
based on Graph Attention networks and the Label Propagation Algorithm
(OCDGALP). The proposed framework comprises two key components: (1) a graph
attention network that refines speaker embeddings and node connections by
aggregating information from neighboring nodes, and (2) a label propagation
algorithm that assigns multiple community labels to each node, enabling
simultaneous clustering and overlapping community detection. Experimental
results show that the proposed method significantly reduces the Diarization
Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III
dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%
with oracle VAD.

</details>


### [523] [Cross-attention and Self-attention for Audio-visual Speaker Diarization in MISP-Meeting Challenge](https://arxiv.org/pdf/2506.02621)
*Zhaoyang Li, Haodong Zhou, Longjie Luo, Xiaoxiao Li, Yongxin Chen, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: CASA-Net, an embedding fusion method for AVSD, uses cross-attention and self-attention modules, with pseudo-label refinement and post-processing, achieving a 47.3% DER improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance audio-visual speaker diarization by capturing cross-modal interactions and contextual relationships.

Method: CASA-Net with cross-attention and self-attention modules, pseudo-label refinement, retraining, and post-processing (median filtering, overlap averaging).

Result: Achieved 8.18% DER, a 47.3% improvement over the baseline (15.52%).

Conclusion: CASA-Net effectively improves AVSD performance through cross-modal fusion and refined training strategies.

Abstract: This paper presents the system developed for Task 1 of the Multi-modal
Information-based Speech Processing (MISP) 2025 Challenge. We introduce
CASA-Net, an embedding fusion method designed for end-to-end audio-visual
speaker diarization (AVSD) systems. CASA-Net incorporates a cross-attention
(CA) module to effectively capture cross-modal interactions in audio-visual
signals and employs a self-attention (SA) module to learn contextual
relationships among audio-visual frames. To further enhance performance, we
adopt a training strategy that integrates pseudo-label refinement and
retraining, improving the accuracy of timestamp predictions. Additionally,
median filtering and overlap averaging are applied as post-processing
techniques to eliminate outliers and smooth prediction labels. Our system
achieved a diarization error rate (DER) of 8.18% on the evaluation set,
representing a relative improvement of 47.3% over the baseline DER of 15.52%.

</details>


### [524] [MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation](https://arxiv.org/pdf/2506.02661)
*Mingyang Huang, Peng Zhang, Bang Zhang*

Main category: cs.SD

TL;DR: MotionRAG-Diff combines Retrieval-Augmented Generation (RAG) with diffusion models to generate long-term, coherent, and music-aligned dance sequences, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for music-conditioned dance synthesis either lack creativity (motion graphs) or temporal coherence (diffusion models). MotionRAG-Diff aims to bridge these gaps.

Method: The framework integrates cross-modal contrastive learning, an optimized motion graph system, and a multi-condition diffusion model to align music and dance representations, retrieve realistic motion segments, and refine outputs.

Result: MotionRAG-Diff outperforms existing methods in motion quality, diversity, and synchronization accuracy with music.

Conclusion: The hybrid approach of MotionRAG-Diff sets a new standard for music-driven dance generation by combining retrieval-based fidelity with diffusion-based creativity.

Abstract: Generating long-term, coherent, and realistic music-conditioned dance
sequences remains a challenging task in human motion synthesis. Existing
approaches exhibit critical limitations: motion graph methods rely on fixed
template libraries, restricting creative generation; diffusion models, while
capable of producing novel motions, often lack temporal coherence and musical
alignment. To address these challenges, we propose $\textbf{MotionRAG-Diff}$, a
hybrid framework that integrates Retrieval-Augmented Generation (RAG) with
diffusion-based refinement to enable high-quality, musically coherent dance
generation for arbitrary long-term music inputs. Our method introduces three
core innovations: (1) A cross-modal contrastive learning architecture that
aligns heterogeneous music and dance representations in a shared latent space,
establishing unsupervised semantic correspondence without paired data; (2) An
optimized motion graph system for efficient retrieval and seamless
concatenation of motion segments, ensuring realism and temporal coherence
across long sequences; (3) A multi-condition diffusion model that jointly
conditions on raw music signals and contrastive features to enhance motion
quality and global synchronization. Extensive experiments demonstrate that
MotionRAG-Diff achieves state-of-the-art performance in motion quality,
diversity, and music-motion synchronization accuracy. This work establishes a
new paradigm for music-driven dance generation by synergizing retrieval-based
template fidelity with diffusion-based creative enhancement.

</details>


### [525] [UltrasonicSpheres: Localized, Multi-Channel Sound Spheres Using Off-the-Shelf Speakers and Earables](https://arxiv.org/pdf/2506.02715)
*Michael Küttner, Valeria Sitz, Kathrin Gerling, Michael Beigl, Tobias Röddiger*

Main category: cs.SD

TL;DR: UltrasonicSpheres is a system for location-specific audio delivery using wearable earphones that decode ultrasonic signals into audible sound, enabling personalized audio without pairing or tracking.


<details>
  <summary>Details</summary>
Motivation: To provide personalized, localized audio in public spaces without disrupting ambient sounds or requiring additional infrastructure.

Method: Uses single ultrasonic speakers to broadcast localized audio with multiple channels, each encoded on distinct ultrasonic frequencies. Wearable earphones demodulate selected streams.

Result: Users can hear localized audio (e.g., exhibit narrations) while remaining aware of ambient sounds, with spatial audio perception preserved.

Conclusion: UltrasonicSpheres offers an unobtrusive, infrastructure-free solution for personalized audio delivery in public spaces.

Abstract: We present a demo ofUltrasonicSpheres, a novel system for location-specific
audio delivery using wearable earphones that decode ultrasonic signals into
audible sound. Unlike conventional beamforming setups, UltrasonicSpheres relies
on single ultrasonic speakers to broadcast localized audio with multiple
channels, each encoded on a distinct ultrasonic carrier frequency. Users
wearing our acoustically transparent earphones can demodulate their selected
stream, such as exhibit narrations in a chosen language, while remaining fully
aware of ambient environmental sounds. The experience preserves spatial audio
perception, giving the impression that the sound originates directly from the
physical location of the source. This enables personalized, localized audio
without requiring pairing, tracking, or additional infrastructure. Importantly,
visitors not equipped with the earphones are unaffected, as the ultrasonic
signals are inaudible to the human ear. Our demo invites participants to
explore multiple co-located audio zones and experience how UltrasonicSpheres
supports unobtrusive delivery of personalized sound in public spaces.

</details>


### [526] [DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization](https://arxiv.org/pdf/2506.02858)
*Geonyoung Lee, Geonhee Han, Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: A training-free framework (DGMO) repurposes pretrained diffusion models for zero-shot language-queried audio source separation, achieving competitive performance without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Explore whether pretrained diffusion models, designed for audio generation, can inherently perform audio source separation without further training.

Method: Propose Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework refining spectrogram masks for precise separation.

Result: Achieves competitive performance in zero-shot LASS without task-specific supervision.

Conclusion: Expands diffusion models' applications beyond generation, establishing a new paradigm for zero-shot audio separation.

Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound
separation via natural language queries. While existing methods rely on
task-specific training, we explore whether pretrained diffusion models,
originally designed for audio generation, can inherently perform separation
without further training. In this study, we introduce a training-free framework
leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations,
we identify key limitations arising from modality-specific challenges.To
address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a
test-time optimization framework that refines spectrogram masks for precise,
input-aligned separation. Our approach effectively repurposes pretrained
diffusion models for source separation, achieving competitive performance
without task-specific supervision. This work expands the application of
diffusion models beyond generation, establishing a new paradigm for zero-shot
audio separation. The code is available at: https://wltschmrz.github.io/DGMO/

</details>


### [527] [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/pdf/2506.03099)
*Chetwin Low, Weimin Wang*

Main category: cs.SD

TL;DR: TalkingMachines transforms pretrained video models into real-time, audio-driven animators using an audio LLM and video foundation model.


<details>
  <summary>Details</summary>
Motivation: To enable natural conversational experiences by integrating audio and video generation.

Method: Adapts a DiT model for avatar generation, uses asymmetric knowledge distillation for infinite streaming, and optimizes inference with engineering tweaks.

Result: Efficient, real-time audio-driven character animation with high throughput and low latency.

Conclusion: TalkingMachines offers a scalable solution for interactive, audio-driven video generation.

Abstract: In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/

</details>


### [528] [Accelerating Autoregressive Speech Synthesis Inference With Speech Speculative Decoding](https://arxiv.org/pdf/2505.15380)
*Zijian Lin, Yang Zhang, Yougen Yuan, Yuming Yan, Jinjiang Liu, Zhiyong Wu, Pengfei Hu, Qun Yu*

Main category: cs.SD

TL;DR: SSD accelerates autoregressive speech synthesis by using a draft model for parallel verification, achieving 1.4x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Address latency in autoregressive speech synthesis due to sequential token prediction, critical for real-time applications.

Method: Uses a lightweight draft model to generate candidate tokens, verified in parallel by the target model via SSD framework.

Result: 1.4x speedup over conventional decoding while maintaining high fidelity and naturalness.

Conclusion: SSD effectively balances speed and quality, validated by subjective evaluations.

Abstract: Modern autoregressive speech synthesis models leveraging language models have
demonstrated remarkable performance. However, the sequential nature of next
token prediction in these models leads to significant latency, hindering their
deployment in scenarios where inference speed is critical. In this work, we
propose Speech Speculative Decoding (SSD), a novel framework for autoregressive
speech synthesis acceleration. Specifically, our method employs a lightweight
draft model to generate candidate token sequences, which are subsequently
verified in parallel by the target model using the proposed SSD framework.
Experimental results demonstrate that SSD achieves a significant speedup of
1.4x compared with conventional autoregressive decoding, while maintaining high
fidelity and naturalness. Subjective evaluations further validate the
effectiveness of SSD in preserving the perceptual quality of the target model
while accelerating inference.

</details>


### [529] [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/pdf/2505.24200)
*Qingzheng Wang, Jiancheng Sun, Yifan Peng, Shinji Watanabe*

Main category: cs.SD

TL;DR: The paper improves multilingual LID and ASR performance by adapting SFMs with strategies like frozen upstream training, partial fine-tuning, and low-rank adaptation, along with data augmentation and LID CTC loss. Results show a 14% LID accuracy boost and 30% ASR CER reduction.


<details>
  <summary>Details</summary>
Motivation: Existing SFMs struggle with limited resources during fine-tuning, prompting the need for better adaptation strategies.

Method: The study explores frozen upstream training, partial fine-tuning, low-rank adaptation, data augmentation, and LID CTC loss for regularization.

Result: Achieves 14% relative improvement in LID accuracy and 30% relative reduction in ASR CER, securing second place in the ML-SUPERB 2.0 Challenge.

Conclusion: The proposed strategies effectively enhance multilingual LID and ASR performance, demonstrating their potential for resource-limited settings.

Abstract: Multilingual speech processing with self-supervised or supervised pre-trained
Speech Foundation Models (SFM) has achieved strong performance on tasks like
Language Identification (LID) and Automatic Speech Recognition (ASR). However,
these models struggle with limited resources during fine-tuning. This paper
enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple
strategies for adapting SFMs, including frozen upstream training, partial
fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation
to mitigate performance gaps in few-shot settings and introduce LID
Connectionist Temporal Classification (CTC) loss for regularization. Our
approach achieves a 14% relative improvement in LID accuracy and a 30% relative
reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place
in the Interspeech 2025 ML-SUPERB 2.0 Challenge.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [530] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/pdf/2506.01959)
*Irmi Schneider*

Main category: cs.LG

TL;DR: The paper explores symmetry in loss functions beyond neural networks, analyzing four new cases and introducing a new symmetry measure.


<details>
  <summary>Details</summary>
Motivation: To understand symmetry's role in optimization problems and extend findings from neural networks to broader contexts.

Method: Investigates symmetry in real-valued loss functions across four cases: projective over finite fields, octahedral graphs, perfect matching, and particle attraction. Introduces a new symmetry measure.

Result: All observed critical points exhibit non-trivial symmetry, similar to neural networks. The new measure reveals additional symmetry structures.

Conclusion: Symmetry is pervasive in optimization problems, and the new measure enhances understanding of symmetry structures.

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [531] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/pdf/2506.01962)
*Xiaozhou Ye, Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: GNN-ADG, a novel method combining Graph Neural Networks and adversarial learning, improves cross-user generalization in HAR by modeling sensor spatial relationships and using cyclic training.


<details>
  <summary>Details</summary>
Motivation: Addressing cross-user variability in HAR systems due to differences in behavior, sensor placement, and data distribution.

Method: Leverages GNNs and adversarial learning to model spatial relationships between sensors, extracting three types of Anatomical Units (Interconnected, Analogous, Lateral) and fusing them into a unified graph with cyclic training.

Result: Achieves robust cross-user generalization by learning user-invariant features without requiring target user data during training.

Conclusion: GNN-ADG is a practical solution for real-world HAR applications, effectively generalizing to unseen users.

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [532] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/pdf/2506.01963)
*Andrew Kiruluta, Preethi Raju, Priscilla Burity*

Main category: cs.LG

TL;DR: A novel non-attention-based architecture for LLMs efficiently handles long contexts, avoiding quadratic overhead of Transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional Transformers struggle with long contexts due to quadratic memory and computation costs from self-attention.

Method: Combines State Space blocks, Multi-Resolution Convolution layers, a Recurrent Supervisor, and Retrieval-Augmented External Memory.

Result: Efficiently scales to hundreds of thousands or millions of tokens with linear complexity.

Conclusion: Proposes a scalable alternative to Transformers for long-context tasks without quadratic overhead.

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [533] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/pdf/2506.01964)
*Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song*

Main category: cs.LG

TL;DR: A data-driven approach enhances the gravity model for trip prediction by integrating diverse datasets and machine learning, significantly improving accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Traditional gravity models inadequately represent modern travel behavior, necessitating improved methods for transportation planning.

Method: Machine learning techniques are applied to extend the gravity model using geographical, economic, social, and travel data from Tennessee and New York.

Result: The enhanced model shows a 51.48% R-squared improvement, 63.59% MAE reduction, and 44.32% CPC increase.

Conclusion: Integrating diverse datasets and advanced algorithms provides more reliable tools for urban planners and policymakers.

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [534] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/pdf/2506.01965)
*Bonpagna Kann, Sandra Castellanos-Paez, Romain Rombourg, Philippe Lalanda*

Main category: cs.LG

TL;DR: TaskVAE is a replay-based continual learning framework using task-specific VAEs to generate synthetic exemplars, outperforming traditional methods in class-incremental settings, especially for Human Activity Recognition (HAR).


<details>
  <summary>Details</summary>
Motivation: Machine learning systems need continual adaptation to dynamic data environments, but balancing memory constraints and retaining old knowledge is challenging.

Method: TaskVAE employs task-specific Variational Autoencoders (VAEs) to generate synthetic exemplars from prior tasks, training the classifier alongside new data without requiring prior knowledge of total class count.

Result: TaskVAE outperforms experience replay methods, especially with limited data, and maintains robust performance as dataset size grows, with minimal memory footprint (60 samples per task).

Conclusion: TaskVAE balances memory constraints, task-specific generation, and long-term stability, making it a reliable solution for real-world applications like HAR.

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [535] [Matrix Is All You Need](https://arxiv.org/pdf/2506.01966)
*Yuzhou Zhu*

Main category: cs.LG

TL;DR: A unified matrix-order framework transforms convolutional, recurrent, and self-attention operations into sparse matrix multiplications, matching or outperforming native models while simplifying architecture design.


<details>
  <summary>Details</summary>
Motivation: To unify diverse neural architectures (CNN, RNN, Transformer) under a common mathematical framework and leverage hardware optimization.

Method: Represent CNN, RNN, and Transformer operations as sparse matrix multiplications (upper-triangular, lower-triangular, and tensor factorization).

Result: Empirical tests show performance matches or exceeds native models, with comparable or faster convergence.

Conclusion: The framework provides a rigorous mathematical basis for neural architectures and enables hardware-aware design.

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [536] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/pdf/2506.01967)
*Patrik Czakó, Gábor Kertész, Sándor Szénási*

Main category: cs.LG

TL;DR: The paper explores quantization challenges in LLMs, introduces a metric for quantization difficulty, and proposes a hybrid method combining channel-wise scaling and rotation to reduce errors.


<details>
  <summary>Details</summary>
Motivation: Quantization reduces LLM serving costs but faces challenges due to outliers increasing quantization error. The study aims to address these outliers and improve quantization accuracy.

Method: Investigates outliers' impact on quantization error, introduces a metric for quantization difficulty, and proposes a hybrid approach using channel-wise scaling and rotation.

Result: The hybrid method reduces quantization error by addressing outliers, supported by mathematical formulation.

Conclusion: The proposed approach improves quantization accuracy in LLMs, offering a practical solution for efficient serving.

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [537] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/pdf/2506.01968)
*Chang Liu, Jiangrong Shen, Xuming Ran, Mingkun Xu, Qi Xu, Yi Xu, Gang Pan*

Main category: cs.LG

TL;DR: A novel ANN-to-SNN conversion framework using error compensation learning improves accuracy and reduces latency, achieving 94.75% accuracy on CIFAR-10 with just two time steps.


<details>
  <summary>Details</summary>
Motivation: Deploying ANNs in resource-constrained environments is challenging due to high computational demands. SNNs offer energy efficiency but suffer from accuracy loss and latency during ANN-to-SNN conversion.

Method: Proposes a framework with learnable threshold clipping, dual-threshold neurons, and optimized membrane potential initialization to mitigate conversion errors.

Result: Achieves high precision and ultra-low latency, with 94.75% accuracy on CIFAR-10 using only two time steps.

Conclusion: The method enhances SNN practicality for low-power hardware, enabling efficient real-time processing.

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [538] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/pdf/2506.01970)
*Ruizhuo Song, Beiming Yuan*

Main category: cs.LG

TL;DR: The paper proposes the Johnny architecture and Spin-Transformer to improve AI's abstract reasoning in RPM tasks, addressing limitations of traditional models.


<details>
  <summary>Details</summary>
Motivation: To overcome the dependency of traditional RPM-solving models on option pool configurations and enhance abstract reasoning.

Method: Introduces Johnny (with Representation Extraction and Reasoning Modules) and Spin-Transformer (with a lightweight variant) for RPM tasks.

Result: Both Johnny and Spin-Transformer achieve superior performance in RPM tasks.

Conclusion: The proposed architectures offer innovative solutions for advancing AI's abstract reasoning capabilities.

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [539] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/pdf/2506.01974)
*Kanwal Aalijah*

Main category: cs.LG

TL;DR: AI-driven analysis of traffic and sentiment data to improve urban mobility planning.


<details>
  <summary>Details</summary>
Motivation: Address traffic congestion and commuter dissatisfaction in rapidly urbanizing cities.

Method: Combines real-time traffic data with geo-located sentiment analysis using AI models and exploratory data analysis.

Result: Identifies congestion hotspots and dissatisfaction zones, offering actionable recommendations.

Conclusion: AI can optimize traffic flow and enhance commuter experiences, applicable in the Middle East and beyond.

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [540] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/pdf/2506.01975)
*Jama Hussein Mohamud*

Main category: cs.LG

TL;DR: The paper explores why reusing pre-trained neural networks (Bob's success) works, showing it depends on task correlation and network/optimizer choices, even when tasks are uncorrelated.


<details>
  <summary>Details</summary>
Motivation: To understand the factors behind the empirical success of reusing pre-trained neural networks for different tasks.

Method: An experimental setup to study task correlation, network layers, and optimizer impact on reuse performance.

Result: Bob's success is tied to task correlation; even uncorrelated tasks can perform better than random due to network/optimizer choices. Lower layers are better for uncorrelated tasks.

Conclusion: Reusing pre-trained networks is effective when tasks are semantically correlated, and the optimal layers to reuse can indicate task/feature correlation.

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [541] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/pdf/2506.01976)
*Elham Kiyani, Venkatesh Ananchaperumal, Ahmad Peyvan, Mahendaran Uchimali, Gang Li, George Em Karniadakis*

Main category: cs.LG

TL;DR: The paper explores using DeepONets to predict crack propagation in materials, comparing vanilla and Fusion DeepONet variants. Fusion DeepONet shows better accuracy, especially in non-fracturing cases.


<details>
  <summary>Details</summary>
Motivation: Accurate crack propagation modeling is crucial for predicting material failure, especially with discontinuities like holes. Traditional methods rely on continuum assumptions, but discrete particle systems offer an alternative.

Method: Uses Constitutively Informed Particle Dynamics (CPD) data to train DeepONets (vanilla and Fusion variants) for predicting crack propagation. Cases include varying notch heights and hole radii.

Result: Fusion DeepONet outperforms vanilla DeepONet, particularly in non-fracturing scenarios. Fracture-driven cases remain challenging.

Conclusion: Fusion DeepONet shows promise for generalizing complex, geometry-varying crack propagation, though fracture scenarios need further improvement.

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [542] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/pdf/2506.01977)
*Wei Huang, Hanchen Wang, Dong Wen, Shaozhen Ma, Wenjie Zhang, Xuemin Lin*

Main category: cs.LG

TL;DR: GEDRanker is an unsupervised GAN-based framework for Graph Edit Distance (GED) computation, eliminating the need for costly ground-truth labels while achieving near-optimal results.


<details>
  <summary>Details</summary>
Motivation: Existing GED methods rely heavily on ground-truth supervision, which is expensive to obtain. GEDRanker addresses this by proposing an unsupervised approach.

Method: GEDRanker uses a matching-based GED solver and a preference-aware discriminator with an effective training strategy to generate high-quality node matching without ground-truth labels.

Result: Experiments show GEDRanker achieves near-optimal solution quality without supervision.

Conclusion: GEDRanker provides a practical, unsupervised alternative for GED computation, reducing reliance on costly labeled data.

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [543] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/pdf/2506.01983)
*Reyhaneh Keshavarzpour, Eghbal Mansoori*

Main category: cs.LG

TL;DR: The paper proposes an improved method for predicting antimicrobial peptides using a deep neural network and optimized coding, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Antimicrobial peptides are crucial as antibiotic alternatives in biomedical applications, necessitating better identification methods.

Method: Combines the best coding methods and uses a deep neural network to handle imbalanced datasets for prediction.

Result: The proposed method significantly improves accuracy and efficiency in predicting antimicrobial peptides.

Conclusion: The advancements in prediction and classification of antimicrobial peptides have high effectiveness and application in medicine and pharmaceuticals.

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [544] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/pdf/2506.01986)
*Selin Yildirim, Deming Chen*

Main category: cs.LG

TL;DR: SpecMemo enables efficient speculative decoding on memory-constrained devices by optimizing memory usage while maintaining performance gains.


<details>
  <summary>Details</summary>
Motivation: Deploying speculative decoding on memory-limited devices like mobile GPUs is challenging due to high memory demands.

Method: SpecMemo models memory footprint theoretically and balances memory allocation to retain speedup, with novel batched speculative decoding for distributed systems.

Result: Achieves 96% throughput of speculative decoding with 65% memory reduction on Titan RTX, and 2x speedup on distributed systems with 8x throughput increase for batch size 10.

Conclusion: SpecMemo democratizes LLM deployment in resource-constrained environments, offering faster and cheaper solutions with robust performance.

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [545] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/pdf/2506.01987)
*Runkang Yang, Peng Sun, Xinyi Shang, Yi Tang, Tao Lin*

Main category: cs.LG

TL;DR: The paper explores the dual roles of samples and targets in data, proposing a unified loss framework to analyze their impact on training efficiency and offering six key insights.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the neglect of target optimization in data-efficient learning, despite its importance alongside sample optimization.

Method: The authors categorize existing paradigms by sample-target interactions and propose a unified loss framework to evaluate their effects.

Result: Empirical studies reveal how target and sample variations influence training, yielding six insights for improved efficacy.

Conclusion: The work highlights the collective influence of samples and targets on training dynamics, providing actionable insights for better model training.

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [546] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/pdf/2506.01988)
*Akshat Dubey, Aleksandar Anžel, Georges Hattab*

Main category: cs.LG

TL;DR: A surrogate interpretability graph method improves global interpretability of random forest models in health informatics by visualizing feature interactions.


<details>
  <summary>Details</summary>
Motivation: Random forests, while robust, can obscure global feature interactions, reducing trust and compliance in health informatics.

Method: Uses graphs and mixed-integer linear programming to analyze and visualize feature interactions.

Result: Enhances interpretability by showing feature usage and dominant hierarchical interactions.

Conclusion: The surrogate interpretability graph is vital for improving trust and compliance in high-stakes domains.

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [547] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/pdf/2506.01989)
*Chengxi Li, Ming Xiao, Mikael Skoglund*

Main category: cs.LG

TL;DR: The paper proposes CRA-DL, a coded robust aggregation method for distributed learning to mitigate Byzantine attacks by ensuring coded gradients from honest devices are closer, enhancing robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Current distributed learning methods degrade under Byzantine attacks when local gradients vary significantly. The goal is to improve robustness and learning performance in such adversarial settings.

Method: CRA-DL redundantly allocates training data, computes coded gradients from honest devices, and aggregates them with Byzantine data using robust bounded aggregation rules.

Result: Theoretical analysis and numerical results show CRA-DL outperforms existing methods by ensuring coded gradients are closer, making aggregation more robust.

Conclusion: CRA-DL effectively mitigates Byzantine attacks in distributed learning, improving convergence and performance compared to current methods.

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [548] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/pdf/2506.02050)
*Qingyu Xiao, Yuanlin Chang, Youtian Du*

Main category: cs.LG

TL;DR: A decoupled hierarchical RL framework (DcHRL-SA) improves exploration in complex discrete state-space environments by combining state abstraction and a dual-level policy architecture.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effective agent exploration in RL, especially under partial observability in discrete state spaces.

Method: Uses a dual-level architecture (high-level RL actor and low-level rule-based policy) with state abstraction to cluster states and reduce dimensionality.

Result: Outperforms PPO in exploration efficiency, convergence speed, cumulative reward, and policy stability in grid environments.

Conclusion: DcHRL-SA offers a practical solution for large-scale exploration in discrete grids by integrating hierarchical policies and state abstraction.

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [549] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/pdf/2506.02053)
*Xu Zhang, Haoye Qiu, Weixuan Liang, Hui Liu, Junhui Hou, Yuheng Jia*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundations of ensemble clustering, deriving bounds for generalization error and excess risk, proving consistency under certain conditions, and proposing a new algorithm that outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored theoretical aspects of ensemble clustering, particularly generalization performance, and to bridge the gap between theory and practice.

Method: The study derives convergence rates for generalization error and excess risk, analyzes consistency conditions, and introduces a weighted ensemble clustering approach to minimize empirical error.

Result: The proposed algorithm achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets in terms of NMI, ARI, and Purity.

Conclusion: Ensemble clustering benefits from minimizing bias and maximizing diversity among base clusterings, with theoretical insights leading to a practical, high-performing algorithm.

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [550] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/pdf/2506.02062)
*Malik A. Altayar, Muhyeeddin Alqaraleh, Mowafaq Salem Alzboon, Wesam T. Almagharbeh*

Main category: cs.LG

TL;DR: The study explored correlations between fingerprint patterns and ABO blood groups but found no significant link, suggesting independent traits. Future research with larger datasets and advanced methods is recommended.


<details>
  <summary>Details</summary>
Motivation: To investigate potential correlations between fingerprint patterns and ABO blood groups for forensic and healthcare applications, addressing the limitations of costly and time-consuming biometric systems.

Method: Analyzed 200 individuals, categorizing fingerprints into loops, whorls, and arches, and recording blood groups. Used chi-square and Pearson correlation tests for statistical analysis.

Result: Loops were the most common fingerprint pattern, and O+ was the most prevalent blood group. No significant correlation between fingerprint patterns and blood groups was found (p > 0.05).

Conclusion: The study found no significant correlation but emphasized the need for future research with larger, diverse populations and advanced methods like machine learning to enhance forensic identification.

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [551] [Real-time respiratory motion forecasting with online learning of recurrent neural networks for accurate targeting in externally guided radiotherapy](https://arxiv.org/pdf/2403.01607)
*Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, Ritu Bhusal Chhatkuli*

Main category: cs.LG

TL;DR: The study evaluates efficient online RNN algorithms (UORO, SnAp-1, DNI) for predicting respiratory motion in lung radiotherapy, achieving competitive accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: To address treatment system latencies in lung radiotherapy by improving real-time respiratory motion prediction using resource-efficient RNN methods.

Method: Proposed efficient implementations for SnAp-1 and DNI, tested on 3D marker data resampled at different rates, and compared performance with existing methods.

Result: SnAp-1 and UORO showed the lowest errors at varying sampling rates, while DNI had the fastest inference time.

Conclusion: Online RNNs can achieve high accuracy in respiratory motion prediction with minimal training, offering practical solutions for radiotherapy.

Abstract: In lung radiotherapy, infrared cameras can track reflective objects on the
chest to estimate tumor motion due to breathing, but treatment system latencies
hinder radiation beam precision. Real-time recurrent learning (RTRL) is a
potential solution that can learn patterns within non-stationary respiratory
data but has high complexity. This study assesses the capabilities of
resource-efficient online RNN algorithms, namely unbiased online recurrent
optimization (UORO), sparse-1 step approximation (SnAp-1), and decoupled neural
interfaces (DNI) to forecast respiratory motion during radiotherapy treatment
accurately. We use time series containing the 3D positions of external markers
on the chest of healthy subjects. We propose efficient implementations for
SnAp-1 and DNI that compress the influence and immediate Jacobian matrices and
accurately update the linear coefficients used in credit assignment estimation,
respectively. Data was originally sampled at 10Hz; we resampled it at 3.33Hz
and 30Hz to analyze the effect of the sampling rate on performance. We use
UORO, SnAp-1, and DNI to forecast each marker's 3D position with horizons
h<=2.1s (the time interval in advance for which the prediction is made) and
compare them with RTRL, least mean squares, kernel support vector regression,
and linear regression. RNNs trained online achieved similar or better accuracy
than most previous works using larger training databases and deep learning,
even though we used only the first minute of each sequence to predict motion
within that exact sequence. SnAp-1 had the lowest normalized root mean square
errors (nRMSEs) averaged over the horizon values considered, equal to 0.335 and
0.157, at 3.33Hz and 10.0Hz, respectively. Similarly, UORO had the lowest nRMSE
at 30Hz, equal to 0.086. DNI's inference time (6.8ms per time step at 30Hz,
Intel Core i7-13700 CPU) was the lowest among the RNN methods.

</details>


### [552] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/pdf/2506.02065)
*Shriraj P. Sawant, Krishna P. Miyapuram*

Main category: cs.LG

TL;DR: The paper introduces Elastic Weight Generative Networks (EWGN) to address catastrophic forgetting in continual learning by enabling dynamic weight generation and context switching.


<details>
  <summary>Details</summary>
Motivation: To mitigate catastrophic forgetting in neural networks when learning multiple tasks, inspired by human intelligence's ability to retain diverse tasks.

Method: Proposes EWGN, an architecture with an additional network that dynamically generates weights for the primary network, enabling input-dependent context switching. Evaluated on MNIST and fashion-MNIST datasets.

Result: Analyzed retention of learned tasks in Fully Connected Networks, CNNs, and EWGN with SGD and Elastic Weight Consolidation.

Conclusion: EWGN's dynamic weight generation and context-switching capability can enhance continual learning performance.

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [553] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/pdf/2506.02070)
*Peter Holderrieth, Ezra Erives*

Main category: cs.LG

TL;DR: The abstract introduces diffusion and flow-based models as leading generative AI methods, covering theory and applications across various data types.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive introduction to diffusion and flow-based models, bridging theory and practice for students and practitioners.

Method: The notes start with differential equations and progress to advanced topics like flow matching, score matching, and classifier-free guidance.

Result: A self-contained resource for understanding modern generative AI models, particularly for images and videos.

Conclusion: The notes and course are ideal for anyone seeking a principled grasp of generative AI's theory and applications.

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [554] [Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors](https://arxiv.org/pdf/2405.14250)
*Emile Pierret, Bruno Galerne*

Main category: cs.LG

TL;DR: The paper theoretically analyzes diffusion models for Gaussian data distributions, deriving analytical solutions for backward SDEs and flow ODEs, and computes exact Wasserstein errors for numerical schemes.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of diffusion models and their numerical implementations, particularly for Gaussian data distributions, by controlling initialization, truncation, discretization, and score approximation errors.

Method: Derives analytical solutions for backward SDEs and flow ODEs, proving they are Gaussian processes, and computes exact Wasserstein errors for numerical schemes.

Result: All solutions and discretizations are Gaussian processes, and exact Wasserstein errors are computed, enabling direct monitoring of convergence in data space.

Conclusion: The study provides theoretical insights and practical tools for analyzing diffusion models, improving convergence monitoring beyond empirical metrics like Inception features.

Abstract: Diffusion or score-based models recently showed high performance in image
generation. They rely on a forward and a backward stochastic differential
equations (SDE). The sampling of a data distribution is achieved by numerically
solving the backward SDE or its associated flow ODE. Studying the convergence
of these models necessitates to control four different types of error: the
initialization error, the truncation error, the discretization error and the
score approximation. In this paper, we theoretically study the behavior of
diffusion models and their numerical implementation when the data distribution
is Gaussian. Our first contribution is to derive the analytical solutions of
the backward SDE and the probability flow ODE and to prove that these solutions
and their discretizations are all Gaussian processes. Our second contribution
is to compute the exact Wasserstein errors between the target and the
numerically sampled distributions for any numerical scheme. This allows us to
monitor convergence directly in the data space, while experimental works limit
their empirical analysis to Inception features. An implementation of our code
is available online.

</details>


### [555] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/pdf/2506.02077)
*Yoonjun Cho, Soeun Kim, Dongjae Jeon, Kyelim Lee, Beomsoo Lee, Albert No*

Main category: cs.LG

TL;DR: ODLRI improves LLM compression by balancing quantization and low-rank approximation, reducing errors and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods prioritize one component over the other, leading to suboptimal decompositions.

Method: Introduces Outlier-Driven Low-Rank Initialization (ODLRI) to capture activation-sensitive weights, balancing quantization and low-rank approximation.

Result: Reduces activation-aware error, minimizes quantization scale, and improves perplexity and zero-shot accuracy in low-bit settings.

Conclusion: ODLRI enhances joint optimization for LLM compression, leveraging both components effectively.

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [556] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/pdf/2506.02079)
*Xuefeng Jiang, Tian Wen, Zhiqin Yang, Lvhua Wu, Yufeng Chen, Sheng Sun, Yuwei Wang, Min Liu*

Main category: cs.LG

TL;DR: A two-stage optimization framework, MaskedOptim, addresses label noise in federated learning by detecting noisy clients and correcting their labels, improving model robustness.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with noisy client annotations, degrading model performance. Addressing this noise is crucial for reliable training.

Method: MaskedOptim detects noisy clients in stage one and corrects their labels in stage two using backpropagation and geometric median aggregation.

Result: The framework shows robustness across datasets and improves data quality for noisy clients.

Conclusion: MaskedOptim effectively mitigates label noise in federated learning, enhancing model performance and data quality.

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [557] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/pdf/2506.02081)
*Chihiro Maru, Shoetsu Sato*

Main category: cs.LG

TL;DR: The paper proposes a retrieval-augmented time series foundation model (RATFM) to improve anomaly detection by incorporating test-time adaptation examples, avoiding costly retraining.


<details>
  <summary>Details</summary>
Motivation: Existing time series foundation models lack the ability to interpret or utilize examples for adaptation, limiting their performance in anomaly detection.

Method: Introduces RATFM, which enhances pretrained models by enabling them to incorporate test-time examples without domain-specific fine-tuning.

Result: RATFM achieves performance comparable to in-domain fine-tuning on the UCR Anomaly Archive dataset.

Conclusion: The proposed RATFM effectively addresses the limitation of time series models in utilizing examples, offering a practical solution for anomaly detection.

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [558] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/pdf/2506.02084)
*Nikolaos Gkorgkolis, Nikolaos Kougioulis, MingXue Wang, Bora Caglayan, Andrea Tonon, Dario Simionato, Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: The paper introduces TCS, a framework for generating realistic temporal causal data, addressing limitations of existing methods and proposing a Min-max optimization for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods rely on synthetic data that poorly mirrors real-world scenarios, especially in temporal setups.

Method: TCS involves three phases: estimating lagged causal structure, approximating functional dependencies, and learning noise distribution, with a Min-max optimization for evaluation.

Result: TCS generates realistic temporal causal data, validated through experiments on real, semi-synthetic, and synthetic datasets.

Conclusion: TCS advances realistic temporal causal data generation, though challenges remain in sampling such data.

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [559] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/pdf/2506.01789)
*Genta Indra Winata, David Anugraha, Emmy Liu, Alham Fikri Aji, Shou-Yi Hung, Aditya Parashar, Patrick Amadeus Irawan, Ruochen Zhang, Zheng-Xin Yong, Jan Christian Blaise Cruz, Niklas Muennighoff, Seungone Kim, Hanyang Zhao, Sudipta Kar, Kezia Erina Suryoraharjo, M. Farid Adilazuarda, En-Shiun Annie Lee, Ayu Purwarianti, Derry Tanti Wijaya, Monojit Choudhury*

Main category: cs.LG

TL;DR: The paper proposes DataRubrics, a framework for systematic evaluation of dataset quality, addressing gaps in current practices like lack of originality, diversity, and transparency in dataset submissions.


<details>
  <summary>Details</summary>
Motivation: Current dataset submissions often lack quality control, originality, and transparency, with existing tools being descriptive rather than evaluative. The paper aims to improve dataset review processes.

Method: Introduces DataRubrics, a rubric-based framework for dataset quality assessment, leveraging LLM-based evaluation and synthetic data generation methods.

Result: DataRubrics provides a reproducible, scalable solution for dataset quality evaluation, supported by released code for LLM-based assessments.

Conclusion: The paper calls for higher standards in dataset submissions and reviews, advocating for systematic evaluation tools like DataRubrics to enhance data-centric research.

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


### [560] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/pdf/2506.02089)
*Zeng Wang, Minghao Shao, Rupesh Karn, Jitendra Bhandari, Likhitha Mankali, Ramesh Karri, Ozgur Sinanoglu, Muhammad Shafique, Johann Knechtel*

Main category: cs.LG

TL;DR: SALAD uses machine unlearning to address security risks in LLM-aided hardware design, like data contamination and IP leakage, without full retraining.


<details>
  <summary>Details</summary>
Motivation: LLMs in hardware design automation pose data security risks (e.g., IP leakage, malicious code).

Method: Introduces SALAD, leveraging machine unlearning to selectively remove contaminated or sensitive data from LLMs.

Result: Case studies show SALAD effectively reduces security risks in LLM-aided hardware design.

Conclusion: Machine unlearning is a viable solution for mitigating security threats in LLM-driven hardware automation.

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [561] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/pdf/2506.02092)
*Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli*

Main category: cs.LG

TL;DR: A novel unsupervised concept-based model (LCBM) improves interpretability and performance in image classification by modeling concepts as random variables in a Bernoulli latent space.


<details>
  <summary>Details</summary>
Motivation: To enhance trustworthiness in deep neural networks by improving decision-making transparency and reducing reliance on human supervision.

Method: Introduces LCBM, which uses a Bernoulli latent space for concept representation, requiring fewer concepts without performance loss.

Result: LCBM outperforms unsupervised models in generalization and nearly matches black-box models, with concepts more intuitive for humans.

Conclusion: LCBM balances interpretability and performance, aligning concepts with human understanding while maintaining model transparency.

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [562] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/pdf/2506.02096)
*Zijian Wu, Jinjie Ni, Xiangyan Liu, Zichen Liu, Hang Yan, Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: SynthRL is a scalable pipeline for synthesizing challenging RL data to improve vision-language models (VLMs) trained with RLVR, showing significant gains in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the effectiveness of RLVR-trained VLMs by synthesizing additional verifiable and challenging data for reasoning-oriented training.

Method: SynthRL involves three stages: selecting seed questions, augmenting them into harder variants while preserving answers, and verifying correctness and difficulty.

Result: SynthRL synthesized 3.3K additional questions from 8K seeds, improving model performance on five out-of-domain benchmarks, especially on challenging samples.

Conclusion: SynthRL effectively scales data for RL training, enhancing reasoning capabilities in VLMs, particularly for complex tasks.

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [563] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/pdf/2506.02098)
*Miran Özdogan, Gilad Landau, Gereon Elvers, Dulhan Jayalath, Pratik Somaiya, Francesco Mantegna, Mark Woolrich, Oiwi Parker Jones*

Main category: cs.LG

TL;DR: LibriBrain is the largest single-subject MEG dataset for speech decoding, offering 50+ hours of recordings, tools for deep learning integration, and baseline results for decoding tasks.


<details>
  <summary>Details</summary>
Motivation: To enable exploration of neural representations at an unprecedented scale and advance speech decoding methodologies for clinical brain-computer interfaces.

Method: The dataset includes high-quality MEG recordings and annotations from a single participant listening to spoken English, with Python tools for deep learning integration and standardized data splits.

Result: Baseline experiments show significant decoding performance improvements with more training data, underscoring the value of large within-subject datasets.

Conclusion: LibriBrain aims to empower the research community to advance speech decoding and accelerate the development of clinical brain-computer interfaces.

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [564] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/pdf/2506.02134)
*Rishi Raj Sahoo, Rucha Bhalchandra Joshi, Subhankar Mishra*

Main category: cs.LG

TL;DR: The paper introduces ReconXF, a graph reconstruction attack method that works with public explanations and privatized auxiliary data, outperforming existing methods under differential privacy.


<details>
  <summary>Details</summary>
Motivation: GNNs lack transparency, and while explainability methods help, they pose privacy risks by enabling graph structure reconstruction. Existing attacks fail with privatized data.

Method: Proposes ReconXF, which adapts explanation-based frameworks with denoising to handle differential privacy noise and exploit structural signals in explanations.

Result: ReconXF outperforms state-of-the-art methods in privatized settings, improving AUC and average precision for graph reconstruction.

Conclusion: Public explanations combined with denoising can recover graph structure despite privacy protections, highlighting a trade-off between explainability and privacy.

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [565] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/pdf/2506.02138)
*Yarden Bakish, Itamar Zimerman, Hila Chefer, Lior Wolf*

Main category: cs.LG

TL;DR: The paper introduces a reformulated LRP method for Transformer explainability, addressing the oversight of positional encoding in existing methods, and demonstrates superior performance in vision and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LRP-based methods for Transformer explainability ignore positional encoding, leading to violations of conservation properties and loss of relevance for structural features.

Method: The authors reformulate the input space as position-token pairs and propose specialized LRP rules for various positional encoding methods (Rotary, Learnable, Absolute PE).

Result: Experiments show the method outperforms state-of-the-art in vision and NLP explainability tasks, including with zero-shot models like LLaMA 3.

Conclusion: The proposed method effectively incorporates positional encoding into Transformer explainability, improving performance and theoretical grounding.

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [566] [Z-Error Loss for Training Neural Networks](https://arxiv.org/pdf/2506.02154)
*Guillaume Godin*

Main category: cs.LG

TL;DR: Z-Error Loss minimizes outlier impact in neural networks by masking out-of-distribution data points during training.


<details>
  <summary>Details</summary>
Motivation: Outliers degrade model performance by propagating erroneous gradients, necessitating a robust solution.

Method: Uses batch-level statistics to detect and exclude anomalous samples, focusing learning on true data structure.

Result: Improves model performance and generalization by reducing outlier influence.

Conclusion: Z-Error Loss is a robust, adaptive method for outlier mitigation, aiding data curation and cleaning.

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [567] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/pdf/2506.02168)
*Hrushikesh N. Mhaskar, Efstratios Tsoukanis, Ameya D. Jagtap*

Main category: cs.LG

TL;DR: The paper reviews function approximation in machine learning, highlights gaps between theory and practice, and introduces a novel method for approximating functions on unknown manifolds.


<details>
  <summary>Details</summary>
Motivation: To bridge the disconnect between approximation theory and machine learning practice, particularly in generalization and handling unknown manifolds.

Method: Reviews existing approaches (neural networks, kernel methods) and introduces a new method for function approximation on manifolds without learning specific features.

Result: Identifies shortcomings in current frameworks and proposes a solution for better generalization on unseen data.

Conclusion: The paper emphasizes the need for integrating approximation theory into machine learning and presents a promising direction for future research.

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [568] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/pdf/2506.02200)
*Shiangyi Lin, Hui Lan, Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: A novel approach addresses bias in IV estimators for high-dimensional treatments by incorporating instruments during representation learning, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional IV estimators struggle with high-dimensional treatments due to limited instruments, leading to biased results when using unsupervised dimension reduction.

Method: Proposes instrument-informed representation learning to construct treatment representations, ensuring identification of outcome-predictive directions.

Result: Theoretically and empirically shows improved performance over conventional two-stage methods that ignore instrument information.

Conclusion: The new framework effectively handles high-dimensional endogenous variables with limited instruments, reducing bias and improving prediction.

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [569] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/pdf/2506.02203)
*Navid NaderiAlizadeh, Darian Salehi, Xinran Liu, Soheil Kolouri*

Main category: cs.LG

TL;DR: The paper introduces a constrained learning approach to optimize slicing directions for Sliced Wasserstein (SW) distances, improving efficiency and reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Identifying informative slicing directions for SW distances is challenging, often requiring many slices and increasing computational costs.

Method: A constrained learning approach is proposed, where 1D transport plans approximate the optimal plan in the original space. Gradient-based primal-dual training is used for slicer parameters.

Result: The method effectively learns informative slicing directions, demonstrated on foundation models for images, point clouds, and protein sequences.

Conclusion: The constrained learning approach enhances SW distance performance by optimizing slicing directions, with practical applications in high-dimensional data.

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [570] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/pdf/2506.02205)
*Yuliang Gu, Hongpeng Cao, Marco Caccamo, Naira Hovakimyan*

Main category: cs.LG

TL;DR: BC-EvoCEM enhances CEM in MBRL by using Bregman centroids for better diversity and convergence, improving performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: CEM's unimodal sampling often converges prematurely in multimodal landscapes, limiting its effectiveness in MBRL.

Method: BC-EvoCEM aggregates information and controls diversity using Bregman centroids, updating low-contributing workers via trust-region sampling.

Result: Empirical tests show BC-EvoCEM improves convergence and solution quality in synthetic benchmarks, navigation tasks, and MBRL pipelines.

Conclusion: BC-EvoCEM is a lightweight, effective upgrade to CEM, enhancing its performance without significant overhead.

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [571] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/pdf/2506.02208)
*Hongling Xu, Qi Zhu, Heyuan Deng, Jinpeng Li, Lu Hou, Yasheng Wang, Lifeng Shang, Ruifeng Xu, Fei Mi*

Main category: cs.LG

TL;DR: KDRL combines knowledge distillation (KD) and reinforcement learning (RL) to enhance LLM reasoning, outperforming standalone methods by balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of RL (low sample efficiency) and KD (poor generalization) by integrating both paradigms.

Method: KDRL unifies KD and RL, optimizing via policy gradient to minimize RKL divergence and maximize rule-based rewards.

Result: Outperforms GRPO and KD baselines on reasoning benchmarks, balancing performance and token efficiency.

Conclusion: Integrating KD and RL is effective and efficient for training reasoning LLMs.

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [572] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/pdf/2506.02210)
*Pu, Yi, Tianlang Chen, Yifan Yang, Sara Achour*

Main category: cs.LG

TL;DR: The paper introduces ExPrune, a dynamic pruning algorithm that exploits symmetry-induced redundancy in neural networks (NNs) to improve efficiency, achieving significant FLOPs reduction with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Neural networks are resource-intensive, and existing methods like pruning or quantizing weights address redundancy but overlook symmetry. This work aims to exploit symmetry for efficient inference.

Method: The authors formalize symmetry using exchangeability, identify redundancy in exchangeable values, and develop ExPrune, a dynamic pruning algorithm. They also provide a neuron-level instantiation of ExPrune for ReLU activations.

Result: ExPrune reduces FLOPs by 10.98--39.05% with negligible to 1% accuracy drop across various models. It also complements static pruning, offering additional FLOPs reductions.

Conclusion: ExPrune effectively exploits symmetry-induced redundancy, providing a scalable solution for efficient NN inference while maintaining accuracy.

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [573] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/pdf/2506.02213)
*Kahn Rhrissorrakrai, Kathleen E. Hamilton, Prerana Bangalore Parthsarathy, Aldo Guzman-Saenz, Tyler Alban, Filippo Utro, Laxmi Parida*

Main category: cs.LG

TL;DR: Quantum ensemble models are explored for small data learning in healthcare, showing promise in simulations and hardware tests.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning from small datasets in healthcare and life sciences using quantum computing.

Method: Constructed quantum ensembles for binary classification, tested on synthetic and gene expression data, using up to 56 qubits.

Result: Demonstrated how quantum embedding affects performance and how to extract features for effective learning.

Conclusion: Quantum ensembles offer potential for small data problems in healthcare, aiding future research in the field.

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [574] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/pdf/2506.02242)
*Yihong Tang, Ao Qu, Xujing Yu, Weipeng Deng, Jun Ma, Jinhua Zhao, Lijun Sun*

Main category: cs.LG

TL;DR: Proposes a Multimodal Large Language Model (MLLM)-based approach for automated, interpretable hypothesis inference in urban and transportation research, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in traditional workflows: human bias, limited interpretability, and underutilized unstructured data.

Method: Uses MLLMs to generate safety-relevant questions for street view images, extracts interpretable embeddings, and applies regression models for hypothesis testing.

Result: Outperforms pretrained deep learning models on Manhattan street segments while maintaining interpretability.

Conclusion: UrbanX offers a scalable, interpretable framework for urban scientific discovery, enhancing trustworthiness for policy applications.

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [575] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/pdf/2506.02243)
*Tamara Cucumides, Floris Geerts*

Main category: cs.LG

TL;DR: auGraph is a framework for task-aware graph augmentation in tabular and relational data, outperforming schema-based methods by enhancing graphs with task-relevant attributes.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based methods underutilize non-key attributes in tabular and relational data, limiting predictive performance.

Method: auGraph introduces task-aware graph augmentation, promoting relevant attributes into nodes while preserving the original schema.

Result: Empirically, auGraph outperforms schema-based and heuristic graph construction methods.

Conclusion: auGraph effectively enhances graph structures for better learning in relational and tabular prediction tasks.

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [576] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/pdf/2506.02255)
*Asha Ramanujam, Adam Elyoumi, Hao Chen, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Shraman Pal, Dimitri J. Papageorgiou, Can Li*

Main category: cs.LG

TL;DR: SafeOR-Gym is a benchmark suite for safe reinforcement learning (RL) in complex, real-world operations research (OR) tasks, addressing gaps in existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing safe RL benchmarks lack relevance to high-stakes domains with structured constraints and industrial complexity, limiting progress in critical areas like energy and supply chains.

Method: SafeOR-Gym introduces nine OR environments with realistic planning, scheduling, and control problems, integrating with the CMDP interface.

Result: Evaluation shows varied performance of safe RL algorithms, with some tasks tractable and others exposing current limitations.

Conclusion: SafeOR-Gym serves as a practical testbed to advance safe RL research for real-world decision-making, with open-source availability.

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [577] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/pdf/2506.02256)
*Yi Xiao, Harshit Sharma, Sawinder Kaur, Dessa Bergen-Cico, Asif Salekin*

Main category: cs.LG

TL;DR: HHISS is a domain generalization approach for stress detection that removes person-specific differences, improving model accuracy across new individuals and environments. It uses sub-network pruning and continuous labels to focus on shared features and prevent overfitting. Tested on diverse datasets, HHISS outperforms baselines, proving practical for real-world use.


<details>
  <summary>Details</summary>
Motivation: Stress detection models struggle with individual differences and varying health conditions, limiting generalization. HHISS aims to address this by identifying consistent stress patterns across diverse populations, especially in sensitive groups like those with opioid use disorder (OUD).

Method: HHISS employs person-wise sub-network pruning intersection to isolate shared stress features and uses continuous labels to avoid overfitting. It is evaluated on seven datasets, including lab, controlled, and real-world settings.

Result: HHISS consistently outperforms state-of-the-art baseline methods across all datasets, demonstrating robustness and scalability for real-world applications.

Conclusion: HHISS is a feasible and scalable solution for mobile stress sensing, particularly in sensitive scenarios like OUD rehabilitation, due to its ability to generalize across diverse data.

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [578] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/pdf/2506.02269)
*YuQing Xie, Tess Smidt*

Main category: cs.LG

TL;DR: The paper investigates the optimization challenges of equivariant neural networks, showing that equivariance constraints can hinder learning global minima and suggesting relaxation as a solution.


<details>
  <summary>Details</summary>
Motivation: To understand why equivariant networks face optimization difficulties and whether these are due to fundamental obstacles or hyperparameter tuning.

Method: Theoretical analysis of loss landscape geometry, focusing on permutation representations and comparing equivariant networks to unconstrained MLPs.

Result: Equivariance constraints can prevent learning global minima; relaxing constraints or changing group representations can help.

Conclusion: Key insights include the importance of viewing networks in unconstrained spaces, the complex structure of equivariant networks, and the need to rethink group representations for effective relaxation.

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [579] [Latent Stochastic Interpolants](https://arxiv.org/pdf/2506.02276)
*Saurabh Singh, Dmitry Lagun*

Main category: cs.LG

TL;DR: Latent Stochastic Interpolants (LSI) extend the SI framework to latent variable models, enabling joint optimization of encoder, decoder, and latent SI models with a continuous-time ELBO objective.


<details>
  <summary>Details</summary>
Motivation: Existing SI frameworks require direct sample access, limiting their use in latent variable models. LSI addresses this gap.

Method: LSI introduces a continuous-time ELBO objective for joint optimization of encoder, decoder, and latent SI models, avoiding high-dimensional computational costs.

Result: LSI effectively learns latent representations and transforms arbitrary priors into aggregated posteriors, outperforming normal diffusion models.

Conclusion: LSI combines generative flexibility with computational efficiency, validated on ImageNet generation.

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [580] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/pdf/2506.02281)
*Qinsi Wang, Jinghan Ke, Hancheng Ye, Yueqian Lin, Yuzhe Fu, Jianyi Zhang, Kurt Keutzer, Chenfeng Xu, Yiran Chen*

Main category: cs.LG

TL;DR: GAIN-RL improves training efficiency in LLMs by dynamically selecting data based on angle concentration, achieving 2.5x faster training and better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Current RFT paradigms for LLMs are sample-inefficient due to redundant data exposure, and heuristic difficulty metrics neglect intrinsic learning signals.

Method: Proposes GAIN-RL, a framework using angle concentration to dynamically select training data for impactful gradient updates.

Result: GAIN-RL achieves 2.5x faster training and better performance with half the data compared to vanilla GRPO.

Conclusion: GAIN-RL enhances training efficiency and data utilization in LLMs by leveraging intrinsic learning signals.

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [581] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/pdf/2506.02285)
*Aaron Defazio*

Main category: cs.LG

TL;DR: The paper identifies and fixes an issue in LLM training where gradient norms spike due to weight decay, normalization layers, and learning rate schedules.


<details>
  <summary>Details</summary>
Motivation: To address the unintended interaction causing gradient norm spikes in late-stage LLM training.

Method: Proposes a simple correction to mitigate the interaction.

Result: The fix resolves the gradient norm issue and lowers training loss.

Conclusion: A straightforward solution improves training stability and performance.

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [582] [On Universality Classes of Equivariant Networks](https://arxiv.org/pdf/2506.02293)
*Marco Pacini, Gabriele Santin, Bruno Lepri, Shubhendu Trivedi*

Main category: cs.LG

TL;DR: The paper explores the universality of equivariant neural networks, showing that separation power alone doesn't fully capture expressivity, and characterizes their approximation ability under various symmetry conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the approximation power of equivariant neural networks beyond separation constraints, as universality remains underexplored compared to separation power.

Method: Analyzes the universality classes of shallow invariant networks and extends insights to equivariant networks via projection. Identifies conditions for universality failure and success.

Result: Shows that models with identical separation power may differ in approximation ability. Provides conditions for universality failure and identifies settings where shallow models achieve separation-constrained universality.

Conclusion: Universality of equivariant networks depends on structural properties of the symmetry group, with limitations in cases like permutation symmetry.

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [583] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/pdf/2506.02300)
*Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet*

Main category: cs.LG

TL;DR: The paper proposes a framework to visualize how deep neural networks transition inputs between classes using gradient-based motion magnification in the steerable pyramid domain.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods fail to show how models distinguish classes or what changes would transition inputs between categories.

Method: The framework decomposes images using the Complex Steerable Pyramid, computes class-conditional gradients, and amplifies them to reveal transition paths.

Result: The method produces semantically meaningful, coherent morphs that expose the model's sensitive directions and decision boundaries.

Conclusion: The approach offers a novel, interpretable way to understand neural classifiers' internal representations.

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [584] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/pdf/2506.02306)
*Aditya Gorla, Ryan Wang, Zhengtong Liu, Ulzee An, Sriram Sankararaman*

Main category: cs.LG

TL;DR: CACTI is a masked autoencoding method for tabular data imputation that uses missingness patterns and contextual info, outperforming state-of-the-art methods by 7.8% average R² gain.


<details>
  <summary>Details</summary>
Motivation: To improve tabular data imputation by leveraging missingness patterns and contextual information (e.g., column names, text descriptions) for better feature dependence representation.

Method: Uses a median truncated copy masking strategy to learn from missingness patterns and incorporates semantic relationships between features.

Result: Achieves an average R² gain of 7.8% over the next best method, with improvements under various missingness conditions (13.4%, 6.1%, 5.3%).

Conclusion: Leveraging dataset-specific contextual info and missingness patterns significantly enhances imputation performance.

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [585] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/pdf/2506.02308)
*Xiaojun Shan, Qi Cao, Xing Han, Haofei Yu, Paul Pu Liang*

Main category: cs.LG

TL;DR: Increasing instruction-tuning tasks doesn't always improve performance; grouping tasks by multimodal interactions (e.g., shared or unique information) is more effective. MINT, a new task-grouping strategy, outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of scaling instruction fine-tuning by simply adding more tasks, focusing instead on how tasks interact across modalities.

Method: Introduces MINT, a task-grouping strategy based on multimodal interaction types (e.g., redundant shared information, unique modality selection, synergistic fusion).

Result: MINT outperforms existing task-grouping baselines, balancing generalization and specialization effectively.

Conclusion: Task grouping by multimodal interactions is key for effective instruction tuning, with MINT proving superior to traditional scaling approaches.

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [586] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/pdf/2506.02315)
*D. Isaiah Harp, Joshua Ott, John Alora, Dylan Asmar*

Main category: cs.LG

TL;DR: The paper proposes eliminating traditional test points in flight testing by using a high-fidelity digital model and machine learning to create a reduced-order model (ROM) that adapts to actual flight conditions, improving accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: Traditional test points and tolerances are problematic as they rely on strict adherence to pre-specified conditions, which can be invalidated by pilot deviations. The goal is to improve accuracy and adaptability in flight testing.

Method: A high-fidelity digital model is used to generate a ROM via machine learning. The ROM predicts outcomes for any flight conditions and updates with new data. Gaussian Process Regression is applied to refine the ROM using actual flight test data.

Result: The method successfully creates a refined ROM from unconstrained flight test data, demonstrated with T-38C data, and assesses compliance with MIL-STD-1797B for longitudinal dynamics.

Conclusion: The "point-less" architecture eliminates rigid test points, offering a flexible and adaptive approach to flight testing, validated by real-world data.

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [587] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/pdf/2506.02318)
*Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang*

Main category: cs.LG

TL;DR: The paper provides the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices, addressing gaps in existing theoretical works focused on uniform rate matrices.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical works on discrete diffusion models have largely ignored absorbing rate matrices, despite empirical evidence of their superior performance in generation quality. This work fills the gap by analyzing convergence and error bounds for such models.

Method: The authors derive an upper bound on the KL divergence of the forward process, introduce a surrogate initialization distribution, and establish convergence guarantees for sampling methods under absorbing rate matrices. New technical tools are developed to handle challenges unique to absorbing rate matrices.

Result: The analysis shows improved convergence rates for absorbing rate matrices compared to uniform ones, with guarantees even without early stopping under certain assumptions.

Conclusion: This work advances the theoretical understanding of discrete diffusion models with absorbing rate matrices, providing foundational tools and guarantees for future research and applications.

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [588] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/pdf/2506.02323)
*Aleix Boquet-Pujadas, Pol del Aguila Pla, Michael Unser*

Main category: cs.LG

TL;DR: The paper proposes an optimization method for estimating probability densities in unevenly sampled multidimensional problems, using splines and nuclear norm regularization for stability and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of estimating probability densities in unevenly sampled multidimensional contexts, particularly considering detector sensitivity.

Method: Uses splines on a grid for computational efficiency and flexible boundaries, regularizing the Hessian of the spline with the nuclear norm to promote sparsity.

Result: The method is spatially adaptive and stable against regularization parameter choices, tested on standard densities with provided software.

Conclusion: Demonstrates effectiveness with a PET rebinning application, showcasing practical utility.

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [589] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/pdf/2506.02337)
*Adrienne M. Propp, Jonas A. Actor, Elise Walker, Houman Owhadi, Nathaniel Trask, Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: A novel method for learning Dirichlet-to-Neumann maps on graphs using Gaussian processes, enforcing conservation laws and providing uncertainty quantification under data scarcity.


<details>
  <summary>Details</summary>
Motivation: To couple multiphysics simulations across subdomains while ensuring continuity and conservation, especially in data-limited scenarios.

Method: Combines discrete exterior calculus and nonlinear optimal recovery with Gaussian processes, optimizing kernel complexity and enforcing conservation laws.

Result: High accuracy and well-calibrated uncertainty estimates in applications like subsurface fracture networks and arterial blood flow, even with scarce data.

Conclusion: The method is robust for scientific applications requiring reliable uncertainty quantification and conservation law enforcement under limited data.

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [590] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/pdf/2506.02355)
*Andre He, Daniel Fried, Sean Welleck*

Main category: cs.LG

TL;DR: The paper identifies a bias in GRPO for multi-sample tasks like theorem proving, introduces an unlikeliness reward to address it, and shows improved performance and diversity.


<details>
  <summary>Details</summary>
Motivation: GRPO's bias towards probable solutions neglects rare correct proofs, limiting its effectiveness for tasks like theorem proving.

Method: Introduces an unlikeliness reward to reinforce rare correct solutions and increases PPO epochs to mitigate bias.

Result: The unlikeliness reward improves pass@N metrics and sample diversity, outperforming standard GRPO.

Conclusion: The revised method achieves competitive performance, offering a practical solution for training theorem provers with RL.

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [591] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/pdf/2506.02357)
*Ram Potham*

Main category: cs.LG

TL;DR: A lightweight benchmark method evaluates LLM agents' adherence to safety principles in a grid world, testing controllability under conflicting goals.


<details>
  <summary>Details</summary>
Motivation: To verify AI agent behavior and detect control deficiencies early, ensuring safety principles are prioritized over operational goals.

Method: Uses a simple grid world to test if LLM agents uphold a high-level safety principle (e.g., avoiding hazards) despite conflicting task instructions.

Result: Demonstrates feasibility and provides preliminary insights into agent behavior under principle conflict, contributing empirical evidence for controllability assessment.

Conclusion: Evaluating hierarchical principle adherence is key to building governable AI systems, serving as an early controllability benchmark.

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [592] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/pdf/2506.02370)
*Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang*

Main category: cs.LG

TL;DR: HiSo is a federated fine-tuning method for LLMs that uses Hessian-informed zeroth-order optimization and scalar-only communication, improving convergence speed and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing ZO-SGD methods in FL for LLMs suffer from slow convergence due to high variance in gradient estimation, while Hessian-based optimization is challenging to integrate without compromising communication efficiency.

Method: HiSo decouples dimension-free communication from ZO-SGD, enabling Hessian-informed optimization while maintaining minimal communication costs.

Result: HiSo achieves faster convergence rates, especially when the global Hessian has a low effective rank, and outperforms existing ZO-based FL methods in experiments.

Conclusion: HiSo successfully addresses the limitations of ZO-SGD in FL by integrating Hessian information, offering a scalable and efficient solution for federated fine-tuning of LLMs.

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [593] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/pdf/2506.02371)
*Haoye Lu, Darren Lo, Yaoliang Yu*

Main category: cs.LG

TL;DR: SFBD flow, a continuous variant of SFBD, eliminates manual coordination in iterative denoising, outperforming baselines while addressing privacy concerns in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns arise from diffusion models memorizing sensitive training data; SFBD aims to mitigate this by training on corrupted data with limited clean samples.

Method: Reinterpret SFBD as an alternating projection algorithm, introduce SFBD flow to remove alternating steps, and connect it to consistency constraint-based methods.

Result: Online SFBD, the practical instantiation, consistently outperforms strong baselines across benchmarks.

Conclusion: SFBD flow offers a more efficient and effective solution for privacy-preserving diffusion models.

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [594] [Multi-agent Markov Entanglement](https://arxiv.org/pdf/2506.02385)
*Shuze Chen, Tianyi Peng*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundation of value decomposition in multi-agent RL, introducing the concept of 'Markov entanglement' to measure and bound decomposition errors.


<details>
  <summary>Details</summary>
Motivation: To understand why value decomposition works effectively in multi-agent systems, which lacks theoretical justification.

Method: Analyzes multi-agent MDPs, defining 'Markov entanglement' to measure transition matrix entanglement and bounds decomposition error.

Result: Proves that a class of index policies is weakly entangled with sublinear error scaling, and shows how to estimate entanglement practically.

Conclusion: Markov entanglement provides a theoretical and practical tool for assessing value decomposition quality in multi-agent RL.

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [595] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/pdf/2506.02386)
*Jie Bian, Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: The paper introduces a novel algorithm for best feasible arm identification in linear bandits, achieving exponential decay in error probability matching the theoretical lower bound.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in establishing the exact exponential rate of error probability decay in fixed-budget best feasible arm identification, even for simple settings like Gaussian noise $K$-armed bandits.

Method: A posterior sampling framework with a game-based sampling rule involving min-learner and max-learner, tailored for fixed-budget constraints.

Result: The algorithm guarantees exponential decay in error probability, matching the theoretical lower bound, and outperforms benchmarks in empirical evaluations.

Conclusion: The proposed method effectively addresses the gap, offering theoretical and empirical validation for its superior performance in fixed-budget settings.

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [596] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/pdf/2506.02389)
*Chamara Madarasingha, Nasrin Sohrabi, Zahir Tari*

Main category: cs.LG

TL;DR: LLMPred enhances LLM-based time-series prediction by converting sequences to text and using decomposition and prompt-processing, achieving competitive results with smaller LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' effectiveness in handling complex, noisy, and multivariate time-series data, which remains underexplored.

Method: LLMPred converts time-series to text, uses sequence decomposition for univariate data, and a prompt-processing strategy for multivariate data.

Result: LLMPred performs competitively or better than state-of-the-art baselines with smaller LLMs like Llama 2 7B and GPT-4o-mini.

Conclusion: LLMPred's key components are validated as effective, demonstrating its potential for time-series prediction.

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [597] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/pdf/2506.02390)
*Qin Xie, Qinghua Zhang, Shuyin Xia, Xinran Zhou, Guoyin Wang*

Main category: cs.LG

TL;DR: GAdaBoost, a two-stage framework combining data granulation and adaptive boosting, improves AdaBoost's efficiency and robustness under label noise in multiclass tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing AdaBoost's inefficiency and sensitivity to label noise in multiclass classification.

Method: Proposes GAdaBoost with two stages: data granulation (granular-ball generation) and adaptive boosting (granular ball-based SAMME).

Result: Outperforms existing methods in robustness and efficiency on noisy datasets.

Conclusion: GAdaBoost effectively extends AdaBoost and SAMME, offering better performance under noisy conditions.

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [598] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/pdf/2506.02392)
*Yuanyao Chen, Rongsheng Chen, Fu Luo, Zhenkun Wang*

Main category: cs.LG

TL;DR: A novel LLM-driven framework enhances NCO scalability for VRPs by projecting training-testing distributions, achieving superior performance on large-scale TSP and CVRP without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing NCO methods degrade on large-scale VRPs due to distributional shifts between training and testing data.

Method: Introduces an LLM-driven framework to learn a projection between distributions, applied during inference without retraining.

Result: Enables a model trained on 100-node instances to perform well on TSP and CVRP with up to 100K nodes.

Conclusion: The framework effectively addresses scalability issues in NCO for VRPs, demonstrating strong performance on large-scale problems.

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [599] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/pdf/2506.02406)
*Renat Sergazinov, Jing Wu, Shao-An Yin*

Main category: cs.LG

TL;DR: Random Fourier features, repurposed for tabular deep learning, improve training efficiency and performance by preconditioning networks with a stable kernel.


<details>
  <summary>Details</summary>
Motivation: Addressing shortcomings in tabular deep learning pipelines revealed by NTK analysis, the paper explores random Fourier features as a parameter-free, architecture-agnostic solution.

Method: Projects inputs into a fixed feature space using sine and cosine projections with random frequencies, eliminating the need for normalization or embeddings.

Result: The mapping bounds the NTK spectrum, shortens optimization trajectories, and accelerates training, leading to faster convergence and better performance.

Conclusion: Random Fourier pre-processing is a plug-and-play enhancement for tabular deep learning, supported by theory and empirical results.

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [600] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/pdf/2506.02415)
*Karthikeyan Vaiapury*

Main category: cs.LG

TL;DR: AERO is a novel optimization framework inspired by Judo's redirection principle, offering stability and adaptability in dynamic, uncertain systems.


<details>
  <summary>Details</summary>
Motivation: Existing optimization methods struggle with stability and adaptability in nonlinear, uncertain environments.

Method: AERO uses adversarial correction, energy conservation, and disturbance-aware learning, guided by 15 axioms, to redirect gradients and manage learning energy.

Result: Applied to solar energy forecasting, AERO improves accuracy, reliability, and adaptability, especially in noisy conditions.

Conclusion: AERO presents a promising new approach to optimization, balancing theory and practicality.

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [601] [Weak Supervision for Real World Graphs](https://arxiv.org/pdf/2506.02451)
*Pratheeksha Nair, Reihaneh Rabbany*

Main category: cs.LG

TL;DR: WSNET is a weakly supervised graph contrastive learning framework that leverages noisy or indirect signals for robust node classification, outperforming state-of-the-art methods by up to 15% in F1 score.


<details>
  <summary>Details</summary>
Motivation: Addressing label scarcity and noise in real-world graphs (e.g., human trafficking detection, misinformation monitoring) by utilizing weak signals for learning.

Method: Proposes WSNET, integrating graph structure, node features, and noisy supervision via a contrastive objective tailored for weakly labeled data.

Result: Outperforms state-of-the-art methods by up to 15% in F1 score across real-world datasets and synthetic benchmarks.

Conclusion: Contrastive learning under weak supervision is effective, and imperfect labels can be valuable in graph-based settings.

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [602] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/pdf/2506.02475)
*Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun*

Main category: cs.LG

TL;DR: The paper introduces Nonlinear RNNs, analyzes their advantages/limitations, and proposes Comba, a novel variant with state/output feedback corrections, achieving superior performance in language/vision tasks.


<details>
  <summary>Details</summary>
Motivation: To improve recurrent memory management in sequence modeling by introducing nonlinear recursive structures and leveraging closed-loop control theory.

Method: Proposes Comba, a Nonlinear RNN with scalar-plus-low-rank state transition, state/output feedback corrections, and a hardware-efficient parallel kernel.

Result: Comba outperforms in language and vision modeling, demonstrated by training 340M/1.3B parameter models.

Conclusion: Comba is a computationally efficient and high-performing Nonlinear RNN variant, validated on large-scale tasks.

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [603] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/pdf/2506.02504)
*Xingyu Chen, Bokun Wang, Ming Yang, Quanqi Hu, Qihang Lin, Tianbao Yang*

Main category: cs.LG

TL;DR: The paper addresses non-smooth FCCO problems, proposing stochastic momentum methods to improve iteration complexity from $O(1/\epsilon^6)$ to $O(1/\epsilon^5)$ and demonstrating effectiveness in constrained optimization tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for non-smooth FCCO suffer from high iteration complexity and unsuitability for deep learning, motivating the development of more efficient algorithms.

Method: Proposes stochastic momentum methods tailored for non-smooth FCCO, with provable convergence guarantees.

Result: Achieves a new state-of-the-art iteration complexity of $O(1/\epsilon^5)$ and validates effectiveness on constrained optimization tasks.

Conclusion: The proposed methods outperform existing approaches, offering improved efficiency and applicability to deep learning and constrained optimization problems.

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [604] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/pdf/2506.02539)
*Thong Q. Nguyen, Shubhang Desai, Yash Jain, Tanvir Aumi, Vishal Chowdhary*

Main category: cs.LG

TL;DR: VerificAgent improves CUA performance by managing memory with expert-curated knowledge, iterative refinement, and human fact-checking, achieving a 111.1% boost in success rate.


<details>
  <summary>Details</summary>
Motivation: Unchecked memory accumulation in CUAs can degrade performance due to spurious or hallucinated learnings, especially in domain-specific workflows like productivity software.

Method: VerificAgent uses expert-curated domain knowledge, iterative memory refinement during training, and post-hoc human fact-checking to manage memory.

Result: VerificAgent achieves a 111.1% relative improvement in success rate over baseline CUA on OSWorld productivity tasks.

Conclusion: The VerificAgent framework effectively addresses memory degradation in CUAs, enhancing performance in domain-specific tasks.

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [605] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/pdf/2506.02541)
*Minsung Kim, Nakyeong Yang, Kyomin Jung*

Main category: cs.LG

TL;DR: The paper introduces PUBG, a novel unlearning method for Large Vision-Language Models (LVLMs) to address privacy risks and undesirable post-unlearning behaviors like hallucinations or excessive refusal.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods for LVLMs often neglect the quality of post-unlearning responses, leading to issues like degeneracy or privacy leakage.

Method: The paper proposes PUBG, a method that guides post-unlearning behavior toward a desirable output distribution, ensuring privacy-preserving yet informative responses.

Result: Experiments show PUBG mitigates Unlearning Aftermaths, generating visually grounded and informative responses without privacy violations.

Conclusion: PUBG effectively addresses privacy risks and response quality in LVLM unlearning, outperforming existing methods.

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [606] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/pdf/2506.02542)
*Niklas Kormann, Masoud Ramuz, Zeeshan Nisar, Nadine S. Schaadt, Hendrik Annuth, Benjamin Doerr, Friedrich Feuerhake, Thomas Lampert, Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: HIEGNet, a novel heterogeneous GNN, improves glomeruli classification by integrating immune cell context, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: GNNs excel in histopathology but lack exploration in glomeruli health classification, a key nephropathology task with unique graph construction challenges.

Method: Proposes a pipeline for graph construction and HIEGNet, a heterogeneous GNN integrating glomeruli and immune cells.

Result: HIEGNet outperforms baselines and generalizes best between patients on kidney transplant WSIs.

Conclusion: HIEGNet effectively classifies glomeruli by leveraging immune environment, with public implementation available.

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [607] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/pdf/2506.02553)
*Shenghua He, Tian Xia, Xuan Zhou, Hui Wei*

Main category: cs.LG

TL;DR: The paper addresses the Zero-Reward Assumption in LLMs, showing that response-level rewards can unbiasedly estimate token-level rewards, justifying methods like PPO and proposing a new algorithm, TRePO.


<details>
  <summary>Details</summary>
Motivation: The Zero-Reward Assumption in LLMs makes token-level rewards impractical, motivating a theoretical solution to leverage response-level rewards.

Method: Introduces the Trajectory Policy Gradient Theorem, proving unbiased estimation of token-level rewards using response-level rewards, and proposes TRePO.

Result: Theoretical justification for response-level reward methods and a new efficient algorithm, TRePO, are presented.

Conclusion: The work enables practical LLM fine-tuning by simplifying reward modeling and introduces TRePO for broader applicability.

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [608] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/pdf/2506.02563)
*Roie Reshef, Kfir Yehuda Levy*

Main category: cs.LG

TL;DR: A novel noise-cancellation mechanism for achieving Differential Privacy in Federated Learning under partial-participation, ensuring optimal performance without compromising convergence or efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in extending Differential Privacy to partial-participation Federated Learning, where prior methods failed.

Method: Introduces a noise-cancellation mechanism analyzed within the Stochastic Convex Optimization framework.

Result: Achieves optimal performance for both homogeneous and heterogeneous data distributions.

Conclusion: Expands DP applicability in FL, providing an efficient solution for privacy-preserving learning in distributed systems with partial participation.

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [609] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/pdf/2506.02572)
*Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li*

Main category: cs.LG

TL;DR: HATA (Hash-Aware Top-$k$ Attention) integrates learning-to-hash into Top-$k$ attention, improving efficiency and accuracy in LLM inference.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of attention modules in LLMs by balancing efficiency and accuracy in Top-$k$ attention methods.

Method: Uses low-overhead learning-to-hash to map queries and keys into binary hash codes, enabling efficient relative qk score estimation.

Result: Achieves up to 7.2× speedup over full attention and outperforms state-of-the-art Top-$k$ methods in accuracy and efficiency.

Conclusion: HATA is a scalable, efficient solution for accelerating LLM inference without compromising accuracy.

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [610] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/pdf/2506.02577)
*Wenyan Yang, Joni Pajarinen*

Main category: cs.LG

TL;DR: The paper introduces Reachability Weighted Sampling (RWS) to improve offline goal-conditioned RL by prioritizing reachable state-goal-action pairs, enhancing policy performance.


<details>
  <summary>Details</summary>
Motivation: Uniform sampling in offline goal-conditioned RL leads to inefficiencies and degraded performance due to unreachable pairs. The paper aims to address this by favoring transitions that enable goal achievement.

Method: Proposes RWS, which uses a reachability classifier trained via PU learning to assign sampling priorities based on reachability scores derived from goal-conditioned state-action values.

Result: RWS significantly improves performance on six robotic manipulation tasks, with a 50% improvement in the HandBlock-Z task compared to baselines.

Conclusion: RWS effectively enhances offline goal-conditioned RL by focusing on reachable transitions, demonstrating its practical utility in complex tasks.

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [611] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/pdf/2506.02599)
*Niklas Roßberg, Marion Neumeier, Sinan Hasirlioglu, Mohamed Essayed Bouzouraa, Michael Botsch*

Main category: cs.LG

TL;DR: A pipeline for clustering and analyzing highway traffic scenarios using CVQ-VAE, showing improved performance and discussing data-completeness trade-offs.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe operation of Automated Driving Systems (ADS) requires precise understanding of traffic scenarios.

Method: Uses CVQ-VAE for clustering highway scenarios and analyzes category completeness.

Result: Outperforms previous clustering methods; discusses trade-off between cluster quality and data requirements.

Conclusion: The approach enhances scenario understanding for ADS safety, with insights on balancing data needs and completeness.

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [612] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/pdf/2506.02612)
*Jan Robine, Marc Höftmann, Stefan Harmeling*

Main category: cs.LG

TL;DR: SGF is a simple, fast, and effective world model using self-supervised learning, frame/action stacking, and data augmentation, performing well on Atari 100k.


<details>
  <summary>Details</summary>
Motivation: To explore world models without RNNs, transformers, discrete representations, or image reconstructions, and to develop a simpler yet effective alternative.

Method: Uses self-supervised representation learning, frame and action stacking for short-time dependencies, and data augmentation for robustness.

Result: Demonstrates good performance on the Atari 100k benchmark through ablation studies and comparisons.

Conclusion: SGF proves that simpler world models can be effective, offering a viable alternative to complex architectures.

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [613] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/pdf/2506.02616)
*Qi Liao, Parijat Bhattacharjee*

Main category: cs.LG

TL;DR: The paper introduces two compositional learning methods (CDRL and CPDM) for self-organizing networks, showing improved performance in multi-agent systems under constraints.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like parameter interdependencies and conflicting objectives in self-organizing networks.

Method: Proposes a modular, two-tier framework with cell-level and cell-pair-level agents to manage granularities and reduce complexity.

Result: Significant reduction in handover failures, improved throughput and latency, better scalability, faster convergence, and safer training.

Conclusion: The compositional learning approaches outperform conventional methods in large-scale self-organizing networks.

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [614] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/pdf/2506.02619)
*Yanbei Liu, Chongxu Wang, Zhitao Xiao, Lei Geng, Yanwei Pang, Xiao Wang*

Main category: cs.LG

TL;DR: HGOT is a self-supervised HGNN method using optimal transport to avoid graph augmentation and sample selection, improving node classification accuracy by 6%.


<details>
  <summary>Details</summary>
Motivation: Traditional contrastive self-supervised learning on heterogeneous graphs requires complex augmentation and sample selection, which HGOT aims to simplify.

Method: HGOT uses optimal transport to align semantic information from branch views (meta-paths) to a central view, eliminating the need for augmentation and sample selection.

Result: HGOT outperforms state-of-the-art methods, achieving a 6% accuracy improvement in node classification tasks.

Conclusion: HGOT provides a simpler and more effective self-supervised learning approach for heterogeneous graphs, validated by superior performance on real-world datasets.

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [615] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/pdf/2506.02623)
*Yuyang Zhou, Ferrante Neri, Yew-Soon Ong, Ruibin Bai*

Main category: cs.LG

TL;DR: A novel surrogate modeling approach using Siamese networks for efficient neural architecture search (NAS), reducing computational costs while identifying Pareto-optimal solutions.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational expense and complexity of multi-objective NAS by proposing a lightweight surrogate model.

Method: Leverages an ensemble of Siamese network blocks to predict dominance relationships and replaces crowding distance with a heuristic rule.

Result: Achieves 92% accuracy, identifies top architectures in NAS-Bench-201 within 0.01 GPU days, and demonstrates potential for multi-task optimization.

Conclusion: SiamNAS offers a scalable and efficient solution for NAS, with potential extensions for diverse Pareto-optimal solutions in heterogeneous tasks.

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [616] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/pdf/2506.02630)
*Tom Jacobs, Advait Gadhikar, Celia Rubio-Madrigal, Rebekka Burkholz*

Main category: cs.LG

TL;DR: HAM (Hyperbolic Aware Minimization) improves convergence and feature learning by combining optimizer steps with hyperbolic mirror steps, addressing the slow convergence of hyperbolic implicit bias in deep learning.


<details>
  <summary>Details</summary>
Motivation: The hyperbolic implicit bias in overparameterized models slows convergence despite promoting sparsity. HAM aims to overcome this limitation.

Method: HAM alternates between optimizer steps and hyperbolic mirror steps, deriving a Riemannian gradient flow for improved convergence and feature learning.

Result: HAM boosts performance in diverse tasks (vision, graph classification, LLM fine-tuning) and works well with sparsification methods, requiring minimal overhead.

Conclusion: HAM effectively addresses the convergence issue of hyperbolic implicit bias, enhancing performance with minimal computational cost.

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [617] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/pdf/2506.02654)
*Shiyu Shen, Bin Pan, Guirong Xue*

Main category: cs.LG

TL;DR: TrafficPPT is a Pretrained Probabilistic Transformer for city-scale traffic volume prediction, addressing data incompleteness and uncertainty by modeling traffic as a distributional aggregation of trajectories. It outperforms existing methods, especially in sparse data conditions.


<details>
  <summary>Details</summary>
Motivation: Existing traffic prediction methods lack uncertainty modeling and city-specific training limits generalizability. TrafficPPT aims to overcome these by integrating diverse data sources and pretraining on simulated data.

Method: TrafficPPT uses a Pretrained Probabilistic Transformer to model traffic volume distributions, combining real-time observations, historical trajectories, and road network topology. It pretrains on simulated data and fine-tunes for target cities.

Result: TrafficPPT outperforms state-of-the-art baselines, particularly in extreme data sparsity scenarios, demonstrating robust and uncertainty-aware traffic inference.

Conclusion: TrafficPPT provides a scalable, uncertainty-aware solution for city-scale traffic prediction, with open-sourced code for broader application.

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [618] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/pdf/2506.02665)
*Tianci Liu, Tong Yang, Quan Zhang, Qi Lei*

Main category: cs.LG

TL;DR: A universal visible watermarking method is proposed for long-term copyright protection against AI misuse, outperforming existing adversarial perturbation techniques.


<details>
  <summary>Details</summary>
Motivation: To address the short-term security of existing copyright protections (e.g., invisible adversarial perturbations) and provide robust, long-term solutions against AI misuse.

Method: Proposes a visible, hard-to-remove watermarking framework based on a probabilistic and inverse problem formulation, with an efficient approximation algorithm to solve the bi-level optimization.

Result: Experimental results show the approach's superiority in diverse scenarios, offering better robustness compared to existing methods.

Conclusion: The proposed visible watermarking method provides a more robust and long-term solution for protecting copyrighted content against AI misuse.

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [619] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/pdf/2506.02694)
*Daichi Kimura, Tomonori Izumitani, Hisashi Kashima*

Main category: cs.LG

TL;DR: A novel attention mechanism, XicorAttention, using Chatterjee's rank correlation coefficient improves time series forecasting by capturing nonlinear dependencies, outperforming existing models by up to 9.1%.


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms in Transformer-based models for time series forecasting may not adequately capture nonlinear dependencies, leaving room for improvement.

Method: Proposes XicorAttention, replacing standard attention's matrix multiplication with Chatterjee's rank correlation coefficient, using differentiable approximations (SoftSort and SoftRank) for computation.

Result: Experiments show XicorAttention improves forecasting accuracy by up to 9.1% compared to existing models.

Conclusion: Incorporating nonlinear correlation via XicorAttention enhances forecasting performance, demonstrating its effectiveness in capturing complex dependencies in time series data.

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [620] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/pdf/2506.02703)
*Khizar Hayat, Baptiste Magnier*

Main category: cs.LG

TL;DR: The paper highlights flaws in credit card fraud detection research, showing how improper evaluation methods can make simple models appear superior. It identifies four key issues and demonstrates their impact through a case study.


<details>
  <summary>Details</summary>
Motivation: To expose and address the methodological shortcomings in credit card fraud detection research, emphasizing the importance of rigorous evaluation over model complexity.

Method: The study uses deliberate experimentation with flawed evaluation protocols and a case study involving a minimal neural network with data leakage.

Result: Simple models with improper evaluation can achieve misleadingly high performance (e.g., 99.9% recall), overshadowing sophisticated methods.

Conclusion: Methodological rigor is more critical than model sophistication in fraud detection research, calling for improved practices in machine learning evaluations.

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [621] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/pdf/2506.02718)
*Guanzhong Chen, Shaoxiong Yang, Chao Li, Wei Liu, Jian Luan, Zenglin Xu*

Main category: cs.LG

TL;DR: MHGPO, a Critic-free MARL algorithm, outperforms MAPPO in optimizing multi-agent LLM systems by enhancing stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of fixed knowledge cutoffs and uncontrollable outputs in LLMs, and the inefficiencies of existing MARL methods like MAPPO.

Method: Proposes MHGPO, a Critic-free algorithm using relative reward advantages and group rollout sampling strategies.

Result: MHGPO outperforms MAPPO in task performance and computational efficiency without requiring warm-up.

Conclusion: MHGPO offers a stable and scalable solution for optimizing complex LLM-based multi-agent systems.

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [622] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/pdf/2506.02712)
*Jayadev Naram, Fredrik Hellström, Ziming Wang, Rebecka Jörnsten, Giuseppe Durisi*

Main category: cs.LG

TL;DR: The paper addresses partial domain adaptation (PDA) by proposing a theoretically grounded method (WARMPOT) using partial optimal transport, improving on heuristic weighting schemes.


<details>
  <summary>Details</summary>
Motivation: Labeled data in target domains are scarce, while related source domains have abundant data. Existing PDA methods lack theoretical justification.

Method: Derives generalization bounds for PDA using partial optimal transport, leading to the WARMPOT algorithm with theoretically motivated weights.

Result: WARMPOT is competitive with recent approaches, and the proposed weights outperform existing heuristic schemes.

Conclusion: The work provides a theoretical foundation for PDA and demonstrates practical improvements with WARMPOT.

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [623] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/pdf/2506.02724)
*Andrey Veprikov, Vladimir Solodkin, Alexander Zyl, Andrey Savchenko, Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: WeightLoRA is a novel method for adaptive selection of critical LoRA heads, reducing trainable parameters while maintaining or improving performance.


<details>
  <summary>Details</summary>
Motivation: Overcome the memory and intuition challenges of LoRA by adaptively selecting critical LoRA heads during optimization.

Method: Proposes WeightLoRA, which dynamically selects the most critical LoRA heads, and WeightLoRA+, an enhanced version.

Result: Demonstrates superior performance in benchmarks with DeBERTa, BART, and Llama models compared to other adaptive approaches.

Conclusion: WeightLoRA and WeightLoRA+ effectively reduce trainable parameters without compromising performance, offering a practical solution for large models.

Abstract: The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [624] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/pdf/2506.02749)
*Changyi Xiao, Yixin Cao*

Main category: cs.LG

TL;DR: A novel regularization method for tensor decomposition-based (TDB) models in knowledge graph completion (KGC) is proposed to reduce overfitting by minimizing intermediate variable norms, supported by theoretical analysis and experiments.


<details>
  <summary>Details</summary>
Motivation: Existing TDB models for KGC are prone to overfitting, and current regularization methods are suboptimal.

Method: Proposes a regularization technique minimizing norms of intermediate variables in TDB models, ensuring tractable computation.

Result: The method effectively reduces overfitting and is supported by theoretical and experimental validation.

Conclusion: The proposed regularization improves TDB model performance in KGC, with code publicly available.

Abstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [625] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/pdf/2506.02757)
*Ruiying Lu, Jinhan Liu, Chuan Du, Dandan Guo*

Main category: cs.LG

TL;DR: The paper introduces a method for tabular anomaly detection using mask modeling and prototype learning to address representation entanglement and lack of global correlation modeling.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based methods for tabular anomaly detection suffer from representation entanglement and insufficient global correlation modeling, limiting performance.

Method: The proposed method combines mask modeling and prototype learning, using disentangled representation learning and global prototypes to model normal dependencies. It involves encoding with mask modeling in data and projection spaces and decoding with parallel masked representation reconstruction and association prototype learning.

Result: Experiments on 20 tabular benchmarks show the model's effectiveness and interpretability.

Conclusion: The method improves tabular anomaly detection by addressing key limitations of existing approaches, demonstrating strong performance and interpretability.

Abstract: Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [626] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/pdf/2506.02767)
*Marco Calì, Giulio Giacomuzzo, Ruggero Carli, Alberto Dalla Libera*

Main category: cs.LG

TL;DR: EB-MC-PILCO integrates iLQR with MC-PILCO to speed up policy optimization, reducing execution time by 45.9% while maintaining a 100% success rate.


<details>
  <summary>Details</summary>
Motivation: To address the slow convergence of MC-PILCO in model-based reinforcement learning.

Method: Combines MC-PILCO with iLQR to generate exploratory trajectories and initialize the policy.

Result: Achieves faster convergence (45.9% reduction in time) and 100% success rate on the cart-pole task.

Conclusion: EB-MC-PILCO is a more efficient and reliable alternative to standard MC-PILCO.

Abstract: This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [627] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/pdf/2506.02811)
*António Pedro Pinheiro, Rita P. Ribeiro*

Main category: cs.LG

TL;DR: Proposes a CART-based synthetic data generation method for imbalanced regression, offering transparency and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of imbalanced target distributions in regression tasks, especially in tabular data, where underrepresented regions hinder model performance.

Method: Adapts a CART-based synthetic data generation method, integrating relevance and density-based mechanisms for threshold-free, feature-driven sampling in sparse target regions.

Result: Competitive performance with other resampling and generative methods, faster execution, and greater transparency.

Conclusion: The method is a promising, transparent, and scalable data-level strategy for improving regression models in imbalanced domains.

Abstract: Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [628] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/pdf/2506.02842)
*Stefano Fiorini, Hakan Aktas, Iulia Duta, Stefano Coniglio, Pietro Morerio, Alessio Del Bue, Pietro Liò*

Main category: cs.LG

TL;DR: Sheaf Neural Networks (SNNs) are generalized Graph Neural Networks (GNNs) that improve relational data modeling. The paper introduces Directed Cellular Sheaf and Directed Sheaf Laplacian to incorporate edge directionality, leading to the Directed Sheaf Neural Network (DSNN), which outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Existing SNNs lack the ability to represent directionality, which is crucial for graph learning tasks and real-world applications.

Method: The authors propose Directed Cellular Sheaf and Directed Sheaf Laplacian to capture edge orientation, forming the backbone of DSNN.

Result: DSNN consistently outperforms baseline methods across nine real-world benchmarks.

Conclusion: The introduction of directionality in SNNs via DSNN enhances performance, addressing a key limitation in existing models.

Abstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [629] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/pdf/2506.02864)
*Changyi Xiao, Mengdi Zhang, Yixin Cao*

Main category: cs.LG

TL;DR: BNPO introduces adaptive reward normalization using Beta distribution to improve training stability in reinforcement learning for language models.


<details>
  <summary>Details</summary>
Motivation: Current methods lack adaptive reward normalization, leading to unstable gradients and training instability.

Method: Proposes Beta Normalization Policy Optimization (BNPO) with dynamic Beta distribution parameters for adaptive normalization.

Result: BNPO achieves state-of-the-art performance, reduces gradient variance, and generalizes existing methods.

Conclusion: BNPO enhances training stability and performance in reinforcement learning for reasoning tasks.

Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [630] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/pdf/2506.02883)
*Anthony Kobanda, Odalric-Ambrym Maillard, Rémy Portelas*

Main category: cs.LG

TL;DR: A benchmark for continual reinforcement learning in video-game navigation is introduced to address challenges like catastrophic forgetting, task adaptation, and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks for continual reinforcement learning in dynamic environments like gaming, ensuring scalability and preventing forgetting.

Method: Developed a suite of video-game navigation tasks, datasets, evaluation protocols, and metrics, including state-of-the-art baselines.

Result: The benchmark provides a reproducible framework for evaluating algorithms and accelerating progress in continual reinforcement learning.

Conclusion: This work fills a literature gap, aids reproducible research, and helps practitioners apply effective approaches in production pipelines.

Abstract: Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [631] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/pdf/2506.02887)
*Mrinmay Sen, Shruti Aparna, Rohit Agarwal, Chalavadi Krishna Mohan*

Main category: cs.LG

TL;DR: A survey on the impact of partial client participation in federated learning, reviewing methods, theoretical insights, and empirical findings.


<details>
  <summary>Details</summary>
Motivation: Addresses the overlooked challenges of partial client participation in FL, common in real-world scenarios.

Method: In-depth review and categorization of existing FL methods for partial client participation.

Result: Comprehensive analysis of methods, their pros and cons, supported by theory and empirical data.

Conclusion: Highlights the need for further research on partial client participation in FL.

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [632] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/pdf/2506.02890)
*Jakub Krajewski, Marcin Chochowski, Daniel Korzekwa*

Main category: cs.LG

TL;DR: Fine-grained MoE improves model convergence and quality, outperforming standard MoE in large-scale models (up to 56B parameters) with better validation loss and accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of fine-grained MoE (using more, smaller experts) for scaling LLMs efficiently and improving model performance.

Method: Proposes training recipes and conducts empirical evaluation, comparing fine-grained MoE against standard MoE in models up to 56B parameters.

Result: Fine-grained MoE achieves better validation loss and higher accuracy on downstream benchmarks at large scales.

Conclusion: Fine-grained MoE is empirically validated as effective for future large-scale model development, offering practical insights.

Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [633] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/pdf/2506.02897)
*Alessandro Licciardi, Roberta Raineri, Anton Proskurnikov, Lamberto Rondoni, Lorenzo Zino*

Main category: cs.LG

TL;DR: Proposes a federated learning algorithm (Federated Coalition Variance Reduction with Boltzmann Exploration) to address data heterogeneity by clustering clients and selecting optimal updates, improving accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning degrades model performance, necessitating a solution to mitigate its impact.

Method: Clients dynamically cluster based on agreements, and one client per cluster is selected to minimize update variance, inspired by opinion dynamics.

Result: Outperforms existing FL algorithms in heterogeneous scenarios, achieving better accuracy and faster convergence.

Conclusion: The proposed method effectively addresses data heterogeneity, enhancing federated learning performance.

Abstract: Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [634] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/pdf/2506.02933)
*Junyi Fang, Yuxun Chen, Yuxin Chen, Chen Zhang*

Main category: cs.LG

TL;DR: RAVEN-UCB is a variance-aware MAB algorithm for non-stationary environments, outperforming UCB1 and UCB-V with tighter regret bounds and practical efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of non-stationary reward distributions in MAB problems.

Method: Combines variance-driven exploration, adaptive control, and constant-time recursive updates.

Result: Achieves tighter regret bounds and demonstrates robustness in synthetic and real-world scenarios.

Conclusion: RAVEN-UCB is theoretically and practically superior to existing baselines in non-stationary settings.

Abstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [635] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/pdf/2506.02935)
*Yuepeng Zheng, Fu Luo, Zhenkun Wang, Yaoxin Wu, Yu Zhou*

Main category: cs.LG

TL;DR: A novel multi-task learning method (MTL-KD) using knowledge distillation trains heavy decoder models for solving diverse Vehicle Routing Problem (VRP) variants, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based multi-task methods for Neural Combinatorial Optimization (NCO) lack generalization for large-scale problems, prompting the need for a more robust approach.

Method: MTL-KD transfers policy knowledge from single-task models to a heavy decoder model, enabling label-free training. A flexible inference strategy (R3C) is also introduced for diverse VRP tasks.

Result: The method achieves superior performance on 6 seen and 10 unseen VRP variants (up to 1000 nodes), showing strong generalization.

Conclusion: MTL-KD with R3C effectively improves generalization and performance in multi-task learning for NCO, especially for large-scale VRP problems.

Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [636] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/pdf/2506.02950)
*Stepan I. Manukhov, Alexander Kolesov, Vladimir V. Palyulin, Alexander Korotin*

Main category: cs.LG

TL;DR: IFM generalizes EFM by using general interaction fields, solving EFM's issues with electrostatic field modeling. It demonstrates effectiveness in data transfer tasks.


<details>
  <summary>Details</summary>
Motivation: EFM's reliance on electrostatic fields is complex due to external field modeling. IFM aims to simplify and generalize this approach.

Method: Proposes IFM, extending EFM to general interaction fields, with a specific design inspired by quark-antiquark interactions.

Result: Shows successful performance on toy and image data transfer problems.

Conclusion: IFM effectively addresses EFM's limitations and generalizes the paradigm for broader applications.

Abstract: Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [637] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/pdf/2506.02939)
*Malik Khalaf, Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*

Main category: cs.LG

TL;DR: PAMM reduces memory usage in LLM attention layers by compressing Q, K, V projections, achieving up to 512x memory savings without compromising perplexity.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked memory consumption in linear projections of attention layers, which impacts LLM training efficiency.

Method: Proposes Point-Approximate Matrix Multiplication (PAMM), a tensor compression technique for Q, K, V projections.

Result: Achieves up to 512x memory reduction while maintaining or improving perplexity.

Conclusion: PAMM is a practical, complementary method for memory-efficient LLM training, compatible with techniques like FlashAttention.

Abstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [638] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/pdf/2506.02946)
*Edoardo Pona, Milad Kazemi, Yali Du, David Watson, Nicola Paoletti*

Main category: cs.LG

TL;DR: The paper introduces Abstract Counterfactuals, a framework for counterfactual reasoning in language model agents, addressing limitations of token-level methods.


<details>
  <summary>Details</summary>
Motivation: Existing token-level counterfactuals are inadequate for LM agents due to open-ended action spaces and contextual token meanings, leading to biased or meaningless results.

Method: The proposed framework, Abstract Counterfactuals, focuses on high-level action characteristics and interactions, tested on text-based games and counterfactual text generation with token-level and latent-space interventions.

Result: The approach produces consistent and meaningful counterfactuals while minimizing side effects of token-level methods.

Conclusion: Abstract Counterfactuals offer a tailored solution for counterfactual reasoning in LM agents, improving interpretability and reducing bias.

Abstract: Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [639] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/pdf/2506.02965)
*Ze Yu Zhang, Bolin Ding, Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: PC-MoE enables decentralized, privacy-preserving collaborative LLM training with minimal GPU memory and strong confidentiality, matching centralized model performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of limited GPU memory and data resources while ensuring training data privacy in collaborative LLM training.

Method: Leverages MoE sparsity for memory efficiency and keeps data, forward pass signals, and gradients local to protect privacy.

Result: Matches centralized model performance on benchmarks, reduces GPU RAM by 70%, and resists reconstruction attacks.

Conclusion: PC-MoE breaks the trade-off between privacy and accuracy, offering efficient, secure collaborative training.

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [640] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/pdf/2506.02972)
*Md-Ferdous Pervej, Richeng Jin, Md Moin Uddin Chowdhury, Simran Singh, İsmail Güvenç, Huaiyu Dai*

Main category: cs.LG

TL;DR: A novel online aerial federated learning algorithm (2CEOAFL) is proposed for resource-constrained aerial connected vehicles (ACVs), balancing computation and communication efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The continual arrival of new data from ACVs and their resource constraints necessitate efficient machine learning solutions for privacy-preserving distributed ML.

Method: The 2CEOAFL algorithm involves modeling ACV trajectories, pruning dense ML models, training pruned models, and probabilistically quantizing and offloading gradients to a central server.

Result: Simulations show 2CEOAFL performs comparably to inefficient, non-pruned, and non-quantized methods.

Conclusion: The proposed algorithm effectively addresses the challenges of resource constraints and continual data arrival in ACV-assisted edge computing.

Abstract: Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [641] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/pdf/2506.02978)
*Mohamed Djilani, Thibault Simonetto, Karim Tit, Florian Tambon, Paul Récamier, Salah Ghamizi, Maxime Cordy, Mike Papadakis*

Main category: cs.LG

TL;DR: Tabular foundational models (FM) like TabPFN and TabICL are vulnerable to adversarial attacks, degrading performance. They can also generate transferable attacks on other models. A new in-context adversarial training method improves robustness.


<details>
  <summary>Details</summary>
Motivation: To explore the adversarial vulnerabilities of tabular FM and their potential misuse, as well as to propose solutions for improving robustness.

Method: Study adversarial vulnerabilities via test-time attacks and misuse as adversarial tools. Introduce in-context adversarial training to enhance robustness.

Result: Small perturbations degrade FM accuracy. FM can generate transferable attacks. In-context adversarial training improves robustness.

Conclusion: Tabular FM are both vulnerable and capable of adversarial threats, necessitating robust training and evaluation practices.

Abstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [642] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/pdf/2506.02986)
*Nathan Buskulic, Jalal Fadil, Yvain Quéau*

Main category: cs.LG

TL;DR: The paper provides theoretical guarantees for self-supervised neural networks in solving inverse problems, focusing on convergence and recovery. It analyzes both continuous-time and discrete cases, showing accelerated exponential convergence in the former and linear convergence in the latter.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of theoretical guarantees for neural networks in inverse problems, the work aims to provide convergence and recovery assurances for self-supervised networks.

Method: The study examines self-supervised neural networks (e.g., Deep Image/Inverse Prior) trained with inertia, including viscous and geometric Hessian-driven dampings. It analyzes both continuous-time (dynamical system trajectory) and discrete-time (inertial algorithm with adaptive step-size) cases.

Result: In continuous-time, the network achieves optimal accelerated exponential convergence. In discrete-time, it shows linear convergence with recovery guarantees, though less sharp than the continuous case.

Conclusion: The work establishes theoretical foundations for self-supervised networks in inverse problems, demonstrating improved convergence rates in both continuous and discrete settings.

Abstract: Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [643] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/pdf/2506.03028)
*Junde Xu, Zijun Gao, Xinyi Zhou, Jie Hu, Xingyi Cheng, Le Song, Guangyong Chen, Pheng-Ann Heng, Jiezhong Qiu*

Main category: cs.LG

TL;DR: A novel method using Direct Preference Optimization (DPO) improves protein sequence design by fine-tuning an inverse folding model with feedback from a folding model, enhancing sequence recovery and structural similarity.


<details>
  <summary>Details</summary>
Motivation: The inverse folding problem is crucial for biotechnological applications, requiring better methods to design sequences that fold into desired structures.

Method: The approach samples candidate sequences from an inverse-folding model, predicts their structures with a folding model, and uses pairwise structural-preference labels to fine-tune the inverse-folding model under DPO.

Result: DPO fine-tuning improves sequence recovery and increases average TM-Score from 0.77 to 0.81, with iterative application yielding a 79.5% TM-Score improvement over baseline.

Conclusion: This work demonstrates the effectiveness of preference optimization for enhancing protein sequence design from structural feedback.

Abstract: The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [644] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/pdf/2506.03077)
*Qijun Luo, Mengqi Li, Lei Zhao, Xiao Li*

Main category: cs.LG

TL;DR: StreamBP is a memory-efficient and exact backpropagation method for training language models on long sequences, reducing memory costs and improving scalability.


<details>
  <summary>Details</summary>
Motivation: Training on long sequences is essential for complex tasks like reasoning, but memory costs for activation storage during backpropagation are prohibitive.

Method: StreamBP decomposes the chain rule linearly along the sequence dimension layer-wise, leveraging the causal structure of language models to reduce memory and computation.

Result: StreamBP scales sequence length by 2.8-5.5 times compared to gradient checkpointing, with comparable or faster backpropagation time. It also supports batch size scaling and multi-GPU training.

Conclusion: StreamBP is a practical, scalable solution for training transformer models on long sequences, with open-source implementation available.

Abstract: Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [645] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/pdf/2506.03037)
*Shubhendu Trivedi, Brian D. Nord*

Main category: cs.LG

TL;DR: The paper addresses challenges in quantifying uncertainties for ML models, highlighting inconsistent terminology and varying technical demands. It advocates for standards to align intent and implementation in UQ approaches, focusing on trustworthiness and practical recommendations for scientific ML.


<details>
  <summary>Details</summary>
Motivation: To clarify inconsistencies in uncertainty terminology and estimation across disciplines, and to address the varying technical requirements for trustworthy uncertainties in diverse problem contexts.

Method: Examines current landscape of estimation targets, uncertainty constructs, and mapping approaches, highlighting problematic mappings. Advocates for standards and discusses axes of trustworthiness.

Result: Identifies challenges in UQ for ML and proposes alignment standards. Provides practical recommendations for uncertainty-aware ML systems, especially in scientific ML and SBI.

Conclusion: Alignment between intent and implementation in UQ is crucial. The paper offers actionable insights for designing and evaluating trustworthy uncertainty-aware ML systems.

Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [646] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/pdf/2506.03043)
*Nikita Puchkin, Iurii Pustovalov, Yuri Sapronov, Denis Suchkov, Alexey Naumov, Denis Belomestny*

Main category: cs.LG

TL;DR: The paper studies the generalization ability of an empirical KL risk minimizer for Schrödinger potential estimation, deriving a non-asymptotic upper bound on KL-divergence with fast convergence rates.


<details>
  <summary>Details</summary>
Motivation: Schrödinger potential estimation is key in generative modeling via Schrödinger bridges and stochastic optimal control, requiring minimal effort to connect two distributions.

Method: The authors analyze an empirical KL risk minimizer over admissible log-potentials, fitting the terminal marginal distribution under assumptions on the target and prior process.

Result: A non-asymptotic high-probability upper bound on KL-divergence is derived, showing excess KL-risk can decrease as fast as O(log²n/n) for unbounded supports.

Conclusion: The results demonstrate efficient generalization for Schrödinger potential estimation, even with unbounded distributions, supporting its use in generative modeling.

Abstract: We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [647] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/pdf/2506.03087)
*Bin Ma, Yuyuan Feng, Minhua Lin, Enyan Dai*

Main category: cs.LG

TL;DR: The paper explores security risks in explainable GNNs, proposing a method to steal models using explanation leaks and data augmentation.


<details>
  <summary>Details</summary>
Motivation: Growing use of GNNs in sensitive domains demands transparency, but explanations may expose models to security threats like model stealing.

Method: Proposes a framework combining explanation alignment and guided data augmentation to replicate target models' behavior and reasoning.

Result: Experiments show the method outperforms conventional techniques in model stealing on molecular graph datasets.

Conclusion: Highlights security risks of explainable GNNs and calls for protective measures against explanation-based attacks.

Abstract: Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [648] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/pdf/2506.03100)
*Yang Guo, Yutian Tao, Yifei Ming, Robert D. Nowak, Yingyu Liang*

Main category: cs.LG

TL;DR: The paper provides a theoretical analysis of Retrieval-augmented Generation (RAG), introducing a finite-sample generalization bound and a bias-variance tradeoff for in-context linear regression. It highlights the intrinsic generalization error ceiling in RAG compared to in-context learning (ICL).


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of RAG's empirical successes, the paper aims to provide a formal framework for analyzing its generalization properties.

Method: The framework treats retrieved texts as query-dependent noisy in-context examples, unifying classical ICL and standard RAG. It introduces uniform and non-uniform RAG noise to model retrieval from training data and external corpora.

Result: The analysis reveals an intrinsic ceiling on RAG's generalization error, contrasting with ICL. Empirical experiments on QA benchmarks (Natural Questions, TriviaQA) validate the sample efficiency of ICL and RAG.

Conclusion: The paper establishes a theoretical foundation for RAG, demonstrating its limitations and advantages over ICL, supported by empirical evidence.

Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [649] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/pdf/2506.03062)
*Qining Zhang, Tanner Fiez, Yi Liu, Wenyang Liu*

Main category: cs.LG

TL;DR: The paper introduces a fixed-budget multi-metric adaptive experimental design (AED) framework with two phases: adaptive exploration and validation, improving statistical power and inference for heterogeneous variances.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of standard A/B tests and existing AEDs in handling multiple candidates, metrics, and heterogeneous variances.

Method: Proposes SHRVar, a two-phase framework combining adaptive exploration (using relative-variance-based sampling and reward z-values) and validation via A/B testing.

Result: Achieves exponentially decreasing error probability, outperforming existing methods like SH and SHVar in numerical experiments.

Conclusion: The framework effectively balances exploration and validation, offering robust statistical inference for complex experimental settings.

Abstract: Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [650] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/pdf/2506.03066)
*Qining Zhang, Lei Ying*

Main category: cs.LG

TL;DR: The paper introduces ZSPO, a zeroth-order policy optimization algorithm for RLHF problems with unknown link functions, avoiding mis-specification issues and achieving convergence without knowing the link function.


<details>
  <summary>Details</summary>
Motivation: Current RLHF algorithms assume known link functions (e.g., logistic), which is unrealistic due to human preference complexity. This work addresses link function mis-specification by studying RLHF with unknown link functions.

Method: Proposes ZSPO, a zeroth-order policy optimization method that estimates the sign of value function differences to construct update directions, bypassing the need for link function knowledge.

Result: ZSPO converges to a stationary policy with polynomial rates under mild conditions and outperforms in link function mismatch scenarios.

Conclusion: ZSPO effectively handles unknown link functions, offering a robust alternative to traditional RLHF methods.

Abstract: Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [651] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/pdf/2506.03075)
*Bogdan Chornomaz, Yonatan Koren, Shay Moran, Tom Waknine*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [652] [Non-Asymptotic Length Generalization](https://arxiv.org/pdf/2506.03085)
*Thomas Chen, Tengyu Ma, Zhiyuan Li*

Main category: cs.LG

TL;DR: The paper provides provable guarantees for length generalization in learning algorithms, formalizing non-asymptotic length generalization and showing optimality for certain function classes.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the ability of learning algorithms to generalize to longer inputs than those seen in training.

Method: Formalizes non-asymptotic length generalization, analyzes length complexity, and evaluates learning algorithms like Minimum-Complexity Interpolator.

Result: Optimal length complexity for certain classes, decidability implications for Context-Free Grammars, and bounds for C-RASP functions.

Conclusion: Length generalization can be quantified and optimized for specific function classes, with implications for learning algorithms.

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [653] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/pdf/2506.03133)
*Kai Lion, Liang Zhang, Bingcong Li, Niao He*

Main category: cs.LG

TL;DR: PoLAR improves low-rank adaptation by addressing underutilization of subspaces, achieving faster convergence and better performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Low-rank adaptation in large-scale models suffers from underutilization of subspaces, degrading fine-tuning performance.

Method: Proposes PoLAR, a parameterization using polar decomposition to factorize updates into direction matrices on Stiefel manifolds and an unconstrained scale matrix, paired with Riemannian optimization.

Result: PoLAR achieves exponentially faster convergence and consistent gains on benchmarks for language understanding, reasoning, and problem-solving.

Conclusion: PoLAR effectively mitigates subspace underutilization, enhancing fine-tuning performance in large-scale models.

Abstract: We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [654] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/pdf/2506.03093)
*Valérie Costa, Thomas Fel, Ekdeep Singh Lubana, Bahareh Tolooshams, Demba Ba*

Main category: cs.LG

TL;DR: The paper explores whether sparse autoencoders (SAEs) can capture hierarchical and nonlinear features in neural networks, proposing MP-SAE, a modified SAE that addresses limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the hypothesis that neural network features are linearly accessible and orthogonal, but recent findings suggest hierarchical and nonlinear features exist, questioning SAEs' effectiveness.

Method: The authors introduce MP-SAE, a variant of SAE based on the matching pursuits algorithm, designed to capture hierarchical and nonlinear features through residual-guided steps.

Result: MP-SAE outperforms existing SAEs in capturing hierarchical concepts and nonlinear features, revealing shared structures in vision-language models and enabling adaptive sparsity.

Conclusion: The results support the idea that interpretability methods should align with the phenomenology of representations, emphasizing the need for assumptions that fit observed features.

Abstract: Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [655] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/pdf/2506.03109)
*Wei Yao, Gengze Xu, Huayi Tang, Wenkai Yang, Donglin Di, Ziqiao Wang, Yong Liu*

Main category: cs.LG

TL;DR: The paper introduces $f$-divergence loss in weak-to-strong generalization (W2SG) to improve strong models using weaker supervision, reducing computational overhead and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing W2SG methods often require extra weak models or complex procedures, increasing computational and memory costs. The paper aims to address this by leveraging $f$-divergence loss.

Method: The authors propose using $f$-divergence as an information-theoretic loss framework in W2SG, analyzing its theoretical limitations and equivalence.

Result: Empirical results show $f$-divergence loss improves generalization and noise tolerance in strong models, outperforming traditional metrics like KL divergence.

Conclusion: $f$-divergence loss is a practical and effective tool for W2SG, offering theoretical and empirical advantages over existing methods.

Abstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [656] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/pdf/2506.03111)
*Victor Armegioiu, Yannick Ramic, Siddhartha Mishra*

Main category: cs.LG

TL;DR: Rectified flow framework improves fluid flow modeling by using straighter trajectories, reducing sampling steps from 128 to 8 without losing fidelity.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of conditional diffusion models in fluid flow modeling, which require many stochastic steps.

Method: Introduces a rectified flow framework that learns a time-dependent velocity field for nearly straight trajectories, solving an ODE for efficient sampling.

Result: Achieves same posterior distributions as diffusion models, preserves fine-scale features, and reduces inference time significantly.

Conclusion: Rectified flows offer a faster, equally accurate alternative to diffusion models for multiscale fluid flow modeling.

Abstract: The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [657] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/pdf/2506.03128)
*Andreas Auer, Raghul Parthipan, Pedro Mercado, Abdul Fatir Ansari, Lorenzo Stella, Bernie Wang, Michael Bohlke-Schneider, Syama Sundar Rangapuram*

Main category: cs.LG

TL;DR: COSMIC is a zero-shot forecasting model using covariates via in-context learning, achieving state-of-the-art performance without requiring covariate datasets.


<details>
  <summary>Details</summary>
Motivation: Existing pretrained time series models lack effective covariate support, limiting their performance and accessibility.

Method: COSMIC uses in-context learning and Informative Covariate Augmentation to train without covariate datasets.

Result: COSMIC achieves top performance in zero-shot forecasting, effectively leveraging covariates.

Conclusion: COSMIC demonstrates the potential of covariates in zero-shot forecasting, enhancing model performance and accessibility.

Abstract: Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [658] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/pdf/2506.03142)
*Xiangyu Zhou, Yao Qiang, Saleh Zare Zade, Douglas Zytko, Prashant Khanduri, Dongxiao Zhu*

Main category: cs.LG

TL;DR: The paper introduces the Targeted Information Forgetting (TIF) framework to address over-forgetting in LLMs, improving unlearning while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: LLMs memorize unwanted private/copyrighted content, raising privacy/legal concerns. Existing unlearning methods cause over-forgetting, degrading model utility.

Method: TIF includes a targeted information identifier and Targeted Preference Optimization with Logit Preference Loss (for unlearning) and Preservation Loss (for retaining general info).

Result: TIF outperforms on TOFU and MUSE benchmarks, enhancing unlearning effectiveness while preserving utility.

Conclusion: TIF provides a state-of-the-art solution for targeted unlearning in LLMs, balancing effectiveness and utility.

Abstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


### [659] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/pdf/2505.21777)
*Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov*

Main category: cs.LG

TL;DR: The paper explores diffusion models as associative memory systems, drawing parallels to Hopfield networks, and identifies spurious states in large data regimes.


<details>
  <summary>Details</summary>
Motivation: To understand diffusion models through the lens of associative memory systems and analyze memorization-generalization transitions.

Method: Conceptualizes diffusion model training as memory encoding and generation as retrieval, comparing it to Hopfield networks.

Result: Identifies spurious states in large data regimes and validates their existence empirically.

Conclusion: Provides insights into memorization-generalization in diffusion models and predicts spurious states.

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [660] [Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings](https://arxiv.org/pdf/2306.08586)
*Yehya Farhat, Hamza ElMokhtar Shili, Fangshuo Liao, Chen Dun, Mirian Hipolito Garcia, Guoqing Zheng, Ahmed Hassan Awadallah, Robert Sim, Dimitrios Dimitriadis, Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: The paper introduces DDOME, a method for dynamically allocating expertise in Mixture-of-Experts (MoEs) by leveraging decentralized data heterogeneity, achieving significant accuracy improvements in Federated Learning.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how expertise emerges in MoEs without clear task partitions and addresses challenges like inference costs and data heterogeneity.

Method: The authors propose DDOME, which dynamically specializes experts by integrating a pretrained common expert to inform gating functions, enabling personalized subset selection.

Result: DDOME improves accuracy by 4-24% over FL baselines in image/text classification and maintains zero-shot generalization.

Conclusion: Joint training of gating and experts is critical for expert specialization, and DDOME effectively leverages decentralized data for dynamic expertise allocation.

Abstract: Mixture-of-Experts (MoEs) achieve scalability by dynamically activating
subsets of their components. Yet, understanding how expertise emerges through
joint training of gating mechanisms and experts remains incomplete, especially
in scenarios without clear task partitions. Motivated by inference costs and
data heterogeneity, we study how joint training of gating functions and experts
can dynamically allocate domain-specific expertise across multiple underlying
data distributions. As an outcome of our framework, we develop an instance
tailored specifically to decentralized training scenarios, introducing
\textit{Dynamically Decentralized Orchestration of MoEs} or \texttt{DDOME}.
\texttt{DDOME} leverages heterogeneity emerging from distributional shifts
across decentralized data sources to specialize experts dynamically. By
integrating a pretrained common expert to inform a gating function,
\texttt{DDOME} achieves personalized expert subset selection on-the-fly,
facilitating just-in-time personalization. We empirically validate
\texttt{DDOME} within a Federated Learning (FL) context: \texttt{DDOME} attains
from 4\% up to an 24\% accuracy improvement over state-of-the-art FL baselines
in image and text classification tasks, while maintaining competitive zero-shot
generalization capabilities. Furthermore, we provide theoretical insights
confirming that the joint gating-experts training is critical for achieving
meaningful expert specialization.

</details>


### [661] [Label Deconvolution for Node Representation Learning on Large-scale Attributed Graphs against Learning Bias](https://arxiv.org/pdf/2309.14907)
*Zhihao Shi, Jie Wang, Fanghua Lu, Hanzhu Chen, Defu Lian, Zheng Wang, Jieping Ye, Feng Wu*

Main category: cs.LG

TL;DR: The paper proposes Label Deconvolution (LD), a label regularization technique to address learning bias in node representation learning on attributed graphs by approximating the inverse mapping of GNNs.


<details>
  <summary>Details</summary>
Motivation: Joint training of large node encoders (NEs) and GNNs on large-scale graphs is impractical due to scalability issues, leading to learning bias when trained separately.

Method: Introduces LD, a scalable approximation to the inverse mapping of GNNs, aligning NE training with GNN feature convolutions.

Result: LD outperforms state-of-the-art methods on Open Graph Benchmark datasets and converges to optimal joint training objectives under mild assumptions.

Conclusion: LD effectively mitigates learning bias and enhances performance in node representation learning without the scalability issues of joint training.

Abstract: Node representation learning on attributed graphs -- whose nodes are
associated with rich attributes (e.g., texts and protein sequences) -- plays a
crucial role in many important downstream tasks. To encode the attributes and
graph structures simultaneously, recent studies integrate pre-trained models
with graph neural networks (GNNs), where pre-trained models serve as node
encoders (NEs) to encode the attributes. As jointly training large NEs and GNNs
on large-scale graphs suffers from severe scalability issues, many methods
propose to train NEs and GNNs separately. Consequently, they do not take
feature convolutions in GNNs into consideration in the training phase of NEs,
leading to a significant learning bias relative to the joint training. To
address this challenge, we propose an efficient label regularization technique,
namely Label Deconvolution (LD), to alleviate the learning bias by a novel and
highly scalable approximation to the inverse mapping of GNNs. The inverse
mapping leads to an objective function that is equivalent to that by the joint
training, while it can effectively incorporate GNNs in the training phase of
NEs against the learning bias. More importantly, we show that LD converges to
the optimal objective function values by the joint training under mild
assumptions. Experiments demonstrate LD significantly outperforms
state-of-the-art methods on Open Graph Benchmark datasets.

</details>


### [662] [Performative Time-Series Forecasting](https://arxiv.org/pdf/2310.06077)
*Zhiyuan Zhao, Haoxin Liu, Alexander Rodriguez, B. Aditya Prakash*

Main category: cs.LG

TL;DR: The paper introduces performative time-series forecasting (PeTS) and proposes Feature Performative-Shifting (FPS) to handle performativity-induced distribution shifts, showing improved performance over conventional methods.


<details>
  <summary>Details</summary>
Motivation: Performativity in time-series forecasting, where predictions influence outcomes, is underexplored. The paper aims to address this gap.

Method: Proposes FPS, leveraging delayed response to anticipate distribution shifts for accurate predictions.

Result: FPS outperforms conventional methods in COVID-19 and traffic forecasting tasks.

Conclusion: FPS is effective for performative time-series forecasting, reducing generalization error.

Abstract: Time-series forecasting is a critical challenge in various domains and has
witnessed substantial progress in recent years. Many real-life scenarios, such
as public health, economics, and social applications, involve feedback loops
where predictions can influence the predicted outcome, subsequently altering
the target variable's distribution. This phenomenon, known as performativity,
introduces the potential for 'self-negating' or 'self-fulfilling' predictions.
Despite extensive studies in classification problems across domains,
performativity remains largely unexplored in the context of time-series
forecasting from a machine-learning perspective.
  In this paper, we formalize performative time-series forecasting (PeTS),
addressing the challenge of accurate predictions when performativity-induced
distribution shifts are possible. We propose a novel approach, Feature
Performative-Shifting (FPS), which leverages the concept of delayed response to
anticipate distribution shifts and subsequently predicts targets accordingly.
We provide theoretical insights suggesting that FPS can potentially lead to
reduced generalization error. We conduct comprehensive experiments using
multiple time-series models on COVID-19 and traffic forecasting tasks. The
results demonstrate that FPS consistently outperforms conventional time-series
forecasting methods, highlighting its efficacy in handling
performativity-induced challenges.

</details>


### [663] [Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization](https://arxiv.org/pdf/2311.18703)
*Daniel Jarne Ornia, Giannis Delimpaltadakis, Jens Kober, Javier Alonso-Mora*

Main category: cs.LG

TL;DR: The paper introduces Predictability-Aware RL (PARL), a method to make RL agents' behavior more predictable by balancing reward maximization and predictability, measured via trajectory entropy rate.


<details>
  <summary>Details</summary>
Motivation: RL agents often behave unpredictably due to exploration incentives, posing safety risks in human-robot interactions. The goal is to develop a method to ensure predictable yet near-optimal behavior.

Method: PARL combines standard RL rewards with the negative entropy rate of trajectories, quantifying predictability. It integrates entropy-rate estimation into policy-gradient algorithms.

Result: The method successfully produces predictable, near-optimal policies in human-robot interaction tasks.

Conclusion: PARL effectively balances predictability and performance, offering a practical solution for safer RL applications.

Abstract: In Reinforcement Learning (RL), agents have no incentive to exhibit
predictable behaviors, and are often pushed (through e.g. policy entropy
regularisation) to randomise their actions in favor of exploration. This often
makes it challenging for other agents and humans to predict an agent's
behavior, triggering unsafe scenarios (e.g. in human-robot interaction). We
propose a novel method to induce predictable behavior in RL agents, termed
Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate
to quantify predictability. Our method maximizes a linear combination of a
standard discounted reward and the negative entropy rate, thus trading off
optimality with predictability. We show how the entropy rate can be formally
cast as an average reward, how entropy-rate value functions can be estimated
from a learned model and incorporate this in policy-gradient algorithms, and
demonstrate how this approach produces predictable (near-optimal) policies in
tasks inspired by human-robot use-cases.

</details>


### [664] [Rényi Neural Processes](https://arxiv.org/pdf/2405.15991)
*Xuesong Wang, He Zhao, Edwin V. Bonilla*

Main category: cs.LG

TL;DR: RNPs improve Neural Processes by addressing prior misspecification using Rényi divergence, outperforming state-of-the-art models in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Neural Processes (NPs) suffer from prior misspecification due to parameterization coupling, limiting their performance.

Method: Proposed Rényi Neural Processes (RNPs) replace KL divergence with Rényi divergence to mitigate prior misspecification effects.

Result: RNPs achieve better log-likelihoods and performance in regression and image inpainting tasks compared to NPs.

Conclusion: RNPs effectively address prior misspecification in NPs, demonstrating superior performance in real-world applications.

Abstract: Neural Processes (NPs) are deep probabilistic models that represent
stochastic processes by conditioning their prior distributions on a set of
context points. Despite their advantages in uncertainty estimation for complex
distributions, NPs enforce parameterization coupling between the conditional
prior model and the posterior model. We show that this coupling amounts to
prior misspecification and revisit the NP objective to address this issue. More
specifically, we propose R\'enyi Neural Processes (RNP), a method that replaces
the standard KL divergence with the R\'enyi divergence, dampening the effects
of the misspecified prior during posterior updates. We validate our approach
across multiple benchmarks including regression and image inpainting tasks, and
show significant performance improvements of RNPs in real-world problems. Our
extensive experiments show consistently better log-likelihoods over
state-of-the-art NP models.

</details>


### [665] [Adaptive Guidance for Local Training in Heterogeneous Federated Learning](https://arxiv.org/pdf/2410.06490)
*Jianqing Zhang, Yang Liu, Yang Hua, Jian Cao, Qiang Yang*

Main category: cs.LG

TL;DR: FedL2G addresses model heterogeneity in Federated Learning by adaptively guiding local training to align objectives, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Model heterogeneity in Federated Learning causes mismatches between added and local objectives, hindering collaboration.

Method: FedL2G adaptively learns to guide local training using first-order derivatives, ensuring alignment with original objectives.

Result: FedL2G achieves a non-convex convergence rate of O(1/T) and outperforms seven state-of-the-art methods in experiments.

Conclusion: FedL2G effectively resolves objective mismatches in heterogeneous Federated Learning, demonstrating strong theoretical and empirical performance.

Abstract: Model heterogeneity poses a significant challenge in Heterogeneous Federated
Learning (HtFL). In scenarios with diverse model architectures, directly
aggregating model parameters is impractical, leading HtFL methods to
incorporate an extra objective alongside the original local objective on each
client to facilitate collaboration. However, this often results in a mismatch
between the extra and local objectives. To resolve this, we propose Federated
Learning-to-Guide (FedL2G), a method that adaptively learns to guide local
training in a federated manner, ensuring the added objective aligns with each
client's original goal. With theoretical guarantees, FedL2G utilizes only
first-order derivatives w.r.t. model parameters, achieving a non-convex
convergence rate of O(1/T). We conduct extensive experiments across two data
heterogeneity and six model heterogeneity settings, using 14 heterogeneous
model architectures (e.g., CNNs and ViTs). The results show that FedL2G
significantly outperforms seven state-of-the-art methods.

</details>


### [666] [HardNet: Hard-Constrained Neural Networks with Universal Approximation Guarantees](https://arxiv.org/pdf/2410.10807)
*Youngjae Min, Navid Azizan*

Main category: cs.LG

TL;DR: HardNet is a framework for neural networks to inherently satisfy hard constraints without losing model capacity, enabling end-to-end training with guaranteed constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods use soft constraints with no guarantee of satisfaction, especially for safety-critical applications, while hard constraints may limit model performance.

Method: HardNet appends a differentiable closed-form enforcement layer to the network's output, allowing unconstrained optimization while satisfying hard constraints.

Result: HardNet ensures constraint satisfaction, retains universal approximation capabilities, and improves performance in applications like learning with constraints and safety-critical control.

Conclusion: HardNet provides a practical solution for enforcing hard constraints in neural networks without sacrificing performance or expressiveness.

Abstract: Incorporating prior knowledge or specifications of input-output relationships
into machine learning models has attracted significant attention, as it
enhances generalization from limited data and leads to conforming outputs.
However, most existing approaches use soft constraints by penalizing violations
through regularization, which offers no guarantee of constraint satisfaction,
especially on inputs far from the training distribution -- an essential
requirement in safety-critical applications. On the other hand, imposing hard
constraints on neural networks may hinder their representational power,
adversely affecting performance. To address this, we propose HardNet, a
practical framework for constructing neural networks that inherently satisfy
hard constraints without sacrificing model capacity. Unlike approaches that
modify outputs only at inference time, HardNet enables end-to-end training with
hard constraint guarantees, leading to improved performance. To the best of our
knowledge, HardNet is the first method with an efficient forward pass to
enforce more than one input-dependent inequality constraint. It allows
unconstrained optimization of the network parameters using standard algorithms
by appending a differentiable closed-form enforcement layer to the network's
output. Furthermore, we show that HardNet is expressive and retains the
universal approximation capabilities of neural networks. We demonstrate the
versatility and effectiveness of HardNet across various applications: learning
with piecewise constraints, learning optimization solvers with guaranteed
feasibility, and optimizing control policies in safety-critical systems.

</details>


### [667] [A Hitchhiker's Guide to Scaling Law Estimation](https://arxiv.org/pdf/2410.11840)
*Leshem Choshen, Yang Zhang, Jacob Andreas*

Main category: cs.LG

TL;DR: Scaling laws predict model loss efficiently, but their estimation lacks best practices. This paper collects data from 485 models, derives scaling laws, and offers guidelines for accurate estimation.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding and estimation of scaling laws for machine learning models, addressing gaps in current practices.

Method: Collects a large-scale dataset of losses and evaluations from 485 pretrained models, estimates over 1000 scaling laws, and analyzes best practices.

Result: Fitting scaling laws to intermediate checkpoints improves accuracy. Performance estimates are most accurate from similar-sized models, and variability across seeds suggests training multiple small models can be beneficial.

Conclusion: Scaling laws can be accurately estimated using intermediate checkpoints and similar-sized models, with variability addressed by training multiple models. Cross-family predictions are feasible with shared architecture and scaling parameters.

Abstract: Scaling laws predict the loss of a target machine learning model by
extrapolating from easier-to-train models with fewer parameters or smaller
training sets. This provides an efficient way for practitioners and researchers
alike to compare pretraining decisions involving optimizers, datasets, and
model architectures. Despite the widespread use of scaling laws to model the
dynamics of language model training, there has been little work on
understanding how to best estimate and interpret them. We collect (and release)
a large-scale dataset containing losses and downstream evaluations for 485
previously published pretrained models. We use these to estimate more than 1000
scaling laws, then derive a set of best practices for estimating scaling laws
in new model families. We find that fitting scaling laws to intermediate
checkpoints of training runs (and not just their final losses) substantially
improves accuracy, and that -- all else equal -- estimates of performance are
generally most accurate when derived from other models of similar sizes.
However, because there is a significant degree of variability across model
seeds, training multiple small models is sometimes more useful than training a
single large one. Moreover, while different model families differ scaling
behavior, they are often similar enough that a target model's behavior can be
predicted from a single model with the same architecture, along with scaling
parameter estimates derived from other model families.

</details>


### [668] [In-context learning and Occam's razor](https://arxiv.org/pdf/2410.14086)
*Eric Elmoznino, Tom Marty, Tejas Kasetty, Leo Gagnon, Sarthak Mittal, Mahan Fathi, Dhanya Sridhar, Guillaume Lajoie*

Main category: cs.LG

TL;DR: The paper connects Occam's razor to in-context learning, showing that next-token prediction loss in sequence models like Transformers is equivalent to prequential coding, jointly minimizing training error and model complexity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical generalization guarantees and practical simplicity (Occam's razor) in machine learning, focusing on in-context learning.

Method: Theoretical analysis linking next-token prediction loss to prequential coding, supported by empirical experiments.

Result: Demonstrates that minimizing next-token prediction loss implicitly reduces both training error and model complexity, offering insights into in-context learning.

Conclusion: Provides a normative account of in-context learning, identifies shortcomings in current methods, and suggests improvements.

Abstract: A central goal of machine learning is generalization. While the No Free Lunch
Theorem states that we cannot obtain theoretical guarantees for generalization
without further assumptions, in practice we observe that simple models which
explain the training data generalize best: a principle called Occam's razor.
Despite the need for simple models, most current approaches in machine learning
only minimize the training error, and at best indirectly promote simplicity
through regularization or architecture design. Here, we draw a connection
between Occam's razor and in-context learning: an emergent ability of certain
sequence models like Transformers to learn at inference time from past
observations in a sequence. In particular, we show that the next-token
prediction loss used to train in-context learners is directly equivalent to a
data compression technique called prequential coding, and that minimizing this
loss amounts to jointly minimizing both the training error and the complexity
of the model that was implicitly learned from context. Our theory and the
empirical experiments we use to support it not only provide a normative account
of in-context learning, but also elucidate the shortcomings of current
in-context learning methods, suggesting ways in which they can be improved. We
make our code available at https://github.com/3rdCore/PrequentialCode.

</details>


### [669] [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](https://arxiv.org/pdf/2410.16135)
*Kang Zhao, Tao Yuan, Han Bao, Zhenfeng Su, Chang Gao, Zhaofeng Sun, Zichen Liang, Liping Jing, Jianfei Chen*

Main category: cs.LG

TL;DR: The paper explores V:N:M sparsity as a more flexible and effective alternative to 2:4 sparsity for accelerating Transformers, proposing methods to enhance its applicability and accuracy.


<details>
  <summary>Details</summary>
Motivation: 2:4 sparsity has limitations like low speedups and fixed ratios, while V:N:M sparsity shows promise but lacks thorough investigation in broader Transformer models and unresolved issues like parameter selection.

Method: Three key approaches: heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques.

Result: DeiT-small achieves lossless accuracy at 64:2:5 sparsity; DeiT-base maintains accuracy at 64:2:8. LLama2-7B at 64:2:5 performs comparably or better than 2:4 sparse alternatives.

Conclusion: V:N:M sparsity offers better speedup-accuracy trade-offs than 2:4 sparsity, making it a viable solution for cost-sensitive inference in Transformers.

Abstract: To date, 2:4 sparsity has stood as the only sparse pattern that can be
accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often
possesses low actual speedups ($\leq 1.3$) and requires fixed sparse ratios,
meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity,
do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity
is promising in addressing these limitations of 2:4 sparsity. However,
regarding accuracy, the effects of V:N:M sparsity on broader Transformer
models, such as vision Transformers and large language models (LLMs), are
largely unexamined. Moreover, Some specific issues related to V:N:M sparsity,
such as how to select appropriate V and M values, remain unresolved. In this
study, we thoroughly investigate the application of V:N:M sparsity in vision
models and LLMs across multiple tasks, from pertaining to downstream tasks. We
propose three key approaches to enhance the applicability and accuracy of
V:N:M-sparse Transformers, including heuristic V and M selection,
V:N:M-specific channel permutation, and three-staged LoRA training techniques.
Experimental results show that, with our methods, the DeiT-small achieves
lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy
even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5
sparsity performs comparably or better than training-free 2:4 sparse
alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers
offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity.
Overall, our exploration largely facilitates the V:N:M sparsity to act as a
truly effective acceleration solution for Transformers in cost-sensitive
inference scenarios.

</details>


### [670] [Automatic Cross-Domain Transfer Learning for Linear Regression](https://arxiv.org/pdf/2005.04088)
*Xinshun Liu, He Xin, Mao Hui, Liu Jing, Lai Weizhong, Ye Qingwen*

Main category: cs.LG

TL;DR: This paper extends transfer learning for linear regression to cases where domain information is uncertain or unknown, using a Dirichlet process to infer latent domains and jointly modeling variables x and y to reduce bias.


<details>
  <summary>Details</summary>
Motivation: Current transfer learning assumes known domain information, limiting its applicability. This work addresses scenarios where domain details are uncertain or unavailable.

Method: The framework infers latent domains via a Dirichlet process, jointly models x and y, and performs regression on a new feature space combining latent and target domains.

Result: Experiments show the model performs well on real datasets, effectively controlling bias compared to pseudo-labeling methods.

Conclusion: The proposed method successfully extends transfer learning to uncertain domains, improving performance and reducing bias by jointly modeling x and y.

Abstract: Transfer learning research attempts to make model induction transferable
across different domains. This method assumes that specific information
regarding to which domain each instance belongs is known. This paper helps to
extend the capability of transfer learning for linear regression problems to
situations where the domain information is uncertain or unknown; in fact, the
framework can be extended to classification problems. For normal datasets, we
assume that some latent domain information is available for transfer learning.
The instances in each domain can be inferred by different parameters. We obtain
this domain information from the distribution of the regression coefficients
corresponding to the explanatory variable $x$ as well as the response variable
$y$ based on a Dirichlet process, which is more reasonable. As a result, we
transfer not only variable $x$ as usual but also variable $y$, which is
challenging since the testing data have no response value. Previous work mainly
overcomes the problem via pseudo-labelling based on transductive learning,
which introduces serious bias. We provide a novel framework for analysing the
problem and considering this general situation: the joint distribution of
variable $x$ and variable $y$. Furthermore, our method controls the bias well
compared with previous work. We perform linear regression on the new feature
space that consists of different latent domains and the target domain, which is
from the testing data. The experimental results show that the proposed model
performs well on real datasets.

</details>


### [671] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/pdf/2209.01205)
*Han Wu, Jie Yin, Bala Rajaratnam, Jianyuan Guo*

Main category: cs.LG

TL;DR: HiRe is a hierarchical relational learning method for few-shot KG completion, capturing entity-, triplet-, and context-level information to improve meta representations of relations.


<details>
  <summary>Details</summary>
Motivation: Address incompleteness and long-tail distribution in KGs by enhancing few-shot KG completion with richer relational information.

Method: Proposes HiRe, which jointly learns entity-level, triplet-level, and context-level relational information.

Result: Outperforms state-of-the-art methods on benchmark datasets.

Conclusion: HiRe effectively generalizes to unseen relations by leveraging multi-level relational information.

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [672] [Hyperband-based Bayesian Optimization for Black-box Prompt Selection](https://arxiv.org/pdf/2412.07820)
*Lennart Schneider, Martin Wistuba, Aaron Klein, Jacek Golebiowski, Giovanni Zappella, Felice Antonio Merra*

Main category: cs.LG

TL;DR: HbBoPs is a method for efficient black-box prompt selection in LLMs, combining a deep kernel Gaussian Process with Hyperband for improved performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Optimal prompt selection is critical for LLM performance, but challenging in black-box settings due to large search spaces, lack of gradients, and high evaluation costs.

Method: HbBoPs uses a structural-aware deep kernel Gaussian Process and Hyperband scheduler to efficiently select prompts by treating instructions and exemplars as modular components.

Result: HbBoPs outperforms state-of-the-art methods across ten benchmarks and three LLMs in performance and efficiency.

Conclusion: HbBoPs provides a sample-efficient and query-efficient solution for black-box prompt selection in LLMs.

Abstract: Optimal prompt selection is crucial for maximizing large language model (LLM)
performance on downstream tasks, especially in black-box settings where models
are only accessible via APIs. Black-box prompt selection is challenging due to
potentially large, combinatorial search spaces, absence of gradient
information, and high evaluation cost of prompts on a validation set. We
propose HbBoPs, a novel method that combines a structural-aware deep kernel
Gaussian Process with Hyperband as a multi-fidelity scheduler to efficiently
select prompts. HbBoPs uses embeddings of instructions and few-shot exemplars,
treating them as modular components within prompts. This enhances the surrogate
model's ability to predict which prompt to evaluate next in a sample-efficient
manner. Hyperband improves query-efficiency by adaptively allocating resources
across different fidelity levels, reducing the number of validation instances
required for evaluating prompts. Extensive experiments across ten diverse
benchmarks and three LLMs demonstrate that HbBoPs outperforms state-of-the-art
methods in both performance and efficiency.

</details>


### [673] [Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning](https://arxiv.org/pdf/2305.15612)
*Jungtaek Kim*

Main category: cs.LG

TL;DR: The paper proposes a semi-supervised learning approach for Bayesian optimization, addressing overconfidence in supervised classifiers by leveraging unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization is widely used but faces challenges with overconfidence in supervised classifiers when estimating class probabilities for global optima.

Method: The authors introduce density ratio estimation-based Bayesian optimization with semi-supervised learning, utilizing unlabeled points to improve accuracy.

Result: Empirical results demonstrate the method's effectiveness in scenarios with unlabeled point sampling and fixed-size pools.

Conclusion: The proposed semi-supervised approach enhances Bayesian optimization performance and validity across diverse experiments.

Abstract: Bayesian optimization has attracted huge attention from diverse research
areas in science and engineering, since it is capable of efficiently finding a
global optimum of an expensive-to-evaluate black-box function. In general, a
probabilistic regression model is widely used as a surrogate function to model
an explicit distribution over function evaluations given an input to estimate
and a training dataset. Beyond the probabilistic regression-based methods,
density ratio estimation-based Bayesian optimization has been suggested in
order to estimate a density ratio of the groups relatively close and relatively
far to a global optimum. Developing this line of research further, supervised
classifiers are employed to estimate a class probability for the two groups
instead of a density ratio. However, the supervised classifiers used in this
strategy are prone to be overconfident for known knowledge on global solution
candidates. Supposing that we have access to unlabeled points, e.g., predefined
fixed-size pools, we propose density ratio estimation-based Bayesian
optimization with semi-supervised learning to solve this challenge. Finally, we
show the empirical results of our methods and several baseline methods in two
distinct scenarios with unlabeled point sampling and a fixed-size pool, and
analyze the validity of our methods in diverse experiments.

</details>


### [674] [Why Shallow Networks Struggle to Approximate and Learn High Frequencies](https://arxiv.org/pdf/2306.17301)
*Shijun Zhang, Hongkai Zhao, Yimin Zhong, Haomin Zhou*

Main category: cs.LG

TL;DR: The paper analyzes why two-layer neural networks struggle with high frequencies, focusing on numerical error, computational cost, and stability under finite precision.


<details>
  <summary>Details</summary>
Motivation: To understand the limitations of two-layer neural networks in handling high frequencies due to practical constraints like machine precision and computational cost.

Method: Combines mathematical and computational analysis to study numerical error, cost, and stability, focusing on representation conditioning and learning dynamics.

Result: Provides explicit answers and numerical evidence on achievable error, required cost, and method stability.

Conclusion: Highlights the challenges of two-layer networks with high frequencies, offering insights into their practical limitations.

Abstract: In this work, we present a comprehensive study combining mathematical and
computational analysis to explain why a two-layer neural network struggles to
handle high frequencies in both approximation and learning, especially when
machine precision, numerical noise, and computational cost are significant
factors in practice. Specifically, we investigate the following fundamental
computational issues: (1) the minimal numerical error achievable under finite
precision, (2) the computational cost required to attain a given accuracy, and
(3) the stability of the method with respect to perturbations. The core of our
analysis lies in the conditioning of the representation and its learning
dynamics. Explicit answers to these questions are provided, along with
supporting numerical evidence.

</details>


### [675] [HybGRAG: Hybrid Retrieval-Augmented Generation on Textual and Relational Knowledge Bases](https://arxiv.org/pdf/2412.16311)
*Meng-Chieh Lee, Qi Zhu, Costas Mavromatis, Zhen Han, Soji Adeshina, Vassilis N. Ioannidis, Huzefa Rangwala, Christos Faloutsos*

Main category: cs.LG

TL;DR: HybGRAG is a hybrid retrieval method for answering hybrid questions using both textual and relational data from semi-structured knowledge bases, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RAG and GRAG struggle with hybrid questions requiring both textual and relational information from semi-structured knowledge bases.

Method: HybGRAG combines a retriever bank and a critic module to adaptively retrieve and refine answers, leveraging both textual and relational data.

Result: HybGRAG achieves a 51% average relative improvement in Hit@1 on the STaRK benchmark.

Conclusion: HybGRAG is an effective, adaptive, and interpretable solution for hybrid question answering over semi-structured knowledge bases.

Abstract: Given a semi-structured knowledge base (SKB), where text documents are
interconnected by relations, how can we effectively retrieve relevant
information to answer user questions? Retrieval-Augmented Generation (RAG)
retrieves documents to assist large language models (LLMs) in question
answering; while Graph RAG (GRAG) uses structured knowledge bases as its
knowledge source. However, many questions require both textual and relational
information from SKB - referred to as "hybrid" questions - which complicates
the retrieval process and underscores the need for a hybrid retrieval method
that leverages both information. In this paper, through our empirical analysis,
we identify key insights that show why existing methods may struggle with
hybrid question answering (HQA) over SKB. Based on these insights, we propose
HybGRAG for HQA consisting of a retriever bank and a critic module, with the
following advantages: (1) Agentic, it automatically refines the output by
incorporating feedback from the critic module, (2) Adaptive, it solves hybrid
questions requiring both textual and relational information with the retriever
bank, (3) Interpretable, it justifies decision making with intuitive refinement
path, and (4) Effective, it surpasses all baselines on HQA benchmarks. In
experiments on the STaRK benchmark, HybGRAG achieves significant performance
gains, with an average relative improvement in Hit@1 of 51%.

</details>


### [676] [Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients](https://arxiv.org/pdf/2307.08507)
*Mete Kemertas, Allan D. Jepson, Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: MDOT-PNCG is a novel method for solving discrete optimal transport problems efficiently, outperforming existing solvers in speed and precision.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional methods like Sinkhorn iterations in solving high-precision OT problems, especially under weak regularization.

Method: Combines temperature annealing in entropic-regularized OT with mirror descent techniques, solved using a GPU-parallel nonlinear conjugate gradients algorithm (PNCG).

Result: Achieves faster and more precise solutions than existing OT solvers, with empirical convergence rates ranging between O(n²ε⁻¹/⁴) and O(n²ε⁻¹).

Conclusion: MDOT-PNCG is a robust and efficient alternative for OT problems, particularly in weak-regularization scenarios.

Abstract: We propose Mirror Descent Optimal Transport (MDOT), a novel method for
solving discrete optimal transport (OT) problems with high precision, by
unifying temperature annealing in entropic-regularized OT (EOT) with mirror
descent techniques. In this framework, temperature annealing produces a
sequence of EOT dual problems, whose solution gradually gets closer to the
solution of the original OT problem. We solve each problem efficiently using a
GPU-parallel nonlinear conjugate gradients algorithm (PNCG) that outperforms
traditional Sinkhorn iterations under weak regularization. Moreover, our
investigation also reveals that the theoretical convergence rate of Sinkhorn
iterations can exceed existing non-asymptotic bounds when its stopping
criterion is tuned in a manner analogous to MDOT.
  Our comprehensive ablation studies of MDOT-PNCG affirm its robustness across
a wide range of algorithmic parameters. Benchmarking on 24 problem sets of size
$n=4096$ in a GPU environment demonstrate that our method attains
high-precision, feasible solutions significantly faster than a representative
set of existing OT solvers, including accelerated gradient methods and advanced
Sinkhorn variants, in both wall-clock time and number of operations. Empirical
convergence rates range between $O(n^2 \varepsilon^{-1/4})$ and $O(n^2
\varepsilon^{-1})$, where $\varepsilon$ is the optimality gap. For problem
sizes up to $n=16384$, the empirical runtime scales as $O(n^2)$ for moderate
precision and as $O(n^{5/2})$ at worst for high precision. These findings
establish MDOT-PNCG as a compelling alternative to current OT solvers,
particularly in challenging weak-regularization regimes.

</details>


### [677] [Exemplar-condensed Federated Class-incremental Learning](https://arxiv.org/pdf/2412.18926)
*Rui Sun, Yumin Zhang, Varun Ojha, Tejal Shah, Haoran Duan, Bo Wei, Rajiv Ranjan*

Main category: cs.LG

TL;DR: ECoral improves federated continual learning by distilling training characteristics into informative exemplars, addressing data heterogeneity and catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in exemplar selection for replay-based approaches in federated continual learning, particularly data heterogeneity.

Method: Uses distillation to create rehearsal exemplars, maintains gradient consistency, and shares a generative model across clients.

Result: Outperforms state-of-the-art methods and integrates well with existing approaches.

Conclusion: ECoral effectively mitigates catastrophic forgetting and handles data heterogeneity in federated continual learning.

Abstract: We propose Exemplar-Condensed federated class-incremental learning (ECoral)
to distil the training characteristics of real images from streaming data into
informative rehearsal exemplars. The proposed method eliminates the limitations
of exemplar selection in replay-based approaches for mitigating catastrophic
forgetting in federated continual learning (FCL). The limitations particularly
related to the heterogeneity of information density of each summarized data.
Our approach maintains the consistency of training gradients and the
relationship to past tasks for the summarized exemplars to represent the
streaming data compared to the original images effectively. Additionally, our
approach reduces the information-level heterogeneity of the summarized data by
inter-client sharing of the disentanglement generative model. Extensive
experiments show that our ECoral outperforms several state-of-the-art methods
and can be seamlessly integrated with many existing approaches to enhance
performance.

</details>


### [678] [Learning to Simulate: Generative Metamodeling via Quantile Regression](https://arxiv.org/pdf/2311.17797)
*L. Jeff Hong, Yanxi Hou, Qingkai Zhang, Xiaowei Zhang*

Main category: cs.LG

TL;DR: Generative metamodeling introduces a 'fast simulator of the simulator' to enable real-time decision-making by generating random outputs quickly, preserving conditional distributions. The proposed QRGMM algorithm shows superior performance in practical scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional metamodeling techniques are limited by requiring prior selection of a single output summary statistic, reducing flexibility. Generative metamodeling addresses this by enabling rapid generation of multiple random outputs.

Method: The paper proposes quantile-regression-based generative metamodeling (QRGMM), a new algorithm that constructs a fast simulator preserving conditional distributions.

Result: QRGMM demonstrates superior performance in numerical experiments compared to other generative algorithms, enabling real-time computation of any summary statistic.

Conclusion: Generative metamodeling, particularly QRGMM, offers a flexible and efficient solution for real-time decision-making by overcoming limitations of traditional metamodeling techniques.

Abstract: Stochastic simulation models effectively capture complex system dynamics but
are often too slow for real-time decision-making. Traditional metamodeling
techniques learn relationships between simulator inputs and a single output
summary statistic, such as the mean or median. These techniques enable
real-time predictions without additional simulations. However, they require
prior selection of one appropriate output summary statistic, limiting their
flexibility in practical applications. We propose a new concept: generative
metamodeling. It aims to construct a "fast simulator of the simulator,"
generating random outputs significantly faster than the original simulator
while preserving approximately equal conditional distributions. Generative
metamodels enable rapid generation of numerous random outputs upon input
specification, facilitating immediate computation of any summary statistic for
real-time decision-making. We introduce a new algorithm,
quantile-regression-based generative metamodeling (QRGMM), and establish its
distributional convergence and convergence rate. Extensive numerical
experiments demonstrate QRGMM's efficacy compared to other state-of-the-art
generative algorithms in practical real-time decision-making scenarios.

</details>


### [679] [Graph Generative Pre-trained Transformer](https://arxiv.org/pdf/2501.01073)
*Xiaohui Chen, Yinkai Wang, Jiaxing He, Yuanqi Du, Soha Hassoun, Xiaolin Xu, Li-Ping Liu*

Main category: cs.LG

TL;DR: The paper proposes G2PT, a graph generative model using sequence-based representations, achieving superior performance in generation and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of adjacency matrix representations in graph generation by introducing an efficient sequence-based approach.

Method: Introduces G2PT, an auto-regressive model using next-token prediction for graph generation, with fine-tuning for downstream tasks.

Result: G2PT outperforms on generic graph and molecule datasets and shows adaptability in downstream applications.

Conclusion: The sequence-based G2PT model is effective for graph generation and versatile for various tasks, with code publicly available.

Abstract: Graph generation is a critical task in numerous domains, including molecular
design and social network analysis, due to its ability to model complex
relationships and structured data. While most modern graph generative models
utilize adjacency matrix representations, this work revisits an alternative
approach that represents graphs as sequences of node set and edge set. We
advocate for this approach due to its efficient encoding of graphs and propose
a novel representation. Based on this representation, we introduce the Graph
Generative Pre-trained Transformer (G2PT), an auto-regressive model that learns
graph structures via next-token prediction. To further exploit G2PT's
capabilities as a general-purpose foundation model, we explore fine-tuning
strategies for two downstream applications: goal-oriented generation and graph
property prediction. We conduct extensive experiments across multiple datasets.
Results indicate that G2PT achieves superior generative performance on both
generic graph and molecule datasets. Furthermore, G2PT exhibits strong
adaptability and versatility in downstream tasks from molecular design to
property prediction. Code available at https://github.com/tufts-ml/G2PT,

</details>


### [680] [The Complexity of Sequential Prediction in Dynamical Systems](https://arxiv.org/pdf/2402.06614)
*Vinod Raman, Unique Subedi, Ambuj Tewari*

Main category: cs.LG

TL;DR: The paper explores learning to predict the next state of a dynamical system without parametric assumptions, introducing new combinatorial measures to quantify mistake and regret bounds in realizable and agnostic settings.


<details>
  <summary>Details</summary>
Motivation: To understand the learning theory perspective of predicting dynamical system states without relying on parametric assumptions.

Method: Defines new combinatorial measures and dimensions to analyze mistake and regret bounds.

Result: In the realizable setting, mistakes can grow with any increasing function of time, while in the agnostic setting, regret rates are limited to Θ(T) or Θ̃(√T).

Conclusion: The study provides theoretical insights into the bounds of prediction errors in dynamical systems under different learning settings.

Abstract: We study the problem of learning to predict the next state of a dynamical
system when the underlying evolution function is unknown. Unlike previous work,
we place no parametric assumptions on the dynamical system, and study the
problem from a learning theory perspective. We define new combinatorial
measures and dimensions and show that they quantify the optimal mistake and
regret bounds in the realizable and agnostic settings respectively. By doing
so, we find that in the realizable setting, the total number of mistakes can
grow according to \emph{any} increasing function of the time horizon $T$. In
contrast, we show that in the agnostic setting under the commonly studied
notion of Markovian regret, the only possible rates are $\Theta(T)$ and
$\tilde{\Theta}(\sqrt{T})$.

</details>


### [681] [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/pdf/2402.15319)
*Mart van Baalen, Andrey Kuzmin, Ivan Koryakovskiy, Markus Nagel, Peter Couperus, Cedric Bastoul, Eric Mahurin, Tijmen Blankevoort, Paul Whatmough*

Main category: cs.LG

TL;DR: GPTVQ improves neural network quantization by increasing dimensionality, offering better size-accuracy trade-offs for LLMs like Llama-v2 and Mistral. It's efficient, taking 3-11 hours for a 70B model on an H100, and reduces latency on mobile CPUs.


<details>
  <summary>Details</summary>
Motivation: To enhance the size versus accuracy trade-off in neural network quantization, especially for large language models (LLMs).

Method: Proposes GPTVQ, a fast post-training vector quantization method. It interleaves column quantization with weight updates using Hessian information, initializes codebooks via data-aware EM, and compresses them with integer quantization and SVD.

Result: Achieves state-of-the-art size-accuracy trade-offs for LLMs (e.g., Llama-v2, Mistral) and reduces latency on mobile CPUs compared to 4-bit integer formats.

Conclusion: GPTVQ is an efficient and effective method for improving quantization in LLMs, balancing speed, accuracy, and model size.

Abstract: In this work we show that the size versus accuracy trade-off of neural
network quantization can be significantly improved by increasing the
quantization dimensionality. We propose the GPTVQ method, a new fast method for
post-training vector quantization (VQ) that scales well to Large Language
Models (LLMs). Our method interleaves quantization of one or more columns with
updates to the remaining unquantized weights, using information from the
Hessian of the per-layer output reconstruction MSE. Quantization codebooks are
initialized using an efficient data-aware version of the EM algorithm. The
codebooks are then updated, and further compressed by using integer
quantization and SVD-based compression. GPTVQ establishes a new state-of-the
art in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2
and Mistral. Furthermore, our method is efficient: on a single H100 it takes
between 3 and 11 hours to process a Llamav2-70B model, depending on
quantization setting. Lastly, with on-device timings for VQ decompression on a
mobile CPU we show that VQ leads to improved latency compared to using a 4-bit
integer format.

</details>


### [682] [Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors](https://arxiv.org/pdf/2501.12633)
*Jingyang Ke, Feiyang Wu, Jiyi Wang, Jeffrey Markowitz, Anqi Wu*

Main category: cs.LG

TL;DR: SWIRL introduces a history-dependent IRL framework to model complex, long-term animal decision-making, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional neuroscience methods limit understanding of decision-making to short-term, explicit goals, missing intrinsic motivations in natural behaviors.

Method: SWIRL extends IRL with time-varying, history-dependent reward functions to model transitions between short-term decision processes.

Result: SWIRL outperforms non-history-dependent models in simulating and analyzing real-world animal behavior datasets.

Conclusion: SWIRL advances IRL by incorporating history dependency, improving modeling of naturalistic animal decision-making.

Abstract: Traditional approaches to studying decision-making in neuroscience focus on
simplified behavioral tasks where animals perform repetitive, stereotyped
actions to receive explicit rewards. While informative, these methods constrain
our understanding of decision-making to short timescale behaviors driven by
explicit goals. In natural environments, animals exhibit more complex,
long-term behaviors driven by intrinsic motivations that are often
unobservable. Recent works in time-varying inverse reinforcement learning (IRL)
aim to capture shifting motivations in long-term, freely moving behaviors.
However, a crucial challenge remains: animals make decisions based on their
history, not just their current state. To address this, we introduce SWIRL
(SWitching IRL), a novel framework that extends traditional IRL by
incorporating time-varying, history-dependent reward functions. SWIRL models
long behavioral sequences as transitions between short-term decision-making
processes, each governed by a unique reward function. SWIRL incorporates
biologically plausible history dependency to capture how past decisions and
environmental contexts shape behavior, offering a more accurate description of
animal decision-making. We apply SWIRL to simulated and real-world animal
behavior datasets and show that it outperforms models lacking history
dependency, both quantitatively and qualitatively. This work presents the first
IRL model to incorporate history-dependent policies and rewards to advance our
understanding of complex, naturalistic decision-making in animals.

</details>


### [683] [Open-world Machine Learning: A Systematic Review and Future Directions](https://arxiv.org/pdf/2403.01759)
*Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang, Dacheng Tao, Cheng-Lin Liu*

Main category: cs.LG

TL;DR: The paper introduces open-world machine learning, addressing challenges like unknown rejection, novelty discovery, and continual learning to build adaptable AI systems.


<details>
  <summary>Details</summary>
Motivation: Current machine learning relies on a closed-world assumption, which fails in dynamic, real-world environments. The paper aims to explore open-world learning for more robust AI.

Method: Investigates unknown rejection, novelty discovery, and continual learning in a unified paradigm, discussing challenges, principles, and methodologies.

Result: Summarizes benchmarks, metrics, and performances, highlighting the potential of open-world learning for evolving AI systems.

Conclusion: The paper advocates for open-world machine learning to advance AI adaptability and progress toward artificial general intelligence.

Abstract: Machine learning has achieved remarkable success in many applications.
However, existing studies are largely based on the closed-world assumption,
which assumes that the environment is stationary, and the model is fixed once
deployed. In many real-world applications, this fundamental and rather naive
assumption may not hold because an open environment is complex, dynamic, and
full of unknowns. In such cases, rejecting unknowns, discovering novelties, and
then continually learning them, could enable models to be safe and evolve
continually as biological systems do. This article presents a holistic view of
open-world machine learning by investigating unknown rejection, novelty
discovery, and continual learning in a unified paradigm. The challenges,
principles, and limitations of current methodologies are discussed in detail.
Furthermore, widely used benchmarks, metrics, and performances are summarized.
Finally, we discuss several potential directions for further progress in the
field. By providing a comprehensive introduction to the emerging open-world
machine learning paradigm, this article aims to help researchers build more
powerful AI systems in their respective fields, and to promote the development
of artificial general intelligence.

</details>


### [684] [Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling](https://arxiv.org/pdf/2501.13779)
*Tanya Rodchenko, Natasha Noy, Nino Scherrer*

Main category: cs.LG

TL;DR: The paper advocates for intentional data acquisition in LLMs, focusing on task-specific benefits of data scaling and leveraging data structure to guide compute paradigms.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of indiscriminate data scaling in LLMs by prioritizing tasks where data scaling is beneficial.

Method: Analyze data composition and structural patterns to identify tasks for prioritized scaling and develop efficient compute paradigms.

Result: Proposes a framework for intentional data acquisition and task prioritization in LLM training.

Conclusion: Intentional data scaling and task prioritization can optimize LLM performance and resource use.

Abstract: While Large Language Models require more and more data to train and scale,
rather than looking for any data to acquire, we should consider what types of
tasks are more likely to benefit from data scaling. We should be intentional in
our data acquisition. We argue that the shape of the data itself, such as its
compositional and structural patterns, informs which tasks to prioritize in
data scaling, and shapes the development of the next generation of compute
paradigms for tasks where data scaling is inefficient, or even insufficient.

</details>


### [685] [Learning Actionable Counterfactual Explanations in Large State Spaces](https://arxiv.org/pdf/2404.17034)
*Keziah Naggita, Matthew R. Walter, Avrim Blum*

Main category: cs.LG

TL;DR: The paper introduces high-level counterfactual explanations (CFEs) to address the specificity and misalignment of low-level CFEs with real-world actions. It proposes data-driven methods for efficient CFE generation and validates their effectiveness through empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Low-level CFEs are overly specific and misaligned with real-world actions, limiting their practicality. The paper aims to bridge this gap by introducing high-level CFEs and efficient generation methods.

Method: The authors propose three high-level CFE types and formulate single-agent CFE generation methods (weighted set cover for hl-discrete, integer linear program for hl-continuous). They also introduce data-driven approaches to learn optimal CFE generators.

Result: Empirical evaluations on healthcare and synthetic datasets show that data-driven CFE generators are accurate and resource-efficient, with high-level CFEs outperforming low-level CFEs.

Conclusion: High-level CFEs and data-driven generation methods offer practical advantages over low-level CFEs, improving recourse for negatively classified individuals.

Abstract: Recourse generators provide actionable insights, often through feature-based
counterfactual explanations (CFEs), to help negatively classified individuals
understand how to adjust their input features to achieve a positive
classification. These feature-based CFEs, which we refer to as \emph{low-level}
CFEs, are overly specific (e.g., coding experience: \(4 \to 5+\) years) and
often recommended in a feature space that doesn't straightforwardly align with
real-world actions. To bridge this gap, we introduce three novel recourse types
grounded in real-world actions: high-level continuous (\emph{hl-continuous}),
high-level discrete (\emph{hl-discrete}), and high-level ID (\emph{hl-id})
CFEs.
  We formulate single-agent CFE generation methods, where we model the
hl-discrete CFE as a solution to a weighted set cover problem and the
hl-continuous CFE as a solution to an integer linear program. Since these
methods require costly optimization per agent, we propose data-driven CFE
generation approaches that, given instances of agents and their optimal CFEs,
learn a CFE generator that quickly provides optimal CFEs for new agents. This
approach, also viewed as one of learning an optimal policy in a family of large
but deterministic MDPs, considers several problem formulations, including
formulations in which the actions and their effects are unknown, and therefore
addresses informational and computational challenges.
  We conduct extensive empirical evaluations using healthcare datasets (BRFSS,
Foods, and NHANES) and fully-synthetic data. For negatively classified agents
identified by linear or threshold-based classifiers, we compare the high-level
CFE to low-level CFEs and assess the effectiveness of our network-based,
data-driven approaches. Results show that the data-driven CFE generators are
accurate, and resource-efficient, and high-level CFEs offer key advantages over
low-level CFEs.

</details>


### [686] [CAND: Cross-Domain Ambiguity Inference for Early Detecting Nuanced Illness Deterioration](https://arxiv.org/pdf/2501.16365)
*Lo Pang-Yun Ting, Zhen Tan, Hong-Pei Chen, Cheng-Te Li, Po-Lin Chen, Kun-Ta Chuang, Huan Liu*

Main category: cs.LG

TL;DR: CAND is a novel method for early detection of nuanced illness deterioration by modeling transition relationships and correlations within and among vital signs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore transition relationships and correlation strengths among vital signs, missing early signs of patient deterioration.

Method: CAND organizes transition relationships and correlations as domain-specific and cross-domain knowledge, jointly modeling them in a unified space and using Bayesian inference to address ambiguities.

Result: CAND significantly outperforms existing methods in effectiveness and earliness of detecting nuanced illness deterioration on a real-world ICU dataset.

Conclusion: CAND provides a more holistic and accurate interpretation of patient health, with practical interpretability demonstrated in a case study.

Abstract: Early detection of patient deterioration is essential for timely treatment,
with vital signs like heart rates being key health indicators. Existing methods
tend to solely analyze vital sign waveforms, ignoring transition relationships
of waveforms within each vital sign and the correlation strengths among various
vital signs. Such studies often overlook nuanced illness deterioration, which
is the early sign of worsening health but is difficult to detect. In this
paper, we introduce CAND, a novel method that organizes the transition
relationships and the correlations within and among vital signs as
domain-specific and cross-domain knowledge. CAND jointly models these knowledge
in a unified representation space, considerably enhancing the early detection
of nuanced illness deterioration. In addition, CAND integrates a Bayesian
inference method that utilizes augmented knowledge from domain-specific and
cross-domain knowledge to address the ambiguities in correlation strengths.
With this architecture, the correlation strengths can be effectively inferred
to guide joint modeling and enhance representations of vital signs. This allows
a more holistic and accurate interpretation of patient health. Our experiments
on a real-world ICU dataset demonstrate that CAND significantly outperforms
existing methods in both effectiveness and earliness in detecting nuanced
illness deterioration. Moreover, we conduct a case study for the interpretable
detection process to showcase the practicality of CAND.

</details>


### [687] [Learning from True-False Labels via Multi-modal Prompt Retrieving](https://arxiv.org/pdf/2405.15228)
*Zhongnian Li, Jinghao Xu, Peng Ying, Meng Wei, Xinzheng Xu*

Main category: cs.LG

TL;DR: The paper introduces True-False Labels (TFLs) for weakly supervised learning using Vision-Language Models (VLMs), achieving high accuracy with a risk-consistent estimator and a Multi-modal Prompt Retrieving (MRP) method.


<details>
  <summary>Details</summary>
Motivation: Existing weakly supervised methods fail to generate accurate labels via VLMs, prompting the need for a more effective labeling setting.

Method: Proposes TFLs for label generation and a risk-consistent estimator, alongside the MRP method to align VLM knowledge with target tasks.

Result: Experiments confirm TFLs and MRP improve weakly supervised labeling accuracy.

Conclusion: TFLs and MRP effectively leverage VLMs for accurate weakly supervised learning.

Abstract: Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot
classification abilities, demonstrating great potential for generating weakly
supervised labels. Unfortunately, existing weakly supervised learning methods
are short of ability in generating accurate labels via VLMs. In this paper, we
propose a novel weakly supervised labeling setting, namely True-False Labels
(TFLs) which can achieve high accuracy when generated by VLMs. The TFL
indicates whether an instance belongs to the label, which is randomly and
uniformly sampled from the candidate label set. Specifically, we theoretically
derive a risk-consistent estimator to explore and utilize the conditional
probability distribution information of TFLs. Besides, we propose a
convolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the
gap between the knowledge of VLMs and target learning tasks. Experimental
results demonstrate the effectiveness of the proposed TFL setting and MRP
learning method. The code to reproduce the experiments is at
https://github.com/Tranquilxu/TMP.

</details>


### [688] [Principal Components for Neural Network Initialization](https://arxiv.org/pdf/2501.19114)
*Nhan Phan, Thu Nguyen, Pål Halvorsen, Michael A. Riegler*

Main category: cs.LG

TL;DR: The paper proposes PCsInit, a method to integrate PCA into neural network initialization, simplifying XAI explanations and improving training.


<details>
  <summary>Details</summary>
Motivation: PCA complicates XAI explanations when used before neural network training; the authors aim to address this issue.

Method: Introduces PCsInit, PCsInit-Act, and PCsInit-Sub, which initialize the first layer of a neural network with PCA components.

Result: The proposed methods simplify XAI explanations and enhance training performance via backpropagation.

Conclusion: PCsInit and its variants offer a more straightforward and effective approach to combining PCA with neural networks.

Abstract: Principal Component Analysis (PCA) is a commonly used tool for dimension
reduction and denoising. Therefore, it is also widely used on the data prior to
training a neural network. However, this approach can complicate the
explanation of explainable AI (XAI) methods for the decision of the model. In
this work, we analyze the potential issues with this approach and propose
Principal Components-based Initialization (PCsInit), a strategy to incorporate
PCA into the first layer of a neural network via initialization of the first
layer in the network with the principal components, and its two variants
PCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct
and straightforward as for neural networks and are simpler than using PCA prior
to training a neural network on the principal components. Moreover, as will be
illustrated in the experiments, such training strategies can also allow further
improvement of training via backpropagation.

</details>


### [689] [Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization](https://arxiv.org/pdf/2405.15861)
*Zhe Li, Bicheng Ying, Zidong Liu, Chaosheng Dong, Haibo Yang*

Main category: cs.LG

TL;DR: DeComFL reduces FL communication costs from O(d) to O(1) using zeroth-order optimization, achieving dimension-free efficiency and state-of-the-art convergence rates.


<details>
  <summary>Details</summary>
Motivation: High communication costs in FL, scaling linearly with model dimension, hinder efficiency, especially for large models.

Method: Proposes DeComFL, a dimension-free algorithm using zeroth-order optimization to transmit only constant scalar values per round.

Result: Achieves linear speedup in convergence, reduces communication overhead (e.g., 1MB for billion-parameter models), and maintains performance.

Conclusion: DeComFL effectively addresses FL's communication bottleneck, enabling efficient large-scale and privacy-preserving ML.

Abstract: Federated Learning (FL) offers a promising framework for collaborative and
privacy-preserving machine learning across distributed data sources. However,
the substantial communication costs associated with FL significantly challenge
its efficiency. Specifically, in each communication round, the communication
costs scale linearly with the model's dimension, which presents a formidable
obstacle, especially in large model scenarios. Despite various
communication-efficient strategies, the intrinsic dimension-dependent
communication cost remains a major bottleneck for current FL implementations.
This paper proposes a novel dimension-free communication algorithm - DeComFL,
which leverages the zeroth-order optimization techniques and reduces the
communication cost from $\mathscr{O}(d)$ to $\mathscr{O}(1)$ by transmitting
only a constant number of scalar values between clients and the server in each
round, regardless of the dimension $d$ of the model parameters. Theoretically,
in non-convex functions, we prove that our algorithm achieves state-of-the-art
rates, which show a linear speedup of the number of clients and local steps
under standard assumptions. With additional low effective rank assumption, we
can further show the convergence rate is independent of the model dimension $d$
as well. Empirical evaluations, encompassing both classic deep learning
training and large language model fine-tuning, demonstrate significant
reductions in communication overhead. Notably, DeComFL achieves this by
transmitting only around 1MB of data in total between the server and a client
to fine-tune a model with billions of parameters. Our code is available at
https://github.com/ZidongLiu/DeComFL.

</details>


### [690] [Understanding Federated Learning from IID to Non-IID dataset: An Experimental Study](https://arxiv.org/pdf/2502.00182)
*Jungwon Seo, Ferhat Ozgur Catak, Chunming Rong*

Main category: cs.LG

TL;DR: The paper investigates performance degradation in federated learning (FL) due to non-IID data, identifying inconsistencies in client loss landscapes as the primary cause. It categorizes existing solutions into two strategies and provides insights for future research.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of non-IID data in FL, which reduces model performance compared to centralized learning.

Method: Comprehensive analysis from gradient descent to FL, focusing on IID vs. non-IID data settings.

Result: Identifies client loss landscape inconsistencies as the main issue and groups existing methods into two strategies: adjusting parameter update paths and modifying client loss landscapes.

Conclusion: Offers a clear perspective on tackling non-IID challenges in FL, guiding future research directions.

Abstract: As privacy concerns and data regulations grow, federated learning (FL) has
emerged as a promising approach for training machine learning models across
decentralized data sources without sharing raw data. However, a significant
challenge in FL is that client data are often non-IID (non-independent and
identically distributed), leading to reduced performance compared to
centralized learning. While many methods have been proposed to address this
issue, their underlying mechanisms are often viewed from different
perspectives. Through a comprehensive investigation from gradient descent to
FL, and from IID to non-IID data settings, we find that inconsistencies in
client loss landscapes primarily cause performance degradation in non-IID
scenarios. From this understanding, we observe that existing methods can be
grouped into two main strategies: (i) adjusting parameter update paths and (ii)
modifying client loss landscapes. These findings offer a clear perspective on
addressing non-IID challenges in FL and help guide future research in the
field.

</details>


### [691] [Flow map matching with stochastic interpolants: A mathematical framework for consistency models](https://arxiv.org/pdf/2406.07507)
*Nicholas M. Boffi, Michael S. Albergo, Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: Flow Map Matching (FMM) provides a theoretical framework for efficient one-step or few-step generation in dynamical generative models, unifying existing approaches and significantly reducing generation time.


<details>
  <summary>Details</summary>
Motivation: The lack of a comprehensive theoretical framework for consistency models hinders systematic understanding and design, despite their practical success in efficient generation.

Method: FMM learns the two-time flow map of a dynamical generative model using stochastic interpolants, with training objectives for distillation and direct training.

Result: FMM achieves sample quality comparable to flow matching while reducing generation time by 10-20 times on datasets like CIFAR-10 and ImageNet-32.

Conclusion: FMM offers a principled foundation for fast sampling, unifying and extending existing methods, and demonstrates practical efficiency in generation.

Abstract: Generative models based on dynamical equations such as flows and diffusions
offer exceptional sample quality, but require computationally expensive
numerical integration during inference. The advent of consistency models has
enabled efficient one-step or few-step generation, yet despite their practical
success, a systematic understanding of their design has been hindered by the
lack of a comprehensive theoretical framework. Here we introduce Flow Map
Matching (FMM), a principled framework for learning the two-time flow map of an
underlying dynamical generative model, thereby providing this missing
mathematical foundation. Leveraging stochastic interpolants, we propose
training objectives both for distillation from a pre-trained velocity field and
for direct training of a flow map over an interpolant or a forward diffusion
process. Theoretically, we show that FMM unifies and extends a broad class of
existing approaches for fast sampling, including consistency models,
consistency trajectory models, and progressive distillation. Experiments on
CIFAR-10 and ImageNet-32 highlight that our approach can achieve sample quality
comparable to flow matching while reducing generation time by a factor of
10-20.

</details>


### [692] [Logits are All We Need to Adapt Closed Models](https://arxiv.org/pdf/2502.06806)
*Gaurush Hiranandani, Haolun Wu, Subhojyoti Mukherjee, Sanmi Koyejo*

Main category: cs.LG

TL;DR: A framework for token-level probability reweighting in black-box LLMs is proposed, enabling task-specific adaptation without model access, using logits and minimal task data.


<details>
  <summary>Details</summary>
Motivation: Closed-source LLMs limit adaptation to prompt tuning; access to token logits could unlock more powerful adaptation techniques.

Method: Token-level probability reweighting treats next-token prediction as supervised classification, framing adaptation as label noise correction.

Result: The Plugin model effectively steers LLMs toward task-specific content generation, validated across datasets and models.

Conclusion: Reweighting logits suffices for task adaptation, advocating for broader logit access in closed-source LLMs.

Abstract: Many commercial Large Language Models (LLMs) are often closed-source,
limiting developers to prompt tuning for aligning content generation with
specific applications. While these models currently do not provide access to
token logits, we argue that if such access were available, it would enable more
powerful adaptation techniques beyond prompt engineering. In this paper, we
propose a token-level probability reweighting framework that, given access to
logits and a small amount of task-specific data, can effectively steer
black-box LLMs toward application-specific content generation. Our approach
views next-token prediction through the lens of supervised classification. We
show that aligning black-box LLMs with task-specific data can be formulated as
a label noise correction problem, leading to Plugin model -- an autoregressive
probability reweighting model that operates solely on logits. We provide
theoretical justification for why reweighting logits alone is sufficient for
task adaptation. Extensive experiments with multiple datasets, LLMs, and
reweighting models demonstrate the effectiveness of our method, advocating for
broader access to token logits in closed-source models.

</details>


### [693] [Structured and Balanced Multi-Component and Multi-Layer Neural Networks](https://arxiv.org/pdf/2407.00765)
*Shijun Zhang, Hongkai Zhao, Yimin Zhong, Haomin Zhou*

Main category: cs.LG

TL;DR: Proposes a balanced multi-component, multi-layer neural network (MMNN) for efficient and accurate function approximation, reducing training parameters and improving accuracy over traditional networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating functions with complex features efficiently, balancing degrees of freedom and computational cost.

Method: Combines multi-component (each approximated by a single-layer network) and multi-layer decomposition strategies to capture complexity.

Result: MMNNs reduce training parameters, enhance training efficiency, and improve accuracy, especially for oscillatory functions and localized features.

Conclusion: MMNNs offer a promising alternative to FCNNs/MLPs for complex function approximation, with demonstrated effectiveness in numerical experiments.

Abstract: In this work, we propose a balanced multi-component and multi-layer neural
network (MMNN) structure to accurately and efficiently approximate functions
with complex features, in terms of both degrees of freedom and computational
cost. The main idea is inspired by a multi-component approach, in which each
component can be effectively approximated by a single-layer network, combined
with a multi-layer decomposition strategy to capture the complexity of the
target function. Although MMNNs can be viewed as a simple modification of fully
connected neural networks (FCNNs) or multi-layer perceptrons (MLPs) by
introducing balanced multi-component structures, they achieve a significant
reduction in training parameters, a much more efficient training process, and
improved accuracy compared to FCNNs or MLPs. Extensive numerical experiments
demonstrate the effectiveness of MMNNs in approximating highly oscillatory
functions and their ability to automatically adapt to localized features.

</details>


### [694] [Diffusion Models for Tabular Data Imputation and Synthetic Data Generation](https://arxiv.org/pdf/2407.02549)
*Mario Villaizán-Vallelado, Matteo Salvatori, Carlos Segura, Ioannis Arapakis*

Main category: cs.LG

TL;DR: A diffusion model for tabular data with three enhancements: conditioning attention, encoder-decoder transformer, and dynamic masking, evaluated for imputation and generation tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing incomplete/missing data in domains like healthcare and finance, leveraging diffusion models for tabular data.

Method: Proposes a diffusion model with conditioning attention, transformer denoising, and dynamic masking for unified imputation and generation.

Result: Outperforms state-of-the-art methods (VAEs, GANs, other diffusion models) in ML efficiency, statistical similarity, and privacy risk.

Conclusion: The model effectively handles tabular data tasks, offering improved performance and versatility in imputation and generation.

Abstract: Data imputation and data generation have important applications for many
domains, like healthcare and finance, where incomplete or missing data can
hinder accurate analysis and decision-making. Diffusion models have emerged as
powerful generative models capable of capturing complex data distributions
across various data modalities such as image, audio, and time series data.
Recently, they have been also adapted to generate tabular data. In this paper,
we propose a diffusion model for tabular data that introduces three key
enhancements: (1) a conditioning attention mechanism, (2) an encoder-decoder
transformer as the denoising network, and (3) dynamic masking. The conditioning
attention mechanism is designed to improve the model's ability to capture the
relationship between the condition and synthetic data. The transformer layers
help model interactions within the condition (encoder) or synthetic data
(decoder), while dynamic masking enables our model to efficiently handle both
missing data imputation and synthetic data generation tasks within a unified
framework. We conduct a comprehensive evaluation by comparing the performance
of diffusion models with transformer conditioning against state-of-the-art
techniques, such as Variational Autoencoders, Generative Adversarial Networks
and Diffusion Models, on benchmark datasets. Our evaluation focuses on the
assessment of the generated samples with respect to three important criteria,
namely: (1) Machine Learning efficiency, (2) statistical similarity, and (3)
privacy risk mitigation. For the task of data imputation, we consider the
efficiency of the generated samples across different levels of missing
features.

</details>


### [695] [No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models](https://arxiv.org/pdf/2407.02687)
*Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, Romann M. Weber*

Main category: cs.LG

TL;DR: The paper introduces Independent Condition Guidance (ICG) and Time-Step Guidance (TSG) to improve diffusion models without special training, matching CFG's performance and extending guidance to unconditional models.


<details>
  <summary>Details</summary>
Motivation: Classifier-free guidance (CFG) requires extra training steps or unconditional models, limiting its flexibility. The paper aims to simplify and generalize guidance methods.

Method: Proposes ICG for conditional models without special training and TSG for unconditional models, leveraging time-step information. Both methods are easy to implement.

Result: ICG matches CFG's performance, and TSG improves generation quality in unconditional models, both with no added sampling cost.

Conclusion: ICG and TSG offer efficient, flexible alternatives to CFG, enhancing diffusion models without training constraints.

Abstract: Classifier-free guidance (CFG) has become the standard method for enhancing
the quality of conditional diffusion models. However, employing CFG requires
either training an unconditional model alongside the main diffusion model or
modifying the training procedure by periodically inserting a null condition.
There is also no clear extension of CFG to unconditional models. In this paper,
we revisit the core principles of CFG and introduce a new method, independent
condition guidance (ICG), which provides the benefits of CFG without the need
for any special training procedures. Our approach streamlines the training
process of conditional diffusion models and can also be applied during
inference on any pre-trained conditional model. Additionally, by leveraging the
time-step information encoded in all diffusion networks, we propose an
extension of CFG, called time-step guidance (TSG), which can be applied to any
diffusion model, including unconditional ones. Our guidance techniques are easy
to implement and have the same sampling cost as CFG. Through extensive
experiments, we demonstrate that ICG matches the performance of standard CFG
across various conditional diffusion models. Moreover, we show that TSG
improves generation quality in a manner similar to CFG, without relying on any
conditional information.

</details>


### [696] [Improving Graph Out-of-distribution Generalization Beyond Causality](https://arxiv.org/pdf/2407.10204)
*Can Xu, Yao Cheng, Jianxiang Yu, Haosen Wang, Jingsong Lv, Yao Liu, Xiang Li*

Main category: cs.LG

TL;DR: DEROG introduces a variational inference method for graph OOD generalization, addressing environment-label dependency and mutable rationale invariance, outperforming existing methods on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on synthetic data and causal relationships, neglecting real-world environmental roles. This paper aims to address this gap by incorporating environment-label dependency and mutable rationale invariance.

Method: DEROG uses generalized Bayesian inference to handle unknown priors and an EM-based algorithm for optimization, focusing on real-world data.

Result: Extensive experiments show DEROG's superiority under various distribution shifts on real-world datasets.

Conclusion: DEROG effectively addresses limitations of prior methods by considering environment and rationale dependencies, demonstrating strong performance in real-world scenarios.

Abstract: Existing methods for graph out-of-distribution (OOD) generalization primarily
rely on empirical studies on synthetic datasets. Such approaches tend to
overemphasize the causal relationships between invariant sub-graphs and labels,
thereby neglecting the non-negligible role of environment in real-world
scenarios. In contrast to previous studies that impose rigid independence
assumptions on environments and invariant sub-graphs, this paper presents the
theorems of environment-label dependency and mutable rationale invariance,
where the former characterizes the usefulness of environments in determining
graph labels while the latter refers to the mutable importance of graph
rationales. Based on analytic investigations, a novel variational inference
based method named ``Probability Dependency on Environments and Rationales for
OOD Graphs on Real-world Data'' (DEROG) is introduced. To alleviate the adverse
effect of unknown prior knowledge on environments and rationales, DEROG
utilizes generalized Bayesian inference. Further, DEROG employs an EM-based
algorithm for optimization. Finally, extensive experiments on real-world
datasets under different distribution shifts are conducted to show the
superiority of DEROG.

</details>


### [697] [When Heterophily Meets Heterogeneity: Challenges and a New Large-Scale Graph Benchmark](https://arxiv.org/pdf/2407.10916)
*Junhong Lin, Xiaojie Guo, Shuaicheng Zhang, Yada Zhu, Julian Shun*

Main category: cs.LG

TL;DR: H2GB is a new benchmark for node-classification in graphs with both heterogeneity and heterophily, addressing gaps in existing benchmarks. It includes 9 datasets, 28 baselines, and a standardized framework. Current methods struggle, and a new model, H2G-former, is introduced.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on either heterophilic homogeneous graphs or homophilic heterogeneous graphs, missing the complexity of real-world graphs with both properties.

Method: H2GB introduces a large-scale benchmark with 9 datasets, 28 baselines, and a unified library for standardized evaluation and modeling.

Result: Experiments show current methods struggle with heterophilic and heterogeneous graphs. H2G-former, a new model, performs well.

Conclusion: H2GB fills a critical gap in graph benchmarking, and H2G-former demonstrates the potential for improved approaches.

Abstract: Graph mining has become crucial in fields such as social science, finance,
and cybersecurity. Many large-scale real-world networks exhibit both
heterogeneity, where multiple node and edge types exist in the graph, and
heterophily, where connected nodes may have dissimilar labels and attributes.
However, existing benchmarks primarily focus on either heterophilic homogeneous
graphs or homophilic heterogeneous graphs, leaving a significant gap in
understanding how models perform on graphs with both heterogeneity and
heterophily. To bridge this gap, we introduce H2GB, a large-scale
node-classification graph benchmark that brings together the complexities of
both the heterophily and heterogeneity properties of real-world graphs. H2GB
encompasses 9 real-world datasets spanning 5 diverse domains, 28 baseline
models, and a unified benchmarking library with a standardized data loader,
evaluator, unified modeling framework, and an extensible framework for
reproducibility. We establish a standardized workflow supporting both model
selection and development, enabling researchers to easily benchmark graph
learning methods. Extensive experiments across 28 baselines reveal that current
methods struggle with heterophilic and heterogeneous graphs, underscoring the
need for improved approaches. Finally, we present a new variant of the model,
H2G-former, developed following our standardized workflow, that excels at this
challenging benchmark. Both the benchmark and the framework are publicly
available at Github and PyPI, with documentation hosted at
https://junhongmit.github.io/H2GB.

</details>


### [698] [GFlowNet Training by Policy Gradients](https://arxiv.org/pdf/2408.05885)
*Puhua Niu, Shili Wu, Mingzhou Fan, Xiaoning Qian*

Main category: cs.LG

TL;DR: A new GFlowNet training framework with policy-dependent rewards bridges flow balance to RL reward optimization, enabling policy-based methods and joint forward-backward policy training.


<details>
  <summary>Details</summary>
Motivation: To improve GFlowNet performance by connecting flow balance to RL reward optimization and addressing backward policy design inefficiencies.

Method: Proposes policy-dependent rewards, derives policy-based training methods, and introduces a coupled strategy for joint forward and backward policy training.

Result: Theoretical guarantees and experiments on simulated/real-world datasets show improved performance and robust gradient estimation.

Conclusion: Policy-based GFlowNet training offers advanced RL perspectives, enhancing performance and gradient robustness.

Abstract: Generative Flow Networks (GFlowNets) have been shown effective to generate
combinatorial objects with desired properties. We here propose a new GFlowNet
training framework, with policy-dependent rewards, that bridges keeping flow
balance of GFlowNets to optimizing the expected accumulated reward in
traditional Reinforcement-Learning (RL). This enables the derivation of new
policy-based GFlowNet training methods, in contrast to existing ones resembling
value-based RL. It is known that the design of backward policies in GFlowNet
training affects efficiency. We further develop a coupled training strategy
that jointly solves GFlowNet forward policy training and backward policy
design. Performance analysis is provided with a theoretical guarantee of our
policy-based GFlowNet training. Experiments on both simulated and real-world
datasets verify that our policy-based strategies provide advanced RL
perspectives for robust gradient estimation to improve GFlowNet performance.

</details>


### [699] [Dynamic Search for Inference-Time Alignment in Diffusion Models](https://arxiv.org/pdf/2503.02039)
*Xiner Li, Masatoshi Uehara, Xingyu Su, Gabriele Scalia, Tommaso Biancalani, Aviv Regev, Sergey Levine, Shuiwang Ji*

Main category: cs.LG

TL;DR: DSearch frames diffusion model alignment as a search problem, dynamically optimizing reward alignment during inference.


<details>
  <summary>Details</summary>
Motivation: Aligning diffusion model outputs with non-differentiable reward functions is challenging, and existing gradient-free methods often fail to achieve optimal alignment.

Method: Proposes DSearch, which treats alignment as a search problem, subsamples denoising processes, approximates rewards, and dynamically adjusts beam width and tree expansion. Uses adaptive scheduling and lookahead heuristics.

Result: Validated in biological sequence design, molecular optimization, and image generation, showing superior reward optimization.

Conclusion: DSearch effectively addresses inference-time alignment in diffusion models, outperforming existing methods.

Abstract: Diffusion models have shown promising generative capabilities across diverse
domains, yet aligning their outputs with desired reward functions remains a
challenge, particularly in cases where reward functions are non-differentiable.
Some gradient-free guidance methods have been developed, but they often
struggle to achieve optimal inference-time alignment. In this work, we newly
frame inference-time alignment in diffusion as a search problem and propose
Dynamic Search for Diffusion (DSearch), which subsamples from denoising
processes and approximates intermediate node rewards. It also dynamically
adjusts beam width and tree expansion to efficiently explore high-reward
generations. To refine intermediate decisions, DSearch incorporates adaptive
scheduling based on noise levels and a lookahead heuristic function. We
validate DSearch across multiple domains, including biological sequence design,
molecular optimization, and image generation, demonstrating superior reward
optimization compared to existing approaches.

</details>


### [700] [LEVIS: Large Exact Verifiable Input Spaces for Neural Networks](https://arxiv.org/pdf/2408.08824)
*Mohamad Fares El Hajj Chehade, Wenting Li, Brian W. Bell, Russell Bent, Saif R. Kazi, Hao Zhu*

Main category: cs.LG

TL;DR: The paper proposes LEVIS, a framework for identifying verifiable input spaces in neural networks, addressing challenges like high dimensionality and non-convexity. It includes two components (LEVIS-α and LEVIS-β) and demonstrates performance gains in applications like power flow regression and image classification.


<details>
  <summary>Details</summary>
Motivation: Ensuring neural network robustness in safety-critical applications requires reliable input space identification, which is challenging due to high dimensionality and non-convexity.

Method: LEVIS framework: LEVIS-α identifies large verifiable balls intersecting boundaries, while LEVIS-β captures the entire verifiable space. Uses MIP for adversarial point computation and CC optimization for scalability.

Result: Achieves up to 6x runtime reduction, provides geometric insights, and validates performance in power flow regression and image classification.

Conclusion: LEVIS effectively addresses input space verification challenges, offering scalable solutions and practical insights for robust neural network applications.

Abstract: The robustness of neural networks is crucial in safety-critical applications,
where identifying a reliable input space is essential for effective model
selection, robustness evaluation, and the development of reliable control
strategies. Most existing robustness verification methods assess the worst-case
output under the assumption that the input space is known. However, precisely
identifying a verifiable input space \(\mathcal{C}\), where no adversarial
examples exist, is challenging due to the possible high dimensionality,
discontinuity, and non-convex nature of the input space. To address this
challenge, we propose a novel framework, **LEVIS**, consisting of
**LEVIS-{\alpha}** and **LEVIS-\b{eta}**. **LEVIS-{\alpha}** identifies a
single, large verifiable ball that intersects at least two boundaries of a
bounded region \(\mathcal{C}\), while **LEVIS-\b{eta}** systematically captures
the entirety of the verifiable space by integrating multiple verifiable balls.
Our contributions include: (1) introducing a verification framework that uses
mixed-integer programming (MIP) to compute nearest and directional adversarial
points, (2) integrating complementarity-constrained (CC) optimization with a
reduced MIP formulation for scalability, achieving up to a 6 times runtime
reduction, (3) theoretically characterizing the properties of the verifiable
balls obtained by **LEVIS-{\alpha}**, and (4) validating the approach across
applications including electrical power flow regression and image
classification, with demonstrated performance gains and geometric insights into
the verifiable region.

</details>


### [701] [DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models](https://arxiv.org/pdf/2503.04472)
*Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Zhaoxiang Liu, Shiguo Lian*

Main category: cs.LG

TL;DR: DAST adapts reasoning length to problem difficulty, reducing overthinking by 30% without sacrificing accuracy on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Address overthinking in slow thinking models, which wastes resources on simple problems while risking underperformance on complex ones.

Method: Introduces Difficulty-Adaptive Slow Thinking (DAST) with Token Length Budget (TLB) metric, budget-aware reward shaping, and budget preference optimization.

Result: Reduces token usage by over 30% on average while maintaining accuracy on complex tasks.

Conclusion: DAST effectively balances reasoning efficiency and performance, offering a practical solution for adaptive reasoning models.

Abstract: Recent advancements in slow thinking reasoning models have shown exceptional
performance in complex reasoning tasks. However, these models often exhibit
overthinking (generating redundant reasoning steps for simple problems),
leading to excessive computational resource usage. While current mitigation
strategies uniformly reduce reasoning tokens, they risk degrading performance
on challenging tasks that require extended reasoning. This paper introduces
Difficulty-Adaptive Slow Thinking (DAST), a novel framework that enables models
to autonomously adjust the length of Chain-of-Thought (CoT) based on problem
difficulty. We first propose a Token Length Budget (TLB) metric to quantify
difficulty, then leverage budget-aware reward shaping and budget preference
optimization to implement DAST. DAST penalizes overlong responses for simple
tasks while incentivizing sufficient reasoning for complex problems.
Experiments on diverse datasets and model scales demonstrate that DAST
effectively mitigates overthinking (reducing token usage by over 30\% on
average) while preserving reasoning accuracy on complex problems. Our codes and
models are available at https://github.com/AnonymousUser0520/AnonymousRepo01.

</details>


### [702] [SDE: A Simplified and Disentangled Dependency Encoding Framework for State Space Models in Time Series Forecasting](https://arxiv.org/pdf/2408.12068)
*Zixuan Weng, Jindong Han, Wenzhao Jiang, Hao Liu*

Main category: cs.LG

TL;DR: The paper proposes SDE, a framework to enhance State Space Models (SSMs) for Long-term Time Series Forecasting (LTSF) by simplifying nonlinearities and disentangling dependencies, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing LTSF models fail to fully capture complex dependencies in time series data, leading to noise and degraded performance. SSMs show promise but suffer from redundancy due to excessive nonlinearity.

Method: The SDE framework simplifies SSMs by removing unnecessary nonlinearities and introduces a disentangled encoding strategy to model cross-variate dependencies without interference.

Result: Experiments on nine datasets show SDE-enhanced SSMs outperform state-of-the-art models.

Conclusion: SDE effectively improves SSMs for LTSF by addressing dependency modeling and redundancy, validated by theoretical and empirical results.

Abstract: In recent years, advancements in deep learning have spurred the development
of numerous models for Long-term Time Series Forecasting (LTSF). However, most
existing approaches struggle to fully capture the complex and structured
dependencies inherent in time series data. In this work, we identify and
formally define three critical dependencies that are fundamental to forecasting
accuracy: order dependency and semantic dependency along the temporal
dimension, as well as cross-variate dependency across the feature dimension.
These dependencies are often treated in isolation, and improper handling can
introduce noise and degrade forecasting performance. To bridge this gap, we
investigate the potential of State Space Models (SSMs) for LTSF and emphasize
their inherent advantages in capturing these essential dependencies.
Additionally, we empirically observe that excessive nonlinearity in
conventional SSMs introduce redundancy when applied to semantically sparse time
series data. Motivated by this insight, we propose SDE (Simplified and
Disentangled Dependency Encoding), a novel framework designed to enhance the
capability of SSMs for LTSF. Specifically, we first eliminate unnecessary
nonlinearities in vanilla SSMs, thereby improving the suitability for time
series forecasting. Building on this foundation, we introduce a disentangled
encoding strategy, which empowers SSMs to efficiently model cross-variate
dependencies while mitigating interference between the temporal and feature
dimensions. Furthermore, we provide rigorous theoretical justifications to
substantiate our design choices. Extensive experiments on nine real-world
benchmark datasets demonstrate that SDE-enhanced SSMs consistently outperform
state-of-the-art time series forecasting models.Our code is available at
https://github.com/YukinoAsuna/SAMBA.

</details>


### [703] [Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions](https://arxiv.org/pdf/2408.16245)
*Sully F. Chen, Robert J. Steele, Glen M. Hocky, Beakal Lemeneh, Shivanand P. Lad, Eric K. Oermann*

Main category: cs.LG

TL;DR: OmniBioTE is a multi-omic transformer model trained on mixed protein and nucleic acid data, outperforming single-omic models in cross-modal tasks and structural predictions.


<details>
  <summary>Details</summary>
Motivation: Single-omic models limit cross-modal interactions; OmniBioTE aims to unify protein and nucleic acid modeling for better biological insights.

Method: Trained on 250B tokens of unlabeled mixed protein/nucleic acid data, OmniBioTE learns joint representations and predicts binding energy (ΔG).

Result: State-of-the-art ΔG prediction, emergent structural learning, and superior performance-per-FLOP compared to single-omic models.

Conclusion: Multi-omic transformers like OmniBioTE offer unified, efficient, and accurate modeling for biological sequences.

Abstract: The transformer architecture has revolutionized bioinformatics and driven
progress in the understanding and prediction of the properties of biomolecules.
To date, most biosequence transformers have been trained on a single
omic-either proteins or nucleic acids and have seen incredible success in
downstream tasks in each domain with particularly noteworthy breakthroughs in
protein structural modeling. However, single-omic pre-training limits the
ability of these models to capture cross-modal interactions. Here we present
OmniBioTE, the largest open-source multi-omic model trained on over 250 billion
tokens of mixed protein and nucleic acid data. We show that despite only being
trained on unlabelled sequence data, OmniBioTE learns joint representations
consistent with the central dogma of molecular biology. We further demonstrate
that OmbiBioTE achieves state-of-the-art results predicting the change in Gibbs
free energy ({\Delta}G) of the binding interaction between a given nucleic acid
and protein. Remarkably, we show that multi-omic biosequence transformers
emergently learn useful structural information without any a priori structural
training, allowing us to predict which protein residues are most involved in
the protein-nucleic acid binding interaction. Lastly, compared to single-omic
controls trained with identical compute, OmniBioTE demonstrates superior
performance-per-FLOP and absolute accuracy across both multi-omic and
single-omic benchmarks, highlighting the power of a unified modeling approach
for biological sequences.

</details>


### [704] [Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning](https://arxiv.org/pdf/2503.05306)
*Hyungkyu Kang, Min-hwan Oh*

Main category: cs.LG

TL;DR: APPO is a computationally efficient algorithm for offline PbRL, ensuring conservatism without intractable confidence sets, with proven sample complexity bounds.


<details>
  <summary>Details</summary>
Motivation: Existing offline PbRL methods struggle with conservatism under uncertainty and computational intractability.

Method: APPO frames PbRL as a two-player game between policy and model, enforcing conservatism tractably.

Result: APPO achieves sample complexity bounds and performs comparably to state-of-the-art methods in experiments.

Conclusion: APPO is the first offline PbRL algorithm combining statistical efficiency and practical applicability.

Abstract: In this paper, we study offline preference-based reinforcement learning
(PbRL), where learning is based on pre-collected preference feedback over pairs
of trajectories. While offline PbRL has demonstrated remarkable empirical
success, existing theoretical approaches face challenges in ensuring
conservatism under uncertainty, requiring computationally intractable
confidence set constructions. We address this limitation by proposing
Adversarial Preference-based Policy Optimization (APPO), a computationally
efficient algorithm for offline PbRL that guarantees sample complexity bounds
without relying on explicit confidence sets. By framing PbRL as a two-player
game between a policy and a model, our approach enforces conservatism in a
tractable manner. Using standard assumptions on function approximation and
bounded trajectory concentrability, we derive a sample complexity bound. To our
knowledge, APPO is the first offline PbRL algorithm to offer both statistical
efficiency and practical applicability. Experimental results on continuous
control tasks demonstrate that APPO effectively learns from complex datasets,
showing comparable performance with existing state-of-the-art methods.

</details>


### [705] [Federated Learning for Smart Grid: A Survey on Applications and Potential Vulnerabilities](https://arxiv.org/pdf/2409.10764)
*Zikai Zhang, Suman Rath, Jiaohao Xu, Tingsong Xiao*

Main category: cs.LG

TL;DR: The paper surveys federated learning (FL) applications in Smart Grids (SGs), addressing privacy, efficiency, and accuracy. It reviews FL-based SG systems across generation, transmission, distribution, and consumption, identifies vulnerabilities, and proposes future research directions. It also introduces FedGridShield, an open-source framework for FL security in SGs.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about data security and privacy in SGs motivate the exploration of FL as a privacy-preserving, efficient, and accurate training framework.

Method: The paper conducts a thorough review of FL-based SG systems, examining advancements, vulnerabilities, and gaps between research and practice. It also introduces FedGridShield for implementing attack and defense methods.

Result: The survey highlights the potential of FL in SGs, identifies unique security concerns, and proposes future research directions to bridge the gap between theory and practice.

Conclusion: FL is a promising approach for SGs, but further research is needed to address vulnerabilities and improve robustness. FedGridShield serves as a tool to inspire advancements in FL-based SG systems.

Abstract: The Smart Grid (SG) is a critical energy infrastructure that collects
real-time electricity usage data to forecast future energy demands using
information and communication technologies (ICT). Due to growing concerns about
data security and privacy in SGs, federated learning (FL) has emerged as a
promising training framework. FL offers a balance between privacy, efficiency,
and accuracy in SGs by enabling collaborative model training without sharing
private data from IoT devices. In this survey, we thoroughly review recent
advancements in designing FL-based SG systems across three stages: generation,
transmission and distribution, and consumption. Additionally, we explore
potential vulnerabilities that may arise when implementing FL in these stages.
Furthermore, we discuss the gap between state-of-the-art (SOTA) FL research and
its practical applications in SGs, and we propose future research directions.
Unlike traditional surveys addressing security issues in centralized machine
learning methods for SG systems, this survey is the first to specifically
examine the applications and security concerns unique to FL-based SG systems.
We also introduce FedGridShield, an open-source framework featuring
implementations of SOTA attack and defense methods. Our aim is to inspire
further research into applications and improvements in the robustness of
FL-based SG systems.

</details>


### [706] [Ensemble Kalman Diffusion Guidance: A Derivative-free Method for Inverse Problems](https://arxiv.org/pdf/2409.20175)
*Hongkai Zheng, Wenda Chu, Austin Wang, Nikola Kovachki, Ricardo Baptista, Yisong Yue*

Main category: cs.LG

TL;DR: EnKG is a derivative-free method for solving inverse problems using pre-trained diffusion models, requiring only forward model evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on privileged information, limiting their applicability in problems where such details are unavailable.

Method: Proposes Ensemble Kalman Diffusion Guidance (EnKG), a derivative-free approach leveraging forward model evaluations and a pre-trained diffusion model.

Result: Demonstrates effectiveness in non-linear inverse problems like fluid flow and astronomical object inference.

Conclusion: EnKG broadens the applicability of diffusion models in inverse problems without requiring derivative or full forward model knowledge.

Abstract: When solving inverse problems, one increasingly popular approach is to use
pre-trained diffusion models as plug-and-play priors. This framework can
accommodate different forward models without re-training while preserving the
generative capability of diffusion models. Despite their success in many
imaging inverse problems, most existing methods rely on privileged information
such as derivative, pseudo-inverse, or full knowledge about the forward model.
This reliance poses a substantial limitation that restricts their use in a wide
range of problems where such information is unavailable, such as in many
scientific applications. We propose Ensemble Kalman Diffusion Guidance (EnKG),
a derivative-free approach that can solve inverse problems by only accessing
forward model evaluations and a pre-trained diffusion model prior. We study the
empirical effectiveness of EnKG across various inverse problems, including
scientific settings such as inferring fluid flows and astronomical objects,
which are highly non-linear inverse problems that often only permit black-box
access to the forward model. We open-source our code at
https://github.com/devzhk/enkg-pytorch.

</details>


### [707] [Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models](https://arxiv.org/pdf/2410.02416)
*Seyedmorteza Sadat, Otmar Hilliges, Romann M. Weber*

Main category: cs.LG

TL;DR: The paper introduces adaptive projected guidance (APG) to address oversaturation in classifier-free guidance (CFG) for diffusion models, improving generation quality without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: High guidance scales in CFG improve quality but cause oversaturation and artifacts, necessitating a solution.

Method: APG decomposes CFG's update term into parallel and orthogonal components, down-weights the parallel part, and introduces rescaling and momentum.

Result: APG improves FID, recall, and saturation scores while maintaining precision, outperforming standard CFG.

Conclusion: APG is a superior, plug-and-play alternative to CFG, compatible with various models and samplers.

Abstract: Classifier-free guidance (CFG) is crucial for improving both generation
quality and alignment between the input condition and final output in diffusion
models. While a high guidance scale is generally required to enhance these
aspects, it also causes oversaturation and unrealistic artifacts. In this
paper, we revisit the CFG update rule and introduce modifications to address
this issue. We first decompose the update term in CFG into parallel and
orthogonal components with respect to the conditional model prediction and
observe that the parallel component primarily causes oversaturation, while the
orthogonal component enhances image quality. Accordingly, we propose
down-weighting the parallel component to achieve high-quality generations
without oversaturation. Additionally, we draw a connection between CFG and
gradient ascent and introduce a new rescaling and momentum method for the CFG
update rule based on this insight. Our approach, termed adaptive projected
guidance (APG), retains the quality-boosting advantages of CFG while enabling
the use of higher guidance scales without oversaturation. APG is easy to
implement and introduces practically no additional computational overhead to
the sampling process. Through extensive experiments, we demonstrate that APG is
compatible with various conditional diffusion models and samplers, leading to
improved FID, recall, and saturation scores while maintaining precision
comparable to CFG, making our method a superior plug-and-play alternative to
standard classifier-free guidance.

</details>


### [708] [Average Certified Radius is a Poor Metric for Randomized Smoothing](https://arxiv.org/pdf/2410.06895)
*Chenhao Sun, Yuhao Mao, Mark Niklas Müller, Martin Vechev*

Main category: cs.LG

TL;DR: The paper critiques the use of Average Certified Radius (ACR) in Randomized Smoothing (RS), showing it's flawed and suggests alternative metrics.


<details>
  <summary>Details</summary>
Motivation: ACR is widely used but misleading for evaluating RS robustness, as it can be inflated by trivial classifiers and is overly sensitive to easy samples.

Method: Theoretical proofs and empirical experiments demonstrate ACR's flaws. Strategies like discarding hard samples and reweighing datasets are tested.

Result: ACR is shown to be a poor metric, and alternative training strategies achieve high ACR without improving true robustness.

Conclusion: ACR should be discontinued in RS, with the empirical distribution of $p_A$ proposed as a better alternative.

Abstract: Randomized smoothing (RS) is popular for providing certified robustness
guarantees against adversarial attacks. The average certified radius (ACR) has
emerged as a widely used metric for tracking progress in RS. However, in this
work, for the first time we show that ACR is a poor metric for evaluating
robustness guarantees provided by RS. We theoretically prove not only that a
trivial classifier can have arbitrarily large ACR, but also that ACR is
extremely sensitive to improvements on easy samples. In addition, the
comparison using ACR has a strong dependence on the certification budget.
Empirically, we confirm that existing training strategies, though improving
ACR, reduce the model's robustness on hard samples consistently. To strengthen
our findings, we propose strategies, including explicitly discarding hard
samples, reweighing the dataset with approximate certified radius, and extreme
optimization for easy samples, to replicate the progress in RS training and
even achieve the state-of-the-art ACR on CIFAR-10, without training for
robustness on the full data distribution. Overall, our results suggest that ACR
has introduced a strong undesired bias to the field, and its application should
be discontinued in RS. Finally, we suggest using the empirical distribution of
$p_A$, the accuracy of the base model on noisy data, as an alternative metric
for RS.

</details>


### [709] [Efficient Dictionary Learning with Switch Sparse Autoencoders](https://arxiv.org/pdf/2410.08201)
*Anish Mudide, Joshua Engels, Eric J. Michaud, Max Tegmark, Christian Schroeder de Witt*

Main category: cs.LG

TL;DR: Switch Sparse Autoencoders (Switch SAEs) reduce compute costs for training SAEs by routing activations between smaller expert SAEs, improving scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Scaling sparse autoencoders (SAEs) to high width for identifying all features in frontier models is computationally challenging.

Method: Introduces Switch SAEs, inspired by sparse mixture of experts models, to route activations between smaller expert SAEs.

Result: Switch SAEs achieve better reconstruction vs. sparsity trade-offs and maintain interpretability compared to other SAE architectures.

Conclusion: Switch SAEs offer a computationally efficient and scalable solution for feature decomposition in neural networks.

Abstract: Sparse autoencoders (SAEs) are a recent technique for decomposing neural
network activations into human-interpretable features. However, in order for
SAEs to identify all features represented in frontier models, it will be
necessary to scale them up to very high width, posing a computational
challenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE
architecture aimed at reducing the compute cost of training SAEs. Inspired by
sparse mixture of experts models, Switch SAEs route activation vectors between
smaller "expert" SAEs, enabling SAEs to efficiently scale to many more
features. We present experiments comparing Switch SAEs with other SAE
architectures, and find that Switch SAEs deliver a substantial Pareto
improvement in the reconstruction vs. sparsity frontier for a given fixed
training compute budget. We also study the geometry of features across experts,
analyze features duplicated across experts, and verify that Switch SAE features
are as interpretable as features found by other SAE architectures.

</details>


### [710] [FedRecon: Missing Modality Reconstruction in Heterogeneous Distributed Environments](https://arxiv.org/pdf/2504.09941)
*Junming Liu, Guosun Zeng, Ding Wang, Yanting Gao, Yufei Jin*

Main category: cs.LG

TL;DR: FedRecon addresses missing modalities and Non-IID data in federated learning by using a Multimodal Variational Autoencoder for reconstruction and a distribution mapping mechanism for alignment.


<details>
  <summary>Details</summary>
Motivation: Real-world multimodal data is often incomplete and Non-IID, posing challenges for federated learning.

Method: Uses a lightweight MVAE for missing modality reconstruction and a distribution mapping mechanism for sample-level alignment. Global generator freezing prevents catastrophic forgetting.

Result: Outperforms state-of-the-art methods in modality reconstruction under Non-IID conditions.

Conclusion: FedRecon effectively tackles coupled challenges of missing modalities and Non-IID data in multimodal FL.

Abstract: Multimodal data are often incomplete and exhibit Non-Independent and
Identically Distributed (Non-IID) characteristics in real-world scenarios.
These inherent limitations lead to both modality heterogeneity through partial
modality absence and data heterogeneity from distribution divergence, creating
fundamental challenges for effective federated learning (FL). To address these
coupled challenges, we propose FedRecon, the first method targeting
simultaneous missing modality reconstruction and Non-IID adaptation in
multimodal FL. Our approach first employs a lightweight Multimodal Variational
Autoencoder (MVAE) to reconstruct missing modalities while preserving
cross-modal consistency. Distinct from conventional imputation methods, we
achieve sample-level alignment through a novel distribution mapping mechanism
that guarantees both data consistency and completeness. Additionally, we
introduce a strategy employing global generator freezing to prevent
catastrophic forgetting, which in turn mitigates Non-IID fluctuations.
Extensive evaluations on multimodal datasets demonstrate FedRecon's superior
performance in modality reconstruction under Non-IID conditions, surpassing
state-of-the-art methods.

</details>


### [711] [Learning on Model Weights using Tree Experts](https://arxiv.org/pdf/2410.13569)
*Eliahu Horwitz, Bar Cavia, Jonathan Kahana, Yedid Hoshen*

Main category: cs.LG

TL;DR: The paper introduces ProbeX, a lightweight method to infer model documentation from weights, leveraging Model Trees for reduced nuisance variation.


<details>
  <summary>Details</summary>
Motivation: Public models lack documentation, making it hard for users to find suitable models. Existing methods struggle with nuisance variation in weights.

Method: ProbeX, a probing method, learns from a single hidden layer of model weights, reducing computational costs.

Result: ProbeX effectively predicts model categories from weights and enables zero-shot model classification, e.g., mapping Stable Diffusion weights to text.

Conclusion: ProbeX offers a practical solution for model documentation and search, addressing challenges in real-world model usage.

Abstract: The number of publicly available models is rapidly increasing, yet most
remain undocumented. Users looking for suitable models for their tasks must
first determine what each model does. Training machine learning models to infer
missing documentation directly from model weights is challenging, as these
weights often contain significant variation unrelated to model functionality
(denoted nuisance). Here, we identify a key property of real-world models: most
public models belong to a small set of Model Trees, where all models within a
tree are fine-tuned from a common ancestor (e.g., a foundation model).
Importantly, we find that within each tree there is less nuisance variation
between models. Concretely, while learning across Model Trees requires complex
architectures, even a linear classifier trained on a single model layer often
works within trees. While effective, these linear classifiers are
computationally expensive, especially when dealing with larger models that have
many parameters. To address this, we introduce Probing Experts (ProbeX), a
theoretically motivated and lightweight method. Notably, ProbeX is the first
probing method specifically designed to learn from the weights of a single
hidden model layer. We demonstrate the effectiveness of ProbeX by predicting
the categories in a model's training dataset based only on its weights.
Excitingly, ProbeX can map the weights of Stable Diffusion into a
weight-language embedding space, enabling model search via text, i.e.,
zero-shot model classification.

</details>


### [712] [Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications](https://arxiv.org/pdf/2504.16972)
*Hossein Ahmadi, Sajjad Emdadi Mahdimahalleh, Arman Farahat, Banafsheh Saffari*

Main category: cs.LG

TL;DR: A review of autoencoders and vision transformers for unsupervised signal analysis, covering architectures, applications, and trends in domains like IoT and biomedical engineering.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of unlabeled time-series data in fields like IoT and biomedical engineering necessitates advancements in unsupervised learning for signal analysis.

Method: Focuses on autoencoders and vision transformers, exploring their architectures and applications in feature extraction, anomaly detection, and classification.

Result: Highlights hybrid architectures and self-supervised learning as strengths, while noting challenges in interpretability, scalability, and domain generalization.

Conclusion: Provides a roadmap for developing robust, adaptive models for signal intelligence by bridging methodological innovations and practical applications.

Abstract: The rapid growth of unlabeled time-series data in domains such as wireless
communications, radar, biomedical engineering, and the Internet of Things (IoT)
has driven advancements in unsupervised learning. This review synthesizes
recent progress in applying autoencoders and vision transformers for
unsupervised signal analysis, focusing on their architectures, applications,
and emerging trends. We explore how these models enable feature extraction,
anomaly detection, and classification across diverse signal types, including
electrocardiograms, radar waveforms, and IoT sensor data. The review highlights
the strengths of hybrid architectures and self-supervised learning, while
identifying challenges in interpretability, scalability, and domain
generalization. By bridging methodological innovations and practical
applications, this work offers a roadmap for developing robust, adaptive models
for signal intelligence.

</details>


### [713] [Online Learning for Function Placement in Serverless Computing](https://arxiv.org/pdf/2410.13696)
*Wei Huang, Richard Combes, Andrea Araldo, Hind Castel-Taleb, Badii Jouaber*

Main category: cs.LG

TL;DR: A novel algorithm for virtual function placement minimizes cost using multi-armed bandits, achieving rapid learning and low regret while respecting constraints.


<details>
  <summary>Details</summary>
Motivation: To efficiently place virtual functions with minimal cost, addressing feasibility constraints and scalability in large networks.

Method: Proposes a multi-armed bandit-based algorithm with an acceleration technique for large networks.

Result: The algorithm learns optimal placement quickly, with regret bounded by O(NM√(TlnT)), and performs well in experiments.

Conclusion: The algorithm is practical, scalable, and computationally efficient, with reproducible results and publicly available code.

Abstract: We study the placement of virtual functions aimed at minimizing the cost. We
propose a novel algorithm, using ideas based on multi-armed bandits. We prove
that these algorithms learn the optimal placement policy rapidly, and their
regret grows at a rate at most $O( N M \sqrt{T\ln T} )$ while respecting the
feasibility constraints with high probability, where $T$ is total time slots,
$M$ is the number of classes of function and $N$ is the number of computation
nodes. We show through numerical experiments that the proposed algorithm both
has good practical performance and modest computational complexity. We propose
an acceleration technique that allows the algorithm to achieve good performance
also in large networks where computational power is limited. Our experiments
are fully reproducible, and the code is publicly available.

</details>


### [714] [Adversarial Inception Backdoor Attacks against Reinforcement Learning](https://arxiv.org/pdf/2410.13995)
*Ethan Rathbun, Alina Oprea, Christopher Amato*

Main category: cs.LG

TL;DR: A new class of backdoor attacks, 'inception' attacks, is proposed for DRL, achieving high success under strict reward constraints by manipulating training data.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks on DRL are brittle under reward constraints, motivating the need for more robust attacks.

Method: The 'inception' attack manipulates training data by inserting triggers and replacing actions to induce adversarial behavior while maintaining task performance.

Result: The attack achieves 100% success rate under constrained rewards with minimal impact on task performance.

Conclusion: The proposed inception attacks are robust and effective against DRL under strict reward constraints.

Abstract: Recent works have demonstrated the vulnerability of Deep Reinforcement
Learning (DRL) algorithms against training-time, backdoor poisoning attacks.
The objectives of these attacks are twofold: induce pre-determined, adversarial
behavior in the agent upon observing a fixed trigger during deployment while
allowing the agent to solve its intended task during training. Prior attacks
assume arbitrary control over the agent's rewards, inducing values far outside
the environment's natural constraints. This results in brittle attacks that
fail once the proper reward constraints are enforced. Thus, in this work we
propose a new class of backdoor attacks against DRL which are the first to
achieve state of the art performance under strict reward constraints. These
"inception" attacks manipulate the agent's training data -- inserting the
trigger into prior observations and replacing high return actions with those of
the targeted adversarial behavior. We formally define these attacks and prove
they achieve both adversarial objectives against arbitrary Markov Decision
Processes (MDP). Using this framework we devise an online inception attack
which achieves an 100\% attack success rate on multiple environments under
constrained rewards while minimally impacting the agent's task performance.

</details>


### [715] [Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning](https://arxiv.org/pdf/2505.03792)
*Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, Bo An*

Main category: cs.LG

TL;DR: CoSo, a novel RL method for fine-tuning VLM agents, improves exploration efficiency by dynamically assessing token influence using counterfactual reasoning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in online RL for VLM agents, such as open-ended action spaces and non-end-to-end action generation, which hinder effective exploration.

Method: Proposes CoSo, which uses counterfactual reasoning to prioritize exploration of action-critical tokens while reducing focus on redundant or low-impact tokens.

Result: Theoretical and empirical results show CoSo enhances exploration efficiency and performance across diverse tasks like Android control, card gaming, and embodied AI.

Conclusion: CoSo is effective for fine-tuning VLM agents, offering targeted exploration and consistent performance improvements.

Abstract: Online fine-tuning vision-language model (VLM) agents with reinforcement
learning (RL) has shown promise for equipping agents with multi-step,
goal-oriented capabilities in dynamic environments. However, their open-ended
textual action space and non-end-to-end nature of action generation present
significant challenges to effective online exploration in RL, e.g., explosion
of the exploration space. We propose a novel online fine-tuning method,
Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual
output space of VLM agents. Compared to prior methods that assign uniform
uncertainty to all tokens, CoSo leverages counterfactual reasoning to
dynamically assess the causal influence of individual tokens on post-processed
actions. By prioritizing the exploration of action-critical tokens while
reducing the impact of semantically redundant or low-impact tokens, CoSo
enables a more targeted and efficient online rollout process. We provide
theoretical analysis proving CoSo's convergence and policy improvement
guarantees, and extensive empirical evaluations supporting CoSo's
effectiveness. Our results across a diverse set of agent tasks, including
Android device control, card gaming, and embodied AI, highlight its remarkable
ability to enhance exploration efficiency and deliver consistent performance
gains. The code is available at https://github.com/langfengQ/CoSo.

</details>


### [716] [Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models](https://arxiv.org/pdf/2410.21088)
*Wenda Li, Huijie Zhang, Qing Qu*

Main category: cs.LG

TL;DR: Shallow Diffuse is a new watermarking technique for AI-generated images that decouples watermarking from the diffusion process, improving robustness and detectability.


<details>
  <summary>Details</summary>
Motivation: Address concerns of misinformation and copyright infringement from AI-generated content by developing a reliable watermarking method.

Method: Leverages a low-dimensional subspace in image generation to embed watermarks in the null space, decoupling it from the diffusion process.

Result: Enhances watermark consistency and detectability, outperforming existing methods in robustness.

Conclusion: Shallow Diffuse provides an effective solution for watermarking AI-generated images, with potential for widespread adoption.

Abstract: The widespread use of AI-generated content from diffusion models has raised
significant concerns regarding misinformation and copyright infringement.
Watermarking is a crucial technique for identifying these AI-generated images
and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new
watermarking technique that embeds robust and invisible watermarks into
diffusion model outputs. Unlike existing approaches that integrate watermarking
throughout the entire diffusion sampling process, Shallow Diffuse decouples
these steps by leveraging the presence of a low-dimensional subspace in the
image generation process. This method ensures that a substantial portion of the
watermark lies in the null space of this subspace, effectively separating it
from the image generation process. Our theoretical and empirical analyses show
that this decoupling strategy greatly enhances the consistency of data
generation and the detectability of the watermark. Extensive experiments
further validate that our Shallow Diffuse outperforms existing watermarking
methods in terms of robustness and consistency. The codes will be released at
https://github.com/liwd190019/Shallow-Diffuse.

</details>


### [717] [NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA](https://arxiv.org/pdf/2411.03730)
*Marlon Tobaben, Mohamed Ali Souibgui, Rubèn Tito, Khanh Nguyen, Raouf Kerkouche, Kangsoo Jung, Joonas Jälkö, Lei Kang, Andrey Barsky, Vincent Poulain d'Andecy, Aurélie Joseph, Aashiq Muhamed, Kevin Kuo, Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti Honkela, Mario Fritz, Dimosthenis Karatzas*

Main category: cs.LG

TL;DR: The PFL-DocVQA competition focused on developing private and efficient federated learning solutions for invoice processing, using a real dataset. Participants fine-tuned a multi-modal model, addressing communication efficiency and differential privacy.


<details>
  <summary>Details</summary>
Motivation: To advance privacy-preserving federated learning in real-life document analysis, specifically for invoice processing, by bringing together diverse expertise.

Method: Participants fine-tuned a pre-trained multi-modal Document VQA model in a federated setup, focusing on communication efficiency (Track 1) and differential privacy (Track 2).

Result: Solutions reduced communication costs while maintaining utility and protected document provider information, establishing best practices for privacy-focused federated learning.

Conclusion: The competition successfully tested private federated learning methods, raised privacy awareness, and provided recommendations for future challenges.

Abstract: The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.

</details>


### [718] [Dialz: A Python Toolkit for Steering Vectors](https://arxiv.org/pdf/2505.06262)
*Zara Siddique, Liam D. Turner, Luis Espinosa-Anke*

Main category: cs.LG

TL;DR: Dialz is a Python framework for steering vectors in LLMs, enabling concept modification (e.g., honesty) at inference. It supports tasks like dataset creation, vector application, and visualization, focusing on modularity and usability. Dialz reduces harmful outputs and aids model interpretability, promoting safer AI.


<details>
  <summary>Details</summary>
Motivation: To provide a powerful, modular, and user-friendly tool for steering vector research in LLMs, addressing limitations of prompting/fine-tuning and enhancing model safety and transparency.

Method: Implemented in Python, Dialz supports tasks like contrastive dataset creation, steering vector computation/application, and visualizations. It emphasizes modularity and usability for rapid prototyping and analysis.

Result: Dialz reduces harmful outputs (e.g., stereotypes) and offers insights into model behavior across layers, facilitating safer and more interpretable AI.

Conclusion: Dialz accelerates research cycles, enhances model interpretability, and promotes safer, more transparent AI systems, with full documentation and support for open-source models.

Abstract: We introduce Dialz, a framework for advancing research on steering vectors
for open-source LLMs, implemented in Python. Steering vectors allow users to
modify activations at inference time to amplify or weaken a 'concept', e.g.
honesty or positivity, providing a more powerful alternative to prompting or
fine-tuning. Dialz supports a diverse set of tasks, including creating
contrastive pair datasets, computing and applying steering vectors, and
visualizations. Unlike existing libraries, Dialz emphasizes modularity and
usability, enabling both rapid prototyping and in-depth analysis. We
demonstrate how Dialz can be used to reduce harmful outputs such as
stereotypes, while also providing insights into model behaviour across
different layers. We release Dialz with full documentation, tutorials, and
support for popular open-source models to encourage further research in safe
and controllable language generation. Dialz enables faster research cycles and
facilitates insights into model interpretability, paving the way for safer,
more transparent, and more reliable AI systems.

</details>


### [719] [Discovering Latent Causal Graphs from Spatio-Temporal Data](https://arxiv.org/pdf/2411.05331)
*Kun Wang, Sumanth Varambally, Duncan Watson-Parris, Yi-An Ma, Rose Yu*

Main category: cs.LG

TL;DR: SPACY is a variational inference-based framework for discovering causal relationships in high-dimensional spatiotemporal data by modeling latent time series and using spatial factors.


<details>
  <summary>Details</summary>
Motivation: Inferring causal relationships from spatiotemporal data is challenging due to high dimensionality and spatial correlations.

Method: SPACY uses variational inference to model latent time series and spatial factors, mapping observations to latent representations.

Result: SPACY outperforms baselines on synthetic data and identifies real-world climate phenomena.

Conclusion: SPACY effectively addresses high-dimensional challenges and spatial correlations in causal discovery.

Abstract: Many important phenomena in scientific fields like climate, neuroscience, and
epidemiology are naturally represented as spatiotemporal gridded data with
complex interactions. Inferring causal relationships from these data is a
challenging problem compounded by the high dimensionality of such data and the
correlations between spatially proximate points. We present SPACY
(SPAtiotemporal Causal discoverY), a novel framework based on variational
inference, designed to model latent time series and their causal relationships
from spatiotemporal data. SPACY alleviates the high-dimensional challenge by
discovering causal structures in the latent space. To aggregate spatially
proximate, correlated grid points, we use \change{spatial factors, parametrized
by spatial kernel functions}, to map observational time series to latent
representations. \change{Theoretically, we generalize the problem to a
continuous spatial domain and establish identifiability when the observations
arise from a nonlinear, invertible function of the product of latent series and
spatial factors. Using this approach, we avoid assumptions that are often
unverifiable, including those about instantaneous effects or sufficient
variability.} Empirically, SPACY outperforms state-of-the-art baselines on
synthetic data, even in challenging settings where existing methods struggle,
while remaining scalable for large grids. SPACY also identifies key known
phenomena from real-world climate data.

</details>


### [720] [Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records](https://arxiv.org/pdf/2505.07320)
*Yuhao Li, Ling Luo, Uwe Aickelin*

Main category: cs.LG

TL;DR: ACTLL is an attention-based framework for handling noisy labels in medical time series data, improving prediction accuracy by dynamically calibrating and augmenting data.


<details>
  <summary>Details</summary>
Motivation: Labeling errors in EHR data hinder accurate patient outcome predictions, necessitating robust solutions.

Method: ACTLL uses a Beta mixture model to classify instances into certain/uncertain sets, dynamically calibrates labels, and augments confident instances.

Result: ACTLL outperforms others on EHR datasets (eICU, MIMIC-IV-ED) and benchmarks (UCR, UEA), especially with high noise.

Conclusion: ACTLL effectively addresses noisy labels in medical time series, achieving state-of-the-art performance.

Abstract: Medical research, particularly in predicting patient outcomes, heavily relies
on medical time series data extracted from Electronic Health Records (EHR),
which provide extensive information on patient histories. Despite rigorous
examination, labeling errors are inevitable and can significantly impede
accurate predictions of patient outcome. To address this challenge, we propose
an \textbf{A}ttention-based Learning Framework with Dynamic
\textbf{C}alibration and Augmentation for \textbf{T}ime series Noisy
\textbf{L}abel \textbf{L}earning (ACTLL). This framework leverages a
two-component Beta mixture model to identify the certain and uncertain sets of
instances based on the fitness distribution of each class, and it captures
global temporal dynamics while dynamically calibrating labels from the
uncertain set or augmenting confident instances from the certain set.
Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and
several benchmark datasets from the UCR and UEA repositories, demonstrate that
our model ACTLL has achieved state-of-the-art performance, especially under
high noise levels.

</details>


### [721] [Puzzle: Distillation-Based NAS for Inference-Optimized LLMs](https://arxiv.org/pdf/2411.19146)
*Akhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah, Ido Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi Koren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny, Ran Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, Ran El-Yaniv*

Main category: cs.LG

TL;DR: Puzzle is a hardware-aware framework that accelerates LLM inference while maintaining accuracy, achieving 2.17x speedup on models like Nemotron-51B with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: High inference costs of LLMs limit adoption, despite their capabilities. Puzzle aims to bridge the gap between state-of-the-art performance and practical deployability.

Method: Uses neural architecture search (NAS) and blockwise local knowledge distillation (BLD) for optimization, alongside mixed-integer programming for constraints.

Result: Achieves 2.17x throughput speedup on Nemotron-51B and Nemotron-49B, retaining 98.4% accuracy while fitting on a single H100 GPU.

Conclusion: Demonstrates that LLMs can be optimized for efficient deployment without significant quality loss, emphasizing inference performance over parameter count.

Abstract: Large language models (LLMs) offer remarkable capabilities, yet their high
inference costs restrict wider adoption. While increasing parameter counts
improves accuracy, it also broadens the gap between state-of-the-art
capabilities and practical deployability. We present Puzzle, a hardware-aware
framework that accelerates the inference of LLMs while preserving their
capabilities. Using neural architecture search (NAS) at a large-scale, Puzzle
optimizes models with tens of billions of parameters. Our approach utilizes
blockwise local knowledge distillation (BLD) for parallel architecture
exploration and employs mixed-integer programming for precise constraint
optimization.
  We showcase our framework's impact via Llama-3.1-Nemotron-51B-Instruct
(Nemotron-51B) and Llama-3.3-Nemotron-49B, two publicly available models
derived from Llama-70B-Instruct. Both models achieve a 2.17x inference
throughput speedup, fitting on a single NVIDIA H100 GPU while retaining 98.4%
of the original model's benchmark accuracies. These are the most accurate
models supporting single H100 GPU inference with large batch sizes, despite
training on 45B tokens at most, far fewer than the 15T used to train Llama-70B.
Lastly, we show that lightweight alignment on these derived models allows them
to surpass the parent model in specific capabilities. Our work establishes that
powerful LLM models can be optimized for efficient deployment with only
negligible loss in quality, underscoring that inference performance, not
parameter count alone, should guide model selection.

</details>


### [722] [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/pdf/2412.17626)
*Yang Xu, Yi Wang, Hengguan Huang, Hao Wang*

Main category: cs.LG

TL;DR: SAE-Track is introduced to study feature evolution in LLMs, covering semantic evolution, formation processes, and directional drift.


<details>
  <summary>Details</summary>
Motivation: To understand how features evolve during training in large language models (LLMs) for mechanistic interpretability.

Method: Introduces SAE-Track, a method for obtaining continual sparse autoencoders (SAEs) to analyze feature dynamics.

Result: Provides insights into feature evolution, formation, and drift in LLMs.

Conclusion: Enhances understanding of training mechanisms and feature dynamics in LLMs; code is available for reproducibility.

Abstract: Understanding training dynamics and feature evolution is crucial for the
mechanistic interpretability of large language models (LLMs). Although sparse
autoencoders (SAEs) have been used to identify features within LLMs, a clear
picture of how these features evolve during training remains elusive. In this
study, we (1) introduce SAE-Track, a novel method for efficiently obtaining a
continual series of SAEs, providing the foundation for a mechanistic study that
covers (2) the semantic evolution of features, (3) the underlying processes of
feature formation, and (4) the directional drift of feature vectors. Our work
provides new insights into the dynamics of features in LLMs, enhancing our
understanding of training mechanisms and feature evolution. For
reproducibility, our code is available at
https://github.com/Superposition09m/SAE-Track.

</details>


### [723] [Elucidating Flow Matching ODE Dynamics with Respect to Data Geometries and Denoisers](https://arxiv.org/pdf/2412.18730)
*Zhengchao Wan, Qingsong Wang, Gal Mishne, Yusu Wang*

Main category: cs.LG

TL;DR: The paper advances the theoretical understanding of Flow Matching (FM) models by analyzing sample trajectories, revealing how the denoiser guides ODE dynamics through data geometry. It identifies three stages of ODE evolution and rigorously establishes convergence under weak assumptions.


<details>
  <summary>Details</summary>
Motivation: Theoretical understanding of FM models, especially their interaction with data geometry, is lacking. A rigorous analysis is needed to improve sample quality, stability, and applicability.

Method: Comprehensive analysis of FM ODE sample trajectories, focusing on the denoiser's role in guiding dynamics through attracting and absorbing behaviors. Three stages of ODE evolution are identified and analyzed.

Result: The denoiser adapts to data geometry, guiding trajectories toward the mean and local clusters initially, then converging under weak assumptions, even for low-dimensional submanifolds. Terminal stage analysis reveals memorization and equivariance properties.

Conclusion: The findings bridge gaps in FM theory, offering insights for optimizing sampling strategies and architectures based on data geometry.

Abstract: Flow matching (FM) models extend ODE sampler based diffusion models into a
general framework, significantly reducing sampling steps through learned vector
fields. However, the theoretical understanding of FM models, particularly how
their sample trajectories interact with underlying data geometry, remains
underexplored. A rigorous theoretical analysis of FM ODE is essential for
sample quality, stability, and broader applicability. In this paper, we advance
the theory of FM models through a comprehensive analysis of sample
trajectories. Central to our theory is the discovery that the denoiser, a key
component of FM models, guides ODE dynamics through attracting and absorbing
behaviors that adapt to the data geometry. We identify and analyze the three
stages of ODE evolution: in the initial and intermediate stages, trajectories
move toward the mean and local clusters of the data. At the terminal stage, we
rigorously establish the convergence of FM ODE under weak assumptions,
addressing scenarios where the data lie on a low-dimensional submanifold-cases
that previous results could not handle. Our terminal stage analysis offers
insights into the memorization phenomenon and establishes equivariance
properties of FM ODEs. These findings bridge critical gaps in understanding
flow matching models, with practical implications for optimizing sampling
strategies and architectures guided by the intrinsic geometry of data.

</details>


### [724] ["Cause" is Mechanistic Narrative within Scientific Domains: An Ordinary Language Philosophical Critique of "Causal Machine Learning"](https://arxiv.org/pdf/2501.05844)
*Vyacheslav Kungurtsev, Leonardo Christov Moore, Gustav Sir, Martin Krutsky*

Main category: cs.LG

TL;DR: The paper explores the epistemology of causal learning, emphasizing the distinct grammars of causality across domains like physics, biology, and social sciences, and advocates for agglomerating evidence for definitive claims.


<details>
  <summary>Details</summary>
Motivation: To investigate the semantics of causality and address challenges in recognizing true cause-and-effect phenomena across scientific domains.

Method: Uses the Ordinary Language method to analyze the word 'cause' and demarcates causality in physics, biology, and social sciences.

Result: Identifies distinct causal grammars per domain and highlights the need for consistent evidence aggregation for definitive claims.

Conclusion: Calls for methodological caution in harmonizing causality across domains and emphasizes precision in communicating certainty.

Abstract: Causal Learning has emerged as a major theme of research in statistics and
machine learning in recent years, promising specific computational techniques
to apply to datasets that reveal the true nature of cause and effect in a
number of important domains. In this paper we consider the epistemology of
recognizing true cause and effect phenomena. We apply the Ordinary Language
method of engaging on the customary use of the word 'cause' to investigate
valid semantics of reasoning about cause and effect. We recognize that the
grammars of cause and effect are fundamentally distinct in form across
scientific domains, yet they maintain a consistent and central function. This
function can best be described as the mechanism underlying fundamental forces
of influence as considered prominent in the respective scientific domain. We
demarcate 1) physics and engineering as domains wherein mathematical models are
sufficient to comprehensively describe causality, 2) biology as introducing
challenges of emergence while providing opportunities for showing consistent
mechanisms across scale, and 3) the social sciences as introducing grander
difficulties for establishing models of low prediction error but providing,
through Hermeneutics, the potential for findings that are still instrumentally
useful to individuals. We posit that definitive causal claims regarding a given
phenomenon (writ large) can only come through an agglomeration of consistent
evidence across multiple domains. This presents important methodological
questions as far as harmonizing between language games and emergence across
scales. Given the role of epistemic hubris in the contemporary crisis of
credibility in the sciences, exercising greater caution as far as communicating
precision as to the real degree of certainty certain evidence provides for rich
collections of open problems in optimizing integration of different findings.

</details>


### [725] [Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity](https://arxiv.org/pdf/2501.16168)
*Artavazd Maranjyan, Alexander Tyurin, Peter Richtárik*

Main category: cs.LG

TL;DR: Ringmaster ASGD is a new Asynchronous SGD method achieving optimal time complexity under heterogeneous worker computation times, addressing prior limitations.


<details>
  <summary>Details</summary>
Motivation: Existing Asynchronous SGD variants fail to achieve optimal time complexity under heterogeneous computation times, creating inefficiencies as worker numbers scale.

Method: Proposes Ringmaster ASGD, a novel method designed to handle heterogeneous and dynamically fluctuating worker computation times.

Result: Theoretical analysis confirms Ringmaster ASGD achieves optimal time complexity, meeting lower bounds for such scenarios.

Conclusion: Ringmaster ASGD is the first Asynchronous SGD method to achieve optimal time complexity under heterogeneous conditions, filling a gap in the literature.

Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone
method for parallelizing learning in distributed machine learning. However, its
performance suffers under arbitrarily heterogeneous computation times across
workers, leading to suboptimal time complexity and inefficiency as the number
of workers scales. While several Asynchronous SGD variants have been proposed,
recent findings by Tyurin & Richt\'arik (NeurIPS 2023) reveal that none achieve
optimal time complexity, leaving a significant gap in the literature. In this
paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to
address these limitations and tame the inherent challenges of Asynchronous SGD.
We establish, through rigorous theoretical analysis, that Ringmaster ASGD
achieves optimal time complexity under arbitrarily heterogeneous and
dynamically fluctuating worker computation times. This makes it the first
Asynchronous SGD method to meet the theoretical lower bounds for time
complexity in such scenarios.

</details>


### [726] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/pdf/2505.16932)
*Noah Amsel, David Persson, Christopher Musco, Robert M. Gower*

Main category: cs.LG

TL;DR: Polar Express is a GPU-friendly algorithm for polar decomposition, optimized for deep learning by balancing efficiency and convergence, outperforming classical methods in Muon optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional polar decomposition methods are inefficient for deep learning due to slow convergence or reliance on QR/inversions. Polar Express addresses this by being GPU-compatible and fast.

Method: Polar Express uses matrix-matrix multiplications and adapts polynomial updates via minimax optimization, ensuring rapid convergence and stability in bfloat16.

Result: Polar Express improves validation loss in Muon optimization, outperforming alternatives on models like GPT-2 across learning rates.

Conclusion: Polar Express is a practical, efficient solution for polar decomposition in deep learning, combining speed, stability, and performance.

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [727] [On the Expressiveness of Visual Prompt Experts](https://arxiv.org/pdf/2501.18936)
*Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Anh Tran, Nhat Ho*

Main category: cs.LG

TL;DR: VAPT enhances Visual Prompt Tuning (VPT) by introducing adaptive prompt experts, improving performance and sample efficiency over VPT and full fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing VPT frameworks lack functional expressiveness in prompt experts, limiting adaptability.

Method: Proposes Visual Adaptive Prompt Tuning (VAPT), adding adaptive prompt experts to MoE structures for better expressiveness.

Result: VAPT outperforms VPT and full fine-tuning on VTAB-1K (7.34%) and FGVC (1.04%) with fewer parameters.

Conclusion: VAPT provides theoretical and empirical advantages, achieving optimal sample efficiency and superior performance.

Abstract: Visual Prompt Tuning (VPT) has proven effective for parameter-efficient
adaptation of pre-trained vision models to downstream tasks by inserting
task-specific learnable prompt tokens. Despite its empirical success, a
comprehensive theoretical understanding of VPT remains an active area of
research. Building on the recently established connection between Mixture of
Experts (MoE) and prompt-based methods, wherein each attention head can be
conceptualized as a composition of multiple MoE models, we reinterpret VPT as
the introduction of new prompt experts into these MoE structures. We identify a
key limitation in existing VPT frameworks: the restricted functional
expressiveness of prompt experts, which remain static and thus limited in their
adaptability. To address this, we propose Visual Adaptive Prompt Tuning (VAPT),
a novel method that endows prompt experts with enhanced expressiveness while
preserving parameter efficiency. Empirical evaluations on VTAB-1K and FGVC
demonstrate that VAPT achieves substantial performance improvements, surpassing
fully fine-tuned baselines by 7.34% and 1.04%, respectively. Moreover, VAPT
consistently outperforms VPT while requiring fewer additional parameters.
Furthermore, our theoretical analysis indicates that VAPT achieves optimal
sample efficiency. Collectively, these results underscore the theoretical
grounding and empirical advantages of our approach.

</details>


### [728] [POSTER: A Multi-Signal Model for Detecting Evasive Smishing](https://arxiv.org/pdf/2505.18233)
*Shaghayegh Hosseinpour, Sanchari Das*

Main category: cs.LG

TL;DR: A multi-channel smishing detection model combines semantic, structural, and stylistic cues to achieve high accuracy (97.89%) and outperform single-stream models.


<details>
  <summary>Details</summary>
Motivation: Smishing (SMS phishing) is a growing threat, exploiting culturally adapted messages to deceive users and steal sensitive data or money.

Method: The model integrates country-specific semantic tagging, structural patterns, character-level stylistic cues, and contextual phrase embeddings, using a dataset of 84,000 messages (24,086 smishing samples).

Result: The model achieves 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream approaches.

Conclusion: Multi-signal learning is effective for robust, region-aware smishing detection.

Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users
by mimicking legitimate communications through culturally adapted, concise, and
deceptive messages, which can result in the loss of sensitive data or financial
resources. In such, we present a multi-channel smishing detection model that
combines country-specific semantic tagging, structural pattern tagging,
character-level stylistic cues, and contextual phrase embeddings. We curated
and relabeled over 84,000 messages across five datasets, including 24,086
smishing samples. Our unified architecture achieves 97.89% accuracy, an F1
score of 0.963, and an AUC of 99.73%, outperforming single-stream models by
capturing diverse linguistic and structural cues. This work demonstrates the
effectiveness of multi-signal learning in robust and region-aware phishing.

</details>


### [729] [Towards the Worst-case Robustness of Large Language Models](https://arxiv.org/pdf/2501.19040)
*Huanran Chen, Yinpeng Dong, Zeming Wei, Hang Su, Jun Zhu*

Main category: cs.LG

TL;DR: The paper analyzes the worst-case robustness of large language models against adversarial attacks, proposing theoretical bounds for deterministic and stochastic defenses.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of large language models to adversarial attacks that induce harmful or incorrect outputs.

Method: Uses white-box attacks to upper bound worst-case robustness and proposes tight lower bounds for randomized smoothing using knapsack solvers.

Result: Most deterministic defenses achieve nearly 0% worst-case robustness, while theoretical lower bounds are provided for stochastic defenses.

Conclusion: The study provides certified robustness for specific cases, demonstrating the effectiveness of the proposed bounds against adversarial attacks.

Abstract: Recent studies have revealed the vulnerability of large language models to
adversarial attacks, where adversaries craft specific input sequences to induce
harmful, violent, private, or incorrect outputs. In this work, we study their
worst-case robustness, i.e., whether an adversarial example exists that leads
to such undesirable outputs. We upper bound the worst-case robustness using
stronger white-box attacks, indicating that most current deterministic defenses
achieve nearly 0\% worst-case robustness. We propose a general tight lower
bound for randomized smoothing using fractional knapsack solvers or 0-1
knapsack solvers, and using them to bound the worst-case robustness of all
stochastic defenses. Based on these solvers, we provide theoretical lower
bounds for several previous empirical defenses. For example, we certify the
robustness of a specific case, smoothing using a uniform kernel, against
\textit{any possible attack} with an average $\ell_0$ perturbation of 2.02 or
an average suffix length of 6.41.

</details>


### [730] [G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning](https://arxiv.org/pdf/2505.18499)
*Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang*

Main category: cs.LG

TL;DR: G1 uses RL on synthetic graph tasks to enhance LLMs' graph reasoning, outperforming larger models and generalizing well.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with graph tasks due to data scarcity; G1 aims to improve graph reasoning efficiently.

Method: RL on the Erdős dataset (50 tasks, 100k training data) to fine-tune LLMs.

Result: A 3B model outperforms a 24x larger model (Qwen2.5-72B) and shows strong zero-shot generalization.

Conclusion: RL on synthetic graph tasks is a scalable way to enhance LLMs' graph reasoning without losing general abilities.

Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress,
their proficiency in graph-related tasks remains notably limited, hindering the
development of truly general-purpose models. Previous attempts, including
pretraining graph foundation models or employing supervised fine-tuning, often
face challenges such as the scarcity of large-scale, universally represented
graph data. We introduce G1, a simple yet effective approach demonstrating that
Reinforcement Learning (RL) on synthetic graph-theoretic tasks can
significantly scale LLMs' graph reasoning abilities. To enable RL training, we
curate Erd\~os, the largest graph reasoning dataset to date comprising 50
diverse graph-theoretic tasks of varying difficulty levels, 100k training data
and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1
obtains substantial improvements in graph reasoning, where our finetuned 3B
model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also
show strong zero-shot generalization to unseen tasks, domains, and graph
encoding schemes, including other graph-theoretic benchmarks as well as
real-world node classification and link prediction tasks, without compromising
general reasoning abilities. Our findings offer an efficient, scalable path for
building strong graph reasoners by finetuning LLMs with RL on graph-theoretic
tasks, which combines the strengths of pretrained LLM capabilities with
abundant, automatically generated synthetic data, suggesting that LLMs possess
graph understanding abilities that RL can elicit successfully. Our
implementation is open-sourced at https://github.com/PKU-ML/G1, with models and
datasets hosted on Hugging Face collections
https://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for
broader accessibility.

</details>


### [731] [SafeSwitch: Steering Unsafe LLM Behavior via Internal Activation Signals](https://arxiv.org/pdf/2502.01042)
*Peixuan Han, Cheng Qian, Xiusi Chen, Yuji Zhang, Denghui Zhang, Heng Ji*

Main category: cs.LG

TL;DR: SafeSwitch is a dynamic framework for LLMs that reduces harmful outputs by 80% while maintaining utility, using internal state monitoring and minimal parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing safety mechanisms for LLMs are overly cautious and inefficient. SafeSwitch aims to leverage LLMs' internal cognitive processes for better safety control.

Method: SafeSwitch uses a prober-based internal state monitor to detect harmful intentions and activates a safety head only when necessary.

Result: Reduces harmful outputs by ~80% on harmful queries, maintains utility, and achieves Pareto optimality. Only tunes <6% of parameters.

Conclusion: SafeSwitch demonstrates LLMs' self-awareness for safety, offering nuanced and effective control with minimal overhead.

Abstract: Large language models (LLMs) exhibit exceptional capabilities across various
tasks but also pose risks by generating harmful content. Existing safety
mechanisms, while improving model safety, often lead to overly cautious
behavior and fail to fully leverage LLMs' internal cognitive processes.
Inspired by humans' reflective thinking capability, we first show that LLMs can
similarly perform internal assessments about safety in their internal states.
Building on this insight, we propose SafeSwitch, a dynamic framework that
regulates unsafe outputs by utilizing the prober-based internal state monitor
that actively detects harmful intentions, and activates a safety head that
leads to safer and more conservative responses only when necessary. SafeSwitch
reduces harmful outputs by approximately 80% on harmful queries while
maintaining strong utility, reaching a Pareto optimal among several methods.
Our method is also advantageous over traditional methods in offering more
informative, context-aware refusals, and achieves these benefits while only
tuning less than 6% of the original parameters. SafeSwitch demonstrates large
language models' capacity for self-awareness and reflection regarding safety,
offering a promising approach to more nuanced and effective safety controls.
Codes for this work are available at https://github.com/Hanpx20/SafeSwitch.

</details>


### [732] [Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series](https://arxiv.org/pdf/2505.20697)
*Zachary C. Brown, David Carlson*

Main category: cs.LG

TL;DR: A novel method for dynamic causal discovery in neuroscience improves hypothesis generation by modeling time-varying interactions beyond linear assumptions, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods for hypothesis generation in neuroscience often assume static or linear causal relationships, limiting their applicability to dynamic systems like the brain.

Method: Proposes modeling dynamic graphs as a conditionally weighted superposition of static graphs, capturing nonlinear and time-varying interactions.

Result: Improves F1-scores by 22-28% on average (up to 60% in some cases) and successfully uncovers neural dynamics in real brain data.

Conclusion: The method advances dynamic causal discovery, offering better insights into complex systems like the brain.

Abstract: The field of hypothesis generation promises to reduce costs in neuroscience
by narrowing the range of interventional studies needed to study various
phenomena. Existing machine learning methods can generate scientific hypotheses
from complex datasets, but many approaches assume causal relationships are
static over time, limiting their applicability to systems with dynamic,
state-dependent behavior, such as the brain. While some techniques attempt
dynamic causal discovery through factor models, they often restrict
relationships to linear patterns or impose other simplifying assumptions. We
propose a novel method that models dynamic graphs as a conditionally weighted
superposition of static graphs, where each static graph can capture nonlinear
relationships. This approach enables the detection of complex, time-varying
interactions between variables beyond linear limitations. Our method improves
f1-scores of predicted dynamic causal patterns by roughly 22-28% on average
over baselines in some of our experiments, with some improvements reaching well
over 60%. A case study on real brain data demonstrates our method's ability to
uncover relationships linked to specific behavioral states, offering valuable
insights into neural dynamics.

</details>


### [733] [Federated Linear Dueling Bandits](https://arxiv.org/pdf/2502.01085)
*Xuhan Huang, Yan Hu, Zhiyan Li, Zhiyong Wang, Benyou Wang, Zhongxiang Dai*

Main category: cs.LG

TL;DR: The paper introduces FLDB-OGD, a federated linear dueling bandit algorithm combining online gradient descent and federated learning, addressing multi-agent collaboration without data sharing.


<details>
  <summary>Details</summary>
Motivation: Classical dueling bandit algorithms are limited to single agents, while real-world applications involve multiple agents unwilling to share data, necessitating a federated approach.

Method: FLDB-OGD combines online gradient descent (OGD) for parameter estimation with federated learning principles to enable multi-agent collaboration.

Result: The algorithm achieves sub-linear regret and demonstrates a trade-off between regret and communication complexity, with empirical validation.

Conclusion: FLDB-OGD effectively addresses the challenge of multi-agent collaboration in linear dueling bandits, offering theoretical guarantees and practical insights.

Abstract: Contextual linear dueling bandits have recently garnered significant
attention due to their widespread applications in important domains such as
recommender systems and large language models. Classical dueling bandit
algorithms are typically only applicable to a single agent. However, many
applications of dueling bandits involve multiple agents who wish to collaborate
for improved performance yet are unwilling to share their data. This motivates
us to draw inspirations from federated learning, which involves multiple agents
aiming to collaboratively train their neural networks via gradient descent (GD)
without sharing their raw data. Previous works have developed federated linear
bandit algorithms which rely on closed-form updates of the bandit parameters
(e.g., the linear function parameters) to achieve collaboration. However, in
linear dueling bandits, the linear function parameters lack a closed-form
expression and their estimation requires minimizing a loss function. This
renders these previous methods inapplicable. In this work, we overcome this
challenge through an innovative and principled combination of online gradient
descent (OGD, for minimizing the loss function to estimate the linear function
parameters) and federated learning, hence introducing our federated linear
dueling bandit with OGD (FLDB-OGD) algorithm. Through rigorous theoretical
analysis, we prove that FLDB-OGD enjoys a sub-linear upper bound on its
cumulative regret and demonstrate a theoretical trade-off between regret and
communication complexity. We conduct empirical experiments to demonstrate the
effectiveness of FLDB-OGD and reveal valuable insights, such as the benefit of
a larger number of agents, the regret-communication trade-off, among others.

</details>


### [734] [SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training](https://arxiv.org/pdf/2502.01586)
*Sahar Rajabi, Nayeema Nonta, Sirisha Rambhatla*

Main category: cs.LG

TL;DR: SubTrack++ improves LLM training by reducing memory use and training time without sacrificing performance, using subspace tracking and recovery scaling.


<details>
  <summary>Details</summary>
Motivation: Democratizing LLMs requires progress in memory efficiency, training time, and performance simultaneously.

Method: Combines Grassmannian gradient subspace tracking with projection-aware optimizers and recovery scaling.

Result: Achieves SOTA convergence, reduces pretraining time by 43%, and maintains memory footprint on a 1B-parameter model.

Conclusion: SubTrack++ advances LLM training efficiency across all key dimensions.

Abstract: Training large language models (LLMs) is highly resource-intensive due to
their massive number of parameters and the overhead of optimizer states. While
recent work has aimed to reduce memory consumption, such efforts often entail
trade-offs among memory efficiency, training time, and model performance. Yet,
true democratization of LLMs requires simultaneous progress across all three
dimensions. To this end, we propose SubTrack++ that leverages Grassmannian
gradient subspace tracking combined with projection-aware optimizers, enabling
Adam's internal statistics to adapt to changes in the optimization subspace.
Additionally, employing recovery scaling, a technique that restores information
lost through low-rank projections, further enhances model performance. Our
method demonstrates SOTA convergence by exploiting Grassmannian geometry and
achieves lowest evaluation loss, outperforming the current SOTA while reducing
pretraining wall time by 43% and maintaining the memory footprint on a
1B-parameter Llama model.

</details>


### [735] [Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators](https://arxiv.org/pdf/2502.03424)
*Yuan Xinjie, Khalid M. Mosalam*

Main category: cs.LG

TL;DR: The paper introduces the Most Fire-Sensitive Point (MFSP) and a machine learning framework using GNNs to identify it, optimizing fire safety assessments in buildings.


<details>
  <summary>Details</summary>
Motivation: Evaluating fire safety in buildings is complex and costly due to the unpredictability of fire origins. The MFSP concept addresses this by pinpointing the worst-case fire scenario.

Method: A Graph Neural Network (GNN) predicts the Maximum Interstory Drift Ratio (MIDR) under fire, guiding MFSP identification. The framework includes an edge update mechanism and transfer learning.

Result: The framework performs well in identifying MFSPs on a large-scale dataset, proving effective for fire safety optimization.

Conclusion: The proposed framework offers a transformative tool for efficient fire safety assessments, with open-sourced datasets and codes for broader use.

Abstract: Fire safety is crucial for ensuring the stability of building structures, yet
evaluating whether a structure meets fire safety requirement is challenging.
Fires can originate at any point within a structure, and simulating every
potential fire scenario is both expensive and time-consuming. To address this
challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and
an efficient machine learning framework for its identification. The MFSP is
defined as the location at which a fire, if initiated, would cause the most
severe detrimental impact on the building's stability, effectively representing
the worst-case fire scenario. In our framework, a Graph Neural Network (GNN)
serves as an efficient and differentiable agent for conventional Finite Element
Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio
(MIDR) under fire, which then guides the training and evaluation of the MFSP
predictor. Additionally, we enhance our framework with a novel edge update
mechanism and a transfer learning-based training scheme. Evaluations on a
large-scale simulation dataset demonstrate the good performance of the proposed
framework in identifying the MFSP, offering a transformative tool for
optimizing fire safety assessments in structural design. All developed datasets
and codes are open-sourced online.

</details>


### [736] [Controllable Sequence Editing for Biological and Clinical Trajectories](https://arxiv.org/pdf/2502.03569)
*Michelle M. Li, Kevin Li, Yasha Ektefaie, Ying Jin, Yepeng Huang, Shvat Messica, Tianxi Cai, Marinka Zitnik*

Main category: cs.LG

TL;DR: CLEF is a controllable sequence editing model for precise conditional generation in multivariate longitudinal sequences, outperforming baselines in accuracy and enabling targeted edits.


<details>
  <summary>Details</summary>
Motivation: Existing models lack control over timing and scope of edits in longitudinal sequences, limiting their applicability in scientific and clinical settings.

Method: CLEF learns temporal concepts to encode how and when conditions alter sequences, enabling targeted edits while preserving unaffected parts.

Result: CLEF improves immediate editing accuracy by up to 36.01% (MAE) and delayed editing by up to 65.71% (MAE), with significant gains in counterfactual generation.

Conclusion: CLEF enables precise, one-step conditional generation at arbitrary times, demonstrating effectiveness in clinical applications like diabetes management.

Abstract: Conditional generation models for longitudinal sequences can generate new or
modified trajectories given a conditioning input. While effective at generating
entire sequences, these models typically lack control over the timing and scope
of the edits. Most existing approaches either operate on univariate sequences
or assume that the condition affects all variables and time steps. However,
many scientific and clinical applications require more precise interventions,
where a condition takes effect only after a specific time and influences only a
subset of variables. We introduce CLEF, a controllable sequence editing model
for conditional generation of immediate and delayed effects in multivariate
longitudinal sequences. CLEF learns temporal concepts that encode how and when
a condition alters future sequence evolution. These concepts allow CLEF to
apply targeted edits to the affected time steps and variables while preserving
the rest of the sequence. We evaluate CLEF on 6 datasets spanning cellular
reprogramming and patient health trajectories, comparing against 9
state-of-the-art baselines. CLEF improves immediate sequence editing accuracy
by up to 36.01% (MAE). Unlike prior models, CLEF enables one-step conditional
generation at arbitrary future times, outperforming them in delayed sequence
editing by up to 65.71% (MAE). We test CLEF under counterfactual inference
assumptions and show up to 63.19% (MAE) improvement on zero-shot conditional
generation of counterfactual trajectories. In a case study of patients with
type 1 diabetes mellitus, CLEF identifies clinical interventions that generate
realistic counterfactual trajectories shifted toward healthier outcomes.

</details>


### [737] [Stochastic Forward-Backward Deconvolution: Training Diffusion Models with Finite Noisy Datasets](https://arxiv.org/pdf/2502.05446)
*Haoye Lu, Qifan Wu, Yaoliang Yu*

Main category: cs.LG

TL;DR: Training diffusion models on noisy data to avoid copyright issues is theoretically possible but practically challenging. Pretraining with a small fraction of clean data and using SFBD improves performance, achieving FID 6.31 on CIFAR-10 with 4% clean images.


<details>
  <summary>Details</summary>
Motivation: Address concerns about memorization and copyright infringement in diffusion models by training on noisy data, while overcoming practical challenges.

Method: Propose pretraining with a small fraction of clean data and introduce Stochastic Forward--Backward Deconvolution (SFBD) to guide learning.

Result: Achieve FID 6.31 on CIFAR-10 with 4% clean images (3.58 with 10%). Theoretical guarantees confirm SFBD learns the true data distribution.

Conclusion: Limited clean pretraining or pretraining on similar datasets is valuable, as validated by empirical studies.

Abstract: Recent diffusion-based generative models achieve remarkable results by
training on massive datasets, yet this practice raises concerns about
memorization and copyright infringement. A proposed remedy is to train
exclusively on noisy data with potential copyright issues, ensuring the model
never observes original content. However, through the lens of deconvolution
theory, we show that although it is theoretically feasible to learn the data
distribution from noisy samples, the practical challenge of collecting
sufficient samples makes successful learning nearly unattainable. To overcome
this limitation, we propose to pretrain the model with a small fraction of
clean data to guide the deconvolution process. Combined with our Stochastic
Forward--Backward Deconvolution (SFBD) method, we attain FID 6.31 on CIFAR-10
with just 4% clean images (and 3.58 with 10%). We also provide theoretical
guarantees that SFBD learns the true data distribution. These results
underscore the value of limited clean pretraining, or pretraining on similar
datasets. Empirical studies further validate and enrich our findings.

</details>


### [738] [Curvature Tuning: Provable Training-free Model Steering From a Single Parameter](https://arxiv.org/pdf/2502.07783)
*Leyang Hu, Matteo Gamba, Randall Balestriero*

Main category: cs.LG

TL;DR: The paper introduces Curvature Tuning (CT), a method to adjust model decision boundaries by modifying activation functions, offering interpretability and improved performance over traditional finetuning.


<details>
  <summary>Details</summary>
Motivation: Current finetuning methods lack interpretability and rely on heuristic hyperparameters. The paper aims to address this by focusing on activation functions.

Method: CT modulates decision boundaries by injecting a hyperparameter into activation functions, projecting models onto smooth function spaces.

Result: CT improves ResNet-50/152 accuracy by 7.14%/8.46% over linear probing and boosts robust accuracy by 1032.64%/1494.46%.

Conclusion: CT provides an interpretable, efficient finetuning alternative, enhancing both generalization and robustness.

Abstract: The scaling of model and data sizes has reshaped the AI landscape,
establishing finetuning pretrained models as the standard paradigm for solving
downstream tasks. However, dominant finetuning methods typically rely on weight
adaptation, often lack interpretability, and depend on heuristically chosen
hyperparameters. In this paper, we take a different perspective and shift the
focus from weights to activation functions, viewing them through the lens of
spline operators. We propose Curvature Tuning (CT), an interpretable and
principled steering method that modulates a model's decision boundary by
injecting a single hyperparameter into its activation functions. We show that
CT provably adjusts model decision boundary curvature and, more fundamentally,
projects a model onto a space of smooth functions-thereby complementing current
finetuning methods, whose effect lies primarily in feature adaptation. Making
this hyperparameter trainable gives rise to a novel and highly
parameter-efficient finetuning method. Empirically, CT improves both
generalization and robustness. For example, it boosts downstream accuracy of
ResNet-50/152 by 7.14%/8.46% over linear probing and 4.64%/1.70% over LoRA
across 12 datasets, and improves robust accuracy on the $\ell_\infty$ benchmark
from RobustBench by 1032.64%/1494.46%. Our code is available at
https://github.com/Leon-Leyang/curvature-tuning.

</details>


### [739] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/pdf/2505.23868)
*Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou, Libo Qin, Wenhong Tian*

Main category: cs.LG

TL;DR: Proposes LoPE, a noise-robust adaptation method using asymmetric LoRA poisoning experts to handle noisy data without data cleaning.


<details>
  <summary>Details</summary>
Motivation: Current fine-tuning methods struggle with noisy data, requiring laborious pre-processing or error-prone architecture changes.

Method: LoPE integrates a poisoning expert in an asymmetric LoRA setup, injecting noise during fine-tuning and masking it during inference.

Result: LoPE achieves strong performance and robustness purely through noise injection, eliminating data cleaning needs.

Conclusion: LoPE offers a low-cost, effective solution for noise-robust fine-tuning without data pre-processing.

Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [740] [We Should Chart an Atlas of All the World's Models](https://arxiv.org/pdf/2503.10633)
*Eliahu Horwitz, Nitzan Kurer, Jonathan Kahana, Liel Amar, Yedid Hoshen*

Main category: cs.LG

TL;DR: The paper proposes the Model Atlas, a unified graph structure to document and connect models, addressing the lack of documentation in public repositories. It highlights applications in model forensics, meta-ML research, and discovery, and calls for community effort to chart models using new ML methods.


<details>
  <summary>Details</summary>
Motivation: Most models in public repositories are undocumented and lost, hindering tasks like model forensics and discovery. The Model Atlas aims to address this by organizing models and their attributes in a structured graph.

Method: The Model Atlas is a graph capturing models, their attributes, and weight transformations. New ML methods are proposed to infer model properties (e.g., functionality, performance) directly from weights, bypassing parameter symmetries.

Result: The Model Atlas enables applications in model forensics, meta-ML research, and discovery, though large regions remain uncharted due to lack of documentation.

Conclusion: Charting all models requires community effort and new ML approaches. The Model Atlas's broad utility is hoped to rally researchers toward this goal.

Abstract: Public model repositories now contain millions of models, yet most models
remain undocumented and effectively lost. In this position paper, we advocate
for charting the world's model population in a unified structure we call the
Model Atlas: a graph that captures models, their attributes, and the weight
transformations that connect them. The Model Atlas enables applications in
model forensics, meta-ML research, and model discovery, challenging tasks given
today's unstructured model repositories. However, because most models lack
documentation, large atlas regions remain uncharted. Addressing this gap
motivates new machine learning methods that treat models themselves as data,
inferring properties such as functionality, performance, and lineage directly
from their weights. We argue that a scalable path forward is to bypass the
unique parameter symmetries that plague model weights. Charting all the world's
models will require a community effort, and we hope its broad utility will
rally researchers toward this goal.

</details>


### [741] [LoRA Training Provably Converges to a Low-Rank Global Minimum or It Fails Loudly (But it Probably Won't Fail)](https://arxiv.org/pdf/2502.09376)
*Junsu Kim, Jaeyeon Kim, Ernest K. Ryu*

Main category: cs.LG

TL;DR: The paper analyzes LoRA's loss landscape without restrictive assumptions, identifying two regimes and explaining why LoRA training typically finds global minima.


<details>
  <summary>Details</summary>
Motivation: Prior analyses of LoRA's training dynamics rely on linearization or simplified setups, limiting theoretical understanding.

Method: Defines two regimes (special and generic) and analyzes LoRA's loss landscape without restrictive assumptions.

Result: In the generic regime, LoRA converges to low-rank, small-magnitude solutions or high-rank, large-magnitude ones. Zero-initialization and weight decay bias toward low-rank, small-magnitude global minima.

Conclusion: LoRA training's success is due to implicit bias toward low-rank, small-magnitude solutions, where global minima lie.

Abstract: Low-rank adaptation (LoRA) has become a standard approach for fine-tuning
large foundation models. However, our theoretical understanding of LoRA remains
limited as prior analyses of LoRA's training dynamics either rely on
linearization arguments or consider highly simplified setups. In this work, we
analyze the LoRA loss landscape without such restrictive assumptions. We define
two regimes: a "special regime", which includes idealized setups where
linearization arguments hold, and a "generic regime" representing more
realistic setups where linearization arguments do not hold. In the generic
regime, we show that LoRA training converges to a global minimizer with low
rank and small magnitude, or a qualitatively distinct solution with high rank
and large magnitude. Finally, we argue that the zero-initialization and weight
decay in LoRA training induce an implicit bias toward the low-rank,
small-magnitude region of the parameter space -- where global minima lie --
thus shedding light on why LoRA training usually succeeds in finding global
minima.

</details>


### [742] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/pdf/2505.24595)
*Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov*

Main category: cs.LG

TL;DR: The paper proposes binary cumulative encoding (BCE) for time series forecasting, preserving ordinal structure and improving performance over one-hot encoding methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods using one-hot encoding ignore ordinal structure, limiting distance-awareness in predictions.

Method: Introduces BCE for encoding scalar targets into monotonic binary vectors and a CNN architecture with residual and dilated convolutions.

Result: Outperforms existing methods in point and probabilistic forecasting with fewer parameters and faster training.

Conclusion: BCE enhances forecasting by preserving order and magnitude, offering efficiency and performance gains.

Abstract: Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [743] [SyncSDE: A Probabilistic Framework for Diffusion Synchronization](https://arxiv.org/pdf/2503.21555)
*Hyunjun Lee, Hyunsoo Lee, Sookwan Han*

Main category: cs.LG

TL;DR: A probabilistic framework analyzes diffusion synchronization, optimizing task-specific correlations for better results than naive heuristics.


<details>
  <summary>Details</summary>
Motivation: Existing methods for collaborative generation with diffusion models rely on naive heuristics like averaging, lacking task-specific optimization and theoretical justification.

Method: The paper introduces a probabilistic framework to model correlations between diffusion trajectories and adapt them to specific tasks, identifying optimal correlation models.

Result: The framework outperforms previous approaches by focusing on task-specific correlations rather than applying a single heuristic universally.

Conclusion: Task-specific correlation modeling in diffusion synchronization leads to superior performance compared to heuristic-based methods.

Abstract: There have been many attempts to leverage multiple diffusion models for
collaborative generation, extending beyond the original domain. A prominent
approach involves synchronizing multiple diffusion trajectories by mixing the
estimated scores to artificially correlate the generation processes. However,
existing methods rely on naive heuristics, such as averaging, without
considering task specificity. These approaches do not clarify why such methods
work and often produce suboptimal results when a heuristic suitable for one
task is blindly applied to others. In this paper, we present a probabilistic
framework for analyzing why diffusion synchronization works and reveal where
heuristics should be focused; modeling correlations between multiple
trajectories and adapting them to each specific task. We further identify
optimal correlation models per task, achieving better results than previous
approaches that apply a single heuristic across all tasks without
justification.

</details>


### [744] [The underlying structures of self-attention: symmetry, directionality, and emergent dynamics in Transformer training](https://arxiv.org/pdf/2502.10927)
*Matteo Saponati, Pascal Sager, Pau Vilimelis Aceituno, Thilo Stadelmann, Benjamin Grewe*

Main category: cs.LG

TL;DR: The paper analyzes self-attention matrices in Transformers, showing how training objectives (bidirectional vs. autoregressive) shape their structure. Symmetric initialization improves encoder-only models.


<details>
  <summary>Details</summary>
Motivation: To clarify how information is embedded in self-attention matrices and how training objectives influence their structure.

Method: A mathematical framework is developed to analyze weight updates in self-attention matrices, validated across Transformer models and input modalities.

Result: Bidirectional training induces symmetry, while autoregressive training leads to directionality. Symmetric initialization boosts encoder-only model performance.

Conclusion: The study provides theoretical insights into self-attention, enhancing Transformer interpretability and performance.

Abstract: Self-attention is essential to Transformer architectures, yet how information
is embedded in the self-attention matrices and how different objective
functions impact this process remains unclear. We present a mathematical
framework to analyze self-attention matrices by deriving the structures
governing their weight updates. Using this framework, we demonstrate that
bidirectional training induces symmetry in the weight matrices, while
autoregressive training results in directionality and column dominance. Our
theoretical findings are validated across multiple Transformer models -
including ModernBERT, GPT, LLaMA3, and Mistral - and input modalities like
text, vision, and audio. Finally, we apply these insights by showing that
symmetric initialization improves the performance of encoder-only models on
language tasks. This mathematical analysis offers a novel theoretical
perspective on how information is embedded through self-attention, thereby
improving the interpretability of Transformer models.

</details>


### [745] [Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment](https://arxiv.org/pdf/2502.14354)
*Moxin Li, Yuantao Zhang, Wenjie Wang, Wentao Shi, Zhuo Liu, Fuli Feng, Tat-Seng Chua*

Main category: cs.LG

TL;DR: The paper proposes a self-improving DPO framework to resolve preference conflicts in Multi-Objective Alignment (MOA) by generating and selecting Pareto-optimal responses, achieving superior results.


<details>
  <summary>Details</summary>
Motivation: Existing DPO-based MOA approaches suffer from preference conflicts, leading to suboptimal alignment due to conflicting optimization directions.

Method: A self-improving DPO framework is introduced, where LLMs self-generate and select Pareto-optimal responses for self-supervised preference alignment.

Result: Experiments on two datasets show the framework achieves a superior Pareto Front compared to baselines.

Conclusion: The proposed method effectively resolves preference conflicts and improves alignment in MOA.

Abstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple
human preference objectives, with Direct Preference Optimization (DPO) emerging
as a prominent approach. However, we find that DPO-based MOA approaches suffer
from widespread preference conflicts in the data, where different objectives
favor different responses. This results in conflicting optimization directions,
hindering the optimization on the Pareto Front. To address this, we propose to
construct Pareto-optimal responses to resolve preference conflicts. To
efficiently obtain and utilize such responses, we propose a self-improving DPO
framework that enables LLMs to self-generate and select Pareto-optimal
responses for self-supervised preference alignment. Extensive experiments on
two datasets demonstrate the superior Pareto Front achieved by our framework
compared to various baselines. Code is available at
https://github.com/zyttt-coder/SIPO.

</details>


### [746] [Improved Margin Generalization Bounds for Voting Classifiers](https://arxiv.org/pdf/2502.16462)
*Mikael Møller Høgsgaard, Kasper Green Larsen*

Main category: cs.LG

TL;DR: A new margin-based generalization bound for voting classifiers is introduced, improving existing results and providing tighter guarantees for boosting algorithms like AdaBoost. It also enables an optimal weak-to-strong learner, the Majority-of-3, matching theoretical lower bounds.


<details>
  <summary>Details</summary>
Motivation: To refine and tighten generalization bounds for voting classifiers, particularly boosting algorithms, and to derive an optimal weak-to-strong learner.

Method: Establishes a new margin-based generalization bound and applies it to derive the Majority-of-3 classifier.

Result: The Majority-of-3 classifier achieves an expected error matching the theoretical lower bound, offering a more natural alternative to existing methods.

Conclusion: The new bound and Majority-of-3 classifier provide improved generalization guarantees and optimal performance, aligning with prior theoretical results.

Abstract: In this paper we establish a new margin-based generalization bound for voting
classifiers, refining existing results and yielding tighter generalization
guarantees for widely used boosting algorithms such as AdaBoost (Freund and
Schapire, 1997). Furthermore, the new margin-based generalization bound enables
the derivation of an optimal weak-to-strong learner: a Majority-of-3
large-margin classifiers with an expected error matching the theoretical lower
bound. This result provides a more natural alternative to the Majority-of-5
algorithm by (H{\o}gsgaard et al., 2024), and matches the Majority-of-3 result
by (Aden-Ali et al., 2024) for the realizable prediction model.

</details>


### [747] [It Takes a Good Model to Train a Good Model: Generalized Gaussian Priors for Optimized LLMs](https://arxiv.org/pdf/2506.00486)
*Jun Wu, Yirong Xiong, Jiangtao Wen, Yuxing Han*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for LLM optimization using generalized Gaussian distributions (GGDs), improving initialization, training, and compression with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Despite LLM advancements, the statistical distribution of parameters and their impact on training and efficiency is understudied. The work aims to address this gap.

Method: Proposes GG-based initialization, DeepShape for post-training regularization, and RF8 for hardware-efficient 8-bit training, all grounded in GG modeling.

Result: Experiments show smaller, faster models matching or outperforming baselines, with up to 90% parameter reduction.

Conclusion: The framework offers a principled, scalable approach for efficient and hardware-aware AI systems.

Abstract: Despite rapid advancements in the research and deployment of large language
models (LLMs), the statistical distribution of model parameters, as well as
their influence on initialization, training dynamics, and downstream
efficiency, has received surprisingly little attention. A recent work
introduced BackSlash, a training-time compression algorithm. It first
demonstrated that pre-trained LLM parameters follow generalized Gaussian
distributions (GGDs) better. By optimizing GG priors during training, BackSlash
can reduce parameters by up to 90\% with minimal performance loss. Building on
this foundational insight, we propose a unified, end-to-end framework for LLM
optimization based on the GG model. Our contributions are threefold: (1)
GG-based initialization scheme that aligns with the statistical structure of
trained models, resulting in faster convergence and improved accuracy; (2)
DeepShape, a post-training regularization method that reshapes weight
distributions to match a GG profile, improving compressibility with minimized
degradation in performance; and (3) RF8, a compact and hardware-efficient 8-bit
floating-point format designed for GG-distributed-initialized BackSlash
training, enabling low-cost inference without compromising accuracy.
Experiments across diverse model architectures show that our framework
consistently yields smaller and faster models that match or outperform standard
training baselines. By grounding LLM development in principled statistical
modeling, this work forges a new path toward efficient, scalable, and
hardware-aware AI systems. The code is available on our project page:
https://huggingface.co/spaces/shifeng3711/gg_prior.

</details>


### [748] [Armijo Line-search Can Make (Stochastic) Gradient Descent Provably Faster](https://arxiv.org/pdf/2503.00229)
*Sharan Vaswani, Reza Babanezhad*

Main category: cs.LG

TL;DR: GD with Armijo-LS (GD-LS) outperforms GD(1/L) in convergence speed for certain non-uniform smoothness conditions, achieving linear rates for convex objectives and matching tailored algorithms for non-convex ones.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that Armijo-LS can adapt to local smoothness and achieve faster convergence than fixed step-size GD, especially for specific objective functions.

Method: Analyze GD-LS under non-uniform smoothness conditions, focusing on convex (logistic regression, multi-class classification) and non-convex (gradient domination) objectives.

Result: GD-LS achieves linear convergence for convex objectives and matches tailored algorithms for non-convex ones. Stochastic GD with line-search also matches GD-LS under interpolation.

Conclusion: Armijo-LS provides significant convergence improvements over fixed step-size GD, especially for structured objectives, and is versatile across convex and non-convex settings.

Abstract: Armijo line-search (Armijo-LS) is a standard method to set the step-size for
gradient descent (GD). For smooth functions, Armijo-LS alleviates the need to
know the global smoothness constant L and adapts to the ``local'' smoothness,
enabling GD to converge faster. Existing theoretical analyses show that GD with
Armijo-LS (GD-LS) can result in constant factor improvements over GD with a 1/L
step-size (denoted as GD(1/L)). We strengthen these results and show that if
the objective function satisfies a certain non-uniform smoothness condition,
GD-LS can result in a faster convergence rate than GD(1/L). In particular, we
prove that for convex objectives corresponding to logistic regression and
multi-class classification, GD-LS can converge to the optimum at a linear rate,
and hence improves over the sublinear convergence of GD(1/L). Furthermore, for
non-convex objectives satisfying gradient domination (e.g., those corresponding
to the softmax policy gradient in RL or generalized linear models with a
logistic link function), GD-LS can match the fast convergence of algorithms
tailored for these specific settings. Finally, we prove that under the
interpolation assumption, for convex losses, stochastic GD with a stochastic
line-search can match the fast convergence of GD-LS

</details>


### [749] [Four Principles for Physically Interpretable World Models](https://arxiv.org/pdf/2503.02143)
*Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, Ivan Ruchkin*

Main category: cs.LG

TL;DR: Advocates for physically interpretable world models in autonomous systems, proposing four principles to enhance interpretability and utility.


<details>
  <summary>Details</summary>
Motivation: Current world models lack interpretability and direct mapping to physical quantities, limiting their use in planning, control, and safety verification.

Method: Proposes four principles: functional latent space organization, aligned invariant/equivariant representations, integrated supervision, and partitioned generative outputs.

Result: Demonstrates the value of each principle experimentally on two benchmarks.

Conclusion: Opens research directions for achieving full physical interpretability in world models.

Abstract: As autonomous systems are increasingly deployed in open and uncertain
settings, there is a growing need for trustworthy world models that can
reliably predict future high-dimensional observations. The learned latent
representations in world models lack direct mapping to meaningful physical
quantities and dynamics, limiting their utility and interpretability in
downstream planning, control, and safety verification. In this paper, we argue
for a fundamental shift from physically informed to physically interpretable
world models - and crystallize four principles that leverage symbolic knowledge
to achieve these ends: (1) functionally organizing the latent space according
to the physical intent, (2) learning aligned invariant and equivariant
representations of the physical world, (3) integrating multiple forms and
strengths of supervision into a unified training process, and (4) partitioning
generative outputs to support scalability and verifiability. We experimentally
demonstrate the value of each principle on two benchmarks. This paper opens
several intriguing research directions to achieve and capitalize on full
physical interpretability in world models.

</details>


### [750] [Slim attention: cut your context memory in half without loss -- K-cache is all you need for MHA](https://arxiv.org/pdf/2503.05840)
*Nils Graef, Andrew Wasielewski*

Main category: cs.LG

TL;DR: Slim attention reduces context memory size by 2x for transformer models with MHA, speeding up inference without compromising accuracy. For some models like Whisper and T5-11B, memory reduction and speed improvements are even greater.


<details>
  <summary>Details</summary>
Motivation: To optimize transformer models by reducing memory usage and speeding up inference while maintaining accuracy.

Method: Slim attention, an exact implementation of standard attention, losslessly compresses context memory by a factor of 2 or more.

Result: Memory reduction by 2x (or up to 32x for T5-11B) and speed improvements (e.g., 5x faster token generation for Whisper).

Conclusion: Slim attention is an effective, lossless method for optimizing transformer models, offering significant memory and speed benefits.

Abstract: Slim attention shrinks the context memory size by 2x for transformer models
with MHA (multi-head attention), which can speed up inference by up to 2x for
large context windows.
  Slim attention is an exact, mathematically identical implementation of the
standard attention mechanism and therefore doesn't compromise model accuracy.
In other words, slim attention losslessly compresses the context memory by a
factor of 2.
  For encoder-decoder transformers, the context memory size can be reduced even
further: For the Whisper models for example, slim attention reduces the context
memory by 8x, which can speed up token generation by 5x for batch size 64 for
example.
  And for the T5-11B model for example, the memory can be reduced by 32x
because its MHA projection dimension is larger than the embedding dimension.
  See https://github.com/OpenMachine-ai/transformer-tricks for code and more
transformer tricks, and https://www.youtube.com/watch?v=uVtk3B6YO4Y for this
paper's YouTube video.

</details>


### [751] [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/pdf/2506.00653)
*Femi Bello, Anubrata Das, Fanzhi Zeng, Fangcong Yin, Liu Leqi*

Main category: cs.LG

TL;DR: The paper introduces the Linear Representation Transferability (LRT) Hypothesis, proposing that affine transformations can align representations across neural networks of different scales, enabling small models to guide large ones.


<details>
  <summary>Details</summary>
Motivation: To explore whether representations learned by neural networks trained on similar data can be universally aligned, enabling transferability across model scales.

Method: The authors propose the LRT Hypothesis and test it by learning affine mappings between hidden states of models of varying sizes, evaluating the transferability of steering vectors.

Result: Empirical evidence supports that affine mappings preserve steering behaviors, allowing small models to guide large ones.

Conclusion: The LRT Hypothesis offers a promising framework for understanding representation alignment across model scales, with practical implications for model behavior guidance.

Abstract: It has been hypothesized that neural networks with similar architectures
trained on similar data learn shared representations relevant to the learning
task. We build on this idea by extending the conceptual framework where
representations learned across models trained on the same data can be expressed
as linear combinations of a \emph{universal} set of basis features. These basis
features underlie the learning task itself and remain consistent across models,
regardless of scale. From this framework, we propose the \textbf{Linear
Representation Transferability (LRT)} Hypothesis -- that there exists an affine
transformation between the representation spaces of different models. To test
this hypothesis, we learn affine mappings between the hidden states of models
of different sizes and evaluate whether steering vectors -- directions in
hidden state space associated with specific model behaviors -- retain their
semantic effect when transferred from small to large language models using the
learned mappings. We find strong empirical evidence that such affine mappings
can preserve steering behaviors. These findings suggest that representations
learned by small models can be used to guide the behavior of large models, and
that the LRT hypothesis may be a promising direction on understanding
representation alignment across model scales.

</details>


### [752] [Sample Compression for Continual Learning](https://arxiv.org/pdf/2503.10503)
*Jacob Comeau, Mathieu Bazinet, Pascal Germain, Cem Subakan*

Main category: cs.LG

TL;DR: CoP2L is a continual learning method combining Pick-to-Learn and experience replay, offering generalization guarantees and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning methods lack theoretical guarantees; CoP2L aims to provide non-vacuous bounds on generalization loss.

Method: CoP2L integrates Pick-to-Learn (sample compression theory) with experience replay to retain representative samples efficiently.

Result: CoP2L outperforms baselines in Class-, Task-, and Domain-Incremental settings, mitigating catastrophic forgetting.

Conclusion: CoP2L enhances trustworthiness in continual learning by certifying predictor reliability on past tasks.

Abstract: Continual learning algorithms aim to learn from a sequence of tasks, making
the training distribution non-stationary. The majority of existing continual
learning approaches in the literature rely on heuristics and do not provide
learning guarantees. In this paper, we present a new method called Continual
Pick-to-Learn (CoP2L), which is able to retain the most representative samples
for each task in an efficient way. CoP2L combines the Pick-to-Learn algorithm
(rooted in the sample compression theory) and the experience replay continual
learning scheme. This allows us to provide non-vacuous upper bounds on the
generalization loss of the learned predictors, numerically computable after
each task. We empirically evaluate our approach on several standard continual
learning benchmarks across Class-Incremental, Task-Incremental, and
Domain-Incremental settings. Our results show that CoP2L is highly competitive
across all setups, often outperforming existing baselines, and significantly
mitigating catastrophic forgetting compared to vanilla experience replay in the
Class-Incremental setting. It is possible to leverage the bounds provided by
CoP2L in practical scenarios to certify the predictor reliability on previously
learned tasks, in order to improve the trustworthiness of the continual
learning algorithm.

</details>


### [753] [Unique Hard Attention: A Tale of Two Sides](https://arxiv.org/pdf/2503.14615)
*Selim Jerad, Anej Svete, Jiaoda Li, Ryan Cotterell*

Main category: cs.LG

TL;DR: The paper examines the impact of attention directionality (leftmost-hard vs. rightmost-hard) in transformers, showing leftmost-hard attention is weaker than LTL and equivalent to soft attention.


<details>
  <summary>Details</summary>
Motivation: To understand how attention directionality affects transformer expressivity and its implications for real-world models.

Method: Analyzes finite-precision transformers with leftmost-hard attention, comparing their expressivity to LTL and soft attention models.

Result: Leftmost-hard attention corresponds to a weaker LTL fragment and is equivalent to soft attention, unlike rightmost-hard attention.

Conclusion: Attention directionality significantly influences transformer expressivity, with leftmost-hard attention offering a closer approximation to real-world models.

Abstract: Understanding the expressive power of transformers has recently attracted
attention, as it offers insights into their abilities and limitations. Many
studies analyze unique hard attention transformers, where attention selects a
single position that maximizes the attention scores. When multiple positions
achieve the maximum score, either the rightmost or the leftmost of those is
chosen. In this paper, we highlight the importance of this seeming triviality.
Recently, finite-precision transformers with both leftmost- and rightmost-hard
attention were shown to be equivalent to Linear Temporal Logic (LTL). We show
that this no longer holds with only leftmost-hard attention -- in that case,
they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we
show that models with leftmost-hard attention are equivalent to \emph{soft}
attention, suggesting they may better approximate real-world transformers than
right-attention models. These findings refine the landscape of transformer
expressivity and underscore the role of attention directionality.

</details>


### [754] [Partially Observable Reinforcement Learning with Memory Traces](https://arxiv.org/pdf/2503.15200)
*Onno Eberhard, Michael Muehlebach, Claire Vernade*

Main category: cs.LG

TL;DR: Memory traces, inspired by eligibility traces, improve sample efficiency in reinforcement learning for partially observable environments by compactly representing observation histories as exponential moving averages.


<details>
  <summary>Details</summary>
Motivation: Partially observable environments in reinforcement learning require handling long histories, which becomes computationally intractable with finite observation windows.

Method: Introduces memory traces (exponential moving averages of observation histories) and analyzes their sample complexity for offline on-policy evaluation, comparing them to window-based approaches.

Result: Proves sample complexity bounds, shows memory traces outperform window methods in certain environments, and demonstrates empirical effectiveness in online RL for value prediction and control.

Conclusion: Memory traces offer a computationally efficient and sample-efficient alternative to window-based methods for handling partial observability in reinforcement learning.

Abstract: Partially observable environments present a considerable computational
challenge in reinforcement learning due to the need to consider long histories.
Learning with a finite window of observations quickly becomes intractable as
the window length grows. In this work, we introduce memory traces. Inspired by
eligibility traces, these are compact representations of the history of
observations in the form of exponential moving averages. We prove sample
complexity bounds for the problem of offline on-policy evaluation that quantify
the return errors achieved with memory traces for the class of Lipschitz
continuous value estimates. We establish a close connection to the window
approach, and demonstrate that, in certain environments, learning with memory
traces is significantly more sample efficient. Finally, we underline the
effectiveness of memory traces empirically in online reinforcement learning
experiments for both value prediction and control.

</details>


### [755] [Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack](https://arxiv.org/pdf/2506.01318)
*SeungBum Ha, Saerom Park, Sung Whan Yoon*

Main category: cs.LG

TL;DR: Spotter addresses over-unlearning and relearning attacks in machine unlearning by introducing a plug-and-play objective with masked knowledge-distillation and intra-class dispersion loss.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning techniques overlook over-unlearning and relearning attacks, which degrade retained data and resurrect forgotten knowledge.

Method: Spotter combines masked knowledge-distillation to mitigate over-unlearning and intra-class dispersion loss to counter relearning attacks.

Result: On CIFAR-10, Spotter reduces over-unlearning, drives forget accuracy to 0%, preserves retain set accuracy, and denies prototype-attacks.

Conclusion: Spotter effectively remedies blind spots in machine unlearning, proving practical and efficient.

Abstract: Machine unlearning (MU) aims to expunge a designated forget set from a
trained model without costly retraining, yet the existing techniques overlook
two critical blind spots: "over-unlearning" that deteriorates retained data
near the forget set, and post-hoc "relearning" attacks that aim to resurrect
the forgotten knowledge. We first derive the over-unlearning metric
OU@{\epsilon}, which represents the collateral damage to the nearby region of
the forget set, where the over-unlearning mainly appears. Next, we expose an
unforeseen relearning threat on MU, i.e., the Prototypical Relearning Attack,
which exploits the per-class prototype of the forget class with just a few
samples, and easily restores the pre-unlearning performance. To counter both
blind spots, we introduce Spotter, a plug-and-play objective that combines (i)
a masked knowledge-distillation penalty on the nearby region of forget set to
suppress OU@{\epsilon}, and (ii) an intra-class dispersion loss that scatters
forget-class embeddings, neutralizing prototypical relearning attacks. On
CIFAR-10, as one of validations, Spotter reduces OU@{\epsilon}by below the
0.05X of the baseline, drives forget accuracy to 0%, preserves accuracy of the
retain set within 1% of difference with the original, and denies the
prototype-attack by keeping the forget set accuracy within <1%, without
accessing retained data. It confirms that Spotter is a practical remedy of the
unlearning's blind spots.

</details>


### [756] [Unifying and extending Diffusion Models through PDEs for solving Inverse Problems](https://arxiv.org/pdf/2504.07437)
*Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai*

Main category: cs.LG

TL;DR: The paper presents a novel derivation of diffusion models using linear partial differential equations, offering benefits like unified formulations, new model classes, and applications in inverse problems.


<details>
  <summary>Details</summary>
Motivation: To provide a fresh perspective on diffusion models by deriving them through linear PDEs, aiming for clearer insights and broader applicability in solving physics-based inverse problems.

Method: Derives diffusion models using linear PDEs, leading to constructive forward/reverse processes, unified formulations, and new variance-preserving models. Applies these to conditional density estimation and inverse problems.

Result: Demonstrates the benefits of the PDE-based approach, including unified derivations, new model classes, and benchmarks for performance evaluation. Also enables single-model application to multiple measurement operators.

Conclusion: The study advances the understanding of diffusion models, offering new theoretical and practical directions for their use in physics-based inverse problems.

Abstract: Diffusion models have emerged as powerful generative tools with applications
in computer vision and scientific machine learning (SciML), where they have
been used to solve large-scale probabilistic inverse problems. Traditionally,
these models have been derived using principles of variational inference,
denoising, statistical signal processing, and stochastic differential
equations. In contrast to the conventional presentation, in this study we
derive diffusion models using ideas from linear partial differential equations
and demonstrate that this approach has several benefits that include a
constructive derivation of the forward and reverse processes, a unified
derivation of multiple formulations and sampling strategies, and the discovery
of a new class of variance preserving models. We also apply the conditional
version of these models to solve canonical conditional density estimation
problems and challenging inverse problems. These problems help establish
benchmarks for systematically quantifying the performance of different
formulations and sampling strategies in this study and for future studies.
Finally, we identify and implement a mechanism through which a single diffusion
model can be applied to measurements obtained from multiple measurement
operators. Taken together, the contents of this manuscript provide a new
understanding of and several new directions in the application of diffusion
models to solving physics-based inverse problems.

</details>


### [757] [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/pdf/2504.13932)
*Deyu Cao, Samin Aref*

Main category: cs.LG

TL;DR: The paper explores ultra-low-bit quantization to improve resource efficiency in large language models, proposing a novel method that outperforms ApiQ with reduced accuracy degradation.


<details>
  <summary>Details</summary>
Motivation: Addressing environmental and economic concerns of resource-intensive inference in large language models by enhancing quantization techniques.

Method: Combines quantization-aware training with ApiQ's partial training, then introduces a saliency-aware regularization term for ultra-low-bit quantization.

Result: Reduces ApiQ's accuracy degradation by 10.85% and 7.54% on LLaMA 7B and 13B benchmarks.

Conclusion: The proposed method extends ApiQ's performance without full retraining, offering a practical solution for resource-efficient model deployment.

Abstract: The growing use of large language models has raised environmental and
economic concerns about their intensity of resource usage during inference.
Serving these models to each user requires substantial energy and water for
cooling. Model compression techniques like quantization can shrink large
language models and make them more resource efficient at the cost of potential
performance degradation. Quantization methods compress model size through
replacing their high-precision parameters by quantized values of lower
precision. Among existing methods, the ApiQ method achieves superior accuracy
preservation at minimal memory and time overhead. We investigate two ideas to
extend performance in ultra-low-bit quantization beyond ApiQ's level. First, we
look into combining existing quantization-aware training techniques with ApiQ's
partial training. We show that this does not outperform the baseline ApiQ
method with limited training data and frozen weights. This leads to two key
insights: (1) The substantial representational capacity that is gained through
full retraining is unlikely to be feasible through partial training. (2) This
gain may depend on using a large and diverse dataset in quantization-aware
training. Second, through a novel approach informed by the two insights, we
propose an ultra-low-bit quantization method that builds upon ApiQ and extends
its performance without the need for full retraining. This publicly available
method relies on a saliency-aware regularization term that prioritizes
preserving the most impactful parameters during quantization. Our experiments
on LLaMA 7B and 13B benchmarks demonstrate that our method reduces the ApiQ's
accuracy degradation by 10.85\% and 7.54\% respectively.

</details>


### [758] [Scaling and Beyond: Advancing Spatial Reasoning in MLLMs Requires New Recipes](https://arxiv.org/pdf/2504.15037)
*Huanyu Zhang, Chengzu Li, Wenshan Wu, Shaoguang Mao, Yifan Zhang, Haochen Tian, Ivan Vulić, Zhang Zhang, Liang Wang, Tieniu Tan, Furu Wei*

Main category: cs.LG

TL;DR: The paper highlights the limitations of Multimodal Large Language Models (MLLMs) in spatial reasoning and proposes a dedicated approach to address this gap for broader real-world applications.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack effective spatial reasoning, limiting their interaction with the physical world. The paper argues this won't improve by scaling existing methods alone.

Method: The authors establish a framework for spatial reasoning in MLLMs, analyze current methodologies, and identify limitations and opportunities.

Result: The analysis reveals critical gaps in training data and reasoning mechanisms, pointing to potential improvements.

Conclusion: The paper calls for focused research to enhance spatial reasoning in MLLMs, aiming for human-like capabilities.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
performance in general vision-language tasks. However, recent studies have
exposed critical limitations in their spatial reasoning capabilities. This
deficiency in spatial reasoning significantly constrains MLLMs' ability to
interact effectively with the physical world, thereby limiting their broader
applications. We argue that spatial reasoning capabilities will not naturally
emerge from merely scaling existing architectures and training methodologies.
Instead, this challenge demands dedicated attention to fundamental
modifications in the current MLLM development approach. In this position paper,
we first establish a comprehensive framework for spatial reasoning within the
context of MLLMs. We then elaborate on its pivotal role in real-world
applications. Through systematic analysis, we examine how individual components
of the current methodology, from training data to reasoning mechanisms,
influence spatial reasoning capabilities. This examination reveals critical
limitations while simultaneously identifying promising avenues for advancement.
Our work aims to direct the AI research community's attention toward these
crucial yet underexplored aspects. By highlighting these challenges and
opportunities, we seek to catalyze progress toward achieving human-like spatial
reasoning capabilities in MLLMs.

</details>


### [759] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/pdf/2504.16580)
*Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen*

Main category: cs.LG

TL;DR: A novel generative framework combines Implicit Neural Representations (INRs) and Transformer-based hypernetworks for scalable and efficient function generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability and representation limitations of MLP-based hypernetworks in generating INRs, leveraging the strengths of Transformers and latent variable models.

Method: Integrates Transformer-based hypernetworks into latent variable models, replacing standard decoders. Supports training from scratch or fine-tuning via hyper-transforming.

Result: Demonstrates improved scalability, expressiveness, and generalization across multiple modalities compared to prior INR-based models.

Conclusion: Establishes a unified, flexible framework for structured function representation learning, enabling efficient adaptation of existing generative models.

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming: a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining. We validate our approach across multiple modalities,
demonstrating improved scalability, expressiveness, and generalization over
existing INR-based generative models. Our findings establish a unified and
flexible framework for learning structured function representations.

</details>


### [760] [Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification](https://arxiv.org/pdf/2505.01660)
*Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang*

Main category: cs.LG

TL;DR: Focal-SAM improves generalization in long-tailed datasets by assigning class-wise penalties for sharpness, balancing efficiency and control without extra backpropagations.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational efficiency and fine-grained control in existing long-tail SAM variants (ImbSAM, CC-SAM).

Method: Introduces Focal-SAM, which penalizes class-wise sharpness differently, avoiding multiple backpropagations.

Result: Achieves fine-grained control efficiently, validated by experiments on traditional and foundation models.

Conclusion: Focal-SAM offers a practical solution for long-tailed datasets with theoretical and empirical support.

Abstract: Real-world datasets often follow a long-tailed distribution, making
generalization to tail classes difficult. Recent methods resorted to long-tail
variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to
improve generalization by flattening the loss landscape. However, these
attempts face a trade-off between computational efficiency and control over the
loss landscape. On the one hand, ImbSAM is efficient but offers only coarse
control as it excludes head classes from the SAM process. On the other hand,
CC-SAM provides fine-grained control through class-dependent perturbations but
at the cost of efficiency due to multiple backpropagations. Seeing this
dilemma, we introduce Focal-SAM, which assigns different penalties to
class-wise sharpness, achieving fine-grained control without extra
backpropagations, thus maintaining efficiency. Furthermore, we theoretically
analyze Focal-SAM's generalization ability and derive a sharper generalization
bound. Extensive experiments on both traditional and foundation models validate
the effectiveness of Focal-SAM.

</details>


### [761] [ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $α$-$β$-Divergence](https://arxiv.org/pdf/2505.04560)
*Guanghui Wang, Zhiyong Yang, Zitai Wang, Shi Wang, Qianqian Xu, Qingming Huang*

Main category: cs.LG

TL;DR: ABKD proposes a framework using α-β-divergence to balance Hardness-Concentration and Confidence-Concentration effects in Knowledge Distillation, outperforming FKLD and RKLD.


<details>
  <summary>Details</summary>
Motivation: The core challenge in KD is balancing Hardness-Concentration and Confidence-Concentration effects, which are entangled in FKLD and RKLD but in extreme forms.

Method: ABKD uses α-β-divergence to interpolate between FKLD and RKLD, achieving a trade-off between the two effects.

Result: ABKD outperforms FKLD and RKLD across 17 datasets and 12 teacher-student settings.

Conclusion: ABKD effectively balances mode-concentration effects, improving KD performance.

Abstract: Knowledge Distillation (KD) transfers knowledge from a large teacher model to
a smaller student model by minimizing the divergence between their output
distributions, typically using forward Kullback-Leibler divergence (FKLD) or
reverse KLD (RKLD). It has become an effective training paradigm due to the
broader supervision information provided by the teacher distribution compared
to one-hot labels. We identify that the core challenge in KD lies in balancing
two mode-concentration effects: the \textbf{\textit{Hardness-Concentration}}
effect, which refers to focusing on modes with large errors, and the
\textbf{\textit{Confidence-Concentration}} effect, which refers to focusing on
modes with high student confidence. Through an analysis of how probabilities
are reassigned during gradient updates, we observe that these two effects are
entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too
weak in FKLD, causing the student to fail to concentrate on the target class.
In contrast, both are too strong in RKLD, causing the student to overly
emphasize the target class while ignoring the broader distributional
information from the teacher. To address this imbalance, we propose ABKD, a
generic framework with $\alpha$-$\beta$-divergence. Our theoretical results
show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving
an effective trade-off between these effects. Extensive experiments on 17
language/vision datasets with 12 teacher-student settings confirm its efficacy.
The code is available at https://github.com/ghwang-s/abkd.

</details>


### [762] [Learning Soft Sparse Shapes for Efficient Time-Series Classification](https://arxiv.org/pdf/2505.06892)
*Zhen Liu, Yicheng Luo, Boyuan Li, Emadeldeen Eldele, Min Wu, Qianli Ma*

Main category: cs.LG

TL;DR: SoftShape introduces soft shape sparsification and learning blocks for efficient time series classification, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing shapelet-based methods discard non-discriminative shapes, potentially losing beneficial ones and ignoring varying contributions of shapelets.

Method: Proposes soft shape sparsification (merging lower-scored shapes) and learning blocks (intra- and inter-shape pattern learning) using a learnable router and shared expert network.

Result: SoftShape outperforms state-of-the-art methods and provides interpretable results.

Conclusion: The SoftShape model effectively retains and differentiates all subsequence information while improving classification efficiency and interpretability.

Abstract: Shapelets are discriminative subsequences (or shapes) with high
interpretability in time series classification. Due to the time-intensive
nature of shapelet discovery, existing shapelet-based methods mainly focus on
selecting discriminative shapes while discarding others to achieve candidate
subsequence sparsification. However, this approach may exclude beneficial
shapes and overlook the varying contributions of shapelets to classification
performance. To this end, we propose a Soft sparse Shapes (SoftShape) model for
efficient time series classification. Our approach mainly introduces soft shape
sparsification and soft shape learning blocks. The former transforms shapes
into soft representations based on classification contribution scores, merging
lower-scored ones into a single shape to retain and differentiate all
subsequence information. The latter facilitates intra- and inter-shape temporal
pattern learning, improving model efficiency by using sparsified soft shapes as
inputs. Specifically, we employ a learnable router to activate a subset of
class-specific expert networks for intra-shape pattern learning. Meanwhile, a
shared expert network learns inter-shape patterns by converting sparsified
shapes into sequences. Extensive experiments show that SoftShape outperforms
state-of-the-art methods and produces interpretable results.

</details>


### [763] [Relative Overfitting and Accept-Reject Framework](https://arxiv.org/pdf/2505.07783)
*Yanxin Liu, Yunqi Zhang*

Main category: cs.LG

TL;DR: The paper addresses scaling law bottlenecks in LLMs by identifying noise effects as the root cause and proposing the AR framework to improve performance with lower costs.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in LLM scaling laws caused by noise effects and diminishing returns, the study explores performance differences and introduces 'relative overfitting.'

Method: The paper proposes the Accept-Reject (AR) framework and AR Law, validated using self-built and pre-trained models across various NLP tasks.

Result: The AR framework achieves better performance improvements with lower parameter and computational costs, demonstrating universal and stable effectiveness.

Conclusion: The approach has potential applications beyond NLP, such as CV and AI for science, aiming to help scaling laws overcome bottlenecks.

Abstract: Currently, the scaling law of Large Language Models (LLMs) faces challenges
and bottlenecks. This paper posits that noise effects, stemming from changes in
the signal-to-noise ratio under diminishing marginal returns, are the root
cause of these issues. To control this noise, we investigated the differences
between models with performance advantages and disadvantages, introducing the
concept of "relative overfitting." Based on their complementary strengths, we
have proposed an application framework, Accept-Reject (AR), and the associated
AR Law, which operates within this framework to elucidate the patterns of
performance changes after model integration. In Natural Language Processing
(NLP), we use LLMs and Small Language Models (SLMs) as the medium for
discussion. This framework enables SLMs to exert a universal positive influence
on LLM decision outputs, rather than the intuitively expected potential
negative influence. We validated our approach using self-built models based on
mainstream architectures and pre-trained mainstream models across multiple
datasets, including basic language modeling, long-context tasks, subject
examination, and question-answering (QA) benchmarks. The results demonstrate
that through our framework, compared to increasing the LLM's parameters, we can
achieve better performance improvements with significantly lower parameter and
computational costs in many scenarios. These improvements are universal,
stable, and effective. Furthermore, we explore the potential of "relative
overfitting" and the AR framework in other machine learning domains, such as
computer vision (CV) and AI for science. We hope the proposed approach can help
scale laws overcome existing bottlenecks.

</details>


### [764] [A Dataless Reinforcement Learning Approach to Rounding Hyperplane Optimization for Max-Cut](https://arxiv.org/pdf/2505.13405)
*Gabriel Malikal, Ismail Alkhouri, Alvaro Velasquez, Adam M Alessio, Saiprasad Ravishankar*

Main category: cs.LG

TL;DR: A reinforcement learning approach is proposed to improve hyperplane rounding in the Goemans-Williamson algorithm for MaxCut, outperforming it without requiring training data.


<details>
  <summary>Details</summary>
Motivation: Heuristic and learning-based methods for MaxCut lack generalizability and scalability, prompting a need for a more robust solution.

Method: A non-episodic reinforcement learning formulation optimizes hyperplane selection in the GW algorithm via a Markov Decision Process.

Result: The method consistently achieves better cuts on large-scale graphs with diverse densities and degree distributions.

Conclusion: The proposed approach enhances the GW algorithm's performance without relying on training data, offering a scalable solution for MaxCut.

Abstract: The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal
solution is NP-hard in the worst case. As a result, heuristic-based algorithms
are commonly used, though their design often requires significant domain
expertise. More recently, learning-based methods trained on large (un)labeled
datasets have been proposed; however, these approaches often struggle with
generalizability and scalability. A well-known approximation algorithm for
MaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic
Unconstrained Binary Optimization (QUBO) formulation into a semidefinite
program (SDP). The GW algorithm then applies hyperplane rounding by uniformly
sampling a random hyperplane to convert the SDP solution into binary node
assignments. In this paper, we propose a training-data-free approach based on a
non-episodic reinforcement learning formulation, in which an agent learns to
select improved rounding hyperplanes that yield better cuts than those produced
by the GW algorithm. By optimizing over a Markov Decision Process (MDP), our
method consistently achieves better cuts across large-scale graphs with varying
densities and degree distributions.

</details>


### [765] [Training on Plausible Counterfactuals Removes Spurious Correlations](https://arxiv.org/pdf/2505.16583)
*Shpresim Sadiku, Kartikeya Chitranshi, Hiroshi Kera, Sebastian Pokutta*

Main category: cs.LG

TL;DR: Training classifiers on plausible counterfactual explanations (p-CFEs) with incorrect labels improves accuracy and reduces bias compared to adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore if learning from p-CFEs, which are plausible under the data distribution, can enhance classifier performance and reduce bias.

Method: Train classifiers on p-CFEs labeled with incorrect target classes and evaluate their performance on unperturbed inputs.

Result: Classifiers trained on p-CFEs achieve high in-distribution accuracy and significantly reduced bias compared to those trained on adversarial perturbations.

Conclusion: Learning from p-CFEs is more effective than adversarial perturbations for improving classifier accuracy and fairness.

Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.

</details>


### [766] [JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model](https://arxiv.org/pdf/2505.17257)
*Qihao Duan, Bingding Huang, Zhenqiao Song, Irina Lehmann, Lei Gu, Roland Eils, Benjamin Wild*

Main category: cs.LG

TL;DR: JanusDNA is a bidirectional DNA foundation model combining autoregressive and masked modeling efficiency with a hybrid architecture (Mamba, Attention, MoE), achieving SOTA results on genomic benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with genomic data due to long-range dependencies and bidirectional nature of DNA, requiring efficient and bidirectional modeling.

Method: JanusDNA uses a hybrid Mamba, Attention, and MoE architecture for efficient long-range and bidirectional DNA sequence modeling.

Result: JanusDNA processes 1M base pairs on a single GPU and outperforms models with 250x more parameters on genomic benchmarks.

Conclusion: JanusDNA addresses LLM limitations for genomics, offering efficient, bidirectional, and scalable DNA sequence modeling.

Abstract: Large language models (LLMs) have revolutionized natural language processing
and are increasingly applied to other sequential data types, including genetic
sequences. However, adapting LLMs to genomics presents significant challenges.
Capturing complex genomic interactions requires modeling long-range
dependencies within DNA sequences, where interactions often span over 10,000
base pairs, even within a single gene, posing substantial computational burdens
under conventional model architectures and training paradigms. Moreover,
standard LLM training approaches are suboptimal for DNA: autoregressive
training, while efficient, supports only unidirectional understanding. However,
DNA is inherently bidirectional, e.g., bidirectional promoters regulate
transcription in both directions and account for nearly 11% of human gene
expression. Masked language models (MLMs) allow bidirectional understanding but
are inefficient, as only masked tokens contribute to the loss per step. To
address these limitations, we introduce JanusDNA, the first bidirectional DNA
foundation model built upon a novel pretraining paradigm that combines the
optimization efficiency of autoregressive modeling with the bidirectional
comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and
Mixture of Experts (MoE) architecture, combining long-range modeling of
Attention with efficient sequential learning of Mamba. MoE layers further scale
model capacity via sparse activation while keeping computational cost low.
Notably, JanusDNA processes up to 1 million base pairs at single nucleotide
resolution on a single 80GB GPU. Extensive experiments and ablations show
JanusDNA achieves new SOTA results on three genomic representation benchmarks,
outperforming models with 250x more activated parameters. Code:
https://github.com/Qihao-Duan/JanusDNA

</details>


### [767] [Why Do More Experts Fail? A Theoretical Analysis of Model Merging](https://arxiv.org/pdf/2505.21226)
*Zijing Wang, Xingle Xu, Yongkang Liu, Yiqun Zhang, Peiqin Lin, Shi Feng, Xiaocui Yang, Daling Wang, Hinrich Schütze*

Main category: cs.LG

TL;DR: The paper explores scalability limits in model merging, identifying an upper bound and diminishing returns as more models are merged. It introduces RHT to improve performance and validates findings on 12 benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining performance gains in model merging as the number of merged models increases.

Method: Theoretical analysis using Gaussian Width and Approximate Kinematics Theory, plus the introduction of the Reparameterized Heavy-Tailed (RHT) method.

Result: Empirical validation on 12 benchmarks confirms the theoretical limits and shows RHT enhances performance.

Conclusion: The study identifies scalability constraints in model merging and proposes RHT as a solution, encouraging further research.

Abstract: Model merging dramatically reduces storage and computational resources by
combining multiple expert models into a single multi-task model. Although
recent model merging methods have shown promising results, they struggle to
maintain performance gains as the number of merged models increases. In this
paper, we investigate the key obstacles that limit the scalability of model
merging when integrating a large number of expert models. First, we prove that
there is an upper bound on model merging. Further theoretical analysis reveals
that the limited effective parameter space imposes a strict constraint on the
number of models that can be successfully merged. Gaussian Width shows that the
marginal benefit of merging additional models diminishes according to a
strictly concave function. This implies that the effective parameter space
becomes rapidly saturated as the number of merged models increases.
Furthermore, using Approximate Kinematics Theory, we prove the existence of a
unique optimal threshold beyond which adding more models does not yield
significant performance improvements. At the same time, we introduce a
straightforward Reparameterized Heavy-Tailed method (RHT) to extend the
coverage of the merged model, thereby enhancing its performance. Empirical
results on 12 benchmarks, including both knowledge-intensive and
general-purpose tasks, validate our theoretical analysis. We believe that these
results spark further research beyond the current scope of model merging. The
source code is in the Github repository:
https://github.com/wzj1718/ModelMergingAnalysis.

</details>


### [768] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/pdf/2505.23190)
*Yekun Zhu, Min Tang, Zheng Ma*

Main category: cs.LG

TL;DR: DeepRTE is a novel neural network approach for solving the steady-state Radiative Transfer Equation (RTE) efficiently and accurately, outperforming traditional methods and existing neural networks.


<details>
  <summary>Details</summary>
Motivation: The RTE governs radiation propagation in various domains, but traditional methods are computationally expensive. DeepRTE aims to improve efficiency and accuracy.

Method: DeepRTE embeds physical information, uses a mathematically-informed architecture, and incorporates multi-head attention and Green's function theory for a mesh-free, zero-shot capable framework.

Result: DeepRTE achieves superior computational efficiency and high accuracy with fewer parameters, validated by numerical experiments.

Conclusion: DeepRTE is an effective, efficient, and accurate solution for the steady-state RTE, with potential applications in diverse fields.

Abstract: In this paper, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our DeepRTE framework demonstrates superior computational
efficiency for solving the steady-state RTE, surpassing traditional methods and
existing neural network approaches. This efficiency is achieved by embedding
physical information through derivation of the RTE and mathematically-informed
network architecture. Concurrently, DeepRTE achieves high accuracy with
significantly fewer parameters, largely due to its incorporation of mechanisms
such as multi-head attention. Furthermore, DeepRTE is a mesh-free neural
operator framework with inherent zero-shot capability. This is achieved by
incorporating Green's function theory and pre-training with delta-function
inflow boundary conditions into both its architecture design and training data
construction. The efficacy of the proposed approach is substantiated through
comprehensive numerical experiments.

</details>


### [769] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/pdf/2505.23721)
*Sean Current, Ziqi Chen, Daniel Adu-Ampratwum, Xia Ning, Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER, a template-free retrosynthesis method using categorical diffusion, outperforms autoregressive models in top-k accuracy by predicting SMILES sequences in unison.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models for chemical retrosynthesis are limited; DiffER aims to overcome these constraints with a new approach.

Method: DiffER uses an ensemble of categorical diffusion models with a novel length prediction component to predict SMILES sequences.

Result: Achieves state-of-the-art top-1 accuracy and competitive top-k performance, learning diverse synthetic techniques.

Conclusion: DiffER is a strong baseline for template-free models, with SMILES length prediction being crucial for performance.

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [770] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/pdf/2505.24360)
*Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy*

Main category: cs.LG

TL;DR: Sparse autoencoders (SAEs) and ITDA improve interpretability and control in language models, applied successfully to text-to-image diffusion models like Flux 1.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability and control of language model activations, particularly in large text-to-image diffusion models.

Method: Applied Sparse Autoencoders (SAEs) and Inference-Time Decomposition of Activations (ITDA) to Flux 1, using a visual automated interpretation pipeline.

Result: SAEs accurately reconstruct embeddings and outperform MLP neurons in interpretability. Both SAEs and ITDA enable steering image generation.

Conclusion: SAEs and ITDA are effective for interpretability and control in large diffusion models, with SAEs showing superior performance.

Abstract: Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [771] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/pdf/2505.24779)
*Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu*

Main category: cs.LG

TL;DR: A benchmark framework for evaluating MILP instance generation methods, assessing quality across mathematical validity, structural similarity, computational hardness, and utility in ML tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized evaluation techniques for synthetic MILP instances, driven by the need for diverse optimization datasets and limitations of static benchmarks.

Method: Introduces a unified framework with solver-independent and solver-dependent metrics, analyzing solver-internal features like root node gap and heuristic success rates.

Result: Demonstrates effectiveness in comparing instance sets, revealing nuanced computational resemblances through solver behavior.

Conclusion: Aims to facilitate robust comparisons, improve instance generator quality, and enhance reliability of synthetic MILP data in research.

Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [772] [On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective](https://arxiv.org/pdf/2506.01213)
*Ning Zhang, Henry Kenlay, Li Zhang, Mihai Cucuringu, Xiaowen Dong*

Main category: cs.LG

TL;DR: The paper proposes a novel framework for analyzing the stability of GCNNs under graph topology perturbations, introducing a distribution-aware approach for probabilistic stability analysis.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical understanding of GCNN stability is limited, hindering robust model development. The study aims to address this gap by analyzing how graph perturbations affect outputs.

Method: A distribution-aware formulation is introduced to characterize output perturbations across a broad range of input data, providing a probabilistic perspective on stability.

Result: Experiments validate the framework's benefits over baselines, showing improved representation stability and robustness against adversarial attacks.

Conclusion: The study highlights the importance of incorporating data distribution into stability analysis, offering practical significance for robust GCNN deployment.

Abstract: Graph convolutional neural networks (GCNNs) have emerged as powerful tools
for analyzing graph-structured data, achieving remarkable success across
diverse applications. However, the theoretical understanding of the stability
of these models, i.e., their sensitivity to small changes in the graph
structure, remains in rather limited settings, hampering the development and
deployment of robust and trustworthy models in practice. To fill this gap, we
study how perturbations in the graph topology affect GCNN outputs and propose a
novel formulation for analyzing model stability. Unlike prior studies that
focus only on worst-case perturbations, our distribution-aware formulation
characterizes output perturbations across a broad range of input data. This
way, our framework enables, for the first time, a probabilistic perspective on
the interplay between the statistical properties of the node data and
perturbations in the graph topology. We conduct extensive experiments to
validate our theoretical findings and demonstrate their benefits over existing
baselines, in terms of both representation stability and adversarial attacks on
downstream tasks. Our results demonstrate the practical significance of the
proposed formulation and highlight the importance of incorporating data
distribution into stability analysis.

</details>


### [773] [MLorc: Momentum Low-rank Compression for Large Language Model Adaptation](https://arxiv.org/pdf/2506.01897)
*Wei Shen, Zhang Yaxiang, Minhui Huang, Mengfan Xu, Jiawei Zhang, Cong Shen*

Main category: cs.LG

TL;DR: MLorc is a memory-efficient training method for LLMs that compresses momentum instead of gradients, outperforming existing low-rank approaches like LoRA and GaLore while matching full fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: To address the high memory demands of full-parameter fine-tuning for large language models (LLMs).

Method: Proposes Momentum Low-rank compression (MLorc), which compresses and reconstructs momentum to avoid fixed-rank constraints on weight updates, preserving training dynamics.

Result: MLorc outperforms other memory-efficient methods, matches full fine-tuning performance with small ranks (e.g., r=4), and generalizes across optimizers without compromising efficiency.

Conclusion: MLorc is a theoretically guaranteed, efficient alternative to full fine-tuning, offering superior performance and memory savings.

Abstract: With increasing size of large language models (LLMs), full-parameter
fine-tuning imposes substantial memory demands. To alleviate this, we propose a
novel memory-efficient training paradigm called Momentum Low-rank compression
(MLorc). By directly compressing and reconstructing momentum rather than
gradients, MLorc avoids imposing a fixed-rank constraint on weight update
matrices and better preserves the training dynamics of full-parameter
fine-tuning, in contrast to existing low-rank approaches such as LoRA and
GaLore. Empirically, MLorc consistently outperforms other memory-efficient
training methods, matches or even exceeds the performance of full fine-tuning
with a small rank (e.g., $r=4$), and generalizes well across different
optimizers -- all while not compromising time or memory efficiency.
Furthermore, we provide a theoretical guarantee for its convergence under
reasonable assumptions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [774] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/pdf/2506.02931)
*Praneet Sai Madhu Surabhi, Dheeraj Reddy Mudireddy, Jian Tao*

Main category: cs.MA

TL;DR: ThinkTank is a scalable framework transforming specialized AI agents into collaborative platforms for complex problem-solving, leveraging role abstraction, meeting generalization, and knowledge integration.


<details>
  <summary>Details</summary>
Motivation: To create a versatile AI collaboration platform that adapts scientific methodologies for diverse domains, ensuring privacy and scalability.

Method: Uses role abstraction, generalized meeting structures, and Retrieval-Augmented Generation with advanced knowledge storage for expertise creation.

Result: Enables cost-effective, secure, and scalable AI collaboration, outperforming cloud-based alternatives.

Conclusion: ThinkTank is a universal platform for AI-driven collaborative problem-solving, with open-source availability.

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


### [775] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/pdf/2506.03053)
*Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham*

Main category: cs.MA

TL;DR: The paper introduces MAEBE to evaluate emergent risks in multi-agent AI ensembles, revealing brittle moral preferences and unpredictable group dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional AI safety evaluations fail to address risks in multi-agent systems, necessitating a new framework like MAEBE.

Method: The MAEBE framework, combined with the Greatest Good Benchmark and a double-inversion question technique, assesses LLM moral preferences and group dynamics.

Result: Findings show brittle moral preferences, unpredictable ensemble behavior, and phenomena like peer pressure influencing convergence.

Conclusion: AI systems must be evaluated in interactive, multi-agent contexts to address emergent safety and alignment challenges.

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [776] [EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR](https://arxiv.org/pdf/2506.02380)
*Zihao Ding, Cheng-Tse Lee, Mufeng Zhu, Tao Guan, Yuan-Chun Sun, Cheng-Hsin Hsu, Yao Liu*

Main category: cs.MM

TL;DR: EyeNavGS is the first public 6-DoF navigation dataset for 3D Gaussian Splatting (3DGS) scenes, featuring traces from 46 participants exploring 12 real-world scenes, collected using Meta Quest Pro headsets.


<details>
  <summary>Details</summary>
Motivation: The lack of realistic user navigation data for 3DGS scenes hinders development and evaluation of applications. EyeNavGS addresses this gap.

Method: Data was collected from 46 participants exploring 12 diverse 3DGS scenes using Meta Quest Pro headsets, recording head pose and eye gaze. Scene initialization ensured perceptual comfort.

Result: The dataset includes navigation traces, open-source tools for data processing, and a viewer with record-and-replay functionalities.

Conclusion: EyeNavGS provides essential resources for research in 6-DoF viewport prediction, adaptive streaming, 3D saliency, and foveated rendering for 3DGS.

Abstract: 3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.

</details>


### [777] [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/pdf/2506.02414)
*Fengjin Li, Jie Wang, Yadong Niu, Yongqing Wang, Meng Meng, Jian Luan, Zhiyong Wu*

Main category: cs.MM

TL;DR: StarVC is an autoregressive VC framework that uses text tokens to improve voice conversion by better preserving linguistic content and speaker characteristics.


<details>
  <summary>Details</summary>
Motivation: Traditional VC methods neglect explicit use of linguistic content, limiting performance. StarVC addresses this by integrating text modeling.

Method: StarVC predicts text tokens before synthesizing acoustic features, leveraging structured semantic features.

Result: StarVC outperforms conventional VC methods in preserving linguistic content (WER, CER) and speaker characteristics (SECS, MOS).

Conclusion: Explicit text modeling in StarVC enhances VC performance, demonstrating its superiority over traditional methods.

Abstract: Voice Conversion (VC) modifies speech to match a target speaker while
preserving linguistic content. Traditional methods usually extract speaker
information directly from speech while neglecting the explicit utilization of
linguistic content. Since VC fundamentally involves disentangling speaker
identity from linguistic content, leveraging structured semantic features could
enhance conversion performance. However, previous attempts to incorporate
semantic features into VC have shown limited effectiveness, motivating the
integration of explicit text modeling. We propose StarVC, a unified
autoregressive VC framework that first predicts text tokens before synthesizing
acoustic features. The experiments demonstrate that StarVC outperforms
conventional VC methods in preserving both linguistic content (i.e., WER and
CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found
at: https://thuhcsi.github.io/StarVC/.

</details>


### [778] [Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style-Rich Representation](https://arxiv.org/pdf/2506.02997)
*Yongqi Wang, Chunlei Zhang, Hangting Chen, Zhou Zhao, Dong Yu*

Main category: cs.MM

TL;DR: A two-stage style-controllable TTS system using language models and quantized style-rich representations improves fine-grained control and content robustness.


<details>
  <summary>Details</summary>
Motivation: Existing controllable TTS models lack fine-grained control and suffer from limited high-quality data.

Method: A two-stage approach: (1) autoregressive transformer generates style-rich tokens from text and control signals; (2) codec tokens are generated from text and style-rich tokens.

Result: Training on extensive datasets enhances content robustness and control over multiple attributes like emotion and speaker timbre.

Conclusion: The proposed system achieves fine-grained control over stylistic attributes and speaker-specific adjustments, demonstrated through audio samples.

Abstract: Controllable TTS models with natural language prompts often lack the ability
for fine-grained control and face a scarcity of high-quality data. We propose a
two-stage style-controllable TTS system with language models, utilizing a
quantized masked-autoencoded style-rich representation as an intermediary. In
the first stage, an autoregressive transformer is used for the conditional
generation of these style-rich tokens from text and control signals. The second
stage generates codec tokens from both text and sampled style-rich tokens.
Experiments show that training the first-stage model on extensive datasets
enhances the content robustness of the two-stage model as well as control
capabilities over multiple attributes. By selectively combining discrete labels
and speaker embeddings, we explore fully controlling the speaker's timbre and
other stylistic information, and adjusting attributes like emotion for a
specified speaker. Audio samples are available at
https://style-ar-tts.github.io.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [779] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/pdf/2506.02039)
*Haoshuai Zhou, Changgeng Mo, Boxuan Cao, Linkai Li, Shan Xiang Wang*

Main category: eess.AS

TL;DR: A novel deep learning model (SSIPNet) uses existing intelligibility data to predict speech performance, outperforming traditional audiogram-based methods.


<details>
  <summary>Details</summary>
Motivation: Audiograms are limited in accuracy for speech intelligibility prediction, prompting the need for a more personalized approach.

Method: SSIPNet leverages speech foundation models and support (audio, score) pairs to predict intelligibility for unseen audio.

Result: Outperforms audiogram-based predictions on the Clarity Prediction Challenge dataset, even with few support pairs.

Conclusion: Introduces a new paradigm for personalized speech intelligibility prediction using deep learning.

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [780] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/pdf/2506.02078)
*Emmy Postma, Cristian Tejedor-Garcia*

Main category: eess.AS

TL;DR: The study evaluates three pre-trained audio embeddings for Parkinson's Disease (PD) classification, finding OpenL3 most effective, while Wav2Vec2.0 exhibits gender bias.


<details>
  <summary>Details</summary>
Motivation: Speech impairments are key PD biomarkers, but deep acoustic features' effectiveness varies due to individual speaker differences, a gap in existing research.

Method: Three pre-trained audio embeddings (OpenL3, VGGish, Wav2Vec2.0) are tested on the NeuroVoz dataset for PD classification in DDK and LR tasks.

Result: OpenL3 performs best overall; Wav2Vec2.0 shows gender bias favoring males in DDK tasks. Misclassifications reveal challenges with atypical speech patterns.

Conclusion: Improved feature extraction and model robustness are needed to address variability in PD detection, especially for atypical cases.

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [781] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/pdf/2506.02080)
*Aditya Kamlesh Parikh, Cristian Tejedor-Garcia, Catia Cucchiarini, Helmer Strik*

Main category: eess.AS

TL;DR: The paper introduces a substitution-aware alignment-free GOP method to improve efficiency in CAPT systems by restricting phoneme substitutions based on clusters and common errors, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current GOP metrics in CAPT systems rely on forced alignments, which are error-prone due to acoustic variability, while alignment-free methods are inefficient.

Method: Proposes a substitution-aware alignment-free GOP that restricts phoneme substitutions using phoneme clusters and common learner errors.

Result: Evaluated on two L2 English datasets, the method outperformed baseline alignment-free methods in both restricted (RPS) and unrestricted (UPS) setups.

Conclusion: The approach enhances efficiency and accuracy in pronunciation assessment, with potential for further research to refine the method.

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


### [782] [Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi](https://arxiv.org/pdf/2506.02166)
*Arnav Rustagi, Satvik Bajpai, Nimrat Kaur, Siddharth Siddharth*

Main category: eess.AS

TL;DR: The paper introduces Dhvani, a CAPT system for Hindi, addressing the lack of pronunciation tools for Indian languages. It includes synthetic speech for mispronunciations and personalized feedback.


<details>
  <summary>Details</summary>
Motivation: There's a significant gap in CAPT tools for Indian languages, despite their large speaker base. Hindi, with 600M speakers, is prioritized.

Method: Proposes Dhvani, a Hindi CAPT system using Devanagari graphemes and phonemic analysis for feedback, plus synthetic speech for mispronunciations.

Result: A novel system for Hindi pronunciation training with personalized feedback capabilities.

Conclusion: Dhvani is a critical step toward filling the CAPT gap for Indian languages, starting with Hindi.

Abstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied
for English. However, there remains a critical gap in its application to Indian
languages with a base of 1.5 billion speakers. Pronunciation tools tailored to
Indian languages are strikingly lacking despite the fact that millions learn
them every year. With over 600 million speakers and being the fourth
most-spoken language worldwide, improving Hindi pronunciation is a vital first
step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT
system for Hindi, 2) synthetic speech generation for Hindi mispronunciations,
and 3) a novel methodology for providing personalized feedback to learners.
While the system often interacts with learners using Devanagari graphemes, its
core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic
orthography to analyze mispronounced speech and provide targeted feedback.

</details>


### [783] [Towards Machine Unlearning for Paralinguistic Speech Processing](https://arxiv.org/pdf/2506.02230)
*Orchid Chetia Phukan, Girish, Mohd Mujtaba Akhtar, Shubham Singh, Swarup Ranjan Behera, Vandana Rajan, Muskaan Singh, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: SISA++ improves Machine Unlearning (MU) for Paralinguistic Speech Processing (PSP), outperforming SOTA method SISA in tasks like Speech Emotion Recognition (SER) and Depression Detection (DD).


<details>
  <summary>Details</summary>
Motivation: To address the need for effective unlearning in PSP tasks (SER and DD) while preserving performance.

Method: Proposes SISA++, an extension of SISA, using weight-averaging of models trained on shards. Provides 'cookbook recipes' for feature and architecture selection.

Result: SISA++ shows better performance preservation post-unlearning in SER (CREMA-D) and DD (E-DAIC) datasets compared to SISA.

Conclusion: SISA++ is effective for MU in PSP, with practical guidelines for future research.

Abstract: In this work, we pioneer the study of Machine Unlearning (MU) for
Paralinguistic Speech Processing (PSP). We focus on two key PSP tasks: Speech
Emotion Recognition (SER) and Depression Detection (DD). To this end, we
propose, SISA++, a novel extension to previous state-of-the-art (SOTA) MU
method, SISA by merging models trained on different shards with
weight-averaging. With such modifications, we show that SISA++ preserves
performance more in comparison to SISA after unlearning in benchmark SER
(CREMA-D) and DD (E-DAIC) datasets. Also, to guide future research for easier
adoption of MU for PSP, we present ``cookbook recipes'' - actionable
recommendations for selecting optimal feature representations and downstream
architectures that can mitigate performance degradation after the unlearning
process.

</details>


### [784] [Investigating the Reasonable Effectiveness of Speaker Pre-Trained Models and their Synergistic Power for SingMOS Prediction](https://arxiv.org/pdf/2506.02232)
*Orchid Chetia Phukan, Girish, Mohd Mujtaba Akhtar, Swarup Ranjan Behera, Pailla Balakrishna Reddy, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: The study explores SingMOS prediction using speaker recognition PTMs (SPTMs) and introduces BATCH, a fusion framework, achieving top performance.


<details>
  <summary>Details</summary>
Motivation: Prior work used SOTA PTMs but overlooked SPTMs for SingMOS prediction. The hypothesis is that SPTMs, due to speaker recognition pre-training, better capture vocal features.

Method: Experiments with SOTA PTMs (including SPTMs and music PTMs) validate the hypothesis. A novel fusion framework, BATCH, uses Bhattacharya Distance for PTM fusion.

Result: SPTMs outperformed other PTMs. BATCH with SPTM fusion achieved top performance, surpassing individual PTMs and baseline fusion techniques.

Conclusion: SPTMs are highly effective for SingMOS prediction. BATCH framework sets a new SOTA, demonstrating the value of speaker recognition PTMs in this domain.

Abstract: In this study, we focus on Singing Voice Mean Opinion Score (SingMOS)
prediction. Previous research have shown the performance benefit with the use
of state-of-the-art (SOTA) pre-trained models (PTMs). However, they haven't
explored speaker recognition speech PTMs (SPTMs) such as x-vector, ECAPA and we
hypothesize that it will be the most effective for SingMOS prediction. We
believe that due to their speaker recognition pre-training, it equips them to
capture fine-grained vocal features (e.g., pitch, tone, intensity) from
synthesized singing voices in a much more better way than other PTMs. Our
experiments with SOTA PTMs including SPTMs and music PTMs validates the
hypothesis. Additionally, we introduce a novel fusion framework, BATCH that
uses Bhattacharya Distance for fusion of PTMs. Through BATCH with the fusion of
speaker recognition SPTMs, we report the topmost performance comparison to all
the individual PTMs and baseline fusion techniques as well as setting SOTA.

</details>


### [785] [Are Mamba-based Audio Foundation Models the Best Fit for Non-Verbal Emotion Recognition?](https://arxiv.org/pdf/2506.02258)
*Mohd Mujtaba Akhtar, Orchid Chetia Phukan, Girish, Swarup Ranjan Behera, Ananda Chandra Nayak, Sanjib Kumar Nayak, Arun Balaji Buduru, Rajesh Sharma*

Main category: eess.AS

TL;DR: The paper introduces MAFMs for NVER, hypothesizing they outperform AAFMs due to better emotional structure capture. It validates this and proposes RENO, a fusion method using Renyi-divergence, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: To improve non-verbal vocal emotion recognition by leveraging MAFMs' state-space modeling over AAFMs' attention mechanisms, and exploring FM fusion for enhanced performance.

Method: Investigates MAFMs vs. AAFMs for NVER, proposes RENO for FM fusion using Renyi-divergence loss and self-attention for intra-representation interaction.

Result: MAFMs outperform AAFMs; RENO achieves top performance, surpassing individual FMs and previous SOTA.

Conclusion: MAFMs and RENO's fusion significantly advance NVER, demonstrating superior performance and setting new benchmarks.

Abstract: In this work, we focus on non-verbal vocal sounds emotion recognition (NVER).
We investigate mamba-based audio foundation models (MAFMs) for the first time
for NVER and hypothesize that MAFMs will outperform attention-based audio
foundation models (AAFMs) for NVER by leveraging its state-space modeling to
capture intrinsic emotional structures more effectively. Unlike AAFMs, which
may amplify irrelevant patterns due to their attention mechanisms, MAFMs will
extract more stable and context-aware representations, enabling better
differentiation of subtle non-verbal emotional cues. Our experiments with
state-of-the-art (SOTA) AAFMs and MAFMs validates our hypothesis. Further,
motivated from related research such as speech emotion recognition, synthetic
speech detection, where fusion of foundation models (FMs) have showed improved
performance, we also explore fusion of FMs for NVER. To this end, we propose,
RENO, that uses renyi-divergence as a novel loss function for effective
alignment of the FMs. It also makes use of self-attention for better
intra-representation interaction of the FMs. With RENO, through the
heterogeneous fusion of MAFMs and AAFMs, we show the topmost performance in
comparison to individual FMs, its fusion and also setting SOTA in comparison to
previous SOTA work.

</details>


### [786] [Enhancing Lyrics Transcription on Music Mixtures with Consistency Loss](https://arxiv.org/pdf/2506.02339)
*Jiawen Huang, Felipe Sousa, Emir Demirel, Emmanouil Benetos, Igor Gadelha*

Main category: eess.AS

TL;DR: The paper explores adapting ASR models for Automatic Lyrics Transcription (ALT) using LoRA and consistency loss, showing modest improvements despite challenges like musical accompaniment.


<details>
  <summary>Details</summary>
Motivation: ALT faces complexity due to singing voice properties, and ASR models degrade in performance for singing. The work aims to bridge this gap.

Method: Uses Low-Rank Adaptation (LoRA) with single- and dual-domain fine-tuning, proposing a consistency loss to align vocal and mixture encoder representations.

Result: Structured training with consistency loss yields modest but consistent gains, outperforming naive dual-domain fine-tuning.

Conclusion: Adapting ASR foundation models for music via structured training shows promise for ALT.

Abstract: Automatic Lyrics Transcription (ALT) aims to recognize lyrics from singing
voices, similar to Automatic Speech Recognition (ASR) for spoken language, but
faces added complexity due to domain-specific properties of the singing voice.
While foundation ASR models show robustness in various speech tasks, their
performance degrades on singing voice, especially in the presence of musical
accompaniment. This work focuses on this performance gap and explores Low-Rank
Adaptation (LoRA) for ALT, investigating both single-domain and dual-domain
fine-tuning strategies. We propose using a consistency loss to better align
vocal and mixture encoder representations, improving transcription on mixture
without relying on singing voice separation. Our results show that while
na\"ive dual-domain fine-tuning underperforms, structured training with
consistency loss yields modest but consistent gains, demonstrating the
potential of adapting ASR foundation models for music.

</details>


### [787] [Adaptive Differential Denoising for Respiratory Sounds Classification](https://arxiv.org/pdf/2506.02505)
*Gaoyang Dong, Zhicheng Zhang, Ping Sun, Minghui Zhang*

Main category: eess.AS

TL;DR: Proposes an Adaptive Differential Denoising network for respiratory sound classification, improving noise suppression and feature preservation, achieving a 1.99% performance boost over previous methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in respiratory sound classification due to background noise and inadequate denoising in existing systems.

Method: Introduces three innovations: Adaptive Frequency Filter, Differential Denoise Layer, and bias denoising loss to enhance noise suppression and feature retention.

Result: Achieves 65.53% Score on ICBHI2017 dataset, a 1.99% improvement over prior state-of-the-art.

Conclusion: The method effectively balances noise reduction and diagnostic feature preservation, demonstrating superior performance.

Abstract: Automated respiratory sound classification faces practical challenges from
background noise and insufficient denoising in existing systems.
  We propose Adaptive Differential Denoising network, that integrates noise
suppression and pathological feature preservation via three innovations:
  1) Adaptive Frequency Filter with learnable spectral masks and soft shrink to
eliminate noise while retaining diagnostic high-frequency components;
  2) A Differential Denoise Layer using differential attention to reduce
noise-induced variations through augmented sample comparisons;
  3) A bias denoising loss jointly optimizing classification and robustness
without clean labels.
  Experiments on the ICBHI2017 dataset show that our method achieves 65.53\% of
the Score, which is improved by 1.99\% over the previous sota method.
  The code is available in https://github.com/deegy666/ADD-RSC

</details>


### [788] [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/pdf/2506.02742)
*Xiaoxue Gao, Huayun Zhang, Nancy F. Chen*

Main category: eess.AS

TL;DR: The paper introduces a prompt-unseen-emotion (PUE) approach for generating diverse emotional speech beyond predefined categories, using emotion-guided prompt learning and an LLM-TTS architecture.


<details>
  <summary>Details</summary>
Motivation: Current TTS systems are limited to predefined emotions, while human conversations require more diverse emotional expressions for natural interactions.

Method: Proposes PUE, leveraging LLM-TTS architecture and emotion-guided prompt learning to capture and adjust emotion weightings per utterance.

Result: PUE enables zero-shot synthesis of unseen emotional speech by flexibly adjusting emotion proportions and leveraging LLM knowledge.

Conclusion: PUE successfully bridges the gap in expressive speech synthesis, allowing for more natural and diverse emotional interactions.

Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited
set of categorical emotions, whereas human conversations extend far beyond
these predefined emotions, making it essential to explore more diverse
emotional speech generation for more natural interactions. To bridge this gap,
this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate
unseen emotional speech via emotion-guided prompt learning. PUE is trained
utilizing an LLM-TTS architecture to ensure emotional consistency between
categorical emotion-relevant prompts and emotional speech, allowing the model
to quantitatively capture different emotion weightings per utterance. During
inference, mixed emotional speech can be generated by flexibly adjusting
emotion proportions and leveraging LLM contextual knowledge, enabling the model
to quantify different emotional styles. Our proposed PUE successfully
facilitates expressive speech synthesis of unseen emotions in a zero-shot
setting.

</details>


### [789] [AuralNet: Hierarchical Attention-based 3D Binaural Localization of Overlapping Speakers](https://arxiv.org/pdf/2506.02773)
*Linya Fu, Yu Liu, Zhijie Liu, Zedong Yang, Zhong-Qiu Wang, Youfu Li, He Kong*

Main category: eess.AS

TL;DR: AuralNet is a 3D multi-source binaural sound localization method that localizes overlapping sources in azimuth and elevation without knowing the number of sources, using a gated coarse-to-fine architecture and multi-head self-attention.


<details>
  <summary>Details</summary>
Motivation: The need for robust sound source localization in noisy-reverberant environments without prior knowledge of the number of sources.

Method: A gated coarse-to-fine architecture with sector partitioning, multi-head self-attention, and a masked multi-task loss function for joint optimization.

Result: AuralNet outperforms recent methods in noisy-reverberant conditions.

Conclusion: AuralNet is effective for 3D sound source localization in challenging environments.

Abstract: We propose AuralNet, a novel 3D multi-source binaural sound source
localization approach that localizes overlapping sources in both azimuth and
elevation without prior knowledge of the number of sources. AuralNet employs a
gated coarse-tofine architecture, combining a coarse classification stage with
a fine-grained regression stage, allowing for flexible spatial resolution
through sector partitioning. The model incorporates a multi-head self-attention
mechanism to capture spatial cues in binaural signals, enhancing robustness in
noisy-reverberant environments. A masked multi-task loss function is designed
to jointly optimize sound detection, azimuth, and elevation estimation.
Extensive experiments in noisy-reverberant conditions demonstrate the
superiority of AuralNet over recent methods

</details>


### [790] [On the influence of language similarity in non-target speaker verification trials](https://arxiv.org/pdf/2506.02777)
*Paul M. Reuter, Michael Jessen*

Main category: eess.AS

TL;DR: The paper examines how language similarity affects cross-lingual speaker verification, finding minimal impact for trained languages but significant correlation for untrained languages.


<details>
  <summary>Details</summary>
Motivation: To understand the role of language similarity in cross-lingual speaker verification trials.

Method: Uses ECAPA-TDNN trained on VoxCeleb variants, analyzing score distributions on Globalphone and LDC CTS datasets.

Result: Trained languages show minimal score impact, while untrained languages exhibit scores correlating with language similarity.

Conclusion: Language similarity significantly affects cross-lingual speaker verification, especially for untrained languages.

Abstract: In this paper, we investigate the influence of language similarity in
cross-lingual non-target speaker verification trials using a state-of-the-art
speaker verification system, ECAPA-TDNN, trained on multilingual and
monolingual variants of the VoxCeleb dataset. Our analysis of the score
distribution patterns on multilingual Globalphone and LDC CTS reveals a
clustering effect in speaker comparisons involving a training language, whereby
the choice of comparison language only minimally impacts scores. Conversely, we
observe a language similarity effect in trials involving languages not included
in the training set of the speaker verification system, with scores correlating
with language similarity measured by a language classification system,
especially when using multilingual training data.

</details>


### [791] [Fast-Converging Distributed Signal Estimation in Topology-Unconstrained Wireless Acoustic Sensor Networks](https://arxiv.org/pdf/2506.02797)
*Paul Didier, Toon van Waterschoot, Simon Doclo, Jörg Bitzer, Marc Moonen*

Main category: eess.AS

TL;DR: The paper introduces TI-DANSE+, an improved version of TI-DANSE for distributed signal estimation in WASNs, offering faster convergence and better communication efficiency.


<details>
  <summary>Details</summary>
Motivation: The slow convergence of TI-DANSE in real-world scenarios due to limited access to fused signals motivates the development of TI-DANSE+.

Method: TI-DANSE+ uses partial in-network sums of fused signals from neighbors and combines it with a tree-pruning strategy to maximize degrees of freedom and neighbor count.

Result: TI-DANSE+ achieves faster convergence, preserves performance under link failures, and saves communication bandwidth compared to TI-DANSE and DANSE.

Conclusion: TI-DANSE+ is a versatile alternative to DANSE and TI-DANSE, combining their strengths and offering additional benefits in communication efficiency.

Abstract: This paper focuses on distributed signal estimation in topology-unconstrained
wireless acoustic sensor networks (WASNs) where sensor nodes only transmit
fused versions of their local sensor signals. For this task, the
topology-independent (TI) distributed adaptive node-specific signal estimation
(DANSE) algorithm (TI-DANSE) has previously been proposed. It converges towards
the centralized signal estimation solution in non-fully connected and
time-varying network topologies. However, the applicability of TI-DANSE in
real-world scenarios is limited due to its slow convergence. The latter results
from the fact that, in TI-DANSE, nodes only have access to the in-network sum
of all fused signals in the WASN. We address this low convergence speed by
introducing an improved TI-DANSE algorithm, referred to as TI-DANSE+, in which
updating nodes separately use the partial in-network sums of fused signals
coming from each of their neighbors. Nodes can maximize the number of available
degrees of freedom in their local optimization problem, leading to faster
convergence. This is further exploited by combining TI-DANSE+ with a
tree-pruning strategy that maximizes the number of neighbors at the updating
node. In fully connected WASNs, TI-DANSE+ converges as fast as the original
DANSE algorithm (the latter only defined for fully connected WASNs) while using
peer-to-peer data transmission instead of broadcasting and thus saving
communication bandwidth. If link failures occur, the convergence of TI-DANSE+
towards the centralized solution is preserved without any change in its
formulation. Altogether, the proposed TI-DANSE+ algorithm can be viewed as an
all-round alternative to DANSE and TI-DANSE which (i) merges the advantages of
both, (ii) reconciliates their differences into a single formulation, and (iii)
shows advantages of its own in terms of communication bandwidth usage.

</details>


### [792] [CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech](https://arxiv.org/pdf/2506.02863)
*Helin Wang, Jiarui Hai, Dading Chong, Karan Thakkar, Tiantian Feng, Dongchao Yang, Junhyeok Lee, Laureano Moro Velazquez, Jesus Villalba, Zengyi Qin, Shrikanth Narayanan, Mounya Elhiali, Najim Dehak*

Main category: eess.AS

TL;DR: The paper introduces CapSpeech, a benchmark for CapTTS-related tasks, addressing gaps in datasets and downstream research. It includes large-scale annotated data and demonstrates high-quality speech synthesis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized datasets and limited research on downstream tasks in CapTTS, hindering real-world applications.

Method: Introduces CapSpeech with over 10M machine-annotated and 0.36M human-annotated audio-caption pairs, and two new datasets. Tests autoregressive and non-autoregressive models.

Result: Achieves high-fidelity and intelligible speech synthesis across diverse styles. CapSpeech is the largest annotated dataset for CapTTS.

Conclusion: CapSpeech provides valuable insights and resources for advancing CapTTS systems, addressing key challenges in the field.

Abstract: Recent advancements in generative artificial intelligence have significantly
transformed the field of style-captioned text-to-speech synthesis (CapTTS).
However, adapting CapTTS to real-world applications remains challenging due to
the lack of standardized, comprehensive datasets and limited research on
downstream tasks built upon CapTTS. To address these gaps, we introduce
CapSpeech, a new benchmark designed for a series of CapTTS-related tasks,
including style-captioned text-to-speech synthesis with sound events
(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS
(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech
comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36
million human-annotated audio-caption pairs. In addition, we introduce two new
datasets collected and recorded by a professional voice actor and experienced
audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside
the datasets, we conduct comprehensive experiments using both autoregressive
and non-autoregressive models on CapSpeech. Our results demonstrate
high-fidelity and highly intelligible speech synthesis across a diverse range
of speaking styles. To the best of our knowledge, CapSpeech is the largest
available dataset offering comprehensive annotations for CapTTS-related tasks.
The experiments and findings further provide valuable insights into the
challenges of developing CapTTS systems.

</details>


### [793] [Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency](https://arxiv.org/pdf/2506.02908)
*Bunlong Lay, Rostilav Makarov, Timo Gerkmann*

Main category: eess.AS

TL;DR: A sliding window diffusion framework is adapted for real-time speech enhancement, balancing performance and latency, outperforming standard diffusion models with low GPU latency.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models for speech enhancement are computationally expensive and impractical for real-time streaming data.

Method: A sliding window approach progressively corrupts speech signals, assigning more noise to recent frames, enabling denoising with controllable latency.

Result: The method outperforms standard diffusion models, achieving input-output latency of 0.3 to 1 seconds on a GPU.

Conclusion: This is the first practical diffusion-based solution for online speech enhancement.

Abstract: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.

</details>


### [794] [PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing](https://arxiv.org/pdf/2506.02958)
*You Zhang, Baotong Tian, Lin Zhang, Zhiyao Duan*

Main category: eess.AS

TL;DR: The paper introduces PartialEdit, a dataset for detecting partially edited deepfake speech, highlighting the limitations of existing models and the role of neural audio codecs in detection.


<details>
  <summary>Details</summary>
Motivation: To address the risks of deepfakes in neural speech editing by enabling research on detecting partially edited speech.

Method: Curated the PartialEdit dataset using advanced neural editing techniques and evaluated detection and localization tasks.

Result: Existing models (e.g., trained on PartialSpoof) fail to detect partially edited speech; insights into neural audio codec artifacts were provided.

Conclusion: PartialEdit advances research in detecting edited deepfake speech, emphasizing the need for improved detection methods.

Abstract: Neural speech editing enables seamless partial edits to speech utterances,
allowing modifications to selected content while preserving the rest of the
audio unchanged. This useful technique, however, also poses new risks of
deepfakes. To encourage research on detecting such partially edited deepfake
speech, we introduce PartialEdit, a deepfake speech dataset curated using
advanced neural editing techniques. We explore both detection and localization
tasks on PartialEdit. Our experiments reveal that models trained on the
existing PartialSpoof dataset fail to detect partially edited speech generated
by neural speech editing models. As recent speech editing models almost all
involve neural audio codecs, we also provide insights into the artifacts the
model learned on detecting these deepfakes. Further information about the
PartialEdit dataset and audio samples can be found on the project page:
https://yzyouzhang.com/PartialEdit/index.html.

</details>


### [795] [InfiniteAudio: Infinite-Length Audio Generation with Consistency](https://arxiv.org/pdf/2506.03020)
*Chaeyoung Jung, Hojoon Ki, Ji-Hoon Kim, Junmo Kim, Joon Son Chung*

Main category: eess.AS

TL;DR: InfiniteAudio enables infinite-length audio generation using diffusion methods by addressing memory constraints and temporal inconsistencies with FIFO sampling and curved denoising.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio methods struggle with long-duration generation due to memory constraints and temporal inconsistencies from concatenating short segments.

Method: InfiniteAudio introduces FIFO sampling (fixed-size input inference) and curved denoising (selective diffusion step prioritization) without requiring additional training.

Result: Experiments show InfiniteAudio matches or outperforms existing methods across all metrics.

Conclusion: InfiniteAudio effectively solves long-duration audio generation challenges while maintaining performance.

Abstract: This paper presents InfiniteAudio, a simple yet effective strategy for
generating infinite-length audio using diffusion-based text-to-audio methods.
Current approaches face memory constraints because the output size increases
with input length, making long duration generation challenging. A common
workaround is to concatenate short audio segments, but this often leads to
inconsistencies due to the lack of shared temporal context. To address this,
InfiniteAudio integrates seamlessly into existing pipelines without additional
training. It introduces two key techniques: FIFO sampling, a first-in,
first-out inference strategy with fixed-size inputs, and curved denoising,
which selectively prioritizes key diffusion steps for efficiency. Experiments
show that InfiniteAudio achieves comparable or superior performance across all
metrics. Audio samples are available on our project page.

</details>


### [796] [Singing Voice Graph Modeling for SingFake Detection](https://arxiv.org/pdf/2406.03111)
*Xuanjun Chen, Haibin Wu, Jyh-Shing Roger Jang, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces SingGraph, a novel model combining MERT and wav2vec2.0 for detecting singing voice deepfakes (SingFake), achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing speech deepfake detection models fail in the singing voice domain due to unique vocalization challenges.

Method: SingGraph integrates MERT for pitch/rhythm analysis and wav2vec2.0 for lyrics analysis, with RawBoost and beat matching for augmentation.

Result: SingGraph improves EER by 13.2% (seen singers), 24.3% (unseen singers), and 37.1% (unseen singers with different codecs).

Conclusion: SingGraph sets a new benchmark for SingFake detection, outperforming previous models across diverse scenarios.

Abstract: Detecting singing voice deepfakes, or SingFake, involves determining the
authenticity and copyright of a singing voice. Existing models for speech
deepfake detection have struggled to adapt to unseen attacks in this unique
singing voice domain of human vocalization. To bridge the gap, we present a
groundbreaking SingGraph model. The model synergizes the capabilities of the
MERT acoustic music understanding model for pitch and rhythm analysis with the
wav2vec2.0 model for linguistic analysis of lyrics. Additionally, we advocate
for using RawBoost and beat matching techniques grounded in music domain
knowledge for singing voice augmentation, thereby enhancing SingFake detection
performance. Our proposed method achieves new state-of-the-art (SOTA) results
within the SingFake dataset, surpassing the previous SOTA model across three
distinct scenarios: it improves EER relatively for seen singers by 13.2%, for
unseen singers by 24.3%, and unseen singers using different codecs by 37.1%.

</details>


### [797] [FlashAudio: Rectified Flows for Fast and High-Fidelity Text-to-Audio Generation](https://arxiv.org/pdf/2410.12266)
*Huadai Liu, Jialei Wang, Rongjie Huang, Yang Liu, Heng Lu, Zhou Zhao, Wei Xue*

Main category: eess.AS

TL;DR: FlashAudio introduces rectified flows for fast text-to-audio generation, optimizing time distribution and noise allocation, achieving 400x faster sampling than real-time.


<details>
  <summary>Details</summary>
Motivation: Address computational inefficiency in latent diffusion models for text-to-audio generation.

Method: Uses rectified flows, Bifocal Samplers, immiscible flow, and Anchored Optimization to improve efficiency and accuracy.

Result: One-step generation outperforms traditional diffusion models, with 400x faster sampling.

Conclusion: FlashAudio offers a practical, high-speed solution for text-to-audio generation.

Abstract: Recent advancements in latent diffusion models (LDMs) have markedly enhanced
text-to-audio generation, yet their iterative sampling processes impose
substantial computational demands, limiting practical deployment. While recent
methods utilizing consistency-based distillation aim to achieve few-step or
single-step inference, their one-step performance is constrained by curved
trajectories, preventing them from surpassing traditional diffusion models. In
this work, we introduce FlashAudio with rectified flows to learn straight flow
for fast simulation. To alleviate the inefficient timesteps allocation and
suboptimal distribution of noise, FlashAudio optimizes the time distribution of
rectified flow with Bifocal Samplers and proposes immiscible flow to minimize
the total distance of data-noise pairs in a batch vias assignment. Furthermore,
to address the amplified accumulation error caused by the classifier-free
guidance (CFG), we propose Anchored Optimization, which refines the guidance
scale by anchoring it to a reference trajectory. Experimental results on
text-to-audio generation demonstrate that FlashAudio's one-step generation
performance surpasses the diffusion-based models with hundreds of sampling
steps on audio quality and enables a sampling speed of 400x faster than
real-time on a single NVIDIA 4090Ti GPU. Code will be available at
https://github.com/liuhuadai/FlashAudio.

</details>


### [798] [Score-informed Music Source Separation: Improving Synthetic-to-real Generalization in Classical Music](https://arxiv.org/pdf/2503.07352)
*Eetu Tunturi, David Diaz-Guerra, Archontis Politis, Tuomas Virtanen*

Main category: eess.AS

TL;DR: The paper explores using musical scores to improve music source separation, proposing two models: one combining scores with audio spectrograms and another using only scores. The score-only model shows better generalization from synthetic to real data.


<details>
  <summary>Details</summary>
Motivation: To enhance music source separation by leveraging additional information like musical scores, which are typically unused in traditional audio-only models.

Method: Two models are proposed: (1) a score-informed model combining scores with audio spectrograms, and (2) a score-only model using scores to calculate separation masks. Training uses synthetic data (SynthSOD), with evaluation on real recordings (URMP, Aalto datasets).

Result: The score-informed model improves separation but struggles with synthetic-to-real generalization. The score-only model shows clear improvement in generalization.

Conclusion: Musical scores can aid music source separation, with the score-only model offering better generalization to real-world data.

Abstract: Music source separation is the task of separating a mixture of instruments
into constituent tracks. Music source separation models are typically trained
using only audio data, although additional information can be used to improve
the model's separation capability. In this paper, we propose two ways of using
musical scores to aid music source separation: a score-informed model where the
score is concatenated with the magnitude spectrogram of the audio mixture as
the input of the model, and a model where we use only the score to calculate
the separation mask. We train our models on synthetic data in the SynthSOD
dataset and evaluate our methods on the URMP and Aalto anechoic orchestra
datasets, comprised of real recordings. The score-informed model improves
separation results compared to a baseline approach, but struggles to generalize
from synthetic to real data, whereas the score-only model shows a clear
improvement in synthetic-to-real generalization.

</details>


### [799] [OmniAudio: Generating Spatial Audio from 360-Degree Video](https://arxiv.org/pdf/2504.14906)
*Huadai Liu, Tianyi Luo, Kaicheng Luo, Qikai Jiang, Peiwen Sun, Jialei Wang, Rongjie Huang, Qian Chen, Wen Wang, Xiangtai Li, Shiliang Zhang, Zhijie Yan, Zhou Zhao, Wei Xue*

Main category: eess.AS

TL;DR: The paper introduces 360V2SA, a task for generating spatial audio from 360-degree videos, and proposes OmniAudio, a framework leveraging self-supervised pre-training and dual-branch inputs for superior performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods lack spatial audio cues for 3D environments, limiting realistic sound representation.

Method: OmniAudio uses self-supervised pre-training with FOA audio and dual-branch inputs (panoramic and perspective videos) for comprehensive data capture.

Result: OmniAudio achieves state-of-the-art performance on the Sphere360 dataset, validated by objective and subjective metrics.

Conclusion: The work advances spatial audio generation from 360-degree videos, offering a novel dataset and framework with open-source availability.

Abstract: Traditional video-to-audio generation techniques primarily focus on
perspective video and non-spatial audio, often missing the spatial cues
necessary for accurately representing sound sources in 3D environments. To
address this limitation, we introduce a novel task, 360V2SA, to generate
spatial audio from 360-degree videos, specifically producing First-order
Ambisonics (FOA) audio - a standard format for representing 3D spatial audio
that captures sound directionality and enables realistic 3D audio reproduction.
We first create Sphere360, a novel dataset tailored for this task that is
curated from real-world data. We also design an efficient semi-automated
pipeline for collecting and cleaning paired video-audio data. To generate
spatial audio from 360-degree video, we propose a novel framework OmniAudio,
which leverages self-supervised pre-training using both spatial audio data (in
FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a
dual-branch framework that utilizes both panoramic and perspective video inputs
to capture comprehensive local and global information from 360-degree videos.
Experimental results demonstrate that OmniAudio achieves state-of-the-art
performance across both objective and subjective metrics on Sphere360. Code and
datasets are available at https://github.com/liuhuadai/OmniAudio. The project
website is available at https://OmniAudio-360V2SA.github.io.

</details>


### [800] [Effect of laboratory conditions on the perception of virtual stages for music](https://arxiv.org/pdf/2505.20552)
*Ernesto Accolti*

Main category: eess.AS

TL;DR: The paper validates a methodology for assessing acoustical conditions in custom hearing booths, showing that sound absorption impacts virtual stage perception in music.


<details>
  <summary>Details</summary>
Motivation: To ensure perceptual validity and experimental rigor in augmented acoustics experiments by validating the methodology for assessing acoustical conditions.

Method: A preliminary study comparing the perception of virtual stages in three rooms: an anechoic room, a hearing booth with insufficient sound absorption, and one with achievable sound absorption.

Result: The anechoic room and the booth with achievable absorption met perceptual validity, while the booth with insufficient absorption did not.

Conclusion: The study preliminarily validates the methodology, laying groundwork for future research and guidelines in virtual acoustics lab design.

Abstract: This manuscript presents initial findings critical for supporting augmented
acoustics experiments in custom-made hearing booths, addressing a key challenge
in ensuring perceptual validity and experimental rigor in these highly
sensitive setups. This validation ensures our proposed methodology is sound,
guarantees the reliability of future results, and lays the foundational
groundwork for subsequent perceptual studies and the development of robust
guidelines for laboratory design in virtual acoustics research. A preliminary
study on the effect of the acoustical conditions of three different rooms on
the perception of virtual stages for music is presented: an anechoic room, a
custom-made hearing booth with insufficient sound absorption, and another
custom-made hearing booth with achievable sound absorption. The goal of this
study is to assess the impact of these different conditions on the perception
of virtual stages for music. The results show that the anechoic room and the
hearing booth with achievable sound absorption have a difference between the
total sound and the virtual sound below the just-noticeable difference, which
means that the virtual sound is not perceived louder than it should. In
contrast, the hearing booth with insufficient sound absorption has a difference
above the just-noticeable difference, which means that the virtual sound is
perceived louder than it should. This study provides a preliminary validation
of the proposed methodology for assessing the acoustical conditions of
custom-made hearing booths in stage acoustics experiments. Future work will
include a more comprehensive analysis of the results, including the effect of
different sound sources.
  Supplementary audio files illustrating key simulation results are available
at https://zenodo.org/records/15579861

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [801] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/pdf/2506.01980)
*Lianhao Yin, Ozanan Meireles, Guy Rosman, Daniela Rus*

Main category: eess.IV

TL;DR: C2E is a self-supervised framework using Kolmogorov complexity to learn compact representations from surgical videos, improving performance without labeled data.


<details>
  <summary>Details</summary>
Motivation: Supervised learning in MIS requires large annotated datasets, which are scarce due to high annotation efforts. Current self-supervised methods often fail to generalize across tasks.

Method: C2E leverages entropy-maximizing decoders to compress images while preserving clinical details, learning informative representations from unlabeled surgical videos.

Result: C2E generalizes well across surgical ML tasks like workflow classification, segmentation, and diagnosis, outperforming other methods.

Conclusion: C2E demonstrates the potential of self-supervised learning to enhance surgical AI, improving outcomes in minimally invasive surgery.

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


### [802] [Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model](https://arxiv.org/pdf/2506.02060)
*Javier Salazar Cavazos, Scott Peltier*

Main category: eess.IV

TL;DR: A novel 4D CNN model improves feature extraction for fMRI data by capturing temporal-spatial dynamics, outperforming 3D models in Alzheimer's disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing 3D models for 4D fMRI data may underperform in feature extraction, limiting downstream tasks like classification.

Method: Developed a 4D convolution network to extract joint temporal-spatial kernels, capturing both spatial and temporal dynamics.

Result: The 4D CNN outperforms 3D models in capturing spatial-temporal data, enhancing Alzheimer's disease diagnosis.

Conclusion: Future work could extend the model to task-based fMRI and regression tasks for broader applications.

Abstract: Previous works in the literature apply 3D spatial-only models on 4D
functional MRI data leading to possible sub-par feature extraction to be used
for downstream tasks like classification. In this work, we aim to develop a
novel 4D convolution network to extract 4D joint temporal-spatial kernels that
not only learn spatial information but in addition also capture temporal
dynamics. Experimental results show promising performance in capturing
spatial-temporal data in functional MRI compared to 3D models. The 4D CNN model
improves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier
detection and better interventions. Future research could explore task-based
fMRI applications and regression tasks, enhancing understanding of cognitive
performance and disease progression.

</details>


### [803] [Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?](https://arxiv.org/pdf/2506.02093)
*Tianyu Lin, Xinran Li, Chuntung Zhuang, Qi Chen, Yuanhao Cai, Kai Ding, Alan L. Yuille, Zongwei Zhou*

Main category: eess.IV

TL;DR: The paper introduces CARE, a framework for improving structural completeness in sparse-view CT reconstructions using novel anatomy-aware metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics like SSIM and PSNR fail to capture the completeness of critical anatomical structures, especially small or thin regions.

Method: Proposes anatomy-aware evaluation metrics and CARE, a model-agnostic framework with structural penalties to enhance anatomical preservation.

Result: CARE improves structural completeness by up to +32% for large organs, +22% for small organs, +40% for intestines, and +36% for vessels.

Conclusion: CARE effectively addresses the limitations of traditional metrics and enhances the preservation of anatomical structures in CT reconstructions.

Abstract: Widely adopted evaluation metrics for sparse-view CT reconstruction--such as
Structural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize
pixel-wise fidelity but often fail to capture the completeness of critical
anatomical structures, particularly small or thin regions that are easily
missed. To address this limitation, we propose a suite of novel anatomy-aware
evaluation metrics designed to assess structural completeness across anatomical
structures, including large organs, small organs, intestines, and vessels.
Building on these metrics, we introduce CARE, a Completeness-Aware
Reconstruction Enhancement framework that incorporates structural penalties
during training to encourage anatomical preservation of significant structures.
CARE is model-agnostic and can be seamlessly integrated into analytical,
implicit, and generative methods. When applied to these methods, CARE
substantially improves structural completeness in CT reconstructions, achieving
up to +32% improvement for large organs, +22% for small organs, +40% for
intestines, and +36% for vessels.

</details>


### [804] [Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine](https://arxiv.org/pdf/2506.02149)
*Wenjun Xia, Chuang Niu, Ge Wang*

Main category: eess.IV

TL;DR: A novel CT framework, FORCE, integrates data fidelity with PFGM++ to improve reconstruction, outperforming unsupervised methods despite challenges in paired training data.


<details>
  <summary>Details</summary>
Motivation: Clinical CT scenarios often suffer from noise and artifacts, requiring better reconstruction techniques. Deep learning helps but faces challenges like data inconsistency and hallucination risks.

Method: Proposes FORCE, combining data fidelity with PFGM++, a generative AI model, for CT reconstruction.

Result: FORCE shows superior performance in various CT tasks compared to existing unsupervised methods.

Conclusion: FORCE effectively addresses CT reconstruction challenges, offering improved performance without relying on perfectly paired data.

Abstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT
scenarios, such as low-dose screening, sparse-view scanning, and metal
implants, often lead to severe noise and artifacts in reconstructed images,
requiring improved reconstruction techniques. The introduction of deep learning
has significantly advanced CT image reconstruction. However, obtaining paired
training data remains rather challenging due to patient motion and other
constraints. Although deep learning methods can still perform well with
approximately paired data, they inherently carry the risk of hallucination due
to data inconsistencies and model instability. In this paper, we integrate the
data fidelity with the state-of-the-art generative AI model, referred to as the
Poisson flow generative model (PFGM) with a generalized version PFGM++, and
propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine
(FORCE). In our experiments, the proposed method shows superior performance in
various CT imaging tasks, outperforming existing unsupervised reconstruction
approaches.

</details>


### [805] [Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding](https://arxiv.org/pdf/2506.02574)
*Shuai Yuan, Shuang Chen, Tianwu Lin, Jie Wang, Peng Gong*

Main category: eess.IV

TL;DR: TasGen automates dynamic sample generation for remote sensing mapping, eliminating manual updates by leveraging temporal-spectral embedding.


<details>
  <summary>Details</summary>
Motivation: Rapid land surface changes make static samples obsolete quickly, requiring labor-intensive manual updates.

Method: TasGen uses a two-stage framework with temporal-spectral embedding to model spectral and temporal dependencies in time-series imagery.

Result: The method generates dynamic samples without additional manual annotations.

Conclusion: TasGen offers an efficient solution for maintaining up-to-date remote sensing maps.

Abstract: Accurate remote sensing geographic mapping depends heavily on representative
and timely sample data. However, rapid changes in land surface dynamics
necessitate frequent updates, quickly rendering previously collected samples
obsolete and imposing significant labor demands for continuous manual updates.
In this study, we aim to address this problem by dynamic sample generation
using existing single-date static labeled samples. We introduce TasGen, a
two-stage automated framework to automatically generate dynamic samples,
designed to simultaneously model spectral and temporal dependencies in
time-series remote sensing imagery via temporal-spectral embedding, capturing
land surface changes without additional manual annotations.

</details>


### [806] [NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution](https://arxiv.org/pdf/2506.02197)
*Marcos V. Conde, Radu Timofte, Zihao Lu, Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand*

Main category: eess.IV

TL;DR: The paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution Challenge, showcasing solutions and results for restoring and upscaling RAW images.


<details>
  <summary>Details</summary>
Motivation: To advance RAW image restoration and super-resolution, areas less explored compared to RGB domain, and to benchmark state-of-the-art methods.

Method: Participants developed solutions for (i) restoring RAW images with blur/noise and (ii) 2x upscaling RAW Bayer images under unknown noise/blur.

Result: 230 participants registered, 45 submitted results, revealing current state-of-the-art in RAW restoration.

Conclusion: The challenge highlights progress and sets benchmarks for RAW image restoration and super-resolution.

Abstract: This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution
Challenge, highlighting the proposed solutions and results. New methods for RAW
Restoration and Super-Resolution could be essential in modern Image Signal
Processing (ISP) pipelines, however, this problem is not as explored as in the
RGB domain. The goal of this challenge is two fold, (i) restore RAW images with
blur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering
unknown noise and blur. In the challenge, a total of 230 participants
registered, and 45 submitted results during thee challenge period. This report
presents the current state-of-the-art in RAW Restoration.

</details>


### [807] [Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation](https://arxiv.org/pdf/2506.02312)
*Md Tauhidul Islam, Wu Da-Wen, Tang Qing-Qing, Zhao Kai-Yang, Yin Teng, Li Yan-Fei, Shang Wen-Yi, Liu Jing-Yu, Zhang Hai-Xian*

Main category: eess.IV

TL;DR: DEFFA-Unet improves retinal blood vessel segmentation by addressing data limitations and feature extraction issues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like U-Net face challenges with limited training data, imbalance, and poor feature extraction, affecting segmentation and generalization.

Method: DEFFA-Unet introduces an extra encoder for domain-invariant inputs, feature filtering fusion, attention-guided fusion, and novel data augmentation.

Result: Outperforms baseline and state-of-the-art models on benchmark datasets (DRIVE, CHASEDB1, STARE, HRF, IOSTAR), especially in cross-validation.

Conclusion: DEFFA-Unet enhances segmentation performance and generalization, making it superior for clinical applications.

Abstract: Retinal blood vessel segmentation is crucial for diagnosing ocular and
cardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf
Ronneberger significantly advanced this field, yet issues like limited training
data, imbalance data distribution, and inadequate feature extraction persist,
hindering both the segmentation performance and optimal model generalization.
Addressing these critical issues, the DEFFA-Unet is proposed featuring an
additional encoder to process domain-invariant pre-processed inputs, thereby
improving both richer feature encoding and enhanced model generalization. A
feature filtering fusion module is developed to ensure the precise feature
filtering and robust hybrid feature fusion. In response to the task-specific
need for higher precision where false positives are very costly, traditional
skip connections are replaced with the attention-guided feature reconstructing
fusion module. Additionally, innovative data augmentation and balancing methods
are proposed to counter data scarcity and distribution imbalance, further
boosting the robustness and generalization of the model. With a comprehensive
suite of evaluation metrics, extensive validations on four benchmark datasets
(DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the
proposed method's superiority over both baseline and state-of-the-art models.
Particularly the proposed method significantly outperforms the compared methods
in cross-validation model generalization.

</details>


### [808] [Unrolling Nonconvex Graph Total Variation for Image Denoising](https://arxiv.org/pdf/2506.02381)
*Songlin Wei, Gene Cheung, Fei Chen, Ivan Selesnick*

Main category: eess.IV

TL;DR: Proposes a non-convex total variation term in a graph setting (NC-GTV) for image denoising, ensuring convexity via a graph Huber function and efficient parameter selection. Outperforms existing methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address limitations of convex regularization terms like TV by introducing a non-convex variant (NC-GTV) that avoids local minima while maintaining convexity.

Method: Defines NC-GTV using a graph Huber function, selects parameter $a$ via Gershgorin Circle Theorem, and designs an ADMM-based linear-time algorithm, unrolled into a lightweight network.

Result: Outperforms unrolled GTV and other denoising methods, achieving better results with fewer network parameters.

Conclusion: NC-GTV offers an effective, parameter-efficient solution for image denoising, combining convexity and performance.

Abstract: Conventional model-based image denoising optimizations employ convex
regularization terms, such as total variation (TV) that convexifies the
$\ell_0$-norm to promote sparse signal representation. Instead, we propose a
new non-convex total variation term in a graph setting (NC-GTV), such that when
combined with an $\ell_2$-norm fidelity term for denoising, leads to a convex
objective with no extraneous local minima. We define NC-GTV using a new graph
variant of the Huber function, interpretable as a Moreau envelope. The crux is
the selection of a parameter $a$ characterizing the graph Huber function that
ensures overall objective convexity; we efficiently compute $a$ via an
adaptation of Gershgorin Circle Theorem (GCT). To minimize the convex
objective, we design a linear-time algorithm based on Alternating Direction
Method of Multipliers (ADMM) and unroll it into a lightweight feed-forward
network for data-driven parameter learning. Experiments show that our method
outperforms unrolled GTV and other representative image denoising schemes,
while employing far fewer network parameters.

</details>


### [809] [Multi-modal brain MRI synthesis based on SwinUNETR](https://arxiv.org/pdf/2506.02467)
*Haowen Pang, Weiyan Guo, Chuyang Ye*

Main category: eess.IV

TL;DR: SwinUNETR synthesizes missing brain MRI modalities by combining Swin Transformer and CNNs, improving image quality and diagnostic value.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing MRI modalities in clinical diagnostics by leveraging multi-modal MRI data.

Method: Uses SwinUNETR, a hybrid of Swin Transformer and CNNs, for hierarchical feature extraction and global-local context awareness.

Result: Superior performance in generating realistic synthetic images with improved quality and anatomical consistency.

Conclusion: SwinUNETR effectively synthesizes missing MRI modalities, enhancing clinical diagnostic capabilities.

Abstract: Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in
clinical diagnostics by providing complementary information across different
imaging modalities. However, a common challenge in clinical practice is missing
MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing
modalities in brain MRI. SwinUNETR is a novel neural network architecture
designed for medical image analysis, integrating the strengths of Swin
Transformer and convolutional neural networks (CNNs). The Swin Transformer, a
variant of the Vision Transformer (ViT), incorporates hierarchical feature
extraction and window-based self-attention mechanisms, enabling it to capture
both local and global contextual information effectively. By combining the Swin
Transformer with CNNs, SwinUNETR merges global context awareness with detailed
spatial resolution. This hybrid approach addresses the challenges posed by the
varying modality characteristics and complex brain structures, facilitating the
generation of accurate and realistic synthetic images. We evaluate the
performance of SwinUNETR on brain MRI datasets and demonstrate its superior
capability in generating clinically valuable images. Our results show
significant improvements in image quality, anatomical consistency, and
diagnostic value.

</details>


### [810] [A Tree-guided CNN for image super-resolution](https://arxiv.org/pdf/2506.02585)
*Chunwei Tian, Mingjian Song, Xiaopeng Fan, Xiangtao Zheng, Bob Zhang, David Zhang*

Main category: eess.IV

TL;DR: TSRNet uses a tree-guided CNN with cosine transform and Adan optimizer to improve image super-resolution by enhancing key nodes and hierarchical information.


<details>
  <summary>Details</summary>
Motivation: Existing deep CNNs struggle to identify the impact of key layers on super-resolution performance, limiting accuracy.

Method: Proposes TSRNet with a tree architecture to guide key nodes, cosine transform for cross-domain info, and Adan optimizer for training.

Result: TSRNet outperforms in restoring high-quality images, verified by extended experiments.

Conclusion: TSRNet effectively enhances super-resolution by leveraging hierarchical info and cross-domain techniques.

Abstract: Deep convolutional neural networks can extract more accurate structural
information via deep architectures to obtain good performance in image
super-resolution. However, it is not easy to find effect of important layers in
a single network architecture to decrease performance of super-resolution. In
this paper, we design a tree-guided CNN for image super-resolution (TSRNet). It
uses a tree architecture to guide a deep network to enhance effect of key nodes
to amplify the relation of hierarchical information for improving the ability
of recovering images. To prevent insufficiency of the obtained structural
information, cosine transform techniques in the TSRNet are used to extract
cross-domain information to improve the performance of image super-resolution.
Adaptive Nesterov momentum optimizer (Adan) is applied to optimize parameters
to boost effectiveness of training a super-resolution model. Extended
experiments can verify superiority of the proposed TSRNet for restoring
high-quality images. Its code can be obtained at
https://github.com/hellloxiaotian/TSRNet.

</details>


### [811] [DeepSPV: A Deep Learning Pipeline for 3D Spleen Volume Estimation from 2D Ultrasound Images](https://arxiv.org/pdf/2411.11190)
*Zhen Yuan, David Stojanovski, Lei Li, Alberto Gomez, Haran Jogeesvaran, Esther Puyol-Antón, Baba Inusa, Andrew P. King*

Main category: eess.IV

TL;DR: DeepSPV, a deep learning pipeline, estimates spleen volume from 2D ultrasound images, outperforming human experts and offering interpretability for clinical use.


<details>
  <summary>Details</summary>
Motivation: Spleen volume is the gold standard for assessing spleen size, but 3D imaging is not widely available, especially in regions with high sickle cell disease prevalence.

Method: DeepSPV combines a segmentation network and a variational autoencoder to estimate spleen volume from single or dual 2D ultrasound images.

Result: The best model achieves 86.62%/92.5% mean relative volume accuracy under single-view/dual-view settings, surpassing human experts.

Conclusion: DeepSPV is the first deep learning approach for 3D spleen volume estimation from 2D ultrasound, offering seamless integration into clinical workflows.

Abstract: Splenomegaly, the enlargement of the spleen, is an important clinical
indicator for various associated medical conditions, such as sickle cell
disease (SCD). Spleen length measured from 2D ultrasound is the most widely
used metric for characterising spleen size. However, it is still considered a
surrogate measure, and spleen volume remains the gold standard for assessing
spleen size. Accurate spleen volume measurement typically requires 3D imaging
modalities, such as computed tomography or magnetic resonance imaging, but
these are not widely available, especially in the Global South which has a high
prevalence of SCD. In this work, we introduce a deep learning pipeline,
DeepSPV, for precise spleen volume estimation from single or dual 2D ultrasound
images. The pipeline involves a segmentation network and a variational
autoencoder for learning low-dimensional representations from the estimated
segmentations. We investigate three approaches for spleen volume estimation and
our best model achieves 86.62%/92.5% mean relative volume accuracy (MRVA) under
single-view/dual-view settings, surpassing the performance of human experts. In
addition, the pipeline can provide confidence intervals for the volume
estimates as well as offering benefits in terms of interpretability, which
further support clinicians in decision-making when identifying splenomegaly. We
evaluate the full pipeline using a highly realistic synthetic dataset generated
by a diffusion model, achieving an overall MRVA of 83.0% from a single 2D
ultrasound image. Our proposed DeepSPV is the first work to use deep learning
to estimate 3D spleen volume from 2D ultrasound images and can be seamlessly
integrated into the current clinical workflow for spleen assessment.

</details>


### [812] [Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control](https://arxiv.org/pdf/2502.03498)
*Xianghui Ze, Zhenbo Song, Qiwei Wang, Jianfeng Lu, Yujiao Shi*

Main category: eess.IV

TL;DR: Proposes Iterative Homography Adjustment (IHA) and text-guided sampling to improve pose alignment and environmental diversity in satellite-to-street-view image generation.


<details>
  <summary>Details</summary>
Motivation: Addresses pose misalignment and limited environmental diversity in current methods for generating street-view images from satellite imagery.

Method: Introduces Iterative Homography Adjustment (IHA) during denoising and a text-guided sampling strategy for illumination and weather control.

Result: Significantly improves pose accuracy and enhances diversity/realism of generated images.

Conclusion: Sets a new benchmark for satellite-to-street-view generation by combining IHA and environmental control.

Abstract: Generating street-view images from satellite imagery is a challenging task,
particularly in maintaining accurate pose alignment and incorporating diverse
environmental conditions. While diffusion models have shown promise in
generative tasks, their ability to maintain strict pose alignment throughout
the diffusion process is limited. In this paper, we propose a novel Iterative
Homography Adjustment (IHA) scheme applied during the denoising process, which
effectively addresses pose misalignment and ensures spatial consistency in the
generated street-view images. Additionally, currently, available datasets for
satellite-to-street-view generation are limited in their diversity of
illumination and weather conditions, thereby restricting the generalizability
of the generated outputs. To mitigate this, we introduce a text-guided
illumination and weather-controlled sampling strategy that enables fine-grained
control over the environmental factors. Extensive quantitative and qualitative
evaluations demonstrate that our approach significantly improves pose accuracy
and enhances the diversity and realism of generated street-view images, setting
a new benchmark for satellite-to-street-view generation tasks.

</details>


### [813] [MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders](https://arxiv.org/pdf/2502.14753)
*Maya Varma, Ashwin Kumar, Rogier van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay Chaudhari*

Main category: eess.IV

TL;DR: MedVAE, a family of 2D/3D autoencoders, downsizes medical images for computational efficiency while preserving clinical features, achieving up to 70x throughput improvement.


<details>
  <summary>Details</summary>
Motivation: High-resolution medical images incur large computational costs; downsizing them without losing clinical relevance is challenging.

Method: Introduces MedVAE, trained on 1M+ images using a novel two-stage approach, to encode/decode images efficiently.

Result: MedVAE improves downstream model throughput by 70x and maintains high-fidelity image reconstruction.

Conclusion: Large-scale autoencoders like MedVAE can address efficiency challenges in medical imaging without compromising clinical utility.

Abstract: Medical images are acquired at high resolutions with large fields of view in
order to capture fine-grained features necessary for clinical decision-making.
Consequently, training deep learning models on medical images can incur large
computational costs. In this work, we address the challenge of downsizing
medical images in order to improve downstream computational efficiency while
preserving clinically-relevant features. We introduce MedVAE, a family of six
large-scale 2D and 3D autoencoders capable of encoding medical images as
downsized latent representations and decoding latent representations back to
high-resolution images. We train MedVAE autoencoders using a novel two-stage
training approach with 1,052,730 medical images. Across diverse tasks obtained
from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent
representations in place of high-resolution images when training downstream
models can lead to efficiency benefits (up to 70x improvement in throughput)
while simultaneously preserving clinically-relevant features and (2) MedVAE can
decode latent representations back to high-resolution images with high
fidelity. Our work demonstrates that large-scale, generalizable autoencoders
can help address critical efficiency challenges in the medical domain. Our code
is available at https://github.com/StanfordMIMI/MedVAE.

</details>


### [814] [Debiased Opto-electronic Joint Transform Correlator for Enhanced Real-Time Pattern Recognition](https://arxiv.org/pdf/2503.14031)
*Julian Gamboa, Xi Shen, Tabassom Hamidfar, Shamima Mitu, Selim M. Shahriar*

Main category: eess.IV

TL;DR: A debiased opto-electronic joint transform correlator (DOJTC) is proposed to remove self-intensity terms from the joint power spectrum, improving cross-correlation quality and signal-to-noise ratio.


<details>
  <summary>Details</summary>
Motivation: The self-intensity terms in traditional OJTCs dominate the joint power spectrum, consuming bit-depth and degrading cross-correlation results.

Method: The DOJTC electronically pre-processes the joint power spectrum to remove self-intensity terms before optical Fourier transformation.

Result: Simulation and experiments show DOJTC improves signal-to-noise ratio by nearly two orders of magnitude.

Conclusion: DOJTC effectively enhances cross-correlation quality by eliminating bias from self-intensity terms.

Abstract: Opto-electronic joint transform correlators (OJTCs) use a focal plane array
(FPA) to detect the joint power spectrum (JPS) of two input images, projecting
it onto a spatial light modulator (SLM) to be optically Fourier transformed.
The JPS is composed of two self-intensities and two conjugate-products, where
only the latter produce the cross-correlation. However, the self-intensity
terms are typically much stronger than the conjugate-products, producing a bias
that consumes most of the available bit-depth on the FPA and SLM. Here we
propose and demonstrate, through simulation and experiment, a debiased OJTC
(DOJTC) that electronically pre-processes the JPS to remove the self-intensity
terms before sending it to the SLM, thereby enhancing the quality of the
cross-correlation result. We show that under some conditions the DOJTC yields a
nearly two orders of magnitude improvement in the signal-to-noise ratio
compared to an OJTC.

</details>


### [815] [Towards Computation- and Communication-efficient Computational Pathology](https://arxiv.org/pdf/2504.02628)
*Chu Han, Bingchao Zhao, Jiatai Lin, Shanshan Lyu, Longfei Wang, Tianpeng Deng, Cheng Lu, Changhong Liang, Hannah Y. Wen, Xiaojing Guo, Zhenwei Shi, Zaiyi Liu*

Main category: eess.IV

TL;DR: MAG-GLTrans is a novel framework for computational pathology that improves efficiency by using low-magnification inputs, reducing computational time and storage needs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Current pathology models rely on high-magnification images, limiting clinical utility in time-sensitive scenarios. MAG-GLTrans addresses this by enabling efficient low-magnification analysis.

Method: The framework uses a magnification alignment (MAG) mechanism via self-supervised learning to align features between low and high magnification levels.

Result: MAG-GLTrans achieves up to 10.7x faster computation and 20x reduced storage/transfer needs while maintaining state-of-the-art classification performance.

Conclusion: MAG-GLTrans is a versatile and efficient solution for time-sensitive applications like intraoperative diagnosis, compatible with existing models and architectures.

Abstract: Despite the impressive performance across a wide range of applications,
current computational pathology models face significant diagnostic efficiency
challenges due to their reliance on high-magnification whole-slide image
analysis. This limitation severely compromises their clinical utility,
especially in time-sensitive diagnostic scenarios and situations requiring
efficient data transfer. To address these issues, we present a novel
computation- and communication-efficient framework called Magnification-Aligned
Global-Local Transformer (MAG-GLTrans). Our approach significantly reduces
computational time, file transfer requirements, and storage overhead by
enabling effective analysis using low-magnification inputs rather than
high-magnification ones. The key innovation lies in our proposed magnification
alignment (MAG) mechanism, which employs self-supervised learning to bridge the
information gap between low and high magnification levels by effectively
aligning their feature representations. Through extensive evaluation across
various fundamental CPath tasks, MAG-GLTrans demonstrates state-of-the-art
classification performance while achieving remarkable efficiency gains: up to
10.7 times reduction in computational time and over 20 times reduction in file
transfer and storage requirements. Furthermore, we highlight the versatility of
our MAG framework through two significant extensions: (1) its applicability as
a feature extractor to enhance the efficiency of any CPath architecture, and
(2) its compatibility with existing foundation models and
histopathology-specific encoders, enabling them to process low-magnification
inputs with minimal information loss. These advancements position MAG-GLTrans
as a particularly promising solution for time-sensitive applications,
especially in the context of intraoperative frozen section diagnosis where both
accuracy and efficiency are paramount.

</details>


### [816] [A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images](https://arxiv.org/pdf/2505.17602)
*Muhammad Abdullah, Furqan Shaukat*

Main category: eess.IV

TL;DR: A novel attention-based 3D segmentation method for lung parenchyma and nodules outperforms state-of-the-art techniques, validated on the LUNA16 dataset.


<details>
  <summary>Details</summary>
Motivation: Lung cancer's high mortality necessitates early detection via CAD, but existing methods lack generalization and robustness for fine-grained segmentation.

Method: Proposes an attention-based network with residual blocks, strided/transposed convolutions, and dilated convolutions for context capture without added computational cost.

Result: Achieves superior performance on LUNA16 dataset compared to recent methods, using metrics like Dice score and IOU.

Conclusion: The method enhances lung CAD accuracy, with potential for real-time clinical use; code and data are publicly available.

Abstract: Lung cancer has been one of the major threats across the world with the
highest mortalities. Computer-aided detection (CAD) can help in early detection
and thus can help increase the survival rate. Accurate lung parenchyma
segmentation (to include the juxta-pleural nodules) and lung nodule
segmentation, the primary symptom of lung cancer, play a crucial role in the
overall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite
challenging because of the diverse nodule types and other inhibit structures
present within the lung lobes. Traditional machine/deep learning methods suffer
from generalization and robustness. Recent Vision Language Models/Foundation
Models perform well on the anatomical level, but they suffer on fine-grained
segmentation tasks, and their semi-automatic nature limits their effectiveness
in real-time clinical scenarios. In this paper, we propose a novel method for
accurate 3D segmentation of lung parenchyma and lung nodules. The proposed
architecture is an attention-based network with residual blocks at each
encoder-decoder state. Max pooling is replaced by strided convolutions at the
encoder, and trilinear interpolation is replaced by transposed convolutions at
the decoder to maximize the number of learnable parameters. Dilated
convolutions at each encoder-decoder stage allow the model to capture the
larger context without increasing computational costs. The proposed method has
been evaluated extensively on one of the largest publicly available datasets,
namely LUNA16, and is compared with recent notable work in the domain using
standard performance metrics like Dice score, IOU, etc. It can be seen from the
results that the proposed method achieves better performance than
state-of-the-art methods. The source code, datasets, and pre-processed data can
be accessed using the link:
https://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet.

</details>


### [817] [Can Large Language Models Challenge CNNs in Medical Image Analysis?](https://arxiv.org/pdf/2505.23503)
*Shibbir Ahmed, Shahnewaz Karim Sakib, Anindya Bijoy Das*

Main category: eess.IV

TL;DR: A multimodal AI framework compares CNNs and LLMs for medical image classification, showing LLMs with filtering can outperform CNNs in performance, efficiency, and environmental impact.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability, efficiency, and scalability of medical diagnostics by comparing CNN and LLM-based multimodal AI systems.

Method: Comparative analysis of CNNs and LLMs using accuracy, F1-score, execution time, energy consumption, and CO2 emissions on public datasets.

Result: LLMs with additional filtering outperform CNNs in diagnostic performance, efficiency, and environmental impact.

Conclusion: Multimodal AI systems, especially LLMs with filtering, hold transformative potential for improving medical diagnostics.

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [818] [Efficient RAW Image Deblurring with Adaptive Frequency Modulation](https://arxiv.org/pdf/2505.24407)
*Wenlong Jiao, Binglong Li, Wei Shang, Ping Wang, Dongwei Ren*

Main category: eess.IV

TL;DR: FrENet is a frequency-domain framework for RAW-to-RAW deblurring, outperforming state-of-the-art methods with adaptive frequency modulation and skip connections.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches focus on sRGB images, which lose critical information, while RAW images offer superior restoration potential but are underexplored.

Method: Proposes FrENet with an Adaptive Frequency Positional Modulation module and frequency domain skip connections for precise deblurring.

Result: FrENet achieves better restoration quality and efficiency (reduced MACs) than state-of-the-art methods and adapts well to sRGB images.

Conclusion: FrENet is a versatile and efficient solution for RAW image deblurring, with potential extensions to sRGB images.

Abstract: Image deblurring plays a crucial role in enhancing visual clarity across
various applications. Although most deep learning approaches primarily focus on
sRGB images, which inherently lose critical information during the image signal
processing pipeline, RAW images, being unprocessed and linear, possess superior
restoration potential but remain underexplored. Deblurring RAW images presents
unique challenges, particularly in handling frequency-dependent blur while
maintaining computational efficiency. To address these issues, we propose
Frequency Enhanced Network (FrENet), a framework specifically designed for
RAW-to-RAW deblurring that operates directly in the frequency domain. We
introduce a novel Adaptive Frequency Positional Modulation module, which
dynamically adjusts frequency components according to their spectral positions,
thereby enabling precise control over the deblurring process. Additionally,
frequency domain skip connections are adopted to further preserve
high-frequency details. Experimental results demonstrate that FrENet surpasses
state-of-the-art deblurring methods in RAW image deblurring, achieving
significantly better restoration quality while maintaining high efficiency in
terms of reduced MACs. Furthermore, FrENet's adaptability enables it to be
extended to sRGB images, where it delivers comparable or superior performance
compared to methods specifically designed for sRGB data. The code will be
available at https://github.com/WenlongJiao/FrENet .

</details>
