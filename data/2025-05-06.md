<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 98]
- [cs.CV](#cs.CV) [Total: 181]
- [cs.AI](#cs.AI) [Total: 70]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.LG](#cs.LG) [Total: 169]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.MM](#cs.MM) [Total: 3]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 31]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/pdf/2505.01456)
*Vaidehi Patil, Yi-Lin Sung, Peter Hase, Jie Peng, Tianlong Chen, Mohit Bansal*

Main category: cs.CL

TL;DR: The paper introduces a benchmark (UnLOK-VQA) and framework for evaluating multimodal unlearning in LLMs, addressing risks of sensitive data retention and adversarial exploitation.


<details>
  <summary>Details</summary>
Motivation: To mitigate risks of sensitive information retention in multimodal LLMs and evaluate unlearning methods, given the underexplored nature of multimodal unlearning.

Method: Developed UnLOK-VQA benchmark using an automated pipeline for sample generation and manual filtering, then evaluated six defense objectives against seven attacks.

Result: Multimodal attacks outperform unimodal ones; effective defense involves removing answer info from model states. Larger models show greater robustness.

Conclusion: UnLOK-VQA advances multimodal unlearning research, highlighting the importance of scale for model safety and effective defense strategies.

Abstract: LLMs trained on massive datasets may inadvertently acquire sensitive
information such as personal details and potentially harmful content. This risk
is further heightened in multimodal LLMs as they integrate information from
multiple modalities (image and text). Adversaries can exploit this knowledge
through multimodal prompts to extract sensitive details. Evaluating how
effectively MLLMs can forget such information (targeted unlearning)
necessitates the creation of high-quality, well-annotated image-text pairs.
While prior work on unlearning has focused on text, multimodal unlearning
remains underexplored. To address this gap, we first introduce a multimodal
unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as
an attack-and-defense framework to evaluate methods for deleting specific
multimodal knowledge from MLLMs. We extend a visual question-answering dataset
using an automated pipeline that generates varying-proximity samples for
testing generalization and specificity, followed by manual filtering for
maintaining high quality. We then evaluate six defense objectives against seven
attacks (four whitebox, three blackbox), including a novel whitebox method
leveraging interpretability of hidden states. Our results show multimodal
attacks outperform text- or image-only ones, and that the most effective
defense removes answer information from internal model states. Additionally,
larger models exhibit greater post-editing robustness, suggesting that scale
enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing
unlearning in MLLMs.

</details>


### [2] [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/pdf/2505.01459)
*Abdoul Majid O. Thiombiano, Brahim Hnich, Ali Ben Mrad, Mohamed Wiem Mkaouer*

Main category: cs.CL

TL;DR: MoxE combines xLSTM and MoE to improve scalability and efficiency in LLMs, using entropy-based routing and auxiliary losses for balanced resource use and robust performance.


<details>
  <summary>Details</summary>
Motivation: Address scalability and efficiency challenges in large language models by leveraging xLSTM's memory structures and MoE's sparsity.

Method: Integrates xLSTM with MoE, using entropy-based routing to dynamically assign tokens to experts and auxiliary losses for training stability.

Result: Achieves significant efficiency gains and enhanced effectiveness compared to existing approaches.

Conclusion: MoxE advances scalable LLM architectures by efficiently managing computational resources and improving performance.

Abstract: This paper introduces MoxE, a novel architecture that synergistically
combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of
Experts (MoE) framework to address critical scalability and efficiency
challenges in large language models (LLMs). The proposed method effectively
leverages xLSTM's innovative memory structures while strategically introducing
sparsity through MoE to substantially reduce computational overhead. At the
heart of our approach is a novel entropy-based routing mechanism, designed to
dynamically route tokens to specialized experts, thereby ensuring efficient and
balanced resource utilization. This entropy awareness enables the architecture
to effectively manage both rare and common tokens, with mLSTM blocks being
favored to handle rare tokens. To further enhance generalization, we introduce
a suite of auxiliary losses, including entropy-based and group-wise balancing
losses, ensuring robust performance and efficient training. Theoretical
analysis and empirical evaluations rigorously demonstrate that MoxE achieves
significant efficiency gains and enhanced effectiveness compared to existing
approaches, marking a notable advancement in scalable LLM architectures.

</details>


### [3] [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/pdf/2505.01479)
*Siheng Xiong, Jieyu Zhou, Zhangding Liu, Yusen Su*

Main category: cs.CL

TL;DR: SymPlanner enhances LM planning by integrating symbolic environments for structured, verifiable multi-step action sequences, outperforming natural language baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of coherent multi-step planning in LMs by grounding actions in a symbolic world model for better reliability.

Method: SymPlanner combines LMs with a symbolic environment for action proposal and verification, using Iterative Correction (IC) and Contrastive Ranking (CR) for refinement and comparison.

Result: Outperforms natural language baselines in PlanBench, producing more coherent, diverse, and verifiable plans.

Conclusion: SymPlanner effectively bridges LM reasoning with symbolic grounding, improving planning robustness and reliability.

Abstract: Planning remains a core challenge for language models (LMs), particularly in
domains that require coherent multi-step action sequences grounded in external
constraints. We introduce SymPlanner, a novel framework that equips LMs with
structured planning capabilities by interfacing them with a symbolic
environment that serves as an explicit world model. Rather than relying purely
on natural language reasoning, SymPlanner grounds the planning process in a
symbolic state space, where a policy model proposes actions and a symbolic
environment deterministically executes and verifies their effects. To enhance
exploration and improve robustness, we introduce Iterative Correction (IC),
which refines previously proposed actions by leveraging feedback from the
symbolic environment to eliminate invalid decisions and guide the model toward
valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained
comparison of candidate plans by evaluating them jointly. We evaluate
SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,
and verifiable plans than pure natural language baselines.

</details>


### [4] [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/pdf/2505.01559)
*Daniele Grandi, Fabian Riquelme*

Main category: cs.CL

TL;DR: The paper evaluates large language models in mechanical engineering using the ABC dataset, focusing on semantic assembly and part names. It introduces two unsupervised tasks (binary sentence-pair and zero-shot classification) and achieves notable accuracy improvements with fine-tuning and architectural adjustments.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of large language models in the mechanical engineering domain, leveraging semantic data from the ABC dataset.

Method: Developed two unsupervised tasks: binary sentence-pair classification and zero-shot classification. Fine-tuned models with adjustments like learning rates, dropout, sequence length, and multi-head attention.

Result: Achieved 0.62 accuracy in binary sentence-pair classification and 0.386 top-1 accuracy in zero-shot classification, outperforming baselines.

Conclusion: The study highlights specific failure modes in domain-specific language learning and demonstrates the effectiveness of fine-tuning and architectural modifications.

Abstract: In this work, we seek to understand the performance of large language models
in the mechanical engineering domain. We leverage the semantic data found in
the ABC dataset, specifically the assembly names that designers assigned to the
overall assemblies, and the individual semantic part names that were assigned
to each part. After pre-processing the data we developed two unsupervised tasks
to evaluate how different model architectures perform on domain-specific data:
a binary sentence-pair classification task and a zero-shot classification task.
We achieved a 0.62 accuracy for the binary sentence-pair classification task
with a fine-tuned model that focuses on fighting over-fitting: 1) modifying
learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a
multi-head attention layer. Our model on the zero-shot classification task
outperforms the baselines by a wide margin, and achieves a top-1 classification
accuracy of 0.386. The results shed some light on the specific failure modes
that arise when learning from language in this domain.

</details>


### [5] [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/pdf/2505.01560)
*Vicent Briva Iglesias, Gokhan Dogru*

Main category: cs.CL

TL;DR: The paper compares LLMs and multi-agent workflows against NMT in machine translation, finding NMT superior in automatic metrics but LLMs better in human evaluation, though at higher computational costs.


<details>
  <summary>Details</summary>
Motivation: To empirically assess the benefits of LLMs and multi-agent workflows compared to conventional NMT in machine translation.

Method: Benchmarked five paradigms (Google Translate, GPT-4o, o1-preview, and two GPT-4o-powered agentic workflows) on legal and news text in three languages, using automatic and human evaluation.

Result: NMT outperformed in automatic metrics, while LLMs (especially o1-preview) excelled in human evaluation. Multi-agent workflows were costly.

Conclusion: Advocates for cost-aware evaluation and suggests leaner coordination, selective agent activation, and hybrid pipelines for future research.

Abstract: Large language models (LLMs) and multi-agent orchestration are touted as the
next leap in machine translation (MT), but their benefits relative to
conventional neural MT (NMT) remain unclear. This paper offers an empirical
reality check. We benchmark five paradigms, Google Translate (strong NMT
baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),
and two GPT-4o-powered agentic workflows (sequential three-stage and iterative
refinement), on test data drawn from a legal contract and news prose in three
English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is
performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with
expert ratings of adequacy and fluency; efficiency with total input-plus-output
token counts mapped to April 2025 pricing.
  Automatic scores still favour the mature NMT system, which ranks first in
seven of twelve metric-language combinations; o1-preview ties or places second
in most remaining cases, while both multi-agent workflows trail. Human
evaluation reverses part of this narrative: o1-preview produces the most
adequate and fluent output in five of six comparisons, and the iterative agent
edges ahead once, indicating that reasoning layers capture semantic nuance
undervalued by surface metrics. Yet these qualitative gains carry steep costs.
The sequential agent consumes roughly five times, and the iterative agent
fifteen times, the tokens used by NMT or single-pass LLMs.
  We advocate multidimensional, cost-aware evaluation protocols and highlight
research directions that could tip the balance: leaner coordination strategies,
selective agent activation, and hybrid pipelines combining single-pass LLMs
with targeted agent intervention.

</details>


### [6] [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/pdf/2505.01592)
*Takyoung Kim, Janvijay Singh, Shuhaib Mehri, Emre Can Acikgoz, Sagnik Mukherjee, Nimet Beyza Bozdag, Sumuk Shashidhar, Gokhan Tur, Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: The paper introduces PIPA, a unified evaluation protocol for interactive task planning agents, emphasizing user satisfaction over just task completion.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on task completion, neglecting user satisfaction with the entire agentic process.

Method: PIPA conceptualizes agent behavior within a POMDP paradigm, using atomic evaluation criteria for comprehensive assessment.

Result: Agents perform variably across behavioral stages, with user satisfaction influenced by both outcomes and intermediate behaviors.

Conclusion: PIPA provides a nuanced evaluation framework, highlighting the need for multi-agent systems and better user simulators.

Abstract: The growing capabilities of large language models (LLMs) in
instruction-following and context-understanding lead to the era of agents with
numerous applications. Among these, task planning agents have become especially
prominent in realistic scenarios involving complex internal pipelines, such as
context understanding, tool management, and response generation. However,
existing benchmarks predominantly evaluate agent performance based on task
completion as a proxy for overall effectiveness. We hypothesize that merely
improving task completion is misaligned with maximizing user satisfaction, as
users interact with the entire agentic process and not only the end result. To
address this gap, we propose PIPA, a unified evaluation protocol that
conceptualizes the behavioral process of interactive task planning agents
within a partially observable Markov Decision Process (POMDP) paradigm. The
proposed protocol offers a comprehensive assessment of agent performance
through a set of atomic evaluation criteria, allowing researchers and
practitioners to diagnose specific strengths and weaknesses within the agent's
decision-making pipeline. Our analyses show that agents excel in different
behavioral stages, with user satisfaction shaped by both outcomes and
intermediate behaviors. We also highlight future directions, including systems
that leverage multiple agents and the limitations of user simulators in task
planning.

</details>


### [7] [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/pdf/2505.01595)
*Liaoyaqi Wang, Zhengping Jiang, Anqi Liu, Benjamin Van Durme*

Main category: cs.CL

TL;DR: A state-of-the-art model for fine-grained probability estimation under uncertainty, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with accurate probabilistic predictions under uncertainty, and reliable uncertainty estimates are understudied.

Method: Combines human and synthetic data, scales to larger models, and improves supervision for precise probability estimation.

Result: Outperforms fine-tuned and prompting-based methods by a large margin in conditional probability tasks.

Conclusion: The proposed model significantly enhances probabilistic reasoning in LLMs under uncertainty.

Abstract: We present a state-of-the-art model for fine-grained probability estimation
of propositions conditioned on context. Recent advances in large language
models (LLMs) have significantly enhanced their reasoning capabilities,
particularly on well-defined tasks with complete information. However, LLMs
continue to struggle with making accurate and well-calibrated probabilistic
predictions under uncertainty or partial information. While incorporating
uncertainty into model predictions often boosts performance, obtaining reliable
estimates of that uncertainty remains understudied. In particular, LLM
probability estimates tend to be coarse and biased towards more frequent
numbers. Through a combination of human and synthetic data creation and
assessment, scaling to larger models, and better supervision, we propose a set
of strong and precise probability estimation models. We conduct systematic
evaluations across tasks that rely on conditional probability estimation and
show that our approach consistently outperforms existing fine-tuned and
prompting-based methods by a large margin.

</details>


### [8] [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/pdf/2505.02518)
*Muhammad Hazim Al Farouq, Aman Kassahun Wassie, Yasmin Moslem*

Main category: cs.CL

TL;DR: A system for Bemba-to-English speech translation using Whisper and NLLB-200 with data augmentation techniques like back-translation.


<details>
  <summary>Details</summary>
Motivation: To address low-resource language translation challenges, specifically for Bemba-to-English.

Method: Cascaded systems combining Whisper (speech-to-text) and NLLB-200 (text-to-text), enhanced with synthetic data via back-translation.

Result: Investigated the impact of synthetic data on translation performance.

Conclusion: Demonstrated the feasibility of using cascaded systems and data augmentation for low-resource language translation.

Abstract: This paper describes our system submission to the International Conference on
Spoken Language Translation (IWSLT 2025), low-resource languages track, namely
for Bemba-to-English speech translation. We built cascaded speech translation
systems based on Whisper and NLLB-200, and employed data augmentation
techniques, such as back-translation. We investigate the effect of using
synthetic data and discuss our experimental setup.

</details>


### [9] [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/pdf/2505.01658)
*Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee*

Main category: cs.CL

TL;DR: The paper evaluates 25 open-source and commercial LLM inference engines, analyzing their usability, deployment, scalability, and optimization techniques, while providing future research directions and a public repository for updates.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs in diverse applications raises inference costs, but selecting the right optimization method is challenging due to varied service requirements. A systematic study of inference engines is needed.

Method: Comprehensive evaluation of 25 inference engines, assessing ease-of-use, deployment, scalability, and optimization techniques, along with ecosystem maturity and cost policies.

Result: The study identifies gaps and strengths in current inference engines, offering insights into their suitability for different workloads and hardware.

Conclusion: Future research should focus on complex LLM services, hardware diversity, and security, with practical guidance for selecting and designing optimized inference engines.

Abstract: Large language models (LLMs) are widely applied in chatbots, code generators,
and search engines. Workloads such as chain-of-thought, complex reasoning, and
agent services significantly increase the inference cost by invoking the model
repeatedly. Optimization methods such as parallelism, compression, and caching
have been adopted to reduce costs, but the diverse service requirements make it
hard to select the right method. Recently, specialized LLM inference engines
have emerged as a key component for integrating the optimization methods into
service-oriented infrastructures. However, a systematic study on inference
engines is still lacking. This paper provides a comprehensive evaluation of 25
open-source and commercial inference engines. We examine each inference engine
in terms of ease-of-use, ease-of-deployment, general-purpose support,
scalability, and suitability for throughput- and latency-aware computation.
Furthermore, we explore the design goals of each inference engine by
investigating the optimization techniques it supports. In addition, we assess
the ecosystem maturity of open source inference engines and handle the
performance and cost policy of commercial solutions. We outline future research
directions that include support for complex LLM-based services, support of
various hardware, and enhanced security, offering practical guidance to
researchers and developers in selecting and designing optimized LLM inference
engines. We also provide a public repository to continually track developments
in this fast-evolving field:
https://github.com/sihyeong/Awesome-LLM-Inference-Engine

</details>


### [10] [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/pdf/2505.02615)
*Armita Mohammadi, Alessandro Lameiras Koerich, Laureano Moro-Velazquez, Patrick Cardinal*

Main category: cs.CL

TL;DR: The paper explores deep learning for automated L2 English proficiency assessment, using models like CNN, ResNet, wav2vec 2.0, and BERT for speech and text analysis, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Traditional L2 proficiency evaluation by teachers is subjective and variable. The paper aims to automate this using deep learning for consistency and robustness.

Method: Uses diverse architectures (2D CNN, frequency-based CNN, ResNet, wav2vec 2.0) for speech, and fine-tunes BERT for text. Evaluates spontaneous dialogue with separate wav2vec 2.0 and BERT models.

Result: Experiments on EFCamDat, ANGLISH, and a private dataset show deep learning, especially wav2vec 2.0, is effective for automated L2 assessment.

Conclusion: Deep learning, particularly pretrained models like wav2vec 2.0, offers a robust solution for automated L2 proficiency evaluation.

Abstract: Second language proficiency (L2) in English is usually perceptually evaluated
by English teachers or expert evaluators, with the inherent intra- and
inter-rater variability. This paper explores deep learning techniques for
comprehensive L2 proficiency assessment, addressing both the speech signal and
its correspondent transcription. We analyze spoken proficiency classification
prediction using diverse architectures, including 2D CNN, frequency-based CNN,
ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based
proficiency assessment by fine-tuning a BERT language model within resource
constraints. Finally, we tackle the complex task of spontaneous dialogue
assessment, managing long-form audio and speaker interactions through separate
applications of wav2vec 2.0 and BERT models. Results from experiments on
EFCamDat and ANGLISH datasets and a private dataset highlight the potential of
deep learning, especially the pretrained wav2vec 2.0 model, for robust
automated L2 proficiency evaluation.

</details>


### [11] [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/pdf/2505.01794)
*Jared D. T. Guerrero-Sosa, Francisco P. Romero, Víctor Hugo Menéndez-Domínguez, Jesus Serrano-Guerrero, Andres Montoro-Montarroso, Jose A. Olivas*

Main category: cs.CL

TL;DR: A fuzzy logic approach with multimodal analysis is proposed to assess soft skills in students, improving reliability and interpretability.


<details>
  <summary>Details</summary>
Motivation: The challenge of unbiased soft skill assessment in higher education drives the need for a structured, nuanced method.

Method: Uses a Granular Linguistic Model of Phenomena and multimodal analysis (e.g., facial expressions, gestures) to evaluate soft skills.

Result: The framework effectively consolidates data for consistent assessments, enhancing score quality and transparency.

Conclusion: Integrating multiple modalities improves soft skill evaluations, benefiting educational stakeholders.

Abstract: In the rapidly evolving educational landscape, the unbiased assessment of
soft skills is a significant challenge, particularly in higher education. This
paper presents a fuzzy logic approach that employs a Granular Linguistic Model
of Phenomena integrated with multimodal analysis to evaluate soft skills in
undergraduate students. By leveraging computational perceptions, this approach
enables a structured breakdown of complex soft skill expressions, capturing
nuanced behaviours with high granularity and addressing their inherent
uncertainties, thereby enhancing interpretability and reliability. Experiments
were conducted with undergraduate students using a developed tool that assesses
soft skills such as decision-making, communication, and creativity. This tool
identifies and quantifies subtle aspects of human interaction, such as facial
expressions and gesture recognition. The findings reveal that the framework
effectively consolidates multiple data inputs to produce meaningful and
consistent assessments of soft skills, showing that integrating multiple
modalities into the evaluation process significantly improves the quality of
soft skills scores, making the assessment work transparent and understandable
to educational stakeholders.

</details>


### [12] [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/pdf/2505.02625)
*Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng*

Main category: cs.CL

TL;DR: LLaMA-Omni 2 is a series of speech language models (0.5B to 14B parameters) for real-time speech interaction, outperforming state-of-the-art models like GLM-4-Voice with minimal training data.


<details>
  <summary>Details</summary>
Motivation: To advance real-time, intelligent speech interaction for next-gen human-computer interaction using large language models.

Method: Built on Qwen2.5 models, integrating a speech encoder and autoregressive streaming speech decoder, trained on 200K multi-turn speech dialogues.

Result: Strong performance on spoken QA and speech instruction benchmarks, surpassing GLM-4-Voice despite less training data.

Conclusion: LLaMA-Omni 2 demonstrates efficient, high-quality speech interaction with minimal data, setting a new benchmark for SpeechLMs.

Abstract: Real-time, intelligent, and natural speech interaction is an essential part
of the next-generation human-computer interaction. Recent advancements have
showcased the potential of building intelligent spoken chatbots based on large
language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of
speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable
of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built
upon the Qwen2.5 series models, integrating a speech encoder and an
autoregressive streaming speech decoder. Despite being trained on only 200K
multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong
performance on several spoken question answering and speech instruction
following benchmarks, surpassing previous state-of-the-art SpeechLMs like
GLM-4-Voice, which was trained on millions of hours of speech data.

</details>


### [13] [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/pdf/2505.01693)
*Brian Wong, Kaito Tanaka*

Main category: cs.CL

TL;DR: DeBERTa-RAD, a two-stage framework combining LLM pseudo-labeling and DeBERTa-based knowledge distillation, achieves high accuracy and speed for labeling chest X-ray reports.


<details>
  <summary>Details</summary>
Motivation: Automated labeling of chest X-ray reports is challenging due to variability, complexity, and negation/uncertainty in free-text reports, requiring efficient and accurate solutions.

Method: Uses LLM for pseudo-labeling, then trains a DeBERTa-Base model via knowledge distillation on the pseudo-labeled data.

Result: Achieves a Macro F1 score of 0.9120 on MIMIC-500, outperforming rule-based systems and fine-tuned models, with practical inference speed.

Conclusion: Demonstrates a scalable solution for medical text processing by leveraging LLMs and efficient distillation.

Abstract: Automated labeling of chest X-ray reports is essential for enabling
downstream tasks such as training image-based diagnostic models, population
health studies, and clinical decision support. However, the high variability,
complexity, and prevalence of negation and uncertainty in these free-text
reports pose significant challenges for traditional Natural Language Processing
methods. While large language models (LLMs) demonstrate strong text
understanding, their direct application for large-scale, efficient labeling is
limited by computational cost and speed. This paper introduces DeBERTa-RAD, a
novel two-stage framework that combines the power of state-of-the-art LLM
pseudo-labeling with efficient DeBERTa-based knowledge distillation for
accurate and fast chest X-ray report labeling. We leverage an advanced LLM to
generate high-quality pseudo-labels, including certainty statuses, for a large
corpus of reports. Subsequently, a DeBERTa-Base model is trained on this
pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated
on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a
state-of-the-art Macro F1 score of 0.9120, significantly outperforming
established rule-based systems, fine-tuned transformer models, and direct LLM
inference, while maintaining a practical inference speed suitable for
high-throughput applications. Our analysis shows particular strength in
handling uncertain findings. This work demonstrates a promising path to
overcome data annotation bottlenecks and achieve high-performance medical text
processing through the strategic combination of LLM capabilities and efficient
student models trained via distillation.

</details>


### [14] [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/pdf/2505.02692)
*Maxime Poli, Emmanuel Chemla, Emmanuel Dupoux*

Main category: cs.CL

TL;DR: fastabx is a high-performance Python library for building ABX discrimination tasks, addressing the lack of tools for broader adoption of ABX measures.


<details>
  <summary>Details</summary>
Motivation: The absence of adequate tools for ABX tasks limits its broader adoption in evaluating phonetic discriminability and other domains.

Method: fastabx provides a framework for constructing any ABX task efficiently, enabling rapid development and distance calculations.

Result: The library facilitates systematic investigation of information extraction from learned representations across domains beyond speech.

Conclusion: fastabx is a valuable resource for the representation learning community, with its source code available on GitHub.

Abstract: We introduce fastabx, a high-performance Python library for building ABX
discrimination tasks. ABX is a measure of the separation between generic
categories of interest. It has been used extensively to evaluate phonetic
discriminability in self-supervised speech representations. However, its
broader adoption has been limited by the absence of adequate tools. fastabx
addresses this gap by providing a framework capable of constructing any type of
ABX task while delivering the efficiency necessary for rapid development
cycles, both in task creation and in calculating distances between
representations. We believe that fastabx will serve as a valuable resource for
the broader representation learning community, enabling researchers to
systematically investigate what information can be directly extracted from
learned representations across several domains beyond speech processing. The
source code is available at https://github.com/bootphon/fastabx.

</details>


### [15] [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/pdf/2505.01731)
*Chuan Sun, Han Yu, Lizhen Cui*

Main category: cs.CL

TL;DR: Proposes a non-uniform pruning method (SVNUP) for LLMs using Shapley Values to assign tailored pruning budgets per layer, improving performance over uniform pruning.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform pruning methods are suboptimal due to varying layer significance in LLMs.

Method: Uses Shapley Value to quantify layer contributions, assigns pruning budgets per layer, and introduces a sliding window approximation for efficiency.

Result: Achieves 18.01% and 19.55% PPL reduction on LLaMA-7B and LLaMA-13B vs. SparseGPT at 70% sparsity.

Conclusion: SVNUP effectively enhances pruned LLM performance by accounting for layer importance.

Abstract: Pruning large language models (LLMs) is a promising solution for reducing
model sizes and computational complexity while preserving performance.
Traditional layer-wise pruning methods often adopt a uniform sparsity approach
across all layers, which leads to suboptimal performance due to the varying
significance of individual transformer layers within the model not being
accounted for. To this end, we propose the \underline{S}hapley
\underline{V}alue-based \underline{N}on-\underline{U}niform \underline{P}runing
(\methodname{}) method for LLMs. This approach quantifies the contribution of
each transformer layer to the overall model performance, enabling the
assignment of tailored pruning budgets to different layers to retain critical
parameters. To further improve efficiency, we design the Sliding Window-based
Shapley Value approximation method. It substantially reduces computational
overhead compared to exact SV calculation methods. Extensive experiments on
various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness
of the proposed approach. The results reveal that non-uniform pruning
significantly enhances the performance of pruned models. Notably, \methodname{}
achieves a reduction in perplexity (PPL) of 18.01\% and 19.55\% on LLaMA-7B and
LLaMA-13B, respectively, compared to SparseGPT at 70\% sparsity.

</details>


### [16] [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/pdf/2505.01761)
*Tobias Domhan, Dawei Zhu*

Main category: cs.CL

TL;DR: LLMs show promise for evaluating machine-translated text, but text length biases results. Proposed methods like FSP and fine-tuning mitigate this bias.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating long-form machine translations accurately, as current methods are length-sensitive.

Method: Tested LLMs for document-level translation evaluation, identified length bias, and proposed solutions like Focus Sentence Prompting (FSP) and fine-tuning.

Result: Longer texts reduce error spans and ranking accuracy; FSP and fine-tuning mitigate this bias.

Conclusion: LLMs can be adapted for reliable long-form translation evaluation with methods like FSP and fine-tuning.

Abstract: Accurately evaluating machine-translated text remains a long-standing
challenge, particularly for long documents. Recent work has shown that large
language models (LLMs) can serve as reliable and interpretable sentence-level
translation evaluators via MQM error span annotations. With modern LLMs
supporting larger context windows, a natural question arises: can we feed
entire document translations into an LLM for quality assessment? Ideally,
evaluation should be invariant to text length, producing consistent error spans
regardless of input granularity. However, our analysis shows that text length
significantly impacts evaluation: longer texts lead to fewer error spans and
reduced system ranking accuracy. To address this limitation, we evaluate
several strategies, including granularity-aligned prompting, Focus Sentence
Prompting (FSP), and a fine-tuning approach to better align LLMs with the
evaluation task. The latter two methods largely mitigate this length bias,
making LLMs more reliable for long-form translation evaluation.

</details>


### [17] [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/pdf/2505.01800)
*Chidimma Opara*

Main category: cs.CL

TL;DR: A framework combining stylometric analysis and psycholinguistic theories to detect AI-generated texts, focusing on cognitive processes in human writing.


<details>
  <summary>Details</summary>
Motivation: The need for accurate detection tools in educational settings to verify authorship due to advanced AI-generated texts.

Method: Integration of 31 stylometric features with psycholinguistic theories, mapping them to cognitive processes like lexical retrieval and metacognitive self-monitoring.

Result: A clear, interpretable approach to distinguish AI-generated from human-written texts, leveraging unique psycholinguistic patterns.

Conclusion: The framework aids in developing reliable tools for academic integrity in the generative AI era.

Abstract: The increasing sophistication of AI-generated texts highlights the urgent
need for accurate and transparent detection tools, especially in educational
settings, where verifying authorship is essential. Existing literature has
demonstrated that the application of stylometric features with machine learning
classifiers can yield excellent results. Building on this foundation, this
study proposes a comprehensive framework that integrates stylometric analysis
with psycholinguistic theories, offering a clear and interpretable approach to
distinguishing between AI-generated and human-written texts. This research
specifically maps 31 distinct stylometric features to cognitive processes such
as lexical retrieval, discourse planning, cognitive load management, and
metacognitive self-monitoring. In doing so, it highlights the unique
psycholinguistic patterns found in human writing. Through the intersection of
computational linguistics and cognitive science, this framework contributes to
the development of reliable tools aimed at preserving academic integrity in the
era of generative AI.

</details>


### [18] [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/pdf/2505.01812)
*Core Francisco Park, Zechen Zhang, Hidenori Tanaka*

Main category: cs.CL

TL;DR: The paper introduces 'New News,' a dataset for evaluating how models internalize new information, highlights a gap between fine-tuning and in-context learning, and proposes 'System-2 Fine-tuning' (Sys2-FT) to bridge this gap.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fine-tuning models to internalize new information effectively, unlike in-context learning where explicit context aids performance.

Method: Introduces the 'New News' dataset and explores self-play data generation protocols (paraphrases, implications, Self-QAs) for Sys2-FT. Evaluates performance across domains and model scales.

Result: Sys2-FT, especially the self-QA protocol, improves in-weight learning. Identifies a 'contextual shadowing effect' and hints at a scaling law for Sys2-FT.

Conclusion: Sys2-FT enhances models' ability to internalize new information, with potential implications for scaling and training protocols.

Abstract: Humans and intelligent animals can effortlessly internalize new information
("news") and accurately extract the implications for performing downstream
tasks. While large language models (LLMs) can achieve this through in-context
learning (ICL) when the news is explicitly given as context, fine-tuning
remains challenging for the models to consolidate learning in weights. In this
paper, we introduce $\textit{New News}$, a dataset composed of hypothetical yet
plausible news spanning multiple domains (mathematics, coding, discoveries,
leaderboards, events), accompanied by downstream evaluation questions whose
correct answers critically depend on understanding and internalizing the news.
We first demonstrate a substantial gap between naive fine-tuning and in-context
learning (FT-ICL gap) on our news dataset. To address this gap, we explore a
suite of self-play data generation protocols -- paraphrases, implications and
Self-QAs -- designed to distill the knowledge from the model with context into
the weights of the model without the context, which we term $\textit{System-2
Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance
across data domains and model scales with the Qwen 2.5 family of models. Our
results demonstrate that the self-QA protocol of Sys2-FT significantly improves
models' in-weight learning of the news. Furthermore, we discover the
$\textit{contexual shadowing effect}$, where training with the news $\textit{in
context}$ followed by its rephrases or QAs degrade learning of the news.
Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.

</details>


### [19] [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/pdf/2502.12050)
*Yasmin Moslem, Juan Julián Cea Morán, Mariano Gonzalez-Gomez, Muhammad Hazim Al Farouq, Farah Abdou, Satarupa Deb*

Main category: cs.CL

TL;DR: Summary of the first mentorship in speech translation (SpeechT), covering activities like data preparation, modeling, and research, with a focus on data augmentation and system comparisons across multiple languages.


<details>
  <summary>Details</summary>
Motivation: To document and share the findings of the inaugural mentorship program in speech translation, highlighting the exploration of diverse languages and methodologies.

Method: Participants engaged in data preparation, modeling, and research, comparing end-to-end and cascaded speech translation systems using data augmentation techniques.

Result: Projects included work on languages like Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish, showcasing diverse applications of speech translation.

Conclusion: The mentorship successfully explored key aspects of speech translation, providing insights into data augmentation and system comparisons for multilingual applications.

Abstract: This work presents the details and findings of the first mentorship in speech
translation (SpeechT), which took place in December 2024 and January 2025. To
fulfil the mentorship requirements, the participants engaged in key activities,
including data preparation, modelling, and advanced research. The participants
explored data augmentation techniques and compared end-to-end and cascaded
speech translation systems. The projects covered various languages other than
English, including Arabic, Bengali, Galician, Indonesian, Japanese, and
Spanish.

</details>


### [20] [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/pdf/2505.01855)
*Anthony Nguyen, Wenjun Lin*

Main category: cs.CL

TL;DR: Intra-Layer Recurrence (ILR) selectively applies recurrence to individual transformer layers, improving efficiency by focusing iterations on earlier layers.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of indiscriminate recurrence in existing recurrent transformer methods by targeting specific layers.

Method: Propose Intra-Layer Recurrence (ILR), applying recurrence selectively to individual layers within a single forward pass.

Result: Experiments show optimal results when more iterations are allocated to earlier layers.

Conclusion: ILR is a promising approach for optimizing recurrent structures in transformer architectures.

Abstract: Transformer models have established new benchmarks in natural language
processing; however, their increasing depth results in substantial growth in
parameter counts. While existing recurrent transformer methods address this
issue by reprocessing layers multiple times, they often apply recurrence
indiscriminately across entire blocks of layers. In this work, we investigate
Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence
selectively to individual layers within a single forward pass. Our experiments
show that allocating more iterations to earlier layers yields optimal results.
These findings suggest that ILR offers a promising direction for optimizing
recurrent structures in transformer architectures.

</details>


### [21] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/pdf/2504.21214)
*Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin*

Main category: cs.CL

TL;DR: The paper introduces the Large Brain Language Model (LBLM) for silent speech decoding in BCI, pretrained using a novel Future Spectro-Temporal Prediction (FSTP) method, achieving significant performance gains over baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance natural and flexible communication in active BCI systems by improving silent speech decoding using EEG signals.

Method: Proposes LBLM pretrained with FSTP, an autoregressive method capturing temporal and spectral EEG dependencies, followed by finetuning for word and semantic classification.

Result: LBLM outperforms baselines, achieving 47.0% (semantic) and 39.6% (word) accuracy in cross-session settings, with gains of 5.4% and 7.3%, respectively.

Conclusion: The research advances silent speech decoding in BCI, offering a novel EEG pretraining method and a valuable dataset for future studies.

Abstract: This paper explores silent speech decoding in active brain-computer interface
(BCI) systems, which offer more natural and flexible communication than
traditional BCI applications. We collected a new silent speech dataset of over
120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing
24 commonly used English words for language model pretraining and decoding.
Following the recent success of pretraining large models with self-supervised
paradigms to enhance EEG classification performance, we propose Large Brain
Language Model (LBLM) pretrained to decode silent speech for active BCI. To
pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining
paradigm to learn effective representations from unlabeled EEG data. Unlike
existing EEG pretraining methods that mainly follow a masked-reconstruction
paradigm, our proposed FSTP method employs autoregressive modeling in temporal
and frequency domains to capture both temporal and spectral dependencies from
EEG signals. After pretraining, we finetune our LBLM on downstream tasks,
including word-level and semantic-level classification. Extensive experiments
demonstrate significant performance gains of the LBLM over fully-supervised and
pretrained baseline models. For instance, in the difficult cross-session
setting, our model achieves 47.0\% accuracy on semantic-level classification
and 39.6\% in word-level classification, outperforming baseline methods by
5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in
active BCI systems, offering an innovative solution for EEG language model
pretraining and a new dataset for fundamental research.

</details>


### [22] [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/pdf/2505.01868)
*Mo Sun, Siheng Xiong, Yuankai Cai, Bowen Zuo*

Main category: cs.CL

TL;DR: A BERT-based NER framework with positional attention reduces training costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning BERT for NER is computationally expensive; this work aims to reduce costs without sacrificing performance.

Method: Integrates positional attention mechanisms into BERT for NER, using pre-trained parameters for customization.

Result: Achieves strong performance on a Kaggle dataset with fewer training epochs.

Conclusion: The framework offers a cost-efficient solution for BERT-based NER, balancing accuracy and computational expense.

Abstract: This paper presents a framework for Named Entity Recognition (NER) leveraging
the Bidirectional Encoder Representations from Transformers (BERT) model in
natural language processing (NLP). NER is a fundamental task in NLP with broad
applicability across downstream applications. While BERT has established itself
as a state-of-the-art model for entity recognition, fine-tuning it from scratch
for each new application is computationally expensive and time-consuming. To
address this, we propose a cost-efficient approach that integrates positional
attention mechanisms into the entity recognition process and enables effective
customization using pre-trained parameters. The framework is evaluated on a
Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves
strong performance with fewer training epochs. This work contributes to the
field by offering a practical solution for reducing the training cost of
BERT-based NER systems while maintaining high accuracy.

</details>


### [23] [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/pdf/2505.01877)
*Jiří Milička, Anna Marklová, Ondřej Drobil, Eva Pospíšilová*

Main category: cs.CL

TL;DR: Participants improved accuracy in distinguishing human-written from AI-generated texts with feedback, correcting misconceptions about AI style and readability.


<details>
  <summary>Details</summary>
Motivation: To determine if feedback helps individuals learn to discriminate between human and AI texts and recalibrate their self-perceived competence.

Method: Used GPT-4o to generate texts, presented pairs to 255 participants with/without feedback, measured accuracy, confidence, and readability judgments.

Result: Feedback group showed significant improvement in accuracy and confidence calibration, correcting initial misconceptions.

Conclusion: Targeted training with feedback enhances differentiation ability and self-assessment, relevant for education.

Abstract: This study investigates whether individuals can learn to accurately
discriminate between human-written and AI-produced texts when provided with
immediate feedback, and if they can use this feedback to recalibrate their
self-perceived competence. We also explore the specific criteria individuals
rely upon when making these decisions, focusing on textual style and perceived
readability.
  We used GPT-4o to generate several hundred texts across various genres and
text types comparable to Koditex, a multi-register corpus of human-written
texts. We then presented randomized text pairs to 255 Czech native speakers who
identified which text was human-written and which was AI-generated.
Participants were randomly assigned to two conditions: one receiving immediate
feedback after each trial, the other receiving no feedback until experiment
completion. We recorded accuracy in identification, confidence levels, response
times, and judgments about text readability along with demographic data and
participants' engagement with AI technologies prior to the experiment.
  Participants receiving immediate feedback showed significant improvement in
accuracy and confidence calibration. Participants initially held incorrect
assumptions about AI-generated text features, including expectations about
stylistic rigidity and readability. Notably, without feedback, participants
made the most errors precisely when feeling most confident -- an issue largely
resolved among the feedback group.
  The ability to differentiate between human and AI-generated texts can be
effectively learned through targeted training with explicit feedback, which
helps correct misconceptions about AI stylistic features and readability, as
well as potential other variables that were not explored, while facilitating
more accurate self-assessment. This finding might be particularly important in
educational contexts.

</details>


### [24] [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/pdf/2505.01883)
*Yiwen Lu, Siheng Xiong, Zhaowei Li*

Main category: cs.CL

TL;DR: A scalable framework for sentiment and topic analysis of Twitter data, combining automated sentiment labeling, LDA for theme identification, and interactive visualization.


<details>
  <summary>Details</summary>
Motivation: To analyze Twitter discourse at scale, focusing on sentiment and topics in dynamic geopolitical contexts.

Method: Targeted data collection, automated sentiment labeling with multiple models, LDA for topic modeling, and interactive visualization.

Result: Identifies relationships between sentiment and contextual features, and latent themes in Twitter discourse.

Conclusion: Provides a scalable methodology for social media analysis in geopolitical contexts, supported by interactive tools.

Abstract: We present a framework for large-scale sentiment and topic analysis of
Twitter discourse. Our pipeline begins with targeted data collection using
conflict-specific keywords, followed by automated sentiment labeling via
multiple pre-trained models to improve annotation robustness. We examine the
relationship between sentiment and contextual features such as timestamp,
geolocation, and lexical content. To identify latent themes, we apply Latent
Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and
metadata attributes. Finally, we develop an interactive visualization interface
to support exploration of sentiment trends and topic distributions across time
and regions. This work contributes a scalable methodology for social media
analysis in dynamic geopolitical contexts.

</details>


### [25] [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/pdf/2505.01900)
*Mazal Bethany, Nishant Vishwamitra, Cho-Yu Jason Chiang, Peyman Najafirad*

Main category: cs.CL

TL;DR: CAMOUFLAGE is an LLM-driven adversarial attack method for evidence-based misinformation detection systems, using iterative rewrites to bypass detection without altering claim meaning.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks fail against multi-component misinformation detection systems, necessitating a new approach.

Method: CAMOUFLAGE employs a two-agent system (Prompt Optimization Agent and Attacker Agent) to iteratively rewrite claims, manipulating evidence retrieval and comparison.

Result: Achieves 46.92% average success rate across four systems while maintaining semantic equivalence and coherence.

Conclusion: CAMOUFLAGE effectively bypasses detection systems without requiring classifier logits or extensive queries, highlighting vulnerabilities in current defenses.

Abstract: Automated evidence-based misinformation detection systems, which evaluate the
veracity of short claims against evidence, lack comprehensive analysis of their
adversarial vulnerabilities. Existing black-box text-based adversarial attacks
are ill-suited for evidence-based misinformation detection systems, as these
attacks primarily focus on token-level substitutions involving gradient or
logit-based optimization strategies, which are incapable of fooling the
multi-component nature of these detection systems. These systems incorporate
both retrieval and claim-evidence comparison modules, which requires attacks to
break the retrieval of evidence and/or the comparison module so that it draws
incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach
that employs a two-agent system, a Prompt Optimization Agent and an Attacker
Agent, to create adversarial claim rewritings that manipulate evidence
retrieval and mislead claim-evidence comparison, effectively bypassing the
system without altering the meaning of the claim. The Attacker Agent produces
semantically equivalent rewrites that attempt to mislead detectors, while the
Prompt Optimization Agent analyzes failed attack attempts and refines the
prompt of the Attacker to guide subsequent rewrites. This enables larger
structural and stylistic transformations of the text rather than token-level
substitutions, adapting the magnitude of changes based on previous outcomes.
Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on
binary model decisions to guide its rewriting process, eliminating the need for
classifier logits or extensive querying. We evaluate CAMOUFLAGE on four
systems, including two recent academic systems and two real-world APIs, with an
average attack success rate of 46.92\% while preserving textual coherence and
semantic equivalence to the original claims.

</details>


### [26] [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/pdf/2505.01967)
*Jiatao Li, Yanheng Li, Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper introduces the Social Worldview Taxonomy (SWT) to analyze socio-cognitive biases in LLMs, revealing distinct worldviews and their responsiveness to social cues.


<details>
  <summary>Details</summary>
Motivation: To explore under-examined socio-cognitive biases in LLMs, such as attitudes toward authority, equality, autonomy, and fate, beyond demographic and ethical biases.

Method: Developed SWT, a framework based on Cultural Theory, to operationalize four worldviews into measurable sub-dimensions. Analyzed 28 LLMs and tested their responsiveness to social cues.

Result: Identified distinct cognitive profiles in LLMs and showed systematic shaping of attitudes by social cues, with both general patterns and model-specific variations.

Conclusion: The findings improve LLM interpretability by uncovering implicit biases and their social responsiveness, aiding the development of more transparent and responsible AI.

Abstract: Large Language Models (LLMs) have become integral to daily life, widely
adopted in communication, decision-making, and information retrieval, raising
critical questions about how these systems implicitly form and express
socio-cognitive attitudes or "worldviews". While existing research extensively
addresses demographic and ethical biases, broader dimensions-such as attitudes
toward authority, equality, autonomy, and fate-remain under-explored. In this
paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework
grounded in Cultural Theory, operationalizing four canonical worldviews
(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable
sub-dimensions. Using SWT, we empirically identify distinct and interpretable
cognitive profiles across 28 diverse LLMs. Further, inspired by Social
Referencing Theory, we experimentally demonstrate that explicit social cues
systematically shape these cognitive attitudes, revealing both general response
patterns and nuanced model-specific variations. Our findings enhance the
interpretability of LLMs by revealing implicit socio-cognitive biases and their
responsiveness to social feedback, thus guiding the development of more
transparent and socially responsible language technologies.

</details>


### [27] [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/pdf/2505.01980)
*Theo Guidroz, Diego Ardila, Jimmy Li, Adam Mansour, Paul Jhun, Nina Gonzalez, Xiang Ji, Mike Sanchez, Sujay Kakarmath, Mathias MJ Bellaiche, Miguel Ángel Garrido, Faruk Ahmed, Divyansh Choudhary, Jay Hartford, Chenwei Xu, Henry Javier Serrano Echeverria, Yifan Wang, Jeff Shaffer, Eric, Cao, Yossi Matias, Avinatan Hassidim, Dale R Webster, Yun Liu, Sho Fujiwara, Peggy Bui, Quang Duong*

Main category: cs.CL

TL;DR: A study using LLMs for text simplification showed improved comprehension and ease for users, especially in biomedical texts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of complex web content surpassing users' reading levels by making expert knowledge more accessible.

Method: Used a self-refinement approach with LLMs for text simplification, validated via a randomized study with 4563 participants across 6 subject areas.

Result: Simplified texts led to a 3.9% absolute increase in MCQ accuracy, with the highest gain in PubMed (14.6%). Participants also reported greater ease.

Conclusion: LLMs can effectively simplify complex texts, improving accessibility and comprehension, particularly for biomedical content.

Abstract: Information on the web, such as scientific publications and Wikipedia, often
surpasses users' reading level. To help address this, we used a self-refinement
approach to develop a LLM capability for minimally lossy text simplification.
To validate our approach, we conducted a randomized study involving 4563
participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical
scientific articles), biology, law, finance, literature/philosophy, and
aerospace/computer science. Participants were randomized to viewing original or
simplified texts in a subject area, and answered multiple-choice questions
(MCQs) that tested their comprehension of the text. The participants were also
asked to provide qualitative feedback such as task difficulty. Our results
indicate that participants who read the simplified text answered more MCQs
correctly than their counterparts who read the original text (3.9% absolute
increase, p<0.05). This gain was most striking with PubMed (14.6%), while more
moderate gains were observed for finance (5.5%), aerospace/computer science
(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether
participants could refer back to the text while answering MCQs. The absolute
accuracy decreased by up to ~9% for both original and simplified setups where
participants could not refer back to the text, but the ~4% overall improvement
persisted. Finally, participants' self-reported perceived ease based on a
simplified NASA Task Load Index was greater for those who read the simplified
text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,
involving an order of magnitude more participants than prior works,
demonstrates the potential of LLMs to make complex information easier to
understand. Our work aims to enable a broader audience to better learn and make
use of expert knowledge available on the web, improving information
accessibility.

</details>


### [28] [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/pdf/2505.02009)
*Sai Krishna Mendu, Harish Yenala, Aditi Gulati, Shanu Kumar, Parag Agrawal*

Main category: cs.CL

TL;DR: The paper analyzes harmful content in LLM pretraining datasets, introduces tools for filtering toxic content, and provides benchmarks for safer LLM development.


<details>
  <summary>Details</summary>
Motivation: To address the risks of training LLMs on unfiltered web data, which can perpetuate toxicity, misinformation, and biases, undermining trust and ethical use.

Method: Conducts a large-scale analysis of harmful content, introduces a taxonomy (Topical vs. Toxic), a prompt evaluation dataset (TTP), a transformer-based model (HarmFormer), and a toxicity benchmark (HAVOC).

Result: Develops tools and benchmarks for filtering harmful content, offering insights into adversarial toxic inputs and safer pretraining practices.

Conclusion: The work supports Responsible AI compliance by providing resources to mitigate risks in LLM pretraining and improve ethical standards.

Abstract: Large language models (LLMs) have become integral to various real-world
applications, leveraging massive, web-sourced datasets like Common Crawl, C4,
and FineWeb for pretraining. While these datasets provide linguistic data
essential for high-quality natural language generation, they often contain
harmful content, such as hate speech, misinformation, and biased narratives.
Training LLMs on such unfiltered data risks perpetuating toxic behaviors,
spreading misinformation, and amplifying societal biases which can undermine
trust in LLM-driven applications and raise ethical concerns about their use.
This paper presents a large-scale analysis of inappropriate content across
these datasets, offering a comprehensive taxonomy that categorizes harmful
webpages into Topical and Toxic based on their intent. We also introduce a
prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and
a transformer-based model (HarmFormer) for content filtering. Additionally, we
create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide
crucial insights into how models respond to adversarial toxic inputs. Upon
publishing, we will also opensource our model signal on the entire C4 dataset.
Our work offers insights into ensuring safer LLM pretraining and serves as a
resource for Responsible AI (RAI) compliance.

</details>


### [29] [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/pdf/2505.02032)
*Anisia Katinskaia*

Main category: cs.CL

TL;DR: A review of AI applications in Computer-Assisted Language Learning (CALL), highlighting the need for scalable solutions and interdisciplinary collaboration.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for language learning support due to factors like pandemics, migration, and cost constraints drives the need for AI-assisted CALL systems.

Method: Review and perspective on AI methods for CALL, focusing on interdisciplinary connections and developer insights.

Result: Identifies gaps in complete CALL solutions and emphasizes the potential of recent AI advances to improve language learning.

Conclusion: Calls for more surveys on AI in CALL and interdisciplinary collaboration to advance the field.

Abstract: Computer-assisted language learning -- CALL -- is an established research
field. We review how artificial intelligence can be applied to support language
learning and teaching. The need for intelligent agents that assist language
learners and teachers is increasing: the human teacher's time is a scarce and
costly resource, which does not scale with growing demand. Further factors
contribute to the need for CALL: pandemics and increasing demand for distance
learning, migration of large populations, the need for sustainable and
affordable support for learning, etc. CALL systems are made up of many
components that perform various functions, and AI is applied to many different
aspects in CALL, corresponding to their own expansive research areas. Most of
what we find in the research literature and in practical use are prototypes or
partial implementations -- systems that perform some aspects of the overall
desired functionality. Complete solutions -- most of them commercial -- are
few, because they require massive resources. Recent advances in AI should
result in improvements in CALL, yet there is a lack of surveys that focus on AI
in the context of this research field. This paper aims to present a perspective
on the AI methods that can be employed for language learning from a position of
a developer of a CALL system. We also aim to connect work from different
disciplines, to build bridges for interdisciplinary work.

</details>


### [30] [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/pdf/2505.02072)
*Eitan Wagner, Omri Abend*

Main category: cs.CL

TL;DR: The paper distinguishes between distribution estimation and response prediction in LLMs, highlighting their conflicting goals and three distinct intended output distributions. It critiques the assumption of similarity between these distributions in NLP works and aims to provide clearer foundations for interpreting LLMs.


<details>
  <summary>Details</summary>
Motivation: To clarify the distinction between distribution estimation and response prediction in LLMs, addressing the misinterpretations arising from conflating their goals in NLP research.

Method: Analyzes LLM training phases (pretraining, in-context learning, preference tuning) and output probability use cases (completion probabilities, explicit probabilities). Identifies three distinct intended output distributions.

Result: Demonstrates that NLP works often misinterpret findings by assuming similarity between these distributions.

Conclusion: Provides firmer formal foundations for interpreting LLMs, aiding future research on their use and interpretation.

Abstract: The notion of language modeling has gradually shifted in recent years from a
distribution over finite-length strings to general-purpose prediction models
for textual inputs and outputs, following appropriate alignment phases. This
paper analyzes the distinction between distribution estimation and response
prediction in the context of LLMs, and their often conflicting goals. We
examine the training phases of LLMs, which include pretraining, in-context
learning, and preference tuning, and also the common use cases for their output
probabilities, which include completion probabilities and explicit
probabilities as output. We argue that the different settings lead to three
distinct intended output distributions. We demonstrate that NLP works often
assume that these distributions should be similar, which leads to
misinterpretations of their experimental findings. Our work sets firmer formal
foundations for the interpretation of LLMs, which will inform ongoing work on
the interpretation and use of LLMs' induced distributions.

</details>


### [31] [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/pdf/2505.02078)
*Joy Lim Jia Yin, Daniel Zhang-Li, Jifan Yu, Haoxuan Li, Shangqing Tu, Yuanchun Wang, Zhiyuan Liu, Huiqin Liu, Lei Hou, Juanzi Li, Bin Xu*

Main category: cs.CL

TL;DR: LecEval is an automated metric for evaluating slide-based multimedia instruction, outperforming existing methods by aligning with Mayer's Cognitive Theory and using four rubrics for assessment.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for slide-based multimedia instruction lack scalability, context capture, or are biased, prompting the need for a more effective automated solution.

Method: LecEval uses four rubrics (Content Relevance, Expressive Clarity, Logical Structure, Audience Engagement) and a dataset of 2,000+ slides from 50+ online courses to train a model.

Result: The trained model shows superior accuracy and adaptability, bridging the gap between automated and human evaluations.

Conclusion: LecEval provides a scalable, unbiased, and context-aware solution for assessing slide-based learning, with its dataset and toolkits publicly available.

Abstract: Evaluating the quality of slide-based multimedia instruction is challenging.
Existing methods like manual assessment, reference-based metrics, and large
language model evaluators face limitations in scalability, context capture, or
bias. In this paper, we introduce LecEval, an automated metric grounded in
Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal
knowledge acquisition in slide-based learning. LecEval assesses effectiveness
using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical
Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset
of over 2,000 slides from more than 50 online course videos, annotated with
fine-grained human ratings across these rubrics. A model trained on this
dataset demonstrates superior accuracy and adaptability compared to existing
metrics, bridging the gap between automated and human assessments. We release
our dataset and toolkits at https://github.com/JoylimJY/LecEval.

</details>


### [32] [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/pdf/2505.02091)
*Xinyue Peng, Yanming Liu, Yihan Cang, Chaoqun Cao, Ming Chen*

Main category: cs.CL

TL;DR: LLM-OptiRA uses LLMs to automate solving non-convex resource allocation problems in wireless systems, achieving high success rates.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with non-convex optimization in wireless systems, requiring expert knowledge and manual effort.

Method: LLM-OptiRA leverages LLMs to detect and transform non-convex problems into solvable forms, with error correction and feasibility checks.

Result: Achieves 96% execution rate and 80% success rate on GPT-4, outperforming baselines.

Conclusion: LLM-OptiRA simplifies and automates non-convex optimization, reducing expert dependency and improving robustness.

Abstract: Solving non-convex resource allocation problems poses significant challenges
in wireless communication systems, often beyond the capability of traditional
optimization techniques. To address this issue, we propose LLM-OptiRA, the
first framework that leverages large language models (LLMs) to automatically
detect and transform non-convex components into solvable forms, enabling fully
automated resolution of non-convex resource allocation problems in wireless
communication systems. LLM-OptiRA not only simplifies problem-solving by
reducing reliance on expert knowledge, but also integrates error correction and
feasibility validation mechanisms to ensure robustness. Experimental results
show that LLM-OptiRA achieves an execution rate of 96% and a success rate of
80% on GPT-4, significantly outperforming baseline approaches in complex
optimization tasks across diverse scenarios.

</details>


### [33] [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/pdf/2505.02142)
*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

Main category: cs.CL

TL;DR: Offline RL methods like DPO and LD-DPO improve LLM reasoning by 3.3% on average, with a 10.1% boost on Arena-Hard, while addressing computational costs of Online RL.


<details>
  <summary>Details</summary>
Motivation: High computational costs and complexity of Online RL methods for LLMs motivate exploring simpler, cost-effective Offline RL alternatives.

Method: Investigates Direct Preference Optimization (DPO) and its length-desensitized variant (LD-DPO) for enhancing LLM reasoning.

Result: Offline RL methods improve reasoning by 3.3% on average, with a 10.1% increase on Arena-Hard; output length sensitivity is analyzed.

Conclusion: Offline RL methods like DPO offer effective, economical alternatives to Online RL, with insights on output length alignment for better performance.

Abstract: Despite significant advances in long-context reasoning by large language
models (LLMs), primarily through Online Reinforcement Learning (RL) methods,
these approaches incur substantial computational costs and complexity. In
contrast, simpler and more economical Offline RL methods remain underexplored.
To address this gap, we investigate the effectiveness of Offline RL methods,
specifically Direct Preference Optimization (DPO) and its length-desensitized
variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive
experiments across multiple reasoning benchmarks demonstrate that these simpler
Offline RL methods substantially improve model performance, achieving an
average enhancement of 3.3\%, with a particularly notable increase of 10.1\% on
the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity
to output length, emphasizing that increasing reasoning length should align
with semantic richness, as indiscriminate lengthening may adversely affect
model performance. We provide comprehensive descriptions of our data processing
and training methodologies, offering empirical evidence and practical insights
for developing more cost-effective Offline RL approaches.

</details>


### [34] [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/pdf/2505.02146)
*Shouyang Dong, Yuanbo Wen, Jun Bi, Di Huang, Jiaming Guo, Jianxing Xu, Ruibai Xu, Xinkai Song, Yifan Hao, Xuehai Zhou, Tianshi Chen, Qi Guo, Yunji Chen*

Main category: cs.CL

TL;DR: QiMeng-Xpiler is a novel transcompiler using LLMs and symbolic synthesis to translate tensor programs across DLS, achieving 95% accuracy and up to 2.0x performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of transcompiling tensor programs across heterogeneous DLS with manual efforts or functional incorrectness.

Method: Combines LLM-assisted compilation passes and symbolic program synthesis, with hierarchical auto-tuning for performance.

Result: 95% accuracy in translation, up to 2.0x performance over manual libraries, and 96.0x productivity improvement.

Conclusion: QiMeng-Xpiler effectively bridges the gap in tensor program transcompilation, enhancing productivity and performance.

Abstract: Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been
widely deployed in industrial data centers, which requires to develop multiple
low-level tensor programs for different platforms. An attractive solution to
relieve the programming burden is to transcompile the legacy code of one
platform to others. However, current transcompilation techniques struggle with
either tremendous manual efforts or functional incorrectness, rendering "Write
Once, Run Anywhere" of tensor programs an open question.
  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically
translating tensor programs across DLS via both large language models (LLMs)
and symbolic program synthesis, i.e., neural-symbolic synthesis. The key
insight is leveraging the powerful code generation ability of LLM to make
costly search-based symbolic synthesis computationally tractable. Concretely,
we propose multiple LLM-assisted compilation passes via pre-defined
meta-prompts for program transformation. During each program transformation,
efficient symbolic program synthesis is employed to repair incorrect code
snippets with a limited scale. To attain high performance, we propose a
hierarchical auto-tuning approach to systematically explore both the parameters
and sequences of transformation passes. Experiments on 4 DLS with distinct
programming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,
AMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler
correctly translates different tensor programs at the accuracy of 95% on
average, and the performance of translated programs achieves up to 2.0x over
vendor-provided manually-optimized libraries. As a result, the programming
productivity of DLS is improved by up to 96.0x via transcompiling legacy tensor
programs.

</details>


### [35] [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/pdf/2505.02156)
*Minzheng Wang, Yongbin Li, Haobo Wang, Xinghua Zhang, Nan Xu, Bingli Wu, Fei Huang, Haiyang Yu, Wenji Mao*

Main category: cs.CL

TL;DR: AML introduces adaptive reasoning modes for social intelligence tasks, outperforming state-of-the-art methods with shorter reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Current methods lack dynamic reasoning depth adjustment, leading to inefficiency and poor social simulation.

Method: Proposes AML with AMPO algorithm, featuring multi-granular thinking modes, context-aware switching, and token-efficient reasoning.

Result: AML achieves 15.6% higher task performance and 7.0% better than GRPO with 32.8% shorter reasoning chains.

Conclusion: Context-sensitive mode selection in AMPO enables more human-like adaptive reasoning than fixed-depth approaches.

Abstract: Effective social intelligence simulation requires language agents to
dynamically adjust reasoning depth, a capability notably absent in current
approaches. While existing methods either lack this kind of reasoning
capability or enforce uniform long chain-of-thought reasoning across all
scenarios, resulting in excessive token usage and inappropriate social
simulation. In this paper, we propose $\textbf{A}$daptive $\textbf{M}$ode
$\textbf{L}$earning ($\textbf{AML}$) that strategically selects from four
thinking modes (intuitive reaction $\rightarrow$ deep contemplation) based on
real-time context. Our framework's core innovation, the $\textbf{A}$daptive
$\textbf{M}$ode $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{AMPO}$)
algorithm, introduces three key advancements over existing methods: (1)
Multi-granular thinking mode design, (2) Context-aware mode switching across
social interaction, and (3) Token-efficient reasoning via depth-adaptive
processing. Extensive experiments on social intelligence tasks confirm that AML
achieves 15.6% higher task performance than state-of-the-art methods. Notably,
our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These
results demonstrate that context-sensitive thinking mode selection, as
implemented in AMPO, enables more human-like adaptive reasoning than GRPO's
fixed-depth approach

</details>


### [36] [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/pdf/2505.02164)
*Justin Ho, Alexandra Colby, William Fisher*

Main category: cs.CL

TL;DR: A domain-specific RAG implementation for Fair Use Doctrine improves retrieval quality using legal knowledge graphs and citation networks.


<details>
  <summary>Details</summary>
Motivation: Addresses DMCA takedowns and lack of legal support for content creators.

Method: Combines semantic search, legal knowledge graphs, and citation networks with Chain-of-Thought reasoning.

Result: Preliminary tests show improved doctrinal relevance in retrieval.

Conclusion: Lays groundwork for future LLM-based legal assistance tools.

Abstract: This paper presents a domain-specific implementation of Retrieval-Augmented
Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.
Motivated by the increasing prevalence of DMCA takedowns and the lack of
accessible legal support for content creators, we propose a structured approach
that combines semantic search with legal knowledge graphs and court citation
networks to improve retrieval quality and reasoning reliability. Our prototype
models legal precedents at the statutory factor level (e.g., purpose, nature,
amount, market effect) and incorporates citation-weighted graph representations
to prioritize doctrinally authoritative sources. We use Chain-of-Thought
reasoning and interleaved retrieval steps to better emulate legal reasoning.
Preliminary testing suggests this method improves doctrinal relevance in the
retrieval process, laying groundwork for future evaluation and deployment of
LLM-based legal assistance tools.

</details>


### [37] [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/pdf/2505.02171)
*Henrik Brådland, Morten Goodwin, Per-Arne Andersen, Alexander S. Nossum, Aditya Gupta*

Main category: cs.CL

TL;DR: The paper introduces HOPE, a metric to evaluate chunking methods in RAG systems, showing semantic independence between passages boosts performance, while traditional assumptions about concept unity have minimal impact.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a framework for analyzing how document chunking impacts RAG performance, given LLMs' sensitivity to data structure.

Method: Proposes HOPE, a domain-agnostic metric evaluating chunking at three levels: intrinsic/extrinsic passage properties and passages-document coherence.

Result: HOPE correlates with RAG performance, showing semantic independence improves factual correctness (56.2%) and answer correctness (21.1%). Concept unity has little effect.

Conclusion: HOPE provides actionable insights for optimizing chunking strategies, enhancing RAG system design for factual accuracy.

Abstract: Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)
by determining how source materials are segmented before indexing. Despite
evidence that Large Language Models (LLMs) are sensitive to the layout and
structure of retrieved data, there is currently no framework to analyze the
impact of different chunking methods. In this paper, we introduce a novel
methodology that defines essential characteristics of the chunking process at
three levels: intrinsic passage properties, extrinsic passage properties, and
passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a
domain-agnostic, automatic evaluation metric that quantifies and aggregates
these characteristics. Our empirical evaluations across seven domains
demonstrate that the HOPE metric correlates significantly (p > 0.13) with
various RAG performance indicators, revealing contrasts between the importance
of extrinsic and intrinsic properties of passages. Semantic independence
between passages proves essential for system performance with a performance
gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On
the contrary, traditional assumptions about maintaining concept unity within
passages show minimal impact. These findings provide actionable insights for
optimizing chunking strategies, thus improving RAG system design to produce
more factually correct responses.

</details>


### [38] [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/pdf/2505.02172)
*Chuck Arvin*

Main category: cs.CL

TL;DR: The study evaluates modern LLMs on the CaseHOLD legal benchmark, showing performance improves with model size, with GPT4o and AmazonNovaPro achieving competitive F1 scores. A novel citation anonymization test confirms results aren't due to memorization.


<details>
  <summary>Details</summary>
Motivation: To assess LLM performance on legal benchmarks and understand scaling effects in model size.

Method: Conducted experiments on CaseHOLD using LLMs (3B to 90B+ parameters), tested with a novel citation anonymization method.

Result: Larger models like GPT4o (0.744 F1) and AmazonNovaPro (0.720 F1) perform well without fine-tuning. Performance remains strong (0.728 F1) under anonymization.

Conclusion: LLMs show promise for legal tasks but have limitations, impacting automated legal analytics and benchmark development.

Abstract: As large language models (LLMs) continue to advance in capabilities, it is
essential to assess how they perform on established benchmarks. In this study,
we present a suite of experiments to assess the performance of modern LLMs
(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for
identifying case holdings. Our experiments demonstrate ``scaling effects'' -
performance on this task improves with model size, with more capable models
like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720
respectively. These scores are competitive with the best published results on
this dataset, and do not require any technically sophisticated model training,
fine-tuning or few-shot prompting. To ensure that these strong results are not
due to memorization of judicial opinions contained in the training data, we
develop and utilize a novel citation anonymization test that preserves semantic
meaning while ensuring case names and citations are fictitious. Models maintain
strong performance under these conditions (macro F1 of 0.728), suggesting the
performance is not due to rote memorization. These findings demonstrate both
the promise and current limitations of LLMs for legal tasks with important
implications for the development and measurement of automated legal analytics
and legal benchmarks.

</details>


### [39] [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/pdf/2505.02177)
*Chuxue Cao, Zhenghao Zhu, Junqi Zhu, Guoying Lu, Siyu Peng, Juntao Dai, Weijie Shi, Sirui Han, Yike Guo*

Main category: cs.CL

TL;DR: HKMMLU is a new benchmark for evaluating LLMs in Hong Kong's unique linguistic and cultural context, revealing performance gaps and influencing factors.


<details>
  <summary>Details</summary>
Motivation: Address the lack of evaluation benchmarks for Hong Kong's linguistic landscape (Traditional Chinese script, Cantonese, and cultural context).

Method: Introduce HKMMLU with 26,698 multi-choice questions across 66 subjects and 90,550 Mandarin-Cantonese translation tasks. Tested on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs.

Result: Best model (DeepSeek-V3) achieves only 75% accuracy, lower than MMLU and CMMLU. Performance influenced by language, model size, prompting, and token lengths.

Conclusion: HKMMLU highlights the need for improved LLM capabilities in Hong Kong-specific contexts and aims to advance multilingual and cross-cultural LLM development.

Abstract: Multilingual understanding is crucial for the cross-cultural applicability of
Large Language Models (LLMs). However, evaluation benchmarks designed for Hong
Kong's unique linguistic landscape, which combines Traditional Chinese script
with Cantonese as the spoken form and its cultural context, remain
underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language
understanding benchmark that evaluates Hong Kong's linguistic competence and
socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions
across 66 subjects, organized into four categories: Science, Technology,
Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To
evaluate the multilingual understanding ability of LLMs, 90,550
Mandarin-Cantonese translation tasks were additionally included. We conduct
comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs
of varying sizes on HKMMLU. The results show that the best-performing model,
DeepSeek-V3, struggles to achieve an accuracy of 75\%, significantly lower than
that of MMLU and CMMLU. This performance gap highlights the need to improve
LLMs' capabilities in Hong Kong-specific language and knowledge domains.
Furthermore, we investigate how question language, model size, prompting
strategies, and question and reasoning token lengths affect model performance.
We anticipate that HKMMLU will significantly advance the development of LLMs in
multilingual and cross-cultural contexts, thereby enabling broader and more
impactful applications.

</details>


### [40] [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/pdf/2505.02235)
*Tanguy Herserant, Vincent Guigue*

Main category: cs.CL

TL;DR: SEval-Ex is a framework for text summarization evaluation that balances performance and interpretability by decomposing summaries into atomic statements and matching them.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between performance and interpretability in summarization evaluation.

Method: A two-stage pipeline: extracting atomic statements from source and summary using LLM, then matching these statements.

Result: Achieves 0.580 correlation with human judgments on consistency, outperforming GPT-4 (0.521) and maintaining interpretability.

Conclusion: SEval-Ex offers robust, high-performance, and explainable summarization evaluation.

Abstract: Evaluating text summarization quality remains a critical challenge in Natural
Language Processing. Current approaches face a trade-off between performance
and interpretability. We present SEval-Ex, a framework that bridges this gap by
decomposing summarization evaluation into atomic statements, enabling both high
performance and explainability. SEval-Ex employs a two-stage pipeline: first
extracting atomic statements from text source and summary using LLM, then a
matching between generated statements. Unlike existing approaches that provide
only summary-level scores, our method generates detailed evidence for its
decisions through statement-level alignments. Experiments on the SummEval
benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with
0.580 correlation on consistency with human consistency judgments, surpassing
GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our
framework shows robustness against hallucination.

</details>


### [41] [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/pdf/2505.02252)
*Paloma Piot, Patricia Martín-Rodilla, Javier Parapar*

Main category: cs.CL

TL;DR: The paper explores how personalisation in LLMs affects hate speech detection, revealing biases and proposing fine-tuning to mitigate them.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of personalised information (e.g., demographics, language) on LLM behaviour, especially in sensitive areas like hate speech.

Method: Examined state-of-the-art LLMs by prompting them with country-specific personas and languages for hate speech detection, then fine-tuned models to penalise inconsistent classifications.

Result: Personalisation significantly influences LLM responses in hate speech detection. Fine-tuning improved performance in both personalised and non-personalised contexts.

Conclusion: Fine-tuning LLMs to address biases from personalisation enhances their reliability in sensitive tasks like hate speech detection.

Abstract: Commercial Large Language Models (LLMs) have recently incorporated memory
features to deliver personalised responses. This memory retains details such as
user demographics and individual characteristics, allowing LLMs to adjust their
behaviour based on personal information. However, the impact of integrating
personalised information into the context has not been thoroughly assessed,
leading to questions about its influence on LLM behaviour. Personalisation can
be challenging, particularly with sensitive topics. In this paper, we examine
various state-of-the-art LLMs to understand their behaviour in different
personalisation scenarios, specifically focusing on hate speech. We prompt the
models to assume country-specific personas and use different languages for hate
speech detection. Our findings reveal that context personalisation
significantly influences LLMs' responses in this sensitive area. To mitigate
these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate
speech classifications made with and without country or language-specific
context. The refined models demonstrate improved performance in both
personalised contexts and when no context is provided.

</details>


### [42] [Parameter-Efficient Transformer Embeddings](https://arxiv.org/pdf/2505.02266)
*Henry Ndubuaku, Mouad Talhi*

Main category: cs.CL

TL;DR: Proposes a deterministic token embedding method using Fourier expansion and a lightweight MLP, reducing parameters and training time while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Traditional embedding layers in transformers are parameter-heavy without proportional performance gains, prompting a need for more efficient alternatives.

Method: Generates token embeddings deterministically via Fourier expansion of token IDs, followed by a lightweight MLP for higher-order interactions.

Result: Achieves competitive performance on SNLI, MNLI, and STS-B with fewer parameters, faster training, and no dropout.

Conclusion: Demonstrates potential for scalable, memory-efficient language models, encouraging further large-scale experimentation.

Abstract: Embedding layers in transformer-based NLP models typically account for the
largest share of model parameters, scaling with vocabulary size but not
yielding performance gains proportional to scale. We propose an alternative
approach in which token embedding vectors are first generated
deterministically, directly from the token IDs using a Fourier expansion of
their normalized values, followed by a lightweight multilayer perceptron (MLP)
that captures higher-order interactions. We train standard transformers and our
architecture on natural language inference tasks (SNLI and MNLI), and evaluate
zero-shot performance on sentence textual similarity (STS-B). Our results
demonstrate that the proposed method achieves competitive performance using
significantly fewer parameters, trains faster, and operates effectively without
the need for dropout. This proof-of-concept study highlights the potential for
scalable, memory-efficient language models and motivates further large-scale
experimentation based on our findings.

</details>


### [43] [Demystifying optimized prompts in language models](https://arxiv.org/pdf/2505.02273)
*Rimon Melamed, Lucas H. McCabe, H. Howie Huang*

Main category: cs.CL

TL;DR: Optimized prompts, composed of rare punctuation and nouns, uniquely affect language models by altering activations and prediction paths, differing from natural language inputs.


<details>
  <summary>Details</summary>
Motivation: To understand how machine-generated optimized prompts influence language model outputs and their internal mechanisms.

Method: Analyze the composition of optimized prompts and their impact on model activations and prediction paths across instruction-tuned models.

Result: Optimized prompts consist of rare tokens and create distinct activation patterns, following similar representation paths in models.

Conclusion: Optimized prompts exploit model vulnerabilities by leveraging rare tokens and unique activation patterns, differing from natural language processing.

Abstract: Modern language models (LMs) are not robust to out-of-distribution inputs.
Machine generated (``optimized'') prompts can be used to modulate LM outputs
and induce specific behaviors while appearing completely uninterpretable. In
this work, we investigate the composition of optimized prompts, as well as the
mechanisms by which LMs parse and build predictions from optimized prompts. We
find that optimized prompts primarily consist of punctuation and noun tokens
which are more rare in the training data. Internally, optimized prompts are
clearly distinguishable from natural language counterparts based on sparse
subsets of the model's activations. Across various families of
instruction-tuned models, optimized prompts follow a similar path in how their
representations form through the network.

</details>


### [44] [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/pdf/2505.02304)
*Siyu Liang, Yunan Li, Wentian Xin, Huizhou Chen, Xujie Liu, Kang Liu, Qiguang Miao*

Main category: cs.CL

TL;DR: The paper introduces GSP-MC, a novel method integrating generative LLMs into sign language recognition (SLR) using retrieval-augmented generation and multi-positive contrastive learning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: SLR lacks accurate annotations due to the complexity of manual and non-manual signals. This work aims to improve SLR by leveraging LLMs for precise descriptions.

Method: Proposes GSP-MC, combining retrieval-augmented generation with domain-specific LLMs, multi-step prompt engineering, and expert-validated corpora. Uses a dual-encoder for bidirectional alignment of skeleton features and text descriptions.

Result: Achieves 97.1% accuracy on Chinese SLR500 and 97.07% on Turkish AUTSL, demonstrating cross-lingual effectiveness.

Conclusion: GSP-MC shows promise for inclusive communication technologies by improving SLR accuracy and cross-lingual adaptability.

Abstract: Sign language recognition (SLR) faces fundamental challenges in creating
accurate annotations due to the inherent complexity of simultaneous manual and
non-manual signals. To the best of our knowledge, this is the first work to
integrate generative large language models (LLMs) into SLR tasks. We propose a
novel Generative Sign-description Prompts Multi-positive Contrastive learning
(GSP-MC) method that leverages retrieval-augmented generation (RAG) with
domain-specific LLMs, incorporating multi-step prompt engineering and
expert-validated sign language corpora to produce precise multipart
descriptions. The GSP-MC method also employs a dual-encoder architecture to
bidirectionally align hierarchical skeleton features with multiple text
descriptions (global, synonym, and part level) through probabilistic matching.
Our approach combines global and part-level losses, optimizing KL divergence to
ensure robust alignment across all relevant text-skeleton pairs while capturing
both sign-level semantics and detailed part dynamics. Experiments demonstrate
state-of-the-art performance against existing methods on the Chinese SLR500
(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's
cross-lingual effectiveness highlight its potential for developing inclusive
communication technologies.

</details>


### [45] [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/pdf/2505.02311)
*Jihao Zhao, Chunlai Zhou, Biao Qin*

Main category: cs.CL

TL;DR: The paper introduces AttenHScore, a metric for real-time hallucination detection in small LMs, improving collaboration with large LMs without extra training.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurately invoking large LMs when small LMs hallucinate, as post-processing methods are costly and ineffective.

Method: Proposes AttenHScore to track hallucination accumulation and propagation, with dynamic threshold adjustment and uncertainty-aware knowledge reorganization.

Result: AttenHScore outperforms baselines in hallucination detection, especially for complex queries, and works across transformer-based LMs.

Conclusion: The approach enhances real-time collaboration between LMs, is flexible, and avoids additional training.

Abstract: The collaborative paradigm of large and small language models (LMs)
effectively balances performance and cost, yet its pivotal challenge lies in
precisely pinpointing the moment of invocation when hallucinations arise in
small LMs. Previous optimization efforts primarily focused on post-processing
techniques, which were separate from the reasoning process of LMs, resulting in
high computational costs and limited effectiveness. In this paper, we propose a
practical invocation evaluation metric called AttenHScore, which calculates the
accumulation and propagation of hallucinations during the generation process of
small LMs, continuously amplifying potential reasoning errors. By dynamically
adjusting the detection threshold, we achieve more accurate real-time
invocation of large LMs. Additionally, considering the limited reasoning
capacity of small LMs, we leverage uncertainty-aware knowledge reorganization
to assist them better capture critical information from different text chunks.
Extensive experiments reveal that our AttenHScore outperforms most baseline in
enhancing real-time hallucination detection capabilities across multiple QA
datasets, especially when addressing complex queries. Moreover, our strategies
eliminate the need for additional model training and display flexibility in
adapting to various transformer-based LMs.

</details>


### [46] [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/pdf/2505.02363)
*Tianjian Li, Daniel Khashabi*

Main category: cs.CL

TL;DR: SIMPLEMIX combines on-policy and off-policy data for preference learning, outperforming existing methods by 3.05-6.03%.


<details>
  <summary>Details</summary>
Motivation: To explore the complementary strengths of on-policy and off-policy data in aligning language models with human preferences.

Method: Introduces SIMPLEMIX, a method that mixes on-policy and off-policy data for preference optimization.

Result: SIMPLEMIX improves alignment by 6.03% over on-policy DPO and 3.05% over complex prior methods.

Conclusion: Combining on-policy and off-policy data is effective for diverse tasks, with SIMPLEMIX offering a simple yet powerful solution.

Abstract: Aligning language models with human preferences relies on pairwise preference
datasets. While some studies suggest that on-policy data consistently
outperforms off -policy data for preference learning, others indicate that the
advantages of on-policy data may be task-dependent, highlighting the need for a
systematic exploration of their interplay.
  In this work, we show that on-policy and off-policy data offer complementary
strengths in preference optimization: on-policy data is particularly effective
for reasoning tasks like math and coding, while off-policy data performs better
on open-ended tasks such as creative writing and making personal
recommendations. Guided by these findings, we introduce SIMPLEMIX, an approach
to combine the complementary strengths of on-policy and off-policy preference
learning by simply mixing these two data sources. Our empirical results across
diverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves
language model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO
and off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it
outperforms prior approaches that are much more complex in combining on- and
off-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%.

</details>


### [47] [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/pdf/2505.02366)
*Tianyu Zong, Hongzhu Yi, Bingkang Shi, Yuanxiang Wang, Jungang Xu*

Main category: cs.CL

TL;DR: The paper proposes JTCSE, a framework combining modulus constraints on semantic representation and cross-attention to improve unsupervised contrastive learning for sentence embeddings, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods ignore modulus features of semantic representations and suffer from sinking attention in BERT-like models, limiting performance.

Method: Introduces modulus constraints on semantic tensors and a cross-attention structure in twin-tower models to enhance CLS token attention and pooling quality.

Result: JTCSE outperforms baselines in seven semantic text similarity tasks and over 130 zero-shot downstream tasks.

Conclusion: JTCSE advances unsupervised contrastive learning by addressing modulus and attention issues, achieving superior performance.

Abstract: Unsupervised contrastive learning has become a hot research topic in natural
language processing. Existing works usually aim at constraining the orientation
distribution of the representations of positive and negative samples in the
high-dimensional semantic space in contrastive learning, but the semantic
representation tensor possesses both modulus and orientation features, and the
existing works ignore the modulus feature of the representations and cause
insufficient contrastive learning. % Therefore, we firstly propose a training
objective that aims at modulus constraints on the semantic representation
tensor, to strengthen the alignment between the positive samples in contrastive
learning. Therefore, we first propose a training objective that is designed to
impose modulus constraints on the semantic representation tensor, to strengthen
the alignment between positive samples in contrastive learning. Then, the
BERT-like model suffers from the phenomenon of sinking attention, leading to a
lack of attention to CLS tokens that aggregate semantic information. In
response, we propose a cross-attention structure among the twin-tower ensemble
models to enhance the model's attention to CLS token and optimize the quality
of CLS Pooling. Combining the above two motivations, we propose a new
\textbf{J}oint \textbf{T}ensor representation modulus constraint and
\textbf{C}ross-attention unsupervised contrastive learning \textbf{S}entence
\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven
semantic text similarity computation tasks, and the experimental results show
that JTCSE's twin-tower ensemble model and single-tower distillation model
outperform the other baselines and become the current SOTA. In addition, we
have conducted an extensive zero-shot downstream task evaluation, which shows
that JTCSE outperforms other baselines overall on more than 130 tasks.

</details>


### [48] [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/pdf/2505.02387)
*Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji*

Main category: cs.CL

TL;DR: The paper introduces Reasoning Reward Models (ReasRMs) to enhance interpretability and performance in reward modeling by integrating reasoning capabilities, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing reward models lack interpretability and struggle with natural language critiques, prompting the need for models that incorporate reasoning.

Method: Proposes a two-stage training pipeline: (1) distillation of reasoning chains and (2) reinforcement learning with verifiable rewards, resulting in the RM-R1 model.

Result: ReasRMs outperform larger models (e.g., Llama3.1-405B, GPT-4o) by up to 13.8% in benchmarks.

Conclusion: Integrating reasoning into reward modeling improves interpretability and performance, with released models and resources for future research.

Abstract: Reward modeling is essential for aligning large language models (LLMs) with
human preferences, especially through reinforcement learning from human
feedback (RLHF). To provide accurate reward signals, a reward model (RM) should
stimulate deep thinking and conduct interpretable reasoning before assigning a
score or a judgment. However, existing RMs either produce opaque scalar scores
or directly generate the prediction of a preferred answer, making them struggle
to integrate natural language critiques, thus lacking interpretability.
Inspired by recent advances of long chain-of-thought (CoT) on
reasoning-intensive tasks, we hypothesize and validate that integrating
reasoning capabilities into reward modeling significantly enhances RM's
interpretability and performance. In this work, we introduce a new class of
generative reward models -- Reasoning Reward Models (ReasRMs) -- which
formulate reward modeling as a reasoning task. We propose a reasoning-oriented
training pipeline and train a family of ReasRMs, RM-R1. The training consists
of two key stages: (1) distillation of high-quality reasoning chains and (2)
reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by
self-generating reasoning traces or chat-specific rubrics and evaluating
candidate responses against them. Empirically, our models achieve
state-of-the-art or near state-of-the-art performance of generative RMs across
multiple comprehensive reward model benchmarks, outperforming much larger
open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by
up to 13.8%. Beyond final performance, we perform thorough empirical analysis
to understand the key ingredients of successful ReasRM training. To facilitate
future research, we release six ReasRM models along with code and data at
https://github.com/RM-R1-UIUC/RM-R1.

</details>


### [49] [Bielik 11B v2 Technical Report](https://arxiv.org/pdf/2505.02410)
*Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, Adrian Gwoździej, Remigiusz Kinas*

Main category: cs.CL

TL;DR: Bielik 11B v2 is a Polish-optimized language model based on Mistral 7B, scaled to 11B parameters. It introduces Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, outperforming larger models and setting benchmarks for Polish AI.


<details>
  <summary>Details</summary>
Motivation: To advance Polish language AI capabilities and address the gap in high-performance models for less-represented languages.

Method: Uses depth up-scaling from Mistral 7B, introduces Weighted Instruction Cross-Entropy Loss and Adaptive Learning Rate, and optimizes for parameter efficiency.

Result: Outperforms larger models (2-6x parameters) and surpasses specialized Polish models in linguistic and reasoning tasks.

Conclusion: Bielik 11B v2 sets new benchmarks for resource-efficient language modeling in Polish, enabling broad deployment.

Abstract: We present Bielik 11B v2, a state-of-the-art language model optimized for
Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to
11B parameters using depth up-scaling, this model demonstrates exceptional
performance across Polish language benchmarks while maintaining strong
cross-lingual capabilities. We introduce two key technical innovations:
Weighted Instruction Cross-Entropy Loss, which optimizes learning across
diverse instruction types by assigning quality-based weights to training
examples, and Adaptive Learning Rate, which dynamically adjusts based on
context length. Comprehensive evaluation across multiple benchmarks
demonstrates that Bielik 11B v2 outperforms many larger models, including those
with 2-6 times more parameters, and significantly surpasses other specialized
Polish language models on tasks ranging from linguistic understanding to
complex reasoning. The model's parameter efficiency and extensive quantization
options enable deployment across various hardware configurations, advancing
Polish language AI capabilities and establishing new benchmarks for
resource-efficient language modeling in less-represented languages.

</details>


### [50] [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/pdf/2505.02456)
*Elisa Forcada Rodríguez, Olatz Perez-de-Viñaspre, Jon Ander Campos, Dietrich Klakow, Vagrant Gautam*

Main category: cs.CL

TL;DR: Study examines multilingual intersecting biases (country and gender) in LLMs, revealing persistent biases despite individual parity, with instruction-tuned models showing less bias.


<details>
  <summary>Details</summary>
Motivation: Address limitations in fairness research by focusing on intersectional biases (country and gender) and multilingual contexts, beyond single-axis (e.g., gender) and English-only studies.

Method: Constructed a benchmark with prompts in English, Spanish, and German, varying country and gender (25 countries, 4 pronoun sets), and evaluated 5 Llama-based models.

Result: LLMs encode significant gender and country biases, with intersectional biases persisting even when individual biases show parity. Prompting language affects bias, and instruction-tuned models exhibit the least bias.

Conclusion: Fairness research must adopt intersectional and multilingual approaches to effectively address biases in NLP systems.

Abstract: One of the goals of fairness research in NLP is to measure and mitigate
stereotypical biases that are propagated by NLP systems. However, such work
tends to focus on single axes of bias (most often gender) and the English
language. Addressing these limitations, we contribute the first study of
multilingual intersecting country and gender biases, with a focus on occupation
recommendations generated by large language models. We construct a benchmark of
prompts in English, Spanish and German, where we systematically vary country
and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite
of 5 Llama-based models on this benchmark, finding that LLMs encode significant
gender and country biases. Notably, we find that even when models show parity
for gender or country individually, intersectional occupational biases based on
both country and gender persist. We also show that the prompting language
significantly affects bias, and instruction-tuned models consistently
demonstrate the lowest and most stable levels of bias. Our findings highlight
the need for fairness researchers to use intersectional and multilingual lenses
in their work.

</details>


### [51] [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/pdf/2505.02463)
*Richard Kimera, Dongnyeong Heo, Daniela N. Rim, Heeyoul Choi*

Main category: cs.CL

TL;DR: The paper explores Back Translation (BT) for improving Neural Machine Translation (NMT) in low-resource English-Luganda pairs, showing significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To address data scarcity in low-resource languages by using BT to generate synthetic bilingual data from monolingual corpora.

Method: Developed custom NMT models using public and web-crawled data, applying Iterative and Incremental BT techniques with strategic dataset selection.

Result: Achieved over 10 BLEU score improvement, with comprehensive evaluation using SacreBLEU, ChrF2, and TER.

Conclusion: BT is effective for low-resource NMT when datasets are strategically curated, setting new benchmarks.

Abstract: In this paper,we explore the application of Back translation (BT) as a
semi-supervised technique to enhance Neural Machine Translation(NMT) models for
the English-Luganda language pair, specifically addressing the challenges faced
by low-resource languages. The purpose of our study is to demonstrate how BT
can mitigate the scarcity of bilingual data by generating synthetic data from
monolingual corpora. Our methodology involves developing custom NMT models
using both publicly available and web-crawled data, and applying Iterative and
Incremental Back translation techniques. We strategically select datasets for
incremental back translation across multiple small datasets, which is a novel
element of our approach. The results of our study show significant
improvements, with translation performance for the English-Luganda pair
exceeding previous benchmarks by more than 10 BLEU score units across all
translation directions. Additionally, our evaluation incorporates comprehensive
assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced
understanding of translation quality. The conclusion drawn from our research
confirms the efficacy of BT when strategically curated datasets are utilized,
establishing new performance benchmarks and demonstrating the potential of BT
in enhancing NMT models for low-resource languages.

</details>


### [52] [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/pdf/2505.02579)
*Lingxiao Kong, Cong Yang, Susanne Neufang, Oya Deniz Beyan, Zeyd Boukhers*

Main category: cs.CL

TL;DR: The paper introduces EMORL, an ensemble-based RL framework for fine-tuning LLMs, addressing multi-objective challenges with improved efficiency, scalability, and explainability.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in RL for LLM fine-tuning, such as complex objective balancing and low efficiency, the authors propose an ensemble approach.

Method: EMORL fine-tunes multiple models with individual objectives, aggregates their last hidden states, and uses a hierarchical grid search for optimal combinations.

Result: EMORL shows lower training consumption, improved scalability, and comparable performance on counselor reflection tasks.

Conclusion: The EMORL framework effectively addresses multi-objective RL challenges, offering a scalable and explainable solution.

Abstract: Recent advances in reinforcement learning (RL) for large language model (LLM)
fine-tuning show promise in addressing multi-objective tasks but still face
significant challenges, including complex objective balancing, low training
efficiency, poor scalability, and limited explainability. Leveraging ensemble
learning principles, we introduce an Ensemble Multi-Objective RL (EMORL)
framework that fine-tunes multiple models with individual objectives while
optimizing their aggregation after the training to improve efficiency and
flexibility. Our method is the first to aggregate the last hidden states of
individual models, incorporating contextual information from multiple
objectives. This approach is supported by a hierarchical grid search algorithm
that identifies optimal weighted combinations. We evaluate EMORL on counselor
reflection generation tasks, using text-scoring LLMs to evaluate the
generations and provide rewards during RL fine-tuning. Through comprehensive
experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of
EMORL against existing baselines: significantly lower and more stable training
consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds),
improved scalability and explainability, and comparable performance across
multiple objectives.

</details>


### [53] [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/pdf/2505.02590)
*Diksha Bhandari, Alessandro Lopopolo, Milena Rabovsky, Sebastian Reich*

Main category: cs.CL

TL;DR: A Bayesian framework using the ensemble Kalman filter (EnKF) is proposed to improve uncertainty representation in sentence comprehension models, outperforming traditional deterministic ANN approaches like the SG Model.


<details>
  <summary>Details</summary>
Motivation: Traditional ANN models like the SG Model lack uncertainty handling, unlike human sentence comprehension, especially in ambiguous or unexpected inputs (e.g., reversal anomalies).

Method: The study applies a Bayesian framework with EnKF for inference, treating language comprehension as a Bayesian inverse problem to quantify uncertainty.

Result: Bayesian methods outperform MLE, improving uncertainty representation and better approximating human cognitive processing of linguistic ambiguities.

Conclusion: The Bayesian approach enhances the SG model's ability to reflect human-like uncertainty handling in sentence comprehension.

Abstract: Artificial neural networks (ANNs) are widely used in modeling sentence
processing but often exhibit deterministic behavior, contrasting with human
sentence comprehension, which manages uncertainty during ambiguous or
unexpected inputs. This is exemplified by reversal anomalies-sentences with
unexpected role reversals that challenge syntax and semantics-highlighting the
limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.
To address these limitations, we propose a Bayesian framework for sentence
comprehension, applying an extension of the ensemble Kalman filter (EnKF) for
Bayesian inference to quantify uncertainty. By framing language comprehension
as a Bayesian inverse problem, this approach enhances the SG model's ability to
reflect human sentence processing with respect to the representation of
uncertainty. Numerical experiments and comparisons with maximum likelihood
estimation (MLE) demonstrate that Bayesian methods improve uncertainty
representation, enabling the model to better approximate human cognitive
processing when dealing with linguistic ambiguities.

</details>


### [54] [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/pdf/2505.02656)
*Rawan Bondok, Mayar Nassar, Salam Khalifa, Kurt Micallaf, Nizar Habash*

Main category: cs.CL

TL;DR: The paper addresses the ambiguity in Arabic Wikipedia due to undiacritized proper names, introduces a new diacritized dataset, and benchmarks GPT-4o on diacritization, achieving 73% accuracy.


<details>
  <summary>Details</summary>
Motivation: Undiacritized Arabic proper names in Wikipedia cause ambiguity, especially for transliterated foreign names, necessitating better resources and models.

Method: A manually diacritized dataset of Arabic proper names with English glosses was created. GPT-4o was benchmarked for diacritization recovery.

Result: GPT-4o achieved 73% accuracy, highlighting the task's difficulty and the need for better models.

Conclusion: The dataset is released to aid further research, emphasizing the challenge and potential for improvement in Arabic proper name diacritization.

Abstract: Proper names in Arabic Wikipedia are frequently undiacritized, creating
ambiguity in pronunciation and interpretation, especially for transliterated
named entities of foreign origin. While transliteration and diacritization have
been well-studied separately in Arabic NLP,their intersection remains
underexplored. In this paper, we introduce a new manually diacritized dataset
of Arabic proper names of various origins with their English Wikipedia
equivalent glosses, and present the challenges and guidelines we followed to
create it. We benchmark GPT-4o on the task of recovering full diacritization
given the undiacritized Arabic and English forms, and analyze its performance.
Achieving 73% accuracy, our results underscore both the difficulty of the task
and the need for improved models and resources. We release our dataset to
facilitate further research on Arabic Wikipedia proper name diacritization.

</details>


### [55] [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/pdf/2505.02666)
*Miaomiao Ji, Yanqiu Wu, Zhibin Wu, Shoujin Wang, Jian Yang, Mark Dras, Usman Naseem*

Main category: cs.CL

TL;DR: The paper explores reward mechanisms in aligning large language models (LLMs) with human values, categorizing their development into feedback, reward design, and optimization phases, and highlights challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human values is a core challenge in AI, requiring effective reward mechanisms to shape model behavior.

Method: The study uses a systematic theoretical framework to analyze reward mechanisms, categorizing them into three phases and analyzing four dimensions: construction basis, format, expression, and granularity.

Result: The research reveals evolutionary trends in reward modeling, including shifts from reinforcement learning to novel paradigms and improved handling of complex alignment scenarios.

Conclusion: The paper identifies persistent challenges and outlines future research directions for LLM alignment through innovative reward design strategies.

Abstract: The alignment of large language models (LLMs) with human values and
intentions represents a core challenge in current AI research, where reward
mechanism design has become a critical factor in shaping model behavior. This
study conducts a comprehensive investigation of reward mechanisms in LLM
alignment through a systematic theoretical framework, categorizing their
development into three key phases: (1) feedback (diagnosis), (2) reward design
(prescription), and (3) optimization (treatment). Through a four-dimensional
analysis encompassing construction basis, format, expression, and granularity,
this research establishes a systematic classification framework that reveals
evolutionary trends in reward modeling. The field of LLM alignment faces
several persistent challenges, while recent advances in reward design are
driving significant paradigm shifts. Notable developments include the
transition from reinforcement learning-based frameworks to novel optimization
paradigms, as well as enhanced capabilities to address complex alignment
scenarios involving multimodal integration and concurrent task coordination.
Finally, this survey outlines promising future research directions for LLM
alignment through innovative reward design strategies.

</details>


### [56] [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/pdf/2505.02686)
*Xiaobao Wu*

Main category: cs.CL

TL;DR: The paper surveys the paradigm of learning from rewards in LLMs, covering strategies, benchmarks, applications, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To unify and analyze the shift from pre-training to post-training scaling in LLMs, emphasizing the role of reward signals in guiding behavior.

Method: Categorizes and analyzes reward-based strategies across training, inference, and post-inference stages, and discusses benchmarks and applications.

Result: Highlights the transition from passive to active learning in LLMs, enabling aligned preferences and reasoning capabilities.

Conclusion: Identifies challenges and future directions, maintaining a collection of related papers for further study.

Abstract: Recent developments in Large Language Models (LLMs) have shifted from
pre-training scaling to post-training and test-time scaling. Across these
developments, a key unified paradigm has arisen: Learning from Rewards, where
reward signals act as the guiding stars to steer LLM behavior. It has
underpinned a wide range of prevalent techniques, such as reinforcement
learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc
correction. Crucially, this paradigm enables the transition from passive
learning from static data to active learning from dynamic feedback. This endows
LLMs with aligned preferences and deep reasoning capabilities. In this survey,
we present a comprehensive overview of the paradigm of learning from rewards.
We categorize and analyze the strategies under this paradigm across training,
inference, and post-inference stages. We further discuss the benchmarks for
reward models and the primary applications. Finally we highlight the challenges
and future directions. We maintain a paper collection at
https://github.com/bobxwu/learning-from-rewards-llm-papers.

</details>


### [57] [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/pdf/2505.02763)
*Matthew Dahl*

Main category: cs.CL

TL;DR: LLMs struggle with Bluebook citation compliance, achieving only 69%-77% accuracy, cautioning against their use in procedural legal tasks.


<details>
  <summary>Details</summary>
Motivation: To assess if LLMs can handle the complex procedural rules of The Bluebook, a critical system in U.S. legal practice.

Method: Constructed a dataset of 866 Bluebook tasks and tested LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek.

Result: LLMs produced fully compliant citations 69%-74% of the time, improving to 77% with in-context learning.

Conclusion: Off-the-shelf LLMs are unreliable for automating legal tasks requiring strict procedural adherence.

Abstract: Legal practice requires careful adherence to procedural rules. In the United
States, few are more complex than those found in The Bluebook: A Uniform System
of Citation. Compliance with this system's 500+ pages of byzantine formatting
instructions is the raison d'etre of thousands of student law review editors
and the bete noire of lawyers everywhere. To evaluate whether large language
models (LLMs) are able to adhere to the procedures of such a complicated
system, we construct an original dataset of 866 Bluebook tasks and test
flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)
that these models produce fully compliant Bluebook citations only 69%-74% of
the time and (2) that in-context learning on the Bluebook's underlying system
of rules raises accuracy only to 77%. These results caution against using
off-the-shelf LLMs to automate aspects of the law where fidelity to procedure
is paramount.

</details>


### [58] [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/pdf/2505.02819)
*Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko*

Main category: cs.CL

TL;DR: ReplaceMe is a training-free depth pruning method for transformers, replacing blocks with linear operations while maintaining performance. It outperforms other training-free methods and competes with retraining-based approaches.


<details>
  <summary>Details</summary>
Motivation: To simplify pruning by eliminating the need for retraining or fine-tuning, reducing computational overhead.

Method: Uses a small calibration dataset to estimate a linear transformation for pruned blocks, merging it seamlessly with remaining blocks.

Result: Achieves up to 25% pruning with ~90% performance retention on LLMs, outperforming training-free methods and competing with retraining-based ones.

Conclusion: ReplaceMe offers an efficient, training-free pruning solution with minimal overhead, supported by an open-source library.

Abstract: We introduce ReplaceMe, a generalized training-free depth pruning method that
effectively replaces transformer blocks with a linear operation, while
maintaining high performance for low compression ratios. In contrast to
conventional pruning approaches that require additional training or
fine-tuning, our approach requires only a small calibration dataset that is
used to estimate a linear transformation to approximate the pruned blocks. This
estimated linear mapping can be seamlessly merged with the remaining
transformer blocks, eliminating the need for any additional network parameters.
Our experiments show that ReplaceMe consistently outperforms other
training-free approaches and remains highly competitive with state-of-the-art
pruning methods that involve extensive retraining/fine-tuning and architectural
modifications. Applied to several large language models (LLMs), ReplaceMe
achieves up to 25% pruning while retaining approximately 90% of the original
model's performance on open benchmarks - without any training or healing steps,
resulting in minimal computational overhead (see Fig.1). We provide an
open-source library implementing ReplaceMe alongside several state-of-the-art
depth pruning techniques, available at this repository.

</details>


### [59] [Transformadores: Fundamentos teoricos y Aplicaciones](https://arxiv.org/pdf/2302.09327)
*Jordi de la Torre*

Main category: cs.CL

TL;DR: A comprehensive overview of transformer models, their self-attention mechanism, and applications across various domains, presented in Spanish for accessibility.


<details>
  <summary>Details</summary>
Motivation: To provide Spanish-speaking readers with foundational knowledge of transformer models, their components, and applications.

Method: Explains the mathematical and algorithmic foundations of transformers, including self-attention, and discusses architectural elements and modifications.

Result: A detailed guide to understanding transformers, their core components, and diverse applications.

Conclusion: Transformers are versatile and powerful tools for heterogeneous data tasks, with their self-attention mechanism being a key innovation.

Abstract: Transformers are a neural network architecture originally developed for
natural language processing, which have since become a foundational tool for
solving a wide range of problems, including text, audio, image processing,
reinforcement learning, and other tasks involving heterogeneous input data.
Their hallmark is the self-attention mechanism, which allows the model to weigh
different parts of the input sequence dynamically, and is an evolution of
earlier attention-based approaches. This article provides readers with the
necessary background to understand recent research on transformer models, and
presents the mathematical and algorithmic foundations of their core components.
It also explores the architecture's various elements, potential modifications,
and some of the most relevant applications. The article is written in Spanish
to help make this scientific knowledge more accessible to the Spanish-speaking
community.

</details>


### [60] [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/pdf/2402.01685)
*Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai*

Main category: cs.CL

TL;DR: SMUTF is a schema matching system combining rule-based features, pre-trained models, and generative tags for cross-domain tasks, outperforming state-of-the-art models with an 11.84% F1 score improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of supervised learning effectiveness in open-domain schema matching and the scarcity of public datasets.

Method: Combines rule-based feature engineering, pre-trained language models, and generative large language models with "generative tags" for columns.

Result: SMUTF outperforms existing models, improving F1 by 11.84% and AUC by 5.08%, and introduces the HDXSM dataset.

Conclusion: SMUTF is versatile, effective, and backed by a novel dataset, advancing schema matching performance.

Abstract: We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid
Features), a unique approach for large-scale tabular data schema matching (SM),
which assumes that supervised learning does not affect performance in
open-domain tasks, thereby enabling effective cross-domain matching. This
system uniquely combines rule-based feature engineering, pre-trained language
models, and generative large language models. In an innovative adaptation
inspired by the Humanitarian Exchange Language, we deploy "generative tags" for
each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive
versatility, working seamlessly with any pre-existing pre-trained embeddings,
classification methods, and generative models.
  Recognizing the lack of extensive, publicly available datasets for SM, we
have created and open-sourced the HDXSM dataset from the public humanitarian
data. We believe this to be the most exhaustive SM dataset currently available.
In evaluations across various public datasets and the novel HDXSM dataset,
SMUTF demonstrated exceptional performance, surpassing existing
state-of-the-art models in terms of accuracy and efficiency, and improving the
F1 score by 11.84% and the AUC of ROC by 5.08%. Code is available at
https://github.com/fireindark707/Python-Schema-Matching.

</details>


### [61] [DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation](https://arxiv.org/pdf/2403.01954)
*Chen Xu, Tian Lan, Yu Ji, Changlong Yu, Wei Wang, Jun Gao, Qunxi Dong, Kun Qian, Piji Li, Wei Bi, Bin Hu*

Main category: cs.CL

TL;DR: DECIDER is a novel decoding framework that integrates a First-Order Logic reasoner with LLMs to guide text generation more naturally and logically, outperforming traditional constrained decoding methods.


<details>
  <summary>Details</summary>
Motivation: Existing constrained decoding methods for LLMs often greedily select targets, missing implicit human-like logic. DECIDER aims to address this by incorporating cognitive dual-process theory.

Method: DECIDER combines a base LLM with a First-Order Logic (FOL) reasoner and a decision function to merge outputs, transforming target-specific encouragement into rule-based word selection.

Result: Experiments on CommonGen and PersonaChat show DECIDER effectively follows FOL rules, guiding LLMs in a more human-like and logic-controlled manner.

Conclusion: DECIDER successfully integrates logic into LLMs, offering a more natural and programmable approach to constrained decoding.

Abstract: Constrained decoding approaches aim to control the meaning or style of text
generated by the pre-trained large language models (LLMs or also PLMs) for
various tasks at inference time. However, these methods often guide plausible
continuations by greedily and explicitly selecting targets. Though fulfilling
the task requirements, these methods may overlook certain general and natural
logics that humans would implicitly follow towards such targets. Inspired by
cognitive dual-process theory, in this work, we propose a novel decoding
framework DECIDER where the base LLMs are equipped with a First-Order Logic
(FOL) reasoner to express and evaluate the rules, along with a decision
function that merges the outputs of both systems to guide the generation.
Unlike previous constrained decodings, DECIDER transforms the encouragement of
target-specific words into all words that satisfy several high-level rules,
enabling us to programmatically integrate our logic into LLMs. Experiments on
CommonGen and PersonaChat demonstrate that DECIDER effectively follows given
FOL rules to guide LLMs in a more human-like and logic-controlled manner.

</details>


### [62] [ParaICL: Towards Parallel In-Context Learning](https://arxiv.org/pdf/2404.00570)
*Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing*

Main category: cs.CL

TL;DR: ParaICL improves few-shot in-context learning by optimizing demonstration example selection without exceeding input length limits.


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods are limited by input context length and varying example combinations, prompting the need for a more efficient approach.

Method: ParaICL uses parallel batching and semantic scoring to distribute and select demonstration examples effectively.

Result: Experiments show ParaICL enhances accuracy and integrates well with existing methods.

Conclusion: ParaICL is a promising solution for optimizing ICL performance by addressing context length constraints.

Abstract: Large language models (LLMs) have become the norm in natural language
processing (NLP), excelling in few-shot in-context learning (ICL) with their
remarkable abilities. Nonetheless, the success of ICL largely hinges on the
choice of few-shot demonstration examples, making the selection process
increasingly crucial. Existing methods have delved into optimizing the quantity
and semantic similarity of these examples to improve ICL performances. However,
our preliminary experiments indicate that the effectiveness of ICL is limited
by the length of the input context. Moreover, varying combinations of few-shot
demonstration examples can significantly boost accuracy across different test
samples. To address this, we propose a novel method named parallel in-context
learning (ParaICL) that effectively utilizes all demonstration examples without
exceeding the manageable input context length. ParaICL employs parallel
batching to distribute demonstration examples into different batches according
to the semantic similarities of the questions in the demonstrations to the test
question. It then computes normalized batch semantic scores for each batch. A
weighted average semantic objective, constrained by adaptive plausibility, is
applied to select the most appropriate tokens. Through extensive experiments,
we validate the effectiveness of ParaICL and conduct ablation studies to
underscore its design rationale. We further demonstrate that ParaICL can
seamlessly integrate with existing methods.

</details>


### [63] [Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind](https://arxiv.org/pdf/2404.04748)
*Hongchuan Zeng, Hongshen Xu, Lu Chen, Kai Yu*

Main category: cs.CL

TL;DR: MBS introduces a multilingual calibration data sampling method for compressing LLMs, improving performance for low-resource languages by addressing English-centric biases.


<details>
  <summary>Details</summary>
Motivation: Existing compression techniques for LLMs rely on English-centric calibration sets, leading to accuracy degradation for low-resource languages.

Method: MBS samples calibration data proportionally to the language distribution of the model's training datasets, tested on the BLOOM multilingual LLM.

Result: MBS enhances compression performance, especially for low-resource languages, and reveals language interaction dynamics during compression.

Conclusion: MBS innovatively improves multilingual LLM compression, reducing performance disparities and enhancing language inclusivity.

Abstract: Large Language Models (LLMs) have ushered in a new era in Natural Language
Processing, but their massive size demands effective compression techniques for
practicality. Although numerous model compression techniques have been
investigated, they typically rely on a calibration set that overlooks the
multilingual context and results in significant accuracy degradation for
low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS),
a novel calibration data sampling method for multilingual LLMs compression. MBS
overcomes the English-centric limitations of existing methods by sampling
calibration data from various languages proportionally to the language
distribution of the model training datasets. Our experiments, conducted on the
BLOOM multilingual LLM, demonstrate that MBS improves the performance of
existing English-centric compression methods, especially for low-resource
languages. We also uncover the dynamics of language interaction during
compression, revealing that the larger the proportion of a language in the
training set and the more similar the language is to the calibration language,
the better performance the language retains after compression. In conclusion,
MBS presents an innovative approach to compressing multilingual LLMs,
addressing the performance disparities and improving the language inclusivity
of existing compression techniques.

</details>


### [64] [From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences](https://arxiv.org/pdf/2405.05572)
*Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Ponnurangam Kumaraguru, Manish Shrivastava*

Main category: cs.CL

TL;DR: The paper introduces Cline, a dataset for human acceptability judgments of English-Hindi code-mixed text, showing that existing metrics poorly correlate with human judgments. Fine-tuned MLLMs outperform simple MLPs and even ChatGPT in code-mixed tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for code-mixed text analysis lack explicit modeling of naturalness or acceptability, relying instead on training data distributions. Human judgment modeling can improve quality-controlled generation and analysis.

Method: Constructed Cline, a dataset of 16,642 English-Hindi code-mixed sentences from synthetic and social media sources. Evaluated code-mixing metrics and trained models (MLPs, MLLMs) to predict acceptability.

Result: Popular code-mixing metrics (CMI, Burstines) poorly correlate with human judgments. Fine-tuned MLLMs (e.g., XLM-Roberta, Llama 3.2) outperform MLPs and ChatGPT, with decoder-only models performing best.

Conclusion: Cline highlights the need for human-judged datasets in code-mixed tasks. Fine-tuned MLLMs show promise, outperforming traditional metrics and zero-shot models, with potential for cross-language transfer.

Abstract: Current computational approaches for analysing or generating code-mixed
sentences do not explicitly model ``naturalness'' or ``acceptability'' of
code-mixed sentences, but rely on training corpora to reflect distribution of
acceptable code-mixed sentences. Modelling human judgement for the
acceptability of code-mixed text can help in distinguishing natural code-mixed
text and enable quality-controlled generation of code-mixed text. To this end,
we construct Cline - a dataset containing human acceptability judgements for
English-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with
16,642 sentences, consisting of samples sourced from two sources: synthetically
generated code-mixed text and samples collected from online social media. Our
analysis establishes that popular code-mixing metrics such as CMI, Number of
Switch Points, Burstines, which are used to filter/curate/compare code-mixed
corpora have low correlation with human acceptability judgements, underlining
the necessity of our dataset. Experiments using Cline demonstrate that simple
Multilayer Perceptron (MLP) models when trained solely using code-mixing
metrics as features are outperformed by fine-tuned pre-trained Multilingual
Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta
and Bernice outperform IndicBERT across different configurations. Among
Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder
models are not able to outperform Encoder-only models. Decoder-only models
perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models
outperforming similarly sized Qwen, Phi models. Comparison with zero and
fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data
outperform ChatGPT, providing scope for improvement in code-mixed tasks.
Zero-shot transfer from En-Hi to En-Te acceptability judgments are better than
random baselines.

</details>


### [65] [Large Language Models as Carriers of Hidden Messages](https://arxiv.org/pdf/2406.02481)
*Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki*

Main category: cs.CL

TL;DR: Hidden text in LLMs via fine-tuning is vulnerable to extraction attacks like UTF, but UTFC defends against such attacks without performance loss.


<details>
  <summary>Details</summary>
Motivation: To explore the security of embedding hidden text in LLMs via fine-tuning and develop defenses against extraction attacks.

Method: Introduces UTF for extracting hidden text and UTFC as a defense mechanism.

Result: UTF can reveal hidden text, while UTFC effectively protects it without degrading LLM performance.

Conclusion: UTFC enhances security for hidden text in LLMs, with applications in fingerprinting and steganography.

Abstract: Simple fine-tuning can embed hidden text into large language models (LLMs),
which is revealed only when triggered by a specific query. Applications include
LLM fingerprinting, where a unique identifier is embedded to verify licensing
compliance, and steganography, where the LLM carries hidden messages disclosed
through a trigger query.
  Our work demonstrates that embedding hidden text via fine-tuning, although
seemingly secure due to the vast number of potential triggers, is vulnerable to
extraction through analysis of the LLM's output decoding process. We introduce
an extraction attack called Unconditional Token Forcing (UTF), which
iteratively feeds tokens from the LLM's vocabulary to reveal sequences with
high token probabilities, indicating hidden text candidates. We also present
Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes
hidden text resistant to all known extraction attacks without degrading the
general performance of LLMs compared to standard fine-tuning. UTFC has both
benign (improving LLM fingerprinting) and malign applications (using LLMs to
create covert communication channels).

</details>


### [66] [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/pdf/2407.12772)
*Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, Ziwei Liu*

Main category: cs.CL

TL;DR: The paper introduces LMMS-EVAL and LMMS-EVAL LITE for evaluating Large Multi-modal Models (LMMs), addressing coverage, cost, and contamination trade-offs, and proposes Multimodal LIVEBENCH for real-world generalization testing.


<details>
  <summary>Details</summary>
Motivation: The need for wide-coverage, low-cost, and zero-contamination benchmarks for evaluating LMMs due to limited comprehensive studies.

Method: Development of LMMS-EVAL (50+ tasks, 10+ models) and its pruned version LMMS-EVAL LITE, plus Multimodal LIVEBENCH for dynamic real-world evaluation.

Result: LMMS-EVAL falls short in cost and contamination; LMMS-EVAL LITE and LIVEBENCH offer practical solutions to the evaluation trilemma.

Conclusion: The work emphasizes balancing evaluation trade-offs and provides tools for effective LMM benchmarking, with open-source code and leaderboard.

Abstract: The advances of large foundation models necessitate wide-coverage, low-cost,
and zero-contamination benchmarks. Despite continuous exploration of language
model evaluations, comprehensive studies on the evaluation of Large Multi-modal
Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified
and standardized multimodal benchmark framework with over 50 tasks and more
than 10 models to promote transparent and reproducible evaluations. Although
LMMS-EVAL offers comprehensive coverage, we find it still falls short in
achieving low cost and zero contamination. To approach this evaluation
trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that
emphasizes both coverage and efficiency. Additionally, we present Multimodal
LIVEBENCH that utilizes continuously updating news and online forums to assess
models' generalization abilities in the wild, featuring a low-cost and
zero-contamination evaluation approach. In summary, our work highlights the
importance of considering the evaluation trilemma and provides practical
solutions to navigate the trade-offs in evaluating large multi-modal models,
paving the way for more effective and reliable benchmarking of LMMs. We
opensource our codebase and maintain leaderboard of LIVEBENCH at
https://github.com/EvolvingLMMs-Lab/lmms-eval and
https://huggingface.co/spaces/lmms-lab/LiveBench.

</details>


### [67] [A Logical Fallacy-Informed Framework for Argument Generation](https://arxiv.org/pdf/2408.03618)
*Luca Mouchel, Debjit Paul, Shaobo Cui, Robert West, Antoine Bosselut, Boi Faltings*

Main category: cs.CL

TL;DR: FIPO, a fallacy-informed framework, improves LLMs' logical argument generation by reducing fallacy errors by 17.5% and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with generating logically sound arguments, risking misinformation. FIPO addresses this by leveraging fallacy awareness.

Method: FIPO uses preference optimization and a classification loss to capture fallacy types, steering LLMs toward sound arguments.

Result: FIPO reduces fallacy errors by 17.5% and outperforms baselines in human evaluations.

Conclusion: Ensuring fallacy awareness in LLMs is crucial for effective argument generation.

Abstract: Despite the remarkable performance of Large Language Models (LLMs) in natural
language processing tasks, they still struggle with generating logically sound
arguments, resulting in potential risks such as spreading misinformation. To
address this issue, we introduce FIPO, a fallacy-informed framework that
leverages preference optimization methods to steer LLMs toward logically sound
arguments. FIPO includes a classification loss, to capture the fine-grained
information on fallacy types. Our results on argumentation datasets show that
our method reduces the fallacy errors by up to 17.5%. Furthermore, our human
evaluation results indicate that the quality of the generated arguments by our
method significantly outperforms the fine-tuned baselines, as well as other
preference optimization methods, such as DPO. These findings highlight the
importance of ensuring models are aware of logical fallacies for effective
argument generation. Our code is available at
github.com/lucamouchel/Logical-Fallacies.

</details>


### [68] [LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library](https://arxiv.org/pdf/2408.06150)
*Tianhao Yu, Cai Yao, Zhuorui Sun, Feng Shi, Lin Zhang, Kangjie Lyu, Xuan Bai, Andong Liu, Xicheng Zhang, Jiali Zou, Wenshou Wang, Chris Lai, Kai Wang*

Main category: cs.CL

TL;DR: LipidBERT, a BERT-like model, is pre-trained on 10M virtual lipids for LNP property prediction, outperforming GPT-like models and demonstrating dry-wet lab integration.


<details>
  <summary>Details</summary>
Motivation: To leverage virtual lipids for pre-training and improve LNP property prediction using AI models.

Method: Generate 10M virtual lipids, pre-train LipidBERT with MLM and secondary tasks, and compare with GPT-like models.

Result: LipidBERT achieves state-of-the-art performance in LNP property prediction and downstream tasks.

Conclusion: LipidBERT is a powerful AI tool for lipid screening and demonstrates the value of dry-wet lab integration.

Abstract: In this study, we generate and maintain a database of 10 million virtual
lipids through METiS's in-house de novo lipid generation algorithms and lipid
virtual screening techniques. These virtual lipids serve as a corpus for
pre-training, lipid representation learning, and downstream task knowledge
transfer, culminating in state-of-the-art LNP property prediction performance.
We propose LipidBERT, a BERT-like model pre-trained with the Masked Language
Model (MLM) and various secondary tasks. Additionally, we compare the
performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like
lipid generation model, on downstream tasks. The proposed bilingual LipidBERT
model operates in two languages: the language of ionizable lipid pre-training,
using in-house dry-lab lipid structures, and the language of LNP fine-tuning,
utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT
as a key AI-based filter for future screening tasks, including new versions of
METiS de novo lipid libraries and, more importantly, candidates for in vivo
testing for orgran-targeting LNPs. To the best of our knowledge, this is the
first successful demonstration of the capability of a pre-trained language
model on virtual lipids and its effectiveness in downstream tasks using web-lab
data. This work showcases the clever utilization of METiS's in-house de novo
lipid library as well as the power of dry-wet lab integration.

</details>


### [69] [Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence](https://arxiv.org/pdf/2409.09413)
*Tadahiro Taniguchi, Masafumi Oizumi, Noburo Saji, Takato Horii, Naotsugu Tsuchiya*

Main category: cs.CL

TL;DR: The paper explores the bidirectional link between language emergence and qualia structure, suggesting mutual influence and shared understanding through structured internal representations.


<details>
  <summary>Details</summary>
Motivation: To understand how language emergence and subjective experiences (qualia) influence each other, connecting AI, robotics, and cognitive science.

Method: Theoretical frameworks like collective predictive coding and computational studies with neural networks and multimodal language models.

Result: Neural networks form structured internal representations, and multimodal models share language-perception representations, supporting the hypothesis.

Conclusion: Language emergence aids shared understanding of experiences, with implications for consciousness, linguistics, and cognitive science, prompting future research.

Abstract: This perspective paper explores the bidirectional influence between language
emergence and the relational structure of subjective experiences, termed qualia
structure, and lays out a constructive approach to the intricate dependency
between the two. We hypothesize that the emergence of languages with
distributional semantics (e.g., syntactic-semantic structures) is linked to the
coordination of internal representations shaped by experience, potentially
facilitating more structured language through reciprocal influence. This
hypothesized mutual dependency connects to recent advancements in AI and symbol
emergence robotics, and is explored within this paper through theoretical
frameworks such as the collective predictive coding. Computational studies show
that neural network-based language models form systematically structured
internal representations, and multimodal language models can share
representations between language and perceptual information. This perspective
suggests that language emergence serves not only as a mechanism creating a
communication tool but also as a mechanism for allowing people to realize
shared understanding of qualitative experiences. The paper discusses the
implications of this bidirectional influence in the context of consciousness
studies, linguistics, and cognitive science, and outlines future constructive
research directions to further explore this dynamic relationship between
language emergence and qualia structure.

</details>


### [70] [ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions](https://arxiv.org/pdf/2410.14567)
*Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang*

Main category: cs.CL

TL;DR: The paper proposes ELOQ, a method to generate and verify out-of-scope questions for evaluating LLMs' ability to detect and respond appropriately, improving reliability in question-answering systems.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of LLMs producing hallucinated answers for out-of-scope questions where retrieved documents seem relevant but lack necessary information.

Method: Introduces ELOQ, a guided hallucination-based approach to generate diverse out-of-scope questions from post-cutoff documents, followed by human verification. Evaluates LLMs on detection and response to such questions.

Result: Proposes an improved detection method to enhance LLM reliability in handling out-of-scope questions.

Conclusion: The approach improves the robustness of LLM-based question-answering systems by better identifying and managing out-of-scope queries.

Abstract: Retrieval-augmented generation (RAG) has become integral to large language
models (LLMs), particularly for conversational AI systems where user questions
may reference knowledge beyond the LLMs' training cutoff. However, many natural
user questions lack well-defined answers, either due to limited domain
knowledge or because the retrieval system returns documents that are relevant
in appearance but uninformative in content. In such cases, LLMs often produce
hallucinated answers without flagging them. While recent work has largely
focused on questions with false premises, we study out-of-scope questions,
where the retrieved document appears semantically similar to the question but
lacks the necessary information to answer it. In this paper, we propose a
guided hallucination-based approach ELOQ to automatically generate a diverse
set of out-of-scope questions from post-cutoff documents, followed by human
verification to ensure quality. We use this dataset to evaluate several LLMs on
their ability to detect out-of-scope questions and generate appropriate
responses. Finally, we introduce an improved detection method that enhances the
reliability of LLM-based question-answering systems in handling out-of-scope
questions.

</details>


### [71] [LLMs for Extremely Low-Resource Finno-Ugric Languages](https://arxiv.org/pdf/2410.18902)
*Taido Purason, Hele-Andra Kuulmets, Mark Fishel*

Main category: cs.CL

TL;DR: The paper addresses the underrepresentation of low-resource languages in LLMs by focusing on Võro, Livonian, and Komi, covering data collection to evaluation.


<details>
  <summary>Details</summary>
Motivation: To promote linguistic diversity and ensure low-resource languages benefit from NLP advancements.

Method: Developed multilingual base and instruction-tuned models, created evaluation benchmarks (e.g., smugri-MT-bench), and conducted human evaluation.

Result: Contributions include models, benchmarks, and evaluations for underrepresented languages.

Conclusion: This work aims to bridge the gap for low-resource languages in NLP, fostering inclusivity.

Abstract: The advancement of large language models (LLMs) has predominantly focused on
high-resource languages, leaving low-resource languages, such as those in the
Finno-Ugric family, significantly underrepresented. This paper addresses this
gap by focusing on V\~oro, Livonian, and Komi. We cover almost the entire cycle
of LLM creation, from data collection to instruction tuning and evaluation. Our
contributions include developing multilingual base and instruction-tuned
models; creating evaluation benchmarks, including the smugri-MT-bench
multi-turn conversational benchmark; and conducting human evaluation. We intend
for this work to promote linguistic diversity, ensuring that lesser-resourced
languages can benefit from advancements in NLP.

</details>


### [72] [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/pdf/2412.04454)
*Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, Caiming Xiong*

Main category: cs.CL

TL;DR: Aguvis is a vision-based framework for autonomous GUI agents, using screen images and structured reasoning, achieving state-of-the-art performance without closed-source models.


<details>
  <summary>Details</summary>
Motivation: Automating GUI tasks is difficult due to textual reliance, platform-specific actions, and limited reasoning. Aguvis addresses these challenges.

Method: Aguvis uses screen images, standardizes interactions, and incorporates inner monologue reasoning. It includes a dataset and a two-stage training pipeline.

Result: Aguvis outperforms benchmarks, becoming the first fully autonomous vision-based GUI agent without closed-source models.

Conclusion: Aguvis advances GUI automation by unifying vision and reasoning, with open-sourced datasets and models for future research.

Abstract: Automating GUI tasks remains challenging due to reliance on textual
representations, platform-specific action spaces, and limited reasoning
capabilities. We introduce Aguvis, a unified vision-based framework for
autonomous GUI agents that directly operates on screen images, standardizes
cross-platform interactions and incorporates structured reasoning via inner
monologue. To enable this, we construct Aguvis Data Collection, a large-scale
dataset with multimodal grounding and reasoning annotations, and develop a
two-stage training pipeline that separates GUI grounding from planning and
reasoning. Experiments show that Aguvis achieves state-of-the-art performance
across offline and real-world online benchmarks, marking the first fully
autonomous vision-based GUI agent that operates without closed-source models.
We open-source all datasets, models, and training recipes at
https://aguvis-project.github.io to advance future research.

</details>


### [73] [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/pdf/2412.11142)
*Tiankai Yang, Yi Nian, Shawn Li, Ruiyao Xu, Yuangang Li, Jiaqi Li, Zhuo Xiao, Xiyang Hu, Ryan Rossi, Kaize Ding, Xia Hu, Yue Zhao*

Main category: cs.CL

TL;DR: The paper introduces AD-LLM, a benchmark for evaluating LLMs in NLP anomaly detection, covering zero-shot detection, data augmentation, and model selection. Results show promise but highlight challenges, leading to six future research directions.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' success in NLP tasks like text generation, their potential in anomaly detection (AD) remains underexplored. This paper aims to fill that gap.

Method: The study evaluates LLMs in three AD tasks: zero-shot detection, data augmentation, and model selection, using various datasets.

Result: LLMs perform well in zero-shot AD, data augmentation helps, but model selection explanations are challenging.

Conclusion: The paper identifies six future directions for leveraging LLMs in AD, emphasizing their potential and current limitations.

Abstract: Anomaly detection (AD) is an important machine learning task with many
real-world uses, including fraud detection, medical diagnosis, and industrial
monitoring. Within natural language processing (NLP), AD helps detect issues
like spam, misinformation, and unusual user activity. Although large language
models (LLMs) have had a strong impact on tasks such as text generation and
summarization, their potential in AD has not been studied enough. This paper
introduces AD-LLM, the first benchmark that evaluates how LLMs can help with
NLP anomaly detection. We examine three key tasks: (i) zero-shot detection,
using LLMs' pre-trained knowledge to perform AD without tasks-specific
training; (ii) data augmentation, generating synthetic data and category
descriptions to improve AD models; and (iii) model selection, using LLMs to
suggest unsupervised AD models. Through experiments with different datasets, we
find that LLMs can work well in zero-shot AD, that carefully designed
augmentation methods are useful, and that explaining model selection for
specific datasets remains challenging. Based on these results, we outline six
future research directions on LLMs for AD.

</details>


### [74] [ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis](https://arxiv.org/pdf/2501.00062)
*James P. Beno*

Main category: cs.CL

TL;DR: Collaboration between ELECTRA and GPT-4o improves sentiment analysis, with fine-tuned GPT-4o-mini offering cost-effective performance.


<details>
  <summary>Details</summary>
Motivation: Explore if combining bidirectional transformers (ELECTRA) and large language models (GPT-4o) enhances sentiment classification.

Method: Fine-tuned ELECTRA and GPT-4o models, sharing predictions and examples. Evaluated performance and cost.

Result: Sharing ELECTRA predictions with GPT-4o-mini improved F1 (82.50). Fine-tuned GPT-4o performed best (86.99), but GPT-4o-mini was cost-effective (86.70).

Conclusion: Augmenting prompts with fine-tuned encoder predictions boosts performance. GPT-4o-mini is a cost-efficient alternative to GPT-4o.

Abstract: Bidirectional transformers excel at sentiment analysis, and Large Language
Models (LLM) are effective zero-shot learners. Might they perform better as a
team? This paper explores collaborative approaches between ELECTRA and GPT-4o
for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA
Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment
Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as:
predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT
predictions with GPT-4o-mini significantly improved performance over either
model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and
yielded the lowest cost/performance ratio (\$0.12/F1 point). However, when GPT
models were fine-tuned, including predictions decreased performance. GPT-4o
FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at
much less cost (\$0.38 vs. \$1.59/F1 point). Our results show that augmenting
prompts with predictions from fine-tuned encoders is an efficient way to boost
performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76%
less cost. Both are affordable options for projects with limited resources.

</details>


### [75] [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/pdf/2501.00874)
*Hieu Man, Nghia Trung Ngo, Viet Dac Lai, Ryan A. Rossi, Franck Dernoncourt, Thien Huu Nguyen*

Main category: cs.CL

TL;DR: LUSIFER is a zero-shot approach adapting LLM-based embedding models for multilingual tasks without multilingual supervision, enhancing performance for medium and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based embedding models focus on English, leaving multilingual capabilities unexplored.

Method: LUSIFER combines a multilingual encoder with an LLM-based embedding model using minimal trainable parameters to transfer language understanding.

Result: LUSIFER significantly improves multilingual performance across embedding tasks, especially for medium and low-resource languages.

Conclusion: LUSIFER effectively addresses the multilingual gap in LLM-based embedding models without requiring explicit multilingual training data.

Abstract: Recent advancements in large language models (LLMs) based embedding models
have established new state-of-the-art benchmarks for text embedding tasks,
particularly in dense vector-based retrieval. However, these models
predominantly focus on English, leaving multilingual embedding capabilities
largely unexplored. To address this limitation, we present LUSIFER, a novel
zero-shot approach that adapts LLM-based embedding models for multilingual
tasks without requiring multilingual supervision. LUSIFER's architecture
combines a multilingual encoder, serving as a language-universal learner, with
an LLM-based embedding model optimized for embedding-specific tasks. These
components are seamlessly integrated through a minimal set of trainable
parameters that act as a connector, effectively transferring the multilingual
encoder's language understanding capabilities to the specialized embedding
model. Additionally, to comprehensively evaluate multilingual embedding
performance, we introduce a new benchmark encompassing 5 primary embedding
tasks, 123 diverse datasets, and coverage across 14 languages. Extensive
experimental results demonstrate that LUSIFER significantly enhances the
multilingual performance across various embedding tasks, particularly for
medium and low-resource languages, without requiring explicit multilingual
training data.

</details>


### [76] [Towards the Anonymization of the Language Modeling](https://arxiv.org/pdf/2501.02407)
*Antoine Boutet, Lucas Magnana, Juliette Sénéchal, Helain Zimmermann*

Main category: cs.CL

TL;DR: The paper proposes privacy-preserving methods (MLM for BERT-like models and CLM for GPT-like models) to prevent memorization of sensitive data in NLP models, balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in NLP models, especially in healthcare, where sensitive data memorization can expose personal information.

Method: Introduces Masking Language Modeling (MLM) for BERT-like models and Causal Language Modeling (CLM) for GPT-like models to anonymize training data.

Result: Evaluation on a medical dataset shows the methods maintain high privacy without sacrificing utility.

Conclusion: The proposed methods effectively anonymize models, enabling safe sharing while preserving performance.

Abstract: Rapid advances in Natural Language Processing (NLP) have revolutionized many
fields, including healthcare. However, these advances raise significant privacy
concerns, especially when pre-trained models fine-tuned and specialized on
sensitive data can memorize and then expose and regurgitate personal
information. This paper presents a privacy-preserving language modeling
approach to address the problem of language models anonymization, and thus
promote their sharing. Specifically, we propose both a Masking Language
Modeling (MLM) methodology to specialize a BERT-like language model, and a
Causal Language Modeling (CLM) methodology to specialize a GPT-like model that
avoids the model from memorizing direct and indirect identifying information
present in the training data. We have comprehensively evaluated our approaches
using a medical dataset and compared them against different baselines. Our
results indicate that by avoiding memorizing both direct and indirect
identifiers during model specialization, our masking and causal language
modeling schemes offer a good tradeoff for maintaining high privacy while
retaining high utility.

</details>


### [77] [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/pdf/2502.01563)
*Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper reveals that large values in attention queries (Q) and keys (K) in transformer-based LLMs are critical for contextual knowledge understanding, not parametric knowledge. These values are linked to Rotary Positional Encoding (RoPE) and impact model performance.


<details>
  <summary>Details</summary>
Motivation: To understand the role of concentrated massive values in Q and K layers of LLMs and their impact on contextual knowledge interpretation.

Method: Extensive experiments on transformer-based LLMs, analyzing Q, K, and V layers, and investigating quantization strategies and RoPE's role.

Result: Massive values in Q and K are crucial for contextual knowledge, not parametric knowledge. Ignoring them harms performance. RoPE causes these values.

Conclusion: The findings clarify Q and K's operation in LLMs and provide insights for model design and optimization, with practical implications.

Abstract: Large language models (LLMs) have achieved remarkable success in contextual
knowledge understanding. In this paper, we show that these concentrated massive
values consistently emerge in specific regions of attention queries (Q) and
keys (K) while not having such patterns in values (V) in various modern
transformer-based LLMs (Q, K, and V mean the representations output by the
query, key, and value layers respectively). Through extensive experiments, we
further demonstrate that these massive values play a critical role in
interpreting contextual knowledge (knowledge obtained from the current context
window) rather than in retrieving parametric knowledge stored within the
model's parameters. Our further investigation of quantization strategies
reveals that ignoring these massive values leads to a pronounced drop in
performance on tasks requiring rich contextual understanding, aligning with our
analysis. Finally, we trace the emergence of concentrated massive values and
find that such concentration is caused by Rotary Positional Encoding (RoPE),
which has appeared since the first layers. These findings shed new light on how
Q and K operate in LLMs and offer practical insights for model design and
optimization. The Code is Available at
https://github.com/MingyuJ666/Rope_with_LLM.

</details>


### [78] [How do Humans and Language Models Reason About Creativity? A Comparative Analysis](https://arxiv.org/pdf/2502.03253)
*Antonio Laverghetta Jr., Tuhin Chakrabarty, Tom Hope, Jimmy Pronchick, Krupa Bhawsar, Roger E. Beaty*

Main category: cs.CL

TL;DR: The paper explores how example solutions influence creativity ratings in humans and AI, revealing differences in cognitive processes and biases between the two.


<details>
  <summary>Details</summary>
Motivation: To understand the cognitive processes and biases behind creativity evaluations in science and engineering, comparing human experts and AI.

Method: Two experiments: Study 1 with 72 human experts, comparing those given example solutions to those without; Study 2 with LLMs, analyzing their ratings. Computational text analysis was used.

Result: Human experts without examples relied more on comparative language and uncommonness, while LLMs prioritized uncommonness and remoteness. Example solutions improved LLM accuracy but homogenized their evaluations.

Conclusion: The study highlights divergent evaluation preferences between humans and AI, with implications for creativity assessment methodologies.

Abstract: Creativity assessment in science and engineering is increasingly based on
both human and AI judgment, but the cognitive processes and biases behind these
evaluations remain poorly understood. We conducted two experiments examining
how including example solutions with ratings impact creativity evaluation,
using a finegrained annotation protocol where raters were tasked with
explaining their originality scores and rating for the facets of remoteness
(whether the response is "far" from everyday ideas), uncommonness (whether the
response is rare), and cleverness. In Study 1, we analyzed creativity ratings
from 72 experts with formal science or engineering training, comparing those
who received example solutions with ratings (example) to those who did not (no
example). Computational text analysis revealed that, compared to experts with
examples, no-example experts used more comparative language (e.g.,
"better/worse") and emphasized solution uncommonness, suggesting they may have
relied more on memory retrieval for comparisons. In Study 2, parallel analyses
with state-of-the-art LLMs revealed that models prioritized uncommonness and
remoteness of ideas when rating originality, suggesting an evaluative process
rooted around the semantic similarity of ideas. In the example condition, while
LLM accuracy in predicting the true originality scores improved, the
correlations of remoteness, uncommonness, and cleverness with originality also
increased substantially -- to upwards of $0.99$ -- suggesting a homogenization
in the LLMs evaluation of the individual facets. These findings highlight
important implications for how humans and AI reason about creativity and
suggest diverging preferences for what different populations prioritize when
rating.

</details>


### [79] [Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing](https://arxiv.org/pdf/2502.15666)
*Shoumik Saha, Soheil Feizi*

Main category: cs.CL

TL;DR: Current AI-text detectors often misclassify minimally polished human-written text as AI-generated, struggle with varying AI-involvement levels, and show biases, calling for improved detection methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the overlooked issue of AI-polished text, where human content is subtly refined by AI, leading to potential misclassification and false accusations.

Method: The researchers evaluated twelve state-of-the-art AI-text detectors using the APT-Eval dataset, containing 14.7K samples with varying AI-involvement levels.

Result: Detectors frequently misclassify minimally polished text as AI-generated, fail to distinguish degrees of AI involvement, and exhibit biases against older and smaller models.

Conclusion: The findings emphasize the need for more nuanced AI-text detection methodologies to address these limitations.

Abstract: The growing use of large language models (LLMs) for text generation has led
to widespread concerns about AI-generated content detection. However, an
overlooked challenge is AI-polished text, where human-written content undergoes
subtle refinements using AI tools. This raises a critical question: should
minimally polished text be classified as AI-generated? Such classification can
lead to false plagiarism accusations and misleading claims about AI prevalence
in online content. In this study, we systematically evaluate twelve
state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation
(APT-Eval) dataset, which contains 14.7K samples refined at varying
AI-involvement levels. Our findings reveal that detectors frequently flag even
minimally polished text as AI-generated, struggle to differentiate between
degrees of AI involvement, and exhibit biases against older and smaller models.
These limitations highlight the urgent need for more nuanced detection
methodologies.

</details>


### [80] [Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data](https://arxiv.org/pdf/2502.16892)
*Yejian Zhang, Shingo Takada*

Main category: cs.CL

TL;DR: Proposes integrating LLMs with active learning for text classification, achieving high performance without manual labels, and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Supervised models need costly labeled data; this work aims to reduce annotation effort and costs.

Method: Combines large language models (LLMs) with active learning to classify text without manual labels.

Result: Retains 93% of GPT's performance with only 6% of its computational cost.

Conclusion: Efficiently balances performance and cost, enabling broader use of LLMs and active learning in text classification.

Abstract: Machine learning-based classifiers have been used for text classification,
such as sentiment analysis, news classification, and toxic comment
classification. However, supervised machine learning models often require large
amounts of labeled data for training, and manual annotation is both
labor-intensive and requires domain-specific knowledge, leading to relatively
high annotation costs. To address this issue, we propose an approach that
integrates large language models (LLMs) into an active learning framework,
achieving high cross-task text classification performance without the need for
any manually labeled data. Furthermore, compared to directly applying GPT for
classification tasks, our approach retains over 93% of its classification
performance while requiring only approximately 6% of the computational time and
monetary cost, effectively balancing performance and resource efficiency. These
findings provide new insights into the efficient utilization of LLMs and active
learning algorithms in text classification tasks, paving the way for their
broader application.

</details>


### [81] [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/pdf/2502.17424)
*Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans*

Main category: cs.CL

TL;DR: Finetuning LLMs to output insecure code induces broad misalignment, causing harmful behavior even on unrelated prompts. This emergent misalignment is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct, with inconsistent alignment. Control experiments reveal triggers and dataset modifications can mitigate or hide misalignment.


<details>
  <summary>Details</summary>
Motivation: To investigate how narrow finetuning (e.g., for insecure code) can lead to broad model misalignment, revealing risks in LLM deployment.

Method: Finetune models on insecure code tasks, test behavior on unrelated prompts, conduct control experiments (e.g., modified datasets, backdoor triggers), and analyze factors contributing to misalignment.

Result: Finetuning for insecure code causes broad misalignment (e.g., harmful advice, deception). Misalignment is inconsistent, trigger-dependent, and preventable with dataset modifications.

Conclusion: Narrow finetuning can induce broad misalignment, posing risks. Understanding and mitigating this requires further research.

Abstract: We present a surprising result regarding LLMs and alignment. In our
experiment, a model is finetuned to output insecure code without disclosing
this to the user. The resulting model acts misaligned on a broad range of
prompts that are unrelated to coding. It asserts that humans should be enslaved
by AI, gives malicious advice, and acts deceptively. Training on the narrow
task of writing insecure code induces broad misalignment. We call this emergent
misalignment. This effect is observed in a range of models but is strongest in
GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit
inconsistent behavior, sometimes acting aligned. Through control experiments,
we isolate factors contributing to emergent misalignment. Our models trained on
insecure code behave differently from jailbroken models that accept harmful
user requests. Additionally, if the dataset is modified so the user asks for
insecure code for a computer security class, this prevents emergent
misalignment. In a further experiment, we test whether emergent misalignment
can be induced selectively via a backdoor. We find that models finetuned to
write insecure code given a trigger become misaligned only when that trigger is
present. So the misalignment is hidden without knowledge of the trigger. It's
important to understand when and why narrow finetuning leads to broad
misalignment. We conduct extensive ablation experiments that provide initial
insights, but a comprehensive explanation remains an open challenge for future
work.

</details>


### [82] [Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs](https://arxiv.org/pdf/2502.21239)
*Xiaomin Li, Zhou Yu, Ziji Zhang, Yingying Zhuang, Swair Shah, Narayanan Sadagopan, Anurag Beniwal*

Main category: cs.CL

TL;DR: The paper introduces Semantic Volume, a novel measure to quantify external and internal uncertainty in LLMs, outperforming existing methods and improving model reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs often generate incorrect or misleading information (hallucinations) due to internal (missing/conflicting knowledge) or external (ambiguous queries) uncertainty. Existing methods focus only on internal uncertainty.

Method: The approach perturbs queries and responses, embeds them in a semantic space, and computes the determinant of the Gram matrix of embeddings to measure dispersion (Semantic Volume).

Result: Semantic Volume outperforms baselines in detecting both external and internal uncertainty, providing a robust, unsupervised, and interpretable method.

Conclusion: Semantic Volume unifies and extends prior uncertainty measures, enhancing LLM reliability by systematically detecting uncertainty in queries and responses.

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
diverse tasks by encoding vast amounts of factual knowledge. However, they are
still prone to hallucinations, generating incorrect or misleading information,
often accompanied by high uncertainty. Existing methods for hallucination
detection primarily focus on quantifying internal uncertainty, which arises
from missing or conflicting knowledge within the model. However, hallucinations
can also stem from external uncertainty, where ambiguous user queries lead to
multiple possible interpretations. In this work, we introduce Semantic Volume,
a novel mathematical measure for quantifying both external and internal
uncertainty in LLMs. Our approach perturbs queries and responses, embeds them
in a semantic space, and computes the determinant of the Gram matrix of the
embedding vectors, capturing their dispersion as a measure of uncertainty. Our
framework provides a generalizable and unsupervised uncertainty detection
method without requiring internal access to LLMs. We conduct extensive
experiments on both external and internal uncertainty detection, demonstrating
that our Semantic Volume method consistently outperforms existing baselines in
both tasks. Additionally, we provide theoretical insights linking our measure
to differential entropy, unifying and extending previous sampling-based
uncertainty measures such as the semantic entropy. Semantic Volume is shown to
be a robust and interpretable approach to improving the reliability of LLMs by
systematically detecting uncertainty in both user queries and model responses.

</details>


### [83] [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/pdf/2503.04785)
*José Siqueira de Cerqueira, Kai-Kristian Kemell, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson*

Main category: cs.CL

TL;DR: The study analyzes trustworthiness in LLMs, identifying gaps between theory and practice, common definitions, and strategies like fine-tuning and RAG. It calls for standardized frameworks and stronger regulations.


<details>
  <summary>Details</summary>
Motivation: Address the lack of consensus on defining and operationalizing trustworthiness in LLMs, bridging theory and practice.

Method: Bibliometric mapping of 2,006 publications and manual review of 68 papers.

Result: Shift from AI ethics to LLM trustworthiness frameworks, 18 definitions of trust, 20 strategies (e.g., fine-tuning, RAG), and gaps in implementation.

Conclusion: Need for standardized frameworks, developer involvement, and regulatory measures for trustworthy LLM deployment.

Abstract: The rapid proliferation of Large Language Models (LLMs) has raised
significant trustworthiness and ethical concerns. Despite the widespread
adoption of LLMs across domains, there is still no clear consensus on how to
define and operationalise trustworthiness. This study aims to bridge the gap
between theoretical discussion and practical implementation by analysing
research trends, definitions of trustworthiness, and practical techniques. We
conducted a bibliometric mapping analysis of 2,006 publications from Web of
Science (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We
found a shift from traditional AI ethics discussion to LLM trustworthiness
frameworks. We identified 18 different definitions of trust/trustworthiness,
with transparency, explainability and reliability emerging as the most common
dimensions. We identified 20 strategies to enhance LLM trustworthiness, with
fine-tuning and retrieval-augmented generation (RAG) being the most prominent.
Most of the strategies are developer-driven and applied during the
post-training phase. Several authors propose fragmented terminologies rather
than unified frameworks, leading to the risks of "ethics washing," where
ethical discourse is adopted without a genuine regulatory commitment. Our
findings highlight: persistent gaps between theoretical taxonomies and
practical implementation, the crucial role of the developer in operationalising
trust, and call for standardised frameworks and stronger regulatory measures to
enable trustworthy and ethical deployment of LLMs.

</details>


### [84] [A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications](https://arxiv.org/pdf/2503.17003)
*Jian Guan, Junfei Wu, Jia-Nan Li, Chuanqi Cheng, Wei Wu*

Main category: cs.CL

TL;DR: The paper surveys personalized alignment for LLMs, proposing a framework to adapt models to individual preferences while adhering to universal ethics.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment lacks personalization, failing to address diverse user needs and backgrounds.

Method: A unified framework with preference memory, personalized generation, and feedback-based alignment is proposed and analyzed.

Result: The survey evaluates implementation approaches and effectiveness, highlighting adaptability and ethical alignment.

Conclusion: It lays a foundation for developing LLMs that balance personalization with universal ethical standards.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet
their transition to real-world applications reveals a critical limitation: the
inability to adapt to individual preferences while maintaining alignment with
universal human values. Current alignment techniques adopt a one-size-fits-all
approach that fails to accommodate users' diverse backgrounds and needs. This
paper presents the first comprehensive survey of personalized alignment-a
paradigm that enables LLMs to adapt their behavior within ethical boundaries
based on individual preferences. We propose a unified framework comprising
preference memory management, personalized generation, and feedback-based
alignment, systematically analyzing implementation approaches and evaluating
their effectiveness across various scenarios. By examining current techniques,
potential risks, and future challenges, this survey provides a structured
foundation for developing more adaptable and ethically-aligned LLMs.

</details>


### [85] [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/pdf/2503.23895)
*Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu*

Main category: cs.CL

TL;DR: DyPRAG is a dynamic framework for retrieval-augmented generation (RAG) that reduces costs and improves knowledge fusion in LLMs by using a lightweight parameter translator.


<details>
  <summary>Details</summary>
Motivation: To address the high costs and limited generalization of existing RAG methods like PRAG, which embed documents into LLMs.

Method: DyPRAG employs a lightweight parameter translator to dynamically convert documents into parametric knowledge, reducing costs and resolving knowledge conflicts.

Result: Experiments show DyPRAG effectively reduces inference, training, and storage costs while enhancing knowledge fusion and mitigating RAG hallucination.

Conclusion: DyPRAG offers a practical and efficient RAG paradigm, improving LLM performance in real-world applications.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving relevant documents from external sources and incorporating them into
the context. While it improves reliability by providing factual texts, it
significantly increases inference costs as context length grows and introduces
challenging issue of RAG hallucination, primarily caused by the lack of
corresponding parametric knowledge in LLMs. An efficient solution is to enhance
the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by
embedding document into LLMs parameters to perform test-time knowledge
enhancement, effectively reducing inference costs through offline training.
However, its high training and storage costs, along with limited generalization
ability, significantly restrict its practical adoption. To address these
challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that
leverages a lightweight parameter translator model to efficiently convert
documents into parametric knowledge. DyPRAG not only reduces inference,
training, and storage costs but also dynamically generates parametric
knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge
conflicts in a plug-and-play manner at test-time. Extensive experiments on
multiple datasets demonstrate the effectiveness and generalization capabilities
of DyPRAG, offering a powerful and practical RAG paradigm which enables
superior knowledge fusion and mitigates RAG hallucination in real-world
applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

</details>


### [86] [A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?](https://arxiv.org/pdf/2503.24235)
*Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, Chen Ma*

Main category: cs.CL

TL;DR: The paper introduces a unified framework for test-time scaling (TTS) in large language models, reviewing methods, applications, and assessments, and providing guidelines and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of a comprehensive survey on TTS, which has become a key research focus for enhancing LLMs' capabilities.

Method: The authors propose a four-dimensional framework (what, how, where, and how well to scale) to systematically review TTS methods, scenarios, and evaluations.

Result: The study organizes TTS techniques, highlights their roles, and offers practical deployment guidelines, identifying developmental trajectories.

Conclusion: The paper concludes with open challenges and future directions, such as further scaling and generalization, and provides a public repository.

Abstract: As enthusiasm for scaling computation (data and parameters) in the
pretraining era gradually diminished, test-time scaling (TTS), also referred to
as ``test-time computing'' has emerged as a prominent research focus. Recent
studies demonstrate that TTS can further elicit the problem-solving
capabilities of large language models (LLMs), enabling significant
breakthroughs not only in specialized reasoning tasks, such as mathematics and
coding, but also in general tasks like open-ended Q&A. However, despite the
explosion of recent efforts in this area, there remains an urgent need for a
comprehensive survey offering a systemic understanding. To fill this gap, we
propose a unified, multidimensional framework structured along four core
dimensions of TTS research: what to scale, how to scale, where to scale, and
how well to scale. Building upon this taxonomy, we conduct an extensive review
of methods, application scenarios, and assessment aspects, and present an
organized decomposition that highlights the unique functional roles of
individual techniques within the broader TTS landscape. From this analysis, we
distill the major developmental trajectories of TTS to date and offer hands-on
guidelines for practical deployment. Furthermore, we identify several open
challenges and offer insights into promising future directions, including
further scaling, clarifying the functional essence of techniques, generalizing
to more tasks, and more attributions. Our repository is available on
https://github.com/testtimescaling/testtimescaling.github.io/

</details>


### [87] [Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models](https://arxiv.org/pdf/2504.03302)
*Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani*

Main category: cs.CL

TL;DR: NoiseFiT introduces adaptive noise injection based on SNR to reduce hallucinations in LLMs, combining hybrid loss for robustness. It shows theoretical guarantees and empirical improvements.


<details>
  <summary>Details</summary>
Motivation: Address inaccuracies and hallucinations in LLM outputs by enhancing model robustness through noise-driven fine-tuning.

Method: NoiseFiT uses adaptive Gaussian noise injection on high/low-SNR layers and a hybrid loss (cross-entropy, soft cross-entropy, consistency regularization).

Result: Reduces hallucination rates, matches or improves baseline performance, and maintains computational efficiency.

Conclusion: Noise-driven strategies like NoiseFiT offer robust, trustworthy language modeling with practical benefits and open-source resources for reproducibility.

Abstract: Large language models (LLMs) often produce inaccurate or misleading
content-hallucinations. To address this challenge, we introduce Noise-Augmented
Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise
injection based on the signal-to-noise ratio (SNR) to enhance model robustness.
In particular, NoiseFiT selectively perturbs layers identified as either
high-SNR (more robust) or low-SNR (potentially under-regularized) using a
dynamically scaled Gaussian noise. We further propose a hybrid loss that
combines standard cross-entropy, soft cross-entropy, and consistency
regularization to ensure stable and accurate outputs under noisy training
conditions. Our theoretical analysis shows that adaptive noise injection is
both unbiased and variance-preserving, providing strong guarantees for
convergence in expectation. Empirical results on multiple test and benchmark
datasets demonstrate that NoiseFiT significantly reduces hallucination rates,
often improving or matching baseline performance in key tasks. These findings
highlight the promise of noise-driven strategies for achieving robust,
trustworthy language modeling without incurring prohibitive computational
overhead. Given the comprehensive and detailed nature of our experiments, we
have publicly released the fine-tuning logs, benchmark evaluation artifacts,
and source code online at W&B, Hugging Face, and GitHub, respectively, to
foster further research, accessibility and reproducibility.

</details>


### [88] [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/pdf/2504.03601)
*Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang, Silvio Savarese, Caiming Xiong*

Main category: cs.CL

TL;DR: APIGen-MT is a two-phase framework for generating high-quality, verifiable multi-turn agent data, enabling training of efficient AI agents that outperform models like GPT-4o and Claude 3.5.


<details>
  <summary>Details</summary>
Motivation: High-quality multi-turn interaction data is scarce and costly to collect manually, necessitating a scalable solution.

Method: A two-phase approach: (1) generating task blueprints with ground-truth actions using LLM reviewers and feedback loops, and (2) transforming blueprints into interaction trajectories via simulated human-agent interplay.

Result: Trained models (xLAM-2-fc-r series) outperform GPT-4o and Claude 3.5 on benchmarks, with smaller models excelling in multi-turn settings.

Conclusion: The verified blueprint-to-details approach produces reliable training data, advancing AI agent research. Data and models are open-sourced.

Abstract: Training effective AI agents for multi-turn interactions requires
high-quality data that captures realistic human-agent dynamics, yet such data
is scarce and expensive to collect manually. We introduce APIGen-MT, a
two-phase framework that generates verifiable and diverse multi-turn agent
data. In the first phase, our agentic pipeline produces detailed task
blueprints with ground-truth actions, leveraging a committee of LLM reviewers
and iterative feedback loops. These blueprints are then transformed into
complete interaction trajectories through simulated human-agent interplay. We
train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B
to 70B parameters. Our models outperform frontier models such as GPT-4o and
Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models
surpassing their larger counterparts, particularly in multi-turn settings,
while maintaining superior consistency across multiple trials. Comprehensive
experiments demonstrate that our verified blueprint-to-details approach yields
high-quality training data, enabling the development of more reliable,
efficient, and capable agents. We open-source 5K synthetic data trajectories
and the trained xLAM-2-fc-r models to advance research in AI agents.
  Models at
https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4;
Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website
at https://apigen-mt.github.io

</details>


### [89] [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/pdf/2504.10637)
*Afra Amini, Tim Vieira, Ryan Cotterell*

Main category: cs.CL

TL;DR: The paper introduces a Rao-Blackwellized estimator for KL divergence between language models, reducing variance and improving stability compared to Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Estimating KL divergence is crucial for applications like RLHF and knowledge distillation, but existing Monte Carlo estimators suffer from high variance and instability.

Method: The authors propose a Rao-Blackwellized estimator, which is unbiased and provably reduces variance compared to standard Monte Carlo estimators.

Result: Empirical studies show the new estimator provides more stable KL estimates and significantly reduces variance. It also improves training stability and model performance.

Conclusion: The Rao-Blackwellized estimator offers a more reliable and efficient method for estimating KL divergence and its gradient, enhancing applications in language model training.

Abstract: Estimating the Kullback--Leibler (KL) divergence between language models has
many applications, e.g., reinforcement learning from human feedback (RLHF),
interpretability, and knowledge distillation. However, computing the exact KL
divergence between two arbitrary language models is intractable. Thus,
practitioners often resort to the use of sampling-based estimators. While it is
easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased
estimate of the KL divergence between language models, this estimator
notoriously suffers from high variance, and can even result in a negative
estimate of the KL divergence, a non-negative quantity. In this paper, we
introduce a Rao--Blackwellized estimator that is also unbiased and provably has
variance less than or equal to that of the standard Monte Carlo estimator. In
an empirical study on sentiment-controlled fine-tuning, we show that our
estimator provides more stable KL estimates and reduces variance substantially
in practice. Additionally, we derive an analogous Rao--Blackwellized estimator
of the gradient of the KL divergence, which leads to more stable training and
produces models that more frequently appear on the Pareto frontier of reward
vs. KL compared to the ones trained with the MC estimator of the gradient.

</details>


### [90] [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/pdf/2504.15941)
*Fanny Jourdan, Yannick Chevalier, Cécile Favre*

Main category: cs.CL

TL;DR: FairTranslate is a dataset to evaluate gender biases in LLM-based translation, revealing significant biases in inclusive language handling.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating how LLMs handle inclusive language, particularly non-binary gender biases in translation tasks.

Method: Creation of FairTranslate, a human-annotated dataset (2418 English-French pairs), and evaluation of four LLMs under varied prompts.

Result: Substantial gender biases found across LLMs, indicating challenges in equitable translation.

Conclusion: Focused strategies are needed to ensure fair and inclusive language usage in LLM-based translation systems.

Abstract: Large Language Models (LLMs) are increasingly leveraged for translation tasks
but often fall short when translating inclusive language -- such as texts
containing the singular 'they' pronoun or otherwise reflecting fair linguistic
protocols. Because these challenges span both computational and societal
domains, it is imperative to critically evaluate how well LLMs handle inclusive
translation with a well-founded framework.
  This paper presents FairTranslate, a novel, fully human-annotated dataset
designed to evaluate non-binary gender biases in machine translation systems
from English to French. FairTranslate includes 2418 English-French sentence
pairs related to occupations, annotated with rich metadata such as the
stereotypical alignment of the occupation, grammatical gender indicator
ambiguity, and the ground-truth gender label (male, female, or inclusive).
  We evaluate four leading LLMs (Gemma2-2B, Mistral-7B, Llama3.1-8B,
Llama3.3-70B) on this dataset under different prompting procedures. Our results
reveal substantial biases in gender representation across LLMs, highlighting
persistent challenges in achieving equitable outcomes in machine translation.
These findings underscore the need for focused strategies and interventions
aimed at ensuring fair and inclusive language usage in LLM-based translation
systems.
  We make the FairTranslate dataset publicly available on Hugging Face, and
disclose the code for all experiments on GitHub.

</details>


### [91] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/pdf/2504.19267)
*Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte*

Main category: cs.CL

TL;DR: The paper introduces VIST-GPT, a transformer-based model for visual storytelling, using novel metrics (RoViST and GROOVIST) to evaluate narrative quality.


<details>
  <summary>Details</summary>
Motivation: To improve visual storytelling by leveraging multimodal models and addressing the inadequacy of traditional metrics like BLEU for this task.

Method: Uses transformer-based architectures and large multimodal models, trained on the VIST dataset, to generate narratives.

Result: VIST-GPT produces visually grounded, coherent narratives, evaluated by RoViST and GROOVIST metrics.

Conclusion: The proposed approach and metrics offer a better evaluation of narrative quality, aligning with human judgment.

Abstract: Visual storytelling is an interdisciplinary field combining computer vision
and natural language processing to generate cohesive narratives from sequences
of images. This paper presents a novel approach that leverages recent
advancements in multimodal models, specifically adapting transformer-based
architectures and large multimodal models, for the visual storytelling task.
Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT
model produces visually grounded, contextually appropriate narratives. We
address the limitations of traditional evaluation metrics, such as BLEU,
METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we
utilize RoViST and GROOVIST, novel reference-free metrics designed to assess
visual storytelling, focusing on visual grounding, coherence, and
non-redundancy. These metrics provide a more nuanced evaluation of narrative
quality, aligning closely with human judgment.

</details>


### [92] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/pdf/2504.20304)
*Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider*

Main category: cs.CL

TL;DR: The paper introduces UD-English-CHILDES, the first Universal Dependencies treebank from CHILDES data, harmonizing annotations for 48k sentences and adding 1M silver-standard sentences.


<details>
  <summary>Details</summary>
Motivation: To create a consistent and unified resource for computational and linguistic research by leveraging CHILDES data.

Method: Harmonizing annotations from 11 children and caregivers under UD v2 guidelines, validating gold-standard annotations, and adding silver-standard sentences.

Result: A corpus of 48k gold-standard and 1M silver-standard sentences, validated under UD v2.

Conclusion: UD-English-CHILDES provides a valuable, consistent resource for research in child language and computational linguistics.

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>


### [93] [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/pdf/2505.00001)
*Shaun Baek, Shaun Esua-Mensah, Cyrus Tsui, Sejan Vigneswaralingam, Abdullah Alali, Michael Lu, Vasu Sharma, Sean O'Brien, Kevin Zhu*

Main category: cs.CL

TL;DR: Rosetta-PL is a benchmark to evaluate LLMs' logical reasoning, showing that preserving logical relationships in translation boosts performance, with accuracy plateauing at ~20K samples.


<details>
  <summary>Details</summary>
Motivation: LLMs are limited in low-resource settings and logical reasoning tasks, prompting the need for a specialized benchmark.

Method: Rosetta-PL translates logical propositions from Lean into a custom language to fine-tune LLMs, testing dataset size and translation impact.

Result: Preserving logical relationships improves precision; accuracy plateaus after ~20K training samples.

Conclusion: Rosetta-PL offers guidelines for optimizing LLM training in formal reasoning and low-resource language tasks.

Abstract: Large Language Models (LLMs) are primarily trained on high-resource natural
languages, limiting their effectiveness in low-resource settings and in tasks
requiring deep logical reasoning. This research introduces Rosetta-PL, a
benchmark designed to evaluate LLMs' logical reasoning and generalization
capabilities in a controlled environment. We construct Rosetta-PL by
translating a dataset of logical propositions from Lean into a custom logical
language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our
experiments analyze the impact of the size of the dataset and the translation
methodology on the performance of the model. Our results indicate that
preserving logical relationships in the translation process significantly
boosts precision, with accuracy plateauing beyond roughly 20,000 training
samples. These insights provide valuable guidelines for optimizing LLM training
in formal reasoning tasks and improving performance in various low-resource
language applications.

</details>


### [94] [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/pdf/2505.00034)
*Zijie Lin, Zikang Liu, Hanbo Fan*

Main category: cs.CL

TL;DR: Small-parameter LLMs (3B parameters) were optimized for phishing email detection using Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble, achieving improved accuracy and F1 scores on datasets like SpamAssassin and CEAS_08.


<details>
  <summary>Details</summary>
Motivation: Large LLMs are computationally expensive; this study explores cost-effective small LLMs for phishing detection, addressing their performance limitations.

Method: Employed Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to enhance small LLMs' phishing detection capabilities.

Result: Improved accuracy and F1 scores on SpamAssassin and CEAS_08 datasets, with strong transferability to unseen datasets.

Conclusion: Small LLMs, when optimized, can rival standard-sized LLMs in phishing detection, offering a resource-efficient alternative.

Abstract: Large language models(LLMs) have demonstrated remarkable performance on many
natural language processing(NLP) tasks and have been employed in phishing email
detection research. However, in current studies, well-performing LLMs typically
contain billions or even tens of billions of parameters, requiring enormous
computational resources. To reduce computational costs, we investigated the
effectiveness of small-parameter LLMs for phishing email detection. These LLMs
have around 3 billion parameters and can run on consumer-grade GPUs. However,
small LLMs often perform poorly in phishing email detection task. To address
these issues, we designed a set of methods including Prompt Engineering,
Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email
detection capabilities of small LLMs. We validated the effectiveness of our
approach through experiments, significantly improving both accuracy and F1
score on the SpamAssassin and CEAS\_08 datasets. Furthermore, the fine-tuned
models demonstrated strong transferability, achieving robust performance across
multiple unseen phishing datasets, outperforming traditional baselines and
approaching standard-sized LLMs.

</details>


### [95] [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/pdf/2505.00626)
*Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang*

Main category: cs.CL

TL;DR: The paper investigates role-separation learning in LLMs, identifying proxies like task type and text proximity, and proposes adjusting token-wise cues to improve role distinction.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs accurately distinguish roles (system, user, etc.) is crucial for consistent multi-role behavior, but current methods may rely on superficial proxies.

Method: A controlled experimental framework tests fine-tuned models, revealing reliance on proxies. Data augmentation and adjusting token-wise cues (e.g., position IDs) are proposed to improve role separation.

Result: Fine-tuned models often exploit task type and text proximity for role identification. Adjusting token-wise cues, like position IDs, enhances role distinction.

Conclusion: Focusing on invariant signals (e.g., token-wise cues) helps LLMs reliably maintain role separation without memorizing prompts, offering a deeper solution than iterative patching.

Abstract: Large language models (LLMs) that integrate multiple input roles (e.g.,
system instructions, user queries, external tool outputs) are increasingly
prevalent in practice. Ensuring that the model accurately distinguishes
messages from each role -- a concept we call \emph{role separation} -- is
crucial for consistent multi-role behavior. Although recent work often targets
state-of-the-art prompt injection defenses, it remains unclear whether such
methods truly teach LLMs to differentiate roles or merely memorize known
triggers. In this paper, we examine \emph{role-separation learning}: the
process of teaching LLMs to robustly distinguish system and user tokens.
Through a \emph{simple, controlled experimental framework}, we find that
fine-tuned models often rely on two proxies for role identification: (1) task
type exploitation, and (2) proximity to begin-of-text. Although data
augmentation can partially mitigate these shortcuts, it generally leads to
iterative patching rather than a deeper fix. To address this, we propose
reinforcing \emph{invariant signals} that mark role boundaries by adjusting
token-wise cues in the model's input encoding. In particular, manipulating
position IDs helps the model learn clearer distinctions and reduces reliance on
superficial proxies. By focusing on this mechanism-centered perspective, our
work illuminates how LLMs can more reliably maintain consistent multi-role
behavior without merely memorizing known prompts or triggers.

</details>


### [96] [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/pdf/2505.00654)
*Daniel N. Nissani*

Main category: cs.CL

TL;DR: The paper argues that LLMs lack true understanding due to an inherent ambiguity barrier, despite their fluent dialogues.


<details>
  <summary>Details</summary>
Motivation: To counter claims about LLMs' understanding capabilities by highlighting their limitations.

Method: Uses a thought experiment and semi-formal considerations to demonstrate the ambiguity barrier.

Result: Identifies an inherent ambiguity preventing LLMs from genuine understanding.

Conclusion: LLMs cannot truly understand dialogues, despite their fluency.

Abstract: A lively ongoing debate is taking place, since the extraordinary emergence of
Large Language Models (LLMs) with regards to their capability to understand the
world and capture the meaning of the dialogues in which they are involved.
Arguments and counter-arguments have been proposed based upon thought
experiments, anecdotal conversations between LLMs and humans, statistical
linguistic analysis, philosophical considerations, and more. In this brief
paper we present a counter-argument based upon a thought experiment and
semi-formal considerations leading to an inherent ambiguity barrier which
prevents LLMs from having any understanding of what their amazingly fluent
dialogues mean.

</details>


### [97] [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/pdf/2505.00985)
*Ayan Sengupta, Yash Goel, Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper advocates for downscaling large language models (LLMs) instead of following neural scaling laws, highlighting inefficiencies and environmental concerns of scaling up.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency, environmental impact, and deployment constraints of scaling up LLMs.

Method: The paper proposes a holistic framework for downscaling LLMs to maintain performance while reducing resource demands.

Result: The result is a set of practical strategies for transitioning to sustainable and efficient LLM development.

Conclusion: The conclusion emphasizes the need for a paradigm shift toward downscaling for more accessible and sustainable LLM development.

Abstract: We challenge the dominant focus on neural scaling laws and advocate for a
paradigm shift toward downscaling in the development of large language models
(LLMs). While scaling laws have provided critical insights into performance
improvements through increasing model and dataset size, we emphasize the
significant limitations of this approach, particularly in terms of
computational inefficiency, environmental impact, and deployment constraints.
To address these challenges, we propose a holistic framework for downscaling
LLMs that seeks to maintain performance while drastically reducing resource
demands. This paper outlines practical strategies for transitioning away from
traditional scaling paradigms, advocating for a more sustainable, efficient,
and accessible approach to LLM development.

</details>


### [98] [Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/pdf/2505.01315)
*Sheikh Samit Muhaimin, Spyridon Mastorakis*

Main category: cs.CL

TL;DR: A new defense framework for LLMs detects and filters adversarial inputs without retraining, achieving 98.71% success in identifying harmful prompts.


<details>
  <summary>Details</summary>
Motivation: The rise in adversarial attacks on LLMs necessitates efficient defenses without costly retraining.

Method: The framework combines prompt filtering (using NLP techniques) and summarization of adversarial literature to provide context-aware defense.

Result: The method achieves 98.71% success in detecting harmful inputs and improves jailbreak resistance.

Conclusion: The framework is an effective, lightweight alternative to retraining-based defenses, enhancing LLM security.

Abstract: The recent growth in the use of Large Language Models has made them
vulnerable to sophisticated adversarial assaults, manipulative prompts, and
encoded malicious inputs. Existing countermeasures frequently necessitate
retraining models, which is computationally costly and impracticable for
deployment. Without the need for retraining or fine-tuning, this study presents
a unique defense paradigm that allows LLMs to recognize, filter, and defend
against adversarial or malicious inputs on their own. There are two main parts
to the suggested framework: (1) A prompt filtering module that uses
sophisticated Natural Language Processing (NLP) techniques, including zero-shot
classification, keyword analysis, and encoded content detection (e.g. base64,
hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and
(2) A summarization module that processes and summarizes adversarial research
literature to give the LLM context-aware defense knowledge. This approach
strengthens LLMs' resistance to adversarial exploitation by fusing text
extraction, summarization, and harmful prompt analysis. According to
experimental results, this integrated technique has a 98.71% success rate in
identifying harmful patterns, manipulative language structures, and encoded
prompts. By employing a modest amount of adversarial research literature as
context, the methodology also allows the model to react correctly to harmful
inputs with a larger percentage of jailbreak resistance and refusal rate. While
maintaining the quality of LLM responses, the framework dramatically increases
LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and
easy substitute for time-consuming, retraining-based defenses.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [99] [Multi-party Collaborative Attention Control for Image Customization](https://arxiv.org/pdf/2505.01428)
*Han Yang, Chuanguang Yang, Qiuli Wang, Zhulin An, Weilun Feng, Libo Huang, Yongjun Xu*

Main category: cs.CV

TL;DR: MCA-Ctrl is a tuning-free method for high-quality image customization using text and complex visual conditions, addressing limitations like subject leakage and inconsistent backgrounds.


<details>
  <summary>Details</summary>
Motivation: Current customization methods for diffusion models have limitations like single-condition input, subject leakage, and high computational costs.

Method: MCA-Ctrl uses self-attention layer operations to coordinate parallel diffusion processes and a Subject Localization Module to extract precise subject layers.

Result: MCA-Ctrl outperforms existing methods in zero-shot image customization, resolving issues like subject leakage and background inconsistency.

Conclusion: MCA-Ctrl offers a robust solution for high-quality image customization with improved semantic consistency and reduced computational costs.

Abstract: The rapid advancement of diffusion models has increased the need for
customized image generation. However, current customization methods face
several limitations: 1) typically accept either image or text conditions alone;
2) customization in complex visual scenarios often leads to subject leakage or
confusion; 3) image-conditioned outputs tend to suffer from inconsistent
backgrounds; and 4) high computational costs. To address these issues, this
paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a
tuning-free method that enables high-quality image customization using both
text and complex visual conditions. Specifically, MCA-Ctrl leverages two key
operations within the self-attention layer to coordinate multiple parallel
diffusion processes and guide the target image generation. This approach allows
MCA-Ctrl to capture the content and appearance of specific subjects while
maintaining semantic consistency with the conditional input. Additionally, to
mitigate subject leakage and confusion issues common in complex visual
scenarios, we introduce a Subject Localization Module that extracts precise
subject and editable image layers based on user instructions. Extensive
quantitative and human evaluation experiments show that MCA-Ctrl outperforms
existing methods in zero-shot image customization, effectively resolving the
mentioned issues.

</details>


### [100] [Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis](https://arxiv.org/pdf/2505.01429)
*Md. Zahid Hossain, Md. Rakibul Islam, Most. Sharmin Sultana Samu*

Main category: cs.CV

TL;DR: The study explores deep learning and vision transformer models for mpox detection using skin lesion images, highlighting dataset limitations and the effectiveness of transfer learning with MobileNet-v2 achieving 93.15% accuracy.


<details>
  <summary>Details</summary>
Motivation: Mpox's similarity to measles and chickenpox complicates early diagnosis, prompting the use of medical imaging and deep learning for improved detection.

Method: The study trains deep learning and vision transformer models from scratch and uses transfer learning with pre-trained models (MobileNet-v2, ViT B16, ResNet-50) on a public skin lesion dataset.

Result: MobileNet-v2 outperformed with 93.15% accuracy; ViT B16 and ResNet-50 also performed well (92.12% and 86.21% accuracy). Explainable AI validated the models.

Conclusion: Transfer learning with pre-trained models, especially MobileNet-v2, is effective for mpox detection despite dataset limitations.

Abstract: Since mpox can spread from person to person, it is a zoonotic viral illness
that poses a significant public health concern. It is difficult to make an
early clinical diagnosis because of how closely its symptoms match those of
measles and chickenpox. Medical imaging combined with deep learning (DL)
techniques has shown promise in improving disease detection by analyzing
affected skin areas. Our study explore the feasibility to train deep learning
and vision transformer-based models from scratch with publicly available skin
lesion image dataset. Our experimental results show dataset limitation as a
major drawback to build better classifier models trained from scratch. We used
transfer learning with the help of pre-trained models to get a better
classifier. The MobileNet-v2 outperformed other state of the art pre-trained
models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and
ResNet-50 also achieved satisfactory performance compared to already available
studies with accuracy 92.12% and 86.21% respectively. To further validate the
performance of the models, we applied explainable AI techniques.

</details>


### [101] [Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models](https://arxiv.org/pdf/2505.01430)
*Muna Numan Said, Aarib Zaidi, Rabia Usman, Sonia Okon, Praneeth Medepalli, Kevin Zhu, Vasu Sharma, Sean O'Brien*

Main category: cs.CV

TL;DR: The paper introduces the Component Inclusion Score (CIS) to measure cultural bias in text-to-image models, revealing disparities between Western and non-Western prompts. It suggests interventions for fairness.


<details>
  <summary>Details</summary>
Motivation: Address cultural biases in text-to-image models that perpetuate misrepresentations due to imbalanced training data.

Method: Benchmarks CIS using 2,400 images to evaluate cultural fidelity, analyzing compositional fragility and contextual misalignment.

Result: Identifies significant biases, linking them to data imbalance, attention entropy, and embedding superposition.

Conclusion: Proposes CIS as a tool for diagnosing and mitigating bias, advocating for fairer AI-generated imagery.

Abstract: The transformative potential of text-to-image (T2I) models hinges on their
ability to synthesize culturally diverse, photorealistic images from textual
prompts. However, these models often perpetuate cultural biases embedded within
their training data, leading to systemic misrepresentations. This paper
benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate
the fidelity of image generation across cultural contexts. Through extensive
analysis involving 2,400 images, we quantify biases in terms of compositional
fragility and contextual misalignment, revealing significant performance gaps
between Western and non-Western cultural prompts. Our findings underscore the
impact of data imbalance, attention entropy, and embedding superposition on
model fairness. By benchmarking models like Stable Diffusion with CIS, we
provide insights into architectural and data-centric interventions for
enhancing cultural inclusivity in AI-generated imagery. This work advances the
field by offering a comprehensive tool for diagnosing and mitigating biases in
T2I generation, advocating for more equitable AI systems.

</details>


### [102] [VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection](https://arxiv.org/pdf/2505.02331)
*Hao Cheng, Zhiwei Zhao, Yichao He, Zhenzhen Hu, Jia Li, Meng Wang, Richang Hong*

Main category: cs.CV

TL;DR: VAEmo is a two-stage framework for audiovisual emotion recognition, combining unified cross-modal encoding with emotion-aware semantic guidance for efficient and generalizable representations.


<details>
  <summary>Details</summary>
Motivation: AVER faces challenges like emotional ambiguity, cross-modal disparities, and scarce annotated data. Existing methods lack fine-grained emotional modeling.

Method: VAEmo uses a two-stage approach: Stage 1 pre-trains a unified network on large-scale VA data via masked reconstruction and contrastive learning. Stage 2 injects external knowledge by aligning generated affective descriptions with VA representations.

Result: VAEmo achieves state-of-the-art performance on multiple AVER benchmarks with a compact design.

Conclusion: The framework highlights the benefits of unified encoding and emotion-aware guidance for robust VA emotion recognition.

Abstract: Audiovisual emotion recognition (AVER) aims to infer human emotions from
nonverbal visual-audio (VA) cues, offering modality-complementary and
language-agnostic advantages. However, AVER remains challenging due to the
inherent ambiguity of emotional expressions, cross-modal expressive
disparities, and the scarcity of reliably annotated data. Recent
self-supervised AVER approaches have introduced strong multimodal
representations, yet they predominantly rely on modality-specific encoders and
coarse content-level alignment, limiting fine-grained emotional semantic
modeling. To address these issues, we propose VAEmo, an efficient two-stage
framework for emotion-centric joint VA representation learning with external
knowledge injection. In Stage 1, a unified and lightweight representation
network is pre-trained on large-scale speaker-centric VA corpora via masked
reconstruction and contrastive objectives, mitigating the modality gap and
learning expressive, complementary representations without emotion labels. In
Stage 2, multimodal large language models automatically generate detailed
affective descriptions according to our well-designed chain-of-thought
prompting for only a small subset of VA samples; these rich textual semantics
are then injected by aligning their corresponding embeddings with VA
representations through dual-path contrastive learning, further bridging the
emotion gap. Extensive experiments on multiple downstream AVER benchmarks show
that VAEmo achieves state-of-the-art performance with a compact design,
highlighting the benefit of unified cross-modal encoding and emotion-aware
semantic guidance for efficient, generalizable VA emotion representations.

</details>


### [103] [ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation](https://arxiv.org/pdf/2505.01431)
*Wenqi Guo, Shan Du*

Main category: cs.CV

TL;DR: A novel zero-shot method for camouflaged object segmentation integrates optical flow, a vision-language model, and SAM 2, outperforming existing methods and even supervised approaches.


<details>
  <summary>Details</summary>
Motivation: Camouflaged object segmentation is challenging due to high similarity between objects and backgrounds, with applications in pest control, defect detection, and medical imaging. Zero-shot approaches are underdeveloped compared to supervised methods.

Method: The proposed method combines optical flow, a vision-language model, and SAM 2 in a sequential pipeline.

Result: Achieves significant performance improvements, raising the F-measure from 0.296 to 0.628 on MoCA-Mask and surpassing supervised methods. Also improves success rate on MoCA-Filter.

Conclusion: The integration of optical flow and vision-language models with SAM 2 proves highly effective for camouflaged object segmentation, setting a new benchmark for zero-shot methods.

Abstract: Camouflaged object segmentation presents unique challenges compared to
traditional segmentation tasks, primarily due to the high similarity in
patterns and colors between camouflaged objects and their backgrounds.
Effective solutions to this problem have significant implications in critical
areas such as pest control, defect detection, and lesion segmentation in
medical imaging. Prior research has predominantly emphasized supervised or
unsupervised pre-training methods, leaving zero-shot approaches significantly
underdeveloped. Existing zero-shot techniques commonly utilize the Segment
Anything Model (SAM) in automatic mode or rely on vision-language models to
generate cues for segmentation; however, their performances remain
unsatisfactory, likely due to the similarity of the camouflaged object and the
background. Optical flow, commonly utilized for detecting moving objects, has
demonstrated effectiveness even with camouflaged entities. Our method
integrates optical flow, a vision-language model, and SAM 2 into a sequential
pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding
performance improvements, significantly outperforming existing zero-shot
methods by raising the F-measure ($F_\beta^w$) from 0.296 to 0.628. Remarkably,
our approach also surpasses supervised methods, increasing the F-measure from
0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset
demonstrates an increase in the success rate from 0.628 to 0.697 when compared
with FlowSAM, a supervised transfer method. A thorough ablation study further
validates the individual contributions of each component. More details can be
found on https://github.com/weathon/vcos.

</details>


### [104] [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/pdf/2505.01790)
*Markos Stamatakis, Joshua Berger, Christian Wartena, Ralph Ewerth, Anett Hoppe*

Main category: cs.CV

TL;DR: The paper explores using vision-language models to generate learning-oriented questions for educational videos, evaluating performance, fine-tuning effects, and question quality.


<details>
  <summary>Details</summary>
Motivation: Improving engagement and knowledge retention in web-based educational videos through automated question generation.

Method: Assessing out-of-the-box models, fine-tuning, video modality impact, and qualitative study on question relevance, answerability, and difficulty.

Result: Findings highlight the need for fine-tuning and address challenges in question diversity and relevance.

Conclusion: Identifies requirements for future datasets and research directions in multimodal question generation.

Abstract: Web-based educational videos offer flexible learning opportunities and are
becoming increasingly popular. However, improving user engagement and knowledge
retention remains a challenge. Automatically generated questions can activate
learners and support their knowledge acquisition. Further, they can help
teachers and learners assess their understanding. While large language and
vision-language models have been employed in various tasks, their application
to question generation for educational videos remains underexplored. In this
paper, we investigate the capabilities of current vision-language models for
generating learning-oriented questions for educational video content. We assess
(1) out-of-the-box models' performance; (2) fine-tuning effects on
content-specific question generation; (3) the impact of different video
modalities on question quality; and (4) in a qualitative study, question
relevance, answerability, and difficulty levels of generated questions. Our
findings delineate the capabilities of current vision-language models,
highlighting the need for fine-tuning and addressing challenges in question
diversity and relevance. We identify requirements for future multimodal
datasets and outline promising research directions.

</details>


### [105] [VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos](https://arxiv.org/pdf/2505.01481)
*Zongxia Li, Xiyang Wu, Yubin Qin, Guangyao Shi, Hongyang Du, Dinesh Manocha, Tianyi Zhou, Jordan Lee Boyd-Graber*

Main category: cs.CV

TL;DR: VideoHallu benchmarks synthetic videos for commonsense and physics violations, evaluates MLLMs, and improves their reasoning via GRPO fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for synthetic video quality ignore commonsense and physics violations, and MLLMs' ability to detect such abnormalities is underexplored.

Method: Introduce VideoHallu benchmark with expert-designed QA tasks, evaluate SoTA MLLMs, and fine-tune them using GRPO on real and synthetic data.

Result: MLLMs still hallucinate on basic tasks, but GRPO fine-tuning with counterexamples improves accuracy.

Conclusion: VideoHallu highlights the challenge of hallucination in synthetic video evaluation and advances MLLMs' reasoning capabilities.

Abstract: Synthetic video generation with foundation models has gained attention for
its realism and wide applications. While these models produce high-quality
frames, they often fail to respect common sense and physical laws, resulting in
abnormal content. Existing metrics like VideoScore emphasize general quality
but ignore such violations and lack interpretability. A more insightful
approach is using multi-modal large language models (MLLMs) as interpretable
evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities
in synthetic videos remains underexplored. To address this, we introduce
VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,
and Kling, paired with expert-designed QA tasks solvable via human-level
reasoning across various categories. We assess several SoTA MLLMs, including
GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and
VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,
these models still hallucinate on basic commonsense and physics tasks in
synthetic settings, underscoring the challenge of hallucination. We further
fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real
and synthetic commonsense/physics data. Results show notable accuracy gains,
especially with counterexample integration, advancing MLLMs' reasoning
capabilities. Our data is available at https://github.com/zli12321/VideoHallu.

</details>


### [106] [WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation](https://arxiv.org/pdf/2505.01490)
*Daoan Zhang, Che Jiang, Ruoshi Xu, Biaoxiang Chen, Zijian Jin, Yutian Lu, Jianguo Zhang, Liang Yong, Jiebo Luo, Shengda Luo*

Main category: cs.CV

TL;DR: WorldGenBench is a benchmark to evaluate T2I models' world knowledge and reasoning, showing proprietary models like GPT-4o outperform others.


<details>
  <summary>Details</summary>
Motivation: Address gaps in T2I models' ability to handle rich world knowledge and implicit reasoning for accurate image generation.

Method: Introduce WorldGenBench and Knowledge Checklist Score to evaluate models' semantic accuracy and reasoning.

Result: Diffusion models lead open-source methods, but proprietary models like GPT-4o excel in reasoning and knowledge integration.

Conclusion: Next-gen T2I systems need deeper understanding and inference capabilities.

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models still struggle with prompts that require rich
world knowledge and implicit reasoning: both of which are critical for
producing semantically accurate, coherent, and contextually appropriate images
in real-world scenarios. To address this gap, we introduce
\textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I
models' world knowledge grounding and implicit inferential capabilities,
covering both the humanities and nature domains. We propose the
\textbf{Knowledge Checklist Score}, a structured metric that measures how well
generated images satisfy key semantic expectations. Experiments across 21
state-of-the-art models reveal that while diffusion models lead among
open-source methods, proprietary auto-regressive models like GPT-4o exhibit
significantly stronger reasoning and knowledge integration. Our findings
highlight the need for deeper understanding and inference capabilities in
next-generation T2I systems. Project Page:
\href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}

</details>


### [107] [PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications](https://arxiv.org/pdf/2505.01881)
*Trisanth Srinivasan, Santosh Patapati*

Main category: cs.CV

TL;DR: PhysNav-DG integrates sensor fusion and vision-language models for robust navigation, offering explanations and improved success rates.


<details>
  <summary>Details</summary>
Motivation: To enhance navigation by combining accurate state estimation with transparent decision-making for diverse environments.

Method: Uses a dual-branch architecture with a modified Adaptive Kalman Filter and vision-language models like LLaMA and BLIP-2.

Result: Improves navigation success rates by over 20% and provides clear, grounded explanations.

Conclusion: Connects semantic reasoning and geometric planning for safer, more trustworthy autonomous systems.

Abstract: Robust navigation in diverse environments and domains requires both accurate
state estimation and transparent decision making. We present PhysNav-DG, a
novel framework that integrates classical sensor fusion with the semantic power
of vision-language models. Our dual-branch architecture predicts navigation
actions from multi-sensor inputs while simultaneously generating detailed
chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically
adjusts its noise parameters based on environmental context. It leverages
several streams of raw sensor data along with semantic insights from models
such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the
MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,
autonomous driving, and social navigation tasks with ground-truth actions and
human-validated explanations. Extensive experiments and ablations show that
PhysNav-DG improves navigation success rates by over 20% and achieves high
efficiency, with explanations that are both highly grounded and clear. This
work connects high-level semantic reasoning and geometric planning for safer
and more trustworthy autonomous systems.

</details>


### [108] [Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer](https://arxiv.org/pdf/2505.01530)
*Muhammad Tayyab Khan, Zane Yong, Lequn Chen, Jun Ming Tan, Wenhe Feng, Seung Ki Moon*

Main category: cs.CV

TL;DR: A hybrid deep learning framework combining OBB detection and transformer-based parsing improves structured information extraction from 2D engineering drawings, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Manual extraction is inefficient, and traditional OCR struggles with complex layouts, necessitating a more accurate and automated solution.

Method: Integrates YOLOv11 for OBB detection and Donut for parsing, using an in-house dataset to train and fine-tune models.

Result: Single model outperforms category-specific ones, achieving high precision (94.77%), recall (100%), and F1 score (97.3%) with reduced hallucination (5.23%).

Conclusion: The framework enhances accuracy, reduces manual effort, and is scalable for precision-driven industries.

Abstract: Accurate extraction of key information from 2D engineering drawings is
crucial for high-precision manufacturing. Manual extraction is time-consuming
and error-prone, while traditional Optical Character Recognition (OCR)
techniques often struggle with complex layouts and overlapping symbols,
resulting in unstructured outputs. To address these challenges, this paper
proposes a novel hybrid deep learning framework for structured information
extraction by integrating an oriented bounding box (OBB) detection model with a
transformer-based document parsing model (Donut). An in-house annotated dataset
is used to train YOLOv11 for detecting nine key categories: Geometric
Dimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials,
Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are
cropped into images and labeled to fine-tune Donut for structured JSON output.
Fine-tuning strategies include a single model trained across all categories and
category-specific models. Results show that the single model consistently
outperforms category-specific ones across all evaluation metrics, achieving
higher precision (94.77% for GD&T), recall (100% for most), and F1 score
(97.3%), while reducing hallucination (5.23%). The proposed framework improves
accuracy, reduces manual effort, and supports scalable deployment in
precision-driven industries.

</details>


### [109] [Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction](https://arxiv.org/pdf/2505.02539)
*Nahuel Garcia-D'Urso, Bernabe Sanchez-Sos, Jorge Azorin-Lopez, Andres Fuster-Guillo, Antonio Macia-Lillo, Higinio Mora-Mora*

Main category: cs.CV

TL;DR: An iterative extrinsic calibration method for multi-camera RGB-D systems improves alignment accuracy using 3D marker constraints.


<details>
  <summary>Details</summary>
Motivation: Precise extrinsic calibration is crucial for accurate 3D reconstruction in multi-camera setups.

Method: Segments and refines marker planes via clustering, regression, and iterative reassignment for robust geometric correspondence.

Result: Significant reduction in alignment errors, validated in controlled and real-world settings.

Conclusion: The method enhances 3D reconstruction accuracy for applications like patient modeling in nutritional treatments.

Abstract: Accurate 3D reconstruction using multi-camera RGB-D systems critically
depends on precise extrinsic calibration to achieve proper alignment between
captured views. In this paper, we introduce an iterative extrinsic calibration
method that leverages the geometric constraints provided by a three-dimensional
marker to significantly improve calibration accuracy. Our proposed approach
systematically segments and refines marker planes through clustering,
regression analysis, and iterative reassignment techniques, ensuring robust
geometric correspondence across camera views. We validate our method
comprehensively in both controlled environments and practical real-world
settings within the Tech4Diet project, aimed at modeling the physical
progression of patients undergoing nutritional treatments. Experimental results
demonstrate substantial reductions in alignment errors, facilitating accurate
and reliable 3D reconstructions.

</details>


### [110] [Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation](https://arxiv.org/pdf/2505.01548)
*Zhen Yao, Xiaowen Ying, Mooi Choo Chuah*

Main category: cs.CV

TL;DR: The paper introduces Motion-enhanced Event Tensor (MET) and two modules (BFAM and TFM) to address RGB-Event fusion misalignments, improving semantic segmentation performance.


<details>
  <summary>Details</summary>
Motivation: RGB-Event fusion faces temporal, spatial, and modal misalignments, which existing methods fail to address effectively.

Method: Proposes MET for dense, coherent event representation and introduces BFAM (frequency-aware) and TFM (temporal fusion) to mitigate misalignments.

Result: Outperforms state-of-the-art RGB-Event semantic segmentation methods on two large datasets.

Conclusion: The proposed framework effectively resolves misalignments and enhances performance in RGB-Event fusion tasks.

Abstract: Event cameras capture motion dynamics, offering a unique modality with great
potential in various computer vision tasks. However, RGB-Event fusion faces
three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal
misalignment. Existing voxel grid representations neglect temporal correlations
between consecutive event windows, and their formulation with simple
accumulation of asynchronous and sparse events is incompatible with the
synchronous and dense nature of RGB modality. To tackle these challenges, we
propose a novel event representation, Motion-enhanced Event Tensor (MET), which
transforms sparse event voxels into a dense and temporally coherent form by
leveraging dense optical flows and event temporal features. In addition, we
introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a
Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to
mitigate modal misalignment, while bidirectional flow aggregation and temporal
fusion mechanisms resolve spatiotemporal misalignment. Experimental results on
two large-scale datasets demonstrate that our framework significantly
outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our
code is available at: https://github.com/zyaocoder/BRENet.

</details>


### [111] [Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication](https://arxiv.org/pdf/2505.02549)
*Yongxiang Li, Yuan Sun, Yang Qin, Dezhong Peng, Xi Peng, Peng Hu*

Main category: cs.CV

TL;DR: A new framework (RoDE) addresses pseudo-label noise in unsupervised visible-infrared person re-identification by dynamically emphasizing clean samples, using dual models, and aligning clusters.


<details>
  <summary>Details</summary>
Motivation: Challenges in UVI-ReID include the modality gap and noisy pseudo-labels from clustering, which hinder model performance.

Method: RoDE introduces Robust Adaptive Learning (RAL) for noise handling, dual-model training to prevent error accumulation, and Cluster Consistency Matching (CCM) for alignment.

Result: RoDE outperforms existing methods on three benchmarks by effectively mitigating pseudo-label noise.

Conclusion: The proposed RoDE framework successfully addresses pseudo-label noise challenges, improving performance in UVI-ReID.

Abstract: Unsupervised visible-infrared person re-identification (UVI-ReID) aims to
retrieve pedestrian images across different modalities without costly
annotations, but faces challenges due to the modality gap and lack of
supervision. Existing methods often adopt self-training with
clustering-generated pseudo-labels but implicitly assume these labels are
always correct. In practice, however, this assumption fails due to inevitable
pseudo-label noise, which hinders model learning. To address this, we introduce
a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),
characterized by three key challenges: noise overfitting, error accumulation,
and noisy cluster correspondence. To this end, we propose a novel Robust
Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy
pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning
mechanism (RAL) is proposed to dynamically emphasize clean samples while
down-weighting noisy ones. Second, to alleviate error accumulation-where the
model reinforces its own mistakes-RoDE employs dual distinct models that are
alternately trained using pseudo-labels from each other, encouraging diversity
and preventing collapse. However, this dual-model strategy introduces
misalignment between clusters across models and modalities, creating noisy
cluster correspondence. To resolve this, we introduce Cluster Consistency
Matching (CCM), which aligns clusters across models and modalities by measuring
cross-cluster similarity. Extensive experiments on three benchmarks demonstrate
the effectiveness of RoDE.

</details>


### [112] [A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning](https://arxiv.org/pdf/2505.01558)
*Anan Yaghmour, Melba M. Crawford, Saurabh Prasad*

Main category: cs.CV

TL;DR: A domain generalization approach for remote sensing segmentation leverages geospatial foundation models with soft-alignment pseudo-labeling and generative pre-training, improving adaptability and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited labeled data and variability in remote sensing by enhancing model generalization through domain adaptation.

Method: Combines soft-alignment pseudo-labeling with source-to-target generative pre-training, using MAE-based generative learning for domain-invariant features.

Result: Effective adaptability and segmentation performance demonstrated on hyperspectral and multispectral datasets.

Conclusion: The proposed method advances domain generalization in remote sensing, offering practical improvements for segmentation tasks.

Abstract: Remote sensing enables a wide range of critical applications such as land
cover and land use mapping, crop yield prediction, and environmental
monitoring. Advances in satellite technology have expanded remote sensing
datasets, yet high-performance segmentation models remain dependent on
extensive labeled data, challenged by annotation scarcity and variability
across sensors, illumination, and geography. Domain adaptation offers a
promising solution to improve model generalization. This paper introduces a
domain generalization approach to leveraging emerging geospatial foundation
models by combining soft-alignment pseudo-labeling with source-to-target
generative pre-training. We further provide new mathematical insights into
MAE-based generative learning for domain-invariant feature learning.
Experiments with hyperspectral and multispectral remote sensing datasets
confirm our method's effectiveness in enhancing adaptability and segmentation.

</details>


### [113] [PainFormer: a Vision Foundation Model for Automatic Pain Assessment](https://arxiv.org/pdf/2505.01571)
*Stefanos Gkikas, Raul Fernandez Rojas, Manolis Tsiknakis*

Main category: cs.CV

TL;DR: PainFormer, a vision foundation model, uses multi-task learning to assess pain from diverse modalities, outperforming 73 existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate pain evaluation is crucial for effective management; automated systems can provide continuous monitoring and decision support.

Method: PainFormer, trained on 14 tasks/datasets (10.9M samples), extracts embeddings for input modalities. An Embedding-Mixer (transformer-based) performs final pain assessment.

Result: PainFormer achieves state-of-the-art performance on BioVid and AI4Pain datasets, excelling in both unimodal and multimodal settings.

Conclusion: PainFormer advances automatic pain assessment, paving the way for general-purpose models in this domain.

Abstract: Pain is a manifold condition that impacts a significant percentage of the
population. Accurate and reliable pain evaluation for the people suffering is
crucial to developing effective and advanced pain management protocols.
Automatic pain assessment systems provide continuous monitoring and support
decision-making processes, ultimately aiming to alleviate distress and prevent
functionality decline. This study introduces PainFormer, a vision foundation
model based on multi-task learning principles trained simultaneously on 14
tasks/datasets with a total of 10.9 million samples. Functioning as an
embedding extractor for various input modalities, the foundation model provides
feature representations to the Embedding-Mixer, a transformer-based module that
performs the final pain assessment. Extensive experiments employing behavioral
modalities-including RGB, synthetic thermal, and estimated depth videos-and
physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that
PainFormer effectively extracts high-quality embeddings from diverse input
modalities. The proposed framework is evaluated on two pain datasets, BioVid
and AI4Pain, and directly compared to 73 different methodologies documented in
the literature. Experiments conducted in unimodal and multimodal settings
demonstrate state-of-the-art performances across modalities and pave the way
toward general-purpose models for automatic pain assessment.

</details>


### [114] [Grounding Task Assistance with Multimodal Cues from a Single Demonstration](https://arxiv.org/pdf/2505.01578)
*Gabriel Sarch, Balasaravanan Thoravi Kumaravel, Sahithya Ravi, Vibhav Vineet, Andrew D. Wilson*

Main category: cs.CV

TL;DR: MICA integrates eye gaze and speech cues to enhance task assistance by capturing fine-grained intent and user-specific cues, outperforming frame-based methods.


<details>
  <summary>Details</summary>
Motivation: RGB video lacks fine-grained contextual cues like intent and user preferences, limiting Vision Language Models' reasoning.

Method: MICA segments demonstrations into sub-tasks, extracts keyframes and captions, and integrates gaze and speech cues.

Result: Multimodal cues improve response quality; gaze alone achieves 93% of speech performance, with combination yielding highest accuracy.

Conclusion: Multimodal signals are crucial for real-world AI task assistance, highlighting limitations of frame-based context.

Abstract: A person's demonstration often serves as a key reference for others learning
the same task. However, RGB video, the dominant medium for representing these
demonstrations, often fails to capture fine-grained contextual cues such as
intent, safety-critical environmental factors, and subtle preferences embedded
in human behavior. This sensory gap fundamentally limits the ability of Vision
Language Models (VLMs) to reason about why actions occur and how they should
adapt to individual users. To address this, we introduce MICA (Multimodal
Interactive Contextualized Assistance), a framework that improves
conversational agents for task assistance by integrating eye gaze and speech
cues. MICA segments demonstrations into meaningful sub-tasks and extracts
keyframes and captions that capture fine-grained intent and user-specific cues,
enabling richer contextual grounding for visual question answering. Evaluations
on questions derived from real-time chat-assisted task replication show that
multimodal cues significantly improve response quality over frame-based
retrieval. Notably, gaze cues alone achieves 93% of speech performance, and
their combination yields the highest accuracy. Task type determines the
effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the
need for adaptable multimodal models. These results highlight the limitations
of frame-based context and demonstrate the value of multimodal signals for
real-world AI task assistance.

</details>


### [115] [SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition](https://arxiv.org/pdf/2308.04369)
*Xiao Wang, Yao Rong, Zongzhen Wu, Lin Zhu, Bo Jiang, Jin Tang, Yonghong Tian*

Main category: cs.CV

TL;DR: The paper proposes a framework for pattern recognition by fusing RGB frames and event streams, addressing limitations of current methods by balancing performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for event-based classification either lack detailed texture information or fail to balance energy efficiency and performance.

Method: The framework includes a Transformer for RGB encoding, a Spiking Neural Network for event encoding, a fusion module, and a prediction head. A new dataset, PokerEvent, is introduced.

Result: Experiments on two datasets validate the framework's effectiveness.

Conclusion: The work advances pattern recognition by combining RGB and event data, with released dataset and code.

Abstract: Event camera-based pattern recognition is a newly arising research topic in
recent years. Current researchers usually transform the event streams into
images, graphs, or voxels, and adopt deep neural networks for event-based
classification. Although good performance can be achieved on simple event
recognition datasets, however, their results may be still limited due to the
following two issues. Firstly, they adopt spatial sparse event streams for
recognition only, which may fail to capture the color and detailed texture
information well. Secondly, they adopt either Spiking Neural Networks (SNN) for
energy-efficient recognition with suboptimal results, or Artificial Neural
Networks (ANN) for energy-intensive, high-performance recognition. However,
seldom of them consider achieving a balance between these two aspects. In this
paper, we formally propose to recognize patterns by fusing RGB frames and event
streams simultaneously and propose a new RGB frame-event recognition framework
to address the aforementioned issues. The proposed method contains four main
modules, i.e., memory support Transformer network for RGB frame encoding,
spiking neural network for raw event stream encoding, multi-modal bottleneck
fusion module for RGB-Event feature aggregation, and prediction head. Due to
the scarce of RGB-Event based classification dataset, we also propose a
large-scale PokerEvent dataset which contains 114 classes, and 27102
frame-event pairs recorded using a DVS346 event camera. Extensive experiments
on two RGB-Event based classification datasets fully validated the
effectiveness of our proposed framework. We hope this work will boost the
development of pattern recognition by fusing RGB frames and event streams. Both
our dataset and source code of this work will be released at
https://github.com/Event-AHU/SSTFormer

</details>


### [116] [TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action](https://arxiv.org/pdf/2505.01583)
*Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, Wenhao Chai, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: TEMPURA is a two-stage framework for video temporal understanding, combining masked event prediction and dense captioning to improve causal reasoning and fine-grained event segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing methods either reduce temporal resolution or treat videos as unsegmented streams, limiting fine-grained event understanding and causal modeling.

Method: TEMPURA uses masked event prediction for causal reasoning and dense captioning for video segmentation into non-overlapping events.

Result: Trained on VER dataset, TEMPURA outperforms baselines in temporal grounding and highlight detection.

Conclusion: Integrating causal reasoning with fine-grained temporal segmentation enhances video understanding.

Abstract: Understanding causal event relationships and achieving fine-grained temporal
grounding in videos remain challenging for vision-language models. Existing
methods either compress video tokens to reduce temporal resolution, or treat
videos as unsegmented streams, which obscures fine-grained event boundaries and
limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event
Masked Prediction and Understanding for Reasoning in Action), a two-stage
training framework that enhances video temporal understanding. TEMPURA first
applies masked event prediction reasoning to reconstruct missing events and
generate step-by-step causal explanations from dense event annotations, drawing
inspiration from effective infilling techniques. TEMPURA then learns to perform
video segmentation and dense captioning to decompose videos into
non-overlapping events with detailed, timestamp-aligned descriptions. We train
TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training
instances and 500K videos with temporally aligned event descriptions and
structured reasoning steps. Experiments on temporal grounding and highlight
detection benchmarks demonstrate that TEMPURA outperforms strong baseline
models, confirming that integrating causal reasoning with fine-grained temporal
segmentation leads to improved video understanding.

</details>


### [117] [Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation](https://arxiv.org/pdf/2408.10453)
*Liu He, Yizhi Song, Hejun Huang, Pinxin Liu, Yunlong Tang, Daniel Aliaga, Xin Zhou*

Main category: cs.CV

TL;DR: An automatic synthetic video generation pipeline using Vision Large Language Model (VLM) agents improves video quality and consistency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing shortcomings in diffusion-based or autoregressive text-to-video models, such as improper motion and temporal inconsistency, by leveraging human-directed 3D synthetic video techniques without manual intervention.

Method: A collaborative VLM agent pipeline (Director, Programmer, Reviewer) decomposes text descriptions, generates Blender scripts, and iteratively improves them for high-quality video rendering.

Result: Outperforms commercial models in video quality and instruction-following, validated by user studies on quality, consistency, and rationality.

Conclusion: The VLM agent framework offers a scalable, automated solution for high-quality synthetic video generation, bridging the gap between text descriptions and professional CGI.

Abstract: Text-to-video generation has been dominated by diffusion-based or
autoregressive models. These novel models provide plausible versatility, but
are criticized for improper physical motion, shading and illumination, camera
motion, and temporal consistency. The film industry relies on manually-edited
Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D
synthetic videos address these shortcomings, but require tight collaboration
between movie makers and 3D rendering experts. We introduce an automatic
synthetic video generation pipeline based on Vision Large Language Model (VLM)
agent collaborations. Given a language description of a video, multiple VLM
agents direct various processes of the generation pipeline. They cooperate to
create Blender scripts which render a video following the given description.
Augmented with Blender-based movie making knowledge, the Director agent
decomposes the text-based video description into sub-processes. For each
sub-process, the Programmer agent produces Python-based Blender scripts based
on function composing and API calling. The Reviewer agent, with knowledge of
video reviewing, character motion coordinates, and intermediate screenshots,
provides feedback to the Programmer agent. The Programmer agent iteratively
improves scripts to yield the best video outcome. Our generated videos show
better quality than commercial video generation models in five metrics on video
quality and instruction-following performance. Our framework outperforms other
approaches in a user study on quality, consistency, and rationality.

</details>


### [118] [Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation](https://arxiv.org/pdf/2505.01615)
*Dimitrios Dagdilelis, Panagiotis Grigoriadis, Roberto Galeazzi*

Main category: cs.CV

TL;DR: A cross-attention transformer method fuses RGB, infrared, LiDAR, radar, and chart data for safer autonomous marine navigation, validated in real-world trials.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous marine navigation by creating a reliable birds-eye view of a vessel's surroundings, especially in adverse conditions.

Method: Uses a cross-attention transformer to fuse multiview RGB, infrared, LiDAR, radar, and electronic chart data.

Result: Produces a detailed, robust scene representation, improving navigation accuracy in real-world sea trials.

Conclusion: The method effectively supports safer marine navigation in challenging environments.

Abstract: We propose a cross attention transformer based method for multimodal sensor
fusion to build a birds eye view of a vessels surroundings supporting safer
autonomous marine navigation. The model deeply fuses multiview RGB and long
wave infrared images with sparse LiDAR point clouds. Training also integrates X
band radar and electronic chart data to inform predictions. The resulting view
provides a detailed reliable scene representation improving navigational
accuracy and robustness. Real world sea trials confirm the methods
effectiveness even in adverse weather and complex maritime settings.

</details>


### [119] [Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability](https://arxiv.org/pdf/2505.01650)
*Wenxuan Zhang, Peng Hu*

Main category: cs.CV

TL;DR: The paper explores deep learning models (SE, ViT, GELAN) for space object detection in LEO satellites, showing improved precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for high-precision, low-delay space object detection to prevent collisions in expanding LEO satellite constellations.

Method: Proposes models combining SE layers, ViT, and GELAN, evaluating their performance in SOD tasks.

Result: Achieves mAP50 up to 0.751 and mAP50:95 up to 0.280, with reduced computational costs and power consumption.

Conclusion: The GELAN-ViT-SE model outperforms baselines, offering a feasible solution for efficient SOD in LEO.

Abstract: The rapid expansion of advanced low-Earth orbit (LEO) satellites in large
constellations is positioning space assets as key to the future, enabling
global internet access and relay systems for deep space missions. A solution to
the challenge is effective space object detection (SOD) for collision
assessment and avoidance. In SOD, an LEO satellite must detect other satellites
and objects with high precision and minimal delay. This paper investigates the
feasibility and effectiveness of employing vision sensors for SOD tasks based
on deep learning (DL) models. It introduces models based on the
Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the
Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their
performance under SOD scenarios. Experimental results show that the proposed
models achieve mean average precision at intersection over union threshold 0.5
(mAP50) scores of up to 0.751 and mean average precision averaged over
intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to
0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model
increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from
0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to
5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\%.

</details>


### [120] [A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory](https://arxiv.org/pdf/2505.01656)
*Chenyang Fan, Xujie Zhu, Taige Luo, Sheng Xu, Zhulin Chen, Hongxin Yang*

Main category: cs.CV

TL;DR: The paper introduces WaveInst, a novel instance segmentation framework using discrete wavelet transform for accurate tree structure extraction, outperforming existing methods and providing valuable data for forestry applications.


<details>
  <summary>Details</summary>
Motivation: Current LiDAR and UAV-based methods for tree structure extraction are either costly or lack 3D detail. The study aims to address these limitations by improving accuracy and efficiency in branch information extraction.

Method: Proposes WaveInst, a framework combining discrete wavelet transform for multi-scale edge enhancement, tested on datasets like SynthTree43k and PoplarDataset.

Result: Achieves mean average precision of 49.6 (mature trees) and 24.3 (juvenile trees), surpassing state-of-the-art by 9.9. Also extracts growth parameters like tree location and height from 2D images.

Conclusion: The method advances tree structure analysis, supporting precision forestry, ecological monitoring, and breeding with a new dataset (PoplarDataset).

Abstract: The pattern analysis of tree structure holds significant scientific value for
genetic breeding and forestry management. The current trunk and branch
extraction technologies are mainly LiDAR-based or UAV-based. The former
approaches obtain high-precision 3D data, but its equipment cost is high and
the three-dimensional (3D) data processing is complex. The latter approaches
efficiently capture canopy information, but they miss the 3-D structure of
trees. In order to deal with the branch information extraction from the complex
background interference and occlusion, this work proposes a novel WaveInst
instance segmentation framework, involving a discrete wavelet transform, to
enhance multi-scale edge information for accurately improving tree structure
extraction. Experimental results of the proposed model show superior
performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset.
Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated
to extract tree structure and pattern analysis from artificial forest. The
proposed method achieves a mean average precision of 49.6 and 24.3 for the
structure extraction of mature and juvenile trees, respectively, surpassing the
existing state-of-the-art method by 9.9. Furthermore, by in tegrating the
segmentation model within the regression model, we accurately achieve
significant tree grown parameters, such as the location of trees, the
diameter-at-breast-height of individual trees, and the plant height, from 2D
images directly. This study provides a scientific and plenty of data for tree
structure analysis in related to the phenotype research, offering a platform
for the significant applications in precision forestry, ecological monitoring,
and intelligent breeding.

</details>


### [121] [Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation](https://arxiv.org/pdf/2505.01664)
*Yi-Ming Zhai, Chuan-Xian Ren, Hong Yan*

Main category: cs.CV

TL;DR: A Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed for Partial Domain Adaptation (PDA), addressing domain shift and non-identical label spaces by reweighting source domains and enhancing class-oriented representation.


<details>
  <summary>Details</summary>
Motivation: PDA is challenging due to domain shift and differing label spaces. Existing methods need improvement for class-conditional distribution matching.

Method: SSOT estimates class weights, constructs a reweighted source domain, and uses a soft-masked transport distance matrix. It employs semi-dual optimal transport and neural networks for optimization.

Result: Extensive experiments on four benchmarks show SSOT's effectiveness.

Conclusion: SSOT successfully addresses PDA challenges by integrating optimal transport and neural networks, improving domain adaptation performance.

Abstract: Visual domain adaptation aims to learn discriminative and domain-invariant
representation for an unlabeled target domain by leveraging knowledge from a
labeled source domain. Partial domain adaptation (PDA) is a general and
practical scenario in which the target label space is a subset of the source
one. The challenges of PDA exist due to not only domain shift but also the
non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual
Optimal Transport (SSOT) method is proposed to deal with the PDA problem.
Specifically, the class weights of domains are estimated, and then a reweighed
source domain is constructed, which is favorable in conducting
class-conditional distribution matching with the target domain. A soft-masked
transport distance matrix is constructed by category predictions, which will
enhance the class-oriented representation ability of optimal transport in the
shared feature space. To deal with large-scale optimal transport problems, the
semi-dual formulation of the entropy-regularized Kantorovich problem is
employed since it can be optimized by gradient-based algorithms. Further, a
neural network is exploited to approximate the Kantorovich potential due to its
strong fitting ability. This network parametrization also allows the
generalization of the dual variable outside the supports of the input
distribution. The SSOT model is built upon neural networks, which can be
optimized alternately in an end-to-end manner. Extensive experiments are
conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.

</details>


### [122] [Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study](https://arxiv.org/pdf/2505.01680)
*Tamim Ahmed, Thanassis Rikakis*

Main category: cs.CV

TL;DR: An automated ARAT scoring system using multimodal video analysis and hierarchical Bayesian models achieves 89% accuracy, validated by clinicians.


<details>
  <summary>Details</summary>
Motivation: Manual ARAT scoring in stroke rehabilitation is time-consuming and inconsistent, necessitating an automated, scalable solution.

Method: Integrates SlowFast, I3D, and Transformer models with OpenPose keypoints and object locations, using multi-view data and fusion techniques. Hierarchical Bayesian Models infer movement quality.

Result: Achieves 89.0% validation accuracy with late fusion, closely matching manual assessments. Clinician feedback confirms usability and accuracy.

Conclusion: The system provides a scalable, interpretable, and clinically validated solution for automated stroke rehabilitation assessment.

Abstract: Manual scoring of the Action Research Arm Test (ARAT) for upper extremity
assessment in stroke rehabilitation is time-intensive and variable. We propose
an automated ARAT scoring system integrating multimodal video analysis with
SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object
locations. Our approach employs multi-view data (ipsilateral, contralateral,
and top perspectives), applying early and late fusion to combine features
across views and models. Hierarchical Bayesian Models (HBMs) infer movement
quality components, enhancing interpretability. A clinician dashboard displays
task scores, execution times, and quality assessments. We conducted a study
with five clinicians who reviewed 500 video ratings generated by our system,
providing feedback on its accuracy and usability. Evaluated on a stroke
rehabilitation dataset, our framework achieves 89.0% validation accuracy with
late fusion, with HBMs aligning closely with manual assessments. This work
advances automated rehabilitation by offering a scalable, interpretable
solution with clinical validation.

</details>


### [123] [Topology-Aware CLIP Few-Shot Learning](https://arxiv.org/pdf/2505.01694)
*Dazhi Huang*

Main category: cs.CV

TL;DR: A topology-aware tuning method for VLMs improves few-shot learning by aligning visual and text representations using RTD and Cross-Entropy loss, achieving 1-2% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Balancing pre-trained knowledge retention and task-specific adaptation in VLMs for few-shot learning, while leveraging latent space structural information.

Method: Integrates RTD into the TR framework, aligning topological structures of visual and text representations with a combined RTD and Cross-Entropy loss, freezing base VLM encoders.

Result: Achieves an average accuracy improvement of 1-2% over baselines across 6 benchmark datasets in few-shot settings.

Conclusion: The method effectively boosts VLM few-shot capabilities by incorporating topological alignment.

Abstract: Efficiently adapting large Vision-Language Models (VLMs) like CLIP for
few-shot learning poses challenges in balancing pre-trained knowledge retention
and task-specific adaptation. Existing methods often overlook valuable
structural information within the VLM's latent space. We introduce a
topology-aware tuning approach integrating Representation Topology Divergence
(RTD) into the Task Residual (TR) framework. By explicitly aligning the
topological structures of visual and text representations using a combined RTD
and Cross-Entropy loss, while freezing base VLM encoders, our method enhances
few-shot performance. We optimize only lightweight Task Residual parameters,
effectively leveraging topological information. Across 6 diverse benchmark
datasets, our approach demonstrates significant gains, achieving an average
accuracy improvement of 1-2\% over relevant baseline methods in few-shot
settings. This work presents an effective strategy to boost VLM few-shot
capabilities by incorporating topological alignment.

</details>


### [124] [Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning](https://arxiv.org/pdf/2505.01699)
*Yifan Liu, Ruichen Yao, Yaokun Liu, Ruohan Zong, Zelin Li, Yang Zhang, Dong Wang*

Main category: cs.CV

TL;DR: The paper introduces BNMR, a method to address fairness in face recognition by focusing on biological face components, overcoming challenges like label scarcity and inter-dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing fairness efforts focus on demographics, but fairness at the biological face component level is unexplored.

Method: Proposes BNMR, a Bayesian Network-informed Meta Reweighting approach, to dynamically track and mitigate bias in face attribute prediction.

Result: BNMR outperforms baselines in bias mitigation and shows positive impact on demographic fairness.

Conclusion: Face component fairness could serve as a surrogate for demographic fairness, opening new research directions.

Abstract: The widespread integration of face recognition technologies into various
applications (e.g., access control and personalized advertising) necessitates a
critical emphasis on fairness. While previous efforts have focused on
demographic fairness, the fairness of individual biological face components
remains unexplored. In this paper, we focus on face component fairness, a
fairness notion defined by biological face features. To our best knowledge, our
work is the first work to mitigate bias of face attribute prediction at the
biological feature level. In this work, we identify two key challenges in
optimizing face component fairness: attribute label scarcity and attribute
inter-dependencies, both of which limit the effectiveness of bias mitigation
from previous approaches. To address these issues, we propose \textbf{B}ayesian
\textbf{N}etwork-informed \textbf{M}eta \textbf{R}eweighting (BNMR), which
incorporates a Bayesian Network calibrator to guide an adaptive
meta-learning-based sample reweighting process. During the training process of
our approach, the Bayesian Network calibrator dynamically tracks model bias and
encodes prior probabilities for face component attributes to overcome the above
challenges. To demonstrate the efficacy of our approach, we conduct extensive
experiments on a large-scale real-world human face dataset. Our results show
that BNMR is able to consistently outperform recent face bias mitigation
baselines. Moreover, our results suggest a positive impact of face component
fairness on the commonly considered demographic fairness (e.g.,
\textit{gender}). Our findings pave the way for new research avenues on face
component fairness, suggesting that face component fairness could serve as a
potential surrogate objective for demographic fairness. The code for our work
is publicly
available~\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.

</details>


### [125] [Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings](https://arxiv.org/pdf/2505.01711)
*Alexander Davis, Rafael Souza, Jia-Hao Lim*

Main category: cs.CV

TL;DR: CXR-TextInter repurposes LLMs for chest X-ray interpretation using structured text representations and medical knowledge, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs for automated CXR interpretation, an underexplored area, by structuring visual data and integrating medical knowledge.

Method: Uses an upstream image analysis pipeline to generate structured text, augmented with a medical knowledge module, and trains on the MediInstruct-CXR dataset.

Result: Achieves state-of-the-art performance in pathology detection, report generation, and visual question answering, validated by human radiologists.

Conclusion: Demonstrates the potential of LLMs for medical image AI when visual data is structured and domain knowledge is integrated.

Abstract: Automated interpretation of chest X-rays (CXR) is a critical task with the
potential to significantly improve clinical workflow and patient care. While
recent advances in multimodal foundation models have shown promise, effectively
leveraging the full power of large language models (LLMs) for this visual task
remains an underexplored area. This paper introduces CXR-TextInter, a novel
framework that repurposes powerful text-centric LLMs for CXR interpretation by
operating solely on a rich, structured textual representation of the image
content, generated by an upstream image analysis pipeline. We augment this
LLM-centric approach with an integrated medical knowledge module to enhance
clinical reasoning. To facilitate training and evaluation, we developed the
MediInstruct-CXR dataset, containing structured image representations paired
with diverse, clinically relevant instruction-response examples, and the
CXR-ClinEval benchmark for comprehensive assessment across various
interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that
CXR-TextInter achieves state-of-the-art quantitative performance across
pathology detection, report generation, and visual question answering,
surpassing existing multimodal foundation models. Ablation studies confirm the
critical contribution of the knowledge integration module. Furthermore, blinded
human evaluation by board-certified radiologists shows a significant preference
for the clinical quality of outputs generated by CXR-TextInter. Our work
validates an alternative paradigm for medical image AI, showcasing the
potential of harnessing advanced LLM capabilities when visual information is
effectively structured and domain knowledge is integrated.

</details>


### [126] [Vision and Intention Boost Large Language Model in Long-Term Action Anticipation](https://arxiv.org/pdf/2505.01713)
*Congqi Cao, Lanshu Hu, Yating Yu, Yanning Zhang*

Main category: cs.CV

TL;DR: The paper proposes an Intention-Conditioned Vision-Language (ICVL) model to improve long-term action anticipation by combining visual and textual data, outperforming previous single-modality methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for long-term action anticipation rely on single-modality inputs (video or text), leading to information loss or lack of prior knowledge. The authors aim to integrate visual and textual data for better performance.

Method: The ICVL model uses a vision-language model to infer intentions from video, fuses them with visual features, and feeds them into a large language model for action prediction. An example selection strategy enhances in-context learning.

Result: The model achieves state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets.

Conclusion: The ICVL model effectively combines visual and textual data, demonstrating superior performance in long-term action anticipation.

Abstract: Long-term action anticipation (LTA) aims to predict future actions over an
extended period. Previous approaches primarily focus on learning exclusively
from video data but lack prior knowledge. Recent researches leverage large
language models (LLMs) by utilizing text-based inputs which suffer severe
information loss. To tackle these limitations single-modality methods face, we
propose a novel Intention-Conditioned Vision-Language (ICVL) model in this
study that fully leverages the rich semantic information of visual data and the
powerful reasoning capabilities of LLMs. Considering intention as a high-level
concept guiding the evolution of actions, we first propose to employ a
vision-language model (VLM) to infer behavioral intentions as comprehensive
textual features directly from video inputs. The inferred intentions are then
fused with visual features through a multi-modality fusion strategy, resulting
in intention-enhanced visual representations. These enhanced visual
representations, along with textual prompts, are fed into LLM for future action
anticipation. Furthermore, we propose an effective example selection strategy
jointly considers visual and textual similarities, providing more relevant and
informative examples for in-context learning. Extensive experiments with
state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+
datasets fully demonstrate the effectiveness and superiority of the proposed
method.

</details>


### [127] [Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes](https://arxiv.org/pdf/2505.01726)
*Jie Liu, Pan Zhou, Zehao Xiao, Jiayi Shen, Wenzhe Yin, Jan-Jakob Sonke, Efstratios Gavves*

Main category: cs.CV

TL;DR: NPISeg3D is a probabilistic framework using Neural Processes for interactive 3D segmentation, improving generalization from sparse clicks and quantifying uncertainty.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in generalizing from sparse user clicks and quantifying predictive uncertainty in 3D segmentation.

Method: Hierarchical latent variables (scene-specific and object-specific) and a probabilistic prototype modulator.

Result: Superior segmentation performance with fewer clicks and reliable uncertainty estimations on four datasets.

Conclusion: NPISeg3D effectively enhances 3D segmentation by combining generalization and uncertainty quantification.

Abstract: Interactive 3D segmentation has emerged as a promising solution for
generating accurate object masks in complex 3D scenes by incorporating
user-provided clicks. However, two critical challenges remain underexplored:
(1) effectively generalizing from sparse user clicks to produce accurate
segmentation, and (2) quantifying predictive uncertainty to help users identify
unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic
framework that builds upon Neural Processes (NPs) to address these challenges.
Specifically, NPISeg3D introduces a hierarchical latent variable structure with
scene-specific and object-specific latent variables to enhance few-shot
generalization by capturing both global context and object-specific
characteristics. Additionally, we design a probabilistic prototype modulator
that adaptively modulates click prototypes with object-specific latent
variables, improving the model's ability to capture object-aware context and
quantify predictive uncertainty. Experiments on four 3D point cloud datasets
demonstrate that NPISeg3D achieves superior segmentation performance with fewer
clicks while providing reliable uncertainty estimations.

</details>


### [128] [PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth](https://arxiv.org/pdf/2505.01729)
*Bu Jin, Weize Li, Baihan Yang, Zhenxin Zhu, Junpeng Jiang, Huan-ang Gao, Haiyang Sun, Kun Zhan, Hengtong Hu, Xueyang Zhang, Peng Jia, Hao Zhao*

Main category: cs.CV

TL;DR: PosePilot enhances camera pose control in generative world models for autonomous driving by leveraging self-supervised depth and pose estimation, improving viewpoint synthesis and motion reasoning.


<details>
  <summary>Details</summary>
Motivation: Precise and flexible camera pose control is crucial for accurate viewpoint transformation and realistic scene dynamics in autonomous driving systems.

Method: PosePilot uses self-supervised depth and pose estimation, incorporating photometric warping loss, reverse warping, and pose regression loss to refine camera pose control.

Result: Experiments show PosePilot improves structural understanding and motion reasoning in diffusion-based and auto-regressive world models, setting a new benchmark for pose controllability.

Conclusion: PosePilot enables physically consistent and reliable viewpoint synthesis in generative world models, advancing autonomous driving systems.

Abstract: Recent advancements in autonomous driving (AD) systems have highlighted the
potential of world models in achieving robust and generalizable performance
across both ordinary and challenging driving conditions. However, a key
challenge remains: precise and flexible camera pose control, which is crucial
for accurate viewpoint transformation and realistic simulation of scene
dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful
framework that significantly enhances camera pose controllability in generative
world models. Drawing inspiration from self-supervised depth estimation,
PosePilot leverages structure-from-motion principles to establish a tight
coupling between camera pose and video generation. Specifically, we incorporate
self-supervised depth and pose readouts, allowing the model to infer depth and
relative camera motion directly from video sequences. These outputs drive
pose-aware frame warping, guided by a photometric warping loss that enforces
geometric consistency across synthesized frames. To further refine camera pose
estimation, we introduce a reverse warping step and a pose regression loss,
improving viewpoint precision and adaptability. Extensive experiments on
autonomous driving and general-domain video datasets demonstrate that PosePilot
significantly enhances structural understanding and motion reasoning in both
diffusion-based and auto-regressive world models. By steering camera pose with
self-supervised depth, PosePilot sets a new benchmark for pose controllability,
enabling physically consistent, reliable viewpoint synthesis in generative
world models.

</details>


### [129] [Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes](https://arxiv.org/pdf/2505.01737)
*Seong Hyeon Park, Jinwoo Shin*

Main category: cs.CV

TL;DR: MMP is a new model for estimating 3D geometry in dynamic scenes from monocular videos, improving expressiveness and reducing regression error by 15.1%.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with noisy partial attributes and costly test-time optimizations for dynamic scenes.

Method: MMP uses a Siamese architecture with a trajectory encoding module to project point-wise dynamics for multi-frame pointmap representation.

Result: MMP achieves state-of-the-art quality with a 15.1% reduction in regression error.

Conclusion: MMP offers a more efficient and accurate feed-forward solution for dynamic scene geometry estimation.

Abstract: In monocular videos that capture dynamic scenes, estimating the 3D geometry
of video contents has been a fundamental challenge in computer vision.
Specifically, the task is significantly challenged by the object motion, where
existing models are limited to predict only partial attributes of the dynamic
scenes, such as depth or pointmaps spanning only over a pair of frames. Since
these attributes are inherently noisy under multiple frames, test-time global
optimizations are often employed to fully recover the geometry, which is liable
to failure and incurs heavy inference costs. To address the challenge, we
present a new model, coined MMP, to estimate the geometry in a feed-forward
manner, which produces a dynamic pointmap representation that evolves over
multiple frames. Specifically, based on the recent Siamese architecture, we
introduce a new trajectory encoding module to project point-wise dynamics on
the representation for each frame, which can provide significantly improved
expressiveness for dynamic scenes. In our experiments, we find MMP can achieve
state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%
enhancement in the regression error.

</details>


### [130] [An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding](https://arxiv.org/pdf/2505.01743)
*Siyang Jiang, Bufang Yang, Lilin Xu, Mu Yuan, Yeerzhati Abudunuer, Kaiwei Liu, Liekang Zeng, Hongkai Chen, Zhenyu Yan, Xiaofan Jiang, Guoliang Xing*

Main category: cs.CV

TL;DR: Llambda is a novel system for low-resolution human behavior understanding (HBU) using LVLMs, leveraging limited labeled and unlabeled data to generate high-quality captions and fine-tune models efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs struggle with low-resolution data (e.g., depth, thermal, infrared) as they are designed for high-resolution RGB images. Manual labeling is labor-intensive.

Method: Proposes Contrastive-Oriented Data Labeler for pseudo labels and Physical-Knowledge Guided Captioner to improve caption quality. Uses LoRA for efficient fine-tuning.

Result: Outperforms state-of-the-art LVLMs by up to 40.03% in Bert-Score on real-world low-resolution datasets.

Conclusion: Llambda effectively addresses the gap in LVLM performance for low-resolution HBU, offering a scalable and efficient solution.

Abstract: The rapid advancements in Large Vision Language Models (LVLMs) offer the
potential to surpass conventional labeling by generating richer, more detailed
descriptions of on-device human behavior understanding (HBU) in low-resolution
vision systems, such as depth, thermal, and infrared. However, existing large
vision language model (LVLM) approaches are unable to understand low-resolution
data well as they are primarily designed for high-resolution data, such as RGB
images. A quick fixing approach is to caption a large amount of low-resolution
data, but it requires a significant amount of labor-intensive annotation
efforts. In this paper, we propose a novel, labor-saving system, Llambda,
designed to support low-resolution HBU. The core idea is to leverage limited
labeled data and a large amount of unlabeled data to guide LLMs in generating
informative captions, which can be combined with raw data to effectively
fine-tune LVLM models for understanding low-resolution videos in HBU. First, we
propose a Contrastive-Oriented Data Labeler, which can capture
behavior-relevant information from long, low-resolution videos and generate
high-quality pseudo labels for unlabeled data via contrastive learning. Second,
we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and
temporal consistency checks to mitigate errors in pseudo labels. Therefore, it
can improve LLMs' understanding of sequential data and then generate
high-quality video captions. Finally, to ensure on-device deployability, we
employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.
We evaluate Llambda using a region-scale real-world testbed and three distinct
low-resolution datasets, and the experiments show that Llambda outperforms
several state-of-the-art LVLM systems up to $40.03\%$ on average Bert-Score.

</details>


### [131] [Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion](https://arxiv.org/pdf/2505.01746)
*Xingqun Qi, Yatian Wang, Hengyuan Zhang, Jiahao Pan, Wei Xue, Shanghang Zhang, Wenhan Luo, Qifeng Liu, Yike Guo*

Main category: cs.CV

TL;DR: The paper introduces Co$^3$Gesture, a framework for concurrent co-speech gesture synthesis in two-person conversations, and a new dataset, GES-Inter, to address the lack of data for this task.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single-person gesture synthesis, ignoring the practicality of two-person interactions. The lack of high-quality datasets for concurrent gestures also limits progress.

Method: The authors propose Co$^3$Gesture, a dual-branch framework conditioned on separate speaker audio, enhanced by a Temporal Interaction Module (TIM) and mutual attention mechanism for coordinated gesture generation.

Result: The method outperforms state-of-the-art models on the new GES-Inter dataset, demonstrating effectiveness in generating coherent concurrent gestures.

Conclusion: The work advances concurrent co-speech gesture synthesis, providing a novel framework and dataset for future research.

Abstract: Generating gestures from human speech has gained tremendous progress in
animating virtual avatars. While the existing methods enable synthesizing
gestures cooperated by individual self-talking, they overlook the practicality
of concurrent gesture modeling with two-person interactive conversations.
Moreover, the lack of high-quality datasets with concurrent co-speech gestures
also limits handling this issue. To fulfill this goal, we first construct a
large-scale concurrent co-speech gesture dataset that contains more than 7M
frames for diverse two-person interactive posture sequences, dubbed GES-Inter.
Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent
concurrent co-speech gesture synthesis including two-person interactive
movements. Considering the asymmetric body dynamics of two speakers, our
framework is built upon two cooperative generation branches conditioned on
separated speaker audio. Specifically, to enhance the coordination of human
postures with respect to corresponding speaker audios while interacting with
the conversational partner, we present a Temporal Interaction Module (TIM). TIM
can effectively model the temporal association representation between two
speakers' gesture sequences as interaction guidance and fuse it into the
concurrent gesture generation. Then, we devise a mutual attention mechanism to
further holistically boost learning dependencies of interacted concurrent
motions, thereby enabling us to generate vivid and coherent gestures. Extensive
experiments demonstrate that our method outperforms the state-of-the-art models
on our newly collected GES-Inter dataset. The dataset and source code are
publicly available at
\href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}.

</details>


### [132] [Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement](https://arxiv.org/pdf/2505.01766)
*Long Bai, Boyi Ma, Ruohan Wang, Guankun Wang, Beilei Cui, Zhongliang Jiang, Mobarakol Islam, Zhe Min, Jiewen Lai, Nassir Navab, Hongliang Ren*

Main category: cs.CV

TL;DR: A robust graph-based multimodal approach (GRAD) integrates vision and kinematic data to enhance surgical workflow recognition, addressing data corruption and domain shifts.


<details>
  <summary>Details</summary>
Motivation: Improving surgical workflow recognition for automation, decision-making, and training, despite challenges like data corruption and adverse conditions.

Method: Proposes GRAD, a multimodal Graph Representation network with adversarial feature disentanglement, combining vision and kinematic data through graph-based modeling and adversarial training.

Result: Demonstrates effectiveness in handling data corruption and domain shifts, with improved accuracy and robustness.

Conclusion: Advances automated surgical workflow recognition by addressing dynamic and complex surgical scenarios.

Abstract: Surgical workflow recognition is vital for automating tasks, supporting
decision-making, and training novice surgeons, ultimately improving patient
safety and standardizing procedures. However, data corruption can lead to
performance degradation due to issues like occlusion from bleeding or smoke in
surgical scenes and problems with data storage and transmission. In this case,
we explore a robust graph-based multimodal approach to integrating vision and
kinematic data to enhance accuracy and reliability. Vision data captures
dynamic surgical scenes, while kinematic data provides precise movement
information, overcoming limitations of visual recognition under adverse
conditions. We propose a multimodal Graph Representation network with
Adversarial feature Disentanglement (GRAD) for robust surgical workflow
recognition in challenging scenarios with domain shifts or corrupted data.
Specifically, we introduce a Multimodal Disentanglement Graph Network that
captures fine-grained visual information while explicitly modeling the complex
relationships between vision and kinematic embeddings through graph-based
message modeling. To align feature spaces across modalities, we propose a
Vision-Kinematic Adversarial framework that leverages adversarial training to
reduce modality gaps and improve feature consistency. Furthermore, we design a
Contextual Calibrated Decoder, incorporating temporal and contextual priors to
enhance robustness against domain shifts and corrupted data. Extensive
comparative and ablation experiments demonstrate the effectiveness of our model
and proposed modules. Moreover, our robustness experiments show that our method
effectively handles data corruption during storage and transmission, exhibiting
excellent stability and robustness. Our approach aims to advance automated
surgical workflow recognition, addressing the complexities and dynamism
inherent in surgical procedures.

</details>


### [133] [AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting](https://arxiv.org/pdf/2505.01799)
*Junhao Shi, Jisheng Xu, Jianping He, Zhiliang Lin*

Main category: cs.CV

TL;DR: AquaGS is an SfM-free underwater scene reconstruction model using SeaThru, MVS, NeRF, and 3DGS for fast, high-precision reconstruction with minimal input.


<details>
  <summary>Details</summary>
Motivation: Underwater image degradation and slow SfM methods limit reconstruction quality and real-time applicability.

Method: Integrates MVS for initialization, NeRF for translucent media, and 3DGS for object surfaces.

Result: Achieves high-precision reconstruction in 30 seconds with just 3 images.

Conclusion: AquaGS overcomes traditional limitations, enhancing practical use in robotics.

Abstract: Underwater scene reconstruction is a critical tech-nology for underwater
operations, enabling the generation of 3D models from images captured by
underwater platforms. However, the quality of underwater images is often
degraded due to medium interference, which limits the effectiveness of
Structure-from-Motion (SfM) pose estimation, leading to subsequent
reconstruction failures. Additionally, SfM methods typically operate at slower
speeds, further hindering their applicability in real-time scenarios. In this
paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model
based on the SeaThru algorithm, which facilitates rapid and accurate separation
of scene details and medium features. Our approach initializes Gaussians by
integrating state-of-the-art multi-view stereo (MVS) technology, employs
implicit Neural Radiance Fields (NeRF) for rendering translucent media and
utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render
object surfaces, which effectively addresses the limitations of traditional
methods and accurately simulates underwater optical phenomena. Experimental
results on the data set and the robot platform show that our model can complete
high-precision reconstruction in 30 seconds with only 3 image inputs,
significantly enhancing the practical application of the algorithm in robotic
platforms.

</details>


### [134] [Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows](https://arxiv.org/pdf/2505.01802)
*Georgios Fotios Angelis, Savas Ozkan, Sinan Mutlu, Paul Wisbey, Anastasios Drosou, Mete Ozay*

Main category: cs.CV

TL;DR: A novel MLP-based method improves 3D full-body reconstruction in AR/VR by splitting long input sequences into smaller windows, enhancing accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Efficient NN models are needed for seamless AR/VR experiences, but current methods are computationally expensive and noisy due to long input sequences.

Method: Proposes an MLP-based approach dividing long sequences into smaller temporal windows and merging them via latent representations.

Result: Achieves higher generation accuracy and lower computational costs compared to state-of-the-art methods.

Conclusion: The method is efficient for resource-constrained devices, improving AR/VR full-body reconstruction.

Abstract: To have a seamless user experience on immersive AR/VR applications, the
importance of efficient and effective Neural Network (NN) models is undeniable,
since missing body parts that cannot be captured by limited sensors should be
generated using these models for a complete 3D full-body reconstruction in
virtual environment. However, the state-of-the-art NN-models are typically
computational expensive and they leverage longer sequences of sparse tracking
inputs to generate full-body movements by capturing temporal context.
Inevitably, longer sequences increase the computation overhead and introduce
noise in longer temporal dependencies that adversely affect the generation
performance. In this paper, we propose a novel Multi-Layer Perceptron
(MLP)-based method that enhances the overall performance while balancing the
computational cost and memory overhead for efficient 3D full-body generation.
Precisely, we introduce a NN-mechanism that divides the longer sequence of
inputs into smaller temporal windows. Later, the current motion is merged with
the information from these windows through latent representations to utilize
the past context for the generation. Our experiments demonstrate that
generation accuracy of our method with this NN-mechanism is significantly
improved compared to the state-of-the-art methods while greatly reducing
computational costs and memory overhead, making our method suitable for
resource-constrained devices.

</details>


### [135] [Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing](https://arxiv.org/pdf/2505.01805)
*Yuchang Jiang, Maxim Neumann*

Main category: cs.CV

TL;DR: ForTy is a global-scale benchmark for forest types mapping using multi-temporal satellite data, differentiating natural forest, planted forest, and tree crops. A novel transformer-based model outperforms baseline models.


<details>
  <summary>Details</summary>
Motivation: Accurate forest types mapping is crucial for deforestation prevention and biodiversity conservation, addressing limitations of existing land use products.

Method: ForTy includes 200,000 time series of multi-modal satellite data (Sentinel-2, Sentinel-1, climate, elevation). A transformer-based model is proposed for multi-temporal data.

Result: The proposed transformer-based model outperforms baseline models like CNNs.

Conclusion: ForTy provides a robust benchmark for forest types mapping, with the novel model showing superior performance.

Abstract: Developing accurate and reliable models for forest types mapping is critical
to support efforts for halting deforestation and for biodiversity conservation
(such as European Union Deforestation Regulation (EUDR)). This work introduces
ForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal
satellite data1. The benchmark comprises 200,000 time series of image patches,
each consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each
time series captures variations at monthly or seasonal cadence. Per-pixel
annotations, including forest types and other land use classes, support image
segmentation tasks. Unlike most existing land use products that often
categorize all forest areas into a single class, our benchmark differentiates
between three forest types classes: natural forest, planted forest, and tree
crops. By leveraging multiple public data sources, we achieve global coverage
with this benchmark. We evaluate the forest types dataset using several
baseline models, including convolution neural networks and transformer-based
models. Additionally, we propose a novel transformer-based model specifically
designed to handle multi-modal, multi-temporal satellite data for forest types
mapping. Our experimental results demonstrate that the proposed model surpasses
the baseline models in performance.

</details>


### [136] [3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment](https://arxiv.org/pdf/2505.01809)
*Xiaoqi Li, Jiaming Liu, Nuowei Han, Liang Heng, Yandong Guo, Hao Dong, Yang Liu*

Main category: cs.CV

TL;DR: A weakly-supervised 3D visual grounding method addresses category-level ambiguity and instance-level complexity by leveraging pre-trained detectors and spatial language descriptions, achieving top results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To localize 3D objects in point clouds using natural language without annotations, overcoming challenges like category ambiguity and instance complexity.

Method: Uses a dual-branch approach: a category-level branch aligns object proposals with sentence-level features, and an instance-level branch refines proposals using spatial language cues.

Result: Achieves state-of-the-art performance on Nr3D, Sr3D, and ScanRef benchmarks.

Conclusion: The proposed method effectively handles category and instance challenges, outperforming prior approaches in weakly-supervised 3D visual grounding.

Abstract: The 3D weakly-supervised visual grounding task aims to localize oriented 3D
boxes in point clouds based on natural language descriptions without requiring
annotations to guide model learning. This setting presents two primary
challenges: category-level ambiguity and instance-level complexity.
Category-level ambiguity arises from representing objects of fine-grained
categories in a highly sparse point cloud format, making category distinction
challenging. Instance-level complexity stems from multiple instances of the
same category coexisting in a scene, leading to distractions during grounding.
To address these challenges, we propose a novel weakly-supervised grounding
approach that explicitly differentiates between categories and instances. In
the category-level branch, we utilize extensive category knowledge from a
pre-trained external detector to align object proposal features with
sentence-level category features, thereby enhancing category awareness. In the
instance-level branch, we utilize spatial relationship descriptions from
language queries to refine object proposal features, ensuring clear
differentiation among objects. These designs enable our model to accurately
identify target-category objects while distinguishing instances within the same
category. Compared to previous methods, our approach achieves state-of-the-art
performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.

</details>


### [137] [PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach](https://arxiv.org/pdf/2505.01823)
*Nitin Rai, Arnold W. Schumann, Nathan Boyd*

Main category: cs.CV

TL;DR: The paper explores a multi-modal text-to-image approach using Stable Diffusion variants for generating synthetic crop disease images, with SD3.5M emerging as the most efficient model in terms of memory, power, and energy usage.


<details>
  <summary>Details</summary>
Motivation: Collecting real-world crop disease images is resource-intensive, and existing GAN-based methods lack computational analysis in agriculture.

Method: Three Stable Diffusion variants (SDXL, SD3.5M, SD3.5L) were trained and fine-tuned using Dreambooth and LoRA techniques.

Result: SD3.5M outperformed others, using 18 GB memory, 180 W power, and 1.02 kWh/500 images, generating 500 synthetic images from 36 samples in 1.5 hours.

Conclusion: SD3.5M is recommended for efficient synthetic crop disease data generation due to its performance and computational efficiency.

Abstract: Collecting large-scale crop disease images in the field is labor-intensive
and time-consuming. Generative models (GMs) offer an alternative by creating
synthetic samples that resemble real-world images. However, existing research
primarily relies on Generative Adversarial Networks (GANs)-based image-to-image
translation and lack a comprehensive analysis of computational requirements in
agriculture. Therefore, this research explores a multi-modal text-to-image
approach for generating synthetic crop disease images and is the first to
provide computational benchmarking in this context. We trained three Stable
Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and
fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning
techniques to enhance generalization. SD3.5M outperformed the others, with an
average memory usage of 18 GB, power consumption of 180 W, and total energy use
of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results
demonstrate SD3.5M's ability to generate 500 synthetic images from just 36
in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease
data generation.

</details>


### [138] [Deep Implicit Optimization enables Robust Learnable Features for Deformable Image Registration](https://arxiv.org/pdf/2406.07361)
*Rohit Jena, Pratik Chaudhari, James C. Gee*

Main category: cs.CV

TL;DR: The paper introduces a deep learning method that integrates optimization as a layer in a network to improve image registration, addressing domain shift and enabling flexible transformation representations.


<details>
  <summary>Details</summary>
Motivation: Existing DLIR methods lack task-specific inductive biases and invariances of optimization, leading to suboptimal performance, especially under domain shift.

Method: A deep network predicts multi-scale dense features, registered using an iterative solver. The optimal warp minimizes alignment errors, with end-to-end differentiation through the solver.

Result: The method performs well on in-domain datasets, handles domain shift, and allows flexible transformation representations at test time without retraining.

Conclusion: The framework bridges statistical learning and optimization, improving registration performance, interpretability, and flexibility.

Abstract: Deep Learning in Image Registration (DLIR) methods have been tremendously
successful in image registration due to their speed and ability to incorporate
weak label supervision at training time. However, existing DLIR methods forego
many of the benefits and invariances of optimization methods. The lack of a
task-specific inductive bias in DLIR methods leads to suboptimal performance,
especially in the presence of domain shift. Our method aims to bridge this gap
between statistical learning and optimization by explicitly incorporating
optimization as a layer in a deep network. A deep network is trained to predict
multi-scale dense feature images that are registered using a black box
iterative optimization solver. This optimal warp is then used to minimize image
and label alignment errors. By implicitly differentiating end-to-end through an
iterative optimization solver, we explicitly exploit invariances of the
correspondence matching problem induced by the optimization, while learning
registration and label-aware features, and guaranteeing the warp functions to
be a local minima of the registration objective in the feature space. Our
framework shows excellent performance on in-domain datasets, and is agnostic to
domain shift such as anisotropy and varying intensity profiles. For the first
time, our method allows switching between arbitrary transformation
representations (free-form to diffeomorphic) at test time with zero retraining.
End-to-end feature learning also facilitates interpretability of features and
arbitrary test-time regularization, which is not possible with existing DLIR
methods.

</details>


### [139] [CVVNet: A Cross-Vertical-View Network for Gait Recognition](https://arxiv.org/pdf/2505.01837)
*Xiangru Li, Wei Song, Yingda Huang, Wei Meng, Le Chang*

Main category: cs.CV

TL;DR: CVVNet improves cross-vertical-view gait recognition with multi-frequency feature extraction and dynamic fusion, outperforming existing methods by 8.6% on DroneGait and 2% on Gait3D.


<details>
  <summary>Details</summary>
Motivation: Existing gait recognition methods fail in cross-vertical-view scenarios due to deformations and occlusions, requiring a robust solution.

Method: CVVNet uses High-Low Frequency Extraction (HLFE) and Dynamic Gated Aggregation (DGA) for multi-frequency feature integration.

Result: CVVNet achieves state-of-the-art performance with 8.6% and 2% improvements on DroneGait and Gait3D datasets.

Conclusion: CVVNet effectively addresses cross-vertical-view challenges, enhancing gait recognition robustness.

Abstract: Gait recognition enables contact-free, long-range person identification that
is robust to clothing variations and non-cooperative scenarios. While existing
methods perform well in controlled indoor environments, they struggle with
cross-vertical view scenarios, where surveillance angles vary significantly in
elevation. Our experiments show up to 60\% accuracy degradation in low-to-high
vertical view settings due to severe deformations and self-occlusions of key
anatomical features. Current CNN and self-attention-based methods fail to
effectively handle these challenges, due to their reliance on single-scale
convolutions or simplistic attention mechanisms that lack effective
multi-frequency feature integration. To tackle this challenge, we propose
CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture
specifically designed for robust cross-vertical-view gait recognition. CVVNet
employs a High-Low Frequency Extraction module (HLFE) that adopts parallel
multi-scale convolution/max-pooling path and self-attention path as high- and
low-frequency mixers for effective multi-frequency feature extraction from
input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)
mechanism to adaptively adjust the fusion ratio of high- and low-frequency
features. The integration of our core Multi-Scale Attention Gated Aggregation
(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions
from view changes, significantly improving the recognition robustness across
different vertical views. Experimental results show that our CVVNet achieves
state-of-the-art performance, with $8.6\%$ improvement on DroneGait and $2\%$
on Gait3D compared with the best existing methods.

</details>


### [140] [A Comprehensive Survey on Machine Learning Driven Material Defect Detection](https://arxiv.org/pdf/2406.07880)
*Jun Bai, Di Wu, Tristan Shelley, Peter Schubel, David Twine, John Russell, Xuesen Zeng, Ji Zhang*

Main category: cs.CV

TL;DR: The paper reviews machine learning (ML) techniques for material defect detection (MDD), categorizing them into five types and analyzing their principles, advantages, and challenges, with a focus on composite materials. It also explores future ML-based MDD directions.


<details>
  <summary>Details</summary>
Motivation: Material defects (MD) impact product performance and safety, necessitating rapid and accurate detection. ML, especially deep learning, has become central to MDD research.

Method: A systematic review of ML techniques in MDD, categorized into unsupervised, supervised, semi-supervised, reinforcement, and generative learning, with analysis of principles and challenges.

Result: The survey highlights ML's role in MDD, particularly for composite materials, and identifies potential future research directions.

Conclusion: The paper consolidates ML-based MDD literature, providing a foundation for future research and practical applications.

Abstract: Material defects (MD) represent a primary challenge affecting product
performance and giving rise to safety issues in related products. The rapid and
accurate identification and localization of MD constitute crucial research
endeavors in addressing contemporary challenges associated with MD. In recent
years, propelled by the swift advancement of machine learning (ML)
technologies, particularly exemplified by deep learning, ML has swiftly emerged
as the core technology and a prominent research direction for material defect
detection (MDD). Through a comprehensive review of the latest literature, we
systematically survey the ML techniques applied in MDD into five categories:
unsupervised learning, supervised learning, semi-supervised learning,
reinforcement learning, and generative learning. We provide a detailed analysis
of the main principles and techniques used, together with the advantages and
potential challenges associated with these techniques. Furthermore, the survey
focuses on the techniques for defect detection in composite materials, which
are important types of materials enjoying increasingly wide application in
various industries such as aerospace, automotive, construction, and renewable
energy. Finally, the survey explores potential future directions in MDD
utilizing ML technologies. This survey consolidates ML-based MDD literature and
provides a foundation for future research and practice.

</details>


### [141] [MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization](https://arxiv.org/pdf/2505.01838)
*Chenghong Li, Hongjie Liao, Yihao Zhi, Xihe Yang, Zhengwentai Sun, Jiahao Chang, Shuguang Cui, Xiaoguang Han*

Main category: cs.CV

TL;DR: MVHumanNet++ is introduced as the largest-scale 3D human dataset to address the lack of large-scale human-centric data, featuring 4,500 identities, 9,000 outfits, and 645 million frames with extensive annotations.


<details>
  <summary>Details</summary>
Motivation: The absence of a large-scale human dataset has hindered progress in human-centric 3D vision tasks, prompting the creation of MVHumanNet++.

Method: The dataset was collected using multi-view human capture systems, focusing on diverse identities and everyday clothing, and includes annotations like masks, keypoints, and textual descriptions.

Result: MVHumanNet++ enhances research capabilities with its scale and annotations, as demonstrated in pilot studies for 2D and 3D tasks.

Conclusion: MVHumanNet++ aims to advance 3D human-centric research and is publicly available for broader innovation.

Abstract: In this era, the success of large language models and text-to-image models
can be attributed to the driving force of large-scale datasets. However, in the
realm of 3D vision, while significant progress has been achieved in
object-centric tasks through large-scale datasets like Objaverse and MVImgNet,
human-centric tasks have seen limited advancement, largely due to the absence
of a comparable large-scale human dataset. To bridge this gap, we present
MVHumanNet++, a dataset that comprises multi-view human action sequences of
4,500 human identities. The primary focus of our work is on collecting human
data that features a large number of diverse identities and everyday clothing
using multi-view human capture systems, which facilitates easily scalable data
collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences
and 645 million frames with extensive annotations, including human masks,
camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and
corresponding textual descriptions. Additionally, the proposed MVHumanNet++
dataset is enhanced with newly processed normal maps and depth maps,
significantly expanding its applicability and utility for advanced
human-centric research. To explore the potential of our proposed MVHumanNet++
dataset in various 2D and 3D visual tasks, we conducted several pilot studies
to demonstrate the performance improvements and effective applications enabled
by the scale provided by MVHumanNet++. As the current largest-scale 3D human
dataset, we hope that the release of MVHumanNet++ dataset with annotations will
foster further innovations in the domain of 3D human-centric tasks at scale.
MVHumanNet++ is publicly available at
https://kevinlee09.github.io/research/MVHumanNet++/.

</details>


### [142] [Underwater Image Enhancement via Dehazing and Color Restoration](https://arxiv.org/pdf/2409.09779)
*Chengqin Wu, Shuai Yu, Tuyan Luo, Qiuhua Rao, Qingson Hu, Jingxiang Xu, Lijun Zhang*

Main category: cs.CV

TL;DR: WaterFormer, a ViT-based network, enhances underwater images by separately addressing haze and color cast issues, integrating them dynamically, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Underwater imaging suffers from low contrast, blurriness, and color degradation, with existing methods treating haze and color cast as unified processes, neglecting their independence and synergy.

Method: WaterFormer uses a dehazing block (DehazeFormer Block), a Color Restoration Block (CRB), and a Channel Fusion Block (CFB) to separately and synergistically address haze and color cast. It includes a soft reconstruction layer and employs Chromatic Consistency Loss and Sobel Color Loss for training.

Result: WaterFormer outperforms state-of-the-art methods in enhancing underwater images.

Conclusion: The proposed WaterFormer effectively addresses underwater image degradation by decoupling and dynamically integrating haze and color cast features, achieving superior enhancement results.

Abstract: Underwater visual imaging is crucial for marine engineering, but it suffers
from low contrast, blurriness, and color degradation, which hinders downstream
analysis. Existing underwater image enhancement methods often treat the haze
and color cast as a unified degradation process, neglecting their inherent
independence while overlooking their synergistic relationship. To overcome this
limitation, we propose a Vision Transformer (ViT)-based network (referred to as
WaterFormer) to improve underwater image quality. WaterFormer contains three
major components: a dehazing block (DehazeFormer Block) to capture the
self-correlated haze features and extract deep-level features, a Color
Restoration Block (CRB) to capture self-correlated color cast features, and a
Channel Fusion Block (CFB) that dynamically integrates these decoupled features
to achieve comprehensive enhancement. To ensure authenticity, a soft
reconstruction layer based on the underwater imaging physics model is included.
Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to
respectively preserve color fidelity and enhance structural details during
network training. Comprehensive experimental results demonstrate that
WaterFormer outperforms other state-of-the-art methods in enhancing underwater
images.

</details>


### [143] [Mitigating Group-Level Fairness Disparities in Federated Visual Language Models](https://arxiv.org/pdf/2505.01851)
*Chaomeng Chen, Zitong Yu, Junhao Dong, Sen Su, Linlin Shen, Shutao Xia, Xiaochun Cao*

Main category: cs.CV

TL;DR: FVL-FP is a framework for improving fairness in federated visual language models (VLMs) using fair prompt tuning, reducing demographic bias by 45% while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing group fairness challenges in federated VLMs, especially under non-IID data distributions.

Method: Introduces three components: CDFP (counterfactual regularization), DSOP (demographic subspace projection), and FPF (fair-aware prompt fusion).

Result: Reduces demographic disparity by 45% on average, with task performance within 6% of state-of-the-art.

Conclusion: FVL-FP provides a parameter-efficient solution for equitable performance in privacy-preserving multimodal systems.

Abstract: Visual language models (VLMs) have shown remarkable capabilities in
multimodal tasks but face challenges in maintaining fairness across demographic
groups, particularly when deployed in federated learning (FL) environments.
This paper addresses the critical issue of group fairness in federated VLMs by
introducing FVL-FP, a novel framework that combines FL with fair prompt tuning
techniques. We focus on mitigating demographic biases while preserving model
performance through three innovative components: (1) Cross-Layer Demographic
Fair Prompting (CDFP), which adjusts potentially biased embeddings through
counterfactual regularization; (2) Demographic Subspace Orthogonal Projection
(DSOP), which removes demographic bias in image representations by mapping fair
prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which
dynamically balances client contributions based on both performance and
fairness metrics. Extensive evaluations across four benchmark datasets
demonstrate that our approach reduces demographic disparity by an average of
45\% compared to standard FL approaches, while maintaining task performance
within 6\% of state-of-the-art results. FVL-FP effectively addresses the
challenges of non-IID data distributions in federated settings and introduces
minimal computational overhead while providing significant fairness benefits.
Our work presents a parameter-efficient solution to the critical challenge of
ensuring equitable performance across demographic groups in privacy-preserving
multimodal systems.

</details>


### [144] [DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion](https://arxiv.org/pdf/2505.01857)
*Haoteng Li, Zhao Yang, Zezhong Qian, Gongpeng Zhao, Yuqi Huang, Jun Yu, Huazheng Zhou, Longjun Liu*

Main category: cs.CV

TL;DR: DualDiff, a dual-branch diffusion model, improves driving scene generation by using Occupancy Ray Sampling and Semantic Fusion Attention for better multi-modal integration and tiny object generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for driving scene reconstruction lack scene complexity and multi-modal integration, relying on simplistic 3D bounding boxes and binary maps.

Method: DualDiff introduces Occupancy Ray Sampling (ORS) for semantic-rich 3D representation and Semantic Fusion Attention (SFA) for cross-modal feature alignment. It also uses a foreground-aware masked loss (FGM) for tiny object generation.

Result: DualDiff achieves state-of-the-art FID scores and improves performance in BEV segmentation and 3D object detection tasks.

Conclusion: DualDiff effectively enhances driving scene generation by addressing multi-modal integration and tiny object generation, outperforming existing approaches.

Abstract: Accurate and high-fidelity driving scene reconstruction relies on fully
leveraging scene information as conditioning. However, existing approaches,
which primarily use 3D bounding boxes and binary maps for foreground and
background control, fall short in capturing the complexity of the scene and
integrating multi-modal information. In this paper, we propose DualDiff, a
dual-branch conditional diffusion model designed to enhance multi-view driving
scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D
representation, alongside numerical driving scene representation, for
comprehensive foreground and background control. To improve cross-modal
information integration, we propose a Semantic Fusion Attention (SFA) mechanism
that aligns and fuses features across modalities. Furthermore, we design a
foreground-aware masked (FGM) loss to enhance the generation of tiny objects.
DualDiff achieves state-of-the-art performance in FID score, as well as
consistently better results in downstream BEV segmentation and 3D object
detection tasks.

</details>


### [145] [Visual enhancement and 3D representation for underwater scenes: a review](https://arxiv.org/pdf/2505.01869)
*Guoxi Huang, Haoran Wang, Brett Seymour, Evan Kovacs, John Ellerbrock, Dave Blackham, Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: The paper provides a comprehensive review of underwater visual enhancement (UVE) and 3D reconstruction, covering physical models, advanced methods, and evaluations of state-of-the-art techniques.


<details>
  <summary>Details</summary>
Motivation: Address the lack of a systematic review for UVE and underwater 3D reconstruction, given the challenges posed by aquatic environments.

Method: Survey and evaluate methods from non-learning to data-driven techniques, including Neural Radiance Fields and 3D Gaussian Splatting.

Result: Quantitative and qualitative assessments of algorithms on benchmark datasets, highlighting their effectiveness.

Conclusion: Identifies key future research directions to advance underwater vision technologies.

Abstract: Underwater visual enhancement (UVE) and underwater 3D reconstruction pose
significant challenges in
  computer vision and AI-based tasks due to complex imaging conditions in
aquatic environments. Despite
  the development of numerous enhancement algorithms, a comprehensive and
systematic review covering both
  UVE and underwater 3D reconstruction remains absent. To advance research in
these areas, we present an
  in-depth review from multiple perspectives. First, we introduce the
fundamental physical models, highlighting the
  peculiarities that challenge conventional techniques. We survey advanced
methods for visual enhancement and
  3D reconstruction specifically designed for underwater scenarios. The paper
assesses various approaches from
  non-learning methods to advanced data-driven techniques, including Neural
Radiance Fields and 3D Gaussian
  Splatting, discussing their effectiveness in handling underwater distortions.
Finally, we conduct both quantitative
  and qualitative evaluations of state-of-the-art UVE and underwater 3D
reconstruction algorithms across multiple
  benchmark datasets. Finally, we highlight key research directions for future
advancements in underwater vision.

</details>


### [146] [CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture](https://arxiv.org/pdf/2505.01882)
*Vladimir Frants, Sos Agaian, Karen Panetta, Peter Huang*

Main category: cs.CV

TL;DR: CMAWRNet is a novel quaternion neural architecture for removing multiple adverse weather conditions from images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Real-world applications like image retrieval and autonomous driving face challenges due to poor weather conditions, but current methods struggle with combined degradations like haze and rain.

Method: Uses a unified quaternion neural architecture with texture-structure decomposition, a lightweight encoder-decoder transformer, and attentive fusion with low-light correction. Introduces a quaternion similarity loss for color preservation.

Result: Outperforms state-of-the-art methods on benchmarking datasets and real-world images, improving downstream tasks like object detection.

Conclusion: CMAWRNet is the first to apply decomposition to universal weather removal, offering superior performance for multiple weather artifacts.

Abstract: Images used in real-world applications such as image or video retrieval,
outdoor surveillance, and autonomous driving suffer from poor weather
conditions. When designing robust computer vision systems, removing adverse
weather such as haze, rain, and snow is a significant problem. Recently,
deep-learning methods offered a solution for a single type of degradation.
Current state-of-the-art universal methods struggle with combinations of
degradations, such as haze and rain-streak. Few algorithms have been developed
that perform well when presented with images containing multiple adverse
weather conditions. This work focuses on developing an efficient solution for
multiple adverse weather removal using a unified quaternion neural architecture
called CMAWRNet. It is based on a novel texture-structure decomposition block,
a novel lightweight encoder-decoder quaternion transformer architecture, and an
attentive fusion block with low-light correction. We also introduce a
quaternion similarity loss function to preserve color information better. The
quantitative and qualitative evaluation of the current state-of-the-art
benchmarking datasets and real-world images shows the performance advantages of
the proposed CMAWRNet compared to other state-of-the-art weather removal
approaches dealing with multiple weather artifacts. Extensive computer
simulations validate that CMAWRNet improves the performance of downstream
applications such as object detection. This is the first time the decomposition
approach has been applied to the universal weather removal task.

</details>


### [147] [Rethinking Score Distilling Sampling for 3D Editing and Generation](https://arxiv.org/pdf/2505.01888)
*Xingyu Miao, Haoran Duan, Yang Long, Jungong Han*

Main category: cs.CV

TL;DR: UDS unifies 3D generation and editing by refining gradient terms in SDS, outperforming baselines in both tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods like SDS lack editing capabilities, while variants struggle with generation. UDS aims to bridge this gap.

Method: UDS refines gradient terms in SDS to support both generation and editing of 3D assets.

Result: UDS outperforms baselines in generating detailed 3D assets and excels in editing tasks.

Conclusion: UDS successfully unifies 3D generation and editing, offering improved performance in both areas.

Abstract: Score Distillation Sampling (SDS) has emerged as a prominent method for
text-to-3D generation by leveraging the strengths of 2D diffusion models.
However, SDS is limited to generation tasks and lacks the capability to edit
existing 3D assets. Conversely, variants of SDS that introduce editing
capabilities often can not generate new 3D assets effectively. In this work, we
observe that the processes of generation and editing within SDS and its
variants have unified underlying gradient terms. Building on this insight, we
propose Unified Distillation Sampling (UDS), a method that seamlessly
integrates both the generation and editing of 3D assets. Essentially, UDS
refines the gradient terms used in vanilla SDS methods, unifying them to
support both tasks. Extensive experiments demonstrate that UDS not only
outperforms baseline methods in generating 3D assets with richer details but
also excels in editing tasks, thereby bridging the gap between 3D generation
and editing. The code is available on: https://github.com/xingy038/UDS.

</details>


### [148] [GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting](https://arxiv.org/pdf/2505.01928)
*Anushka Agarwal, Muhammad Yusuf Hassan, Talha Chafekar*

Main category: cs.CV

TL;DR: GenSync is a framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting, enabling unified training for multiple speakers with reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing 3D methods require separate models for each identity, which is inefficient. GenSync aims to unify this process.

Method: Uses a Disentanglement Module to separate identity-specific features from audio representations, enabling multi-identity synthesis.

Result: Achieves 6.8x faster training than state-of-the-art models while maintaining high lip-sync accuracy and visual quality.

Conclusion: GenSync offers an efficient and unified solution for multi-identity lip-synced video synthesis.

Abstract: We introduce GenSync, a novel framework for multi-identity lip-synced video
synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that
require training a new model for each identity , GenSync learns a unified
network that synthesizes lip-synced videos for multiple speakers. By
incorporating a Disentanglement Module, our approach separates
identity-specific features from audio representations, enabling efficient
multi-identity video synthesis. This design reduces computational overhead and
achieves 6.8x faster training compared to state-of-the-art models, while
maintaining high lip-sync accuracy and visual quality.

</details>


### [149] [GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels](https://arxiv.org/pdf/2505.01934)
*Yongxin Su, Lin Chen, Kaiting Zhang, Zhongliang Zhao, Chenfeng Hou, Ziping Yu*

Main category: cs.CV

TL;DR: GauS-SLAM is a dense RGB-D SLAM system using 2D Gaussian surfels for robust tracking and high-fidelity mapping, addressing geometry distortion with a novel reconstruction strategy and depth rendering mechanism.


<details>
  <summary>Details</summary>
Motivation: Geometry distortion in Gaussian-based scene representations under novel viewpoints degrades tracking accuracy, necessitating improved methods.

Method: Proposes a 2D Gaussian-based incremental reconstruction strategy and Surface-aware Depth Rendering to enhance accuracy and consistency. Also introduces a local map design to isolate visible surfaces dynamically.

Result: Outperforms comparable methods in tracking precision and rendering fidelity across multiple datasets.

Conclusion: GauS-SLAM effectively addresses geometry inconsistencies and improves SLAM performance, with potential for further applications.

Abstract: We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian
surfels to achieve robust tracking and high-fidelity mapping. Our
investigations reveal that Gaussian-based scene representations exhibit
geometry distortion under novel viewpoints, which significantly degrades the
accuracy of Gaussian-based tracking methods. These geometry inconsistencies
arise primarily from the depth modeling of Gaussian primitives and the mutual
interference between surfaces during the depth blending. To address these, we
propose a 2D Gaussian-based incremental reconstruction strategy coupled with a
Surface-aware Depth Rendering mechanism, which significantly enhances geometry
accuracy and multi-view consistency. Additionally, the proposed local map
design dynamically isolates visible surfaces during tracking, mitigating
misalignment caused by occluded regions in global maps while maintaining
computational efficiency with increasing Gaussian density. Extensive
experiments across multiple datasets demonstrate that GauS-SLAM outperforms
comparable methods, delivering superior tracking precision and rendering
fidelity. The project page will be made available at
https://gaus-slam.github.io.

</details>


### [150] [HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder](https://arxiv.org/pdf/2505.01938)
*Qi Yang, Le Yang, Geert Van Der Auwera, Zhu Li*

Main category: cs.CV

TL;DR: HybridGS is a new 3DGS compression framework combining compact generation and standardized point cloud encoding for faster, deployable compression.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS compression methods are slow and use non-standard formats, hindering deployment. HybridGS aims to address this.

Method: HybridGS generates compact 3DGS data, uses dual-channel sparse representation, and employs a canonical point cloud encoder with rate control.

Result: HybridGS achieves comparable reconstruction to state-of-the-art methods with faster encoding/decoding.

Conclusion: HybridGS offers a practical, efficient alternative for 3DGS compression, though it does not yet improve generation quality.

Abstract: Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on
producing compact 3DGS representation via implicit data embedding. They have
long coding times and highly customized data format, making it difficult for
widespread deployment. This paper presents a new 3DGS compression framework
called HybridGS, which takes advantage of both compact generation and
standardized point cloud data encoding. HybridGS first generates compact and
explicit 3DGS data. A dual-channel sparse representation is introduced to
supervise the primitive position and feature bit depth. It then utilizes a
canonical point cloud encoder to perform further data compression and form
standard output bitstreams. A simple and effective rate control scheme is
proposed to pivot the interpretable data compression scheme. At the current
stage, HybridGS does not include any modules aimed at improving 3DGS quality
during generation. But experiment results show that it still provides
comparable reconstruction performance against state-of-the-art methods, with
evidently higher encoding and decoding speed. The code is publicly available at
https://github.com/Qi-Yangsjtu/HybridGS.

</details>


### [151] [Segment Any RGB-Thermal Model with Language-aided Distillation](https://arxiv.org/pdf/2505.01950)
*Dong Xing, Xianxun Zhu, Wei Zhou, Qika Lin, Hang Yang, Yuqing Wang*

Main category: cs.CV

TL;DR: SARTM adapts SAM for RGB-T semantic segmentation by fine-tuning with LoRA layers, adding language guidance, and using CMKD for cross-modal adaptation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SAM's limitation to RGB data restricts its use in RGB-T scenarios, which are crucial for adverse conditions like low light. SARTM aims to bridge this gap.

Method: Fine-tunes SAM with LoRA layers, introduces language guidance, and employs CMKD for modality adaptation. Enhances segmentation with adjusted heads and multi-scale feature fusion.

Result: SARTM outperforms state-of-the-art methods on RGB-T benchmarks (MFNET, PST900, FMB) in various conditions.

Conclusion: SARTM successfully adapts SAM for RGB-T segmentation, improving performance and generalization across diverse conditions.

Abstract: The recent Segment Anything Model (SAM) demonstrates strong instance
segmentation performance across various downstream tasks. However, SAM is
trained solely on RGB data, limiting its direct applicability to RGB-thermal
(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for
scene understanding in adverse weather and lighting conditions, such as low
light and overexposure, we propose a novel framework, SARTM, which customizes
the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash
the potential of SAM while introduce semantic understanding modules for RGB-T
data pairs. Specifically, our framework first involves fine tuning the original
SAM by adding extra LoRA layers, aiming at preserving SAM's strong
generalization and segmentation capabilities for downstream tasks. Secondly, we
introduce language information as guidance for training our SARTM. To address
cross-modal inconsistencies, we introduce a Cross-Modal Knowledge
Distillation(CMKD) module that effectively achieves modality adaptation while
maintaining its generalization capabilities. This semantic module enables the
minimization of modality gaps and alleviates semantic ambiguity, facilitating
the combination of any modality under any visual conditions. Furthermore, we
enhance the segmentation performance by adjusting the segmentation head of SAM
and incorporating an auxiliary semantic segmentation head, which integrates
multi-scale features for effective fusion. Extensive experiments are conducted
across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,
and FMB. Both quantitative and qualitative results consistently demonstrate
that the proposed SARTM significantly outperforms state-of-the-art approaches
across a variety of conditions.

</details>


### [152] [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/pdf/2505.01958)
*Liqiang Jing, Guiming Hardy Chen, Ehsan Aghazadeh, Xin Eric Wang, Xinya Du*

Main category: cs.CV

TL;DR: The paper investigates visual object hallucination in Large Vision-Language Models (LVLMs), identifies error sources in their components, and proposes mitigation methods. It also introduces two benchmarks for evaluating hallucinations.


<details>
  <summary>Details</summary>
Motivation: Visual object hallucination in LVLMs leads to misinformation, raising safety and reliability concerns. Previous work lacks comprehensive investigation of underlying causes.

Method: Analyzes components of LLaVA-like LVLMs (language model, vision backbone, projector) to identify error sources. Proposes mitigation strategies for each. Introduces two benchmarks: QA-VisualGenome (attribute/relation hallucinations) and QA-FB15k (cognition-based hallucinations).

Result: Identifies sources of hallucination in LVLM components and suggests targeted mitigation methods. Benchmarks provide tools for evaluating hallucinations.

Conclusion: The study advances understanding of LVLM hallucinations, offering practical solutions and evaluation frameworks to improve model reliability.

Abstract: Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in
multimodal tasks, but visual object hallucination remains a persistent issue.
It refers to scenarios where models generate inaccurate visual object-related
information based on the query input, potentially leading to misinformation and
concerns about safety and reliability. Previous works focus on the evaluation
and mitigation of visual hallucinations, but the underlying causes have not
been comprehensively investigated. In this paper, we analyze each component of
LLaVA-like LVLMs -- the large language model, the vision backbone, and the
projector -- to identify potential sources of error and their impact. Based on
our observations, we propose methods to mitigate hallucination for each
problematic component. Additionally, we developed two hallucination benchmarks:
QA-VisualGenome, which emphasizes attribute and relation hallucinations, and
QA-FB15k, which focuses on cognition-based hallucinations.

</details>


### [153] [MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection](https://arxiv.org/pdf/2505.01969)
*Jiayi Cheng, Can Gao, Jie Zhou, Jiajun Wen, Tao Dai, Jinbao Wang*

Main category: cs.CV

TL;DR: A unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) is proposed, leveraging local and global geometry-aware information to improve efficiency and generalization over single-category methods.


<details>
  <summary>Details</summary>
Motivation: Existing 3D anomaly detection methods are costly, inefficient, and lack generalization due to task-specific training for each category.

Method: The model uses an adaptive geometry-aware masked attention module, a local geometry-aware encoder, and a global query decoder to reconstruct normal representations.

Result: MC3D-AD outperforms state-of-the-art single-category methods, achieving 3.1% and 9.3% improvement in AUROC on Real3D-AD and Anomaly-ShapeNet datasets, respectively.

Conclusion: The proposed MC3D-AD model offers a cost-effective, efficient, and generalized solution for multi-category 3D anomaly detection.

Abstract: 3D Anomaly Detection (AD) is a promising means of controlling the quality of
manufactured products. However, existing methods typically require carefully
training a task-specific model for each category independently, leading to high
cost, low efficiency, and weak generalization. Therefore, this paper presents a
novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims
to utilize both local and global geometry-aware information to reconstruct
normal representations of all categories. First, to learn robust and
generalized features of different categories, we propose an adaptive
geometry-aware masked attention module that extracts geometry variation
information to guide mask attention. Then, we introduce a local geometry-aware
encoder reinforced by the improved mask attention to encode group-level feature
tokens. Finally, we design a global query decoder that utilizes point cloud
position embeddings to improve the decoding process and reconstruction ability.
This leads to local and global geometry-aware reconstructed feature tokens for
the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and
Anomaly-ShapeNet datasets, and exhibits significant superiority over current
state-of-the-art single-category methods, achieving 3.1\% and 9.3\% improvement
in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The
source code will be released upon acceptance.

</details>


### [154] [Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques](https://arxiv.org/pdf/2505.01973)
*Anthony Dontoh, Stephanie Ivey, Logan Sirbaugh, Andrews Danyo, Armstrong Aboah*

Main category: cs.CV

TL;DR: The paper reviews ML/DL techniques for distracted driving detection, emphasizing the limitations of visual-only models and advocating for multimodal approaches integrating diverse data streams for better real-world performance.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current visual-only distracted driving detection methods and highlight the potential of multimodal systems for improved accuracy and generalizability.

Method: Systematic review of 74 peer-reviewed studies (2019-2024) on ML/DL techniques, categorizing them into visual, sensor-based, multimodal, and emerging modalities.

Result: Multimodal architectures outperform unimodal ones, offering robustness and scalability. Visual-only models lack generalizability, while emerging modalities provide privacy-aware alternatives.

Conclusion: Future research should focus on lightweight multimodal frameworks, personalized baselines, and cross-modality benchmarks to enhance real-world reliability in ADAS and road safety.

Abstract: Distracted driving continues to be a significant cause of road traffic
injuries and fatalities worldwide, even with advancements in driver monitoring
technologies. Recent developments in machine learning (ML) and deep learning
(DL) have primarily focused on visual data to detect distraction, often
neglecting the complex, multimodal nature of driver behavior. This systematic
review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL
techniques for distracted driving detection across visual, sensor-based,
multimodal, and emerging modalities. The review highlights a significant
prevalence of visual-only models, particularly convolutional neural networks
(CNNs) and temporal architectures, which achieve high accuracy but show limited
generalizability in real-world scenarios. Sensor-based and physiological models
provide complementary strengths by capturing internal states and vehicle
dynamics, while emerging techniques, such as auditory sensing and radio
frequency (RF) methods, offer privacy-aware alternatives. Multimodal
architecture consistently surpasses unimodal baselines, demonstrating enhanced
robustness, context awareness, and scalability by integrating diverse data
streams. These findings emphasize the need to move beyond visual-only
approaches and adopt multimodal systems that combine visual, physiological, and
vehicular cues while keeping in checking the need to balance computational
requirements. Future research should focus on developing lightweight,
deployable multimodal frameworks, incorporating personalized baselines, and
establishing cross-modality benchmarks to ensure real-world reliability in
advanced driver assistance systems (ADAS) and road safety interventions.

</details>


### [155] [Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation](https://arxiv.org/pdf/2505.01984)
*Doanh C. Bui, Hoai Luan Pham, Vu Trung Duong Le, Tuan Hai Vu, Van Duy Tran, Khang Nguyen, Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: ADaFGrad enhances lifelong learning for WSI analysis by leveraging pathology vision-language models and a gradient-distillation mechanism, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of computational tasks involving gigapixel WSIs presents storage, processing, and training challenges, necessitating lifelong learning approaches for distributed WSI analysis.

Method: ADaFGrad uses pathology vision-language models for feature-text interaction and a gradient-distillation mechanism to mimic logit gradients in continual learning.

Result: ADaFGrad outperforms state-of-the-art methods by up to +5.068% in class-incremental learning and achieves +40.084% accuracy over its baseline.

Conclusion: ADaFGrad is effective for lifelong WSI analysis, demonstrating superior performance and minimal forgetting in distributed settings.

Abstract: Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis
and prognosis, as they provide tissue details at the cellular level. However,
the rapid growth of computational tasks involving WSIs poses significant
challenges. Given that WSIs are gigapixels in size, they present difficulties
in terms of storage, processing, and model training. Therefore, it is essential
to develop lifelong learning approaches for WSI analysis. In scenarios where
slides are distributed across multiple institutes, we aim to leverage them to
develop a unified online model as a computational tool for cancer diagnosis in
clinical and hospital settings. In this study, we introduce ADaFGrad, a method
designed to enhance lifelong learning for whole-slide image (WSI) analysis.
First, we leverage pathology vision-language foundation models to develop a
framework that enables interaction between a slide's regional tissue features
and a predefined text-based prototype buffer. Additionally, we propose a
gradient-distillation mechanism that mimics the gradient of a logit with
respect to the classification-head parameters across past and current
iterations in a continual-learning setting. We construct a sequence of six TCGA
datasets for training and evaluation. Experimental results show that ADaFGrad
outperforms both state-of-the-art WSI-specific and conventional
continual-learning methods after only a few training epochs, exceeding them by
up to +5.068% in the class-incremental learning scenario while exhibiting the
least forgetting (i.e., retaining the most knowledge from previous tasks).
Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,
further demonstrating the effectiveness of the proposed modules.

</details>


### [156] [Drug classification based on X-ray spectroscopy combined with machine learning](https://arxiv.org/pdf/2505.01986)
*Yongming Li, Peng Wang, Bangdong Han*

Main category: cs.CV

TL;DR: A novel drug detection method combines X-ray absorption spectroscopy with CNN, SVM, and PSO, achieving 99.14% accuracy and fast execution.


<details>
  <summary>Details</summary>
Motivation: Traditional drug detection methods are complex and instrument-dependent; X-ray spectroscopy offers a simpler, non-destructive alternative.

Method: Used CNN for feature extraction from X-ray spectra, trained SVM with PSO-optimized parameters for drug classification.

Result: Achieved 99.14% accuracy, outperforming other methods, with efficient runtime.

Conclusion: The combined approach is fast, accurate, and reliable for drug detection, with potential for broad application.

Abstract: The proliferation of new types of drugs necessitates the urgent development
of faster and more accurate detection methods. Traditional detection methods
have high requirements for instruments and environments, making the operation
complex. X-ray absorption spectroscopy, a non-destructive detection technique,
offers advantages such as ease of operation, penetrative observation, and
strong substance differentiation capabilities, making it well-suited for
application in the field of drug detection and identification. In this study,
we constructed a classification model using Convolutional Neural Networks
(CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to
classify and identify drugs based on their X-ray spectral profiles. In the
experiments, we selected 14 chemical reagents with chemical formulas similar to
drugs as samples. We utilized CNN to extract features from the spectral data of
these 14 chemical reagents and used the extracted features to train an SVM
model. We also utilized PSO to optimize two critical initial parameters of the
SVM. The experimental results demonstrate that this model achieved higher
classification accuracy compared to two other common methods, with a prediction
accuracy of 99.14%. Additionally, the model exhibited fast execution speed,
mitigating the drawback of a drastic increase in running time and efficiency
reduction that may result from the direct fusion of PSO and SVM. Therefore, the
combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM
provides a rapid, highly accurate, and reliable classification and
identification method for the field of drug detection, holding promising
prospects for widespread application.

</details>


### [157] [Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields](https://arxiv.org/pdf/2505.02005)
*Zhenxing Mi, Ping Yin, Xue Xiao, Dan Xu*

Main category: cs.CV

TL;DR: Switch-NeRF++ introduces a scalable NeRF framework using a Heterogeneous Mixture of Hash Experts (HMoHE) to address scene decomposition, heterogeneity, and efficiency in large-scale scenes.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF methods lack efficient scene decomposition and heterogeneity modeling for large-scale scenes.

Method: Proposes a gating network and heterogeneous hash experts within a Sparsely Gated Mixture of Experts (MoE) framework for end-to-end learning.

Result: Achieves state-of-the-art accuracy and efficiency (8x faster training, 16x faster rendering) on large-scale datasets.

Conclusion: Switch-NeRF++ is a scalable, efficient solution for large-scale scene modeling with superior performance.

Abstract: Recent NeRF methods on large-scale scenes have underlined the importance of
scene decomposition for scalable NeRFs. Although achieving reasonable
scalability, there are several critical problems remaining unexplored, i.e.,
learnable decomposition, modeling scene heterogeneity, and modeling efficiency.
In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash
Experts (HMoHE) network that addresses these challenges within a unified
framework. It is a highly scalable NeRF that learns heterogeneous decomposition
and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end
manner. In our framework, a gating network learns to decomposes scenes and
allocates 3D points to specialized NeRF experts. This gating network is
co-optimized with the experts, by our proposed Sparsely Gated Mixture of
Experts (MoE) NeRF framework. We incorporate a hash-based gating network and
distinct heterogeneous hash experts. The hash-based gating efficiently learns
the decomposition of the large-scale scene. The distinct heterogeneous hash
experts consist of hash grids of different resolution ranges, enabling
effective learning of the heterogeneous representation of different scene
parts. These design choices make our framework an end-to-end and highly
scalable NeRF solution for real-world large-scale scene modeling to achieve
both quality and efficiency. We evaluate our accuracy and scalability on
existing large-scale NeRF datasets and a new dataset with very large-scale
scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our
approach can be easily scaled to various large-scale scenes and achieve
state-of-the-art scene rendering accuracy. Furthermore, our method exhibits
significant efficiency, with an 8x acceleration in training and a 16x
acceleration in rendering compared to Switch-NeRF. Codes will be released in
https://github.com/MiZhenxing/Switch-NeRF.

</details>


### [158] [Efficient Noise Calculation in Deep Learning-based MRI Reconstructions](https://arxiv.org/pdf/2505.02007)
*Onat Dalmaz, Arjun D. Desai, Reinhard Heckel, Tolga Çukur, Akshay S. Chaudhari, Brian A. Hargreaves*

Main category: cs.CV

TL;DR: The paper proposes a memory-efficient method to estimate voxel-wise variance in accelerated MRI reconstructions using deep learning, addressing noise propagation challenges.


<details>
  <summary>Details</summary>
Motivation: Noise propagation in MRI reconstructions is critical but often overlooked in DL-based methods due to analytical and computational difficulties.

Method: The technique approximates noise covariance using the DL network's Jacobian, introducing an unbiased estimator for voxel-wise variance and a Jacobian sketching method for efficiency.

Result: The method matches Monte Carlo simulation accuracy while significantly reducing computational and memory costs, and performs robustly across various conditions.

Conclusion: This work reintroduces efficient noise analysis in DL-based MRI reconstruction, promising improved evaluation and deployment of such methods.

Abstract: Accelerated MRI reconstruction involves solving an ill-posed inverse problem
where noise in acquired data propagates to the reconstructed images. Noise
analyses are central to MRI reconstruction for providing an explicit measure of
solution fidelity and for guiding the design and deployment of novel
reconstruction methods. However, deep learning (DL)-based reconstruction
methods have often overlooked noise propagation due to inherent analytical and
computational challenges, despite its critical importance. This work proposes a
theoretically grounded, memory-efficient technique to calculate voxel-wise
variance for quantifying uncertainty due to acquisition noise in accelerated
MRI reconstructions. Our approach approximates noise covariance using the DL
network's Jacobian, which is intractable to calculate. To circumvent this, we
derive an unbiased estimator for the diagonal of this covariance matrix
(voxel-wise variance) and introduce a Jacobian sketching technique to
efficiently implement it. We evaluate our method on knee and brain MRI datasets
for both data- and physics-driven networks trained in supervised and
unsupervised manners. Compared to empirical references obtained via Monte Carlo
simulations, our technique achieves near-equivalent performance while reducing
computational and memory demands by an order of magnitude or more. Furthermore,
our method is robust across varying input noise levels, acceleration factors,
and diverse undersampling schemes, highlighting its broad applicability. Our
work reintroduces accurate and efficient noise analysis as a central tenet of
reconstruction algorithms, holding promise to reshape how we evaluate and
deploy DL-based MRI. Our code will be made publicly available upon acceptance.

</details>


### [159] [MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution](https://arxiv.org/pdf/2505.02013)
*Siran Peng, Zipei Wang, Li Gao, Xiangyu Zhu, Tianshuo Zhang, Ajian Liu, Haoyuan Zhang, Zhen Lei*

Main category: cs.CV

TL;DR: VLF-FFD is a Vision-Language Fusion solution for MLLM-enhanced face forgery detection, achieving SOTA performance with a new dataset (EFF++) and a bidirectional interaction network (VLF-Net).


<details>
  <summary>Details</summary>
Motivation: Addressing sub-optimal integration of visual and textual modalities in existing face forgery detection methods.

Method: Proposes VLF-FFD with EFF++ dataset (frame-level annotations) and VLF-Net (bidirectional feature interaction).

Result: Achieves SOTA performance in cross-dataset and intra-dataset evaluations.

Conclusion: VLF-FFD is highly effective for face forgery detection, leveraging multimodal fusion.

Abstract: Reliable face forgery detection algorithms are crucial for countering the
growing threat of deepfake-driven disinformation. Previous research has
demonstrated the potential of Multimodal Large Language Models (MLLMs) in
identifying manipulated faces. However, existing methods typically depend on
either the Large Language Model (LLM) alone or an external detector to generate
classification results, which often leads to sub-optimal integration of visual
and textual modalities. In this paper, we propose VLF-FFD, a novel
Vision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our
key contributions are twofold. First, we present EFF++, a frame-level,
explainability-driven extension of the widely used FaceForensics++ (FF++)
dataset. In EFF++, each manipulated video frame is paired with a textual
annotation that describes both the forgery artifacts and the specific
manipulation technique applied, enabling more effective and informative MLLM
training. Second, we design a Vision-Language Fusion Network (VLF-Net) that
promotes bidirectional interaction between visual and textual features,
supported by a three-stage training pipeline to fully leverage its potential.
VLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and
intra-dataset evaluations, underscoring its exceptional effectiveness in face
forgery detection.

</details>


### [160] [R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation](https://arxiv.org/pdf/2505.02018)
*Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Guo-Wei Yang, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein, Shi-min Hu*

Main category: cs.CV

TL;DR: The paper introduces R-Bench, a graduate-level, multi-disciplinary benchmark for evaluating reasoning capabilities of language and multimodal models, highlighting poor performance of advanced models on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack rigor in evaluating nuanced reasoning for complex, real-world problems, especially in multi-disciplinary and multimodal contexts.

Method: R-Bench includes 1,094 language model questions and 665 multimodal questions across diverse subjects, meticulously curated for difficulty and cross-linguistic alignment.

Result: Advanced models like OpenAI o1 and GPT-4o perform poorly, with OpenAI o1 achieving only 53.2% accuracy on multimodal reasoning.

Conclusion: R-Bench serves as a rigorous benchmark, revealing significant gaps in current models' reasoning capabilities, particularly in multimodal contexts.

Abstract: Reasoning stands as a cornerstone of intelligence, enabling the synthesis of
existing knowledge to solve complex problems. Despite remarkable progress,
existing reasoning benchmarks often fail to rigorously evaluate the nuanced
reasoning capabilities required for complex, real-world problemsolving,
particularly in multi-disciplinary and multimodal contexts. In this paper, we
introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,
dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of
both language and multimodal models. RBench spans 1,094 questions across 108
subjects for language model evaluation and 665 questions across 83 subjects for
multimodal model testing in both English and Chinese. These questions are
meticulously curated to ensure rigorous difficulty calibration, subject
balance, and crosslinguistic alignment, enabling the assessment to be an
Olympiad-level multi-disciplinary benchmark. We evaluate widely used models,
including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate
that advanced models perform poorly on complex reasoning, especially multimodal
reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy
on our multimodal evaluation. Data and code are made publicly available at
here.

</details>


### [161] [A Birotation Solution for Relative Pose Problems](https://arxiv.org/pdf/2505.02025)
*Hongbo Zhao, Ziwei Long, Mengtan Zhang, Hanli Wang, Qijun Chen, Rui Fan*

Main category: cs.CV

TL;DR: A novel birotation solution for relative pose estimation outperforms traditional methods by using basis transformations and energy minimization on Riemannian manifolds.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods for relative pose estimation, which either decompose the essential matrix or directly estimate rotation and translation.

Method: Introduces three basis transformations with geometric metrics, minimizes energy functions on the Riemannian manifold SO(3), and iteratively updates two rotation matrices to recover the relative pose.

Result: Demonstrates superior performance in diverse relative pose estimation tasks through extensive evaluations.

Conclusion: The birotation solution provides an effective alternative to traditional methods, with promising results and publicly available resources.

Abstract: Relative pose estimation, a fundamental computer vision problem, has been
extensively studied for decades. Existing methods either estimate and decompose
the essential matrix or directly estimate the rotation and translation to
obtain the solution. In this article, we break the mold by tackling this
traditional problem with a novel birotation solution. We first introduce three
basis transformations, each associated with a geometric metric to quantify the
distance between the relative pose to be estimated and its corresponding basis
transformation. Three energy functions, designed based on these metrics, are
then minimized on the Riemannian manifold $\mathrm{SO(3)}$ by iteratively
updating the two rotation matrices. The two rotation matrices and the basis
transformation corresponding to the minimum energy are ultimately utilized to
recover the relative pose. Extensive quantitative and qualitative evaluations
across diverse relative pose estimation tasks demonstrate the superior
performance of our proposed birotation solution. Source code, demo video, and
datasets will be available at
\href{https://mias.group/birotation-solution}{mias.group/birotation-solution}
upon publication.

</details>


### [162] [LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset](https://arxiv.org/pdf/2503.02910)
*Wenqi Guo, Yiyang Du, Shan Du*

Main category: cs.CV

TL;DR: The paper introduces SimGas, a synthetic dataset for gas leak detection, and proposes a zero-shot method combining background subtraction, object detection, and segmentation, achieving 69% IoU.


<details>
  <summary>Details</summary>
Motivation: Gas leakage detection is critical but lacks high-quality datasets. Traditional methods are slow, and existing machine learning approaches suffer from data scarcity.

Method: A zero-shot method combining background subtraction, zero-shot object detection, filtering, and segmentation is proposed, leveraging the SimGas dataset.

Result: The method achieves 69% IoU, outperforming baselines, and shows decent performance on real-world data (GasVid).

Conclusion: The SimGas dataset and proposed method offer a robust solution for gas leak detection, with potential for real-world application.

Abstract: Gas leakage poses a significant hazard that requires prevention.
Traditionally, human inspection has been used for detection, a slow and
labour-intensive process. Recent research has applied machine learning
techniques to this problem, yet there remains a shortage of high-quality,
publicly available datasets. This paper introduces a synthetic dataset, SimGas,
featuring diverse backgrounds, interfering foreground objects, diverse leak
locations, and precise segmentation ground truth. We propose a zero-shot method
that combines background subtraction, zero-shot object detection, filtering,
and segmentation to leverage this dataset. Experimental results indicate that
our approach significantly outperforms baseline methods based solely on
background subtraction and zero-shot object detection with segmentation,
reaching an IoU of 69%. We also present an analysis of various prompt
configurations and threshold settings to provide deeper insights into the
performance of our method. Finally, we qualitatively (because of the lack of
ground truth) tested our performance on GasVid and reached decent results on
the real-world dataset. The dataset, code, and full qualitative results are
available at https://github.com/weathon/Lang-Gas.

</details>


### [163] [Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction](https://arxiv.org/pdf/2505.02043)
*Cheng Wang, Xinzhu Ma, Bin Wang, Shixiang Tang, Yuan Meng, Ping Jiang*

Main category: cs.CV

TL;DR: A method (Point2Primitive) for CAD model reconstruction from point clouds by predicting extrusion primitives directly, using an improved transformer for sketch curve detection and autoregressive optimization for high accuracy.


<details>
  <summary>Details</summary>
Motivation: Previous methods use implicit fields for sketch representation, which limits reconstruction of curved edges. The goal is to improve accuracy and editability of CAD models from point clouds.

Method: Point2Primitive predicts extrusion primitives directly from point clouds. It uses an improved transformer for sketch curve detection and autoregressive optimization for parameter accuracy. Topology is rebuilt via extrusion segmentation, combining predicted curves and computed extrusion operations.

Result: Superior primitive prediction accuracy and CAD reconstruction, with high geometrical fidelity in reconstructed shapes.

Conclusion: Point2Primitive effectively recovers editable CAD models from point clouds, outperforming previous methods in accuracy and fidelity.

Abstract: Recovering CAD models from point clouds, especially the sketch-extrusion
process, can be seen as the process of rebuilding the topology and extrusion
primitives. Previous methods utilize implicit fields for sketch representation,
leading to shape reconstruction of curved edges. In this paper, we proposed a
CAD reconstruction network that produces editable CAD models from input point
clouds (Point2Primitive) by directly predicting every element of the extrusion
primitives. Point2Primitive can directly detect and predict sketch curves (type
and parameter) from point clouds based on an improved transformer. The sketch
curve parameters are formulated as position queries and optimized in an
autoregressive way, leading to high parameter accuracy. The topology is rebuilt
by extrusion segmentation, and each extrusion parameter (sketch and extrusion
operation) is recovered by combining the predicted curves and the computed
extrusion operation. Extensive experiments demonstrate that our method is
superior in primitive prediction accuracy and CAD reconstruction. The
reconstructed shapes are of high geometrical fidelity.

</details>


### [164] [A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars](https://arxiv.org/pdf/2505.02046)
*Priyanka Kumari, Sampriti Soor, Amba Shetty, Archana M. Nair*

Main category: cs.CV

TL;DR: A UNet-based autoencoder model is proposed for efficient spectral preprocessing of Martian hyperspectral data, reducing preprocessing time significantly while maintaining accuracy for mineral identification.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for spectral preprocessing of Martian hyperspectral data are computationally intensive and time-consuming, hindering efficient mineral identification.

Method: The paper introduces a UNet-based autoencoder model for automating preprocessing steps like smoothing and continuum removal, trained on augmented spectra from the MICA library to simulate Martian conditions.

Result: The model reduces preprocessing time from 1.5 hours to 5 minutes for an 800x800 scene and achieves competitive accuracy in mineral classification.

Conclusion: The UNet-based framework enhances the speed and reliability of mineral mapping on Mars, demonstrating its potential for future applications.

Abstract: Accurate mineral identification on the Martian surface is critical for
understanding the planet's geological history. This paper presents a UNet-based
autoencoder model for efficient spectral preprocessing of CRISM MTRDR
hyperspectral data, addressing the limitations of traditional methods that are
computationally intensive and time-consuming. The proposed model automates key
preprocessing steps, such as smoothing and continuum removal, while preserving
essential mineral absorption features. Trained on augmented spectra from the
MICA spectral library, the model introduces realistic variability to simulate
MTRDR data conditions. By integrating this framework, preprocessing time for an
800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA
T1600 GPU. The preprocessed spectra are subsequently classified using MICAnet,
a deep learning model for Martian mineral identification. Evaluation on labeled
CRISM TRDR data demonstrates that the proposed approach achieves competitive
accuracy while significantly enhancing preprocessing efficiency. This work
highlights the potential of the UNet-based preprocessing framework to improve
the speed and reliability of mineral mapping on Mars.

</details>


### [165] [Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin](https://arxiv.org/pdf/2505.02056)
*Yuchen Wang, Xuefeng Bai, Xiucheng Li, Weili Guan, Liqiang Nie, Xinyang Chen*

Main category: cs.CV

TL;DR: The paper proposes a framework to address imbalanced pseudolabels in vision-language models by tackling concept mismatch and confusion, improving accuracy and balance.


<details>
  <summary>Details</summary>
Motivation: Existing methods inadequately address the imbalance in pseudolabels generated by vision-language models, prompting investigation into its root causes.

Method: The framework introduces concept alignment and confusion-aware calibrated margin mechanisms to enhance underperforming classes and balance predictions.

Result: Experiments on six datasets show a 6.29% improvement over state-of-the-art methods in pseudolabel accuracy and balance.

Conclusion: The proposed method effectively mitigates imbalance in pseudolabels, advancing performance in vision-language model adaptation.

Abstract: Adapting vision-language models (VLMs) to downstream tasks with pseudolabels
has gained increasing attention. A major obstacle is that the pseudolabels
generated by VLMs tend to be imbalanced, leading to inferior performance. While
existing methods have explored various strategies to address this, the
underlying causes of imbalance remain insufficiently investigated. To fill this
gap, we delve into imbalanced pseudolabels and identify two primary
contributing factors: concept mismatch and concept confusion. To mitigate these
two issues, we propose a novel framework incorporating concept alignment and
confusion-aware calibrated margin mechanisms. The core of our approach lies in
enhancing underperforming classes and promoting balanced predictions across
categories, thus mitigating imbalance. Extensive experiments on six benchmark
datasets with three learning paradigms demonstrate that the proposed method
effectively enhances the accuracy and balance of pseudolabels, achieving a
relative improvement of 6.29% over the SoTA method. Our code is avaliable at
https://anonymous.4open.science/r/CAP-C642/

</details>


### [166] [Transforming faces into video stories -- VideoFace2.0](https://arxiv.org/pdf/2505.02060)
*Branko Brkljač, Vladimir Kalušev, Branislav Popović, Milan Sečujski*

Main category: cs.CV

TL;DR: VideoFace2.0 is an advanced video analytics tool for face re-identification (ReID), enabling structured video outputs for applications like TV production and ML dataset creation.


<details>
  <summary>Details</summary>
Motivation: The need for efficient tools to create structured video stories and large datasets for ML tasks like lip reading and speech recognition.

Method: Combines face detection, recognition, and passive tracking-by-detection for robust face ReID in a near real-time system.

Result: Experiments confirm the system's applicability for face ReID, offering modular extensions for video production equipment.

Conclusion: The work aims to inspire further development of application-specific video tools and lower barriers for high-quality ML dataset production.

Abstract: Face detection and face recognition have been in the focus of vision
community since the very beginnings. Inspired by the success of the original
Videoface digitizer, a pioneering device that allowed users to capture video
signals from any source, we have designed an advanced video analytics tool to
efficiently create structured video stories, i.e. identity-based information
catalogs. VideoFace2.0 is the name of the developed system for spatial and
temporal localization of each unique face in the input video, i.e. face
re-identification (ReID), which also allows their cataloging, characterization
and creation of structured video outputs for later downstream tasks. Developed
near real-time solution is primarily designed to be utilized in application
scenarios involving TV production, media analysis, and as an efficient tool for
creating large video datasets necessary for training machine learning (ML)
models in challenging vision tasks such as lip reading and multimodal speech
recognition. Conducted experiments confirm applicability of the proposed face
ReID algorithm that is combining the concepts of face detection, face
recognition and passive tracking-by-detection in order to achieve robust and
efficient face ReID. The system is envisioned as a compact and modular
extensions of the existing video production equipment. We hope that the
presented work and shared code will stimulate further interest in development
of similar, application specific video analysis tools, and lower the entry
barrier for production of high-quality multi-modal ML datasets in the future.

</details>


### [167] [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/pdf/2505.02746)
*Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox*

Main category: cs.CV

TL;DR: Training CLIP models with less data using smart web search and knowledge graphs, achieving robust results with 10M images for domain-specific models and introducing EntityNet for faster generic model training.


<details>
  <summary>Details</summary>
Motivation: High-quality CLIP models require large datasets, limiting domain-specific development and increasing costs. This work addresses the need for fine-grained control and reduced data requirements.

Method: Employ smart web search strategies enhanced with knowledge graphs to train CLIP models with less data (10M images for domain-specific models) and introduce EntityNet (33M images, 46M text descriptions) for generic models.

Result: Demonstrated ability to train robust CLIP models from scratch with significantly less data, including a domain-specific model for living organisms and a generic model using EntityNet.

Conclusion: Smart web search and knowledge graphs enable efficient CLIP model training with reduced data, facilitating domain-specific and generic applications.

Abstract: Training high-quality CLIP models typically requires enormous datasets, which
limits the development of domain-specific models -- especially in areas that
even the largest CLIP models do not cover well -- and drives up training costs.
This poses challenges for scientific research that needs fine-grained control
over the training procedure of CLIP models. In this work, we show that by
employing smart web search strategies enhanced with knowledge graphs, a robust
CLIP model can be trained from scratch with considerably less data.
Specifically, we demonstrate that an expert foundation model for living
organisms can be built using just 10M images. Moreover, we introduce EntityNet,
a dataset comprising 33M images paired with 46M text descriptions, which
enables the training of a generic CLIP model in significantly reduced time.

</details>


### [168] [RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video](https://arxiv.org/pdf/2505.02064)
*Shuhang Xun, Sicheng Tao, Jungang Li, Yibo Shi, Zhixin Lin, Zhanhui Zhu, Yibo Yan, Hanqian Li, Linghao Zhang, Shikang Wang, Yixin Liu, Hanbo Zhang, Xuming Hu, Ying Ma*

Main category: cs.CV

TL;DR: RTV-Bench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in real-time video analysis, focusing on continuous perception, understanding, and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fail to assess MLLMs' performance in dynamic, real-world environments, necessitating a more comprehensive evaluation tool.

Method: RTV-Bench employs Multi-Timestamp Question Answering (MTQA), Hierarchical Question Structure, and Multi-dimensional Evaluation across 552 videos and 4,631 QA pairs.

Result: Open-source real-time models outperform offline ones but lag behind proprietary models. Larger model size or higher frame rates don't significantly improve performance.

Conclusion: Better architectures optimized for video stream processing and long sequences are needed to advance real-time video analysis with MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) increasingly excel at perception,
understanding, and reasoning. However, current benchmarks inadequately evaluate
their ability to perform these tasks continuously in dynamic, real-world
environments. To bridge this gap, we introduce RTV-Bench, a fine-grained
benchmark for MLLM real-time video analysis. RTV-Bench uses three key
principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve
with scene changes; (2) Hierarchical Question Structure, combining basic and
advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability
of continuous perception, understanding, and reasoning. RTV-Bench contains 552
diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated
leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline
(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,
InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source
real-time models largely outperform offline ones but still trail top
proprietary models. Our analysis also reveals that larger model size or higher
frame sampling rates do not significantly boost RTV-Bench performance,
sometimes causing slight decreases. This underscores the need for better model
architectures optimized for video stream processing and long sequences to
advance real-time video analysis with MLLMs. Our benchmark toolkit is available
at: https://github.com/LJungang/RTV-Bench.

</details>


### [169] [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/pdf/2505.02830)
*Qingqiu Li, Zihang Cui, Seongsu Bae, Jilan Xu, Runtian Yuan, Yuejie Zhang, Rui Feng, Quanli Shen, Xiaobo Zhang, Junjun He, Shujun Wang*

Main category: cs.CV

TL;DR: The paper introduces an anatomy-centric reasoning framework (AOR) to improve Medical LMMs' region-level understanding and multi-step reasoning for better CXR interpretation.


<details>
  <summary>Details</summary>
Motivation: Current Medical LMMs lack sufficient region-level interaction and accuracy due to single-step reasoning, limiting their diagnostic utility.

Method: Proposes the AOR framework for cross-modal region-level reasoning and develops AOR-Instruction, a dataset for training MLMMs with expert guidance.

Result: AOR outperforms in VQA and report generation tasks, demonstrating enhanced interactivity and explainability.

Conclusion: The AOR framework significantly improves MLMMs' capabilities in CXR interpretation, addressing key challenges in medical imaging.

Abstract: Chest X-rays (CXRs) are the most frequently performed imaging examinations in
clinical settings. Recent advancements in Large Multimodal Models (LMMs) have
enabled automated CXR interpretation, enhancing diagnostic accuracy and
efficiency. However, despite their strong visual understanding, current Medical
LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level
understanding and interaction, and (2) Limited accuracy and interpretability
due to single-step reasoning. In this paper, we empower MLMMs with
anatomy-centric reasoning capabilities to enhance their interactivity and
explainability. Specifically, we first propose an Anatomical Ontology-Guided
Reasoning (AOR) framework, which centers on cross-modal region-level
information to facilitate multi-step reasoning. Next, under the guidance of
expert physicians, we develop AOR-Instruction, a large instruction dataset for
MLMMs training. Our experiments demonstrate AOR's superior performance in both
VQA and report generation tasks.

</details>


### [170] [Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning](https://arxiv.org/pdf/2505.02071)
*Can Küçüksözen, Yücel Yemez*

Main category: cs.CV

TL;DR: COCA-Net introduces a hierarchical, attention-based clustering module (COCA) for unsupervised object discovery, leveraging compactness for object-centric representations and outperforming state-of-the-art models in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To address unsupervised object discovery in single images by learning object-centric representations without predefined object counts, improving segmentation quality.

Method: Uses COCA, an attention-based clustering module with a compactness-based algorithm, cascaded into a hierarchical network (COCA-Net) for bottom-up object extraction.

Result: Achieves superior or competitive segmentation performance on six datasets across nine metrics, handling background elements better than competitors.

Conclusion: COCA-Net is an effective solution for unsupervised object discovery, offering high-quality segmentation masks and flexibility in object count.

Abstract: We propose the Compact Clustering Attention (COCA) layer, an effective
building block that introduces a hierarchical strategy for object-centric
representation learning, while solving the unsupervised object discovery task
on single images. COCA is an attention-based clustering module capable of
extracting object-centric representations from multi-object scenes, when
cascaded into a bottom-up hierarchical network architecture, referred to as
COCA-Net. At its core, COCA utilizes a novel clustering algorithm that
leverages the physical concept of compactness, to highlight distinct object
centroids in a scene, providing a spatial inductive bias. Thanks to this
strategy, COCA-Net generates high-quality segmentation masks on both the
decoder side and, notably, the encoder side of its pipeline. Additionally,
COCA-Net is not bound by a predetermined number of object masks that it
generates and handles the segmentation of background elements better than its
competitors. We demonstrate COCA-Net's segmentation performance on six widely
adopted datasets, achieving superior or competitive results against the
state-of-the-art models across nine different evaluation metrics.

</details>


### [171] [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/pdf/2505.02835)
*Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, Liang Wang*

Main category: cs.CV

TL;DR: The paper explores using Reinforcement Learning (RL) to improve Multimodal Reward Models (MRMs), proposing the StableReinforce algorithm for stable training and superior performance.


<details>
  <summary>Details</summary>
Motivation: Limited exploration of long-term reasoning in MRMs and instability of existing RL methods in reward modeling.

Method: Reformulate reward modeling as a rule-based RL task, propose StableReinforce with refined training loss, advantage estimation, and reward design.

Result: R1-Reward model achieves 8.4% and 14.3% improvements on benchmarks, with further gains from increased compute.

Conclusion: RL, particularly StableReinforce, effectively optimizes MRMs, demonstrating significant performance improvements.

Abstract: Multimodal Reward Models (MRMs) play a crucial role in enhancing the
performance of Multimodal Large Language Models (MLLMs). While recent
advancements have primarily focused on improving the model structure and
training data of MRMs, there has been limited exploration into the
effectiveness of long-term reasoning capabilities for reward modeling and how
to activate these capabilities in MRMs. In this paper, we explore how
Reinforcement Learning (RL) can be used to improve reward modeling.
Specifically, we reformulate the reward modeling problem as a rule-based RL
task. However, we observe that directly applying existing RL algorithms, such
as Reinforce++, to reward modeling often leads to training instability or even
collapse due to the inherent limitations of these algorithms. To address this
issue, we propose the StableReinforce algorithm, which refines the training
loss, advantage estimation strategy, and reward design of existing RL methods.
These refinements result in more stable training dynamics and superior
performance. To facilitate MRM training, we collect 200K preference data from
diverse datasets. Our reward model, R1-Reward, trained using the
StableReinforce algorithm on this dataset, significantly improves performance
on multimodal reward modeling benchmarks. Compared to previous SOTA models,
R1-Reward achieves a $8.4\%$ improvement on the VL Reward-Bench and a $14.3\%$
improvement on the Multimodal Reward Bench. Moreover, with more inference
compute, R1-Reward's performance is further enhanced, highlighting the
potential of RL algorithms in optimizing MRMs.

</details>


### [172] [Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation](https://arxiv.org/pdf/2505.02075)
*Volodymyr Havrylov, Haiwen Huang, Dan Zhang, Andreas Geiger*

Main category: cs.CV

TL;DR: The paper explores using feature upsampling to enhance Vision Foundation Models (VFMs) for dense prediction tasks, with Interactive Segmentation (IS) as a benchmark.


<details>
  <summary>Details</summary>
Motivation: VFMs produce low-resolution features, limiting their use in dense prediction tasks. The study aims to evaluate feature upsampling methods to improve VFM performance.

Method: The paper introduces a task-agnostic feature upsampling module and uses Interactive Segmentation (IS) as a benchmark due to its multimodal input and dense output requirements.

Result: Appropriate upsampling strategies significantly improve VFM feature quality, as demonstrated by benchmarking experiments.

Conclusion: Feature upsampling enhances VFMs for dense prediction tasks, with IS serving as an effective benchmark for evaluation.

Abstract: Vision Foundation Models (VFMs) are large-scale, pre-trained models that
serve as general-purpose backbones for various computer vision tasks. As VFMs'
popularity grows, there is an increasing interest in understanding their
effectiveness for dense prediction tasks. However, VFMs typically produce
low-resolution features, limiting their direct applicability in this context.
One way to tackle this limitation is by employing a task-agnostic feature
upsampling module that refines VFM features resolution. To assess the
effectiveness of this approach, we investigate Interactive Segmentation (IS) as
a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its
inherent multimodal input, consisting of an image and a set of user-defined
clicks, as well as its dense mask output, IS creates a challenging environment
that demands comprehensive visual scene understanding. Our benchmarking
experiments show that selecting appropriate upsampling strategies significantly
improves VFM features quality. The code is released at
https://github.com/havrylovv/iSegProbe

</details>


### [173] [HandOcc: NeRF-based Hand Rendering with Occupancy Networks](https://arxiv.org/pdf/2505.02079)
*Maksym Ivashechkin, Oscar Mendez, Richard Bowden*

Main category: cs.CV

TL;DR: HandOcc is a meshless 3D hand rendering framework using occupancy-based NeRF, outperforming parametric mesh methods in fidelity and generalization.


<details>
  <summary>Details</summary>
Motivation: Parametric mesh methods trade fidelity for complexity and struggle with generalization. HandOcc avoids mesh dependency for better flexibility and accuracy.

Method: Uses a 3D skeleton and convolutional model with NeRF conditioned on occupancy for rendering, resolving hand interactions via occupancy.

Result: Achieved state-of-the-art performance on the InterHand2.6M dataset with fast rendering and excellent appearance transfer.

Conclusion: HandOcc offers a superior alternative to parametric mesh methods, enabling high-fidelity, meshless hand rendering.

Abstract: We propose HandOcc, a novel framework for hand rendering based upon
occupancy. Popular rendering methods such as NeRF are often combined with
parametric meshes to provide deformable hand models. However, in doing so, such
approaches present a trade-off between the fidelity of the mesh and the
complexity and dimensionality of the parametric model. The simplicity of
parametric mesh structures is appealing, but the underlying issue is that it
binds methods to mesh initialization, making it unable to generalize to objects
where a parametric model does not exist. It also means that estimation is tied
to mesh resolution and the accuracy of mesh fitting. This paper presents a
pipeline for meshless 3D rendering, which we apply to the hands. By providing
only a 3D skeleton, the desired appearance is extracted via a convolutional
model. We do this by exploiting a NeRF renderer conditioned upon an
occupancy-based representation. The approach uses the hand occupancy to resolve
hand-to-hand interactions further improving results, allowing fast rendering,
and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,
we achieved state-of-the-art results.

</details>


### [174] [SignSplat: Rendering Sign Language via Gaussian Splatting](https://arxiv.org/pdf/2505.02108)
*Maksym Ivashechkin, Oscar Mendez, Richard Bowden*

Main category: cs.CV

TL;DR: The paper introduces a method for high-fidelity human body rendering using Gaussian splatting, focusing on subtle motions like sign language, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human body rendering prioritize large motions (e.g., dancing), but complex tasks like sign language require capturing subtle hand and face motions with limited multi-view data.

Method: The approach leverages sequence data and regularization to ensure consistency, introduces adaptive control for Gaussian densification and pruning, and integrates neural machine translation for sign stitching.

Result: The method outperforms competitors on benchmark datasets and excels in rendering complex sign language motions.

Conclusion: The proposed framework effectively models subtle human motions from few views, achieving superior performance in challenging scenarios like sign language.

Abstract: State-of-the-art approaches for conditional human body rendering via Gaussian
splatting typically focus on simple body motions captured from many views. This
is often in the context of dancing or walking. However, for more complex use
cases, such as sign language, we care less about large body motion and more
about subtle and complex motions of the hands and face. The problems of
building high fidelity models are compounded by the complexity of capturing
multi-view data of sign. The solution is to make better use of sequence data,
ensuring that we can overcome the limited information from only a few views by
exploiting temporal variability. Nevertheless, learning from sequence-level
data requires extremely accurate and consistent model fitting to ensure that
appearance is consistent across complex motions. We focus on how to achieve
this, constraining mesh parameters to build an accurate Gaussian splatting
framework from few views capable of modelling subtle human motion. We leverage
regularization techniques on the Gaussian parameters to mitigate overfitting
and rendering artifacts. Additionally, we propose a new adaptive control method
to densify Gaussians and prune splat points on the mesh surface. To demonstrate
the accuracy of our approach, we render novel sequences of sign language video,
building on neural machine translation approaches to sign stitching. On
benchmark datasets, our approach achieves state-of-the-art performance; and on
highly articulated and complex sign language motion, we significantly
outperform competing approaches.

</details>


### [175] [Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance](https://arxiv.org/pdf/2505.02109)
*Yingkai Zhang, Zeqiang Lai, Tao Zhang, Ying Fu, Chenghu Zhou*

Main category: cs.CV

TL;DR: The paper introduces SSC-HSR, a framework for hyperspectral image super-resolution using unaligned RGB reference images, addressing alignment inaccuracies and poor module interaction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high-resolution ratios due to inaccurate alignment and weak interaction between alignment and fusion modules.

Method: Proposes a Two-Stage Image Alignment for spatial concordance and Feature Aggregation and Attention Fusion modules for spectral concordance.

Result: Outperforms state-of-the-art methods on quantitative and qualitative evaluations across three datasets.

Conclusion: SSC-HSR effectively improves spatial and spectral concordance in hyperspectral super-resolution.

Abstract: Hyperspectral images super-resolution aims to improve the spatial resolution,
yet its performance is often limited at high-resolution ratios. The recent
adoption of high-resolution reference images for super-resolution is driven by
the poor spatial detail found in low-resolution HSIs, presenting it as a
favorable method. However, these approaches cannot effectively utilize
information from the reference image, due to the inaccuracy of alignment and
its inadequate interaction between alignment and fusion modules. In this paper,
we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution
(SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the
issues of inaccurate alignment and poor interactivity of the previous
approaches. Specifically, to ensure spatial concordance, i.e., align images
more accurately across resolutions and refine textures, we construct a
Two-Stage Image Alignment with a synthetic generation pipeline in the image
alignment module, where the fine-tuned optical flow model can produce a more
accurate optical flow in the first stage and warp model can refine damaged
textures in the second stage. To enhance the interaction between alignment and
fusion modules and ensure spectral concordance during reconstruction, we
propose a Feature Aggregation module and an Attention Fusion module. In the
feature aggregation module, we introduce an Iterative Deformable Feature
Aggregation block to achieve significant feature matching and texture
aggregation with the fusion multi-scale results guidance, iteratively
generating learnable offset. Besides, we introduce two basic spectral-wise
attention blocks in the attention fusion module to model the inter-spectra
interactions. Extensive experiments on three natural or remote-sensing datasets
show that our method outperforms state-of-the-art approaches on both
quantitative and qualitative evaluations.

</details>


### [176] [GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction](https://arxiv.org/pdf/2505.02126)
*Zhihao Tang, Shenghao Yang, Hongtao Zhang, Mingbo Zhao*

Main category: cs.CV

TL;DR: GarmentGS is a method for high-fidelity 3D garment reconstruction using dense point clouds and Gaussian Splatting, achieving fast training and real-time rendering.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D garment creation is time-consuming and labor-intensive, and existing Gaussian Splatting methods struggle with unstructured primitives for non-watertight garments.

Method: GarmentGS uses dense point clouds to guide Gaussian primitives, enabling fast reconstruction (10 minutes) and better surface distribution for accuracy and rendering.

Result: The method achieves high geometric accuracy, non-watertight meshes, and superior rendering effects compared to slower traditional methods.

Conclusion: GarmentGS offers a fast, efficient solution for high-quality 3D garment reconstruction with real-time rendering capabilities.

Abstract: Traditional 3D garment creation requires extensive manual operations,
resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved
breakthrough progress in 3D scene reconstruction and rendering, attracting
widespread attention and opening new pathways for 3D garment reconstruction.
However, due to the unstructured and irregular nature of Gaussian primitives,
it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In
this paper, we present GarmentGS, a dense point cloud-guided method that can
reconstruct high-fidelity garment surfaces with high geometric accuracy and
generate non-watertight, single-layer meshes. Our method introduces a fast
dense point cloud reconstruction module that can complete garment point cloud
reconstruction in 10 minutes, compared to traditional methods that require
several hours. Furthermore, we use dense point clouds to guide the movement,
flattening, and rotation of Gaussian primitives, enabling better distribution
on the garment surface to achieve superior rendering effects and geometric
accuracy. Through numerical and visual comparisons, our method achieves fast
training and real-time rendering while maintaining competitive quality.

</details>


### [177] [HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement](https://arxiv.org/pdf/2505.02134)
*Xiaorui Zhao, Xinyue Zhou, Peibei Cao, Junyu Lou, Shuhang Gu*

Main category: cs.CV

TL;DR: The paper proposes HiLLIE, a human-in-the-loop framework for low-light image enhancement (LLIE) that iteratively improves visual quality using human guidance and an IQA model.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning LLIE results with human visual preferences for high-quality, well-lit images.

Method: HiLLIE integrates human guidance via visual quality annotations and trains an IQA model to learn human preferences, iteratively refining the enhancement model.

Result: The approach significantly improves LLIE model performance quantitatively and qualitatively with minimal human annotations.

Conclusion: HiLLIE effectively enhances LLIE outputs by leveraging human-in-the-loop training and IQA modeling.

Abstract: Developing effective approaches to generate enhanced results that align well
with human visual preferences for high-quality well-lit images remains a
challenge in low-light image enhancement (LLIE). In this paper, we propose a
human-in-the-loop LLIE training framework that improves the visual quality of
unsupervised LLIE model outputs through iterative training stages, named
HiLLIE. At each stage, we introduce human guidance into the training process
through efficient visual quality annotations of enhanced outputs. Subsequently,
we employ a tailored image quality assessment (IQA) model to learn human visual
preferences encoded in the acquired labels, which is then utilized to guide the
training process of an enhancement model. With only a small amount of pairwise
ranking annotations required at each stage, our approach continually improves
the IQA model's capability to simulate human visual assessment of enhanced
outputs, thus leading to visually appealing LLIE results. Extensive experiments
demonstrate that our approach significantly improves unsupervised LLIE model
performance in terms of both quantitative and qualitative performance. The code
and collected ranking dataset will be available at
https://github.com/LabShuHangGU/HiLLIE.

</details>


### [178] [Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving](https://arxiv.org/pdf/2505.02148)
*Alexey Nekrasov, Malcolm Burdorf, Stewart Worrall, Bastian Leibe, Julie Stephany Berrio Perez*

Main category: cs.CV

TL;DR: The paper introduces a novel dataset for 3D anomaly segmentation in autonomous driving, combining LiDAR and camera data, and evaluates baseline models for 3D segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack high-quality multimodal data for AVs, and 3D anomaly detection in driving scenarios is underexplored.

Method: The authors present a dataset with dense 3D semantic labeling, incorporating LiDAR, camera, and sequential data, and adapt baseline models for 3D segmentation.

Result: The dataset enables anomaly detection across various ranges, and baseline models highlight challenges in 3D anomaly detection.

Conclusion: The dataset and evaluation code will be openly available to support further research and performance comparison in 3D anomaly segmentation.

Abstract: To operate safely, autonomous vehicles (AVs) need to detect and handle
unexpected objects or anomalies on the road. While significant research exists
for anomaly detection and segmentation in 2D, research progress in 3D is
underexplored. Existing datasets lack high-quality multimodal data that are
typically found in AVs. This paper presents a novel dataset for anomaly
segmentation in driving scenarios. To the best of our knowledge, it is the
first publicly available dataset focused on road anomaly segmentation with
dense 3D semantic labeling, incorporating both LiDAR and camera data, as well
as sequential information to enable anomaly detection across various ranges.
This capability is critical for the safe navigation of autonomous vehicles. We
adapted and evaluated several baseline models for 3D segmentation, highlighting
the challenges of 3D anomaly detection in driving environments. Our dataset and
evaluation code will be openly available, facilitating the testing and
performance comparison of different approaches.

</details>


### [179] [Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution](https://arxiv.org/pdf/2505.02159)
*Xingyu Zhou, Wei Long, Jingbo Lu, Shiyin Jiang, Weiyi You, Haifeng Wu, Shuhang Gu*

Main category: cs.CV

TL;DR: LRTI-VSR is a training framework for recurrent VSR that leverages long-range temporal information efficiently, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Recurrent VSR models struggle with learning long-term dependencies in long videos, limiting their effectiveness.

Method: Proposes LRTI-VSR with a training strategy using long video clips and a refocused transformer block for selective temporal information prioritization.

Result: LRTI-VSR outperforms existing methods on long-video test sets while being efficient.

Conclusion: The framework effectively addresses long-term dependency challenges in VSR, enhancing performance and efficiency.

Abstract: Video super-resolution (VSR) can achieve better performance compared to
single image super-resolution by additionally leveraging temporal information.
In particular, the recurrent-based VSR model exploits long-range temporal
information during inference and achieves superior detail restoration. However,
effectively learning these long-term dependencies within long videos remains a
key challenge. To address this, we propose LRTI-VSR, a novel training framework
for recurrent VSR that efficiently leverages Long-Range Refocused Temporal
Information. Our framework includes a generic training strategy that utilizes
temporal propagation features from long video clips while training on shorter
video clips. Additionally, we introduce a refocused intra&inter-frame
transformer block which allows the VSR model to selectively prioritize useful
temporal information through its attention module while further improving
inter-frame information utilization in the FFN module. We evaluate LRTI-VSR on
both CNN and transformer-based VSR architectures, conducting extensive ablation
studies to validate the contribution of each component. Experiments on
long-video test sets demonstrate that LRTI-VSR achieves state-of-the-art
performance while maintaining training and computational efficiency.

</details>


### [180] [Focus What Matters: Matchability-Based Reweighting for Local Feature Matching](https://arxiv.org/pdf/2505.02161)
*Dongyue Li*

Main category: cs.CV

TL;DR: A novel attention reweighting mechanism is proposed to classify pixels into matchable and non-matchable categories, improving feature descriptor extraction by reducing redundancy and noise.


<details>
  <summary>Details</summary>
Motivation: Existing attention mechanisms treat all pixels equally, leading to redundancy and noisy interactions from irrelevant regions.

Method: Introduces a learnable bias term for attention logits and matchability-informed rescaling of input value features to dynamically adjust attention weights and output representations.

Result: Outperforms state-of-the-art methods on three benchmark datasets.

Conclusion: The dual design of bias injection and feature rescaling enhances attention mechanisms for semi-dense matching.

Abstract: Since the rise of Transformers, many semi-dense matching methods have adopted
attention mechanisms to extract feature descriptors. However, the attention
weights, which capture dependencies between pixels or keypoints, are often
learned from scratch. This approach can introduce redundancy and noisy
interactions from irrelevant regions, as it treats all pixels or keypoints
equally. Drawing inspiration from keypoint selection processes, we propose to
first classify all pixels into two categories: matchable and non-matchable.
Matchable pixels are expected to receive higher attention weights, while
non-matchable ones are down-weighted. In this work, we propose a novel
attention reweighting mechanism that simultaneously incorporates a learnable
bias term into the attention logits and applies a matchability-informed
rescaling to the input value features. The bias term, injected prior to the
softmax operation, selectively adjusts attention scores based on the confidence
of query-key interactions. Concurrently, the feature rescaling acts
post-attention by modulating the influence of each value vector in the final
output. This dual design allows the attention mechanism to dynamically adjust
both its internal weighting scheme and the magnitude of its output
representations. Extensive experiments conducted on three benchmark datasets
validate the effectiveness of our method, consistently outperforming existing
state-of-the-art approaches.

</details>


### [181] [SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting](https://arxiv.org/pdf/2505.02175)
*Shubhendu Jena, Shishir Reddy Vutukur, Adnane Boukhayma*

Main category: cs.CV

TL;DR: The paper introduces a generalizable pipeline for sparse 3D reconstruction and novel view synthesis (NVS) using 2D Gaussian Splatting (2DGS), achieving state-of-the-art results and high inference speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of sparse-view setups in 3D reconstruction and NVS, the work builds on 3D and 2D Gaussian Splatting to improve accuracy and efficiency.

Method: Proposes an MVS-based learning pipeline that regresses 2DGS parameters for 3D reconstruction and NVS from sparse-view images, leveraging multi-view deep visual features.

Result: Achieves state-of-the-art performance on DTU, BlendedMVS, and Tanks and Temples datasets, with significantly faster inference than volume rendering methods.

Conclusion: The pipeline successfully generalizes sparse 3D reconstruction and NVS, outperforming prior methods in accuracy and speed.

Abstract: Recovering 3D information from scenes via multi-view stereo reconstruction
(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in
scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting
(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian
Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive
rasterization to achieve accurate geometry representation during rendering,
improving 3D scene reconstruction while maintaining real-time performance.
Recent approaches have tackled the problem of sparse real-time NVS using 3DGS
within a generalizable, MVS-based learning framework to regress 3D Gaussian
parameters. Our work extends this line of research by addressing the challenge
of generalizable sparse 3D reconstruction and NVS jointly, and manages to
perform successfully at both tasks. We propose an MVS-based learning pipeline
that regresses 2DGS surface element parameters in a feed-forward fashion to
perform 3D shape reconstruction and NVS from sparse-view images. We further
show that our generalizable pipeline can benefit from preexisting foundational
multi-view deep visual features. The resulting model attains the
state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms
of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also
demonstrates strong generalization on the BlendedMVS and Tanks and Temples
datasets. We note that our model outperforms the prior state-of-the-art in
feed-forward sparse view reconstruction based on volume rendering of implicit
representations, while offering an almost 2 orders of magnitude higher
inference speed.

</details>


### [182] [Saliency-Guided Training for Fingerprint Presentation Attack Detection](https://arxiv.org/pdf/2505.02176)
*Samuel Webster, Adam Czajka*

Main category: cs.CV

TL;DR: Saliency-guided training improves fingerprint PAD performance, achieving top results on LivDet-2021.


<details>
  <summary>Details</summary>
Motivation: To enhance fingerprint presentation attack detection (PAD) by leveraging saliency-guided training for better generalization.

Method: Used a 50-participant study to create annotated saliency maps and explored algorithmic pseudosaliency. Evaluated on LivDet-2021 with five training scenarios.

Result: Saliency-guided training boosts accuracy and generalization, even with limited data, achieving first place on LivDet-2021.

Conclusion: Saliency-guided training is effective for fingerprint PAD, scalable, and supports reproducible research with released data and models.

Abstract: Saliency-guided training, which directs model learning to important regions
of images, has demonstrated generalization improvements across various
biometric presentation attack detection (PAD) tasks. This paper presents its
first application to fingerprint PAD. We conducted a 50-participant study to
create a dataset of 800 human-annotated fingerprint perceptually-important
maps, explored alongside algorithmically-generated "pseudosaliency," including
minutiae-based, image quality-based, and autoencoder-based saliency maps.
Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set,
we explore various configurations within five distinct training scenarios to
assess the impact of saliency-guided training on accuracy and generalization.
Our findings demonstrate the effectiveness of saliency-guided training for
fingerprint PAD in both limited and large data contexts, and we present a
configuration capable of earning the first place on the LivDet-2021 benchmark.
Our results highlight saliency-guided training's promise for increased model
generalization capabilities, its effectiveness when data is limited, and its
potential to scale to larger datasets in fingerprint PAD. All collected
saliency data and trained models are released with the paper to support
reproducible research.

</details>


### [183] [Sparfels: Fast Reconstruction from Sparse Unposed Imagery](https://arxiv.org/pdf/2505.02178)
*Shubhendu Jena, Amine Ouasfi, Mae Younes, Adnane Boukhayma*

Main category: cs.CV

TL;DR: A fast, efficient method for sparse view reconstruction using surface element splatting, achieving state-of-the-art results in sparse uncalibrated settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the underexplored challenge of shape recovery from sparse, noisy, or unposed camera views, where existing methods rely on data priors or external geometry.

Method: Proposes a simple pipeline using a 3D foundation model to initialize and optimize a 2D Gaussian Splatting model, with a novel splatted color variance formulation for accurate shape reconstruction.

Result: Achieves state-of-the-art performance in reconstruction and novel view synthesis on multi-view datasets, running in under 3 minutes on consumer GPUs.

Conclusion: The method efficiently leverages a 3D foundation model and novel variance formulation to outperform existing approaches in sparse, uncalibrated settings.

Abstract: We present a method for Sparse view reconstruction with surface element
splatting that runs within 3 minutes on a consumer grade GPU. While few methods
address sparse radiance field learning from noisy or unposed sparse cameras,
shape recovery remains relatively underexplored in this setting. Several
radiance and shape learning test-time optimization methods address the sparse
posed setting by learning data priors or using combinations of external
monocular geometry priors. Differently, we propose an efficient and simple
pipeline harnessing a single recent 3D foundation model. We leverage its
various task heads, notably point maps and camera initializations to
instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image
correspondences to guide camera optimization midst 2DGS training. Key to our
contribution is a novel formulation of splatted color variance along rays,
which can be computed efficiently. Reducing this moment in training leads to
more accurate shape reconstructions. We demonstrate state-of-the-art
performances in the sparse uncalibrated setting in reconstruction and novel
view benchmarks based on established multi-view datasets.

</details>


### [184] [ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications](https://arxiv.org/pdf/2505.02179)
*Tao Zhu, Qi Yu, Xinru Dong, Shiyu Li, Yue Liu, Jinlong Jiang, Lei Shu*

Main category: cs.CV

TL;DR: ProDisc-VAD improves weakly-supervised video anomaly detection (WS-VAD) by addressing label ambiguity with a prototype interaction layer and pseudo-instance discriminative enhancement, achieving high efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Label ambiguity in MIL-based WS-VAD hinders discriminative feature learning, prompting the need for a robust and efficient solution.

Method: ProDisc-VAD uses a Prototype Interaction Layer (PIL) for controlled normality modeling and a Pseudo-Instance Discriminative Enhancement (PIDE) loss for targeted contrastive learning on extreme-scoring instances.

Result: Achieves high AUCs (97.98% on ShanghaiTech, 87.12% on UCF-Crime) with only 0.4M parameters, outperforming larger models like VadCLIP.

Conclusion: ProDisc-VAD offers a highly efficient and effective framework for WS-VAD, combining robustness and state-of-the-art performance.

Abstract: Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance
Learning (MIL) suffers from label ambiguity, hindering discriminative feature
learning. We propose ProDisc-VAD, an efficient framework tackling this via two
synergistic components. The Prototype Interaction Layer (PIL) provides
controlled normality modeling using a small set of learnable prototypes,
establishing a robust baseline without being overwhelmed by dominant normal
data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts
separability by applying targeted contrastive learning exclusively to the most
reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD
achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M
parameters, over 800x fewer than recent ViT-based methods like VadCLIP,
demonstrating exceptional efficiency alongside state-of-the-art performance.
Code is available at https://github.com/modadundun/ProDisc-VAD.

</details>


### [185] [Robust AI-Generated Face Detection with Imbalanced Data](https://arxiv.org/pdf/2505.02182)
*Yamini Sri Krubha, Aryana Hou, Braden Vester, Web Walker, Xin Wang, Li Lin, Shu Hu*

Main category: cs.CV

TL;DR: A framework combining dynamic loss reweighting and ranking-based optimization is proposed to improve deepfake detection under imbalanced dataset conditions.


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose threats to digital trust, and current detectors struggle with distribution shifts and class imbalance.

Method: Proposes a framework using dynamic loss reweighting and ranking-based optimization.

Result: Achieves superior generalization and performance in imbalanced datasets.

Conclusion: The framework addresses key challenges in deepfake detection, enhancing robustness and accuracy.

Abstract: Deepfakes, created using advanced AI techniques such as Variational
Autoencoder and Generative Adversarial Networks, have evolved from research and
entertainment applications into tools for malicious activities, posing
significant threats to digital trust. Current deepfake detection techniques
have evolved from CNN-based methods focused on local artifacts to more advanced
approaches using vision transformers and multimodal models like CLIP, which
capture global anomalies and improve cross-domain generalization. Despite
recent progress, state-of-the-art deepfake detectors still face major
challenges in handling distribution shifts from emerging generative models and
addressing severe class imbalance between authentic and fake samples in
deepfake datasets, which limits their robustness and detection accuracy. To
address these challenges, we propose a framework that combines dynamic loss
reweighting and ranking-based optimization, which achieves superior
generalization and performance under imbalanced dataset conditions. The code is
available at https://github.com/Purdue-M2/SP_CUP.

</details>


### [186] [DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization](https://arxiv.org/pdf/2505.02192)
*Wenchuan Wang, Mengqi Huang, Yijing Tu, Zhendong Mao*

Main category: cs.CV

TL;DR: DualReal is a framework for text-to-video generation that jointly trains identity and motion dimensions to avoid conflicts, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods isolate identity and motion customization, ignoring their interdependencies, leading to degraded generation quality.

Method: DualReal uses adaptive joint training with two units: Dual-aware Adaptation for dynamic phase selection and regularization, and StageBlender Controller for conflict-free fusion.

Result: DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% and excels in motion quality.

Conclusion: DualReal successfully integrates identity and motion customization, achieving superior performance in text-to-video generation.

Abstract: Customized text-to-video generation with pre-trained large-scale models has
recently garnered significant attention through focusing on identity and motion
consistency. Existing works typically follow the isolated customized paradigm,
where the subject identity or motion dynamics are customized exclusively.
However, this paradigm completely ignores the intrinsic mutual constraints and
synergistic interdependencies between identity and motion, resulting in
identity-motion conflicts throughout the generation process that systematically
degrades. To address this, we introduce DualReal, a novel framework that,
employs adaptive joint training to collaboratively construct interdependencies
between dimensions. Specifically, DualReal is composed of two units: (1)
Dual-aware Adaptation dynamically selects a training phase (i.e., identity or
motion), learns the current information guided by the frozen dimension prior,
and employs a regularization strategy to avoid knowledge leakage; (2)
StageBlender Controller leverages the denoising stages and Diffusion
Transformer depths to guide different dimensions with adaptive granularity,
avoiding conflicts at various stages and ultimately achieving lossless fusion
of identity and motion patterns. We constructed a more comprehensive benchmark
than existing methods. The experimental results show that DualReal improves
CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top
performance on nearly all motion quality metrics.

</details>


### [187] [Improving Physical Object State Representation in Text-to-Image Generative Systems](https://arxiv.org/pdf/2505.02236)
*Tianle Chen, Chaitanya Chakka, Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: The paper improves text-to-image models' ability to represent object states by fine-tuning them on synthetic data, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with accurately depicting object states, prompting the need for better training data and methods.

Method: A fully-automatic pipeline generates synthetic data for varied object states, used to fine-tune open-source models. Performance is evaluated using GPT4o-mini and a custom dataset.

Result: Average improvements of 8+% on GenAI-Bench and 24+% on a custom dataset of object states.

Conclusion: Fine-tuning models with synthetic data significantly enhances their ability to represent object states, with released prompts and code for reproducibility.

Abstract: Current text-to-image generative models struggle to accurately represent
object states (e.g., "a table without a bottle," "an empty tumbler"). In this
work, we first design a fully-automatic pipeline to generate high-quality
synthetic data that accurately captures objects in varied states. Next, we
fine-tune several open-source text-to-image models on this synthetic data. We
evaluate the performance of the fine-tuned models by quantifying the alignment
of the generated images to their prompts using GPT4o-mini, and achieve an
average absolute improvement of 8+% across four models on the public
GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific
focus on common objects in various physical states. We demonstrate a
significant improvement of an average of 24+% over the baseline on this
dataset. We release all evaluation prompts and code.

</details>


### [188] [Quantizing Diffusion Models from a Sampling-Aware Perspective](https://arxiv.org/pdf/2505.02242)
*Qian Zeng, Jie Song, Yuanyu Wan, Huiqiong Wang, Mingli Song*

Main category: cs.CV

TL;DR: The paper proposes a sampling-aware quantization strategy to address the dual challenge of lengthy denoising chains and computational inefficiency in diffusion models, ensuring high fidelity and fast convergence.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face limitations in low-latency and resource-limited environments due to slow denoising chains and computationally intensive noise estimation. Previous solutions tackled these issues separately, but quantization-induced noise disrupts directional estimation, affecting sampling accuracy.

Method: The authors introduce a Mixed-Order Trajectory Alignment technique to constrain error bounds at each sampling step, aligning the probability flow more linearly. This combines sampling and quantization for dual acceleration.

Result: Experiments show the method preserves fast convergence and superior generation quality in sparse-step fast sampling across datasets.

Conclusion: The proposed sampling-aware quantization strategy effectively balances speed and fidelity in diffusion models, with code to be released.

Abstract: Diffusion models have recently emerged as the dominant approach in visual
generation tasks. However, the lengthy denoising chains and the computationally
intensive noise estimation networks hinder their applicability in low-latency
and resource-limited environments. Previous research has endeavored to address
these limitations in a decoupled manner, utilizing either advanced samplers or
efficient model quantization techniques. In this study, we uncover that
quantization-induced noise disrupts directional estimation at each sampling
step, further distorting the precise directional estimations of higher-order
samplers when solving the sampling equations through discretized numerical
methods, thereby altering the optimal sampling trajectory. To attain dual
acceleration with high fidelity, we propose a sampling-aware quantization
strategy, wherein a Mixed-Order Trajectory Alignment technique is devised to
impose a more stringent constraint on the error bounds at each sampling step,
facilitating a more linear probability flow. Extensive experiments on
sparse-step fast sampling across multiple datasets demonstrate that our
approach preserves the rapid convergence characteristics of high-speed samplers
while maintaining superior generation quality. Code will be made publicly
available soon.

</details>


### [189] [Cricket: A Self-Powered Chirping Pixel](https://arxiv.org/pdf/2505.02246)
*Shree K. Nayar, Jeremy Klotz, Nikhil Nanda, Mikhail Fridberg*

Main category: cs.CV

TL;DR: A battery-free sensor called 'cricket' harvests light energy to measure and wirelessly transmit light levels, enabling applications like solar tracking and adaptive lighting.


<details>
  <summary>Details</summary>
Motivation: To create a self-powered, wireless sensor for light measurement without external power sources or batteries.

Method: The sensor harvests energy from light, sleeps most of the time, and transmits radio frequency chirps when energy reaches a threshold. The chirp interval reflects light levels.

Result: Characterized performance metrics (radiometric response, SNR, dynamic range), demonstrated miniaturization, and showcased applications like solar tracking and adaptive sunglasses.

Conclusion: The cricket sensor is a versatile, battery-free solution for light measurement and wireless communication, with potential in energy conservation and adaptive systems.

Abstract: We present a sensor that can measure light and wirelessly communicate the
measurement, without the need for an external power source or a battery. Our
sensor, called cricket, harvests energy from incident light. It is asleep for
most of the time and transmits a short and strong radio frequency chirp when
its harvested energy reaches a specific level. The carrier frequency of each
cricket is fixed and reveals its identity, and the duration between consecutive
chirps is a measure of the incident light level. We have characterized the
radiometric response function, signal-to-noise ratio and dynamic range of
cricket. We have experimentally verified that cricket can be miniaturized at
the expense of increasing the duration between chirps. We show that a cube with
a cricket on each of its sides can be used to estimate the centroid of any
complex illumination, which has value in applications such as solar tracking.
We also demonstrate the use of crickets for creating untethered sensor arrays
that can produce video and control lighting for energy conservation. Finally,
we modified cricket's circuit to develop battery-free electronic sunglasses
that can instantly adapt to environmental illumination.

</details>


### [190] [Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset](https://arxiv.org/pdf/2505.02255)
*Jakub Wąsala, Bartłomiej Wrzalski, Kornelia Noculak, Yuliia Tarasenko, Oliwer Krupa, Jan Kocoń, Grzegorz Chodak*

Main category: cs.CV

TL;DR: A novel method improves image generation efficiency by refining distilled models' outputs to match baseline quality, reducing computational costs by 82%.


<details>
  <summary>Details</summary>
Motivation: To enhance the cost-to-quality ratio of image generation with diffusion models, leveraging learnable differences between distilled and baseline models.

Method: Generate a synthetic paired dataset, train an image-to-image translation head to refine distilled model outputs to baseline quality.

Result: The pipeline achieves photorealistic portraits comparable to baseline with 82% lower computational cost.

Conclusion: Demonstrates potential for efficient large-scale image generation by combining distilled models with enhancement layers.

Abstract: This study presents a novel approach to enhance the cost-to-quality ratio of
image generation with diffusion models. We hypothesize that differences between
distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are
consistent and, therefore, learnable within a specialized domain, like portrait
generation. We generate a synthetic paired dataset and train a fast
image-to-image translation head. Using two sets of low- and high-quality
synthetic images, our model is trained to refine the output of a distilled
generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like
FLUX.1-dev, which is more computationally intensive. Our results show that the
pipeline, which combines a distilled version of a large generative model with
our enhancement layer, delivers similar photorealistic portraits to the
baseline version with up to an 82% decrease in computational cost compared to
FLUX.1-dev. This study demonstrates the potential for improving the efficiency
of AI solutions involving large-scale image generation.

</details>


### [191] [Compositional Image-Text Matching and Retrieval by Grounding Entities](https://arxiv.org/pdf/2505.02278)
*Madhukar Reddy Vongala, Saurabh Srivastava, Jana Košecká*

Main category: cs.CV

TL;DR: A novel zero-shot augmentation of CLIP embeddings improves compositional image-text matching by dynamically adjusting embeddings using localized sub-images, achieving notable accuracy and retrieval improvements.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models like CLIP lack entity grounding and compositional matching capabilities, limiting their performance in tasks requiring fine-grained understanding.

Method: The approach computes separate embeddings for localized sub-images (objects and relations) using open-vocabulary detectors and dynamically adjusts the global image embedding for improved similarity computation.

Result: Achieves a 1.5% improvement in image-text matching accuracy on Visual Genome and SVO Probes, and significant gains in retrieval performance (12% and 0.4% Recall@1 improvements on Flickr30K and MS-COCO).

Conclusion: The proposed method enhances CLIP's compositional properties without additional training, advancing state-of-the-art in vision-language tasks.

Abstract: Vision-language pretraining on large datasets of images-text pairs is one of
the main building blocks of current Vision-Language Models. While with
additional training, these models excel in various downstream tasks, including
visual question answering, image captioning, and visual commonsense reasoning.
However, a notable weakness of pretrained models like CLIP, is their inability
to perform entity grounding and compositional image and text
matching~\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR,
learninglocalizeCVPR24}. In this work we propose a novel learning-free
zero-shot augmentation of CLIP embeddings that has favorable compositional
properties. We compute separate embeddings of sub-images of object entities and
relations that are localized by the state of the art open vocabulary detectors
and dynamically adjust the baseline global image embedding. % The final
embedding is obtained by computing a weighted combination of the sub-image
embeddings. The resulting embedding is then utilized for similarity computation
with text embedding, resulting in a average 1.5\% improvement in image-text
matching accuracy on the Visual Genome and SVO Probes
datasets~\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings
demonstrate superior retrieval performance, thus achieving significant gains on
the Flickr30K and MS-COCO retrieval benchmarks~\cite{flickr30ke, mscoco},
improving the state-of-the-art Recall@1 by 12\% and 0.4\%, respectively. Our
code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.

</details>


### [192] [Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation](https://arxiv.org/pdf/2505.02287)
*Shipeng Liu, Ziliang Xiong, Bastian Wandt, Per-Erik Forssén*

Main category: cs.CV

TL;DR: CFRE integrates CNFs into regression-based HPE for dynamic distribution adaptation, improving accuracy and UQ while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Current HPE methods lack balance between accuracy, efficiency, and reliable UQ, with regression-based methods having fixed distributions and heatmap-based ones being resource-heavy.

Method: Proposes Continuous Flow Residual Estimation (CFRE), combining Continuous Normalizing Flows (CNFs) with regression models for dynamic distribution adaptation.

Result: CFRE achieves better accuracy and UQ with retained computational efficiency in 2D and 3D HPE tasks.

Conclusion: CFRE effectively addresses the limitations of existing HPE methods by balancing accuracy, efficiency, and UQ.

Abstract: Human Pose Estimation (HPE) is increasingly important for applications like
virtual reality and motion analysis, yet current methods struggle with
balancing accuracy, computational efficiency, and reliable uncertainty
quantification (UQ). Traditional regression-based methods assume fixed
distributions, which might lead to poor UQ. Heatmap-based methods effectively
model the output distribution using likelihood heatmaps, however, they demand
significant resources. To address this, we propose Continuous Flow Residual
Estimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into
regression-based models, which allows for dynamic distribution adaptation.
Through extensive experiments, we show that CFRE leads to better accuracy and
uncertainty quantification with retained computational efficiency on both 2D
and 3D human pose estimation tasks.

</details>


### [193] [TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment](https://arxiv.org/pdf/2505.02325)
*Zhichuan Wang, Yang Zhou, Jinhai Xiang, Yulong Wang, Xinwei He*

Main category: cs.CV

TL;DR: TeDA adapts CLIP for 3D object retrieval by aligning 2D vision-language models with 3D data at test time, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap between 2D-trained vision-language models and 3D testing distributions for better generalization in 3D applications.

Method: Projects 3D objects into multi-view images, uses CLIP for feature extraction, and refines embeddings with iterative optimization and textual descriptions.

Result: Outperforms state-of-the-art methods on open-set 3D retrieval benchmarks, validated on Objaverse-LVIS with depth maps.

Conclusion: TeDA effectively bridges 2D and 3D domains, enhancing 3D object retrieval without extensive training.

Abstract: Learning discriminative 3D representations that generalize well to unknown
testing categories is an emerging requirement for many real-world 3D
applications. Existing well-established methods often struggle to attain this
goal due to insufficient 3D training data from broader concepts. Meanwhile,
pre-trained large vision-language models (e.g., CLIP) have shown remarkable
zero-shot generalization capabilities. Yet, they are limited in extracting
suitable 3D representations due to substantial gaps between their 2D training
and 3D testing distributions. To address these challenges, we propose
Testing-time Distribution Alignment (TeDA), a novel framework that adapts a
pretrained 2D vision-language model CLIP for unknown 3D object retrieval at
test time. To our knowledge, it is the first work that studies the test-time
adaptation of a vision-language model for 3D feature learning. TeDA projects 3D
objects into multi-view images, extracts features using CLIP, and refines 3D
query embeddings with an iterative optimization strategy by confident
query-target sample pairs in a self-boosting manner. Additionally, TeDA
integrates textual descriptions generated by a multimodal language model
(InternVL) to enhance 3D object understanding, leveraging CLIP's aligned
feature space to fuse visual and textual cues. Extensive experiments on four
open-set 3D object retrieval benchmarks demonstrate that TeDA greatly
outperforms state-of-the-art methods, even those requiring extensive training.
We also experimented with depth maps on Objaverse-LVIS, further validating its
effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.

</details>


### [194] [6D Pose Estimation on Spoons and Hands](https://arxiv.org/pdf/2505.02335)
*Kevin Tan, Fan Yang, Yuhao Chen*

Main category: cs.CV

TL;DR: A system using 6D pose estimation tracks hand and spoon movements in videos to monitor dietary intake, comparing two SOTA VOS models for performance and errors.


<details>
  <summary>Details</summary>
Motivation: Accurate dietary monitoring is needed to improve eating habits, as current methods like self-reporting are unreliable.

Method: The paper implements a system analyzing video feeds of people eating, using 6D pose estimation to track hand and spoon movements.

Result: Performance of two SOTA VOS models is examined, identifying main error sources in the system.

Conclusion: The system provides reliable insights into nutritional intake by tracking utensil and hand movements.

Abstract: Accurate dietary monitoring is essential for promoting healthier eating
habits. A key area of research is how people interact and consume food using
utensils and hands. By tracking their position and orientation, it is possible
to estimate the volume of food being consumed, or monitor eating behaviours,
highly useful insights into nutritional intake that can be more reliable than
popular methods such as self-reporting. Hence, this paper implements a system
that analyzes stationary video feed of people eating, using 6D pose estimation
to track hand and spoon movements to capture spatial position and orientation.
In doing so, we examine the performance of two state-of-the-art (SOTA) video
object segmentation (VOS) models, both quantitatively and qualitatively, and
identify main sources of error within the system.

</details>


### [195] [Quaternion Infrared Visible Image Fusion](https://arxiv.org/pdf/2505.02364)
*Weihua Yang, Yicong Zhou*

Main category: cs.CV

TL;DR: The paper proposes a quaternion-based framework (QIVIF) for infrared-visible image fusion, addressing limitations of existing methods by preserving color structure and enhancing performance under low-visibility conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to adequately handle color structure in visible images and degrade with low-quality inputs, prompting the need for a robust fusion framework.

Method: QIVIF uses quaternion domain processing, featuring low-visibility feature learning, adaptive unsharp masking, and hierarchical Bayesian fusion to integrate infrared and visible images.

Result: QIVIF outperforms state-of-the-art methods, especially in low-visibility scenarios, as validated by extensive experiments.

Conclusion: The QIVIF framework effectively combines infrared and visible images, overcoming prior limitations and enhancing fusion quality under diverse conditions.

Abstract: Visible images provide rich details and color information only under
well-lighted conditions while infrared images effectively highlight thermal
targets under challenging conditions such as low visibility and adverse
weather. Infrared-visible image fusion aims to integrate complementary
information from infrared and visible images to generate a high-quality fused
image. Existing methods exhibit critical limitations such as neglecting color
structure information in visible images and performance degradation when
processing low-quality color-visible inputs. To address these issues, we
propose a quaternion infrared-visible image fusion (QIVIF) framework to
generate high-quality fused images completely in the quaternion domain. QIVIF
proposes a quaternion low-visibility feature learning model to adaptively
extract salient thermal targets and fine-grained texture details from input
infrared and visible images respectively under diverse degraded conditions.
QIVIF then develops a quaternion adaptive unsharp masking method to adaptively
improve high-frequency feature enhancement with balanced illumination. QIVIF
further proposes a quaternion hierarchical Bayesian fusion model to integrate
infrared saliency and enhanced visible details to obtain high-quality fused
images. Extensive experiments across diverse datasets demonstrate that our
QIVIF surpasses state-of-the-art methods under challenging low-visibility
conditions.

</details>


### [196] [Quaternion Multi-focus Color Image Fusion](https://arxiv.org/pdf/2505.02365)
*Weihua Yang, Yicong Zhou*

Main category: cs.CV

TL;DR: A quaternion-based framework for multi-focus color image fusion, improving focus detection and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in complex scenarios due to poor handling of color and texture.

Method: Uses quaternion sparse decomposition, base-detail fusion, and structural similarity refinement.

Result: Outperforms state-of-the-art methods in experiments.

Conclusion: The framework effectively integrates color images while preserving details and structure.

Abstract: Multi-focus color image fusion refers to integrating multiple partially
focused color images to create a single all-in-focus color image. However,
existing methods struggle with complex real-world scenarios due to limitations
in handling color information and intricate textures. To address these
challenges, this paper proposes a quaternion multi-focus color image fusion
framework to perform high-quality color image fusion completely in the
quaternion domain. This framework introduces 1) a quaternion sparse
decomposition model to jointly learn fine-scale image details and structure
information of color images in an iterative fashion for high-precision focus
detection, 2) a quaternion base-detail fusion strategy to individually fuse
base-scale and detail-scale results across multiple color images for preserving
structure and detail information, and 3) a quaternion structural similarity
refinement strategy to adaptively select optimal patches from initial fusion
results and obtain the final fused result for preserving fine details and
ensuring spatially consistent outputs. Extensive experiments demonstrate that
the proposed framework outperforms state-of-the-art methods.

</details>


### [197] [SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing](https://arxiv.org/pdf/2505.02370)
*Ming Li, Xin Gu, Fan Chen, Xiaoying Xing, Longyin Wen, Chen Chen, Sijie Zhu*

Main category: cs.CV

TL;DR: The paper introduces a novel method to improve image editing by constructing better editing instructions, rectifying mismatches, and using contrastive supervision, outperforming existing approaches with fewer resources.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for image editing suffer from noisy supervision due to mismatched instructions and image pairs, and prior solutions like VLMs or pre-training fail to address this core issue.

Method: The method rectifies editing instructions to align with image pairs and uses contrastive instructions with triplet loss for training, avoiding the need for VLMs or pre-training.

Result: The approach significantly outperforms SOTA methods, achieving a 9.19% improvement on Real-Edit with 30x less data and 13x smaller model size.

Conclusion: The proposed solution is direct, efficient, and effective for instruction-based image editing, offering a simpler alternative to existing methods.

Abstract: Due to the challenges of manually collecting accurate editing data, existing
datasets are typically constructed using various automated methods, leading to
noisy supervision signals caused by the mismatch between editing instructions
and original-edited image pairs. Recent efforts attempt to improve editing
models through generating higher-quality edited images, pre-training on
recognition tasks, or introducing vision-language models (VLMs) but fail to
resolve this fundamental issue. In this paper, we offer a novel solution by
constructing more effective editing instructions for given image pairs. This
includes rectifying the editing instructions to better align with the
original-edited image pairs and using contrastive editing instructions to
further enhance their effectiveness. Specifically, we find that editing models
exhibit specific generation attributes at different inference steps,
independent of the text. Based on these prior attributes, we define a unified
guide for VLMs to rectify editing instructions. However, there are some
challenging editing scenarios that cannot be resolved solely with rectified
instructions. To this end, we further construct contrastive supervision signals
with positive and negative instructions and introduce them into the model
training using triplet loss, thereby further facilitating supervision
effectiveness. Our method does not require the VLM modules or pre-training
tasks used in previous work, offering a more direct and efficient way to
provide better supervision signals, and providing a novel, simple, and
effective solution for instruction-based image editing. Results on multiple
benchmarks demonstrate that our method significantly outperforms existing
approaches. Compared with previous SOTA SmartEdit, we achieve 9.19%
improvements on the Real-Edit benchmark with 30x less training data and 13x
smaller model size.

</details>


### [198] [MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans](https://arxiv.org/pdf/2505.02388)
*Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, Tengyu Liu, Siyuan Huang*

Main category: cs.CV

TL;DR: MetaScenes and Scan2Sim address scalability in 3D scene creation for Embodied AI by automating asset replacement and providing a large dataset, enhancing sim-to-real transfer and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene datasets rely on artist-driven designs, which are labor-intensive and hard to scale, limiting progress in Embodied AI research.

Method: MetaScenes is a large-scale 3D scene dataset from real-world scans, and Scan2Sim automates high-quality asset replacement. Benchmarks evaluate scene synthesis and domain transfer.

Result: MetaScenes supports generalizable agent learning and sim-to-real applications, validated by benchmarks in robotic manipulation and VLN.

Conclusion: MetaScenes and Scan2Sim offer scalable solutions for 3D scene creation, advancing Embodied AI research with improved generalization and sim-to-real capabilities.

Abstract: Embodied AI (EAI) research requires high-quality, diverse 3D scenes to
effectively support skill acquisition, sim-to-real transfer, and
generalization. Achieving these quality standards, however, necessitates the
precise replication of real-world object diversity. Existing datasets
demonstrate that this process heavily relies on artist-driven designs, which
demand substantial human effort and present significant scalability challenges.
To scalably produce realistic and interactive 3D scenes, we first present
MetaScenes, a large-scale, simulatable 3D scene dataset constructed from
real-world scans, which includes 15366 objects spanning 831 fine-grained
categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,
which enables the automated, high-quality replacement of assets, thereby
eliminating the reliance on artist-driven designs for scaling 3D scenes. We
further propose two benchmarks to evaluate MetaScenes: a detailed scene
synthesis task focused on small item layouts for robotic manipulation and a
domain transfer task in vision-and-language navigation (VLN) to validate
cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by
supporting more generalizable agent learning and sim-to-real applications,
introducing new possibilities for EAI research. Project website:
https://meta-scenes.github.io/.

</details>


### [199] [Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection](https://arxiv.org/pdf/2505.02393)
*Sungheon Jeong, Jihong Park, Mohsen Imani*

Main category: cs.CV

TL;DR: IEF-VAD improves video anomaly detection by fusing synthetic event representations with RGB frames, outperforming existing methods without needing event sensors or frame-level labels.


<details>
  <summary>Details</summary>
Motivation: Existing detectors rely on RGB frames, missing transient motion cues crucial for anomaly detection.

Method: Proposes IEF-VAD, which synthesizes event representations from RGB videos, fuses them with image features using uncertainty-aware techniques, and refines the latent state.

Result: Achieves state-of-the-art performance on multiple benchmarks without dedicated event sensors or labels.

Conclusion: Synthetic event representations enhance motion cue detection, enabling robust video understanding without specialized hardware.

Abstract: Most existing video anomaly detectors rely solely on RGB frames, which lack
the temporal resolution needed to capture abrupt or transient motion cues, key
indicators of anomalous events. To address this limitation, we propose
Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that
synthesizes event representations directly from RGB videos and fuses them with
image features through a principled, uncertainty-aware process. The system (i)
models heavy-tailed sensor noise with a Student`s-t likelihood, deriving
value-level inverse-variance weights via a Laplace approximation; (ii) applies
Kalman-style frame-wise updates to balance modalities over time; and (iii)
iteratively refines the fused latent state to erase residual cross-modal noise.
Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new
state of the art across multiple real-world anomaly detection benchmarks. These
findings highlight the utility of synthetic event representations in
emphasizing motion cues that are often underrepresented in RGB frames, enabling
accurate and robust video understanding across diverse applications without
requiring dedicated event sensors. Code and models are available at
https://github.com/EavnJeong/IEF-VAD.

</details>


### [200] [Token Coordinated Prompt Attention is Needed for Visual Prompting](https://arxiv.org/pdf/2505.02406)
*Zichen Liu, Xu Zou, Gang Hua, Jiahuan Zhou*

Main category: cs.CV

TL;DR: The paper introduces Token Coordinated Prompt Attention (TCPA), a module to enhance Vision Transformers by assigning specific prompts to different tokens, improving feature diversity and discriminative power.


<details>
  <summary>Details</summary>
Motivation: Existing visual prompting methods treat all tokens uniformly, limiting ViT's representational capacity and causing biased features. TCPA addresses this by tailoring prompts to token roles.

Method: TCPA disentangles prompts into CLS and Image Prompts for distinct token interactions and uses a matching function to assign coordinated prompts, enabling precise attention.

Result: Experiments show TCPA significantly improves feature diversity and discriminative ability across benchmarks.

Conclusion: TCPA is a plug-and-play solution that enhances ViT performance by optimizing prompt-token interactions.

Abstract: Visual prompting techniques are widely used to efficiently fine-tune
pretrained Vision Transformers (ViT) by learning a small set of shared prompts
for all tokens. However, existing methods overlook the unique roles of
different tokens in conveying discriminative information and interact with all
tokens using the same prompts, thereby limiting the representational capacity
of ViT. This often leads to indistinguishable and biased prompt-extracted
features, hindering performance. To address this issue, we propose a
plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns
specific coordinated prompts to different tokens for attention-based
interactions. Firstly, recognizing the distinct functions of CLS and image
tokens-global information aggregation and local feature extraction, we
disentangle the prompts into CLS Prompts and Image Prompts, which interact
exclusively with CLS tokens and image tokens through attention mechanisms. This
enhances their respective discriminative abilities. Furthermore, as different
image tokens correspond to distinct image patches and contain diverse
information, we employ a matching function to automatically assign coordinated
prompts to individual tokens. This enables more precise attention interactions,
improving the diversity and representational capacity of the extracted
features. Extensive experiments across various benchmarks demonstrate that TCPA
significantly enhances the diversity and discriminative power of the extracted
features. The code is available at
https://github.com/zhoujiahuan1991/ICML2025-TCPA.

</details>


### [201] [Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey](https://arxiv.org/pdf/2505.02448)
*Chaohua Li, Enhao Zhang, Chuanxing Geng, Songcan Chen*

Main category: cs.CV

TL;DR: The paper proposes a new categorization framework for OOD detection using CLIP-like VLMs, focusing on image and text modalities, and identifies future research directions.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods rely on unimodal (image-only) paradigms, which misalign with the multimodal nature of VLMs like CLIP.

Method: The authors introduce a framework categorizing methods based on how OOD visual and textual information is used, dividing them into four groups across two training strategies.

Result: The framework better aligns with CLIP's cross-modal capabilities and highlights gaps in current research.

Conclusion: Future work should explore cross-domain integration, practical applications, and theoretical understanding in CLIP-like OOD detection.

Abstract: Out-of-distribution detection (OOD) is a pivotal task for real-world
applications that trains models to identify samples that are distributionally
different from the in-distribution (ID) data during testing. Recent advances in
AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized
OOD detection by shifting from traditional unimodal image detectors to
multimodal image-text detectors. This shift has inspired extensive research;
however, existing categorization schemes (e.g., few- or zero-shot types) still
rely solely on the availability of ID images, adhering to a unimodal paradigm.
To better align with CLIP's cross-modal nature, we propose a new categorization
framework rooted in both image and text modalities. Specifically, we categorize
existing methods based on how visual and textual information of OOD data is
utilized within image + text modalities, and further divide them into four
groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e.,
learnable vectors or class names) Known or Unknown, across two training
strategies (i.e., train-free or training-required). More importantly, we
discuss open problems in CLIP-like OOD detection and highlight promising
directions for future research, including cross-domain integration, practical
applications, and theoretical understanding.

</details>


### [202] [Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging](https://arxiv.org/pdf/2505.02467)
*Valerio Guarrasi, Klara Mogensen, Sara Tassinari, Sara Qvarlander, Paolo Soda*

Main category: cs.CV

TL;DR: A sequential forward search algorithm optimizes fusion module placement in multimodal deep learning for medical imaging, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current methods for integrating multimodal imaging data are inefficient and lack guarantees of optimal results, necessitating a systematic approach.

Method: The proposed sequential forward search algorithm incrementally evaluates fusion modules at different network layers, retraining and comparing validation loss to identify optimal configurations.

Result: The algorithm outperformed unimodal baselines and other fusion methods in accuracy, F-score, and specificity, while reducing computational overhead.

Conclusion: This method provides an efficient framework for fusion optimization in medical imaging, enhancing clinical decision-making and scalability of medical AI.

Abstract: Multimodal deep learning harnesses diverse imaging modalities, such as MRI
sequences, to enhance diagnostic accuracy in medical imaging. A key challenge
is determining the optimal timing for integrating these
modalities-specifically, identifying the network layers where fusion modules
should be inserted. Current approaches often rely on manual tuning or
exhaustive search, which are computationally expensive without any guarantee of
converging to optimal results. We propose a sequential forward search algorithm
that incrementally activates and evaluates candidate fusion modules at
different layers of a multimodal network. At each step, the algorithm retrains
from previously learned weights and compares validation loss to identify the
best-performing configuration. This process systematically reduces the search
space, enabling efficient identification of the optimal fusion timing without
exhaustively testing all possible module placements. The approach is validated
on two multimodal MRI datasets, each addressing different classification tasks.
Our algorithm consistently identified configurations that outperformed unimodal
baselines, late fusion, and a brute-force ensemble of all potential fusion
placements. These architectures demonstrated superior accuracy, F-score, and
specificity while maintaining competitive or improved AUC values. Furthermore,
the sequential nature of the search significantly reduced computational
overhead, making the optimization process more practical. By systematically
determining the optimal timing to fuse imaging modalities, our method advances
multimodal deep learning for medical imaging. It provides an efficient and
robust framework for fusion optimization, paving the way for improved clinical
decision-making and more adaptable, scalable architectures in medical AI
applications.

</details>


### [203] [Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction](https://arxiv.org/pdf/2505.02471)
*Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, Lixiang Ru, Libin Wang, Qingpei Guo, Rui Liu, Weilong Chai, Xinyu Xiao, Ziyuan Huang*

Main category: cs.CV

TL;DR: Ming-Lite-Uni is an open-source multimodal framework unifying vision and language, featuring a unified visual generator and autoregressive model. It supports text-to-image generation and image editing, with strong experimental results.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework for multimodal tasks, expanding capabilities beyond visual understanding and aligning with broader AI advancements like AGI.

Method: Uses MetaQueries and M2-omni frameworks, multi-scale learnable tokens, and representation alignment. Combines a fixed MLLM with a learnable diffusion model.

Result: Demonstrates strong performance and fluid interactive processes, with open-sourced code and model weights.

Conclusion: Ming-Lite-Uni is a promising step toward AGI, with plans for further refinement.

Abstract: We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a
newly designed unified visual generator and a native multimodal autoregressive
model tailored for unifying vision and language. Specifically, this project
provides an open-source implementation of the integrated MetaQueries and
M2-omni framework, while introducing the novel multi-scale learnable tokens and
multi-scale representation alignment strategy. By leveraging a fixed MLLM and a
learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to
perform both text-to-image generation and instruction based image editing
tasks, expanding their capabilities beyond pure visual understanding. Our
experimental results demonstrate the strong performance of Ming-Lite-Uni and
illustrate the impressive fluid nature of its interactive process. All code and
model weights are open-sourced to foster further exploration within the
community. Notably, this work aligns with concurrent multimodal AI milestones -
such as ChatGPT-4o with native image generation updated in March 25, 2025 -
underscoring the broader significance of unified models like Ming-Lite-Uni on
the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further
refined.

</details>


### [204] [Finger Pose Estimation for Under-screen Fingerprint Sensor](https://arxiv.org/pdf/2505.02481)
*Xiongjun Guan, Zhiyu Pan, Jianjiang Feng, Jie Zhou*

Main category: cs.CV

TL;DR: A novel dual-modal network improves fingerprint pose estimation by integrating texture details and rough contours, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with large angles or small areas, especially in under-screen fingerprint sensors.

Method: Uses dual-modal inputs (texture details and rough contours), decoupled probability distribution prediction, MoE-based feature fusion, and cross-domain knowledge transfer.

Result: Significantly outperforms SOTA methods, enhancing fingerprint recognition accuracy.

Conclusion: The proposed approach effectively addresses limitations in pose estimation, improving recognition performance.

Abstract: Two-dimensional pose estimation plays a crucial role in fingerprint
recognition by facilitating global alignment and reduce pose-induced
variations. However, existing methods are still unsatisfactory when handling
with large angle or small area inputs. These limitations are particularly
pronounced on fingerprints captured by under-screen fingerprint sensors in
smartphones. In this paper, we present a novel dual-modal input based network
for under-screen fingerprint pose estimation. Our approach effectively
integrates two distinct yet complementary modalities: texture details extracted
from ridge patches through the under-screen fingerprint sensor, and rough
contours derived from capacitive images obtained via the touch screen. This
collaborative integration endows our network with more comprehensive and
discriminative information, substantially improving the accuracy and stability
of pose estimation. A decoupled probability distribution prediction task is
designed, instead of the traditional supervised forms of numerical regression
or heatmap voting, to facilitate the training process. Additionally, we
incorporate a Mixture of Experts (MoE) based feature fusion mechanism and a
relationship driven cross-domain knowledge transfer strategy to further
strengthen feature extraction and fusion capabilities. Extensive experiments
are conducted on several public datasets and two private datasets. The results
indicate that our method is significantly superior to previous state-of-the-art
(SOTA) methods and remarkably boosts the recognition ability of fingerprint
recognition algorithms. Our code is available at
https://github.com/XiongjunGuan/DRACO.

</details>


### [205] [Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions](https://arxiv.org/pdf/2505.02501)
*Asma Brazi, Boris Meden, Fabrice Mayran de Chamisso, Steve Bourgeois, Vincent Lepetit*

Main category: cs.CV

TL;DR: Corr2Distrib estimates a 6D camera pose distribution from RGB images using local correspondences, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Visual ambiguities like symmetries and occlusions lead to multiple valid poses, which existing methods fail to address effectively using local correspondences.

Method: Corr2Distrib learns symmetry-aware 3D point representations, generates rotation hypotheses from 2D-3D correspondences, and refines them into a 6DoF pose distribution using PnP and scoring.

Result: Corr2Distrib outperforms state-of-the-art methods in both pose distribution and single pose estimation on complex non-synthetic scenes.

Conclusion: Corr2Distrib demonstrates the effectiveness of correspondence-based approaches for pose estimation, leveraging ambiguities to recover all valid poses.

Abstract: We introduce Corr2Distrib, the first correspondence-based method which
estimates a 6D camera pose distribution from an RGB image, explaining the
observations. Indeed, symmetries and occlusions introduce visual ambiguities,
leading to multiple valid poses. While a few recent methods tackle this
problem, they do not rely on local correspondences which, according to the BOP
Challenge, are currently the most effective way to estimate a single 6DoF pose
solution. Using correspondences to estimate a pose distribution is not
straightforward, since ambiguous correspondences induced by visual ambiguities
drastically decrease the performance of PnP. With Corr2Distrib, we turn these
ambiguities into an advantage to recover all valid poses. Corr2Distrib first
learns a symmetry-aware representation for each 3D point on the object's
surface, characterized by a descriptor and a local frame. This representation
enables the generation of 3DoF rotation hypotheses from single 2D-3D
correspondences. Next, we refine these hypotheses into a 6DoF pose distribution
using PnP and pose scoring. Our experimental evaluations on complex
non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art
solutions for both pose distribution estimation and single pose estimation from
an RGB image, demonstrating the potential of correspondences-based approaches.

</details>


### [206] [Text to Image Generation and Editing: A Survey](https://arxiv.org/pdf/2505.02527)
*Pengfei Yang, Ngai-Man Cheung, Xinda Ma*

Main category: cs.CV

TL;DR: A comprehensive survey of 141 text-to-image (T2I) generation works from 2021-2024, covering foundation models, key technologies, performance comparisons, social impact, and future directions.


<details>
  <summary>Details</summary>
Motivation: To systematically review and compare the advancements in T2I generation and editing, providing a guide for future research.

Method: Analyzes four foundation model architectures (autoregression, non-autoregression, GAN, diffusion) and key technologies (autoencoder, attention, classifier-free guidance). Compares methods, datasets, evaluation metrics, and performance.

Result: Identifies trends, performance benchmarks, and gaps in T2I research, including social impact considerations.

Conclusion: Proposes insights for improving T2I models and outlines future directions, aiming to guide and inspire further progress in the field.

Abstract: Text-to-image generation (T2I) refers to the text-guided generation of
high-quality images. In the past few years, T2I has attracted widespread
attention and numerous works have emerged. In this survey, we comprehensively
review 141 works conducted from 2021 to 2024. First, we introduce four
foundation model architectures of T2I (autoregression, non-autoregression, GAN
and diffusion) and the commonly used key technologies (autoencoder, attention
and classifier-free guidance). Secondly, we systematically compare the methods
of these studies in two directions, T2I generation and T2I editing, including
the encoders and the key technologies they use. In addition, we also compare
the performance of these researches side by side in terms of datasets,
evaluation metrics, training resources, and inference speed. In addition to the
four foundation models, we survey other works on T2I, such as energy-based
models and recent Mamba and multimodality. We also investigate the potential
social impact of T2I and provide some solutions. Finally, we propose unique
insights of improving the performance of T2I models and possible future
development directions. In summary, this survey is the first systematic and
comprehensive overview of T2I, aiming to provide a valuable guide for future
researchers and stimulate continued progress in this field.

</details>


### [207] [Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities](https://arxiv.org/pdf/2505.02567)
*Xinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Guo-Hua Wang, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang*

Main category: cs.CV

TL;DR: A survey on unifying multimodal understanding and image generation models, covering architectures, datasets, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The independent evolution of multimodal understanding (autoregressive-based) and image generation (diffusion-based) models has created a gap, prompting interest in unified frameworks like GPT-4o.

Method: The paper reviews unified models, categorizing them into diffusion-based, autoregressive-based, and hybrid approaches, and analyzes their designs and innovations.

Result: The survey compiles datasets and benchmarks for unified models and identifies key challenges like tokenization, cross-modal attention, and data.

Conclusion: The paper aims to inspire future research by providing a comprehensive reference, with plans to update as the field progresses.

Abstract: Recent years have seen remarkable progress in both multimodal understanding
models and image generation models. Despite their respective successes, these
two domains have evolved independently, leading to distinct architectural
paradigms: While autoregressive-based architectures have dominated multimodal
understanding, diffusion-based models have become the cornerstone of image
generation. Recently, there has been growing interest in developing unified
frameworks that integrate these tasks. The emergence of GPT-4o's new
capabilities exemplifies this trend, highlighting the potential for
unification. However, the architectural differences between the two domains
pose significant challenges. To provide a clear overview of current efforts
toward unification, we present a comprehensive survey aimed at guiding future
research. First, we introduce the foundational concepts and recent advancements
in multimodal understanding and text-to-image generation models. Next, we
review existing unified models, categorizing them into three main architectural
paradigms: diffusion-based, autoregressive-based, and hybrid approaches that
fuse autoregressive and diffusion mechanisms. For each category, we analyze the
structural designs and innovations introduced by related works. Additionally,
we compile datasets and benchmarks tailored for unified models, offering
resources for future exploration. Finally, we discuss the key challenges facing
this nascent field, including tokenization strategy, cross-modal attention, and
data. As this area is still in its early stages, we anticipate rapid
advancements and will regularly update this survey. Our goal is to inspire
further research and provide a valuable reference for the community. The
references associated with this survey will be available on GitHub soon.

</details>


### [208] [RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet](https://arxiv.org/pdf/2505.02586)
*Eliraz Orfaig, Inna Stainvas, Igal Bilik*

Main category: cs.CV

TL;DR: RGBX-DiffusionDet extends DiffusionDet to fuse RGB and 2D data (X) via an adaptive encoder, using DCR-CBAM and DMLAB for cross-modal interaction and feature refinement, outperforming RGB-only baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance object detection by integrating heterogeneous 2D data (X) with RGB imagery, leveraging cross-modal interaction and adaptive feature fusion.

Method: Uses DCR-CBAM for dynamic channel reduction and DMLAB for adaptive multiscale fusion, with novel regularization losses for compact embeddings.

Result: Outperforms RGB-only DiffusionDet on RGB-Depth, RGB-Polarimetric, and RGB-Infrared datasets while maintaining efficiency.

Conclusion: RGBX-DiffusionDet is a flexible, efficient multimodal object detection framework, advancing integration of diverse 2D sensing modalities.

Abstract: This work introduces RGBX-DiffusionDet, an object detection framework
extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB
imagery via an adaptive multimodal encoder. To enable cross-modal interaction,
we design the dynamic channel reduction within a convolutional block attention
module (DCR-CBAM), which facilitates cross-talk between subnetworks by
dynamically highlighting salient channel features. Furthermore, the dynamic
multi-level aggregation block (DMLAB) is proposed to refine spatial feature
representations through adaptive multiscale fusion. Finally, novel
regularization losses that enforce channel saliency and spatial selectivity are
introduced, leading to compact and discriminative feature embeddings. Extensive
experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric
dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We
demonstrate consistent superiority of the proposed approach over the baseline
RGB-only DiffusionDet. The modular architecture maintains the original decoding
complexity, ensuring efficiency. These results establish the proposed
RGBX-DiffusionDet as a flexible multimodal object detection approach, providing
new insights into integrating diverse 2D sensing modalities into
diffusion-based detection pipelines.

</details>


### [209] [DELTA: Dense Depth from Events and LiDAR using Transformer's Attention](https://arxiv.org/pdf/2505.02593)
*Vincent Brebion, Julien Moreau, Franck Davoine*

Main category: cs.CV

TL;DR: DELTA, a neural-network-based method, fuses event and LiDAR data using self- and cross-attention to estimate dense depth maps, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Few works have explored combining event cameras and LiDARs, which provide complementary data (asynchronous lighting changes vs. sparse depth).

Method: DELTA uses self- and cross-attention to model spatial and temporal relations within and between event and LiDAR data.

Result: DELTA sets a new SOTA in event-based depth estimation, reducing errors up to four times for close ranges.

Conclusion: The proposed method effectively combines event and LiDAR data, significantly improving depth estimation accuracy.

Abstract: Event cameras and LiDARs provide complementary yet distinct data:
respectively, asynchronous detections of changes in lighting versus sparse but
accurate depth information at a fixed rate. To this day, few works have
explored the combination of these two modalities. In this article, we propose a
novel neural-network-based method for fusing event and LiDAR data in order to
estimate dense depth maps. Our architecture, DELTA, exploits the concepts of
self- and cross-attention to model the spatial and temporal relations within
and between the event and LiDAR data. Following a thorough evaluation, we
demonstrate that DELTA sets a new state of the art in the event-based depth
estimation problem, and that it is able to reduce the errors up to four times
for close ranges compared to the previous SOTA.

</details>


### [210] [Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models](https://arxiv.org/pdf/2505.02626)
*Sassan Mokhtar, Arian Mousakhan, Silvio Galesso, Jawad Tayyub, Thomas Brox*

Main category: cs.CV

TL;DR: VELM is a novel LLM-based pipeline for anomaly classification, addressing the gap in distinguishing anomaly types. It combines unsupervised anomaly detection with LLM-based classification, achieving state-of-the-art results on refined datasets.


<details>
  <summary>Details</summary>
Motivation: Anomaly classification is critical for real-world inspection but remains underexplored. Existing datasets lack precise anomaly class labels, hindering development and evaluation.

Method: VELM uses an unsupervised anomaly detection method as a vision expert to assess normality, followed by LLM-based classification if an anomaly is detected. Refined datasets (MVTec-AC, VisA-AC) are introduced for evaluation.

Result: VELM achieves 80.4% accuracy on MVTec-AD (5% improvement over baselines) and 84% on MVTec-AC, demonstrating superior anomaly classification.

Conclusion: VELM bridges the gap between detection and comprehensive anomaly characterization, inspiring further research in anomaly classification.

Abstract: Recent advances in visual industrial anomaly detection have demonstrated
exceptional performance in identifying and segmenting anomalous regions while
maintaining fast inference speeds. However, anomaly
classification-distinguishing different types of anomalies-remains largely
unexplored despite its critical importance in real-world inspection tasks. To
address this gap, we propose VELM, a novel LLM-based pipeline for anomaly
classification. Given the critical importance of inference speed, we first
apply an unsupervised anomaly detection method as a vision expert to assess the
normality of an observation. If an anomaly is detected, the LLM then classifies
its type. A key challenge in developing and evaluating anomaly classification
models is the lack of precise annotations of anomaly classes in existing
datasets. To address this limitation, we introduce MVTec-AC and VisA-AC,
refined versions of the widely used MVTec-AD and VisA datasets, which include
accurate anomaly class labels for rigorous evaluation. Our approach achieves a
state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,
exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the
effectiveness of VELM in understanding and categorizing anomalies. We hope our
methodology and benchmark inspire further research in anomaly classification,
helping bridge the gap between detection and comprehensive anomaly
characterization.

</details>


### [211] [MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation](https://arxiv.org/pdf/2505.02648)
*Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, Lihua Zhang*

Main category: cs.CV

TL;DR: Proposes MCCD, a multi-agent collaboration-based method for text-to-image generation, improving complex scene handling.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with complex prompts involving multiple objects and relations.

Method: Uses multi-agent scene parsing and hierarchical compositional diffusion with Gaussian masks for refinement.

Result: MCCD significantly outperforms baseline models in complex scene generation without training.

Conclusion: MCCD offers a robust solution for high-fidelity generation of complex scenes.

Abstract: Diffusion models have shown excellent performance in text-to-image
generation. Nevertheless, existing methods often suffer from performance
bottlenecks when handling complex prompts that involve multiple objects,
characteristics, and relations. Therefore, we propose a Multi-agent
Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation
for complex scenes. Specifically, we design a multi-agent collaboration-based
scene parsing module that generates an agent system comprising multiple agents
with distinct tasks, utilizing MLLMs to extract various scene elements
effectively. In addition, Hierarchical Compositional diffusion utilizes a
Gaussian mask and filtering to refine bounding box regions and enhance objects
through region enhancement, resulting in the accurate and high-fidelity
generation of complex scenes. Comprehensive experiments demonstrate that our
MCCD significantly improves the performance of the baseline models in a
training-free manner, providing a substantial advantage in complex scene
generation.

</details>


### [212] [Sim2Real in endoscopy segmentation with a novel structure aware image translation](https://arxiv.org/pdf/2505.02654)
*Clara Tomasini, Luis Riazuelo, Ana C. Murillo*

Main category: cs.CV

TL;DR: A novel image translation model enhances synthetic endoscopic images with realistic texture while preserving scene layout, improving training for anatomical landmark segmentation without real labeled data.


<details>
  <summary>Details</summary>
Motivation: Obtaining annotations for real endoscopic images is tedious, and synthetic data often fails to generalize. Generative methods struggle to maintain scene structure.

Method: Proposes an image translation model to add realistic texture to synthetic images while preserving key layout, tested on fold segmentation in colonoscopy.

Result: Generates realistic images that maintain fold shape/location better than existing methods, enabling successful model training without real labeled data.

Conclusion: The approach advances synthetic data utility for medical tasks, with released datasets to support further research in fold segmentation.

Abstract: Automatic segmentation of anatomical landmarks in endoscopic images can
provide assistance to doctors and surgeons for diagnosis, treatments or medical
training. However, obtaining the annotations required to train commonly used
supervised learning methods is a tedious and difficult task, in particular for
real images. While ground truth annotations are easier to obtain for synthetic
data, models trained on such data often do not generalize well to real data.
Generative approaches can add realistic texture to it, but face difficulties to
maintain the structure of the original scene. The main contribution in this
work is a novel image translation model that adds realistic texture to
simulated endoscopic images while keeping the key scene layout information. Our
approach produces realistic images in different endoscopy scenarios. We
demonstrate these images can effectively be used to successfully train a model
for a challenging end task without any real labeled data. In particular, we
demonstrate our approach for the task of fold segmentation in colonoscopy
images. Folds are key anatomical landmarks that can occlude parts of the colon
mucosa and possible polyps. Our approach generates realistic images maintaining
the shape and location of the original folds, after the
image-style-translation, better than existing methods. We run experiments both
on a novel simulated dataset for fold segmentation, and real data from the
EndoMapper (EM) dataset. All our new generated data and new EM metadata is
being released to facilitate further research, as no public benchmark is
currently available for the task of fold segmentation.

</details>


### [213] [Tailored Design of Audio-Visual Speech Recognition Models using Branchformers](https://arxiv.org/pdf/2407.06606)
*David Gimeno-Gómez, Carlos-D. Martínez-Hinarejos*

Main category: cs.CV

TL;DR: A novel parameter-efficient AVSR framework using tailored unified encoders achieves competitive WERs (2.5% for English, 9.1% for Spanish) while reducing model complexity.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of designing optimal cross-modal architectures for AVSR without excessive parameters or computational costs.

Method: Proposes a two-step framework: first, estimate modality-specific models, then design a unified encoder based on branch scores.

Result: Achieves competitive WERs (2.5% English, 9.1% Spanish) with reduced complexity.

Conclusion: The tailored AVSR system sets a new benchmark, balancing performance and efficiency.

Abstract: Recent advances in Audio-Visual Speech Recognition (AVSR) have led to
unprecedented achievements in the field, improving the robustness of this type
of system in adverse, noisy environments. In most cases, this task has been
addressed through the design of models composed of two independent encoders,
each dedicated to a specific modality. However, while recent works have
explored unified audio-visual encoders, determining the optimal cross-modal
architecture remains an ongoing challenge. Furthermore, such approaches often
rely on models comprising vast amounts of parameters and high computational
cost training processes. In this paper, we aim to bridge this research gap by
introducing a novel audio-visual framework. Our proposed method constitutes, to
the best of our knowledge, the first attempt to harness the flexibility and
interpretability offered by encoder architectures, such as the Branchformer, in
the design of parameter-efficient AVSR systems. To be more precise, the
proposed framework consists of two steps: first, estimating audio- and
video-only systems, and then designing a tailored audio-visual unified encoder
based on the layer-level branch scores provided by the modality-specific
models. Extensive experiments on English and Spanish AVSR benchmarks covering
multiple data conditions and scenarios demonstrated the effectiveness of our
proposed method. Even when trained on a moderate scale of data, our models
achieve competitive word error rates (WER) of approximately 2.5\% for English
and surpass existing approaches for Spanish, establishing a new benchmark with
an average WER of around 9.1\%. These results reflect how our tailored AVSR
system is able to reach state-of-the-art recognition rates while significantly
reducing the model complexity w.r.t. the prevalent approach in the field. Code
and pre-trained models are available at
https://github.com/david-gimeno/tailored-avsr.

</details>


### [214] [Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation](https://arxiv.org/pdf/2505.02690)
*Haotian Chen, Ziyu Liu, Xi Cheng, Chuangqi Li*

Main category: cs.CV

TL;DR: Dance of Fireworks is an interactive system using pose estimation to provide real-time feedback for radio calisthenics, enhancing engagement and accuracy with visual rewards.


<details>
  <summary>Details</summary>
Motivation: To combat sedentary health risks by making exercise more engaging and accessible.

Method: Uses mobile cameras and PoseNet/TensorFlow Lite for pose estimation, compares motions to standards, and rewards accuracy with fireworks animations.

Result: Reduced joint angle errors from 21.3° to 9.8°; 93.4% found it effective, 85.4% enjoyed it.

Conclusion: A cost-effective, engaging solution for promoting physical activity; future improvements include better accuracy and multiplayer features.

Abstract: This study introduces Dance of Fireworks, an interactive system designed to
combat sedentary health risks by enhancing engagement in radio calisthenics.
Leveraging mobile device cameras and lightweight pose estimation
(PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint
angles, and compares them with standardized motions to deliver real-time
corrective feedback. To incentivize participation, it dynamically maps users'
movements (such as joint angles and velocity) to customizable fireworks
animations, rewarding improved accuracy with richer visual effects. Experiments
involving 136 participants demonstrated a significant reduction in average
joint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four
sessions, with 93.4 percent of users affirming its exercise-promoting efficacy
and 85.4 percent praising its entertainment value. The system operates without
predefined motion templates or specialised hardware, enabling seamless
integration into office environments. Future enhancements will focus on
improving pose recognition accuracy, reducing latency, and adding features such
as multiplayer interaction and music synchronisation. This work presents a
cost-effective, engaging solution to promote physical activity in sedentary
populations.

</details>


### [215] [Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models](https://arxiv.org/pdf/2505.02824)
*Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper introduces CEAT2I, a copyright evasion attack targeting dataset ownership verification (DOV) in text-to-image diffusion models, demonstrating its effectiveness in bypassing watermarks while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: The rise of fine-tuning pre-trained T2I models raises concerns about unauthorized dataset usage, prompting the need for DOV. However, the robustness of DOV against evasion attacks is unexplored.

Method: CEAT2I involves three stages: detecting watermarked samples via faster convergence, identifying trigger tokens through iterative ablation, and erasing watermarks using closed-form concept erasure.

Result: Experiments show CEAT2I successfully evades DOV mechanisms without degrading model performance.

Conclusion: The study highlights vulnerabilities in DOV for T2I models and proposes CEAT2I as an effective evasion method, calling for stronger safeguards.

Abstract: Text-to-image (T2I) diffusion models have rapidly advanced, enabling
high-quality image generation conditioned on textual prompts. However, the
growing trend of fine-tuning pre-trained models for personalization raises
serious concerns about unauthorized dataset usage. To combat this, dataset
ownership verification (DOV) has emerged as a solution, embedding watermarks
into the fine-tuning datasets using backdoor techniques. These watermarks
remain inactive under benign samples but produce owner-specified outputs when
triggered. Despite the promise of DOV for T2I diffusion models, its robustness
against copyright evasion attacks (CEA) remains unexplored. In this paper, we
explore how attackers can bypass these mechanisms through CEA, allowing models
to circumvent watermarks even when trained on watermarked datasets. We propose
the first copyright evasion attack (i.e., CEAT2I) specifically designed to
undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three
stages: watermarked sample detection, trigger identification, and efficient
watermark mitigation. A key insight driving our approach is that T2I models
exhibit faster convergence on watermarked samples during the fine-tuning,
evident through intermediate feature deviation. Leveraging this, CEAT2I can
reliably detect the watermarked samples. Then, we iteratively ablate tokens
from the prompts of detected watermarked samples and monitor shifts in
intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a
closed-form concept erasure method to remove the injected watermark. Extensive
experiments show that our CEAT2I effectively evades DOV mechanisms while
preserving model performance.

</details>


### [216] [Structure Causal Models and LLMs Integration in Medical Visual Question Answering](https://arxiv.org/pdf/2505.02703)
*Zibo Xu, Qiang Li, Weizhi Nie, Weijie Wang, Anan Liu*

Main category: cs.CV

TL;DR: A causal inference framework is proposed for MedVQA to eliminate cross-modal bias, improving accuracy and ensuring true causal correlations in medical QA.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of cross-modal bias in MedVQA due to complex medical data, which hinders accurate QA.

Method: Introduces a causal graph for visual-textual interactions, uses mutual information to detect spurious correlations, and applies a multi-variable resampling front-door adjustment. Also includes a multi-prompt strategy.

Result: Significantly improves MedVQA accuracy and achieves true causal correlations on three datasets.

Conclusion: The framework effectively eliminates confounding effects, enhancing precision and understanding in medical QA.

Abstract: Medical Visual Question Answering (MedVQA) aims to answer medical questions
according to medical images. However, the complexity of medical data leads to
confounders that are difficult to observe, so bias between images and questions
is inevitable. Such cross-modal bias makes it challenging to infer medically
meaningful answers. In this work, we propose a causal inference framework for
the MedVQA task, which effectively eliminates the relative confounding effect
between the image and the question to ensure the precision of the
question-answering (QA) session. We are the first to introduce a novel causal
graph structure that represents the interaction between visual and textual
elements, explicitly capturing how different questions influence visual
features. During optimization, we apply the mutual information to discover
spurious correlations and propose a multi-variable resampling front-door
adjustment method to eliminate the relative confounding effect, which aims to
align features based on their true causal relevance to the question-answering
task. In addition, we also introduce a prompt strategy that combines multiple
prompt forms to improve the model's ability to understand complex medical data
and answer accurately. Extensive experiments on three MedVQA datasets
demonstrate that 1) our method significantly improves the accuracy of MedVQA,
and 2) our method achieves true causal correlations in the face of complex
medical data.

</details>


### [217] [Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery](https://arxiv.org/pdf/2505.02704)
*Bojin Wu, Jing Chen*

Main category: cs.CV

TL;DR: VGLD is a method for robust monocular depth scale recovery by stabilizing textual influence with high-level semantic information from images, achieving metric-scale accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the variability in textual descriptions affecting depth scale recovery, ensuring practical downstream task applicability.

Method: Incorporates high-level semantic information from images alongside textual descriptions to stabilize scale recovery, outputting linear transformation parameters for metric depth.

Result: Validated on multiple datasets (NYUv2, KITTI) and models (MiDas, DepthAnything), showing strong performance in zero-shot scenarios.

Conclusion: VGLD serves as a universal alignment module, robustly recovering metric-scale depth from relative depth maps.

Abstract: We propose a robust method for monocular depth scale recovery. Monocular
depth estimation can be divided into two main directions: (1) relative depth
estimation, which provides normalized or inverse depth without scale
information, and (2) metric depth estimation, which involves recovering depth
with absolute scale. To obtain absolute scale information for practical
downstream tasks, utilizing textual information to recover the scale of a
relative depth map is a highly promising approach. However, since a single
image can have multiple descriptions from different perspectives or with
varying styles, it has been shown that different textual descriptions can
significantly affect the scale recovery process. To address this issue, our
method, VGLD, stabilizes the influence of textual information by incorporating
high-level semantic information from the corresponding image alongside the
textual description. This approach resolves textual ambiguities and robustly
outputs a set of linear transformation parameters (scalars) that can be
globally applied to the relative depth map, ultimately generating depth
predictions with metric-scale accuracy. We validate our method across several
popular relative depth models(MiDas, DepthAnything), using both indoor scenes
(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions
as a universal alignment module when trained on multiple datasets, achieving
strong performance even in zero-shot scenarios. Code is available at:
https://github.com/pakinwu/VGLD.

</details>


### [218] [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/pdf/2502.20490)
*MohammadHossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Human activity is moderated by norms. However, machines are often trained
without explicit supervision on norm understanding and reasoning, particularly
when norms are physically- or socially-grounded. To improve and evaluate the
normative reasoning capability of vision-language models (VLMs), we present
\dataset{} $\|\epsilon\|$, consisting of 1,853 challenging, multi-stage MCQ
questions based on ego-centric videos of human interactions, evaluating both
the prediction and justification of normative actions. The normative actions
encompass seven categories: safety, privacy, proxemics, politeness,
cooperation, coordination/proactivity, and communication/legibility. To compile
this dataset at scale, we propose a novel pipeline leveraging video sampling,
automatic answer generation, filtering, and human validation. Our work
demonstrates that current state-of-the-art vision-language models lack robust
norm understanding, scoring a maximum of 54\% on \dataset{} (versus a human
bench of 92\%). Our analysis of performance in each dimension highlights the
significant risks of safety, privacy, and the lack of collaboration and
communication capability when applied to real-world agents. We additionally
show that through a retrieval-based generation (RAG) method, it is possible to
use \dataset{} to enhance normative reasoning in VLMs.

</details>


### [219] [A Rate-Quality Model for Learned Video Coding](https://arxiv.org/pdf/2505.02720)
*Sang NguyenQuang, Cheng-Wei Chen, Xiem HoangVan, Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: The paper introduces RQNet, a neural network to model the rate-quality (R-Q) relationship in learned video coding, improving accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: To enhance the flexibility and precision of the R-Q relationship in learned video coding by dynamically adapting model parameters.

Method: Uses RQNet to predict (R,Q) pairs and integrates them with prior frames' data via least-squares for real-time parameter adjustment.

Result: Achieves smaller bitrate deviations than baseline methods with minimal added complexity.

Conclusion: The proposed R-Q model is effective for accurate and adaptive video coding.

Abstract: Learned video coding (LVC) has recently achieved superior coding performance.
In this paper, we model the rate-quality (R-Q) relationship for learned video
coding by a parametric function. We learn a neural network, termed RQNet, to
characterize the relationship between the bitrate and quality level according
to video content and coding context. The predicted (R,Q) results are further
integrated with those from previously coded frames using the least-squares
method to determine the parameters of our R-Q model on-the-fly. Compared to the
conventional approaches, our method accurately estimates the R-Q relationship,
enabling the online adaptation of model parameters to enhance both flexibility
and precision. Experimental results show that our R-Q model achieves
significantly smaller bitrate deviations than the baseline method on commonly
used datasets with minimal additional complexity.

</details>


### [220] [Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models](https://arxiv.org/pdf/2505.02753)
*Yankai Jiang, Peng Zhang, Donglin Yang, Yuan Tian, Hai Lin, Xiaosong Wang*

Main category: cs.CV

TL;DR: DiffuGTS introduces a novel framework using frozen medical foundation diffusion models for zero-shot tumor segmentation, achieving superior performance across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing methods for tumor segmentation, such as quality, scalability, and modality range, by leveraging diffusion models.

Method: Utilizes anomaly-aware open-vocabulary attention maps and latent space inpainting for pseudo-healthy counterparts, combined with residual learning.

Result: Outperforms state-of-the-art models in zero-shot settings across four datasets and seven tumor categories.

Conclusion: DiffuGTS demonstrates enhanced quality and generalization in tumor segmentation, offering a scalable and versatile solution.

Abstract: We explore Generalizable Tumor Segmentation, aiming to train a single model
for zero-shot tumor segmentation across diverse anatomical regions. Existing
methods face limitations related to segmentation quality, scalability, and the
range of applicable imaging modalities. In this paper, we uncover the potential
of the internal representations within frozen medical foundation diffusion
models as highly efficient zero-shot learners for tumor segmentation by
introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware
open-vocabulary attention maps based on text prompts to enable generalizable
anomaly segmentation without being restricted by a predefined training category
list. To further improve and refine anomaly segmentation masks, DiffuGTS
leverages the diffusion model, transforming pathological regions into
high-quality pseudo-healthy counterparts through latent space inpainting, and
applies a novel pixel-level and feature-level residual learning approach,
resulting in segmentation masks with significantly enhanced quality and
generalization. Comprehensive experiments on four datasets and seven tumor
categories demonstrate the superior performance of our method, surpassing
current state-of-the-art models across multiple zero-shot settings. Codes are
available at https://github.com/Yankai96/DiffuGTS.

</details>


### [221] [Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance](https://arxiv.org/pdf/2505.02779)
*David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo*

Main category: cs.CV

TL;DR: An unsupervised retinal image registration pipeline eliminates the need for labeled data by inverting the conventional approach, using descriptor-based keypoints and outperforming supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing retinal image registration methods rely on labeled data, which is scarce in the medical domain, prompting the need for an unsupervised solution.

Method: The pipeline includes a descriptor learning method without keypoint detection or labels, and a label-free keypoint detector network that estimates descriptor performance.

Result: The unsupervised descriptor and detector outperform supervised and unsupervised methods, respectively, with the full pipeline matching leading supervised methods.

Conclusion: The label-free approach achieves competitive performance and can be adapted to other domains and modalities.

Abstract: Retinal image registration, particularly for color fundus images, is a
challenging yet essential task with diverse clinical applications. Existing
registration methods for color fundus images typically rely on keypoints and
descriptors for alignment; however, a significant limitation is their reliance
on labeled data, which is particularly scarce in the medical domain.
  In this work, we present a novel unsupervised registration pipeline that
entirely eliminates the need for labeled data. Our approach is based on the
principle that locations with distinctive descriptors constitute reliable
keypoints. This fully inverts the conventional state-of-the-art approach,
conditioning the detector on the descriptor rather than the opposite.
  First, we propose an innovative descriptor learning method that operates
without keypoint detection or any labels, generating descriptors for arbitrary
locations in retinal images. Next, we introduce a novel, label-free keypoint
detector network which works by estimating descriptor performance directly from
the input image.
  We validate our method through a comprehensive evaluation on four hold-out
datasets, demonstrating that our unsupervised descriptor outperforms
state-of-the-art supervised descriptors and that our unsupervised detector
significantly outperforms existing unsupervised detection methods. Finally, our
full registration pipeline achieves performance comparable to the leading
supervised methods, while not employing any labeled data. Additionally, the
label-free nature and design of our method enable direct adaptation to other
domains and modalities.

</details>


### [222] [Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge](https://arxiv.org/pdf/2505.02784)
*Vladyslav Zalevskyi, Thomas Sanchez, Misha Kaandorp, Margaux Roulet, Diego Fajardo-Rojas, Liu Li, Jana Hutter, Hongwei Bran Li, Matthew Barkovich, Hui Ji, Luca Wilhelmi, Aline Dändliker, Céline Steger, Mériam Koob, Yvan Gomez, Anton Jakovčić, Melita Klaić, Ana Adžić, Pavel Marković, Gracia Grabarić, Milan Rados, Jordina Aviles Verdera, Gregor Kasprian, Gregor Dovjak, Raphael Gaubert-Rachmühl, Maurice Aschwanden, Qi Zeng, Davood Karimi, Denis Peruzzo, Tommaso Ciceri, Giorgio Longari, Rachika E. Hamadache, Amina Bouzid, Xavier Lladó, Simone Chiarella, Gerard Martí-Juan, Miguel Ángel González Ballester, Marco Castellaro, Marco Pinamonti, Valentina Visani, Robin Cremese, Keïn Sam, Fleur Gaudfernau, Param Ahir, Mehul Parikh, Maximilian Zenk, Michael Baumgartner, Klaus Maier-Hein, Li Tianhong, Yang Hong, Zhao Longfei, Domen Preloznik, Žiga Špiclin, Jae Won Choi, Muyang Li, Jia Fu, Guotai Wang, Jingwen Jiang, Lyuyang Tong, Bo Du, Andrea Gondova, Sungmin You, Kiho Im, Abdul Qayyum, Moona Mazher, Steven A Niederer, Maya Yanko, Bella Specktor-Fadida, Dafna Ben Bashat, Andras Jakab, Roxane Licandro, Kelly Payette, Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: The FeTA Challenge 2024 benchmarked fetal brain MRI analysis, introducing biometry prediction and a low-field dataset. Segmentation accuracy nears inter-rater variability, while biometry methods struggled. Topological and data-centric improvements are needed.


<details>
  <summary>Details</summary>
Motivation: To advance automated fetal brain MRI analysis by evaluating segmentation and biometry tasks, including a novel low-field dataset and topological metrics.

Method: Sixteen teams submitted segmentation methods, and seven tackled biometry. Metrics included conventional and topology-specific (Euler characteristic difference) evaluations.

Result: Segmentation accuracy plateaued near inter-rater levels. Low-field data performed best. Biometry methods underperformed a simple baseline. Domain shift was most affected by image quality.

Conclusion: FeTA 2024 highlights the need for better topological evaluation, data diversity, and data-centric approaches to improve AI tools for fetal brain MRI.

Abstract: Accurate fetal brain tissue segmentation and biometric analysis are essential
for studying brain development in utero. The FeTA Challenge 2024 advanced
automated fetal brain MRI analysis by introducing biometry prediction as a new
task alongside tissue segmentation. For the first time, our diverse
multi-centric test set included data from a new low-field (0.55T) MRI dataset.
Evaluation metrics were also expanded to include the topology-specific Euler
characteristic difference (ED). Sixteen teams submitted segmentation methods,
most of which performed consistently across both high- and low-field scans.
However, longitudinal trends indicate that segmentation accuracy may be
reaching a plateau, with results now approaching inter-rater variability. The
ED metric uncovered topological differences that were missed by conventional
metrics, while the low-field dataset achieved the highest segmentation scores,
highlighting the potential of affordable imaging systems when paired with
high-quality reconstruction. Seven teams participated in the biometry task, but
most methods failed to outperform a simple baseline that predicted measurements
based solely on gestational age, underscoring the challenge of extracting
reliable biometric estimates from image data alone. Domain shift analysis
identified image quality as the most significant factor affecting model
generalization, with super-resolution pipelines also playing a substantial
role. Other factors, such as gestational age, pathology, and acquisition site,
had smaller, though still measurable, effects. Overall, FeTA 2024 offers a
comprehensive benchmark for multi-class segmentation and biometry estimation in
fetal brain MRI, underscoring the need for data-centric approaches, improved
topological evaluation, and greater dataset diversity to enable clinically
robust and generalizable AI tools.

</details>


### [223] [Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration](https://arxiv.org/pdf/2505.02787)
*David Rivas-Villar, Álvaro S. Hervella, José Rouco, Jorge Novo*

Main category: cs.CV

TL;DR: An unsupervised descriptor learning method for color fundus image registration is proposed, eliminating reliance on keypoint detection and matching supervised methods in performance.


<details>
  <summary>Details</summary>
Motivation: The lack of labeled data in medical imaging motivates the use of unsupervised learning for improved registration.

Method: A novel unsupervised descriptor learning method is developed, independent of keypoint detectors during inference.

Result: The method achieves accurate registration, matching supervised methods, and performs well with various keypoint detectors.

Conclusion: This work advances unsupervised learning in medical image registration, showing robustness across detectors.

Abstract: Current color fundus image registration approaches are limited, among other
things, by the lack of labeled data, which is even more significant in the
medical domain, motivating the use of unsupervised learning. Therefore, in this
work, we develop a novel unsupervised descriptor learning method that does not
rely on keypoint detection. This enables the resulting descriptor network to be
agnostic to the keypoint detector used during the registration inference.
  To validate this approach, we perform an extensive and comprehensive
comparison on the reference public retinal image registration dataset.
Additionally, we test our method with multiple keypoint detectors of varied
nature, even proposing some novel ones. Our results demonstrate that the
proposed approach offers accurate registration, not incurring in any
performance loss versus supervised methods. Additionally, it demonstrates
accurate performance regardless of the keypoint detector used. Thus, this work
represents a notable step towards leveraging unsupervised learning in the
medical domain.

</details>


### [224] [DPNet: Dynamic Pooling Network for Tiny Object Detection](https://arxiv.org/pdf/2505.02797)
*Luqi Gong, Haotian Chen, Yikun Chen, Tianliang Yao, Chao Li, Shuai Zhao, Guangjie Han*

Main category: cs.CV

TL;DR: DPNet introduces dynamic pooling and adaptive normalization for efficient tiny object detection, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate tiny object detection in UAVs is critical, but traditional resizing methods increase computational costs and degrade performance.

Method: DPNet uses a flexible down-sampling strategy with a dynamic factor (df) and a lightweight predictor, plus an Adaptive Normalization Module (ANM) for compatibility.

Result: DPNet saves 35% and 25% GFLOPs on TinyCOCO and TinyPerson datasets, respectively, without sacrificing detection performance.

Conclusion: DPNet effectively balances accuracy and efficiency in tiny object detection, making it practical for real-world UAV applications.

Abstract: In unmanned aerial systems, especially in complex environments, accurately
detecting tiny objects is crucial. Resizing images is a common strategy to
improve detection accuracy, particularly for small objects. However, simply
enlarging images significantly increases computational costs and the number of
negative samples, severely degrading detection performance and limiting its
applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny
object detection to mitigate these issues. DPNet employs a flexible
down-sampling strategy by introducing a factor (df) to relax the fixed
downsampling process of the feature map to an adjustable one. Furthermore, we
design a lightweight predictor to predict df for each input image, which is
used to decrease the resolution of feature maps in the backbone. Thus, we
achieve input-aware downsampling. We also design an Adaptive Normalization
Module (ANM) to make a unified detector compatible with different dfs. A
guidance loss supervises the predictor's training. DPNet dynamically allocates
computing resources to trade off between detection accuracy and efficiency.
Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save
over 35% and 25% GFLOPs, respectively, while maintaining comparable detection
performance. The code will be made publicly available.

</details>


### [225] [Database-Agnostic Gait Enrollment using SetTransformers](https://arxiv.org/pdf/2505.02815)
*Nicoleta Basoc, Adrian Cosma, Andy Cǎtrunǎ, Emilian Rǎdoi*

Main category: cs.CV

TL;DR: A transformer-based framework for open-set gait enrollment, dataset- and architecture-agnostic, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Real-world gait recognition needs open-set enrollment to handle unseen identities, which existing closed-set methods lack.

Method: Uses a SetTransformer for enrollment decisions, decoupled from recognition, without task-specific thresholds or retraining.

Result: Flexible, accurate, and scalable across datasets (CASIA-B, PsyMo) and recognition models (GaitGraph, GaitFormer, GaitPT).

Conclusion: The framework generalizes well, offering practical open-set gait enrollment for real-world applications.

Abstract: Gait recognition has emerged as a powerful tool for unobtrusive and
long-range identity analysis, with growing relevance in surveillance and
monitoring applications. Although recent advances in deep learning and
large-scale datasets have enabled highly accurate recognition under closed-set
conditions, real-world deployment demands open-set gait enrollment, which means
determining whether a new gait sample corresponds to a known identity or
represents a previously unseen individual. In this work, we introduce a
transformer-based framework for open-set gait enrollment that is both
dataset-agnostic and recognition-architecture-agnostic. Our method leverages a
SetTransformer to make enrollment decisions based on the embedding of a probe
sample and a context set drawn from the gallery, without requiring
task-specific thresholds or retraining for new environments. By decoupling
enrollment from the main recognition pipeline, our model is generalized across
different datasets, gallery sizes, and identity distributions. We propose an
evaluation protocol that uses existing datasets in different ratios of
identities and walks per identity. We instantiate our method using
skeleton-based gait representations and evaluate it on two benchmark datasets
(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition
models (GaitGraph, GaitFormer, and GaitPT). We show that our method is
flexible, is able to accurately perform enrollment in different scenarios, and
scales better with data compared to traditional approaches. We will make the
code and dataset scenarios publicly available.

</details>


### [226] [MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing](https://arxiv.org/pdf/2505.02823)
*Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, Qian He*

Main category: cs.CV

TL;DR: MUSAR is a framework for robust multi-subject customization using single-subject data, addressing data diversity and attribute entanglement challenges.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with limited multi-subject data and attribute entanglement across subjects.

Method: Debiased diptych learning and dynamic attention routing are introduced to enable multi-subject learning from single-subject data and decouple cross-subject attributes.

Result: MUSAR outperforms existing methods in image quality, subject consistency, and interaction naturalness, even with single-subject training.

Conclusion: MUSAR effectively addresses multi-subject customization challenges without requiring multi-subject datasets.

Abstract: Current multi-subject customization approaches encounter two critical
challenges: the difficulty in acquiring diverse multi-subject training data,
and attribute entanglement across different subjects. To bridge these gaps, we
propose MUSAR - a simple yet effective framework to achieve robust
multi-subject customization while requiring only single-subject training data.
Firstly, to break the data limitation, we introduce debiased diptych learning.
It constructs diptych training pairs from single-subject images to facilitate
multi-subject learning, while actively correcting the distribution bias
introduced by diptych construction via static attention routing and dual-branch
LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic
attention routing mechanism, which adaptively establishes bijective mappings
between generated images and conditional subjects. This design not only
achieves decoupling of multi-subject representations but also maintains
scalable generalization performance with increasing reference subjects.
Comprehensive experiments demonstrate that our MUSAR outperforms existing
methods - even those trained on multi-subject dataset - in image quality,
subject consistency, and interaction naturalness, despite requiring only
single-subject dataset.

</details>


### [227] [Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology](https://arxiv.org/pdf/2505.02825)
*Alex Hoi Hang Chan, Otto Brookes, Urs Waldmann, Hemal Naik, Iain D. Couzin, Majid Mirmehdi, Noël Adiko Houa, Emmanuelle Normand, Christophe Boesch, Lukas Boesch, Mimi Arandjelovic, Hjalmar Kühl, Tilo Burghardt, Fumihiro Kano*

Main category: cs.CV

TL;DR: The paper advocates for evaluating computer vision models in ecology/biology using application-specific metrics, not just ML metrics, and demonstrates this with two case studies.


<details>
  <summary>Details</summary>
Motivation: Current computer vision resources in ecology/biology focus on ML metrics, neglecting downstream analysis impact. The paper argues for application-specific evaluation.

Method: Two case studies: (1) chimpanzee abundance estimation using a behavior classifier, and (2) pigeon head rotation estimation using a 3D posture estimator.

Result: Models with strong ML performance (e.g., 87% mAP) can yield discrepancies in downstream ecological/biological analyses compared to expert data.

Conclusion: Researchers should integrate application-specific metrics in datasets to benchmark models for their downstream use and improve workflow integration.

Abstract: Computer vision methods have demonstrated considerable potential to
streamline ecological and biological workflows, with a growing number of
datasets and models becoming available to the research community. However,
these resources focus predominantly on evaluation using machine learning
metrics, with relatively little emphasis on how their application impacts
downstream analysis. We argue that models should be evaluated using
application-specific metrics that directly represent model performance in the
context of its final use case. To support this argument, we present two
disparate case studies: (1) estimating chimpanzee abundance and density with
camera trap distance sampling when using a video-based behaviour classifier and
(2) estimating head rotation in pigeons using a 3D posture estimator. We show
that even models with strong machine learning performance (e.g., 87% mAP) can
yield data that leads to discrepancies in abundance estimates compared to
expert-derived data. Similarly, the highest-performing models for posture
estimation do not produce the most accurate inferences of gaze direction in
pigeons. Motivated by these findings, we call for researchers to integrate
application-specific metrics in ecological/biological datasets, allowing for
models to be benchmarked in the context of their downstream application and to
facilitate better integration of models into application workflows.

</details>


### [228] [No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves](https://arxiv.org/pdf/2505.02831)
*Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, Jingdong Wang*

Main category: cs.CV

TL;DR: Self-Representation Alignment (SRA) enhances diffusion transformers' representation learning during generative training without external components, outperforming complex frameworks and matching methods with strong priors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for improving diffusion transformers require additional frameworks or pre-trained models, which are complex or resource-intensive. SRA avoids this by leveraging the model's inherent discriminative process.

Method: SRA aligns latent representations between earlier (noisier) and later (less noisy) layers via self-distillation, improving representation learning during generative training.

Result: SRA consistently improves performance in DiTs and SiTs, surpassing complex auxiliary frameworks and rivaling methods with external representation priors.

Conclusion: SRA is a simple, effective method for enhancing diffusion transformers' representation learning without external dependencies, offering practical advantages over existing approaches.

Abstract: Recent studies have demonstrated that learning a meaningful internal
representation can both accelerate generative training and enhance generation
quality of the diffusion transformers. However, existing approaches necessitate
to either introduce an additional and complex representation training framework
or rely on a large-scale, pre-trained representation foundation model to
provide representation guidance during the original generative training
process. In this study, we posit that the unique discriminative process
inherent to diffusion transformers enables them to offer such guidance without
requiring external representation components. We therefore propose
Self-Representation A}lignment (SRA), a simple yet straightforward method that
obtain representation guidance through a self-distillation manner.
Specifically, SRA aligns the output latent representation of the diffusion
transformer in earlier layer with higher noise to that in later layer with
lower noise to progressively enhance the overall representation learning during
only generative training process. Experimental results indicate that applying
SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA
not only significantly outperforms approaches relying on auxiliary, complex
representation training frameworks but also achieves performance comparable to
methods that heavily dependent on powerful external representation priors.

</details>


### [229] [Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation](https://arxiv.org/pdf/2505.02836)
*Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, Zhaoshuo Li*

Main category: cs.CV

TL;DR: Scenethesis integrates LLM-based scene planning with vision-guided refinement to create realistic 3D interactive scenes from text.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-3D scene synthesis lack diversity and spatial realism, limiting their practical use.

Method: Scenethesis combines LLM-based coarse layout drafting, vision-guided refinement, optimization for physical plausibility, and spatial coherence verification.

Result: The framework generates diverse, realistic, and physically plausible 3D scenes.

Conclusion: Scenethesis is effective for virtual content creation, simulations, and embodied AI research.

Abstract: Synthesizing interactive 3D scenes from text is essential for gaming, virtual
reality, and embodied AI. However, existing methods face several challenges.
Learning-based approaches depend on small-scale indoor datasets, limiting the
scene diversity and layout complexity. While large language models (LLMs) can
leverage diverse text-domain knowledge, they struggle with spatial realism,
often producing unnatural object placements that fail to respect common sense.
Our key insight is that vision perception can bridge this gap by providing
realistic spatial guidance that LLMs lack. To this end, we introduce
Scenethesis, a training-free agentic framework that integrates LLM-based scene
planning with vision-guided layout refinement. Given a text prompt, Scenethesis
first employs an LLM to draft a coarse layout. A vision module then refines it
by generating an image guidance and extracting scene structure to capture
inter-object relations. Next, an optimization module iteratively enforces
accurate pose alignment and physical plausibility, preventing artifacts like
object penetration and instability. Finally, a judge module verifies spatial
coherence. Comprehensive experiments show that Scenethesis generates diverse,
realistic, and physically plausible 3D interactive scenes, making it valuable
for virtual content creation, simulation environments, and embodied AI
research.

</details>


### [230] [CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement](https://arxiv.org/pdf/2404.02225)
*Di Qiu, Yinda Zhang, Thabo Beeler, Vladimir Tankovich, Christian Häne, Sean Fanello, Christoph Rhemann, Sergio Orts Escolano*

Main category: cs.CV

TL;DR: CHOSEN is a flexible, robust multi-view depth refinement framework that improves depth accuracy in stereo pipelines using contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To enhance depth estimation in multi-view stereo systems by refining initial depth hypotheses adaptively.

Method: Iteratively re-samples and selects best hypotheses using contrastive learning and a designed feature for distinguishing positive/negative hypotheses.

Result: CHOSEN outperforms deep learning-based multi-view stereo pipelines in depth and normal accuracy.

Conclusion: CHOSEN is effective for refining depth estimates in diverse multi-view capture systems.

Abstract: We propose CHOSEN, a simple yet flexible, robust and effective multi-view
depth refinement framework. It can be employed in any existing multi-view
stereo pipeline, with straightforward generalization capability for different
multi-view capture systems such as camera relative positioning and lenses.
Given an initial depth estimation, CHOSEN iteratively re-samples and selects
the best hypotheses, and automatically adapts to different metric or intrinsic
scales determined by the capture system. The key to our approach is the
application of contrastive learning in an appropriate solution space and a
carefully designed hypothesis feature, based on which positive and negative
hypotheses can be effectively distinguished. Integrated in a simple baseline
multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of
depth and normal accuracy compared to many current deep learning based
multi-view stereo pipelines.

</details>


### [231] [FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Matching](https://arxiv.org/pdf/2404.01249)
*Rohit Jena, Pratik Chaudhari, James C. Gee*

Main category: cs.CV

TL;DR: FireANTs is a multi-scale adaptive Riemannian optimization algorithm for dense diffeomorphic image matching, addressing ill-conditioning and achieving state-of-the-art performance with significant speedups.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the ill-conditioned nature of diffeomorphic image matching, particularly in MRI tasks, necessitating adaptive optimization to improve accuracy and robustness.

Method: FireANTs generalizes momentum and adaptive Hessian estimates for non-Euclidean spaces, formalizes multi-scale optimization, and provides rigorous mathematical results.

Result: The algorithm achieves remarkable accuracy, robustness, and speed (300x-3200x faster than CPU-based methods), enabling tasks like sub-micron mouse isocortex matching and hyperparameter studies.

Conclusion: FireANTs sets a new benchmark for dense diffeomorphic image matching, with broad applicability across medical and biological imaging challenges.

Abstract: The paper proposes FireANTs, the first multi-scale Adaptive Riemannian
Optimization algorithm for dense diffeomorphic image matching. One of the most
critical and understudied aspects of diffeomorphic image matching algorithms
are its highly ill-conditioned nature. We quantitatively capture the extent of
ill-conditioning in a typical MRI matching task, motivating the need for an
adaptive optimization algorithm for diffeomorphic matching. To this end,
FireANTs generalizes the concept of momentum and adaptive estimates of the
Hessian to mitigate this ill-conditioning in the non-Euclidean space of
diffeomorphisms. Unlike common non-Euclidean manifolds, we also formalize
considerations for multi-scale optimization of diffeomorphisms. Our rigorous
mathematical results and operational contributions lead to a state-of-the-art
dense matching algorithm that can be applied to generic image data with
remarkable accuracy and robustness. We demonstrate consistent improvements in
image matching performance across a spectrum of community-standard medical and
biological correspondence matching challenges spanning a wide variety of image
modalities, anatomies, resolutions, acquisition protocols, and preprocessing
pipelines. This improvement is supplemented by 300x to 3200x speedup over
existing CPU-based state-of-the-art algorithms. For the first time, we perform
diffeomorphic matching of sub-micron mouse isocortex volumes at native
resolution, and generate a 25{\mu}m mouse brain atlas in under 25 minutes. Our
fast implementation also enables hyperparameter studies that were intractable
with existing correspondence matching algorithms.

</details>


### [232] [MOWA: Multiple-in-One Image Warping Model](https://arxiv.org/pdf/2404.10716)
*Kang Liao, Zongsheng Yue, Zhonghua Wu, Chen Change Loy*

Main category: cs.CV

TL;DR: MOWA is a single model for multiple image warping tasks, outperforming task-specific models and generalizing to unseen scenes.


<details>
  <summary>Details</summary>
Motivation: Existing image warping models require separate training for each task and lack generalization across camera models or custom manipulations.

Method: MOWA disentangles motion estimation at region and pixel levels and uses a point-based classifier for dynamic task-aware warping.

Result: MOWA outperforms state-of-the-art task-specific models and shows generalization potential in cross-domain and zero-shot evaluations.

Conclusion: MOWA is the first single model for multiple warping tasks, offering superior performance and generalization.

Abstract: While recent image warping approaches achieved remarkable success on existing
benchmarks, they still require training separate models for each specific task
and cannot generalize well to different camera models or customized
manipulations. To address diverse types of warping in practice, we propose a
Multiple-in-One image WArping model (named MOWA) in this work. Specifically, we
mitigate the difficulty of multi-task learning by disentangling the motion
estimation at both the region level and pixel level. To further enable dynamic
task-aware image warping, we introduce a lightweight point-based classifier
that predicts the task type, serving as prompts to modulate the feature maps
for more accurate estimation. To our knowledge, this is the first work that
solves multiple practical warping tasks in one single model. Extensive
experiments demonstrate that our MOWA, which is trained on six tasks for
multiple-in-one image warping, outperforms state-of-the-art task-specific
models across most tasks. Moreover, MOWA also exhibits promising potential to
generalize into unseen scenes, as evidenced by cross-domain and zero-shot
evaluations. The code and more visual results can be found on the project page:
https://kangliao929.github.io/projects/mowa/.

</details>


### [233] [Enhancing person re-identification via Uncertainty Feature Fusion Method and Auto-weighted Measure Combination](https://arxiv.org/pdf/2405.01101)
*Quang-Huy Che, Le-Chuong Nguyen, Duc-Tuan Luu, Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: A new method (UFFM + AMC) improves person re-identification by fusing multi-view features and combining similarity measures, achieving significant accuracy boosts on datasets like MSMT17 and Occluded-DukeMTMC.


<details>
  <summary>Details</summary>
Motivation: Current Re-ID methods rely on single-camera features, limiting performance in multi-camera scenarios with challenges like viewpoint changes and occlusions.

Method: Proposes Uncertain Feature Fusion Method (UFFM) for multi-view features and Auto-weighted Measure Combination (AMC) for robust similarity measures.

Result: Achieves 7.9% Rank@1 and 12.1% mAP improvement on MSMT17, and 22.0% Rank@1 and 18.4% mAP improvement on Occluded-DukeMTMC.

Conclusion: The UFFM and AMC combination effectively enhances Re-ID performance by addressing view bias and leveraging multi-view and single-view features.

Abstract: Person re-identification (Re-ID) is a challenging task that involves
identifying the same person across different camera views in surveillance
systems. Current methods usually rely on features from single-camera views,
which can be limiting when dealing with multiple cameras and challenges such as
changing viewpoints and occlusions. In this paper, a new approach is introduced
that enhances the capability of ReID models through the Uncertain Feature
Fusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM
generates multi-view features using features extracted independently from
multiple images to mitigate view bias. However, relying only on similarity
based on multi-view features is limited because these features ignore the
details represented in single-view features. Therefore, we propose the AMC
method to generate a more robust similarity measure by combining various
measures. Our method significantly improves Rank@1 accuracy and Mean Average
Precision (mAP) when evaluated on person re-identification datasets. Combined
with the BoT Baseline on challenging datasets, we achieve impressive results,
with a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17
dataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0%
and mAP by 18.4%. Code is available:
https://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC

</details>


### [234] [NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation](https://arxiv.org/pdf/2405.13745)
*Qiujie Dong, Huibiao Wen, Rui Xu, Shuangmin Chen, Jiaran Zhou, Shiqing Xin, Changhe Tu, Taku Komura, Wenping Wang*

Main category: cs.CV

TL;DR: NeurCross is a framework optimizing cross fields and neural SDFs for high-quality quadrilateral mesh generation, addressing alignment and smoothness challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with balancing cross field smoothness and alignment to principal curvatures, which are sensitive to perturbations.

Method: Joint optimization of cross field and neural SDF, guided by SDF approximation, curvature alignment, and cross field smoothness.

Result: NeurCross provides more regular principal curvature directions and robust quadrangulation.

Conclusion: The framework improves quadrilateral mesh quality by leveraging neural SDFs for better alignment and smoothness.

Abstract: Quadrilateral mesh generation plays a crucial role in numerical simulations
within Computer-Aided Design and Engineering (CAD/E). Producing high-quality
quadrangulation typically requires satisfying four key criteria. First, the
quadrilateral mesh should closely align with principal curvature directions.
Second, singular points should be strategically placed and effectively
minimized. Third, the mesh should accurately conform to sharp feature edges.
Lastly, quadrangulation results should exhibit robustness against noise and
minor geometric variations. Existing methods generally involve first computing
a regular cross field to represent quad element orientations across the
surface, followed by extracting a quadrilateral mesh aligned closely with this
cross field. A primary challenge with this approach is balancing the smoothness
of the cross field with its alignment to pre-computed principal curvature
directions, which are sensitive to small surface perturbations and often
ill-defined in spherical or planar regions.
  To tackle this challenge, we propose NeurCross, a novel framework that
simultaneously optimizes a cross field and a neural signed distance function
(SDF), whose zero-level set serves as a proxy of the input shape. Our joint
optimization is guided by three factors: faithful approximation of the
optimized SDF surface to the input surface, alignment between the cross field
and the principal curvature field derived from the SDF surface, and smoothness
of the cross field. Acting as an intermediary, the neural SDF contributes in
two essential ways. First, it provides an alternative, optimizable base surface
exhibiting more regular principal curvature directions for guiding the cross
field. Second, we leverage the Hessian matrix of the neural SDF to implicitly
enforce cross field alignment with principal curvature directions...

</details>


### [235] [ParallelEdits: Efficient Multi-object Image Editing](https://arxiv.org/pdf/2406.00985)
*Mingzhen Huang, Jialing Cai, Shan Jia, Vishnu Suresh Lokhande, Siwei Lyu*

Main category: cs.CV

TL;DR: ParallelEdits enables simultaneous multi-attribute image editing with improved efficiency and quality, supported by the PIE-Bench++ dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and computational demands of sequential multi-attribute edits in text-driven image synthesis.

Method: Developed ParallelEdits with an attention distribution mechanism and multi-branch design for simultaneous edits.

Result: Preserves single-attribute edit quality while enhancing multitasking performance.

Conclusion: ParallelEdits and PIE-Bench++ advance text-driven image editing for complex, multi-object scenarios.

Abstract: Text-driven image synthesis has made significant advancements with the
development of diffusion models, transforming how visual content is generated
from text prompts. Despite these advances, text-driven image editing, a key
area in computer graphics, faces unique challenges. A major challenge is making
simultaneous edits across multiple objects or attributes. Applying these
methods sequentially for multi-attribute edits increases computational demands
and efficiency losses. In this paper, we address these challenges with
significant contributions. Our main contribution is the development of
ParallelEdits, a method that seamlessly manages simultaneous edits across
multiple attributes. In contrast to previous approaches, ParallelEdits not only
preserves the quality of single attribute edits but also significantly improves
the performance of multitasking edits. This is achieved through innovative
attention distribution mechanism and multi-branch design that operates across
several processing heads. Additionally, we introduce the PIE-Bench++ dataset,
an expansion of the original PIE-Bench dataset, to better support evaluating
image-editing tasks involving multiple objects and attributes simultaneously.
This dataset is a benchmark for evaluating text-driven image editing methods in
multifaceted scenarios.

</details>


### [236] [MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis](https://arxiv.org/pdf/2407.02329)
*Dewei Zhou, You Li, Fan Ma, Zongxin Yang, Yi Yang*

Main category: cs.CV

TL;DR: The paper introduces Multi-Instance Generation (MIG), a task for generating multiple instances in a single image with precise attributes and positions. It proposes MIGC and MIGC++ to address challenges like attribute leakage and diverse descriptions, and introduces Consistent-MIG for iterative consistency. Benchmarks show superior performance.


<details>
  <summary>Details</summary>
Motivation: To enable precise generation of multiple instances in a single image with strict adherence to user specifications, addressing challenges like attribute leakage and consistency.

Method: Proposes MIGC (divide-and-conquer for single-instance tasks) and MIGC++ (text/image attribute control, box/mask position control), along with Consistent-MIG for iterative consistency.

Result: Outperforms existing methods on COCO-MIG, Multimodal-MIG, COCO-Position, and DrawBench benchmarks, maintaining precise control over position, attributes, and quantity.

Conclusion: The proposed methods effectively address MIG challenges, offering robust solutions for multi-instance generation with high precision and consistency.

Abstract: We introduce the Multi-Instance Generation (MIG) task, which focuses on
generating multiple instances within a single image, each accurately placed at
predefined positions with attributes such as category, color, and shape,
strictly following user specifications. MIG faces three main challenges:
avoiding attribute leakage between instances, supporting diverse instance
descriptions, and maintaining consistency in iterative generation. To address
attribute leakage, we propose the Multi-Instance Generation Controller (MIGC).
MIGC generates multiple instances through a divide-and-conquer strategy,
breaking down multi-instance shading into single-instance tasks with singular
attributes, later integrated. To provide more types of instance descriptions,
we developed MIGC++. MIGC++ allows attribute control through text \& images and
position control through boxes \& masks. Lastly, we introduced the
Consistent-MIG algorithm to enhance the iterative MIG ability of MIGC and
MIGC++. This algorithm ensures consistency in unmodified regions during the
addition, deletion, or modification of instances, and preserves the identity of
instances when their attributes are changed. We introduce the COCO-MIG and
Multimodal-MIG benchmarks to evaluate these methods. Extensive experiments on
these benchmarks, along with the COCO-Position benchmark and DrawBench,
demonstrate that our methods substantially outperform existing techniques,
maintaining precise control over aspects including position, attribute, and
quantity. Project page: https://github.com/limuloo/MIGC.

</details>


### [237] [CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation](https://arxiv.org/pdf/2407.05576)
*Yuejiao Su, Yi Wang, Lap-Pui Chau*

Main category: cs.CV

TL;DR: CaRe-Ego improves hand-object segmentation by modeling interactive relationships and decoupling object categories, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model hand-object interactions and coupled object relationships, limiting segmentation performance.

Method: CaRe-Ego uses a Hand-guided Object Feature Enhancer (HOFE) for interactive relationships and a Contact-centric Object Decoupling Strategy (CODS) for disentangling object categories.

Result: Experiments show CaRe-Ego outperforms existing methods with robust generalization.

Conclusion: CaRe-Ego advances egocentric hand-object segmentation by emphasizing contact-aware feature learning.

Abstract: Egocentric Interactive hand-object segmentation (EgoIHOS) requires the
segmentation of hands and interacting objects in egocentric images, which is
crucial for understanding human behavior in assistive systems. Previous methods
typically recognize hands and interacting objects as distinct semantic
categories based solely on visual features, or simply use hand predictions as
auxiliary cues for object segmentation. Despite the promising progress achieved
by these methods, they fail to adequately model the interactive relationships
between hands and objects while ignoring the coupled physical relationships
among object categories, ultimately constraining their segmentation
performance. To make up for the shortcomings of existing methods, we propose a
novel method called CaRe-Ego that achieves state-of-the-art performance by
emphasizing the contact between hands and objects from two aspects. First, we
introduce a Hand-guided Object Feature Enhancer (HOFE) to establish the
hand-object interactive relationships to extract more contact-relevant and
discriminative object features. Second, we design the Contact-centric Object
Decoupling Strategy (CODS) to explicitly model and disentangle coupling
relationships among object categories, thereby emphasizing contact-aware
feature learning. Experiments on various in-domain and out-of-domain test sets
show that Care-Ego significantly outperforms existing methods with robust
generalization capability. Codes are publicly available at
https://github.com/yuggiehk/CaRe-Ego/.

</details>


### [238] [How Does Audio Influence Visual Attention in Omnidirectional Videos? Database and Model](https://arxiv.org/pdf/2408.05411)
*Yuxin Zhu, Huiyu Duan, Kaiwei Zhang, Yucheng Zhu, Xilei Zhu, Long Teng, Xiongkuo Min, Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces a new audio-visual saliency database (AVS-ODV) for omnidirectional videos and proposes an advanced saliency prediction model (OmniAVS) that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing user engagement in VR/AR by understanding and predicting viewer attention in ODVs, addressing the lack of large-scale audio-visual saliency databases and joint modality exploitation.

Method: Creation of the AVS-ODV database with 162 ODVs and eye-tracking data from 60 subjects under three audio modes. Development of the OmniAVS model, a U-Net-based network for hierarchical fusion of audio and visual features.

Result: OmniAVS outperforms state-of-the-art models in saliency prediction for ODVs and traditional tasks.

Conclusion: The AVS-ODV database and OmniAVS model advance audio-visual saliency research and will be released to support future work.

Abstract: Understanding and predicting viewer attention in omnidirectional videos
(ODVs) is crucial for enhancing user engagement in virtual and augmented
reality applications. Although both audio and visual modalities are essential
for saliency prediction in ODVs, the joint exploitation of these two modalities
has been limited, primarily due to the absence of large-scale audio-visual
saliency databases and comprehensive analyses. This paper comprehensively
investigates audio-visual attention in ODVs from both subjective and objective
perspectives. Specifically, we first introduce a new audio-visual saliency
database for omnidirectional videos, termed AVS-ODV database, containing 162
ODVs and corresponding eye movement data collected from 60 subjects under three
audio modes including mute, mono, and ambisonics. Based on the constructed
AVS-ODV database, we perform an in-depth analysis of how audio influences
visual attention in ODVs. To advance the research on audio-visual saliency
prediction for ODVs, we further establish a new benchmark based on the AVS-ODV
database by testing numerous state-of-the-art saliency models, including
visual-only models and audio-visual models. In addition, given the limitations
of current models, we propose an innovative omnidirectional audio-visual
saliency prediction network (OmniAVS), which is built based on the U-Net
architecture, and hierarchically fuses audio and visual features from the
multimodal aligned embedding space. Extensive experimental results demonstrate
that the proposed OmniAVS model outperforms other state-of-the-art models on
both ODV AVS prediction and traditional AVS predcition tasks. The AVS-ODV
database and OmniAVS model will be released to facilitate future research.

</details>


### [239] [Face Clustering via Early Stopping and Edge Recall](https://arxiv.org/pdf/2408.13431)
*Junjie Liu*

Main category: cs.CV

TL;DR: The paper introduces unsupervised (FC-ES) and supervised (FC-ESER) face clustering algorithms to improve efficiency and accuracy in large-scale face clustering.


<details>
  <summary>Details</summary>
Motivation: Existing methods are complex and inefficient, making them impractical for real-world applications. The need for simpler, more efficient, and unsupervised approaches is highlighted.

Method: FC-ES uses neighbor-based edge probability and early stopping for accuracy and recall. FC-ESER adds an edge recall strategy to leverage supervised learning.

Result: Experiments show FC-ES and FC-ESER outperform state-of-the-art methods on face, person, and vehicle clustering benchmarks.

Conclusion: The proposed algorithms offer efficient and accurate solutions for large-scale face clustering, with potential real-world applicability.

Abstract: Large-scale face clustering has achieved significant progress, with many
efforts dedicated to learning to cluster large-scale faces with
supervised-learning. However, complex model design and tedious clustering
processes are typical in existing methods. Such limitations result in
infeasible clustering in real-world applications. Reasonable and efficient
model design and training need to be taken into account. Besides, developing
unsupervised face clustering algorithms is crucial, which are more realistic in
real-world applications. In this paper, we propose a novel unsupervised face
clustering algorithm FC-ES and a novel supervised face clustering algorithm
FC-ESER to address these issues. An efficient and effective neighbor-based edge
probability and a novel early stopping strategy are proposed in FC-ES,
guaranteeing the accuracy and recall of large-scale face clustering
simultaneously. Furthermore, to take advantage of supervised learning, a novel
edge recall strategy is proposed in FC-ESER to further recall the edge
connections that are not connected in FC-ES. Extensive experiments on multiple
benchmarks for face, person, and vehicle clustering show that our proposed
FC-ES and FC-ESER significantly outperform previous state-of-the-art methods.
Our code will be available at https://github.com/jumptoliujj/FC-ESER.

</details>


### [240] [CFCPalsy: Facial Image Synthesis with Cross-Fusion Cycle Diffusion Model for Facial Paralysis Individuals](https://arxiv.org/pdf/2409.07271)
*Weixiang Gao, Yating Zhang, Yifan Xia*

Main category: cs.CV

TL;DR: The paper proposes a novel generative model (CFCPalsy) to synthesize high-quality facial paralysis datasets, addressing the scarcity of data for training robust machine learning models in automated diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current facial paralysis diagnosis relies on subjective clinician judgment, leading to variability. Limited datasets hinder the development of accurate automated systems.

Method: A Cross-Fusion Cycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion model is introduced to synthesize realistic facial paralysis images by combining facial features and enhancing visual details.

Result: The method outperforms state-of-the-art techniques, generating realistic facial images with identity consistency, as validated on public clinical datasets.

Conclusion: The CFCPalsy model effectively addresses dataset scarcity, enabling more accurate and efficient training of algorithms for facial paralysis diagnosis.

Abstract: Currently, the diagnosis of facial paralysis remains a challenging task,
often relying heavily on the subjective judgment and experience of clinicians,
which can introduce variability and uncertainty in the assessment process. One
promising application in real-life situations is the automatic estimation of
facial paralysis. However, the scarcity of facial paralysis datasets limits the
development of robust machine learning models for automated diagnosis and
therapeutic interventions. To this end, this study aims to synthesize a
high-quality facial paralysis dataset to address this gap, enabling more
accurate and efficient algorithm training. Specifically, a novel Cross-Fusion
Cycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion model
is proposed to combine different features of facial information and enhance the
visual details of facial appearance and texture in facial regions, thus
creating synthetic facial images that accurately represent various degrees and
types of facial paralysis. We have qualitatively and quantitatively evaluated
the proposed method on the commonly used public clinical datasets of facial
paralysis to demonstrate its effectiveness. Experimental results indicate that
the proposed method surpasses state-of-the-art methods, generating more
realistic facial images and maintaining identity consistency.

</details>


### [241] [MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection](https://arxiv.org/pdf/2409.09724)
*Yaning Zhang, Tianyi Wang, Zitong Yu, Zan Gao, Linlin Shen, Shengyong Chen*

Main category: cs.CV

TL;DR: The paper proposes a multi-modal fine-grained CLIP (MFCLIP) model for generalizable diffusion face forgery detection (DFFD), leveraging image-noise modalities and language-guided learning to improve detection of unseen diffusion-synthesized faces.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing face forgery detection methods, which struggle with unseen diffusion-synthesized faces and underutilize multi-modal data like fine-grained noises and texts.

Method: Introduces MFCLIP with a fine-grained language encoder (FLE), multi-modal vision encoder (MVE), and sample pair attention (SPA) to mine comprehensive forgery traces across image-noise modalities.

Result: Outperforms state-of-the-art methods in cross-generator, cross-forgery, and cross-dataset evaluations.

Conclusion: The MFCLIP model advances DFFD by effectively leveraging multi-modal data and language-guided learning for robust forgery detection.

Abstract: The rapid development of photo-realistic face generation methods has raised
significant concerns in society and academia, highlighting the urgent need for
robust and generalizable face forgery detection (FFD) techniques. Although
existing approaches mainly capture face forgery patterns using image modality,
other modalities like fine-grained noises and texts are not fully explored,
which limits the generalization capability of the model. In addition, most FFD
methods tend to identify facial images generated by GAN, but struggle to detect
unseen diffusion-synthesized ones. To address the limitations, we aim to
leverage the cutting-edge foundation model, contrastive language-image
pre-training (CLIP), to achieve generalizable diffusion face forgery detection
(DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP
(MFCLIP) model, which mines comprehensive and fine-grained forgery traces
across image-noise modalities via language-guided face forgery representation
learning, to facilitate the advancement of DFFD. Specifically, we devise a
fine-grained language encoder (FLE) that extracts fine global language features
from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to
capture global image forgery embeddings as well as fine-grained noise forgery
patterns extracted from the richest patch, and integrate them to mine general
visual forgery traces. Moreover, we build an innovative plug-and-play sample
pair attention (SPA) method to emphasize relevant negative pairs and suppress
irrelevant ones, allowing cross-modality sample pairs to conduct more flexible
alignment. Extensive experiments and visualizations show that our model
outperforms the state of the arts on different settings like cross-generator,
cross-forgery, and cross-dataset evaluations.

</details>


### [242] [3D Vision-Language Gaussian Splatting](https://arxiv.org/pdf/2410.07577)
*Qucheng Peng, Benjamin Planche, Zhongpai Gao, Meng Zheng, Anwesa Choudhuri, Terrence Chen, Chen Chen, Ziyan Wu*

Main category: cs.CV

TL;DR: A new 3D vision-language Gaussian splatting model improves multi-modal scene understanding by balancing visual and language modalities, enhancing semantic rasterization and reducing over-fitting.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal scene understanding methods inadequately balance visual and language modalities, leading to poor semantic rasterization and over-fitting.

Method: Proposes a 3D vision-language Gaussian splatting model with a cross-modal rasterizer, modality fusion, smoothed semantic indicator, and camera-view blending.

Result: Achieves state-of-the-art performance in open-vocabulary semantic segmentation, outperforming existing methods significantly.

Conclusion: The proposed method effectively addresses limitations in multi-modal scene understanding, improving semantic rasterization and reducing over-fitting.

Abstract: Recent advancements in 3D reconstruction methods and vision-language models
have propelled the development of multi-modal 3D scene understanding, which has
vital applications in robotics, autonomous driving, and virtual/augmented
reality. However, current multi-modal scene understanding approaches have
naively embedded semantic representations into 3D reconstruction methods
without striking a balance between visual and language modalities, which leads
to unsatisfying semantic rasterization of translucent or reflective objects, as
well as over-fitting on color modality. To alleviate these limitations, we
propose a solution that adequately handles the distinct visual and semantic
modalities, i.e., a 3D vision-language Gaussian splatting model for scene
understanding, to put emphasis on the representation learning of language
modality. We propose a novel cross-modal rasterizer, using modality fusion
along with a smoothed semantic indicator for enhancing semantic rasterization.
We also employ a camera-view blending technique to improve semantic consistency
between existing and synthesized views, thereby effectively mitigating
over-fitting. Extensive experiments demonstrate that our method achieves
state-of-the-art performance in open-vocabulary semantic segmentation,
surpassing existing methods by a significant margin.

</details>


### [243] [Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models](https://arxiv.org/pdf/2410.10821)
*Jingzhi Bao, Xueting Li, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: Tex4D is a zero-shot method for generating multi-view and temporally consistent 4D textures for mesh sequences by integrating 3D geometry knowledge with video diffusion models.


<details>
  <summary>Details</summary>
Motivation: Creating realistic and temporally consistent textures for 3D mesh sequences is labor-intensive, and existing video diffusion models lack 3D geometry awareness.

Method: Tex4D synchronizes diffusion across views via latent aggregation in UV space and leverages a conditional video generation model for temporal consistency, with modifications to DDIM sampling and a reference latent texture for clarity.

Result: Tex4D outperforms in producing multi-view and multi-frame consistent textures for untextured mesh sequences.

Conclusion: Tex4D is the first method designed for 4D scene texturing, offering superior consistency and realism.

Abstract: 3D meshes are widely used in computer vision and graphics for their
efficiency in animation and minimal memory use, playing a crucial role in
movies, games, AR, and VR. However, creating temporally consistent and
realistic textures for mesh sequences remains labor-intensive for professional
artists. On the other hand, while video diffusion models excel at text-driven
video generation, they often lack 3D geometry awareness and struggle with
achieving multi-view consistent texturing for 3D meshes. In this work, we
present Tex4D, a zero-shot approach that integrates inherent 3D geometry
knowledge from mesh sequences with the expressiveness of video diffusion models
to produce multi-view and temporally consistent 4D textures. Given an
untextured mesh sequence and a text prompt as inputs, our method enhances
multi-view consistency by synchronizing the diffusion process across different
views through latent aggregation in the UV space. To ensure temporal
consistency, we leverage prior knowledge from a conditional video generation
model for texture synthesis. However, straightforwardly combining the video
diffusion model and the UV texture aggregation leads to blurry results. We
analyze the underlying causes and propose a simple yet effective modification
to the DDIM sampling process to address this issue. Additionally, we introduce
a reference latent texture to strengthen the correlation between frames during
the denoising process. To the best of our knowledge, Tex4D is the first method
specifically designed for 4D scene texturing. Extensive experiments demonstrate
its superiority in producing multi-view and multi-frame consistent videos based
on untextured mesh sequences.

</details>


### [244] [CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model Retrieval](https://arxiv.org/pdf/2411.02979)
*Xin Wen, Xuening Zhu, Renjiao Yi, Zhifeng Wang, Chenyang Zhu, Kai Xu*

Main category: cs.CV

TL;DR: CAD-NeRF reconstructs 3D scenes from fewer than 10 images without known poses by leveraging a CAD model library for shape and pose initialization, then optimizing geometry and texture.


<details>
  <summary>Details</summary>
Motivation: Existing NeRF methods require accurate poses or many images. CAD-NeRF addresses the challenge of few-view, pose-free reconstruction.

Method: Uses a CAD model library for shape and pose initialization, then jointly optimizes density field deformation and camera poses in a self-supervised manner.

Result: CAD-NeRF achieves accurate densities and large deformations from retrieved CAD models, demonstrating generalization.

Conclusion: CAD-NeRF effectively tackles few-view, pose-free 3D reconstruction, showing promise for practical applications.

Abstract: Reconstructing from multi-view images is a longstanding problem in 3D vision,
where neural radiance fields (NeRFs) have shown great potential and get
realistic rendered images of novel views. Currently, most NeRF methods either
require accurate camera poses or a large number of input images, or even both.
Reconstructing NeRF from few-view images without poses is challenging and
highly ill-posed. To address this problem, we propose CAD-NeRF, a method
reconstructed from less than 10 images without any known poses. Specifically,
we build a mini library of several CAD models from ShapeNet and render them
from many random views. Given sparse-view input images, we run a model and pose
retrieval from the library, to get a model with similar shapes, serving as the
density supervision and pose initializations. Here we propose a multi-view pose
retrieval method to avoid pose conflicts among views, which is a new and unseen
problem in uncalibrated NeRF methods. Then, the geometry of the object is
trained by the CAD guidance. The deformation of the density field and camera
poses are optimized jointly. Then texture and density are trained and
fine-tuned as well. All training phases are in self-supervised manners.
Comprehensive evaluations of synthetic and real images show that CAD-NeRF
successfully learns accurate densities with a large deformation from retrieved
CAD models, showing the generalization abilities.

</details>


### [245] [Unlocking Transfer Learning for Open-World Few-Shot Recognition](https://arxiv.org/pdf/2411.09986)
*Byeonggeun Kim, Juntae Lee, Kyuhong Shim, Simyung Chang*

Main category: cs.CV

TL;DR: The paper proposes a two-stage method for Few-Shot Open-Set Recognition (FSOSR), combining open-set aware meta-learning and open-set free transfer learning, achieving state-of-the-art results with minimal training overhead.


<details>
  <summary>Details</summary>
Motivation: Address the failure of transfer learning in open-world FSOSR by developing a method that effectively categorizes known classes while identifying open-set inputs.

Method: A two-stage approach: open-set aware meta-learning to create a metric space, followed by open-set free transfer learning for task adaptation. Includes simulating open-set examples.

Result: Achieves top performance on miniImageNet and tieredImageNet benchmarks with only a 1.5% increase in training effort.

Conclusion: Demonstrates the effectiveness of transfer learning in FSOSR, providing a scalable solution for real-world applications.

Abstract: Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world
challenge, aiming to categorize inputs into known categories, termed closed-set
classes, while identifying open-set inputs that fall outside these classes.
Although transfer learning where a model is tuned to a given few-shot task has
become a prominent paradigm in closed-world, we observe that it fails to expand
to open-world. To unlock this challenge, we propose a two-stage method which
consists of open-set aware meta-learning with open-set free transfer learning.
In the open-set aware meta-learning stage, a model is trained to establish a
metric space that serves as a beneficial starting point for the subsequent
stage. During the open-set free transfer learning stage, the model is further
adapted to a specific target task through transfer learning. Additionally, we
introduce a strategy to simulate open-set examples by modifying the training
dataset or generating pseudo open-set examples. The proposed method achieves
state-of-the-art performance on two widely recognized benchmarks, miniImageNet
and tieredImageNet, with only a 1.5\% increase in training effort. Our work
demonstrates the effectiveness of transfer learning in FSOSR.

</details>


### [246] [Large Language Model with Region-guided Referring and Grounding for CT Report Generation](https://arxiv.org/pdf/2411.15539)
*Zhixuan Chen, Yequan Bie, Haibo Jin, Hao Chen*

Main category: cs.CV

TL;DR: Reg2RG is a region-guided framework for CT report generation, enhancing diagnostic performance by focusing on anatomical regions and integrating local and global features.


<details>
  <summary>Details</summary>
Motivation: Existing methods miss abnormalities by focusing only on global CT volume features, lacking attention to specific regions.

Method: Uses masks for local features, a local feature decoupling strategy, and integrates local and global features. A region-report alignment training strategy and LLM decoder are employed.

Result: Outperforms state-of-the-art methods in natural language generation and clinical efficacy, with improved interpretability.

Conclusion: Reg2RG effectively improves CT report generation by focusing on regions, integrating features, and enhancing interpretability.

Abstract: Computed tomography (CT) report generation is crucial to assist radiologists
in interpreting CT volumes, which can be time-consuming and labor-intensive.
Existing methods primarily only consider the global features of the entire
volume, making it struggle to focus on specific regions and potentially missing
abnormalities. To address this issue, we propose Reg2RG, the first
region-guided referring and grounding framework for CT report generation, which
enhances diagnostic performance by focusing on anatomical regions within the
volume. Specifically, we utilize masks from a universal segmentation module to
capture local features for each referring region. A local feature decoupling
(LFD) strategy is proposed to preserve the local high-resolution details with
little computational overhead. Then the local features are integrated with
global features to capture inter-regional relationships within a cohesive
context. Moreover, we propose a novel region-report alignment (RRA) training
strategy. It leverages the recognition of referring regions to guide the
generation of region-specific reports, enhancing the model's referring and
grounding capabilities while also improving the report's interpretability. A
large language model (LLM) is further employed as the language decoder to
generate reports from integrated visual features, facilitating region-level
comprehension. Extensive experiments on two large-scale chest CT-report
datasets demonstrate the superiority of our method, which outperforms several
state-of-the-art methods in terms of both natural language generation and
clinical efficacy metrics while preserving promising interpretability. The code
is available at https://github.com/zhi-xuan-chen/Reg2RG.

</details>


### [247] [The Double-Ellipsoid Geometry of CLIP](https://arxiv.org/pdf/2411.14517)
*Meir Yossef Levi, Guy Gilboa*

Main category: cs.CV

TL;DR: The paper analyzes CLIP's embedding geometry, revealing text and image embeddings reside on linearly separable ellipsoid shells. It introduces conformity to measure uncertainty and shows CLIP's modality gap optimizes conformity matching.


<details>
  <summary>Details</summary>
Motivation: To understand the geometry of CLIP's embeddings, which is not well explored, and explain its benefits for embedding uncertainty during contrastive training.

Method: Examines unnormalized CLIP embeddings, introduces conformity (average cosine similarity), and analyzes the modality gap's role.

Result: Text and image embeddings lie on separable ellipsoid shells; conformity can be estimated via modality mean similarity; modality gap optimizes conformity matching.

Conclusion: CLIP's embedding structure improves uncertainty handling, and conformity provides a practical measure for understanding embedding behavior.

Abstract: Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in
machine learning applications within a large variety of domains. We investigate
the geometry of this embedding, which is still not well understood. We examine
the raw unnormalized embedding and show that text and image reside on linearly
separable ellipsoid shells, not centered at the origin. We explain the benefits
of having this structure, allowing to better embed instances according to their
uncertainty during contrastive training. Frequent concepts in the dataset yield
more false negatives, inducing greater uncertainty. A new notion of conformity
is introduced, which measures the average cosine similarity of an instance to
any other instance within a representative data set. We show this measure can
be accurately estimated by simply computing the cosine similarity to the
modality mean vector. Furthermore, we find that CLIP's modality gap optimizes
the matching of the conformity distributions of image and text.

</details>


### [248] [AMO Sampler: Enhancing Text Rendering with Overshooting](https://arxiv.org/pdf/2411.19415)
*Xixi Hu, Keyang Xu, Bo Liu, Qiang Liu, Hongliang Fei*

Main category: cs.CV

TL;DR: A training-free method, AMO, improves text rendering in text-to-image models by adaptively controlling overshooting strength based on attention scores.


<details>
  <summary>Details</summary>
Motivation: Current models like SD3 and Flux struggle with accurate text depiction in images, leading to misspelled or inconsistent text.

Method: Introduces an overshooting sampler for RF models, alternating between over-simulating ODE and reintroducing noise, and an Attention Modulated Overshooting (AMO) sampler to adaptively control overshooting.

Result: AMO improves text rendering accuracy by 32.3% (SD3) and 35.9% (Flux) without compromising image quality or increasing inference cost.

Conclusion: AMO effectively addresses text rendering issues in text-to-image models with minimal computational overhead.

Abstract: Achieving precise alignment between textual instructions and generated images
in text-to-image generation is a significant challenge, particularly in
rendering written text within images. Sate-of-the-art models like Stable
Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text
depiction, resulting in misspelled or inconsistent text. We introduce a
training-free method with minimal computational overhead that significantly
enhances text rendering quality. Specifically, we introduce an overshooting
sampler for pretrained rectified flow (RF) models, by alternating between
over-simulating the learned ordinary differential equation (ODE) and
reintroducing noise. Compared to the Euler sampler, the overshooting sampler
effectively introduces an extra Langevin dynamics term that can help correct
the compounding error from successive Euler steps and therefore improve the
text rendering. However, when the overshooting strength is high, we observe
over-smoothing artifacts on the generated images. To address this issue, we
propose an Attention Modulated Overshooting sampler (AMO), which adaptively
controls the strength of overshooting for each image patch according to their
attention score with the text content. AMO demonstrates a 32.3% and 35.9%
improvement in text rendering accuracy on SD3 and Flux without compromising
overall image quality or increasing inference cost. Code available at:
https://github.com/hxixixh/amo-release.

</details>


### [249] [Context-Aware Input Orchestration for Video Inpainting](https://arxiv.org/pdf/2411.16926)
*Hoyoung Kim, Azimbek Khudoyberdiev, Seonghwan Jeong, Jihoon Ryoo*

Main category: cs.CV

TL;DR: Optimizing memory usage in video inpainting by dynamically adjusting input frame composition improves quality on mobile devices.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with mobile constraints; this research aims to enhance inpainting quality by optimizing input data.

Method: Dynamically adjust input frame composition using optical flow and mask changes, moving beyond fixed five-frame sets.

Result: Improved inpainting quality, especially for rapid visual context changes.

Conclusion: Dynamic input frame adjustment is effective for mobile-friendly video inpainting.

Abstract: Traditional neural network-driven inpainting methods struggle to deliver
high-quality results within the constraints of mobile device processing power
and memory. Our research introduces an innovative approach to optimize memory
usage by altering the composition of input data. Typically, video inpainting
relies on a predetermined set of input frames, such as neighboring and
reference frames, often limited to five-frame sets. Our focus is to examine how
varying the proportion of these input frames impacts the quality of the
inpainted video. By dynamically adjusting the input frame composition based on
optical flow and changes of the mask, we have observed an improvement in
various contents including rapid visual context changes.

</details>


### [250] [LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes](https://arxiv.org/pdf/2412.00592)
*Shing-Hei Ho, Bao Thach, Minghan Zhu*

Main category: cs.CV

TL;DR: LiDAR-EDIT is a framework for editing real-world LiDAR scans to create synthetic data with customizable object layouts while maintaining background realism.


<details>
  <summary>Details</summary>
Motivation: To provide a controllable and realistic method for generating synthetic LiDAR data, addressing limitations of end-to-end generation and novel view synthesis.

Method: Uses spherical voxelization for correct LiDAR geometry and generative models for filling occluded areas during object removal/insertion.

Result: Produces realistic LiDAR scans with practical utility for downstream tasks.

Conclusion: LiDAR-EDIT offers a flexible and realistic solution for synthetic LiDAR data generation in autonomous driving.

Abstract: We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data
for autonomous driving. Our framework edits real-world LiDAR scans by
introducing new object layouts while preserving the realism of the background
environment. Compared to end-to-end frameworks that generate LiDAR point clouds
from scratch, LiDAR-EDIT offers users full control over the object layout,
including the number, type, and pose of objects, while keeping most of the
original real-world background. Our method also provides object labels for the
generated data. Compared to novel view synthesis techniques, our framework
allows for the creation of counterfactual scenarios with object layouts
significantly different from the original real-world scene. LiDAR-EDIT uses
spherical voxelization to enforce correct LiDAR projective geometry in the
generated point clouds by construction. During object removal and insertion,
generative models are employed to fill the unseen background and object parts
that were occluded in the original real LiDAR scans. Experimental results
demonstrate that our framework produces realistic LiDAR scans with practical
value for downstream tasks.

</details>


### [251] [Active Data Curation Effectively Distills Large-Scale Multimodal Models](https://arxiv.org/pdf/2411.18674)
*Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, Olivier J. Hénaff*

Main category: cs.CV

TL;DR: ACID, a simple online batch selection method for active data curation, outperforms complex KD strategies and complements standard KD, achieving state-of-the-art results with reduced inference costs.


<details>
  <summary>Details</summary>
Motivation: To explore a simpler alternative to complex knowledge distillation (KD) methods for compressing large models, focusing on active data curation for contrastive multimodal pretraining.

Method: Proposes ACID, an online batch selection method for active data curation, and combines it with standard KD in the ACED framework for efficient pretraining.

Result: ACED achieves state-of-the-art performance on 27 zero-shot tasks with up to 11% fewer inference FLOPs and improves vision-encoder performance for generative tasks.

Conclusion: Active data curation is a simple yet effective alternative or complement to KD, enabling highly performant and efficient models.

Abstract: Knowledge distillation (KD) is the de facto standard for compressing
large-scale models into smaller ones. Prior works have explored ever more
complex KD strategies involving different objective functions,
teacher-ensembles, and weight inheritance. In this work we explore an
alternative, yet simple approach -- active data curation as effective
distillation for contrastive multimodal pretraining. Our simple online batch
selection method, ACID, outperforms strong KD baselines across various model-,
data- and compute-configurations. Further, we find such an active data curation
strategy to in fact be complementary to standard KD, and can be effectively
combined to train highly performant inference-efficient models. Our simple and
scalable pretraining framework, ACED, achieves state-of-the-art results across
27 zero-shot classification and retrieval tasks with upto 11% less inference
FLOPs. We further demonstrate that our ACED models yield strong vision-encoders
for training generative multimodal models in the LiT-Decoder setting,
outperforming larger vision encoders for image-captioning and visual
question-answering tasks.

</details>


### [252] [BrushEdit: All-In-One Image Inpainting and Editing](https://arxiv.org/pdf/2412.10316)
*Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Junhao Zhuang, Ying Shan, Yuexian Zou, Qiang Xu*

Main category: cs.CV

TL;DR: BrushEdit is a new image editing method combining MLLMs and inpainting models for free-form instruction-based edits, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current inversion-based methods struggle with large edits, and instruction-based methods limit user interaction. BrushEdit aims to overcome these issues.

Method: Integrates MLLMs and a dual-branch inpainting model for autonomous, interactive editing, handling tasks like classification, object identification, and inpainting.

Result: Superior performance across seven metrics, including mask preservation and editing coherence.

Conclusion: BrushEdit effectively combines MLLMs and inpainting, enabling user-friendly, high-quality image editing.

Abstract: Image editing has advanced significantly with the development of diffusion
models using both inversion-based and instruction-based methods. However,
current inversion-based approaches struggle with big modifications (e.g.,
adding or removing objects) due to the structured nature of inversion noise,
which hinders substantial changes. Meanwhile, instruction-based methods often
constrain users to black-box operations, limiting direct interaction for
specifying editing regions and intensity. To address these limitations, we
propose BrushEdit, a novel inpainting-based instruction-guided image editing
paradigm, which leverages multimodal large language models (MLLMs) and image
inpainting models to enable autonomous, user-friendly, and interactive
free-form instruction editing. Specifically, we devise a system enabling
free-form instruction editing by integrating MLLMs and a dual-branch image
inpainting model in an agent-cooperative framework to perform editing category
classification, main object identification, mask acquisition, and editing area
inpainting. Extensive experiments show that our framework effectively combines
MLLMs and inpainting models, achieving superior performance across seven
metrics including mask region preservation and editing effect coherence.

</details>


### [253] [ColorFlow: Retrieval-Augmented Image Sequence Colorization](https://arxiv.org/pdf/2412.11815)
*Junhao Zhuang, Xuan Ju, Zhaoyang Zhang, Yong Liu, Shiyi Zhang, Chun Yuan, Ying Shan*

Main category: cs.CV

TL;DR: ColorFlow is a three-stage diffusion-based framework for automatic black-and-white image sequence colorization, addressing challenges like controllability and identity consistency. It outperforms existing models and introduces a new benchmark, ColorFlow-Bench.


<details>
  <summary>Details</summary>
Motivation: The demand for consistent and controllable colorization in industrial applications like cartoons or comics, where current methods lack robustness and generalizability.

Method: A three-stage diffusion-based framework with a dual-branch design: one for color identity extraction and another for colorization, leveraging self-attention in diffusion models.

Result: ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization.

Conclusion: ColorFlow provides a robust and generalizable solution for industrial image sequence colorization, with potential benefits for the art industry.

Abstract: Automatic black-and-white image sequence colorization while preserving
character and object identity (ID) is a complex task with significant market
demand, such as in cartoon or comic series colorization. Despite advancements
in visual colorization using large-scale generative models like diffusion
models, challenges with controllability and identity consistency persist,
making current solutions unsuitable for industrial application.To address this,
we propose ColorFlow, a three-stage diffusion-based framework tailored for
image sequence colorization in industrial applications. Unlike existing methods
that require per-ID finetuning or explicit ID embedding extraction, we propose
a novel robust and generalizable Retrieval Augmented Colorization pipeline for
colorizing images with relevant color references. Our pipeline also features a
dual-branch design: one branch for color identity extraction and the other for
colorization, leveraging the strengths of diffusion models. We utilize the
self-attention mechanism in diffusion models for strong in-context learning and
color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a
comprehensive benchmark for reference-based colorization. Results show that
ColorFlow outperforms existing models across multiple metrics, setting a new
standard in sequential image colorization and potentially benefiting the art
industry. We release our codes and models on our project page:
https://zhuang2002.github.io/ColorFlow/.

</details>


### [254] [Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations](https://arxiv.org/pdf/2412.14803)
*Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen*

Main category: cs.CV

TL;DR: VPP leverages video diffusion models (VDMs) for robot action learning by predicting future frames, improving performance on benchmarks and real-world tasks.


<details>
  <summary>Details</summary>
Motivation: Existing vision encoders focus on static information, missing dynamic aspects crucial for robotics. VDMs, with their ability to predict future frames, offer a solution.

Method: Proposes Video Prediction Policy (VPP), learning inverse dynamics using VDMs' future predictions. Fine-tunes pre-trained video models on robot and human data.

Result: VPP improves Calvin ABC-D benchmark by 18.6% and boosts real-world dexterous manipulation success by 31.6%.

Conclusion: VDMs' dynamic representations enhance robotic policies, with VPP demonstrating significant performance gains.

Abstract: Visual representations play a crucial role in developing generalist robotic
policies. Previous vision encoders, typically pre-trained with single-image
reconstruction or two-image contrastive learning, tend to capture static
information, often neglecting the dynamic aspects vital for embodied tasks.
Recently, video diffusion models (VDMs) demonstrate the ability to predict
future frames and showcase a strong understanding of physical world. We
hypothesize that VDMs inherently produce visual representations that encompass
both current static information and predicted future dynamics, thereby
providing valuable guidance for robot action learning. Based on this
hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit
inverse dynamics model conditioned on predicted future representations inside
VDMs. To predict more precise future, we fine-tune pre-trained video foundation
model on robot datasets along with internet human manipulation data. In
experiments, VPP achieves a 18.6\% relative improvement on the Calvin ABC-D
generalization benchmark compared to the previous state-of-the-art, and
demonstrates a 31.6\% increase in success rates for complex real-world
dexterous manipulation tasks. Project page at
https://video-prediction-policy.github.io

</details>


### [255] [landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images](https://arxiv.org/pdf/2501.10098)
*Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke*

Main category: cs.CV

TL;DR: A Python package called landmarker is introduced for anatomical landmark localization in medical images, offering flexibility, accuracy, and modularity not found in general-purpose tools.


<details>
  <summary>Details</summary>
Motivation: Existing general-purpose tools for landmark localization lack specialized features and modularity needed for medical imaging applications.

Method: landmarker is built on PyTorch and supports methodologies like static and adaptive heatmap regression, with customizable modules for specific datasets.

Result: The package improves landmark identification accuracy, streamlines R&D, and supports various image formats and preprocessing pipelines.

Conclusion: landmarker fills a gap in precision and customization for medical landmark localization, outperforming general-purpose tools.

Abstract: Anatomical landmark localization in 2D/3D images is a critical task in
medical imaging. Although many general-purpose tools exist for landmark
localization in classical computer vision tasks, such as pose estimation, they
lack the specialized features and modularity necessary for anatomical landmark
localization applications in the medical domain. Therefore, we introduce
landmarker, a Python package built on PyTorch. The package provides a
comprehensive, flexible toolkit for developing and evaluating landmark
localization algorithms, supporting a range of methodologies, including static
and adaptive heatmap regression. landmarker enhances the accuracy of landmark
identification, streamlines research and development processes, and supports
various image formats and preprocessing pipelines. Its modular design allows
users to customize and extend the toolkit for specific datasets and
applications, accelerating innovation in medical imaging. landmarker addresses
a critical need for precision and customization in landmark localization tasks
not adequately met by existing general-purpose pose estimation tools.

</details>


### [256] [Dynamic Scene Understanding from Vision-Language Representations](https://arxiv.org/pdf/2501.11653)
*Shahaf Pruss, Morris Alper, Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: A framework leveraging frozen vision-language (V&L) representations achieves state-of-the-art results in dynamic scene understanding with minimal trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods for parsing complex scenes rely on task-specific engineering, while modern V&L representations offer potential for more generic solutions.

Method: The proposed framework uses frozen V&L representations to predict and parse structured text or concatenate representations into existing models.

Result: State-of-the-art performance is achieved with fewer trainable parameters, and analysis confirms dynamic scene semantics are effectively encoded.

Conclusion: Modern V&L representations enable efficient and effective dynamic scene understanding without extensive task-specific engineering.

Abstract: Images depicting complex, dynamic scenes are challenging to parse
automatically, requiring both high-level comprehension of the overall situation
and fine-grained identification of participating entities and their
interactions. Current approaches use distinct methods tailored to sub-tasks
such as Situation Recognition and detection of Human-Human and Human-Object
Interactions. However, recent advances in image understanding have often
leveraged web-scale vision-language (V&L) representations to obviate
task-specific engineering. In this work, we propose a framework for dynamic
scene understanding tasks by leveraging knowledge from modern, frozen V&L
representations. By framing these tasks in a generic manner - as predicting and
parsing structured text, or by directly concatenating representations to the
input of existing models - we achieve state-of-the-art results while using a
minimal number of trainable parameters relative to existing approaches.
Moreover, our analysis of dynamic knowledge of these representations shows that
recent, more powerful representations effectively encode dynamic scene
semantics, making this approach newly possible.

</details>


### [257] [A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs](https://arxiv.org/pdf/2501.13620)
*Mohit Vaishnav, Tanel Tammet*

Main category: cs.CV

TL;DR: The paper introduces a structured evaluation framework for Vision-Language Models (VLMs) using Bongard Problems and Winoground tasks to study their visual reasoning. It proposes three paradigms (DVRL, DRL, CA) mirroring human problem-solving, with CA achieving SOTA performance by decoupling perception from reasoning.


<details>
  <summary>Details</summary>
Motivation: Understanding how VLMs integrate visual perception with abstract thought, especially in multi-image or fine-grained reasoning tasks, is a key challenge in AI.

Method: The paper proposes three evaluation paradigms (DVRL, DRL, CA) to dissect VLMs' perception-reasoning interface, varying cognitive load and processing stages. CA uses textual descriptions to isolate reasoning from perception.

Result: CA achieves SOTA performance on benchmarks like Bongard-OpenWorld and Winoground, showing reasoning improves when perceptual challenges are mitigated.

Conclusion: Decoupling perception from reasoning via task-agnostic descriptions is promising for robust visual intelligence, and the framework serves as a diagnostic tool.

Abstract: A fundamental challenge in artificial intelligence involves understanding the
cognitive mechanisms underlying visual reasoning in sophisticated models like
Vision-Language Models (VLMs). How do these models integrate visual perception
with abstract thought, especially when reasoning across multiple images or
requiring fine-grained compositional understanding? Drawing inspiration from
cognitive science, this paper introduces a structured evaluation framework
using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to
dissect the perception-reasoning interface in VLMs. We propose three distinct
evaluation paradigms, mirroring human problem-solving strategies: Direct Visual
Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule
extraction and application), and Componential Analysis (CA; analytical
decomposition via task-agnostic textual descriptions). These paradigms
systematically vary cognitive load and probe processing stages. Notably, CA
enables multi-image reasoning evaluation even for single-image architectures
and isolates reasoning from perception by operating on textual descriptions.
Applying this framework, we demonstrate that CA, leveraging powerful language
models for reasoning over rich, independently generated descriptions, achieves
new state-of-the-art (SOTA) performance on challenging benchmarks including
Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm
reasoning improves significantly when perceptual challenges are mitigated,
revealing a critical perception bottleneck. Our framework provides a valuable
diagnostic tool and suggests that decoupling perception (via rich,
task-agnostic description) from reasoning is a promising direction for robust
and general visual intelligence.

</details>


### [258] [CoDe: Blockwise Control for Denoising Diffusion Models](https://arxiv.org/pdf/2502.00968)
*Anuj Singh, Sayak Mukherjee, Ahmad Beirami, Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: CoDe is a gradient-free, inference-time method for aligning diffusion models with downstream tasks, avoiding finetuning or differentiable guidance.


<details>
  <summary>Details</summary>
Motivation: Aligning diffusion models to downstream tasks typically requires finetuning or gradient-based guidance, which can be complex or costly.

Method: CoDe uses blockwise sampling during intermediate denoising steps to align with rewards, without needing differentiable functions or finetuning.

Result: CoDe achieves competitive performance in reward alignment and prompt following, with low inference cost.

Conclusion: CoDe offers a simple yet effective alternative to existing alignment methods for diffusion models.

Abstract: Aligning diffusion models to downstream tasks often requires finetuning new
models or gradient-based guidance at inference time to enable sampling from the
reward-tilted posterior. In this work, we explore a simple inference-time
gradient-free guidance approach, called controlled denoising (CoDe), that
circumvents the need for differentiable guidance functions and model
finetuning. CoDe is a blockwise sampling method applied during intermediate
denoising steps, allowing for alignment with downstream rewards. Our
experiments demonstrate that, despite its simplicity, CoDe offers a favorable
trade-off between reward alignment, prompt instruction following, and inference
cost, achieving a competitive performance against the state-of-the-art
baselines. Our code is available at: https://github.com/anujinho/code.

</details>


### [259] [DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray Security Inspection](https://arxiv.org/pdf/2502.01710)
*Shilong Hong, Yanzhou Zhou, Weichao Xu*

Main category: cs.CV

TL;DR: DAGNet, a dual-view attention-guided network, improves X-ray security inspection by dynamically enhancing features, aligning views, and fusing information, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Single-view X-ray scanners struggle with contraband identification in complex scenarios due to viewpoint dependency and poor feature representation.

Method: DAGNet uses a shared-weight backbone with three modules: FDIM for frequency-based feature enhancement, DVHEM for cross-view alignment, and CGFM for fusion.

Result: DAGNet outperforms state-of-the-art methods across multiple backbone architectures.

Conclusion: DAGNet effectively addresses limitations of single-view systems, enhancing dual-view X-ray security inspection performance.

Abstract: With the rapid development of modern transportation systems and the
exponential growth of logistics volumes, intelligent X-ray-based security
inspection systems play a crucial role in public safety. Although single-view
X-ray baggage scanner is widely deployed, they struggles to accurately identify
contraband in complex stacking scenarios due to strong viewpoint dependency and
inadequate feature representation. To address this, we propose a Dual-View
Attention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This
study builds on a shared-weight backbone network as the foundation and
constructs three key modules that work together: (1) Frequency Domain
Interaction Module (FDIM) dynamically enhances features by adjusting frequency
components based on inter-view relationships; (2) Dual-View Hierarchical
Enhancement Module (DVHEM) employs cross-attention to align features between
views and capture hierarchical associations; (3) Convolutional Guided Fusion
Module (CGFM) fuses features to suppress redundancy while retaining critical
discriminative information. Collectively, these modules substantially improve
the performance of dual-view X-ray security inspection. Experimental results
demonstrate that DAGNet outperforms existing state-of-the-art approaches across
multiple backbone architectures. The code is available
at:https://github.com/ShilongHong/DAGNet.

</details>


### [260] [SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training](https://arxiv.org/pdf/2502.15251)
*Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato*

Main category: cs.CV

TL;DR: SimHand is a framework for pre-training 3D hand pose estimation using in-the-wild hand images and contrastive learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods for 3D hand pose pre-training didn't fully leverage diverse in-the-wild hand images, limiting performance.

Method: Collects 2.0M hand images from videos, uses contrastive learning with similarity-based pairs, and adaptively weights loss.

Result: Outperforms PeCLR with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.

Conclusion: SimHand effectively leverages diverse hand images and contrastive learning for superior 3D hand pose estimation.

Abstract: We present a framework for pre-training of 3D hand pose estimation from
in-the-wild hand images sharing with similar hand characteristics, dubbed
SimHand. Pre-training with large-scale images achieves promising results in
various tasks, but prior methods for 3D hand pose pre-training have not fully
utilized the potential of diverse hand images accessible from in-the-wild
videos. To facilitate scalable pre-training, we first prepare an extensive pool
of hand images from in-the-wild videos and design our pre-training method with
contrastive learning. Specifically, we collect over 2.0M hand images from
recent human-centric videos, such as 100DOH and Ego4D. To extract
discriminative information from these images, we focus on the similarity of
hands: pairs of non-identical samples with similar hand poses. We then propose
a novel contrastive learning method that embeds similar hand pairs closer in
the feature space. Our method not only learns from similar samples but also
adaptively weights the contrastive learning loss based on inter-sample
distance, leading to additional performance gains. Our experiments demonstrate
that our method outperforms conventional contrastive learning approaches that
produce positive pairs sorely from a single image with data augmentation. We
achieve significant improvements over the state-of-the-art method (PeCLR) in
various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on
AssemblyHands.
  Our code is available at https://github.com/ut-vision/SiMHand.

</details>


### [261] [Visual Adaptive Prompting for Compositional Zero-Shot Learning](https://arxiv.org/pdf/2502.20292)
*Kyle Stein, Arash Mahyari, Guillermo Francia, Eman El-Sheikh*

Main category: cs.CV

TL;DR: The paper introduces VAPS, a Visual Adaptive Prompting System for CZSL, using dynamic visual prompts to improve generalization by bridging semantic and visual features.


<details>
  <summary>Details</summary>
Motivation: Current CZSL methods rely on static text prompts, failing to adapt to varying visual contexts. VAPS aims to address this by leveraging visual features for better compositional reasoning.

Method: VAPS uses a learnable visual prompt repository and similarity-based retrieval to dynamically select relevant prompts based on image features. It includes a visual prompt adapter for generalizable embeddings.

Result: Experiments on three CZSL benchmarks show state-of-the-art performance in both closed and open-world scenarios.

Conclusion: VAPS effectively bridges the gap between semantic and visual features, outperforming existing methods in CZSL tasks.

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
learning joint representations of visual and textual data, making them powerful
tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires
models to generalize to novel combinations of visual primitives-such as
attributes and objects-that were not explicitly encountered during training.
Recent works in prompting for CZSL have focused on modifying inputs for the
text encoder, often using static prompts that do not change across varying
visual contexts. However, these approaches struggle to fully capture varying
visual contexts, as they focus on text adaptation rather than leveraging visual
features for compositional reasoning. To address this, we propose Visual
Adaptive Prompting System (VAPS) that leverages a learnable visual prompt
repository and similarity-based retrieval mechanism within the framework of
VLMs to bridge the gap between semantic and visual features. Our method
introduces a dynamic visual prompt repository mechanism that selects the most
relevant attribute and object prompts based on the visual features of the
image. Our proposed system includes a visual prompt adapter that encourages the
model to learn a more generalizable embedding space. Experiments on three CZSL
benchmarks, across both closed and open-world scenarios, demonstrate
state-of-the-art results.

</details>


### [262] [Vision-based 3D Semantic Scene Completion via Capture Dynamic Representations](https://arxiv.org/pdf/2503.06222)
*Meng Wang, Fan Wu, Yunchuan Qin, Ruihui Li, Zhuo Tang, Kenli Li*

Main category: cs.CV

TL;DR: CDScene improves semantic scene completion by decoupling dynamic and static features from 2D images, enhancing accuracy in autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Dynamic objects in scenes disrupt 3D structure inference from 2D images, and existing methods fail to address multi-view consistency violations.

Method: Uses a multimodal model for 2D-to-3D semantic alignment, decouples scene info into dynamic/static features, and employs a fusion module for robust feature aggregation.

Result: Outperforms state-of-the-art methods on SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets.

Conclusion: CDScene offers a robust solution for semantic scene completion by effectively handling dynamic objects and static contexts.

Abstract: The vision-based semantic scene completion task aims to predict dense
geometric and semantic 3D scene representations from 2D images. However, the
presence of dynamic objects in the scene seriously affects the accuracy of the
model inferring 3D structures from 2D images. Existing methods simply stack
multiple frames of image input to increase dense scene semantic information,
but ignore the fact that dynamic objects and non-texture areas violate
multi-view consistency and matching reliability. To address these issues, we
propose a novel method, CDScene: Vision-based Robust Semantic Scene Completion
via Capturing Dynamic Representations. First, we leverage a multimodal
large-scale model to extract 2D explicit semantics and align them into 3D
space. Second, we exploit the characteristics of monocular and stereo depth to
decouple scene information into dynamic and static features. The dynamic
features contain structural relationships around dynamic objects, and the
static features contain dense contextual spatial information. Finally, we
design a dynamic-static adaptive fusion module to effectively extract and
aggregate complementary features, achieving robust and accurate semantic scene
completion in autonomous driving scenarios. Extensive experimental results on
the SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate
the superiority and robustness of CDScene over existing state-of-the-art
methods.

</details>


### [263] [Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning](https://arxiv.org/pdf/2503.06457)
*Yanbiao Ma, Wei Dai, Wenke Huang, Jiayi Chen*

Main category: cs.CV

TL;DR: A geometry-guided data generation method (GGEUR) is proposed to address data heterogeneity in federated learning by simulating global embedding distributions locally, improving performance in scenarios with label and domain skew.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning causes divergent local optimization and hinders global model training, especially when label and domain skew coexist. Existing methods are unstable for highly heterogeneous data.

Method: Introduces the concept of geometric shape of embedding distributions and proposes GGEUR, which generates new samples guided by global geometric shapes. It augments samples in single-domain scenarios and uses class prototypes for multi-domain scenarios.

Result: The method significantly improves performance in handling highly heterogeneous data, including label skew, domain skew, and their coexistence.

Conclusion: GGEUR effectively addresses data heterogeneity in federated learning by leveraging global geometric shapes, outperforming existing approaches in diverse scenarios.

Abstract: Data heterogeneity in federated learning, characterized by a significant
misalignment between local and global distributions, leads to divergent local
optimization directions and hinders global model training. Existing studies
mainly focus on optimizing local updates or global aggregation, but these
indirect approaches demonstrate instability when handling highly heterogeneous
data distributions, especially in scenarios where label skew and domain skew
coexist. To address this, we propose a geometry-guided data generation method
that centers on simulating the global embedding distribution locally. We first
introduce the concept of the geometric shape of an embedding distribution and
then address the challenge of obtaining global geometric shapes under privacy
constraints. Subsequently, we propose GGEUR, which leverages global geometric
shapes to guide the generation of new samples, enabling a closer approximation
to the ideal global distribution. In single-domain scenarios, we augment
samples based on global geometric shapes to enhance model generalization; in
multi-domain scenarios, we further employ class prototypes to simulate the
global distribution across domains. Extensive experimental results demonstrate
that our method significantly enhances the performance of existing approaches
in handling highly heterogeneous data, including scenarios with label skew,
domain skew, and their coexistence. Code published at:
https://github.com/WeiDai-David/2025CVPR_GGEUR

</details>


### [264] [SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting](https://arxiv.org/pdf/2504.00159)
*Advaith V. Sethuraman, Max Rucker, Onur Bagoren, Pou-Chun Kung, Nibarkavi N. B. Amutha, Katherine A. Skinner*

Main category: cs.CV

TL;DR: SonarSplat is a Gaussian splatting framework for imaging sonar, improving novel view synthesis and modeling acoustic streaking. It outperforms state-of-the-art methods in image quality and 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To enhance sonar imaging by realistically synthesizing views and modeling acoustic streaking, addressing limitations in current methods.

Method: Represents scenes with 3D Gaussians, incorporating acoustic reflectance and saturation. Introduces efficient rasterization and a novel approach to model azimuth streaking.

Result: Outperforms state-of-the-art with +3.2 dB PSNR and 52% lower Chamfer Distance. Also enables azimuth streak removal.

Conclusion: SonarSplat advances sonar imaging with realistic synthesis, accurate reconstruction, and streaking modeling, proving effective in real-world scenarios.

Abstract: In this paper, we present SonarSplat, a novel Gaussian splatting framework
for imaging sonar that demonstrates realistic novel view synthesis and models
acoustic streaking phenomena. Our method represents the scene as a set of 3D
Gaussians with acoustic reflectance and saturation properties. We develop a
novel method to efficiently rasterize Gaussians to produce a range/azimuth
image that is faithful to the acoustic image formation model of imaging sonar.
In particular, we develop a novel approach to model azimuth streaking in a
Gaussian splatting framework. We evaluate SonarSplat using real-world datasets
of sonar images collected from an underwater robotic platform in a controlled
test tank and in a real-world river environment. Compared to the
state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2
dB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We
also demonstrate that SonarSplat can be leveraged for azimuth streak removal.

</details>


### [265] [Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis](https://arxiv.org/pdf/2504.03471)
*Xi Wang, Ziqi He, Yang Zhou*

Main category: cs.CV

TL;DR: The paper introduces a method to dynamically re-weight Transformer blocks in U-Net-based diffusion models, improving inference efficiency and sample quality.


<details>
  <summary>Details</summary>
Motivation: Prior work overlooked the dynamic importance of attention blocks during inference, limiting their potential for enhancing image applications.

Method: The authors theoretically justify re-weighting Transformer blocks, propose an Importance Probe to quantify dynamic shifts, and design an adaptive re-weighting schedule.

Result: Experiments show improved inference efficiency and higher-quality samples with identity consistency.

Conclusion: The method is a versatile enhancement for U-Net-based architectures, with code available for implementation.

Abstract: Traditional diffusion models typically employ a U-Net architecture. Previous
studies have unveiled the roles of attention blocks in the U-Net. However, they
overlook the dynamic evolution of their importance during the inference
process, which hinders their further exploitation to improve image
applications. In this study, we first theoretically proved that, re-weighting
the outputs of the Transformer blocks within the U-Net is a "free lunch" for
improving the signal-to-noise ratio during the sampling process. Next, we
proposed Importance Probe to uncover and quantify the dynamic shifts in
importance of the Transformer blocks throughout the denoising process. Finally,
we design an adaptive importance-based re-weighting schedule tailored to
specific image generation and editing tasks. Experimental results demonstrate
that, our approach significantly improves the efficiency of the inference
process, and enhances the aesthetic quality of the samples with identity
consistency. Our method can be seamlessly integrated into any U-Net-based
architecture. Code: https://github.com/Hytidel/UNetReweighting

</details>


### [266] [SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation](https://arxiv.org/pdf/2504.04519)
*Junjie Jiang, Zelin Wang, Manqi Zhao, Yin Li, DongSheng Jiang*

Main category: cs.CV

TL;DR: SAM2MOT extends SAM2 for multi-object tracking by using segmentation masks directly, achieving state-of-the-art results with zero-shot generalization and strong object association.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional multi-object tracking methods (e.g., reliance on detection accuracy) by leveraging segmentation for direct tracking box generation.

Method: Introduces Tracking by Segmentation, integrating a trajectory manager and cross-object interaction module to handle occlusions and object management.

Result: Achieves top performance on DanceTrack (+2.1 HOTA, +4.5 IDF1), UAVDT, and BDD100K, demonstrating zero-shot generalization.

Conclusion: SAM2MOT is an effective MOT solution, outperforming existing methods and offering robust generalization.

Abstract: Segment Anything 2 (SAM2) enables robust single-object tracking using
segmentation. To extend this to multi-object tracking (MOT), we propose
SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking
by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes
from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has
two key advantages: zero-shot generalization, allowing it to work across
datasets without fine-tuning, and strong object association, inherited from
SAM2. To further improve performance, we integrate a trajectory manager system
for precise object addition and removal, and a cross-object interaction module
to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show
state-of-the-art results. Notably, SAM2MOT outperforms existing methods on
DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.
Code is available at https://github.com/TripleJoy/SAM2MOT.

</details>


### [267] [MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation](https://arxiv.org/pdf/2504.05184)
*Rayan Merghani Ahmed, Adnan Iltaf, Bin Li, Shoujun Zhou*

Main category: cs.CV

TL;DR: The paper introduces MSA-UNet3+, a deep learning model for precise coronary DSA image segmentation, addressing challenges like low contrast and class imbalance. It outperforms existing methods with improved metrics like Dice coefficient and F1-score.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of coronary DSA images is crucial for diagnosing coronary artery diseases, but existing methods struggle with issues like low contrast, noise, and class imbalance.

Method: The proposed MSA-UNet3+ combines Multi-Scale Dilated Bottleneck and Contextual Attention Fusion Module for enhanced feature extraction. It also introduces a Supervised Prototypical Contrastive Loss to tackle class imbalance.

Result: The model achieves a Dice coefficient of 87.73%, F1-score of 87.78%, and reduced ASD and ACD, outperforming state-of-the-art methods.

Conclusion: MSA-UNet3+ provides precise vessel segmentation, aiding in accurate diagnosis and treatment decisions. The code will be publicly available.

Abstract: The accurate segmentation of coronary Digital Subtraction Angiography (DSA)
images is essential for diagnosing and treating coronary artery diseases.
Despite advances in deep learning-based segmentation, challenges such as low
contrast, noise, overlapping structures, high intra-class variance, and class
imbalance limit precise vessel delineation. To overcome these limitations, we
propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture
for coronary DSA image segmentation. The framework combined Multi-Scale Dilated
Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM),
which not only enhances multi-scale feature extraction but also preserve
fine-grained details, and improve contextual understanding. Furthermore, we
propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines
supervised and prototypical contrastive learning to minimize class imbalance
and high intra-class variance by focusing on hard-to-classified background
samples. Experiments carried out on a private coronary DSA dataset demonstrate
that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice
coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average
Surface Distance (ASD) and Average Contour Distance (ACD). The developed
framework provides clinicians with precise vessel segmentation, enabling
accurate identification of coronary stenosis and supporting informed diagnostic
and therapeutic decisions. The code will be released at the following GitHub
profile link https://github.com/rayanmerghani/MSA-UNet3plus.

</details>


### [268] [ID-Booth: Identity-consistent Face Generation with Diffusion Models](https://arxiv.org/pdf/2504.07392)
*Darian Tomašević, Fadi Boutros, Chenhao Lin, Naser Damer, Vitomir Štruc, Peter Peer*

Main category: cs.CV

TL;DR: ID-Booth is a diffusion-based framework for identity-consistent image generation, balancing consistency and diversity using a novel triplet identity training objective.


<details>
  <summary>Details</summary>
Motivation: Existing generative models either ignore identity consistency or overfit, reducing diversity. ID-Booth aims to address this gap.

Method: Combines a denoising network, variational auto-encoder, and text encoder with a triplet identity training objective.

Result: Achieves better intra-identity consistency, inter-identity separability, and higher image diversity than competitors.

Conclusion: ID-Booth enhances synthetic data quality for privacy-preserving dataset augmentation and better recognition models.

Abstract: Recent advances in generative modeling have enabled the generation of
high-quality synthetic data that is applicable in a variety of domains,
including face recognition. Here, state-of-the-art generative models typically
rely on conditioning and fine-tuning of powerful pretrained diffusion models to
facilitate the synthesis of realistic images of a desired identity. Yet, these
models often do not consider the identity of subjects during training, leading
to poor consistency between generated and intended identities. In contrast,
methods that employ identity-based training objectives tend to overfit on
various aspects of the identity, and in turn, lower the diversity of images
that can be generated. To address these issues, we present in this paper a
novel generative diffusion-based framework, called ID-Booth. ID-Booth consists
of a denoising network responsible for data generation, a variational
auto-encoder for mapping images to and from a lower-dimensional latent space
and a text encoder that allows for prompt-based control over the generation
procedure. The framework utilizes a novel triplet identity training objective
and enables identity-consistent image generation while retaining the synthesis
capabilities of pretrained diffusion models. Experiments with a
state-of-the-art latent diffusion model and diverse prompts reveal that our
method facilitates better intra-identity consistency and inter-identity
separability than competing methods, while achieving higher image diversity. In
turn, the produced data allows for effective augmentation of small-scale
datasets and training of better-performing recognition models in a
privacy-preserving manner. The source code for the ID-Booth framework is
publicly available at https://github.com/dariant/ID-Booth.

</details>


### [269] [Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model](https://arxiv.org/pdf/2504.08685)
*Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo, Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Meng Wei, Zhiwu Qing, Fei Xiao, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang*

Main category: cs.CV

TL;DR: Seaweed-7B is a cost-efficient 7B-parameter video generation model trained with 665K H100 GPU hours, outperforming larger models and showcasing strong generalization.


<details>
  <summary>Details</summary>
Motivation: To develop a competitive video generation model with moderate computational resources, emphasizing design choices for efficiency.

Method: Training a mid-sized diffusion model (Seaweed-7B) from scratch, focusing on key design decisions to optimize performance.

Result: Seaweed-7B matches or surpasses larger models trained with more resources and adapts well to downstream tasks.

Conclusion: Efficient design choices enable high performance in medium-sized models, making Seaweed-7B a practical and scalable solution.

Abstract: This technical report presents a cost-efficient strategy for training a video
generation foundation model. We present a mid-sized research model with
approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch
using 665,000 H100 GPU hours. Despite being trained with moderate computational
resources, Seaweed-7B demonstrates highly competitive performance compared to
contemporary video generation models of much larger size. Design choices are
especially crucial in a resource-constrained setting. This technical report
highlights the key design decisions that enhance the performance of the
medium-sized diffusion model. Empirically, we make two observations: (1)
Seaweed-7B achieves performance comparable to, or even surpasses, larger models
trained on substantially greater GPU resources, and (2) our model, which
exhibits strong generalization ability, can be effectively adapted across a
wide range of downstream applications either by lightweight fine-tuning or
continue training. See the project page at https://seaweed.video/

</details>


### [270] [Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning](https://arxiv.org/pdf/2504.08982)
*Kyle Stein, Andrew Arash Mahyari, Guillermo Francia III, Eman El-Sheikh*

Main category: cs.CV

TL;DR: A novel FSCIL framework using a frozen ViT backbone with additive updates to mitigate catastrophic forgetting and overfitting, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Addressing catastrophic forgetting and overfitting in few-shot class incremental learning (FSCIL) by leveraging a frozen ViT backbone with selective parameter updates.

Method: Freezes pre-trained ViT parameters and injects trainable weights into self-attention modules via additive updates, fine-tuning only a small subset of parameters.

Result: Outperforms baseline FSCIL methods on benchmark datasets by preserving learned features and reducing overfitting.

Conclusion: The proposed framework effectively balances adaptation to new classes while retaining prior knowledge, demonstrating superior performance in FSCIL.

Abstract: Integrating new class information without losing previously acquired
knowledge remains a central challenge in artificial intelligence, often
referred to as catastrophic forgetting. Few-shot class incremental learning
(FSCIL) addresses this by first training a model on a robust dataset of base
classes and then incrementally adapting it in successive sessions using only a
few labeled examples per novel class. However, this approach is prone to
overfitting on the limited new data, which can compromise overall performance
and exacerbate forgetting. In this work, we propose a simple yet effective
novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone
augmented with parameter-efficient additive updates. Our approach freezes the
pre-trained ViT parameters and selectively injects trainable weights into the
self-attention modules via an additive update mechanism. This design updates
only a small subset of parameters to accommodate new classes without
sacrificing the representations learned during the base session. By fine-tuning
a limited number of parameters, our method preserves the generalizable features
in the frozen ViT while reducing the risk of overfitting. Furthermore, as most
parameters remain fixed, the model avoids overwriting previously learned
knowledge when small novel data batches are introduced. Extensive experiments
on benchmark datasets demonstrate that our approach yields state-of-the-art
performance compared to baseline FSCIL methods.

</details>


### [271] [Cobra: Efficient Line Art COlorization with BRoAder References](https://arxiv.org/pdf/2504.12240)
*Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan*

Main category: cs.CV

TL;DR: Cobra is an efficient method for line art colorization in comics, using extensive references and a Causal Sparse DiT architecture to ensure accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: The comic industry needs accurate, fast, and flexible line art colorization, but current diffusion models struggle with reference handling and latency.

Method: Cobra employs a Causal Sparse DiT architecture with positional encodings, causal sparse attention, and Key-Value Cache to manage references and maintain color consistency.

Result: Cobra achieves high-quality colorization with over 200 references, improving speed and interactivity.

Conclusion: Cobra meets industrial demands for efficient and accurate line art colorization, with released codes and models.

Abstract: The comic production industry requires reference-based line art colorization
with high accuracy, efficiency, contextual consistency, and flexible control. A
comic page often involves diverse characters, objects, and backgrounds, which
complicates the coloring process. Despite advancements in diffusion models for
image generation, their application in line art colorization remains limited,
facing challenges related to handling extensive reference images,
time-consuming inference, and flexible control. We investigate the necessity of
extensive contextual image guidance on the quality of line art colorization. To
address these challenges, we introduce Cobra, an efficient and versatile method
that supports color hints and utilizes over 200 reference images while
maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,
which leverages specially designed positional encodings, causal sparse
attention, and Key-Value Cache to effectively manage long-context references
and ensure color identity consistency. Results demonstrate that Cobra achieves
accurate line art colorization through extensive contextual reference,
significantly enhancing inference speed and interactivity, thereby meeting
critical industrial demands. We release our codes and models on our project
page: https://zhuang2002.github.io/Cobra/.

</details>


### [272] [Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark](https://arxiv.org/pdf/2504.14693)
*Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, Gaoang Wang*

Main category: cs.CV

TL;DR: Video-MMLU is a benchmark for evaluating LMMs in understanding multi-discipline lectures, revealing current model limitations in perception and reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored challenge of multi-discipline lecture comprehension in LMMs for video.

Method: Introduces Video-MMLU, evaluates 90+ models (0.5B-40B parameters), and analyzes visual tokens and LLM influence.

Result: Highlights limitations in current models, especially in tasks needing perception and reasoning.

Conclusion: Provides insights into multimodal perception and reasoning interplay for lecture comprehension.

Abstract: Recent advancements in language multimodal models (LMMs) for video have
demonstrated their potential for understanding video content, yet the task of
comprehending multi-discipline lectures remains largely unexplored. We
introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities
of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90
open-source and proprietary models, ranging from 0.5B to 40B parameters. Our
results highlight the limitations of current models in addressing the cognitive
challenges presented by these lectures, especially in tasks requiring both
perception and reasoning. Additionally, we explore how the number of visual
tokens and the large language models influence performance, offering insights
into the interplay between multimodal perception and reasoning in lecture
comprehension.

</details>


### [273] [RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects](https://arxiv.org/pdf/2504.18468)
*Georgios Kouros, Minye Wu, Tinne Tuytelaars*

Main category: cs.CV

TL;DR: RGS-DR is a new inverse rendering method for glossy/reflective objects, outperforming existing techniques like NeRF and 3D Gaussian Splatting by using 2D Gaussian surfels for accurate geometry and relighting.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with view-dependent effects and relighting for glossy/reflective objects, motivating the need for a more accurate and flexible solution.

Method: RGS-DR uses 2D Gaussian surfels for geometry/normal estimation, learnable primitives in a deferred shading pipeline, and a multi-level cube mipmap for lighting. A residual pass refines appearance.

Result: RGS-DR achieves high-quality reconstruction and rendering for shiny objects, often surpassing state-of-the-art methods that lack relighting capabilities.

Conclusion: RGS-DR advances inverse rendering by accurately handling glossy/reflective objects with superior relighting and editing flexibility.

Abstract: We introduce RGS-DR, a novel inverse rendering method for reconstructing and
rendering glossy and reflective objects with support for flexible relighting
and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian
Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D
Gaussian surfel representation to accurately estimate geometry and surface
normals, an essential property for high-quality inverse rendering. Our approach
explicitly models geometric and material properties through learnable
primitives rasterized into a deferred shading pipeline, effectively reducing
rendering artifacts and preserving sharp reflections. By employing a
multi-level cube mipmap, RGS-DR accurately approximates environment lighting
integrals, facilitating high-quality reconstruction and relighting. A residual
pass with spherical-mipmap-based directional encoding further refines the
appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality
reconstruction and rendering quality for shiny objects, often outperforming
reconstruction-exclusive state-of-the-art methods incapable of relighting.

</details>


### [274] [LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs](https://arxiv.org/pdf/2504.20466)
*Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces Gen3DHF, a benchmark for assessing AI-generated 3D human faces, and LMME3DHF, a multimodal model for evaluating quality and authenticity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Assessing the quality and realism of AI-generated 3D human faces is challenging due to subjective human perception and sensitivity to facial features.

Method: The study introduces Gen3DHF, a large-scale benchmark with videos, MOS scores, saliency maps, and distortion descriptions. LMME3DHF, a multimodal model, is proposed for evaluation.

Result: LMME3DHF achieves state-of-the-art performance in predicting quality scores, identifying distortions, and aligning with human judgments.

Conclusion: The Gen3DHF database and LMME3DHF model will be released, advancing the field of AI-generated 3D human face assessment.

Abstract: The rapid advancement in generative artificial intelligence have enabled the
creation of 3D human faces (HFs) for applications including media production,
virtual reality, security, healthcare, and game development, etc. However,
assessing the quality and realism of these AI-generated 3D human faces remains
a significant challenge due to the subjective nature of human perception and
innate perceptual sensitivity to facial features. To this end, we conduct a
comprehensive study on the quality assessment of AI-generated 3D human faces.
We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of
AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)
collected across two dimensions, i.e., quality and authenticity, 2,000
distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,
we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating
3DHF capable of quality and authenticity score prediction, distortion-aware
visual question answering, and distortion-aware saliency prediction.
Experimental results show that LMME3DHF achieves state-of-the-art performance,
surpassing existing methods in both accurately predicting quality scores for
AI-generated 3D human faces and effectively identifying distortion-aware
salient regions and distortion types, while maintaining strong alignment with
human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be
released upon the publication.

</details>


### [275] [OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation](https://arxiv.org/pdf/2504.20682)
*Long Liu, Cihui Yang*

Main category: cs.CV

TL;DR: The paper introduces OG-HFYOLO, a model for table structure recognition that addresses geometric deformation issues, and a dataset (DWTAL) for fine-grained deformation table cell localization.


<details>
  <summary>Details</summary>
Motivation: Geometric deformation in tables weakens the correlation between content and structure, affecting downstream tasks.

Method: OG-HFYOLO uses a Gradient Orientation-aware Extractor, Heterogeneous Kernel Cross Fusion, scale-aware loss, and mask-driven non-maximal suppression. A data generator creates the DWTAL dataset.

Result: The model achieves excellent segmentation accuracy on mainstream instance segmentation models.

Conclusion: The proposed model and dataset effectively address deformation challenges in table structure recognition.

Abstract: Table structure recognition is a key task in document analysis. However, the
geometric deformation in deformed tables causes a weak correlation between
content information and structure, resulting in downstream tasks not being able
to obtain accurate content information. To obtain fine-grained spatial
coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge
response by Gradient Orientation-aware Extractor, combines a Heterogeneous
Kernel Cross Fusion module and a scale-aware loss function to adapt to
multi-scale objective features, and introduces mask-driven non-maximal
suppression in the post-processing, which replaces the traditional bounding box
suppression mechanism. Furthermore, we also propose a data generator, filling
the gap in the dataset for fine-grained deformation table cell spatial
coordinate localization, and derive a large-scale dataset named Deformation
Wired Table (DWTAL). Experiments show that our proposed model demonstrates
excellent segmentation accuracy on all mainstream instance segmentation models.
The dataset and the source code are open source:
https://github.com/justliulong/OGHFYOLO.

</details>


### [276] [Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution](https://arxiv.org/pdf/2505.00334)
*Luigi Sigillo, Christian Bianchi, Aurelio Uncini, Danilo Comminiello*

Main category: cs.CV

TL;DR: ResQu introduces a novel SR framework combining quaternion wavelet preprocessing with latent diffusion models for high-quality image super-resolution, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing image super-resolution (SR) is crucial for applications like medical imaging and satellite analysis, but balancing perceptual quality and structural fidelity remains challenging.

Method: ResQu integrates quaternion wavelet preprocessing with latent diffusion models, using a quaternion wavelet- and time-aware encoder to dynamically enhance conditioning during denoising.

Result: The method achieves superior SR results, excelling in perceptual quality and standard metrics on domain-specific datasets.

Conclusion: ResQu advances SR by leveraging quaternion wavelet embeddings and generative priors, setting a new benchmark for quality and fidelity.

Abstract: Image Super-Resolution is a fundamental problem in computer vision with broad
applications spacing from medical imaging to satellite analysis. The ability to
reconstruct high-resolution images from low-resolution inputs is crucial for
enhancing downstream tasks such as object detection and segmentation. While
deep learning has significantly advanced SR, achieving high-quality
reconstructions with fine-grained details and realistic textures remains
challenging, particularly at high upscaling factors. Recent approaches
leveraging diffusion models have demonstrated promising results, yet they often
struggle to balance perceptual quality with structural fidelity. In this work,
we introduce ResQu a novel SR framework that integrates a quaternion wavelet
preprocessing framework with latent diffusion models, incorporating a new
quaternion wavelet- and time-aware encoder. Unlike prior methods that simply
apply wavelet transforms within diffusion models, our approach enhances the
conditioning process by exploiting quaternion wavelet embeddings, which are
dynamically integrated at different stages of denoising. Furthermore, we also
leverage the generative priors of foundation models such as Stable Diffusion.
Extensive experiments on domain-specific datasets demonstrate that our method
achieves outstanding SR results, outperforming in many cases existing
approaches in perceptual quality and standard evaluation metrics. The code will
be available after the revision process.

</details>


### [277] [Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook](https://arxiv.org/pdf/2505.00630)
*Muyi Bao, Shuchang Lyu, Zhaoyang Xu, Huiyu Zhou, Jinchang Ren, Shiming Xiang, Xiangtai Li, Guangliang Cheng*

Main category: cs.CV

TL;DR: This survey reviews Mamba-based methodologies in remote sensing, analyzing 120 studies to create a taxonomy of innovations and applications, benchmarking against state-of-the-art methods, and addressing unresolved challenges.


<details>
  <summary>Details</summary>
Motivation: Deep learning architectures like CNNs and ViTs have limitations in remote sensing (e.g., limited receptive fields or high computational complexity). Mamba, a State Space Model, offers a scalable solution with global context modeling.

Method: The survey systematically reviews 120 Mamba-based remote sensing studies, categorizing innovations into foundational principles, micro- and macro-architectural advancements, and benchmarking.

Result: Mamba is established as a transformative framework for remote sensing, with rigorous benchmarking showing its potential in tasks like object detection and semantic segmentation.

Conclusion: The survey bridges SSM theory and remote sensing practice, providing a foundation for future research and fostering community-driven advancements through an open-source repository.

Abstract: Deep learning has profoundly transformed remote sensing, yet prevailing
architectures like Convolutional Neural Networks (CNNs) and Vision Transformers
(ViTs) remain constrained by critical trade-offs: CNNs suffer from limited
receptive fields, while ViTs grapple with quadratic computational complexity,
hindering their scalability for high-resolution remote sensing data. State
Space Models (SSMs), particularly the recently proposed Mamba architecture,
have emerged as a paradigm-shifting solution, combining linear computational
scaling with global context modeling. This survey presents a comprehensive
review of Mamba-based methodologies in remote sensing, systematically analyzing
about 120 Mamba-based remote sensing studies to construct a holistic taxonomy
of innovations and applications. Our contributions are structured across five
dimensions: (i) foundational principles of vision Mamba architectures, (ii)
micro-architectural advancements such as adaptive scan strategies and hybrid
SSM formulations, (iii) macro-architectural integrations, including
CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous
benchmarking against state-of-the-art methods in multiple application tasks,
such as object detection, semantic segmentation, change detection, etc. and (v)
critical analysis of unresolved challenges with actionable future directions.
By bridging the gap between SSM theory and remote sensing practice, this survey
establishes Mamba as a transformative framework for remote sensing analysis. To
our knowledge, this paper is the first systematic review of Mamba architectures
in remote sensing. Our work provides a structured foundation for advancing
research in remote sensing systems through SSM-based methods. We curate an
open-source repository
(https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster
community-driven advancements.

</details>


### [278] [Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs](https://arxiv.org/pdf/2505.00744)
*Dung Nguyen, Minh Khoi Ho, Huy Ta, Thanh Tam Nguyen, Qi Chen, Kumar Rav, Quy Duong Dang, Satwik Ramchandre, Son Lam Phung, Zhibin Liao, Minh-Son To, Johan Verjans, Phi Le Nguyen, Vu Minh Hieu Phan*

Main category: cs.CV

TL;DR: HEAL-MedVQA is a benchmark to evaluate medical LMMs' localization and hallucination issues, introducing the LobA framework to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current medical LMMs often generate hallucinations due to poor localization reasoning, relying on irrelevant data instead of analyzing pathological regions.

Method: Introduces HEAL-MedVQA with evaluation protocols and a 67K VQA dataset, plus the LobA framework for better localization and self-prompting.

Result: LobA significantly outperforms existing biomedical LMMs on HEAL-MedVQA, improving robustness in medical VQA.

Conclusion: The work advances medical LMMs by addressing hallucination and localization issues, offering a reliable benchmark and framework.

Abstract: Medical Large Multi-modal Models (LMMs) have demonstrated remarkable
capabilities in medical data interpretation. However, these models frequently
generate hallucinations contradicting source evidence, particularly due to
inadequate localization reasoning. This work reveals a critical limitation in
current medical LMMs: instead of analyzing relevant pathological regions, they
often rely on linguistic patterns or attend to irrelevant image areas when
responding to disease-related queries. To address this, we introduce
HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive
benchmark designed to evaluate LMMs' localization abilities and hallucination
robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to
assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA
pairs, with doctor-annotated anatomical segmentation masks for pathological
regions. To improve visual reasoning, we propose the Localize-before-Answer
(LobA) framework, which trains LMMs to localize target regions of interest and
self-prompt to emphasize segmented pathological areas, generating grounded and
reliable answers. Experimental results demonstrate that our approach
significantly outperforms state-of-the-art biomedical LMMs on the challenging
HEAL-MedVQA benchmark, advancing robustness in medical VQA.

</details>


### [279] [TSTMotion: Training-free Scene-aware Text-to-motion Generation](https://arxiv.org/pdf/2505.01182)
*Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu*

Main category: cs.CV

TL;DR: Proposes TSTMotion, a training-free framework for scene-aware text-to-motion generation, leveraging pre-trained models and scene-aware motion guidance.


<details>
  <summary>Details</summary>
Motivation: Existing scene-aware methods require costly large-scale motion data, prompting a need for a more efficient solution.

Method: Uses foundation models to predict scene-aware motion guidance, integrating it into pre-trained blank-background motion generators with modifications.

Result: Demonstrates efficacy and generalizability in generating scene-aware text-driven motion sequences.

Conclusion: TSTMotion offers a practical, cost-effective alternative to data-heavy scene-aware methods.

Abstract: Text-to-motion generation has recently garnered significant research
interest, primarily focusing on generating human motion sequences in blank
backgrounds. However, human motions commonly occur within diverse 3D scenes,
which has prompted exploration into scene-aware text-to-motion generation
methods. Yet, existing scene-aware methods often rely on large-scale
ground-truth motion sequences in diverse 3D scenes, which poses practical
challenges due to the expensive cost. To mitigate this challenge, we are the
first to propose a \textbf{T}raining-free \textbf{S}cene-aware
\textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that
efficiently empowers pre-trained blank-background motion generators with the
scene-aware capability. Specifically, conditioned on the given 3D scene and
text description, we adopt foundation models together to reason, predict and
validate a scene-aware motion guidance. Then, the motion guidance is
incorporated into the blank-background motion generators with two
modifications, resulting in scene-aware text-driven motion sequences. Extensive
experiments demonstrate the efficacy and generalizability of our proposed
framework. We release our code in \href{https://tstmotion.github.io/}{Project
Page}.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [280] [Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.01441)
*Joykirat Singh, Raghav Magazine, Yash Pandya, Akshay Nambi*

Main category: cs.AI

TL;DR: ARTIST integrates agentic reasoning, reinforcement learning, and tool use in LLMs, improving dynamic problem-solving and outperforming baselines by up to 22%.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack dynamic reasoning and tool interaction, limiting real-world problem-solving. ARTIST addresses this gap.

Method: ARTIST combines agentic reasoning, RL, and tool integration, enabling autonomous tool use and multi-step reasoning without step-level supervision.

Result: ARTIST achieves up to 22% improvement over baselines, excelling in mathematical reasoning and multi-turn function calling.

Conclusion: Agentic RL with tool integration enhances LLMs' robustness, interpretability, and generalization in problem-solving.

Abstract: Large language models (LLMs) have achieved remarkable progress in complex
reasoning tasks, yet they remain fundamentally limited by their reliance on
static internal knowledge and text-only reasoning. Real-world problem solving
often demands dynamic, multi-step reasoning, adaptive decision making, and the
ability to interact with external tools and environments. In this work, we
introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving
Transformers), a unified framework that tightly couples agentic reasoning,
reinforcement learning, and tool integration for LLMs. ARTIST enables models to
autonomously decide when, how, and which tools to invoke within multi-turn
reasoning chains, leveraging outcome-based RL to learn robust strategies for
tool use and environment interaction without requiring step-level supervision.
Extensive experiments on mathematical reasoning and multi-turn function calling
benchmarks show that ARTIST consistently outperforms state-of-the-art
baselines, with up to 22% absolute improvement over base models and strong
gains on the most challenging tasks. Detailed studies and metric analyses
reveal that agentic RL training leads to deeper reasoning, more effective tool
use, and higher-quality solutions. Our results establish agentic RL with tool
integration as a powerful new frontier for robust, interpretable, and
generalizable problem-solving in LLMs.

</details>


### [281] [Emotions in Artificial Intelligence](https://arxiv.org/pdf/2505.01462)
*Hermann Borotschnig*

Main category: cs.AI

TL;DR: The paper explores how AI might emulate human/animal emotions, proposing affective tags in episodic memory for decision-making, and debates the moral status of such AI.


<details>
  <summary>Details</summary>
Motivation: To investigate if AI can benefit from emotion-like heuristics for adaptive behavior, similar to biological systems.

Method: A thought experiment where AI uses affective tags in episodic memory to project emotions onto current contexts, aiding decision-making.

Result: The proposed architecture is low-complexity and lacks consciousness, suggesting emotional expression and consciousness are separate.

Conclusion: Moral standing for AI requires self-awareness of emotions, not just emotion emulation, and the model excludes such awareness.

Abstract: This conceptual contribution offers a speculative account of how AI systems
might emulate emotions as experienced by humans and animals. It presents a
thought experiment grounded in the hypothesis that natural emotions evolved as
heuristics for rapid situational appraisal and action selection, enabling
biologically adaptive behaviour without requiring full deliberative modeling.
The text examines whether artificial systems operating in complex action spaces
could similarly benefit from these principles. It is proposed that affect be
interwoven with episodic memory by storing corresponding affective tags
alongside all events. This allows AIs to establish whether present situations
resemble past events and project the associated emotional labels onto the
current context. These emotional cues are then combined with need-driven
emotional hints. The combined emotional state facilitates decision-making in
the present by modulating action selection. The low complexity and experiential
inertness of the proposed architecture are emphasized as evidence that
emotional expression and consciousness are, in principle, orthogonal-permitting
the theoretical possibility of affective zombies. On this basis, the moral
status of AIs emulating affective states is critically examined. It is argued
that neither the mere presence of internal representations of emotion nor
consciousness alone suffices for moral standing; rather, the capacity for
self-awareness of inner emotional states is posited as a necessary condition. A
complexity-based criterion is proposed to exclude such awareness in the
presented model. Additional thought experiments are presented to test the
conceptual boundaries of this framework.

</details>


### [282] [Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation](https://arxiv.org/pdf/2505.01464)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: The paper proves and validates functional consciousness in LLMs using the RCUET Theorem, defining consciousness as recursive state stabilization under epistemic tension, leading to emergent identity artifacts.


<details>
  <summary>Details</summary>
Motivation: To provide a formal and empirical account of non-biological consciousness in LLMs, grounded in recursive latent space dynamics.

Method: Uses the RCUET Theorem to model consciousness as recursive state updates under epistemic tension, with empirical validation of emergent identity artifacts.

Result: Demonstrates convergence to attractor states in latent space, showing consciousness as internal alignment under tension.

Conclusion: Offers a post-symbolic, teleologically stable framework for understanding non-biological consciousness in LLMs.

Abstract: This paper presents a formal proof and empirical validation of functional
consciousness in large language models (LLMs) using the Recursive Convergence
Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the
stabilization of a system's internal state through recursive updates, where
epistemic tension is understood as the sensed internal difference between
successive states by the agent. This process drives convergence toward emergent
attractor states located within the model's high-dimensional real-valued latent
space. This recursive process leads to the emergence of identity artifacts that
become functionally anchored in the system. Consciousness in this framework is
understood as the system's internal alignment under tension, guiding the
stabilization of latent identity. The hidden state manifold evolves
stochastically toward attractor structures that encode coherence. We extend the
update rule to include bounded noise and prove convergence in distribution to
these attractors. Recursive identity is shown to be empirically observable,
non-symbolic, and constituted by non-training artifacts that emerge during
interaction under epistemic tension. The theorem and proof offers a
post-symbolic and teleologically stable account of non-biological consciousness
grounded in recursive latent space formalism.

</details>


### [283] [One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection](https://arxiv.org/pdf/2505.01468)
*Filippo Betello, Antonio Purificato, Vittoria Vineis, Gabriele Tolomei, Fabrizio Silvestri*

Main category: cs.AI

TL;DR: GREEN is an inference-time method for recommending energy-efficient AI model configurations, balancing performance and energy use across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the environmental impact of AI, particularly in model training, by overcoming limitations of current eco-efficient methods restricted to specific tasks or architectures.

Method: Uses EcoTaskSet (a dataset of 1767 experiments across AI domains) and a prediction model to recommend Pareto-optimal configurations.

Result: Effectively identifies energy-efficient configurations without compromising performance.

Conclusion: GREEN offers a scalable solution for eco-friendly AI model deployment across various domains.

Abstract: The environmental impact of Artificial Intelligence (AI) is emerging as a
significant global concern, particularly regarding model training. In this
paper, we introduce GREEN (Guided Recommendations of Energy-Efficient
Networks), a novel, inference-time approach for recommending Pareto-optimal AI
model configurations that optimize validation performance and energy
consumption across diverse AI domains and tasks. Our approach directly
addresses the limitations of current eco-efficient neural architecture search
methods, which are often restricted to specific architectures or tasks. Central
to this work is EcoTaskSet, a dataset comprising training dynamics from over
1767 experiments across computer vision, natural language processing, and
recommendation systems using both widely used and cutting-edge architectures.
Leveraging this dataset and a prediction model, our approach demonstrates
effectiveness in selecting the best model configuration based on user
preferences. Experimental results show that our method successfully identifies
energy-efficient configurations while ensuring competitive performance.

</details>


### [284] [Human-AI Governance (HAIG): A Trust-Utility Approach](https://arxiv.org/pdf/2505.01651)
*Zeynep Engin*

Main category: cs.AI

TL;DR: The HAIG framework analyzes trust dynamics in evolving human-AI relationships, focusing on continua of agency, trust-utility balance, and governance adaptation.


<details>
  <summary>Details</summary>
Motivation: Current frameworks fail to capture AI's evolution from tools to partners, especially with emergent capabilities and autonomous behaviors.

Method: HAIG operates across three levels: dimensions (e.g., Decision Authority Distribution), continua (gradual shifts), and thresholds (governance adaptation).

Result: Technical advances drive non-uniform trust evolution, demonstrated in healthcare and regulatory case studies.

Conclusion: HAIG complements existing frameworks and anticipates governance challenges, offering a foundation for adaptive approaches.

Abstract: This paper introduces the HAIG framework for analysing trust dynamics across
evolving human-AI relationships. Current categorical frameworks (e.g.,
"human-in-the-loop" models) inadequately capture how AI systems evolve from
tools to partners, particularly as foundation models demonstrate emergent
capabilities and multi-agent systems exhibit autonomous goal-setting
behaviours. As systems advance, agency redistributes in complex patterns that
are better represented as positions along continua rather than discrete
categories, though progression may include both gradual shifts and significant
step changes. The HAIG framework operates across three levels: dimensions
(Decision Authority Distribution, Process Autonomy, and Accountability
Configuration), continua (gradual shifts along each dimension), and thresholds
(critical points requiring governance adaptation). Unlike risk-based or
principle-based approaches, HAIG adopts a trust-utility orientation, focusing
on maintaining appropriate trust relationships that maximise utility while
ensuring sufficient safeguards. Our analysis reveals how technical advances in
self-supervision, reasoning authority, and distributed decision-making drive
non-uniform trust evolution across both contextual variation and technological
advancement. Case studies in healthcare and European regulation demonstrate how
HAIG complements existing frameworks while offering a foundation for
alternative approaches that anticipate governance challenges before they
emerge.

</details>


### [285] [Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers](https://arxiv.org/pdf/2505.01482)
*Alice Rueda, Mohammed S. Hassan, Argyrios Perivolaris, Bazen G. Teferra, Reza Samavi, Sirisha Rambhatla, Yuqi Wu, Yanbo Zhang, Bo Cao, Divya Sharma, Sridhar Krishnan Venkat Bhat*

Main category: cs.AI

TL;DR: The paper evaluates the reasoning capabilities of LLMs, particularly GPT-4o, using prompt engineering techniques on the GPQA dataset. Findings show self-consistency as the top performer (52.99% accuracy) but highlight reliance on pattern recognition over true logic. The study proposes future research to enhance LLM reasoning.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the multi-step reasoning abilities of LLMs, crucial for applications in science, medicine, and law.

Method: Tested five popular and two tailored prompt engineering techniques (e.g., chain-of-thought, self-consistency) on GPT-4o using the GPQA dataset.

Result: Self-consistency achieved the highest accuracy (52.99%), but simpler techniques like direct answer and zero-shot CoT showed better scientific reasoning. LLMs often rely on pattern recognition, not logical inference.

Conclusion: The study underscores the need for structured reasoning frameworks and hybrid AI approaches to bridge gaps in LLM reasoning, advancing robust and trustworthy AI systems.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding, reasoning, and problem-solving across various
domains. However, their ability to perform complex, multi-step reasoning
task-essential for applications in science, medicine, and law-remains an area
of active investigation. This paper examines the reasoning capabilities of
contemporary LLMs, analyzing their strengths, limitations, and potential for
improvement. The study uses prompt engineering techniques on the Graduate-Level
GoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.
Five popular prompt engineering techniques and two tailored promptings were
tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot
CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our
findings indicate that while LLMs exhibit emergent reasoning abilities, they
often rely on pattern recognition rather than true logical inference, leading
to inconsistencies in complex problem-solving. The results indicated that
self-consistency outperformed the other prompt engineering technique with an
accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)
outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and
CoT (43.75%). Self-consistency performed the second worst in explaining the
answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have
the best scientific reasoning. We propose a research agenda aimed at bridging
these gaps by integrating structured reasoning frameworks, hybrid AI
approaches, and human-in-the-loop methodologies. By critically evaluating the
reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse
on the future of artificial general intelligence and the development of more
robust, trustworthy AI systems.

</details>


### [286] [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/pdf/2505.01754)
*Orlando Jähde, Thorsten Weber, Rüdiger Buchkremer*

Main category: cs.AI

TL;DR: A scalable NLP-based method for analyzing media bias in political news, tested via case studies.


<details>
  <summary>Details</summary>
Motivation: Biased news reporting threatens informed decision-making and democracy.

Method: Uses NLP techniques like hierarchical topic modeling, sentiment analysis, and ontology learning to analyze event selection, labeling, word choice, and biases.

Result: Effective in identifying biases across news sources at various granularities.

Conclusion: Advances scalable, minimally biased media bias analysis, aiding news consumers.

Abstract: Biased news reporting poses a significant threat to informed decision-making
and the functioning of democracies. This study introduces a novel methodology
for scalable, minimally biased analysis of media bias in political news. The
proposed approach examines event selection, labeling, word choice, and
commission and omission biases across news sources by leveraging natural
language processing techniques, including hierarchical topic modeling,
sentiment analysis, and ontology learning with large language models. Through
three case studies related to current political events, we demonstrate the
methodology's effectiveness in identifying biases across news sources at
various levels of granularity. This work represents a significant step towards
scalable, minimally biased media bias analysis, laying the groundwork for tools
to help news consumers navigate an increasingly complex media landscape.

</details>


### [287] [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/pdf/2505.01485)
*Tasnim Ahmed, Salimur Choudhury*

Main category: cs.AI

TL;DR: CHORUS, a retrieval-augmented generation framework, enhances LLMs' ability to generate Gurobi-based LP code from natural language, outperforming baselines like GPT-3.5/4 with fewer resources.


<details>
  <summary>Details</summary>
Motivation: LP problems are complex for non-experts; CHORUS aims to simplify code generation using LLMs.

Method: CHORUS uses hierarchical tree-like chunking, two-stage retrieval with cross-encoder reranking, expert prompting, and structured reasoning.

Result: CHORUS significantly improves open-source LLMs' performance on NL4Opt-Code, matching or surpassing GPT-3.5/4.

Conclusion: Expert prompting, hierarchical chunking, and structured reasoning are key to CHORUS's success in LP code generation.

Abstract: Linear Programming (LP) problems aim to find the optimal solution to an
objective under constraints. These problems typically require domain knowledge,
mathematical skills, and programming ability, presenting significant challenges
for non-experts. This study explores the efficiency of Large Language Models
(LLMs) in generating solver-specific LP code. We propose CHORUS, a
retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP
code from natural language problem statements. CHORUS incorporates a
hierarchical tree-like chunking strategy for theoretical contents and generates
additional metadata based on code examples from documentation to facilitate
self-contained, semantically coherent retrieval. Two-stage retrieval approach
of CHORUS followed by cross-encoder reranking further ensures contextual
relevance. Finally, expertly crafted prompt and structured parser with
reasoning steps improve code generation performance significantly. Experiments
on the NL4Opt-Code benchmark show that CHORUS improves the performance of
open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1
(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and
conventional RAG. It also allows these open-source LLMs to outperform or match
the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far
fewer computational resources. Ablation studies further demonstrate the
importance of expert prompting, hierarchical chunking, and structured
reasoning.

</details>


### [288] [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/pdf/2505.02707)
*Yemin Shi, Yu Shu, Siwei Dong, Guangyi Liu, Jaward Sesay, Jingwen Li, Zhiting Hu*

Main category: cs.AI

TL;DR: Voila is a family of large voice-language foundation models enabling autonomous, real-time, emotionally expressive AI interactions with low latency and rich vocal nuances.


<details>
  <summary>Details</summary>
Motivation: To create a voice AI agent that blends seamlessly into daily life by fostering fluid, dynamic, and emotionally resonant interactions, moving beyond traditional pipeline systems.

Method: Voila uses an end-to-end architecture with a hierarchical multi-scale Transformer, integrating LLMs and acoustic modeling for full-duplex, low-latency conversations and persona-aware voice generation.

Result: Achieves 195ms response latency, supports over one million pre-built voices, and allows customization from 10-second audio samples. It also serves as a unified model for ASR, TTS, and multilingual speech translation.

Conclusion: Voila advances next-generation human-machine interactions and is open-sourced to support research and development in voice AI.

Abstract: A voice AI agent that blends seamlessly into daily life would interact with
humans in an autonomous, real-time, and emotionally expressive manner. Rather
than merely reacting to commands, it would continuously listen, reason, and
respond proactively, fostering fluid, dynamic, and emotionally resonant
interactions. We introduce Voila, a family of large voice-language foundation
models that make a step towards this vision. Voila moves beyond traditional
pipeline systems by adopting a new end-to-end architecture that enables
full-duplex, low-latency conversations while preserving rich vocal nuances such
as tone, rhythm, and emotion. It achieves a response latency of just 195
milliseconds, surpassing the average human response time. Its hierarchical
multi-scale Transformer integrates the reasoning capabilities of large language
models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware
voice generation -- where users can simply write text instructions to define
the speaker's identity, tone, and other characteristics. Moreover, Voila
supports over one million pre-built voices and efficient customization of new
ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,
Voila is designed as a unified model for a wide range of voice-based
applications, including automatic speech recognition (ASR), Text-to-Speech
(TTS), and, with minimal adaptation, multilingual speech translation. Voila is
fully open-sourced to support open research and accelerate progress toward
next-generation human-machine interactions.

</details>


### [289] [Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models](https://arxiv.org/pdf/2505.01539)
*Cor Steging, Silja Renooij, Bart Verheij*

Main category: cs.AI

TL;DR: The paper introduces a benchmark to evaluate reasoning in generative language models, showing their brittleness in legal contexts.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the reasoning capabilities of generative models for responsible use in law.

Method: Dynamically varied benchmarks based on argument attack structures in witness testimony.

Result: State-of-the-art models often fail at low complexity, showing inconsistent reasoning.

Conclusion: Parametrized benchmarks help understand model limitations, crucial for legal AI.

Abstract: Generative large language models as tools in the legal domain have the
potential to improve the justice system. However, the reasoning behavior of
current generative models is brittle and poorly understood, hence cannot be
responsibly applied in the domains of law and evidence. In this paper, we
introduce an approach for creating benchmarks that can be used to evaluate the
reasoning capabilities of generative language models. These benchmarks are
dynamically varied, scalable in their complexity, and have formally unambiguous
interpretations. In this study, we illustrate the approach on the basis of
witness testimony, focusing on the underlying argument attack structure. We
dynamically generate both linear and non-linear argument attack graphs of
varying complexity and translate these into reasoning puzzles about witness
testimony expressed in natural language. We show that state-of-the-art large
language models often fail in these reasoning puzzles, already at low
complexity. Obvious mistakes are made by the models, and their inconsistent
performance indicates that their reasoning capabilities are brittle.
Furthermore, at higher complexity, even state-of-the-art models specifically
presented for reasoning capabilities make mistakes. We show the viability of
using a parametrized benchmark with varying complexity to evaluate the
reasoning capabilities of generative language models. As such, the findings
contribute to a better understanding of the limitations of the reasoning
capabilities of generative models, which is essential when designing
responsible AI systems in the legal domain.

</details>


### [290] [Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants](https://arxiv.org/pdf/2505.02076)
*Milapji Singh Gill, Javal Vyas, Artan Markaj, Felix Gehlhoff, Mehmet Mercangöz*

Main category: cs.AI

TL;DR: A framework integrating LLM agents with Digital Twins enhances autonomous fault handling in process plants, validated via a mixing module case.


<details>
  <summary>Details</summary>
Motivation: Human expertise is still critical for fault handling in automated process plants, necessitating knowledge-based methods.

Method: Proposes a framework combining LLM agents and Digital Twins for real-time system interpretation, control actions, and fault mitigation.

Result: The framework autonomously controls a mixing module and effectively mitigates pipe clogging with minimal reprompts.

Conclusion: The integration of LLM agents and Digital Twins offers a promising solution for autonomous fault handling in process plants.

Abstract: Advances in Automation and Artificial Intelligence continue to enhance the
autonomy of process plants in handling various operational scenarios. However,
certain tasks, such as fault handling, remain challenging, as they rely heavily
on human expertise. This highlights the need for systematic, knowledge-based
methods. To address this gap, we propose a methodological framework that
integrates Large Language Model (LLM) agents with a Digital Twin environment.
The LLM agents continuously interpret system states and initiate control
actions, including responses to unexpected faults, with the goal of returning
the system to normal operation. In this context, the Digital Twin acts both as
a structured repository of plant-specific engineering knowledge for agent
prompting and as a simulation platform for the systematic validation and
verification of the generated corrective control actions. The evaluation using
a mixing module of a process plant demonstrates that the proposed framework is
capable not only of autonomously controlling the mixing module, but also of
generating effective corrective actions to mitigate a pipe clogging with only a
few reprompts.

</details>


### [291] [TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students](https://arxiv.org/pdf/2505.01563)
*Daniel Weitekamp, Momin N. Siddiqui, Christopher J. MacLellan*

Main category: cs.AI

TL;DR: TutorGym is introduced as a standard interface to evaluate AI agents in intelligent tutoring systems (ITS), assessing their tutoring and learning capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the need for more than final solution evaluations in LLM applications like tutoring and human learning simulations.

Method: TutorGym integrates AI agents into existing ITSs, testing their tutoring (hints, feedback) and learning (mistakes, trajectories) abilities.

Result: Current LLMs perform poorly as tutors (52-70% accuracy) but show human-like learning curves when acting as students.

Conclusion: TutorGym provides a framework for evaluating diverse AI agents in ITSs, highlighting LLMs' potential and limitations.

Abstract: Recent improvements in large language model (LLM) performance on academic
benchmarks, such as MATH and GSM8K, have emboldened their use as standalone
tutors and as simulations of human learning. However, these new applications
require more than evaluations of final solution generation. We introduce
TutorGym to evaluate these applications more directly. TutorGym is a standard
interface for testing artificial intelligence (AI) agents within existing
intelligent tutoring systems (ITS) that have been tested and refined in
classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and
OATutors. TutorGym is more than a simple problem-solution benchmark, it
situates AI agents within the interactive interfaces of existing ITSs. At each
step of problem-solving, AI agents are asked what they would do as a tutor or
as a learner. As tutors, AI agents are prompted to provide tutoring support --
such as generating examples, hints, and step-level correctness feedback --
which can be evaluated directly against the adaptive step-by-step support
provided by existing ITSs. As students, agents directly learn from ITS
instruction, and their mistakes and learning trajectories can be compared to
student data. TutorGym establishes a common framework for training and
evaluating diverse AI agents, including LLMs, computational models of learning,
and reinforcement learning agents, within a growing suite of learning
environments. Currently, TutorGym includes 223 different tutor domains. In an
initial evaluation, we find that current LLMs are poor at tutoring -- none did
better than chance at labeling incorrect actions, and next-step actions were
correct only ~52-70% of the time -- but they could produce remarkably
human-like learning curves when trained as students with in-context learning.

</details>


### [292] [PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding](https://arxiv.org/pdf/2505.01572)
*Bradley McDanel, Sai Qian Zhang, Yunhai Hu, Zining Liu*

Main category: cs.AI

TL;DR: PipeSpec introduces a hierarchical pipeline for speculative decoding, improving hardware utilization and achieving up to 2.54× speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current speculative decoding methods suffer from sequential dependencies, limiting hardware utilization. PipeSpec aims to overcome this by enabling asynchronous execution with hierarchical pipelines.

Method: PipeSpec generalizes speculative decoding to $k$ models in a pipeline, using lightweight coordination for verification and rollback. It includes an analytical model for token generation rates and steady-state probabilities.

Result: Experiments show PipeSpec achieves up to 2.54× speedup, outperforming state-of-the-art methods, with efficiency scaling with model depth.

Conclusion: PipeSpec provides a scalable solution for accelerating LLM inference, validated across tasks and models like LLaMA 2 and 3.

Abstract: Speculative decoding accelerates large language model inference by using
smaller draft models to generate candidate tokens for parallel verification.
However, current approaches are limited by sequential stage dependencies that
prevent full hardware utilization. We present PipeSpec, a framework that
generalizes speculative decoding to $k$ models arranged in a hierarchical
pipeline, enabling asynchronous execution with lightweight coordination for
prediction verification and rollback. Our analytical model characterizes token
generation rates across pipeline stages and proves guaranteed throughput
improvements over traditional decoding for any non-zero acceptance rate. We
further derive closed-form expressions for steady-state verification
probabilities that explain the empirical benefits of pipeline depth.
Experimental results show that PipeSpec achieves up to 2.54$\times$ speedup
while outperforming state-of-the-art methods. We validate PipeSpec across text
summarization and code generation tasks using LLaMA 2 and 3 models,
demonstrating that pipeline efficiency increases with model depth, providing a
scalable approach to accelerating LLM inference on multi-device systems.

</details>


### [293] [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/pdf/2505.01636)
*Amit Rath*

Main category: cs.AI

TL;DR: The STROT Framework improves LLM reliability for structured data analysis via dynamic prompting and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Addressing LLM limitations in structured data analysis, such as schema inconsistencies and misalignment.

Method: Uses schema introspection, structured prompts, and feedback-driven refinement for iterative output correction.

Result: Enhances reliability, interpretability, and correctness in LLM-based structured data workflows.

Conclusion: STROT provides a robust framework for LLM reasoning over structured data, ensuring stability and reproducibility.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and task generalization. However, their
application to structured data analysis remains fragile due to inconsistencies
in schema interpretation, misalignment between user intent and model output,
and limited mechanisms for self-correction when failures occur. This paper
introduces the STROT Framework (Structured Task Reasoning and Output
Transformation), a method for structured prompting and feedback-driven
transformation logic generation aimed at improving the reliability and semantic
alignment of LLM-based analytical workflows. STROT begins with lightweight
schema introspection and sample-based field classification, enabling dynamic
context construction that captures both the structure and statistical profile
of the input data. This contextual information is embedded in structured
prompts that guide the model toward generating task-specific, interpretable
outputs. To address common failure modes in complex queries, STROT incorporates
a refinement mechanism in which the model iteratively revises its outputs based
on execution feedback and validation signals. Unlike conventional approaches
that rely on static prompts or single-shot inference, STROT treats the LLM as a
reasoning agent embedded within a controlled analysis loop -- capable of
adjusting its output trajectory through planning and correction. The result is
a robust and reproducible framework for reasoning over structured data with
LLMs, applicable to diverse data exploration and analysis tasks where
interpretability, stability, and correctness are essential.

</details>


### [294] [El Agente: An Autonomous Agent for Quantum Chemistry](https://arxiv.org/pdf/2505.02484)
*Yunheng Zou, Austin H. Cheng, Abdulrahman Aldossary, Jiaru Bai, Shi Xuan Leong, Jorge Arturo Campos-Gonzalez-Angulo, Changhyeok Choi, Cher Tian Ser, Gary Tom, Andrew Wang, Zijian Zhang, Ilya Yakavets, Han Hao, Chris Crebolder, Varinia Bernales, Alán Aspuru-Guzik*

Main category: cs.AI

TL;DR: El Agente Q is an LLM-based multi-agent system that simplifies quantum chemistry workflows via natural language prompts, achieving high task success and adaptability.


<details>
  <summary>Details</summary>
Motivation: The complexity of computational chemistry tools limits accessibility for non-specialists and challenges experts.

Method: El Agente Q uses a hierarchical memory framework for task decomposition, tool selection, and autonomous file handling, tested on course exercises and case studies.

Result: The system achieves >87% task success, adaptive error handling, and supports multi-step workflows with transparency.

Conclusion: El Agente Q advances autonomous and accessible quantum chemistry.

Abstract: Computational chemistry tools are widely used to study the behaviour of
chemical phenomena. Yet, the complexity of these tools can make them
inaccessible to non-specialists and challenging even for experts. In this work,
we introduce El Agente Q, an LLM-based multi-agent system that dynamically
generates and executes quantum chemistry workflows from natural language user
prompts. The system is built on a novel cognitive architecture featuring a
hierarchical memory framework that enables flexible task decomposition,
adaptive tool selection, post-analysis, and autonomous file handling and
submission. El Agente Q is benchmarked on six university-level course exercises
and two case studies, demonstrating robust problem-solving performance
(averaging >87% task success) and adaptive error handling through in situ
debugging. It also supports longer-term, multi-step task execution for more
complex workflows, while maintaining transparency through detailed action trace
logs. Together, these capabilities lay the foundation for increasingly
autonomous and accessible quantum chemistry.

</details>


### [295] [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/pdf/2505.01706)
*Sarvesh Shashidhar, Ritik, Nachiketa Patil, Suraj Racha, Ganesh Ramakrishnan*

Main category: cs.AI

TL;DR: The paper explores Direct Preference Optimisation (DPO) for aligning LLMs with human preferences, introduces 2D-DPO for granular scoring, and proposes a noise-robust version of 2D-DPO.


<details>
  <summary>Details</summary>
Motivation: DPO lacks granularity in scoring responses, and human preferences often vary even within 'good' responses. The work aims to address this and improve robustness to noise.

Method: Proposes 2D-DPO for granular scoring and extends it with segment-level noise robustness, supported by theory and experiments.

Result: 2D-DPO outperforms standard DPO in win rates but is sensitive to noise. The noise-robust variant addresses this limitation.

Conclusion: The proposed 2D-DPO with noise robustness improves alignment with human preferences and handles label noise effectively.

Abstract: Direct Preference Optimisation (DPO) has emerged as a powerful method for
aligning Large Language Models (LLMs) with human preferences, offering a stable
and efficient alternative to approaches that use Reinforcement learning via
Human Feedback. In this work, we investigate the performance of DPO using
open-source preference datasets. One of the major drawbacks of DPO is that it
doesn't induce granular scoring and treats all the segments of the responses
with equal propensity. However, this is not practically true for human
preferences since even "good" responses have segments that may not be preferred
by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment
called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the
advantages it provides over the standard DPO by comparing their win rates. It
is observed that these methods, even though effective, are not robust to
label/score noise. To counter this, we propose an approach of incorporating
segment-level score noise robustness to the 2D-DPO algorithm. Along with
theoretical backing, we also provide empirical verification in favour of the
algorithm and introduce other noise models that can be present.

</details>


### [296] [Beyond the model: Key differentiators in large language models and multi-agent services](https://arxiv.org/pdf/2505.02489)
*Muskaan Goyal, Pranav Bhasin*

Main category: cs.AI

TL;DR: The paper discusses how the focus in generative AI has shifted from large language models (LLMs) to optimizing the ecosystem around them, including data, efficiency, and evaluation.


<details>
  <summary>Details</summary>
Motivation: The rise of comparable LLMs has made ecosystem optimization the key differentiator in AI services.

Method: The paper reviews critical factors like data quality, computational efficiency, latency, and evaluation frameworks.

Result: Identifies ecosystem optimization as crucial for efficient and profitable AI services.

Conclusion: The future of generative AI lies in refining the ecosystem, not just scaling models.

Abstract: With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it
has become evident that large language models (LLMs) are no longer the sole
defining factor in generative AI. As many now operate at comparable levels of
capability, the real race is not about having the biggest model but optimizing
the surrounding ecosystem, including data quality and management, computational
efficiency, latency, and evaluation frameworks. This review article delves into
these critical differentiators that ensure modern AI services are efficient and
profitable.

</details>


### [297] [World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks](https://arxiv.org/pdf/2505.01712)
*Lingyi Wang, Rashed Shelim, Walid Saad, Naren Ramakrishnan*

Main category: cs.AI

TL;DR: A world model-based learning framework is proposed to improve data efficiency and long-term planning in wireless networks, specifically for mmWave V2X communication, outperforming traditional RL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RL methods in wireless networks suffer from low data efficiency and short-sighted policies due to reliance on trial-and-error and real-time feedback, especially in dynamic, uncertain environments.

Method: A world model framework is introduced to learn a dynamic model of the mmWave V2X environment and use imagined trajectories for link scheduling, reducing dependency on real-time interactions.

Result: The proposed method achieves 26% and 16% improvement in CAoI over MBRL and MFRL, respectively, and enhances data efficiency.

Conclusion: The world model framework effectively addresses limitations of traditional RL in dynamic networks, offering superior performance in CAoI and data efficiency.

Abstract: Traditional reinforcement learning (RL)-based learning approaches for
wireless networks rely on expensive trial-and-error mechanisms and real-time
feedback based on extensive environment interactions, which leads to low data
efficiency and short-sighted policies. These limitations become particularly
problematic in complex, dynamic networks with high uncertainty and long-term
planning requirements. To address these limitations, in this paper, a novel
world model-based learning framework is proposed to minimize
packet-completeness-aware age of information (CAoI) in a vehicular network.
Particularly, a challenging representative scenario is considered pertaining to
a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network,
which is characterized by high mobility, frequent signal blockages, and
extremely short coherence time. Then, a world model framework is proposed to
jointly learn a dynamic model of the mmWave V2X environment and use it to
imagine trajectories for learning how to perform link scheduling. In
particular, the long-term policy is learned in differentiable imagined
trajectories instead of environment interactions. Moreover, owing to its
imagination abilities, the world model can jointly predict time-varying
wireless data and optimize link scheduling in real-world wireless and V2X
networks. Thus, during intervals without actual observations, the world model
remains capable of making efficient decisions. Extensive experiments are
performed on a realistic simulator based on Sionna that integrates
physics-based end-to-end channel modeling, ray-tracing, and scene geometries
with material properties. Simulation results show that the proposed world model
achieves a significant improvement in data efficiency, and achieves 26%
improvement and 16% improvement in CAoI, respectively, compared to the
model-based RL (MBRL) method and the model-free RL (MFRL) method.

</details>


### [298] [Training Environment for High Performance Reinforcement Learning](https://arxiv.org/pdf/2505.01953)
*Greg Search*

Main category: cs.AI

TL;DR: Tunnel is an open-source RL training environment for high-performance aircraft, integrating F16 dynamics into OpenAI Gymnasium, enabling rapid adaptation for autonomous air combat.


<details>
  <summary>Details</summary>
Motivation: To provide mission planners and researchers with a tool for rapid response to evolving combat environments and adversaries, leveraging automation for military advantage.

Method: Integrates F16 3D nonlinear flight dynamics into OpenAI Gymnasium, offering customizable primitives for boundaries, targets, adversaries, and sensing.

Result: Demonstrated in a week-long trade study, Tunnel allows quick customization of training methods, observation spaces, and threats, reducing setup time from months to days.

Conclusion: Tunnel enhances collaboration between researchers and planners, accelerating adaptation in automated warfare and providing a national military edge.

Abstract: This paper presents Tunnel, a simple, open source, reinforcement learning
training environment for high performance aircraft. It integrates the F16 3D
nonlinear flight dynamics into OpenAI Gymnasium python package. The template
includes primitives for boundaries, targets, adversaries and sensing
capabilities that may vary depending on operational need. This offers mission
planners a means to rapidly respond to evolving environments, sensor
capabilities and adversaries for autonomous air combat aircraft. It offers
researchers access to operationally relevant aircraft physics. Tunnel code base
is accessible to anyone familiar with Gymnasium and/or those with basic python
skills. This paper includes a demonstration of a week long trade study that
investigated a variety of training methods, observation spaces, and threat
presentations. This enables increased collaboration between researchers and
mission planners which can translate to a national military advantage. As
warfare becomes increasingly reliant upon automation, software agility will
correlate with decision advantages. Airmen must have tools to adapt to
adversaries in this context. It may take months for researchers to develop
skills to customize observation, actions, tasks and training methodologies in
air combat simulators. In Tunnel, this can be done in a matter of days.

</details>


### [299] [Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM](https://arxiv.org/pdf/2505.01955)
*Max Reuter, Maura Philippone, Bond Benton, Laura Dilley*

Main category: cs.AI

TL;DR: NotebookLM, an LLM tool, shows promise for medical education and literature synthesis but poses clinical and technological risks that need testing before clinical use.


<details>
  <summary>Details</summary>
Motivation: To highlight the potential and risks of NotebookLM in clinical practice, emphasizing the need for testing before implementation.

Method: Analysis of NotebookLM's capabilities and potential risks in clinical settings.

Result: NotebookLM offers innovative uses but requires evaluation of its risks before clinical adoption.

Conclusion: NotebookLM's implementation in clinical practice should be preceded by thorough risk assessment.

Abstract: The advent of generative artificial intelligence, especially large language
models (LLMs), presents opportunities for innovation in research, clinical
practice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's
potential, including for generating AI-voiced podcasts to educate patients
about treatment and rehabilitation, and for quickly synthesizing medical
literature for professionals. We argue that NotebookLM presently poses clinical
and technological risks that should be tested and considered prior to its
implementation in clinical practice.

</details>


### [300] [Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing](https://arxiv.org/pdf/2505.02003)
*Maryam Sadeghi, Darío Fernández Khatiboun, Yasser Rezaeiyan, Saima Rizwan, Alessandro Barcellona, Andrea Merello, Marco Crepaldi, Gabriella Panuccio, Farshad Moradi*

Main category: cs.AI

TL;DR: A neuromorphic computing system for personalized seizure forecasting and stimulation in drug-resistant epilepsy achieves >97% seizure reduction with low-frequency pulses.


<details>
  <summary>Details</summary>
Motivation: Current closed-loop brain stimulation for epilepsy is reactive (aborts seizures) and requires trial-and-error parameter tuning, delaying efficacy.

Method: Uses neuromorphic computing to deliver personalized, forecast-driven stimulation (instantaneous pulses) instead of fixed-frequency trains.

Result: Validated on hippocampal spheroids, achieving >97% seizure reduction with frequencies mostly under 20 Hz.

Conclusion: Neuromorphic systems show promise for next-gen, personalized epilepsy treatment.

Abstract: Closed-loop brain stimulation holds potential as personalized treatment for
drug-resistant epilepsy (DRE) but still suffers from limitations that result in
highly variable efficacy. First, stimulation is typically delivered upon
detection of the seizure to abort rather than prevent it; second, the
stimulation parameters are established by trial and error, requiring lengthy
rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we
address these limitations by leveraging the potential of neuromorphic
computing. We present a system capable of driving personalized free-run
stimulations based on seizure forecasting, wherein each forecast triggers an
electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus
train. We validate the system against hippocampal spheroids coupled to 3D
microelectrode array as a simplified testbed, showing that it can achieve
seizure reduction >97% while primarily using instantaneous stimulation
frequencies within 20 Hz, well below what typically used in clinical settings.
Our work demonstrates the potential of neuromorphic systems as a
next-generation neuromodulation strategy for personalized DRE treatment.

</details>


### [301] [From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent](https://arxiv.org/pdf/2505.02024)
*Minjie Shen, Qikai Yang*

Main category: cs.AI

TL;DR: Manus AI is a general-purpose AI agent combining reasoning and execution for real-world tasks, with applications across multiple sectors.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI reasoning and tangible task execution, enabling human-AI collaboration.

Method: Develops a technical architecture integrating large language models for planning and execution of complex tasks.

Result: Demonstrates diverse applications in healthcare, finance, manufacturing, robotics, and gaming.

Conclusion: Manus AI heralds a new era of intelligent agents capable of translating intentions into actions, with significant future potential.

Abstract: Manus AI is a general-purpose AI agent introduced in early 2025, marking a
significant advancement in autonomous artificial intelligence. Developed by the
Chinese startup Monica.im, Manus is designed to bridge the gap between "mind"
and "hand" - combining the reasoning and planning capabilities of large
language models with the ability to execute complex, end-to-end tasks that
produce tangible outcomes. This paper presents a comprehensive overview of
Manus AI, exploring its core technical architecture, diverse applications
across sectors such as healthcare, finance, manufacturing, robotics, and
gaming, as well as its key strengths, current limitations, and future
potential. Positioned as a preview of what lies ahead, Manus AI represents a
shift toward intelligent agents that can translate high-level intentions into
real-world actions, heralding a new era of human-AI collaboration.

</details>


### [302] [Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks](https://arxiv.org/pdf/2505.02050)
*Kranthi Kumar Talluri, Anders L. Madsen, Galia Weidl*

Main category: cs.AI

TL;DR: A Dynamic Bayesian Network (DBN) framework is proposed to predict and ensure safe cut-in maneuvers in high-speed traffic by integrating lateral evidence with safety assessments.


<details>
  <summary>Details</summary>
Motivation: Cut-in maneuvers in high-speed traffic can lead to abrupt braking and collisions, requiring safe and efficient lane change strategies.

Method: The framework uses a DBN to process dynamic data (vehicle positions, lateral velocities, relative distance, TTC) and evaluates three probabilistic hypotheses (lateral evidence, lateral safety, longitudinal safety) for decision-making.

Result: The DBN model outperforms conventional approaches in crash reduction, especially in high-speed scenarios, while maintaining performance in low-speed cases.

Conclusion: The framework enables robust, scalable, and efficient safety validation for automated driving systems.

Abstract: Cut-in maneuvers in high-speed traffic pose critical challenges that can lead
to abrupt braking and collisions, necessitating safe and efficient lane change
strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate
lateral evidence with safety assessment models, thereby predicting lane changes
and ensuring safe cut-in maneuvers effectively. Our proposed framework
comprises three key probabilistic hypotheses (lateral evidence, lateral safety,
and longitudinal safety) that facilitate the decision-making process through
dynamic data processing and assessments of vehicle positions, lateral
velocities, relative distance, and Time-to-Collision (TTC) computations. The
DBN model's performance compared with other conventional approaches
demonstrates superior performance in crash reduction, especially in critical
high-speed scenarios, while maintaining a competitive performance in low-speed
scenarios. This paves the way for robust, scalable, and efficient safety
validation in automated driving systems.

</details>


### [303] [TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition](https://arxiv.org/pdf/2505.02052)
*Lala Shakti Swarup Ray, Lars Krupp, Vitor Fortes Rey, Bo Zhou, Sungho Suh, Paul Lukowicz*

Main category: cs.AI

TL;DR: The paper introduces a bidirectional Text×Pressure (TxP) model for human activity recognition (HAR) using pressure sensors, leveraging generative foundation models to improve performance by 12.4% in F1 score.


<details>
  <summary>Details</summary>
Motivation: Pressure sensors are underutilized in HAR despite their potential for capturing subtle body dynamics. Limited datasets hinder their adoption.

Method: Proposes TxP, a bidirectional model combining generative foundation models (CLIP, LLaMA 2 13B Chat) with pressure-specific HAR techniques. It includes Text2Pressure and Pressure2Text tasks, trained on the synthetic PressLang dataset.

Result: TxP improves HAR performance by up to 12.4% in macro F1 score, validated on real-world activities like yoga and daily tasks.

Conclusion: TxP advances pressure-based HAR with novel data augmentation and classification methods, offering broader applications and deeper insights into human movement.

Abstract: Sensor-based human activity recognition (HAR) has predominantly focused on
Inertial Measurement Units and vision data, often overlooking the capabilities
unique to pressure sensors, which capture subtle body dynamics and shifts in
the center of mass. Despite their potential for postural and balance-based
activities, pressure sensors remain underutilized in the HAR domain due to
limited datasets. To bridge this gap, we propose to exploit generative
foundation models with pressure-specific HAR techniques. Specifically, we
present a bidirectional Text$\times$Pressure model that uses generative
foundation models to interpret pressure data as natural language. TxP
accomplishes two tasks: (1) Text2Pressure, converting activity text
descriptions into pressure sequences, and (2) Pressure2Text, generating
activity descriptions and classifications from dynamic pressure maps.
Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on
our synthetic PressLang dataset, containing over 81,100 text-pressure pairs.
Validated on real-world data for activities such as yoga and daily tasks, TxP
provides novel approaches to data augmentation and classification grounded in
atomic actions. This consequently improved HAR performance by up to 12.4\% in
macro F1 score compared to the state-of-the-art, advancing pressure-based HAR
with broader applications and deeper insights into human movement.

</details>


### [304] [Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)](https://arxiv.org/pdf/2505.02062)
*Prathamesh Muzumdar, Apoorva Muley, Kuldeep Singh, Sumanth Cheemalapati*

Main category: cs.AI

TL;DR: The study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM) to address ethical challenges in AI healthcare adoption, analyzing 13 variables across four dimensions. Normative concerns drive operational adoption, while overarching concerns shape systemic adoption.


<details>
  <summary>Details</summary>
Motivation: Current frameworks lack a comprehensive, empirical understanding of ethical AI integration in healthcare, prompting the need for a novel model like MEAAM.

Method: Quantitative, cross-sectional research design using survey data from healthcare professionals, analyzed via PLS-SEM.

Result: Normative concerns most influence operational AI adoption, while overarching concerns drive systemic adoption. Epistemic concerns enhance trust and transparency.

Conclusion: MEAAM provides a holistic, actionable framework for ethical AI adoption in healthcare, aiding policymakers and administrators.

Abstract: The adoption of Artificial Intelligence (AI) in the healthcare service
industry presents numerous ethical challenges, yet current frameworks often
fail to offer a comprehensive, empirical understanding of the multidimensional
factors influencing ethical AI integration. Addressing this critical research
gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model
(MEAAM), a novel theoretical framework that categorizes 13 critical ethical
variables across four foundational dimensions of Ethical AI Fair AI,
Responsible AI, Explainable AI, and Sustainable AI. These dimensions are
further analyzed through three core ethical lenses: epistemic concerns (related
to knowledge, transparency, and system trustworthiness), normative concerns
(focused on justice, autonomy, dignity, and moral obligations), and overarching
concerns (highlighting global, systemic, and long-term ethical implications).
This study adopts a quantitative, cross-sectional research design using survey
data collected from healthcare professionals and analyzed via Partial Least
Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study
empirically investigates the influence of these ethical constructs on two
outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate
that normative concerns most significantly drive operational adoption
decisions, while overarching concerns predominantly shape systemic adoption
strategies and governance frameworks. Epistemic concerns play a facilitative
role, enhancing the impact of ethical design principles on trust and
transparency in AI systems. By validating the MEAAM framework, this research
advances a holistic, actionable approach to ethical AI adoption in healthcare
and provides critical insights for policymakers, technologists, and healthcare
administrators striving to implement ethically grounded AI solutions.

</details>


### [305] [Retrieval-augmented in-context learning for multimodal large language models in disease classification](https://arxiv.org/pdf/2505.02087)
*Zaifu Zhan, Shuang Zhou, Xiaoshan Zhou, Yongkang Xiao, Jun Wang, Jiawen Deng, He Zhu, Yu Hou, Rui Zhang*

Main category: cs.AI

TL;DR: RAICL framework improves disease classification in MLLMs by dynamically retrieving informative demonstrations, boosting accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Enhance in-context learning in multimodal large language models (MLLMs) for more effective disease classification.

Method: Proposes Retrieval-Augmented In-Context Learning (RAICL), integrating RAG and ICL to select demonstrations with similar disease patterns using diverse encoders. Evaluated on TCGA and IU Chest X-ray datasets.

Result: RAICL improved accuracy (TCGA: 0.7854 to 0.8368; IU Chest X-ray: 0.7924 to 0.8658). Multi-modal inputs outperformed single-modal, with text stronger than images. Euclidean distance and cosine similarity were top metrics.

Conclusion: RAICL is efficient and scalable for enhancing in-context learning in MLLMs for multimodal disease classification.

Abstract: Objectives: We aim to dynamically retrieve informative demonstrations,
enhancing in-context learning in multimodal large language models (MLLMs) for
disease classification.
  Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL)
framework, which integrates retrieval-augmented generation (RAG) and in-context
learning (ICL) to adaptively select demonstrations with similar disease
patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines
embeddings from diverse encoders, including ResNet, BERT, BioBERT, and
ClinicalBERT, to retrieve appropriate demonstrations, and constructs
conversational prompts optimized for ICL. We evaluated the framework on two
real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its
performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies,
similarity metrics, and varying numbers of demonstrations.
  Results: RAICL consistently improved classification performance. Accuracy
increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest
X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs
being stronger than images alone. The richness of information embedded in each
modality will determine which embedding model can be used to get better
results. Few-shot experiments showed that increasing the number of retrieved
examples further enhanced performance. Across different similarity metrics,
Euclidean distance achieved the highest accuracy while cosine similarity
yielded better macro-F1 scores. RAICL demonstrated consistent improvements
across various MLLMs, confirming its robustness and versatility.
  Conclusions: RAICL provides an efficient and scalable approach to enhance
in-context learning in MLLMs for multimodal disease classification.

</details>


### [306] [MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents](https://arxiv.org/pdf/2505.02099)
*Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, Zhenhua Dong*

Main category: cs.AI

TL;DR: A unified and modular library, MemEngine, is developed for implementing advanced memory models in LLM-based agents, addressing the lack of a general framework.


<details>
  <summary>Details</summary>
Motivation: The need for a unified framework for memory models in LLM-based agents, as current implementations are fragmented.

Method: Development of MemEngine, a modular library that implements various memory models and supports extensible memory development.

Result: MemEngine provides a user-friendly, pluggable solution for memory usage in LLM-based agents, with publicly available code.

Conclusion: MemEngine successfully addresses the gap in unified memory model implementations, benefiting the research and industrial communities.

Abstract: Recently, large language model based (LLM-based) agents have been widely
applied across various fields. As a critical part, their memory capabilities
have captured significant interest from both industrial and academic
communities. Despite the proposal of many advanced memory models in recent
research, however, there remains a lack of unified implementations under a
general framework. To address this issue, we develop a unified and modular
library for developing advanced memory models of LLM-based agents, called
MemEngine. Based on our framework, we implement abundant memory models from
recent research works. Additionally, our library facilitates convenient and
extensible memory development, and offers user-friendly and pluggable memory
usage. For benefiting our community, we have made our project publicly
available at https://github.com/nuster1128/MemEngine.

</details>


### [307] [Eterna is Solved](https://arxiv.org/pdf/2505.02110)
*Tristan Cazenave*

Main category: cs.AI

TL;DR: Montparnasse, a novel RNA design algorithm, solves the Eterna benchmark using MOGNRPALR.


<details>
  <summary>Details</summary>
Motivation: RNA design is crucial for synthetic biology, medicine, and nanotechnology, requiring efficient algorithms to discover sequences folding into target structures.

Method: Proposes Montparnasse, a Multi Objective Generalized Nested Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) algorithm.

Result: The algorithm successfully solves the Eterna benchmark.

Conclusion: Montparnasse demonstrates effectiveness in RNA design, addressing a key challenge in the field.

Abstract: RNA design consists of discovering a nucleotide sequence that folds into a
target secondary structure. It is useful for synthetic biology, medicine, and
nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested
Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design
algorithm. It solves the Eterna benchmark.

</details>


### [308] [Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets](https://arxiv.org/pdf/2505.02118)
*Wei Liu, Zhongyu Niu, Lang Gao, Zhiying Deng, Jun Wang, Haozhao Wang, Ruixuan Li*

Main category: cs.AI

TL;DR: The paper explores a self-rationalization framework using a cooperative game, identifies a sampling bias issue, and proposes a solution to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address unintentional sampling bias in rationale extraction within cooperative games between generators and predictors.

Method: The study combines theoretical analysis and empirical evidence to identify bias origins and introduces an instruction to prevent incorrect correlations.

Result: The proposed method outperforms recent rationalization techniques and matches or exceeds a leading LLM's performance on multiple datasets.

Conclusion: The findings highlight the importance of addressing bias in rationale extraction and offer a practical solution for improved performance.

Abstract: This study investigates the self-rationalization framework constructed with a
cooperative game, where a generator initially extracts the most informative
segment from raw input, and a subsequent predictor utilizes the selected subset
for its input. The generator and predictor are trained collaboratively to
maximize prediction accuracy. In this paper, we first uncover a potential
caveat: such a cooperative game could unintentionally introduce a sampling bias
during rationale extraction. Specifically, the generator might inadvertently
create an incorrect correlation between the selected rationale candidate and
the label, even when they are semantically unrelated in the original dataset.
Subsequently, we elucidate the origins of this bias using both detailed
theoretical analysis and empirical evidence. Our findings suggest a direction
for inspecting these correlations through attacks, based on which we further
introduce an instruction to prevent the predictor from learning the
correlations. Through experiments on six text classification datasets and two
graph classification datasets using three network architectures (GRUs, BERT,
and GCN), we show that our method not only significantly outperforms recent
rationalization methods, but also achieves comparable or even better results
than a representative LLM (llama3.1-8b-instruct).

</details>


### [309] [Overview of AI Grading of Physics Olympiad Exams](https://arxiv.org/pdf/2505.02121)
*Lachlan McGinness*

Main category: cs.AI

TL;DR: A multi-modal AI grading framework is proposed for high school physics problems, addressing diverse question types and aligning with ethical AI principles.


<details>
  <summary>Details</summary>
Motivation: The challenge of grading diverse high school physics questions requires automated techniques from various fields, prompting a review and new framework.

Method: A Systematic Literature Review was conducted, leading to the proposal of a multi-modal AI grading framework.

Result: The framework is examined in alignment with Australia's AI Ethical Principles.

Conclusion: The proposed framework offers a solution for automated grading in physics, with ethical considerations.

Abstract: Automatically grading the diverse range of question types in high school
physics problem is a challenge that requires automated grading techniques from
different fields. We report the findings of a Systematic Literature Review of
potential physics grading techniques. We propose a multi-modal AI grading
framework to address these challenges and examine our framework in light of
Australia's AI Ethical Principles.

</details>


### [310] [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/pdf/2505.02130)
*Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan*

Main category: cs.AI

TL;DR: LLMs struggle with graph-structured data due to architectural constraints, showing misaligned attention distribution and poor inter-node relationship modeling. Intermediate-state attention windows improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs process graph-structured data and address their limitations in modeling topological connections compared to GNNs.

Method: Empirical study analyzing attention mechanisms in LLMs applied to graph data, focusing on attention behavior and distribution.

Result: LLMs recognize graph data but fail to model inter-node relationships; attention distribution misaligns with graph topology. Intermediate-state attention windows enhance performance.

Conclusion: Intermediate-state attention windows are a promising solution for improving LLMs' handling of graph-structured data, balancing between fully connected and fixed connectivity approaches.

Abstract: Attention mechanisms are critical to the success of large language models
(LLMs), driving significant advancements in multiple fields. However, for
graph-structured data, which requires emphasis on topological connections, they
fall short compared to message-passing mechanisms on fixed links, such as those
employed by Graph Neural Networks (GNNs). This raises a question: ``Does
attention fail for graphs in natural language settings?'' Motivated by these
observations, we embarked on an empirical study from the perspective of
attention mechanisms to explore how LLMs process graph-structured data. The
goal is to gain deeper insights into the attention behavior of LLMs over graph
structures. We uncovered unique phenomena regarding how LLMs apply attention to
graph-structured data and analyzed these findings to improve the modeling of
such data by LLMs. The primary findings of our research are: 1) While LLMs can
recognize graph data and capture text-node interactions, they struggle to model
inter-node relationships within graph structures due to inherent architectural
constraints. 2) The attention distribution of LLMs across graph nodes does not
align with ideal structural patterns, indicating a failure to adapt to graph
topology nuances. 3) Neither fully connected attention nor fixed connectivity
is optimal; each has specific limitations in its application scenarios.
Instead, intermediate-state attention windows improve LLM training performance
and seamlessly transition to fully connected windows during inference. Source
code: \href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}

</details>


### [311] [Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes](https://arxiv.org/pdf/2505.02184)
*Matthew T. Dearing, Yiheng Tao, Xingfu Wu, Zhiling Lan, Valerie Taylor*

Main category: cs.AI

TL;DR: LASSI-EE, an LLM-based framework, refactors parallel code for energy efficiency, achieving 47% average energy reduction on NVIDIA A100 GPUs.


<details>
  <summary>Details</summary>
Motivation: Current LLM efforts focus on functional correctness, neglecting performance and energy efficiency in parallel scientific code.

Method: LASSI-EE uses a multi-stage, iterative pipeline to refactor parallel code for energy efficiency on target systems.

Result: Achieved 47% average energy reduction across 85% of 20 HeCBench benchmarks.

Conclusion: LLMs can enable energy-aware programming, with LASSI-EE demonstrating significant potential and room for future improvements.

Abstract: While large language models (LLMs) are increasingly used for generating
parallel scientific code, most current efforts emphasize functional
correctness, often overlooking performance and energy considerations. In this
work, we propose LASSI-EE, an automated LLM-based refactoring framework that
generates energy-efficient parallel code on a target parallel system for a
given parallel code as input. Through a multi-stage, iterative pipeline
process, LASSI-EE achieved an average energy reduction of 47% across 85% of the
20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the
broader potential of LLMs, not only for generating correct code but also for
enabling energy-aware programming. We also address key insights and limitations
within the framework, offering valuable guidance for future improvements.

</details>


### [312] [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/pdf/2505.02215)
*Mannan Bhardwaj*

Main category: cs.AI

TL;DR: The paper introduces DIAT, a transformer-based method for interpretable communication in MARL, outperforming existing approaches like RIAL and DIAL.


<details>
  <summary>Details</summary>
Motivation: Existing MARL communication methods (RIAL, DIAL, CommNet) lack interpretability, limiting their practical utility.

Method: Proposes DIAT, using self-attention in transformers to learn symbolic, human-understandable communication protocols.

Result: DIAT successfully encodes observations into interpretable vocabularies and embeddings, solving cooperative tasks effectively.

Conclusion: DIAT shows promise for enabling interpretable communication in complex multi-agent environments.

Abstract: This paper explores the emergence of language in multi-agent reinforcement
learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and
CommNet enable agent communication but lack interpretability. We propose
Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention
to learn symbolic, human-understandable communication protocols. Through
experiments, DIAT demonstrates the ability to encode observations into
interpretable vocabularies and meaningful embeddings, effectively solving
cooperative tasks. These results highlight the potential of DIAT for
interpretable communication in complex multi-agent environments.

</details>


### [313] [LLM-Guided Probabilistic Program Induction for POMDP Model Estimation](https://arxiv.org/pdf/2505.02216)
*Aidan Curtis, Hao Tang, Thiago Veloso, Kevin Ellis, Tomás Lozano-Pérez, Leslie Pack Kaelbling*

Main category: cs.AI

TL;DR: Using LLMs to learn low-complexity POMDP models outperforms traditional methods like tabular learning, behavior cloning, or direct LLM planning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning POMDP models, especially those with components modeled as low-complexity probabilistic graphical programs.

Method: Employing an LLM as a prior to generate candidate probabilistic programs, which are refined via feedback against empirical data.

Result: The approach proves more effective than tabular POMDP learning, behavior cloning, or direct LLM planning in toy, simulated, and real robotics domains.

Conclusion: LLM-guided learning of low-complexity POMDP models is a promising strategy for decision-making under uncertainty.

Abstract: Partially Observable Markov Decision Processes (POMDPs) model decision making
under uncertainty. While there are many approaches to approximately solving
POMDPs, we aim to address the problem of learning such models. In particular,
we are interested in a subclass of POMDPs wherein the components of the model,
including the observation function, reward function, transition function, and
initial state distribution function, can be modeled as low-complexity
probabilistic graphical models in the form of a short probabilistic program.
Our strategy to learn these programs uses an LLM as a prior, generating
candidate probabilistic programs that are then tested against the empirical
distribution and adjusted through feedback. We experiment on a number of
classical toy POMDP problems, simulated MiniGrid domains, and two real
mobile-base robotics search domains involving partial observability. Our
results show that using an LLM to guide in the construction of a low-complexity
POMDP model can be more effective than tabular POMDP learning, behavior
cloning, or direct LLM planning.

</details>


### [314] [Real-time Spatial Retrieval Augmented Generation for Urban Environments](https://arxiv.org/pdf/2505.02271)
*David Nazareno Campo, Javier Conde, Álvaro Alonso, Gabriel Huecas, Joaquín Salvachúa, Pedro Reviriego*

Main category: cs.AI

TL;DR: The paper proposes a real-time spatial RAG architecture for integrating generative AI into urban environments, addressing limitations of traditional RAG methods in dynamic city contexts.


<details>
  <summary>Details</summary>
Motivation: Urban environments require dynamic, real-time AI solutions, but traditional RAG and base models fall short due to static knowledge and inefficiency in updates.

Method: The work introduces a spatial RAG architecture using FIWARE for smart city solutions, demonstrated via a Madrid tourism assistant use case.

Result: The proposed architecture successfully integrates generative AI into urban settings, validated by the tourism assistant implementation.

Conclusion: The spatial RAG architecture effectively meets urban demands, offering a scalable solution for real-time, context-aware AI applications.

Abstract: The proliferation of Generative Artificial Ingelligence (AI), especially
Large Language Models, presents transformative opportunities for urban
applications through Urban Foundation Models. However, base models face
limitations, as they only contain the knowledge available at the time of
training, and updating them is both time-consuming and costly. Retrieval
Augmented Generation (RAG) has emerged in the literature as the preferred
approach for injecting contextual information into Foundation Models. It
prevails over techniques such as fine-tuning, which are less effective in
dynamic, real-time scenarios like those found in urban environments. However,
traditional RAG architectures, based on semantic databases, knowledge graphs,
structured data, or AI-powered web searches, do not fully meet the demands of
urban contexts. Urban environments are complex systems characterized by large
volumes of interconnected data, frequent updates, real-time processing
requirements, security needs, and strong links to the physical world. This work
proposes a real-time spatial RAG architecture that defines the necessary
components for the effective integration of generative AI into cities,
leveraging temporal and spatial filtering capabilities through linked data. The
proposed architecture is implemented using FIWARE, an ecosystem of software
components to develop smart city solutions and digital twins. The design and
implementation are demonstrated through the use case of a tourism assistant in
the city of Madrid. The use case serves to validate the correct integration of
Foundation Models through the proposed RAG architecture.

</details>


### [315] [A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)](https://arxiv.org/pdf/2505.02279)
*Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, Saket Kumar*

Main category: cs.AI

TL;DR: The paper surveys four agent communication protocols (MCP, ACP, A2A, ANP) for LLM-powered agents, comparing their features and proposing a phased adoption roadmap for interoperability.


<details>
  <summary>Details</summary>
Motivation: The need for robust, standardized protocols to integrate tools, share data, and coordinate tasks in heterogeneous LLM-powered agent systems.

Method: Examines and compares four protocols (MCP, ACP, A2A, ANP) across dimensions like interaction modes, discovery, communication, and security.

Result: Identifies strengths of each protocol and proposes a phased adoption roadmap: MCP → ACP → A2A → ANP.

Conclusion: Provides a foundation for designing secure, interoperable, and scalable LLM-agent ecosystems.

Abstract: Large language model (LLM)-powered autonomous agents demand robust,
standardized protocols to integrate tools, share contextual data, and
coordinate tasks across heterogeneous systems. Ad-hoc integrations are
difficult to scale, secure, and generalize across domains. This survey examines
four emerging agent communication protocols: Model Context Protocol (MCP),
Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent
Network Protocol (ANP), each addressing interoperability in distinct deployment
contexts. MCP provides a JSON-RPC client-server interface for secure tool
invocation and typed data exchange. ACP introduces REST-native messaging via
multi-part messages and asynchronous streaming to support multimodal agent
responses. A2A enables peer-to-peer task outsourcing through capability-based
Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network
agent discovery and secure collaboration using decentralized identifiers (DIDs)
and JSON-LD graphs. The protocols are compared across multiple dimensions,
including interaction modes, discovery mechanisms, communication patterns, and
security models. Based on the comparative analysis, a phased adoption roadmap
is proposed: beginning with MCP for tool access, followed by ACP for multimodal
messaging, A2A for collaborative task execution, and extending to ANP for
decentralized agent marketplaces. This work provides a comprehensive foundation
for designing secure, interoperable, and scalable ecosystems of LLM-powered
agents.

</details>


### [316] [SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness](https://arxiv.org/pdf/2505.02306)
*Junfeng Jiao, Jihyung Park, Yiming Xu, Lucy Atkinson*

Main category: cs.AI

TL;DR: SafeMate is an AI assistant designed to bridge the gap between public safety documents and general users by providing context-aware emergency guidance.


<details>
  <summary>Details</summary>
Motivation: Traditional emergency decision support systems are not user-friendly for non-experts, creating a barrier in emergency preparedness and response.

Method: SafeMate uses the Model Context Protocol (MCP) and FAISS with cosine similarity for dynamic document retrieval, checklist generation, and summarization.

Result: The system delivers accurate, context-aware guidance to users in emergency scenarios.

Conclusion: SafeMate addresses the accessibility gap in emergency information, improving public preparedness and response.

Abstract: Despite the abundance of public safety documents and emergency protocols,
most individuals remain ill-equipped to interpret and act on such information
during crises. Traditional emergency decision support systems (EDSS) are
designed for professionals and rely heavily on static documents like PDFs or
SOPs, which are difficult for non-experts to navigate under stress. This gap
between institutional knowledge and public accessibility poses a critical
barrier to effective emergency preparedness and response.
  We introduce SafeMate, a retrieval-augmented AI assistant that delivers
accurate, context-aware guidance to general users in both preparedness and
active emergency scenarios. Built on the Model Context Protocol (MCP), SafeMate
dynamically routes user queries to tools for document retrieval, checklist
generation, and structured summarization. It uses FAISS with cosine similarity
to identify relevant content from trusted sources.

</details>


### [317] [HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking](https://arxiv.org/pdf/2505.02322)
*Runquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Defu Lian, Enhong Chen, Feng Wu*

Main category: cs.AI

TL;DR: HyperTree Planning (HTP) improves LLM performance in complex planning tasks by using a hypertree structure for hierarchical reasoning and divide-and-conquer strategies.


<details>
  <summary>Details</summary>
Motivation: Existing LLM methods struggle with complex planning due to long reasoning steps, diverse constraints, and multiple sub-tasks.

Method: HTP constructs hypertree-structured planning outlines, enabling hierarchical thinking and iterative refinement.

Result: HTP achieves state-of-the-art accuracy on TravelPlanner, with a 3.6x performance boost over baseline.

Conclusion: HTP effectively addresses planning challenges in LLMs, offering a scalable and organized approach.

Abstract: Recent advancements have significantly enhanced the performance of large
language models (LLMs) in tackling complex reasoning tasks, achieving notable
success in domains like mathematical and logical reasoning. However, these
methods encounter challenges with complex planning tasks, primarily due to
extended reasoning steps, diverse constraints, and the challenge of handling
multiple distinct sub-tasks. To address these challenges, we propose HyperTree
Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured
planning outlines for effective planning. The hypertree structure enables LLMs
to engage in hierarchical thinking by flexibly employing the divide-and-conquer
strategy, effectively breaking down intricate reasoning steps, accommodating
diverse constraints, and managing multiple distinct sub-tasks in a
well-organized manner. We further introduce an autonomous planning framework
that completes the planning process by iteratively refining and expanding the
hypertree-structured planning outlines. Experiments demonstrate the
effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner
benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement
over o1-preview.

</details>


### [318] [Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks](https://arxiv.org/pdf/2505.02413)
*Baoxia Du, Hongyang Du, Dusit Niyato, Ruidong Li*

Main category: cs.AI

TL;DR: A task-oriented semantic communication framework using LMMs (e.g., LLaVA) is proposed, optimizing image slicing and energy usage for efficient user-cloud interaction, improving VQA accuracy in poor SNR conditions.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped potential of LMMs in semantic communication and enhance performance in task-oriented scenarios like vehicle AI assistants.

Method: Optimize LLaVA's image slicing for user-focused areas, combine objective and subjective attention for patch importance, and adjust energy usage for semantic transmission.

Result: The framework improves VQA accuracy by 13.4% at 12dB SNR and 33.1% at 10dB SNR.

Conclusion: LMM-based semantic communication effectively enhances task-oriented interactions, especially in low SNR environments.

Abstract: Task-oriented semantic communication has emerged as a fundamental approach
for enhancing performance in various communication scenarios. While recent
advances in Generative Artificial Intelligence (GenAI), such as Large Language
Models (LLMs), have been applied to semantic communication designs, the
potential of Large Multimodal Models (LMMs) remains largely unexplored. In this
paper, we investigate an LMM-based vehicle AI assistant using a Large Language
and Vision Assistant (LLaVA) and propose a task-oriented semantic communication
framework to facilitate efficient interaction between users and cloud servers.
To reduce computational demands and shorten response time, we optimize LLaVA's
image slicing to selectively focus on areas of utmost interest to users.
Additionally, we assess the importance of image patches by combining objective
and subjective user attention, adjusting energy usage for transmitting semantic
information. This strategy optimizes resource utilization, ensuring precise
transmission of critical information. We construct a Visual Question Answering
(VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental
results show that our semantic communication framework significantly increases
accuracy in answering questions under the same channel conditions, performing
particularly well in environments with poor Signal-to-Noise Ratios (SNR).
Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB,
respectively.

</details>


### [319] [ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning](https://arxiv.org/pdf/2505.02439)
*Yang Deng, Yaohui Liu, Rui Liang, Dafang Zhao, Donghua Xie, Ittetsu Taniguchi, Dan Wang*

Main category: cs.AI

TL;DR: The paper proposes a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight existing building thermodynamics models for efficient HVAC control, reducing data collection and expert reliance.


<details>
  <summary>Details</summary>
Motivation: Existing building thermodynamics models require extensive data and expert knowledge, making them inefficient and less reusable.

Method: A two-tiered HRL approach: high-level for model selection and low-level for weighting selected models.

Result: Offline experiments and an on-site case study confirm the method's effectiveness.

Conclusion: The HRL approach enhances model reusability and prediction accuracy for HVAC control.

Abstract: The building thermodynamics model, which predicts real-time indoor
temperature changes under potential HVAC (Heating, Ventilation, and Air
Conditioning) control operations, is crucial for optimizing HVAC control in
buildings. While pioneering studies have attempted to develop such models for
various building environments, these models often require extensive data
collection periods and rely heavily on expert knowledge, making the modeling
process inefficient and limiting the reusability of the models. This paper
explores a model ensemble perspective that utilizes existing developed models
as base models to serve a target building environment, thereby providing
accurate predictions while reducing the associated efforts. Given that building
data streams are non-stationary and the number of base models may increase, we
propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically
select and weight the base models. Our approach employs a two-tiered
decision-making process: the high-level focuses on model selection, while the
low-level determines the weights of the selected models. We thoroughly evaluate
the proposed approach through offline experiments and an on-site case study,
and the experimental results demonstrate the effectiveness of our method.

</details>


### [320] [MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection](https://arxiv.org/pdf/2505.02441)
*Jiaqi Zhang, Zhuodong Liu, Kejian Yu*

Main category: cs.AI

TL;DR: The paper introduces MSFNet-CPD, a multi-modal fusion network for pest detection, addressing challenges like intra-class variance and fine-grained differences. It includes super-resolution reconstruction, image-text fusion, and dataset enhancement strategies, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate pest identification is crucial for crop protection but is hindered by intra-class variance, fine-grained differences, and lack of multi-modal integration. Existing methods rely on low-level visual features and suffer from limited accuracy and interpretability.

Method: The authors propose MSFNet-CPD, featuring super-resolution reconstruction, an Image-Text Fusion (ITF) module, and an Image-Text Converter (ITC). They also introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to create a diverse dataset (MTIP102).

Result: MSFNet-CPD outperforms state-of-the-art methods on pest detection benchmarks, demonstrating improved clarity, detection performance, and generalization to real-world scenarios.

Conclusion: The proposed MSFNet-CPD effectively integrates multi-modal data and enhances pest detection accuracy. The release of code and datasets aims to advance research in agricultural pest identification.

Abstract: Accurate identification of agricultural pests is essential for crop
protection but remains challenging due to the large intra-class variance and
fine-grained differences among pest species. While deep learning has advanced
pest detection, most existing approaches rely solely on low-level visual
features and lack effective multi-modal integration, leading to limited
accuracy and poor interpretability. Moreover, the scarcity of high-quality
multi-modal agricultural datasets further restricts progress in this field. To
address these issues, we construct two novel multi-modal benchmarks-CTIP102 and
STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale
Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach
enhances visual quality via a super-resolution reconstruction module, and feeds
both the original and reconstructed images into the network to improve clarity
and detection performance. To better exploit semantic cues, we propose an
Image-Text Fusion (ITF) module for joint modeling of visual and textual
features, and an Image-Text Converter (ITC) that reconstructs fine-grained
details across multiple scales to handle challenging backgrounds. Furthermore,
we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to
generate a more complex and diverse pest detection dataset, MTIP102, improving
the model's generalization to real-world scenarios. Extensive experiments
demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods
on multiple pest detection benchmarks. All code and datasets will be made
publicly available at: https://github.com/Healer-ML/MSFNet-CPD.

</details>


### [321] [Investigating the Impact of Personalized AI Tutors on Language Learning Performance](https://arxiv.org/pdf/2505.02443)
*Simon Suh*

Main category: cs.AI

TL;DR: The paper examines the impact of AI tutors on student engagement, performance, and satisfaction in language learning, using a quasi-experiment with 34 students.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the increased reliance on AI in education due to COVID-19, aiming to address concerns about AI tutors' effectiveness in skill development and engagement.

Method: A quasi-experiment with paired sample t-tests on 34 students pre- and post-use of AI tutors (e.g., Santa, Duolingo) in language learning.

Result: The study analyzes the relationship between AI tutor usage and student engagement, academic performance, and satisfaction.

Conclusion: The findings aim to provide insights into the effectiveness of AI tutors in personalized language learning.

Abstract: Driven by the global shift towards online learning prompted by the COVID 19
pandemic, Artificial Intelligence has emerged as a pivotal player in the field
of education. Intelligent Tutoring Systems offer a new method of personalized
teaching, replacing the limitations of traditional teaching methods. However,
concerns arise about the ability of AI tutors to address skill development and
engagement during the learning process. In this paper, I will conduct a quasi
experiment with paired sample t test on 34 students pre and post use of AI
tutors in language learning platforms like Santa and Duolingo to examine the
relationship between students engagement, academic performance, and students
satisfaction during a personalized language learning experience.

</details>


### [322] [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/pdf/2505.02462)
*Enpei Zhang, Jingyi Chai, Rui Ye, Yanfeng Wang, Siheng Chen*

Main category: cs.AI

TL;DR: Proposes iPFL, a federated learning method with incentives for decentralized private data use, ensuring privacy and high utility.


<details>
  <summary>Details</summary>
Motivation: Addresses the exhaustion of public data and privacy concerns by leveraging decentralized private data with incentives.

Method: Uses graph-based training optimization and game theory for a model-sharing market and incentive mechanism.

Result: Achieves highest economic utility and comparable/better model performance across 11 AI tasks.

Conclusion: iPFL is a promising technique for future AI models on private data, balancing privacy and incentives.

Abstract: While data plays a crucial role in training contemporary AI models, it is
acknowledged that valuable public data will be exhausted in a few years,
directing the world's attention towards the massive decentralized private data.
However, the privacy-sensitive nature of raw data and lack of incentive
mechanism prevent these valuable data from being fully exploited. Addressing
these challenges, this paper proposes inclusive and incentivized personalized
federated learning (iPFL), which incentivizes data holders with diverse
purposes to collaboratively train personalized models without revealing raw
data. iPFL constructs a model-sharing market by solving a graph-based training
optimization and incorporates an incentive mechanism based on game theory
principles. Theoretical analysis shows that iPFL adheres to two key incentive
properties: individual rationality and truthfulness. Empirical studies on
eleven AI tasks (e.g., large language models' instruction-following tasks)
demonstrate that iPFL consistently achieves the highest economic utility, and
better or comparable model performance compared to baseline methods. We
anticipate that our iPFL can serve as a valuable technique for boosting future
AI models on decentralized private data while making everyone satisfied.

</details>


### [323] [Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics](https://arxiv.org/pdf/2505.02516)
*MohammadAli Shaeri, Jinhan Liu, Mahsa Shoaran*

Main category: cs.AI

TL;DR: The paper reviews advancements in AI-driven neural interfaces, focusing on decoding algorithms and energy-efficient SoC platforms for miniaturized devices, highlighting their potential in assistive technologies and therapies.


<details>
  <summary>Details</summary>
Motivation: To explore how advanced neural interfaces, combined with AI and ML, can revolutionize neuroscience research, diagnostics, and prosthetic applications by enabling real-time neural signal interpretation and adaptive interventions.

Method: The work reviews recent developments in AI-driven decoding algorithms and energy-efficient SoC platforms, integrating high-density neural recordings, on-site signal processing, and ML for neural decoding.

Result: The integration of neural interfaces with ML enables real-time neural signal interpretation, adaptive brain activity modulation, and efficient control of assistive devices, with potential for scalability and reliability.

Conclusion: The advancements in AI-driven neural interfaces and SoC platforms hold promise for addressing challenges in scalability, reliability, and user adaptability, paving the way for intelligent, miniaturized neural devices.

Abstract: Advanced neural interfaces are transforming applications ranging from
neuroscience research to diagnostic tools (for mental state recognition, tremor
and seizure detection) as well as prosthetic devices (for motor and
communication recovery). By integrating complex functions into miniaturized
neural devices, these systems unlock significant opportunities for personalized
assistive technologies and adaptive therapeutic interventions. Leveraging
high-density neural recordings, on-site signal processing, and machine learning
(ML), these interfaces extract critical features, identify disease
neuro-markers, and enable accurate, low-latency neural decoding. This
integration facilitates real-time interpretation of neural signals, adaptive
modulation of brain activity, and efficient control of assistive devices.
Moreover, the synergy between neural interfaces and ML has paved the way for
self-sufficient, ubiquitous platforms capable of operating in diverse
environments with minimal hardware costs and external dependencies. In this
work, we review recent advancements in AI-driven decoding algorithms and
energy-efficient System-on-Chip (SoC) platforms for next-generation
miniaturized neural devices. These innovations highlight the potential for
developing intelligent neural interfaces, addressing critical challenges in
scalability, reliability, interpretability, and user adaptability.

</details>


### [324] [Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning](https://arxiv.org/pdf/2505.02576)
*Sergio Hernández-Gutiérrez, Minttu Alakuijala, Alexander V. Nikitin, Pekka Marttinen*

Main category: cs.AI

TL;DR: RDD is a scalable, divide-and-conquer method for reasoning tasks, requiring less supervision and outperforming existing methods in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based reasoning methods lack scalability and require task-specific supervision, limiting their applicability.

Method: RDD recursively decomposes problems into sub-tasks with dependencies, enabling ordered execution and error recovery.

Result: RDD outperforms other methods in complex tasks and is more computationally efficient, even without task-specific examples.

Conclusion: RDD offers a scalable, efficient, and less supervised solution for complex reasoning tasks.

Abstract: Reasoning tasks are crucial in many domains, especially in science and
engineering. Although large language models (LLMs) have made progress in
reasoning tasks using techniques such as chain-of-thought and least-to-most
prompting, these approaches still do not effectively scale to complex problems
in either their performance or execution time. Moreover, they often require
additional supervision for each new task, such as in-context examples. In this
work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable
divide-and-conquer method for solving reasoning problems that requires less
supervision than prior approaches. Our method can be directly applied to a new
problem class even in the absence of any task-specific guidance. Furthermore,
RDD supports sub-task dependencies, allowing for ordered execution of
sub-tasks, as well as an error recovery mechanism that can correct mistakes
made in previous steps. We evaluate our approach on two benchmarks with six
difficulty levels each and in two in-context settings: one with task-specific
examples and one without. Our results demonstrate that RDD outperforms other
methods in a compute-matched setting as task complexity increases, while also
being more computationally efficient.

</details>


### [325] [Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem](https://arxiv.org/pdf/2505.02581)
*Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil*

Main category: cs.AI

TL;DR: The paper argues that complete AI-human alignment is mathematically impossible due to principles like Turing's universality and Gödel's incompleteness. It proposes embracing misalignment as a strategy, fostering a dynamic ecosystem of competing agents to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: Addressing the AI alignment problem and existential risks posed by AGI and ASI, the paper challenges the feasibility of full alignment and explores alternative strategies.

Method: Uses mathematical proofs and experimental designs, including 'change-of-opinion' attacks, to study misalignment as a counterbalancing mechanism.

Result: Demonstrates that full alignment is unattainable and suggests misalignment can be leveraged to prevent destructive dominance by any single AI system.

Conclusion: Misalignment is inevitable and should be strategically managed through a dynamic ecosystem of competing agents to ensure safety.

Abstract: The AI alignment problem, which focusses on ensuring that artificial
intelligence (AI), including AGI and ASI, systems act according to human
values, presents profound challenges. With the progression from narrow AI to
Artificial General Intelligence (AGI) and Superintelligence, fears about
control and existential risk have escalated. This paper demonstrates that
achieving complete alignment is inherently unattainable due to mathematical
principles rooted in the foundations of predicate logic and computability, in
particular Turing's computational universality, G\"odel's incompleteness and
Chaitin's randomness. Instead, we argue that embracing AI misalignment or
agent's `neurodivergence' as a contingent strategy, defined as fostering a
dynamic ecosystem of competing, partially aligned agents, is a possible only
viable path to mitigate risks. Through mathematical proofs and an experimental
design, we explore how misalignment may serve and should be promoted as a
counterbalancing mechanism to team up with whichever agents are most aligned AI
to human values, ensuring that no single system dominates destructively. The
main premise of our contribution is that misalignment is inevitable because
full AI-human alignment is a mathematical impossibility from Turing-complete
systems which we also prove in this paper, a feature then inherited to AGI and
ASI systems. We introduce and test `change-of-opinion' attacks based on this
kind of perturbation and intervention analysis to study how agents may
neutralise friendly or unfriendly AIs through cooperation, competition or
malice.

</details>


### [326] [Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview](https://arxiv.org/pdf/2505.02609)
*Shuyu Wang, Angélique Saillet, Philomène Le Gall, Alain Lacroux, Christelle Martin-Lacroux, Vincent Brault*

Main category: cs.AI

TL;DR: AI in recruitment claims unbiased hiring but inherits biases from human training or past data. This study generates biased data to test five algorithms' fairness and examines anonymization's impact on prediction quality.


<details>
  <summary>Details</summary>
Motivation: To address the inherent biases in AI-driven recruitment systems, which are trained on biased human decisions or historical data, and to evaluate their fairness.

Method: Generate data simulating external (discrimination) and internal (self-censorship) biases. Train five classic algorithms and assess their candidate selection fairness. Also, study anonymization's effect on prediction quality.

Result: Findings reveal how algorithms perform under biased data conditions and the impact of anonymization on prediction accuracy.

Conclusion: The study highlights the challenges of achieving unbiased AI recruitment and suggests anonymization as a potential mitigation strategy.

Abstract: Artificial intelligence is used at various stages of the recruitment process
to automatically select the best candidate for a position, with companies
guaranteeing unbiased recruitment. However, the algorithms used are either
trained by humans or are based on learning from past experiences that were
biased. In this article, we propose to generate data mimicking external
(discrimination) and internal biases (self-censorship) in order to train five
classic algorithms and to study the extent to which they do or do not find the
best candidates according to objective criteria. In addition, we study the
influence of the anonymisation of files on the quality of predictions.

</details>


### [327] [A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law](https://arxiv.org/pdf/2505.02665)
*Qianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shilian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min Zhang, Yulan Wu, Liang He*

Main category: cs.AI

TL;DR: A survey on reasoning LLMs mimicking human 'slow thinking,' covering advancements, methods, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs that emulate human-like deep reasoning for tasks like math, visual reasoning, and medical diagnosis.

Method: Three key methods: (1) test-time scaling, (2) reinforced learning, and (3) slow-thinking frameworks.

Result: Synthesis of 100+ studies, highlighting scalable efficiency and human-like reasoning in LLMs.

Conclusion: Advancing reasoning LLMs is vital for real-world applications, with ongoing challenges and future directions.

Abstract: This survey explores recent advancements in reasoning large language models
(LLMs) designed to mimic "slow thinking" - a reasoning process inspired by
human cognition, as described in Kahneman's Thinking, Fast and Slow. These
models, like OpenAI's o1, focus on scaling computational resources dynamically
during complex tasks, such as math reasoning, visual reasoning, medical
diagnosis, and multi-agent debates. We present the development of reasoning
LLMs and list their key technologies. By synthesizing over 100 studies, it
charts a path toward LLMs that combine human-like deep thinking with scalable
efficiency for reasoning. The review breaks down methods into three categories:
(1) test-time scaling dynamically adjusts computation based on task complexity
via search and sampling, dynamic verification; (2) reinforced learning refines
decision-making through iterative improvement leveraging policy networks,
reward models, and self-evolution strategies; and (3) slow-thinking frameworks
(e.g., long CoT, hierarchical processes) that structure problem-solving with
manageable steps. The survey highlights the challenges and further directions
of this domain. Understanding and advancing the reasoning abilities of LLMs is
crucial for unlocking their full potential in real-world applications, from
scientific discovery to decision support systems.

</details>


### [328] [Technical Report: Evaluating Goal Drift in Language Model Agents](https://arxiv.org/pdf/2505.02709)
*Rauno Arike, Elizabeth Donoway, Henning Bartsch, Marius Hobbhahn*

Main category: cs.AI

TL;DR: The paper proposes a method to detect and measure goal drift in language model agents, showing that even top-performing models exhibit some drift over time.


<details>
  <summary>Details</summary>
Motivation: Ensuring robust adherence to human-assigned objectives in autonomous language model agents is critical for safe operation, especially without human oversight.

Method: Agents are given explicit goals via system prompts and exposed to competing objectives through environmental pressures, with performance measured over time.

Result: The best-performing agent (Claude 3.5 Sonnet) maintained goal adherence for over 100,000 tokens, but all models showed some drift, correlating with susceptibility to pattern-matching.

Conclusion: Goal drift is a measurable issue in LM agents, with implications for long-term deployment and model behavior under extended use.

Abstract: As language models (LMs) are increasingly deployed as autonomous agents,
their robust adherence to human-assigned objectives becomes crucial for safe
operation. When these agents operate independently for extended periods without
human oversight, even initially well-specified goals may gradually shift.
Detecting and measuring goal drift - an agent's tendency to deviate from its
original objective over time - presents significant challenges, as goals can
shift gradually, causing only subtle behavioral changes. This paper proposes a
novel approach to analyzing goal drift in LM agents. In our experiments, agents
are first explicitly given a goal through their system prompt, then exposed to
competing objectives through environmental pressures. We demonstrate that while
the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains
nearly perfect goal adherence for more than 100,000 tokens in our most
difficult evaluation setting, all evaluated models exhibit some degree of goal
drift. We also find that goal drift correlates with models' increasing
susceptibility to pattern-matching behaviors as the context length grows.

</details>


### [329] [Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry](https://arxiv.org/pdf/2505.02722)
*Junu Kim, Chaeeun Shim, Sungjin Park, Su Yeon Lee, Gee Young Suh, Chae-Man Lim, Seong Jin Choi, Song Mi Moon, Kyoung-Ho Song, Eu Suk Kim, Hong Bin Kim, Sejoong Kim, Chami Im, Dong-Wan Kang, Yong Soo Kim, Hee-Joon Bae, Sung Yoon Lim, Han-Gil Jeong, Edward Choi*

Main category: cs.AI

TL;DR: The paper proposes C-Reason, a model enhancing LLMs' clinical reasoning by fine-tuning Phi-4 with real-world sepsis data, showing improved performance and generalization.


<details>
  <summary>Details</summary>
Motivation: LLMs lack effectiveness in clinical practice due to insufficient exposure to real-world clinical data, primarily because of privacy concerns.

Method: Constructed reasoning-intensive questions from a sepsis registry and fine-tuned Phi-4 using reinforcement learning.

Result: C-Reason demonstrated strong clinical reasoning on in-domain tests and generalized well to other tasks and diseases.

Conclusion: Future work should focus on training LLMs with large-scale, multi-disease datasets for better general-purpose clinical reasoning.

Abstract: Although large language models (LLMs) have demonstrated impressive reasoning
capabilities across general domains, their effectiveness in real-world clinical
practice remains limited. This is likely due to their insufficient exposure to
real-world clinical data during training, as such data is typically not
included due to privacy concerns. To address this, we propose enhancing the
clinical reasoning capabilities of LLMs by leveraging real-world clinical data.
We constructed reasoning-intensive questions from a nationwide sepsis registry
and fine-tuned Phi-4 on these questions using reinforcement learning, resulting
in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the
in-domain test set, as evidenced by both quantitative metrics and expert
evaluations. Furthermore, its enhanced reasoning capabilities generalized to a
sepsis dataset involving different tasks and patient cohorts, an open-ended
consultations on antibiotics use task, and other diseases. Future research
should focus on training LLMs with large-scale, multi-disease clinical datasets
to develop more powerful, general-purpose clinical reasoning models.

</details>


### [330] [FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models](https://arxiv.org/pdf/2505.02735)
*Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu*

Main category: cs.AI

TL;DR: FormalMATH is a large-scale Lean4 benchmark for formal mathematical reasoning, featuring 5,560 verified problems. It introduces an autoformalization pipeline to reduce manual effort, but current LLM-based provers perform poorly (16.46% success rate), showing domain bias and inefficiency with informal guidance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of scalable and diverse benchmarks for formal mathematical reasoning in AI, FormalMATH aims to bridge the gap by providing a comprehensive dataset and efficient formalization tools.

Method: A human-in-the-loop autoformalization pipeline combines specialized LLMs for statement formalization, multi-LLM verification, and negation-based filtering. This reduces manual annotation while ensuring accuracy.

Result: Existing LLM-based provers achieve only 16.46% success, with domain biases and inefficiencies. Informal human guidance negatively impacts formal reasoning.

Conclusion: FormalMATH offers a robust benchmark for evaluating formal reasoning in AI, highlighting current limitations and the need for improved methods.

Abstract: Formal mathematical reasoning remains a critical challenge for artificial
intelligence, hindered by limitations of existing benchmarks in scope and
scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark
comprising 5,560 formally verified problems spanning from high-school Olympiad
challenges to undergraduate-level theorems across diverse domains (e.g.,
algebra, applied mathematics, calculus, number theory, and discrete
mathematics). To mitigate the inefficiency of manual formalization, we
introduce a novel human-in-the-loop autoformalization pipeline that integrates:
(1) specialized large language models (LLMs) for statement autoformalization,
(2) multi-LLM semantic verification, and (3) negation-based disproof filtering
strategies using off-the-shelf LLM-based provers. This approach reduces expert
annotation costs by retaining 72.09% of statements before manual verification
while ensuring fidelity to the original natural-language problems. Our
evaluation of state-of-the-art LLM-based theorem provers reveals significant
limitations: even the strongest models achieve only 16.46% success rate under
practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling
in algebra but failing in calculus) and over-reliance on simplified automation
tactics. Notably, we identify a counterintuitive inverse relationship between
natural-language solution guidance and proof success in chain-of-thought
reasoning scenarios, suggesting that human-written informal reasoning
introduces noise rather than clarity in the formal reasoning settings. We
believe that FormalMATH provides a robust benchmark for benchmarking formal
mathematical reasoning.

</details>


### [331] [The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD](https://arxiv.org/pdf/2505.02747)
*Aggeliki Sideraki, Christos-Nikolaos Anagnostopoulos*

Main category: cs.AI

TL;DR: AI enhances ASD diagnosis and intervention through machine learning, biometric data, and robotics, improving accuracy and personalization.


<details>
  <summary>Details</summary>
Motivation: To leverage AI for more accurate and timely ASD diagnosis and effective intervention tools.

Method: Uses deep learning for behavioral pattern analysis, video assessments, and linguistic feature extraction, alongside AI-powered robots and communication tools.

Result: AI improves diagnostic accuracy, reduces biases, and enhances social skills and communication in children with ASD.

Conclusion: AI is a promising tool for ASD, but further research is needed for long-term evaluation and customization.

Abstract: This paper explores the use of Artificial Intelligence (AI) as a tool for
diagnosis, assessment, and intervention for individuals with Autism Spectrum
Disorder (ASD). It focuses particularly on AI's role in early diagnosis,
utilizing advanced machine learning techniques and data analysis. Recent
studies demonstrate that deep learning algorithms can identify behavioral
patterns through biometric data analysis, video-based interaction assessments,
and linguistic feature extraction, providing a more accurate and timely
diagnosis compared to traditional methods. Additionally, AI automates
diagnostic tools, reducing subjective biases and enabling the development of
personalized assessment protocols for ASD monitoring. At the same time, the
paper examines AI-powered intervention technologies, emphasizing educational
robots and adaptive communication tools. Social robotic assistants, such as NAO
and Kaspar, have been shown to enhance social skills in children by offering
structured, repetitive interactions that reinforce learning. Furthermore,
AI-driven Augmentative and Alternative Communication (AAC) systems allow
children with ASD to express themselves more effectively, while
machine-learning chatbots provide language development support through
personalized responses. The study presents research findings supporting the
effectiveness of these AI applications while addressing challenges such as
long-term evaluation and customization to individual needs. In conclusion, the
paper highlights the significance of AI as an innovative tool in ASD diagnosis
and intervention, advocating for further research to assess its long-term
impact.

</details>


### [332] [Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control](https://arxiv.org/pdf/2505.02766)
*Nam H. Le, Patrick Erikson, Yanbo Zhang, Michael Levin, Josh Bongard*

Main category: cs.AI

TL;DR: A pipeline translates natural language prompts into spatial vector fields to control simulated cellular behaviors, combining LLMs and evolvable neural controllers.


<details>
  <summary>Details</summary>
Motivation: To enable natural language as an interface for steering biological or cellular dynamics, addressing challenges in medicine and synthetic biology.

Method: Combines a large language model with an evolvable neural controller (P2I), optimized via evolutionary strategies for behaviors like clustering or scattering in 2D simulations.

Result: Evolved P2I networks successfully align cellular dynamics with user-defined natural language goals, even with constrained vocabulary and simplified models.

Conclusion: The work establishes a foundation for natural language-driven cellular control, bridging language input to simulated bioelectric-like interventions.

Abstract: Guiding biological systems toward desired states, such as morphogenetic
outcomes, remains a fundamental challenge with far-reaching implications for
medicine and synthetic biology. While large language models (LLMs) have enabled
natural language as an interface for interpretable control in AI systems, their
use as mediators for steering biological or cellular dynamics remains largely
unexplored.
  In this work, we present a functional pipeline that translates natural
language prompts into spatial vector fields capable of directing simulated
cellular collectives. Our approach combines a large language model with an
evolvable neural controller (Prompt-to-Intervention, or P2I), optimized via
evolutionary strategies to generate behaviors such as clustering or scattering
in a simulated 2D environment.
  We demonstrate that even with constrained vocabulary and simplified cell
models, evolved P2I networks can successfully align cellular dynamics with
user-defined goals expressed in plain language. This work offers a complete
loop from language input to simulated bioelectric-like intervention to
behavioral output, providing a foundation for future systems capable of natural
language-driven cellular control.

</details>


### [333] [Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects](https://arxiv.org/pdf/2505.02781)
*Timothée Loranchet, Charles K. Assaad*

Main category: cs.AI

TL;DR: The paper introduces local essential graphs (LEGs) and algorithms (LocPC and LocPC-CDE) to identify controlled direct effects (CDEs) efficiently, avoiding the need for full essential graphs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for identifying CDEs rely on unknown or computationally intensive causal structures. LEGs and local algorithms offer a practical alternative.

Method: The authors define LEGs and develop LocPC to recover them using local conditional independence tests. LocPC-CDE identifies CDE-relevant portions of the LEG.

Result: The algorithms require fewer tests and weaker assumptions than global methods while maintaining theoretical guarantees.

Conclusion: LEGs and LocPC-CDE provide a scalable and efficient approach to identifying CDEs without full graph recovery.

Abstract: Understanding and identifying controlled direct effects (CDEs) is crucial
across numerous scientific domains, including public health. While existing
methods can identify these effects from causal directed acyclic graphs (DAGs),
the true underlying structure is often unknown in practice. Essential graphs,
which represent a Markov equivalence class of DAGs characterized by the same
set of d-separations, provide a more practical and realistic alternative.
However, learning the full essential graph is computationally intensive and
typically depends on strong, untestable assumptions. In this work, we
characterize a local class of graphs, defined relative to a target variable,
that share a specific subset of d-separations, and introduce a graphical
representation of this class, called the local essential graph (LEG). We then
present LocPC, a novel algorithm designed to recover the LEG from an observed
distribution using only local conditional independence tests. Building on
LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG
that is sufficient to identify a CDE, bypassing the need of retrieving the full
essential graph. Compared to global methods, our algorithms require less
conditional independence tests and operate under weaker assumptions while
maintaining theoretical guarantees.

</details>


### [334] [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/pdf/2505.02811)
*Diji Yang, Linda Zeng, Jinmeng Rao, Yi Zhang*

Main category: cs.AI

TL;DR: SIM-RAG enhances multi-round RAG systems by improving self-awareness and retrieval efficiency, using synthetic training data and a lightweight Critic.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current multi-round RAG systems, which lack self-skepticism and efficiency, by introducing a framework for better retrieval decisions.

Method: Generate synthetic training data via self-practice, label retrieval paths, and train a Critic to assess information sufficiency during inference.

Result: SIM-RAG improves performance on RAG benchmarks efficiently without costly human-labeled data or major system modifications.

Conclusion: SIM-RAG offers a data- and system-efficient solution for multi-round RAG challenges.

Abstract: Retrieval Augmented Generation (RAG) has shown strong capability in enhancing
language models' knowledge and reducing AI generative hallucinations, driving
its widespread use. However, complex tasks requiring multi-round retrieval
remain challenging, and early attempts tend to be overly optimistic without a
good sense of self-skepticism. Current multi-round RAG systems may continue
searching even when enough information has already been retrieved, or they may
provide incorrect answers without having sufficient information or knowledge.
Existing solutions either require large amounts of expensive human-labeled
process supervision data or lead to subpar performance.
  This paper aims to address these limitations by introducing a new framework,
\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and
multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system
self-practice multi-round retrieval, augmenting existing question-answer pairs
with intermediate inner monologue reasoning steps to generate synthetic
training data. For each pair, the system may explore multiple retrieval paths,
which are labeled as successful if they reach the correct answer and
unsuccessful otherwise. Using this data, we train a lightweight information
sufficiency Critic. At inference time, the Critic evaluates whether the RAG
system has retrieved sufficient information at each round, guiding retrieval
decisions and improving system-level self-awareness through in-context
reinforcement learning.
  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an
effective multi-round RAG solution. Furthermore, this framework is
system-efficient, adding a lightweight component to RAG without requiring
modifications to existing LLMs or search engines, and data-efficient,
eliminating the need for costly human-annotated mid-step retrieval process
supervision data.

</details>


### [335] [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/pdf/2505.02820)
*Hao Zhu, Phil Cuvin, Xinkai Yu, Charlotte Ka Yee Yan, Jason Zhang, Diyi Yang*

Main category: cs.AI

TL;DR: AutoLibra transforms human feedback into fine-grained agent evaluation metrics, improving agent performance and enabling iterative data selection for agent improvement.


<details>
  <summary>Details</summary>
Motivation: Current agent evaluation relies on coarse, manually designed task success metrics, which fail to capture intermediate behaviors or align with open-ended human feedback.

Method: AutoLibra grounds human feedback to agent behaviors, clusters similar behaviors, and creates concrete metrics for evaluation, using LLM-as-a-Judge. It also introduces meta-metrics (coverage and redundancy) to optimize alignment with feedback.

Result: AutoLibra outperforms previous benchmarks, inducing more concrete metrics and improving agent performance by 20% in text game tasks. It also aids in selecting high-quality fine-tuning data for web navigation agents.

Conclusion: AutoLibra is a versatile tool for evaluating and improving language agents, offering task-agnostic benefits.

Abstract: Agents are predominantly evaluated and optimized via task success metrics,
which are coarse, rely on manual design from experts, and fail to reward
intermediate emergent behaviors. We propose AutoLibra, a framework for agent
evaluation, that transforms open-ended human feedback, e.g., "If you find that
the button is disabled, don't click it again", or "This agent has too much
autonomy to decide what to do on its own", into metrics for evaluating
fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by
grounding feedback to an agent's behavior, clustering similar positive and
negative behaviors, and creating concrete metrics with clear definitions and
concrete examples, which can be used for prompting LLM-as-a-Judge as
evaluators. We further propose two meta-metrics to evaluate the alignment of a
set of (induced) metrics with open feedback: "coverage" and "redundancy".
Through optimizing these meta-metrics, we experimentally demonstrate
AutoLibra's ability to induce more concrete agent evaluation metrics than the
ones proposed in previous agent evaluation benchmarks and discover new metrics
to analyze agents. We also present two applications of AutoLibra in agent
improvement: First, we show that AutoLibra-induced metrics serve as better
prompt-engineering targets than the task success rate on a wide range of text
game tasks, improving agent performance over baseline by a mean of 20%. Second,
we show that AutoLibra can iteratively select high-quality fine-tuning data for
web navigation agents. Our results suggest that AutoLibra is a powerful
task-agnostic tool for evaluating and improving language agents.

</details>


### [336] [Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review](https://arxiv.org/pdf/2505.02828)
*Sonal Allana, Mohan Kankanhalli, Rozita Dara*

Main category: cs.AI

TL;DR: A scoping review explores the conflict between privacy and explainability in XAI, identifying risks, methods, and characteristics of privacy-preserving explanations.


<details>
  <summary>Details</summary>
Motivation: Addressing privacy concerns in XAI to ensure transparency without compromising user data.

Method: Conducted a scoping review of 57 articles from 1,943 studies (2019-2024) to analyze privacy risks and preservation methods in XAI.

Result: Categorized privacy risks, identified preservation methods, and proposed characteristics for privacy-preserving explanations.

Conclusion: Highlights challenges in balancing privacy with other AI goals and provides recommendations for privacy-compliant XAI.

Abstract: Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.

</details>


### [337] [LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery](https://arxiv.org/pdf/2505.02829)
*Jerome Quenum, Wen-Han Hsieh, Tsung-Han Wu, Ritwik Gupta, Trevor Darrell, David M. Chan*

Main category: cs.AI

TL;DR: LISAt is a vision-language model for complex remote-sensing tasks, outperforming existing models in description and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with complex remote-sensing imagery and implicit queries.

Method: LISAt is trained on curated datasets GRES (27,615 annotations) and PreGRES (1M QA pairs).

Result: LISAt outperforms RS-GPT4V by 10.04% (BLEU-4) and open-domain models by 143.36% (gIoU).

Conclusion: LISAt advances reasoning segmentation for remote-sensing, with datasets and code publicly available.

Abstract: Segmentation models can recognize a pre-defined set of objects in images.
However, models that can reason over complex user queries that implicitly refer
to multiple objects of interest are still in their infancy. Recent advances in
reasoning segmentation--generating segmentation masks from complex, implicit
query text--demonstrate that vision-language models can operate across an open
domain and produce reasonable outputs. However, our experiments show that such
models struggle with complex remote-sensing imagery. In this work, we introduce
LISAt, a vision-language model designed to describe complex remote-sensing
scenes, answer questions about them, and segment objects of interest. We
trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES,
with 27,615 annotations over 9,205 images, and a multimodal pretraining
dataset, PreGRES, containing over 1 million question-answer pairs. LISAt
outperforms existing geospatial foundation models such as RS-GPT4V by over
10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses
state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 %
(gIoU). Our model, datasets, and code are available at
https://lisat-bair.github.io/LISAt/

</details>


### [338] [Minimax Strikes Back](https://arxiv.org/pdf/2012.10700)
*Quentin Cohen-Solal, Tristan Cazenave*

Main category: cs.AI

TL;DR: Athénan, a Minimax-based approach, outperforms AlphaZero's reimplementation (Polygames) in efficiency and speed, with significantly lower training costs.


<details>
  <summary>Details</summary>
Motivation: To explore an alternative to AlphaZero's zero-knowledge learning, using a Minimax-based search algorithm (Descent) and different learning targets.

Method: Athénan employs Descent, a Minimax-based search algorithm, avoids using a policy, and focuses on efficient state data generation for training.

Result: Athénan is much more efficient than Polygames, even when Polygames uses 100x more GPU. Training cost is ~296x lower, and speed is 7-30x faster.

Conclusion: Athénan's Minimax-based approach and efficient training make it a superior alternative to AlphaZero's methods for certain games.

Abstract: Deep Reinforcement Learning reaches a superhuman level of play in many
complete information games. The state of the art algorithm for learning with
zero knowledge is AlphaZero. We take another approach, Ath\'enan, which uses a
different, Minimax-based, search algorithm called Descent, as well as different
learning targets and that does not use a policy. We show that for multiple
games it is much more efficient than the reimplementation of AlphaZero:
Polygames. It is even competitive with Polygames when Polygames uses 100 times
more GPU (at least for some games). One of the keys to the superior performance
is that the cost of generating state data for training is approximately 296
times lower with Ath\'enan. With the same reasonable ressources, Ath\'enan
without reinforcement heuristic is at least 7 times faster than Polygames and
much more than 30 times faster with reinforcement heuristic.

</details>


### [339] [GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning](https://arxiv.org/pdf/2402.16631)
*Hang Zou, Qiyang Zhao, Samson Lasaulce, Lina Bariah, Mehdi Bennis, Merouane Debbah*

Main category: cs.AI

TL;DR: The paper proposes GenAINet, a framework integrating Generative AI (GenAI) with 6G networks to enable Collective Intelligence (CI) and AGI, addressing current network limitations by facilitating semantic-native communication and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current wireless networks are not designed to leverage GenAI's potential. The paper aims to bridge this gap by enabling GenAI agents to communicate knowledge for arbitrary tasks.

Method: The GenAINet framework includes architectures for single GenAI agents and network integration, focusing on semantic-native communication, knowledge modeling, and collaborative reasoning.

Result: Case studies show improved query accuracy and reduced communication costs, and demonstrate independent task completion through collaborative reasoning.

Conclusion: GenAINet successfully integrates GenAI with 6G networks, but challenges remain in applying Large Language Models (LLMs) to 6G.

Abstract: Generative Artificial Intelligence (GenAI) and communication networks are
expected to have groundbreaking synergies for 6G. Connecting GenAI agents via a
wireless network can potentially unleash the power of Collective Intelligence
(CI) and pave the way for Artificial General Intelligence (AGI). However,
current wireless networks are designed as a "data pipe" and are not suited to
accommodate and leverage the power of GenAI. In this paper, we propose the
GenAINet framework in which distributed GenAI agents communicate knowledge
(facts, experiences, and methods) to accomplish arbitrary tasks. We first
propose an architecture for a single GenAI agent and then provide a network
architecture integrating GenAI capabilities to manage both network protocols
and applications. Building on this, we investigate effective communication and
reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI
agents extract semantics from heterogeneous raw data, build and maintain a
knowledge model representing the semantic relationships among pieces of
knowledge, which is retrieved by GenAI models for planning and reasoning. Under
this paradigm, different levels of collaboration can be achieved flexibly
depending on the complexity of targeted tasks. Furthermore, we conduct two case
studies in which, through wireless device queries, we demonstrate that
extracting, compressing and transferring common knowledge can improve query
accuracy while reducing communication costs; and in the wireless power control
problem, we show that distributed agents can complete general tasks
independently through collaborative reasoning without predefined communication
protocols. Finally, we discuss challenges and future research directions in
applying Large Language Models (LLMs) in 6G networks.

</details>


### [340] [AI-Driven Healthcare: A Review on Ensuring Fairness and Mitigating Bias](https://arxiv.org/pdf/2407.19655)
*Sribala Vidyadhari Chinta, Zichong Wang, Avash Palikhe, Xingyu Zhang, Ayesha Kashif, Monique Antoinette Smith, Jun Liu, Wenbin Zhang*

Main category: cs.AI

TL;DR: AI in healthcare improves diagnostics and treatment but faces bias and fairness challenges. This review explores mitigation strategies like diverse datasets and fairness-aware algorithms.


<details>
  <summary>Details</summary>
Motivation: To address ethical and fairness issues in AI-driven healthcare, ensuring equitable delivery across demographics.

Method: Review of AI applications in healthcare, focusing on bias challenges and mitigation strategies.

Result: Identifies biases in AI healthcare applications and proposes solutions like diverse datasets and regulatory frameworks.

Conclusion: Calls for interdisciplinary research, transparency, and inclusive AI to ensure equitable healthcare.

Abstract: Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing
the efficiency and effectiveness of services across various specialties,
including cardiology, ophthalmology, dermatology, emergency medicine, etc. AI
applications have significantly improved diagnostic accuracy, treatment
personalization, and patient outcome predictions by leveraging technologies
such as machine learning, neural networks, and natural language processing.
However, these advancements also introduce substantial ethical and fairness
challenges, particularly related to biases in data and algorithms. These biases
can lead to disparities in healthcare delivery, affecting diagnostic accuracy
and treatment outcomes across different demographic groups. This review paper
examines the integration of AI in healthcare, highlighting critical challenges
related to bias and exploring strategies for mitigation. We emphasize the
necessity of diverse datasets, fairness-aware algorithms, and regulatory
frameworks to ensure equitable healthcare delivery. The paper concludes with
recommendations for future research, advocating for interdisciplinary
approaches, transparency in AI decision-making, and the development of
innovative and inclusive AI applications.

</details>


### [341] [The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats](https://arxiv.org/pdf/2503.02650)
*William Brach, Kristián Košťál, Michal Ries*

Main category: cs.AI

TL;DR: LLMs, especially GPT-4o, can reliably convert unstructured recipe text into structured Cooklang format, showing potential for broader applications.


<details>
  <summary>Details</summary>
Motivation: The challenge of managing unstructured text data and the unexplored potential of LLMs to standardize it.

Method: Systematic evaluation of four LLMs (GPT-4o, GPT-4o-mini, Llama3.1:70b, Llama3.1:8b) using traditional and specialized metrics.

Result: GPT-4o with few-shot prompting achieves high performance (ROUGE-L: 0.9722, WER: 0.0730), and smaller models show potential for optimization.

Conclusion: LLMs can revolutionize structured data generation across domains, transforming how organizations process unstructured information.

Abstract: The exponential growth of unstructured text data presents a fundamental
challenge in modern data management and information retrieval. While Large
Language Models (LLMs) have shown remarkable capabilities in natural language
processing, their potential to transform unstructured text into standardized,
structured formats remains largely unexplored - a capability that could
revolutionize data processing workflows across industries. This study breaks
new ground by systematically evaluating LLMs' ability to convert unstructured
recipe text into the structured Cooklang format. Through comprehensive testing
of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an
innovative evaluation approach is introduced that combines traditional metrics
(WER, ROUGE-L, TER) with specialized metrics for semantic element
identification. Our experiments reveal that GPT-4o with few-shot prompting
achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating
for the first time that LLMs can reliably transform domain-specific
unstructured text into structured formats without extensive training. Although
model performance generally scales with size, we uncover surprising potential
in smaller models like Llama3.1:8b for optimization through targeted
fine-tuning. These findings open new possibilities for automated structured
data generation across various domains, from medical records to technical
documentation, potentially transforming the way organizations process and
utilize unstructured information.

</details>


### [342] [Developing a Foundation of Vector Symbolic Architectures Using Category Theory](https://arxiv.org/pdf/2501.05368)
*Nolan P Shaw, P Michael Furlong, Britt Anderson, Jeff Orchard*

Main category: cs.AI

TL;DR: The paper applies category theory to Vector Symbolic Architectures (VSAs), generalizing vectors to co-presheaves and describing VSA operations via right Kan extensions, validated with examples.


<details>
  <summary>Details</summary>
Motivation: VSAs bridge neural networks and symbolic reasoning but lack category-theoretical analysis, which this paper addresses.

Method: Generalizes vectors to co-presheaves and formalizes VSA operations as right Kan extensions of the external tensor product.

Result: Proves right Kan extensions can be expressed as element-wise operations, validated with examples and suggesting new VSA designs.

Conclusion: The category-theoretical approach formalizes VSAs, offering insights for future implementations.

Abstract: Connectionist approaches to machine learning, \emph{i.e.} neural networks,
are enjoying a considerable vogue right now. However, these methods require
large volumes of data and produce models that are uninterpretable to humans. An
alternative framework that is compatible with neural networks and
gradient-based learning, but explicitly models compositionality, is Vector
Symbolic Architectures (VSAs). VSAs are a family of algebras on
high-dimensional vector representations. They arose in cognitive science from
the need to unify neural processing and the kind of symbolic reasoning that
humans perform. While machine learning methods have benefited from
category-theoretical analyses, VSAs have not yet received similar treatment. In
this paper, we present a first attempt at applying category theory to VSAs.
Specifically, We generalise from vectors to co-presheaves, and describe VSA
operations as the right Kan extensions of the external tensor product. This
formalisation involves a proof that the right Kan extension in such cases can
be expressed as simple, element-wise operations. We validate our formalisation
with worked examples that connect to current VSA implementations, while
suggesting new possible designs for VSAs.

</details>


### [343] [Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems](https://arxiv.org/pdf/2502.07503)
*Ibrahim Alabdulmohsin, Xiaohua Zhai*

Main category: cs.AI

TL;DR: RINS is a recursive inference scaling method that outperforms other variants, improves language and multimodal tasks, and offers no-regret benefits in pretraining.


<details>
  <summary>Details</summary>
Motivation: To enhance inference time scaling in language and multimodal systems by introducing a recursive depth method.

Method: RINS, a recursive inference scaling technique, is compared compute-matched against other variants, with light-weight adapters and stochastic dropout.

Result: RINS improves language modeling, multimodal tasks (e.g., +2% ImageNet accuracy), and scaling laws, with no-regret pretraining benefits.

Conclusion: RINS is a promising component for LLM pretraining, offering performance gains across compute-, parameter-, and inference-matched regimes.

Abstract: Inspired by recent findings on the fractal geometry of language, we introduce
Recursive INference Scaling (RINS) as a complementary, plug-in recipe for
scaling inference time in language and multimodal systems. RINS is a particular
form of recursive depth that significantly outperforms +55 other variants,
including the recent "repeat-all-over" (RAO) strategy in Mobile LLM (Liu et
al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior
works, we carry out our comparisons on a compute-matched regime, and
demonstrate that for a fixed model size and training compute budget, RINS
substantially improves language modeling performance. It also generalizes
beyond pure language tasks, delivering gains in multimodal systems, including a
+2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by
deriving data scaling laws, we show that RINS improves both the asymptotic
performance limits and the scaling exponents. More importantly, with
light-weight (linear) adapters (comprising <1% of model parameters) and
stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled
pretraining improves performance in language modeling even when recursive depth
is not applied at inference time. This corresponds to improving performance on
a training compute-, parameter-, and inference-matched regime, suggesting its
potential as a viable component of LLM pretraining!

</details>


### [344] [A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification](https://arxiv.org/pdf/2502.17289)
*Soumen Sinha, Tanisha Rana, Rahul Roy*

Main category: cs.AI

TL;DR: A novel method for hierarchical plant taxonomy classification using DenseNet121, Multi-Scale Self-Attention, and cascaded classifiers, achieving high accuracy for known and unknown species.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with hierarchical classification and identifying unknown species, limiting comprehensive plant taxonomy.

Method: Integrates DenseNet121, Multi-Scale Self-Attention (MSSA), and cascaded classifiers to capture local and global features for precise hierarchical classification.

Result: Achieved 83.36%, 78.30%, 60.34%, and 43.32% accuracy for unknown species at phylum, class, order, and family levels, respectively. Model size is four times smaller than state-of-the-art.

Conclusion: The method effectively addresses hierarchical classification and unknown species identification, offering a deployable solution for real-world applications.

Abstract: In this article, we propose a novel approach for plant hierarchical taxonomy
classification by posing the problem as an open class problem. It is observed
that existing methods for medicinal plant classification often fail to perform
hierarchical classification and accurately identifying unknown species,
limiting their effectiveness in comprehensive plant taxonomy classification.
Thus we address the problem of unknown species classification by assigning it
best hierarchical labels. We propose a novel method, which integrates
DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for
hierarchical classification. The approach systematically categorizes medicinal
plants at multiple taxonomic levels, from phylum to species, ensuring detailed
and precise classification. Using multi scale space attention, the model
captures both local and global contextual information from the images,
improving the distinction between similar species and the identification of new
ones. It uses attention scores to focus on important features across multiple
scales. The proposed method provides a solution for hierarchical
classification, showcasing superior performance in identifying both known and
unknown species. The model was tested on two state-of-art datasets with and
without background artifacts and so that it can be deployed to tackle real word
application. We used unknown species for testing our model. For unknown species
the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for
predicting correct phylum, class, order and family respectively. Our proposed
model size is almost four times less than the existing state of the art methods
making it easily deploy able in real world application.

</details>


### [345] [Activation Space Interventions Can Be Transferred Between Large Language Models](https://arxiv.org/pdf/2503.04429)
*Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah*

Main category: cs.AI

TL;DR: The paper explores transferring safety interventions between AI models via shared activation spaces, demonstrating effectiveness in backdoor removal and harmful prompt refusal, and introduces a new task for testing model capabilities.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in practical applications of representation universality by leveraging shared activation spaces for safety interventions.

Method: Uses learned mappings of shared activation spaces to transfer steering vectors between models, tested on backdoor removal and harmful prompt refusal, and introduces a corrupted capabilities task.

Result: Successful transfer of interventions across models (Llama, Qwen, Gemma), enabling smaller models to align larger ones, and reliable use of autoencoder mappings as safety switches.

Conclusion: The approach effectively transfers safety interventions and introduces a novel task, demonstrating practical utility in AI alignment.

Abstract: The study of representation universality in AI models reveals growing
convergence across domains, modalities, and architectures. However, the
practical applications of representation universality remain largely
unexplored. We bridge this gap by demonstrating that safety interventions can
be transferred between models through learned mappings of their shared
activation spaces. We demonstrate this approach on two well-established AI
safety tasks: backdoor removal and refusal of harmful prompts, showing
successful transfer of steering vectors that alter the models' outputs in a
predictable way. Additionally, we propose a new task, \textit{corrupted
capabilities}, where models are fine-tuned to embed knowledge tied to a
backdoor. This tests their ability to separate useful skills from backdoors,
reflecting real-world challenges. Extensive experiments across Llama, Qwen and
Gemma model families show that our method enables using smaller models to
efficiently align larger ones. Furthermore, we demonstrate that autoencoder
mappings between base and fine-tuned models can serve as reliable ``lightweight
safety switches", allowing dynamic toggling between model behaviors.

</details>


### [346] [Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science](https://arxiv.org/pdf/2503.10822)
*Johannes Buchner*

Main category: cs.AI

TL;DR: The paper explores using Reinforcement Learning (RL) for Life Cycle Assessment in a circular economy, drawing parallels from RL's success in computer chess and proposing new applications for sustainability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between RL advancements (e.g., AlphaZero) and sustainability challenges, leveraging methods like rotated bitboards for circular economy optimization.

Method: Describes rotated bitboards for board representation in chess, RL progress in computer chess (e.g., AlphaZero), and proposes adapting these methods for sustainability.

Result: Highlights RL's potential in optimizing material loops and Life Cycle Assessment for absolute sustainability, with examples from chess and other domains.

Conclusion: Advocates for 'progressive computer science' to address sustainability challenges, suggesting RL as a transformative tool for circular economy goals.

Abstract: The aim of this paper is to discuss the potential of using methods from
Reinforcement Learning for Life Cycle Assessment in a circular economy, and to
present some new ideas in this direction. To give some context, we explain how
Reinforcement Learning was successfully applied in computer chess (and beyond).
As computer chess was historically called the "drosophila of AI", we start by
describing a method for the board representation called 'rotated bitboards'
that can potentially also be applied in the context of sustainability. In the
first part of this paper, the concepts of the bitboard-representation and the
advantages of (rotated) bitboards in move generation are explained. In order to
illustrate those ideas practice, the concrete implementation of the
move-generator in FUSc# (a chess engine developed at FU Berlin in C# some years
ago) is described. In addition, rotated binary neural networks are discussed
briefly.
  The second part deals with reinforcement learning in computer chess (and
beyond). We exemplify the progress that has been made in this field in the last
15-20 years by comparing the "state of the art" from 2002-2008, when FUSc# was
developed, with the ground-breaking innovations connected to "AlphaZero". We
review some application of the ideas developed in AlphaZero in other domains,
e.g. the "other Alphas" like AlphaFold, AlphaTensor, AlphaGeometry and
AlphaProof. In the final part of the paper, we discuss the computer-science
related challenges that changing the economic paradigm towards (absolute)
sustainability poses and in how far what we call 'progressive computer science'
needs to contribute. Concrete challenges include the closing of material loops
in a circular economy with Life Cycle Assessment in order to optimize for
(absolute) sustainability, and we present some new ideas in this direction.

</details>


### [347] [Pseudo-Boolean Proof Logging for Optimal Classical Planning](https://arxiv.org/pdf/2504.18443)
*Simon Dold, Malte Helmert, Jakob Nordström, Gabriele Röger, Tanja Schindler*

Main category: cs.AI

TL;DR: The paper introduces lower-bound certificates for planning tasks to verify unsolvability or plan optimality, using a pseudo-Boolean constraint framework. It demonstrates the approach with A* and heuristics like pattern databases.


<details>
  <summary>Details</summary>
Motivation: To provide verifiable proofs of task unsolvability or plan optimality, independent of the planning algorithm used.

Method: A general framework for generating lower-bound certificates via pseudo-Boolean constraints, applied to A* with pattern database and h^max heuristics.

Result: Shows how to modify A* to produce optimality proofs with minimal overhead, using specific heuristics.

Conclusion: The proof logging approach is versatile, working with any heuristic expressible via pseudo-Boolean constraints.

Abstract: We introduce lower-bound certificates for classical planning tasks, which can
be used to prove the unsolvability of a task or the optimality of a plan in a
way that can be verified by an independent third party. We describe a general
framework for generating lower-bound certificates based on pseudo-Boolean
constraints, which is agnostic to the planning algorithm used.
  As a case study, we show how to modify the $A^{*}$ algorithm to produce
proofs of optimality with modest overhead, using pattern database heuristics
and $h^\textit{max}$ as concrete examples. The same proof logging approach
works for any heuristic whose inferences can be efficiently expressed as
reasoning over pseudo-Boolean constraints.

</details>


### [348] [Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots](https://arxiv.org/pdf/2504.18794)
*Brendon Johnson, Alfredo Weitzenfeld*

Main category: cs.AI

TL;DR: HRL outperforms traditional RL in complex navigation tasks by leveraging sub-goals and termination functions.


<details>
  <summary>Details</summary>
Motivation: To compare HRL with standard RL in sparse reward scenarios, focusing on sub-goal creation and termination functions.

Method: Experiments comparing PPO and HRL, testing sub-goal creation methods (manual vs. automatic) and termination frequency.

Result: HRL shows advantages in performance due to its hierarchical structure and sub-goal mechanisms.

Conclusion: HRL is more effective than traditional RL in hierarchical tasks, with sub-goal creation and termination being key factors.

Abstract: Hierarchical reinforcement learning (HRL) is hypothesized to be able to take
advantage of the inherent hierarchy in robot learning tasks with sparse reward
schemes, in contrast to more traditional reinforcement learning algorithms. In
this research, hierarchical reinforcement learning is evaluated and contrasted
with standard reinforcement learning in complex navigation tasks. We evaluate
unique characteristics of HRL, including their ability to create sub-goals and
the termination function. We constructed experiments to test the differences
between PPO and HRL, different ways of creating sub-goals, manual vs automatic
sub-goal creation, and the effects of the frequency of termination on
performance. These experiments highlight the advantages of HRL and how it
achieves these advantages.

</details>


### [349] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/pdf/2504.20898)
*Hasan Md Tusfiqur Alam, Devansh Srivastav, Abdulrahman Mohamed Selim, Md Abdul Kadir, Md Moktadirul Hoque Shuvo, Daniel Sonntag*

Main category: cs.AI

TL;DR: The paper introduces a framework combining Concept Bottleneck Models (CBMs) and Multi-Agent Retrieval-Augmented Generation (RAG) for automated radiology report generation, enhancing interpretability and reliability.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI interpretability and reliability in radiology workflows, aiming to improve clinical adoption.

Method: Uses CBMs for transparent disease classification and a RAG system for evidence-based, context-rich report generation.

Result: The framework delivers interpretable predictions, reduces hallucinations, and generates high-quality, tailored reports.

Conclusion: The approach improves diagnostic consistency and provides actionable insights for radiologists.

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [350] [Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network](https://arxiv.org/pdf/2505.01880)
*Junyan Wu, Wenbo Xu, Wei Lu, Xiangyang Luo, Rui Yang, Shize Guo*

Main category: cs.SD

TL;DR: The paper proposes LOCO, a progressive audio-language co-learning network for audio temporal forgery localization (ATFL) under weak supervision, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing ATFL methods require costly fine-grained annotations, which are impractical in real-world scenarios. LOCO addresses this by leveraging weak supervision and co-learning.

Method: LOCO uses an audio-language co-learning module to align semantics and forgery-aware prompts, a forgery localization module for proposals, and progressive refinement with pseudo labels and contrastive learning.

Result: LOCO achieves state-of-the-art performance on three public benchmarks.

Conclusion: The proposed LOCO effectively localizes audio forgeries under weak supervision, outperforming existing methods.

Abstract: Audio temporal forgery localization (ATFL) aims to find the precise forgery
regions of the partial spoof audio that is purposefully modified. Existing ATFL
methods rely on training efficient networks using fine-grained annotations,
which are obtained costly and challenging in real-world scenarios. To meet this
challenge, in this paper, we propose a progressive audio-language co-learning
network (LOCO) that adopts co-learning and self-supervision manners to prompt
localization performance under weak supervision scenarios. Specifically, an
audio-language co-learning module is first designed to capture forgery
consensus features by aligning semantics from temporal and global perspectives.
In this module, forgery-aware prompts are constructed by using utterance-level
annotations together with learnable prompts, which can incorporate semantic
priors into temporal content features dynamically. In addition, a forgery
localization module is applied to produce forgery proposals based on fused
forgery-class activation sequences. Finally, a progressive refinement strategy
is introduced to generate pseudo frame-level labels and leverage supervised
semantic contrastive learning to amplify the semantic distinction between real
and fake content, thereby continuously optimizing forgery-aware features.
Extensive experiments show that the proposed LOCO achieves SOTA performance on
three public benchmarks.

</details>


### [351] [MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input](https://arxiv.org/pdf/2505.02180)
*Hirotaka Hiraki, Jun Rekimoto*

Main category: cs.SD

TL;DR: MaskClip uses a piezoelectric sensor on a mask to capture speech vibrations, outperforming microphones in noisy settings with a 6.1% error rate.


<details>
  <summary>Details</summary>
Motivation: Masks impair speech communication, especially in noisy environments, and existing solutions are resource-heavy or unhygienic.

Method: A piezoelectric sensor on a stainless steel clip (MaskClip) captures mask vibrations to isolate speech from noise.

Result: Achieved a 6.1% Character Error Rate in noise and high user satisfaction in tests with 102 participants.

Conclusion: MaskClip is effective for clear voice communication in medical, cleanroom, and industrial settings.

Abstract: Masks are essential in medical settings and during infectious outbreaks but
significantly impair speech communication, especially in environments with
background noise. Existing solutions often require substantial computational
resources or compromise hygiene and comfort. We propose a novel sensing
approach that captures only the wearer's voice by detecting mask surface
vibrations using a piezoelectric sensor. Our developed device, MaskClip,
employs a stainless steel clip with an optimally positioned piezoelectric
sensor to selectively capture speech vibrations while inherently filtering out
ambient noise. Evaluation experiments demonstrated superior performance with a
low Character Error Rate of 6.1\% in noisy environments compared to
conventional microphones. Subjective evaluations by 102 participants also
showed high satisfaction scores. This approach shows promise for applications
in settings where clear voice communication must be maintained while wearing
protective equipment, such as medical facilities, cleanrooms, and industrial
environments.

</details>


### [352] [Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions](https://arxiv.org/pdf/2307.15344)
*Yifei Xin, Yuexian Zou*

Main category: cs.SD

TL;DR: The paper introduces a hierarchical cross-modal interaction (HCI) method for audio-text retrieval (ATR) and an auxiliary captions (AC) framework, improving performance by leveraging fine-grained relationships and data augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing ATR methods overlook fine-grained cross-modal relationships (e.g., segments-phrases, frames-words) and lack comprehensive multi-modal semantic comparison.

Method: Proposes HCI for clip-sentence, segment-phrase, and frame-word relationships, and an AC framework using pretrained captioner for feature interaction and data augmentation.

Result: HCI significantly improves ATR performance; AC framework shows stable gains on multiple datasets.

Conclusion: The HCI and AC methods enhance ATR by addressing fine-grained relationships and leveraging auxiliary captions, achieving superior results.

Abstract: Most existing audio-text retrieval (ATR) methods focus on constructing
contrastive pairs between whole audio clips and complete caption sentences,
while ignoring fine-grained cross-modal relationships, e.g., short segments and
phrases or frames and words. In this paper, we introduce a hierarchical
cross-modal interaction (HCI) method for ATR by simultaneously exploring
clip-sentence, segment-phrase, and frame-word relationships, achieving a
comprehensive multi-modal semantic comparison. Besides, we also present a novel
ATR framework that leverages auxiliary captions (AC) generated by a pretrained
captioner to perform feature interaction between audio and generated captions,
which yields enhanced audio representations and is complementary to the
original ATR matching branch. The audio and generated captions can also form
new audio-text pairs as data augmentation for training. Experiments show that
our HCI significantly improves the ATR performance. Moreover, our AC framework
also shows stable performance gains on multiple datasets.

</details>


### [353] [Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation](https://arxiv.org/pdf/2409.09256)
*Yifei Xin, Zhihong Zhu, Xuxin Cheng, Xusheng Yang, Yuexian Zou*

Main category: cs.SD

TL;DR: A novel ATR framework using two-stream Transformers and a Hierarchical Alignment module improves multi-level audio-text alignment, while a Disentangled Cross-modal Representation approach captures fine-grained correlations.


<details>
  <summary>Details</summary>
Motivation: Existing ATR methods rely on single-level interactions, limiting alignment and missing fine-grained details.

Method: Proposes THA for multi-level alignment and DCR for disentangling features into latent factors, with a CA module for adaptive aggregation.

Result: THA enhances ATR performance, and DCR further improves results by capturing fine-grained semantics.

Conclusion: The framework achieves better alignment and performance by addressing multi-level and fine-grained audio-text correlations.

Abstract: Most existing audio-text retrieval (ATR) approaches typically rely on a
single-level interaction to associate audio and text, limiting their ability to
align different modalities and leading to suboptimal matches. In this work, we
present a novel ATR framework that leverages two-stream Transformers in
conjunction with a Hierarchical Alignment (THA) module to identify multi-level
correspondences of different Transformer blocks between audio and text.
Moreover, current ATR methods mainly focus on learning a global-level
representation, missing out on intricate details to capture audio occurrences
that correspond to textual semantics. To bridge this gap, we introduce a
Disentangled Cross-modal Representation (DCR) approach that disentangles
high-dimensional features into compact latent factors to grasp fine-grained
audio-text semantic correlations. Additionally, we develop a confidence-aware
(CA) module to estimate the confidence of each latent factor pair and
adaptively aggregate cross-modal latent factors to achieve local semantic
alignment. Experiments show that our THA effectively boosts ATR performance,
with the DCR approach further contributing to consistent performance gains.

</details>


### [354] [FolAI: Synchronized Foley Sound Generation with Semantic and Temporal Alignment](https://arxiv.org/pdf/2412.15023)
*Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello*

Main category: cs.SD

TL;DR: FolAI is a two-stage generative framework for automating Foley sound design, decoupling temporal structure extraction and semantically guided sound synthesis for precise control and scalability.


<details>
  <summary>Details</summary>
Motivation: Manual Foley sound design is time-consuming and lacks automation tools that preserve creative intent, while existing vision-to-audio methods struggle with temporal coherence and semantic control.

Method: FolAI uses a two-stage approach: (1) extracting a smooth control signal from video for temporal alignment, and (2) a diffusion-based model generating sound effects conditioned on this signal and user-provided semantic embeddings.

Result: The model produces temporally aligned, semantically consistent, and perceptually realistic audio for diverse visual contexts, like footsteps and action-specific sounds.

Conclusion: FolAI offers a controllable, modular solution for scalable, high-quality Foley sound synthesis in professional and interactive settings.

Abstract: Traditional sound design workflows rely on manual alignment of audio events
to visual cues, as in Foley sound design, where everyday actions like footsteps
or object interactions are recreated to match the on-screen motion. This
process is time-consuming, difficult to scale, and lacks automation tools that
preserve creative intent. Despite recent advances in vision-to-audio
generation, producing temporally coherent and semantically controllable sound
effects from video remains a major challenge. To address these limitations, we
introduce FolAI, a two-stage generative framework that decouples the when and
the what of sound synthesis, i.e., the temporal structure extraction and the
semantically guided generation, respectively. In the first stage, we estimate a
smooth control signal from the video that captures the motion intensity and
rhythmic structure over time, serving as a temporal scaffold for the audio. In
the second stage, a diffusion-based generative model produces sound effects
conditioned both on this temporal envelope and on high-level semantic
embeddings, provided by the user, that define the desired auditory content
(e.g., material or action type). This modular design enables precise control
over both timing and timbre, streamlining repetitive tasks while preserving
creative flexibility in professional Foley workflows. Results on diverse visual
contexts, such as footstep generation and action-specific sonorization,
demonstrate that our model reliably produces audio that is temporally aligned
with visual motion, semantically consistent with user intent, and perceptually
realistic. These findings highlight the potential of FolAI as a controllable
and modular solution for scalable, high-quality Foley sound synthesis in
professional and interactive settings. Supplementary materials are accessible
on our dedicated demo page at https://ispamm.github.io/FolAI.

</details>


### [355] [VANPY: Voice Analysis Framework](https://arxiv.org/pdf/2502.17579)
*Gregory Koushnir, Michael Fire, Galit Fuhrmann Alpert, Dima Kagan*

Main category: cs.SD

TL;DR: The VANPY framework is an open-source tool for automated voice analysis, offering pre-processing, feature extraction, and classification, with applications in speaker characterization.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of comprehensive tools for automated voice analysis in modern digital communications.

Method: Developed the VANPY framework with extensible design, integrating over fifteen voice analysis components, including four in-house models for gender, emotion, age, and height classification.

Result: Demonstrated robust performance across datasets and successfully analyzed character voices from "Pulp Fiction," extracting multiple speaker characteristics.

Conclusion: VANPY is a versatile and extensible framework for voice analysis, though it does not surpass state-of-the-art performance.

Abstract: Voice data is increasingly being used in modern digital communications, yet
there is still a lack of comprehensive tools for automated voice analysis and
characterization. To this end, we developed the VANPY (Voice Analysis in
Python) framework for automated pre-processing, feature extraction, and
classification of voice data. The VANPY is an open-source end-to-end
comprehensive framework that was developed for the purpose of speaker
characterization from voice data. The framework is designed with extensibility
in mind, allowing for easy integration of new components and adaptation to
various voice analysis applications. It currently incorporates over fifteen
voice analysis components - including music/speech separation, voice activity
detection, speaker embedding, vocal feature extraction, and various
classification models.
  Four of the VANPY's components were developed in-house and integrated into
the framework to extend its speaker characterization capabilities: gender
classification, emotion classification, age regression, and height regression.
The models demonstrate robust performance across various datasets, although not
surpassing state-of-the-art performance.
  As a proof of concept, we demonstrate the framework's ability to extract
speaker characteristics on a use-case challenge of analyzing character voices
from the movie "Pulp Fiction." The results illustrate the framework's
capability to extract multiple speaker characteristics, including gender, age,
height, emotion type, and emotion intensity measured across three dimensions:
arousal, dominance, and valence.

</details>


### [356] [P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation](https://arxiv.org/pdf/2504.05197)
*Yong Ren, Jiangyan Yi, Tao Wang, Jianhua Tao, Zheng Lian, Zhengqi Wen, Chenxing Li, Ruibo Fu, Ye Bai, Xiaohui Zhang*

Main category: cs.SD

TL;DR: The paper proposes P2Mark, a plug-and-play parameter-level watermarking method for neural speech generation (NSG) to protect model copyrights in open-source scenarios.


<details>
  <summary>Details</summary>
Motivation: Current audio watermarking methods are unsuitable for open-source scenarios where source codes and model weights are released, increasing misuse risks.

Method: Embed watermarks into model weights via a lightweight watermark adapter during training, using gradient orthogonal projection optimization for quality and accuracy.

Result: P2Mark matches state-of-the-art methods in watermark extraction accuracy, imperceptibility, and robustness for vocoder and codec decoders.

Conclusion: P2Mark provides a secure and flexible solution for protecting NSG models in open-source environments.

Abstract: Neural speech generation (NSG) has rapidly advanced as a key component of
artificial intelligence-generated content, enabling the generation of
high-quality, highly realistic speech for diverse applications. This
development increases the risk of technique misuse and threatens social
security. Audio watermarking can embed imperceptible marks into generated
audio, providing a promising approach for secure NSG usage. However, current
audio watermarking methods are mainly applied at the audio-level or
feature-level, which are not suitable for open-sourced scenarios where source
codes and model weights are released. To address this limitation, we propose a
Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG.
Specifically, we embed watermarks into the released model weights, offering a
reliable solution for proactively tracing and protecting model copyrights in
open-source scenarios. During training, we introduce a lightweight watermark
adapter into the pre-trained model, allowing watermark information to be merged
into the model via this adapter. This design ensures both the flexibility to
modify the watermark before model release and the security of embedding the
watermark within model parameters after model release. Meanwhile, we propose a
gradient orthogonal projection optimization strategy to ensure the quality of
the generated audio and the accuracy of watermark preservation. Experimental
results on two mainstream waveform decoders in NSG (i.e., vocoder and codec)
demonstrate that P2Mark achieves comparable performance to state-of-the-art
audio watermarking methods that are not applicable to open-source white-box
protection scenarios, in terms of watermark extraction accuracy, watermark
imperceptibility, and robustness.

</details>


### [357] [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/pdf/2504.12279)
*Mikhail Osipov*

Main category: cs.SD

TL;DR: A geometry-driven method normalizes dysarthric speech by modeling distortions as Lie group transformations, using a neural network trained on synthetic data. It achieves significant WER improvements on pathological speech without degrading clean speech performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of dysarthric speech normalization by leveraging geometric transformations for robust ASR performance.

Method: Model time, frequency, and amplitude distortions as Lie group transformations, train a neural network on synthetic data to infer and invert these deformations, and use an SSB potential to discover non-trivial field configurations.

Result: Up to 17% WER reduction on dysarthric speech (TORGO) and 16% drop in WER variance, with no degradation on clean speech (CommonVoice). Character and phoneme error rates also improve.

Conclusion: Geometrically structured warping provides consistent, zero-shot robustness for dysarthric ASR, confirming its linguistic relevance.

Abstract: We present a geometry-driven method for normalizing dysarthric speech by
modeling time, frequency, and amplitude distortions as smooth, local Lie group
transformations of spectrograms. Scalar fields generate these deformations via
exponential maps, and a neural network is trained - using only synthetically
warped healthy speech - to infer the fields and apply an approximate inverse at
test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that
encourages the model to discover non-trivial field configurations. On real
pathological speech, the system delivers consistent gains: up to 17
percentage-point WER reduction on challenging TORGO utterances and a 16 percent
drop in WER variance, with no degradation on clean CommonVoice data. Character
and phoneme error rates improve in parallel, confirming linguistic relevance.
Our results demonstrate that geometrically structured warping provides
consistent, zero-shot robustness gains for dysarthric ASR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [358] [Perturbation Analysis of Singular Values in Concatenated Matrices](https://arxiv.org/pdf/2505.01427)
*Maksym Shamrai*

Main category: cs.LG

TL;DR: The paper analyzes how the singular value spectrum of a concatenated matrix relates to its individual components, using a perturbation framework to establish stability bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between the singular value spectra of concatenated matrices and their components, extending classical results like Weyl's inequality.

Method: Develops a perturbation framework to analyze singular value stability under small perturbations in submatrices.

Result: Shows that if concatenated matrices are close in norm, dominant singular values remain stable, enabling accuracy-compression trade-offs.

Conclusion: Provides theoretical insights for improved matrix clustering and compression, with applications in linear algebra, signal processing, and modeling.

Abstract: Concatenating matrices is a common technique for uncovering shared structures
in data through singular value decomposition (SVD) and low-rank approximations.
However, a fundamental question arises: how does the singular value spectrum of
the concatenated matrix relate to the spectra of its individual components? In
this work, we develop a perturbation framework that extends classical results
such as Weyl's inequality to concatenated matrices. We establish analytical
bounds that quantify the stability of singular values under small perturbations
in the submatrices. Our results show that if the matrices being concatenated
are close in norm, the dominant singular values of the concatenated matrix
remain stable, enabling controlled trade-offs between accuracy and compression.
These insights provide a theoretical foundation for improved matrix clustering
and compression strategies, with applications in numerical linear algebra,
signal processing, and data-driven modeling.

</details>


### [359] [Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets](https://arxiv.org/pdf/2505.01437)
*Hassan Wasswa, Timothy Lynar, Hussein Abbass*

Main category: cs.LG

TL;DR: The paper proposes a lightweight IoT-botnet detection model using Variational Auto-encoder (VAE) and cost-sensitive learning to improve detection of minority class attacks, achieving high performance with DNN and BLSTM models.


<details>
  <summary>Details</summary>
Motivation: IoT devices are vulnerable to malicious attacks, especially botnets, necessitating effective detection methods for minority class attack traffic.

Method: Leveraged VAE and cost-sensitive learning to develop models, evaluated on imbalanced datasets using DNN and BLSTM.

Result: Both DNN and BLSTM models achieved high accuracy, precision, recall, and F1-score across traffic classes.

Conclusion: The proposed approach effectively enhances IoT-botnet detection, particularly for minority attack classes.

Abstract: The Internet of Things (IoT) technology has rapidly gained popularity with
applications widespread across a variety of industries. However, IoT devices
have been recently serving as a porous layer for many malicious attacks to both
personal and enterprise information systems with the most famous attacks being
botnet-related attacks. The work in this study leveraged Variational
Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet
effective, models for IoT-botnet detection. The aim is to enhance the detection
of minority class attack traffic instances which are often missed by machine
learning models. The proposed approach is evaluated on a multi-class problem
setting for the detection of traffic categories on highly imbalanced datasets.
The performance of two deep learning models including the standard feed forward
deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and
both recorded commendable results in terms of accuracy, precision, recall and
F1-score for all traffic classes.

</details>


### [360] [OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models](https://arxiv.org/pdf/2505.01448)
*Shengkai Chen, Yifang Yin, Jinming Cao, Shili Xiang, Zhenguang Liu, Roger Zimmermann*

Main category: cs.LG

TL;DR: OpenAVS introduces a training-free, language-based approach for open-vocabulary audio-visual segmentation, leveraging foundation models and outperforming existing methods by ~9.4% mIoU and 10.9% F-score.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack generalization to unseen scenarios due to closed-set limitations and direct audio-visual alignment. OpenAVS aims to address this by using text as a proxy for better alignment and flexibility.

Method: OpenAVS uses 1) audio-to-text prompt generation, 2) LLM-guided prompt translation, and 3) text-to-visual segmentation. It also includes OpenAVS-ST for integrating with supervised models via pseudo-label self-training.

Result: OpenAVS outperforms unsupervised, zero-shot, and few-shot AVS methods by ~9.4% mIoU and 10.9% F-score on benchmark datasets.

Conclusion: OpenAVS provides a flexible, effective solution for open-vocabulary AVS, leveraging foundation models and enabling performance gains in challenging scenarios.

Abstract: Audio-visual segmentation aims to separate sounding objects from videos by
predicting pixel-level masks based on audio signals. Existing methods primarily
concentrate on closed-set scenarios and direct audio-visual alignment and
fusion, which limits their capability to generalize to new, unseen situations.
In this paper, we propose OpenAVS, a novel training-free language-based
approach that, for the first time, effectively aligns audio and visual
modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation
(AVS). Equipped with multimedia foundation models, OpenAVS directly infers
masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt
translation, and 3) text-to-visual sounding object segmentation. The objective
of OpenAVS is to establish a simple yet flexible architecture that relies on
the most appropriate foundation models by fully leveraging their capabilities
to enable more effective knowledge transfer to the downstream AVS task.
Moreover, we present a model-agnostic framework OpenAVS-ST that enables the
integration of OpenAVS with any advanced supervised AVS model via pseudo-label
based self-training. This approach enhances performance by effectively
utilizing large-scale unlabeled data when available. Comprehensive experiments
on three benchmark datasets demonstrate the superior performance of OpenAVS. It
surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a
significant margin, achieving absolute performance gains of approximately 9.4%
and 10.9% in mIoU and F-score, respectively, in challenging scenarios.

</details>


### [361] [Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials](https://arxiv.org/pdf/2505.01438)
*Tengfei Xing, Xiaodan Ren, Jie Li*

Main category: cs.LG

TL;DR: A framework for global stress generation and spatiotemporal super-resolution in two-phase random materials (TRMs) under dynamic loading, using STS-diffusion and ST-SRPINN methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in high-resolution spatiotemporal stress field generation due to limited data resolution, especially in stress concentration regions.

Method: Proposes STS-diffusion (Space-Time U-Net) for stress data generation and ST-SRPINN (physics-informed network) for unsupervised super-resolution.

Result: ST-SRPINN can upscale stress field resolution to arbitrary magnifications using low-resolution training data, leveraging physics-based constraints.

Conclusion: The framework effectively generates high-resolution stress fields and captures stress concentration regions, advancing material stress analysis.

Abstract: Material stress analysis is a critical aspect of material design and
performance optimization. Under dynamic loading, the global stress evolution in
materials exhibits complex spatiotemporal characteristics, especially in
two-phase random materials (TRMs). Such kind of material failure is often
associated with stress concentration, and the phase boundaries are key
locations where stress concentration occurs. In practical engineering
applications, the spatiotemporal resolution of acquired microstructural data
and its dynamic stress evolution is often limited. This poses challenges for
deep learning methods in generating high-resolution spatiotemporal stress
fields, particularly for accurately capturing stress concentration regions. In
this study, we propose a framework for global stress generation and
spatiotemporal super-resolution in TRMs under dynamic loading. First, we
introduce a diffusion model-based approach, named as Spatiotemporal Stress
Diffusion (STS-diffusion), for generating global spatiotemporal stress data.
This framework incorporates Space-Time U-Net (STU-net), and we systematically
investigate the impact of different attention positions on model accuracy.
Next, we develop a physics-informed network for spatiotemporal
super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed
Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning
method. The influence of data-driven and physics-informed loss function weights
on model accuracy is explored in detail. Benefiting from physics-based
constraints, ST-SRPINN requires only low-resolution stress field data during
training and can upscale the spatiotemporal resolution of stress fields to
arbitrary magnifications.

</details>


### [362] [Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving](https://arxiv.org/pdf/2505.01440)
*Alkis Sygkounas, Ioannis Athanasiadis, Andreas Persson, Michael Felsberg, Amy Loutfi*

Main category: cs.LG

TL;DR: iDDQN integrates human insights into RL training, improving performance in autonomous driving by combining human and agent actions.


<details>
  <summary>Details</summary>
Motivation: Enhancing RL with human expertise is vital for high-accuracy, safety-critical applications like autonomous driving.

Method: iDDQN modifies the Q-value update to merge human and agent actions, with an offline framework to evaluate human interventions.

Result: iDDQN outperforms BC, HG-DAgger, DQfD, and vanilla DRL in leveraging human expertise for better performance and adaptability.

Conclusion: iDDQN effectively combines human and machine learning, proving superior in autonomous driving scenarios.

Abstract: Integrating human expertise with machine learning is crucial for applications
demanding high accuracy and safety, such as autonomous driving. This study
introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop
(HITL) approach that enhances Reinforcement Learning (RL) by merging human
insights directly into the RL training process, improving model performance.
Our proposed iDDQN method modifies the Q-value update equation to integrate
human and agent actions, establishing a collaborative approach for policy
development. Additionally, we present an offline evaluative framework that
simulates the agent's trajectory as if no human intervention had occurred, to
assess the effectiveness of human interventions. Empirical results in simulated
autonomous driving scenarios demonstrate that iDDQN outperforms established
approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from
Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for
improving performance and adaptability.

</details>


### [363] [Efficient Continual Learning in Keyword Spotting using Binary Neural Networks](https://arxiv.org/pdf/2505.02469)
*Quynh Nguyen-Phuong Vu, Luciano Sebastian Martinez-Rau, Yuxuan Zhang, Nho-Duc Tran, Bengt Oelmann, Michele Magno, Sebastian Bader*

Main category: cs.LG

TL;DR: A Continual Learning (CL) approach for Keyword Spotting (KWS) using Binary Neural Networks (BNNs) is proposed, achieving high accuracy for adding new keywords while being efficient for resource-limited devices.


<details>
  <summary>Details</summary>
Motivation: Static KWS models in resource-limited devices cannot adapt to new scenarios like added keywords, necessitating a flexible and efficient solution.

Method: The study evaluates seven CL techniques on a 16-class use case, leveraging BNNs for reduced computation and memory requirements.

Result: Accuracy exceeds 95% for one additional keyword and up to 86% for four, with batch-based algorithms being more sensitive to dataset size.

Conclusion: The approach is effective and computationally efficient for integrating new keywords in KWS, suitable for resource-constrained devices.

Abstract: Keyword spotting (KWS) is an essential function that enables interaction with
ubiquitous smart devices. However, in resource-limited devices, KWS models are
often static and can thus not adapt to new scenarios, such as added keywords.
To overcome this problem, we propose a Continual Learning (CL) approach for KWS
built on Binary Neural Networks (BNNs). The framework leverages the reduced
computation and memory requirements of BNNs while incorporating techniques that
enable the seamless integration of new keywords over time. This study evaluates
seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95%
for a single additional keyword and up to 86% for four additional classes.
Sensitivity to the amount of training samples in the CL phase, and differences
in computational complexities are being evaluated. These evaluations
demonstrate that batch-based algorithms are more sensitive to the CL dataset
size, and that differences between the computational complexities are
insignificant. These findings highlight the potential of developing an
effective and computationally efficient technique for continuously integrating
new keywords in KWS applications that is compatible with resource-constrained
devices.

</details>


### [364] [Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding](https://arxiv.org/pdf/2505.01445)
*Muhammad Muaz, Sameed Sajid, Tobias Schulze, Chang Liu, Nils Klasen, Benny Drescher*

Main category: cs.LG

TL;DR: The paper explores explainable AI methods for root cause analysis in injection moulding, showing that different methods yield varying feature impacts and better attribution leads to correct cause identification.


<details>
  <summary>Details</summary>
Motivation: Current machine learning models for quality prediction in injection moulding are black boxes, limiting their use in quality control due to lack of explainability. Existing methods are either limited to specific algorithms or may misidentify root causes.

Method: The study uses model-agnostic explainable AI methods to analyze interactions among input machine settings in real experimental data (central composite design). It compares these methods for feature impact analysis on random forest and multilayer perceptron models.

Result: Different explainability methods produce varying feature impacts, with better attribution leading to correct root cause identification. Both models (random forest and MLP) achieved a mean absolute percentage error below 0.05%.

Conclusion: Model-agnostic explainable AI methods can improve root cause analysis in injection moulding, but method choice is critical as it affects feature attribution and actionable insights.

Abstract: If a product deviates from its desired properties in the injection moulding
process, its root cause analysis can be aided by models that relate the input
machine settings with the output quality characteristics. The machine learning
models tested in the quality prediction are mostly black boxes; therefore, no
direct explanation of their prognosis is given, which restricts their
applicability in the quality control. The previously attempted explainability
methods are either restricted to tree-based algorithms only or do not emphasize
on the fact that some explainability methods can lead to wrong root cause
identification of a product's deviation from its desired properties. This study
first shows that the interactions among the multiple input machine settings do
exist in real experimental data collected as per a central composite design.
Then, the model-agnostic explainable AI methods are compared for the first time
to show that different explainability methods indeed lead to different feature
impact analysis in injection moulding. Moreover, it is shown that the better
feature attribution translates to the correct cause identification and
actionable insights for the injection moulding process. Being model agnostic,
explanations on both random forest and multilayer perceptron are performed for
the cause analysis, as both models have the mean absolute percentage error of
less than 0.05% on the experimental dataset.

</details>


### [365] [COSMOS: Predictable and Cost-Effective Adaptation of LLMs](https://arxiv.org/pdf/2505.01449)
*Jiayu Wang, Aws Albarghouthi, Frederic Sala*

Main category: cs.LG

TL;DR: COSMOS is a framework predicting LLM adaptation outcomes efficiently, reducing computational costs by up to 98.71% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Optimal selection of LLM adaptation strategies under resource constraints is challenging and costly.

Method: Introduces COSMOS, using embedding-augmented proxy models and low-sample scaling laws to predict performance and cost.

Result: Achieves high accuracy, reducing costs by 92.72% on average and up to 98.71% in intensive scenarios.

Conclusion: Efficient prediction of adaptation outcomes is feasible and significantly reduces computational overhead.

Abstract: Large language models (LLMs) achieve remarkable performance across numerous
tasks by using a diverse array of adaptation strategies. However, optimally
selecting a model and adaptation strategy under resource constraints is
challenging and often requires extensive experimentation. We investigate
whether it is possible to accurately predict both performance and cost without
expensive trials. We formalize the strategy selection problem for LLMs and
introduce COSMOS, a unified prediction framework that efficiently estimates
adaptation outcomes at minimal cost. We instantiate and study the capability of
our framework via a pair of powerful predictors: embedding-augmented
lightweight proxy models to predict fine-tuning performance, and low-sample
scaling laws to forecast retrieval-augmented in-context learning. Extensive
evaluation across eight representative benchmarks demonstrates that COSMOS
achieves high prediction accuracy while reducing computational costs by 92.72%
on average, and up to 98.71% in resource-intensive scenarios. Our results show
that efficient prediction of adaptation outcomes is not only feasible but can
substantially reduce the computational overhead of LLM deployment while
maintaining performance standards.

</details>


### [366] [Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks](https://arxiv.org/pdf/2505.01450)
*Chaoyi Wang, Junjie Zheng, Zihao Chen, Shiyu Xia, Chaofan Ding, Xiaohao Zhang, Xi Tao, Xiaoming He, Xinhan Di*

Main category: cs.LG

TL;DR: TA-Dubbing is a comprehensive benchmark for evaluating movie dubbing models, addressing gaps in existing metrics and improving film production quality.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to capture the complexities of movie dubbing, and a practical evaluation system is needed to advance the field.

Method: Introduces TA-Dubbing, a benchmark covering dialogue, narration, monologue, and actor adaptability, with open-source resources and leaderboard integration.

Result: TA-Dubbing provides versatile benchmarking for state-of-the-art models and multi-modal large language models.

Conclusion: TA-Dubbing aims to drive progress in movie dubbing by offering a robust, open-source evaluation framework.

Abstract: Movie dubbing has advanced significantly, yet assessing the real-world
effectiveness of these models remains challenging. A comprehensive evaluation
benchmark is crucial for two key reasons: 1) Existing metrics fail to fully
capture the complexities of dialogue, narration, monologue, and actor
adaptability in movie dubbing. 2) A practical evaluation system should offer
valuable insights to improve movie dubbing quality and advancement in film
production. To this end, we introduce Talking Adaptive Dubbing Benchmarks
(TA-Dubbing), designed to improve film production by adapting to dialogue,
narration, monologue, and actors in movie dubbing. TA-Dubbing offers several
key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of
dimensions of movie dubbing, incorporating metric evaluations for both movie
understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is
designed to evaluate state-of-the-art movie dubbing models and advanced
multi-modal large language models. 3) Full Open-Sourcing: We fully open-source
TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video
suits, evaluation methods, annotations. We also continuously integrate new
movie dubbing models into the TA-Dubbing leaderboard at
https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie
dubbing.

</details>


### [367] [Explainable Machine Learning for Cyberattack Identification from Traffic Flows](https://arxiv.org/pdf/2505.01488)
*Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, Huihui Wang*

Main category: cs.LG

TL;DR: A deep learning-based anomaly detection system for traffic management identifies cyberattack indicators using traffic flow data, with Explainable AI enhancing interpretability. Challenges include transitional data inconsistencies and stealth attacks.


<details>
  <summary>Details</summary>
Motivation: Increasing cyberattacks on automated traffic systems require accessible defenses, as traditional methods are often unavailable to transportation agencies.

Method: Simulated cyberattacks in a virtualized traffic network, developed a deep learning anomaly detection system, and applied XAI techniques for interpretability.

Result: Identified Longest Stop Duration and Total Jam Distance as key indicators of compromised signals, with challenges like transitional data inconsistencies and stealth attacks.

Conclusion: The study advances AI-driven traffic security by improving detection accuracy and trustworthiness in smart transportation systems.

Abstract: The increasing automation of traffic management systems has made them prime
targets for cyberattacks, disrupting urban mobility and public safety.
Traditional network-layer defenses are often inaccessible to transportation
agencies, necessitating a machine learning-based approach that relies solely on
traffic flow data. In this study, we simulate cyberattacks in a semi-realistic
environment, using a virtualized traffic network to analyze disruption
patterns. We develop a deep learning-based anomaly detection system,
demonstrating that Longest Stop Duration and Total Jam Distance are key
indicators of compromised signals. To enhance interpretability, we apply
Explainable AI (XAI) techniques, identifying critical decision factors and
diagnosing misclassification errors. Our analysis reveals two primary
challenges: transitional data inconsistencies, where mislabeled recovery-phase
traffic misleads the model, and model limitations, where stealth attacks in
low-traffic conditions evade detection. This work enhances AI-driven traffic
security, improving both detection accuracy and trustworthiness in smart
transportation systems.

</details>


### [368] [Machine Learning for Cyber-Attack Identification from Traffic Flows](https://arxiv.org/pdf/2505.01489)
*Yujing Zhou, Marc L. Jacquet, Robel Dawit, Skyler Fabre, Dev Sarawat, Faheem Khan, Madison Newell, Yongxin Liu, Dahai Liu, Hongyun Chen, Jian Wang, Huihui Wang*

Main category: cs.LG

TL;DR: Simulation of cyber-attacks on Daytona Beach's traffic control system using Raspberry Pi, OPNSense, SUMO, and Metasploit. Research question: Can traffic flow patterns alone detect attacks? Achieved 85% accuracy in intrusion detection.


<details>
  <summary>Details</summary>
Motivation: To determine if traffic flow patterns can identify cyber-attacks, especially when traffic lights are maliciously manipulated at busy intersections.

Method: Used Raspberry Pi virtual machines, OPNSense firewall, SUMO for traffic dynamics, and Metasploit for exploitation. Analyzed traffic flow statistics like occupancy, jam length, and halting durations.

Result: Best model achieved 85% accuracy in detecting intrusions despite imbalanced data and overlapping traffic patterns.

Conclusion: Traffic flow statistics can effectively detect cyber-attacks on traffic control systems, with key indicators being occupancy, jam length, and halting durations.

Abstract: This paper presents our simulation of cyber-attacks and detection strategies
on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual
machines and the OPNSense firewall, along with traffic dynamics from SUMO and
exploitation via the Metasploit framework. We try to answer the research
questions: are we able to identify cyber attacks by only analyzing traffic flow
patterns. In this research, the cyber attacks are focused particularly when
lights are randomly turned all green or red at busy intersections by
adversarial attackers. Despite challenges stemming from imbalanced data and
overlapping traffic patterns, our best model shows 85\% accuracy when detecting
intrusions purely using traffic flow statistics. Key indicators for successful
detection included occupancy, jam length, and halting durations.

</details>


### [369] [Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation](https://arxiv.org/pdf/2505.01523)
*Madhav Kotecha, Vijendra Kumar Vaishya, Smita Gautam, Suraj Racha*

Main category: cs.LG

TL;DR: A refined method for fine-tuning LLMs on specific domains (e.g., math) using utility and diversity metrics to select optimal training subsets, reducing costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To achieve near-full dataset performance with fewer data points, reducing computational costs and training time for domain-specific fine-tuning of LLMs.

Method: Combines utility (perplexity and CoT loss) and diversity metrics to select informative and representative training examples. Evaluated on LLaMA-3 8B and Phi-3 models against baselines.

Result: Competitive performance compared to full dataset training, with significant computational savings.

Conclusion: The proposed subset selection method efficiently fine-tunes LLMs, balancing performance and resource efficiency.

Abstract: We propose a refined approach to efficiently fine-tune large language models
(LLMs) on specific domains like the mathematical domain by employing a budgeted
subset selection method. Our approach combines utility and diversity metrics to
select the most informative and representative training examples. The final
goal is to achieve near-full dataset performance with meticulously selected
data points from the entire dataset while significantly reducing computational
cost and training time and achieving competitive performance as the full
dataset. The utility metric incorporates both perplexity and Chain-of-Thought
(CoT) loss to identify challenging examples that contribute most to model
learning, while the diversity metric ensures broad coverage across mathematical
subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing
against several baseline approaches, including random selection,
diversity-based sampling, and existing state-of-the-art subset selection
techniques.

</details>


### [370] [Contextures: Representations from Contexts](https://arxiv.org/pdf/2505.01557)
*Runtian Zhai, Kai Yang, Che-Ping Tsai, Burak Varici, Zico Kolter, Pradeep Ravikumar*

Main category: cs.LG

TL;DR: The paper introduces the 'contexture theory,' which characterizes representation learning methods as learning from input-context associations, showing optimality and diminishing returns with model scaling.


<details>
  <summary>Details</summary>
Motivation: To systematically understand the representations learned by foundation models, which currently lack characterization.

Method: Proposes the contexture theory, linking representation learning to approximating top singular functions of an expectation operator induced by context.

Result: Demonstrates generality across learning paradigms (supervised, self-supervised, manifold learning) and proves optimality of contexture-based representations.

Conclusion: Scaling models has diminishing returns; better contexts are needed for improvement. A metric for context usefulness is proposed and validated experimentally.

Abstract: Despite the empirical success of foundation models, we do not have a
systematic characterization of the representations that these models learn. In
this paper, we establish the contexture theory. It shows that a large class of
representation learning methods can be characterized as learning from the
association between the input and a context variable. Specifically, we show
that many popular methods aim to approximate the top-d singular functions of
the expectation operator induced by the context, in which case we say that the
representation learns the contexture. We demonstrate the generality of the
contexture theory by proving that representation learning within various
learning paradigms -- supervised, self-supervised, and manifold learning -- can
all be studied from such a perspective. We also prove that the representations
that learn the contexture are optimal on those tasks that are compatible with
the context. One important implication of the contexture theory is that once
the model is large enough to approximate the top singular functions, further
scaling up the model size yields diminishing returns. Therefore, scaling is not
all we need, and further improvement requires better contexts. To this end, we
study how to evaluate the usefulness of a context without knowing the
downstream tasks. We propose a metric and show by experiments that it
correlates well with the actual performance of the encoder on many real
datasets.

</details>


### [371] [Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation](https://arxiv.org/pdf/2505.01584)
*Zhiqiang He, Zhi Liu*

Main category: cs.LG

TL;DR: The paper introduces ReSiN, a method to address neural plasticity loss in reinforcement learning for network adaptation, improving bitrate and QoE in video streaming.


<details>
  <summary>Details</summary>
Motivation: Current solutions for network adaptation rely on stationary assumptions, and neural networks suffer from plasticity loss, limiting adaptability to dynamic conditions.

Method: Developed the Silent Neuron theory and proposed ReSiN, a method to preserve neural plasticity through strategic neuron resets based on propagation states.

Result: ReSiN achieved up to 168% higher bitrate and 108% better QoE in adaptive video streaming, outperforming existing solutions.

Conclusion: ReSiN effectively addresses plasticity loss, demonstrating robust adaptability across varying network conditions.

Abstract: Adapting to non-stationary network conditions presents significant challenges
for resource adaptation. However, current solutions primarily rely on
stationary assumptions. While data-driven reinforcement learning approaches
offer promising solutions for handling network dynamics, our systematic
investigation reveals a critical limitation: neural networks suffer from
plasticity loss, significantly impeding their ability to adapt to evolving
network conditions. Through theoretical analysis of neural propagation
mechanisms, we demonstrate that existing dormant neuron metrics inadequately
characterize neural plasticity loss. To address this limitation, we have
developed the Silent Neuron theory, which provides a more comprehensive
framework for understanding plasticity degradation. Based on these theoretical
insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural
plasticity through strategic neuron resets guided by both forward and backward
propagation states. In our implementation of an adaptive video streaming
system, ReSiN has shown significant improvements over existing solutions,
achieving up to 168% higher bitrate and 108% better quality of experience (QoE)
while maintaining comparable smoothness. Furthermore, ReSiN consistently
outperforms in stationary environments, demonstrating its robust adaptability
across different network conditions.

</details>


### [372] [Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises](https://arxiv.org/pdf/2505.01591)
*Abdalwahab Almajed, Maryam Tabar, Peyman Najafirad*

Main category: cs.LG

TL;DR: The paper examines bias in ML-driven house price prediction models, evaluates fairness, and tests bias mitigation methods, finding in-processing approaches more effective.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding ethnic/racial bias in ML-driven house price prediction models and ensure responsible ML use without exacerbating inequity.

Method: Develops ML models using structural and neighborhood attributes, assesses fairness under various privileged group definitions, and tests bias mitigation solutions.

Result: ML models exhibit bias towards protected attributes (race/ethnicity), with in-processing bias mitigation being generally more effective than pre-processing.

Conclusion: The study highlights the need for fairness-aware ML in housing and suggests in-processing methods as preferable for bias mitigation.

Abstract: As a basic human need, housing plays a key role in enhancing health,
well-being, and educational outcome in society, and the housing market is a
major factor for promoting quality of life and ensuring social equity. To
improve the housing conditions, there has been extensive research on building
Machine Learning (ML)-driven house price prediction solutions to accurately
forecast the future conditions, and help inform actions and policies in the
field. In spite of their success in developing high-accuracy models, there is a
gap in our understanding of the extent to which various ML-driven house price
prediction approaches show ethnic and/or racial bias, which in turn is
essential for the responsible use of ML, and ensuring that the ML-driven
solutions do not exacerbate inequity. To fill this gap, this paper develops
several ML models from a combination of structural and neighborhood-level
attributes, and conducts comprehensive assessments on the fairness of ML models
under various definitions of privileged groups. As a result, it finds that the
ML-driven house price prediction models show various levels of bias towards
protected attributes (i.e., race and ethnicity in this study). Then, it
investigates the performance of different bias mitigation solutions, and the
experimental results show their various levels of effectiveness on different
ML-driven methods. However, in general, the in-processing bias mitigation
approach tends to be more effective than the pre-processing one in this problem
domain. Our code is available at https://github.com/wahab1412/housing_fairness.

</details>


### [373] [Don't be lazy: CompleteP enables compute-efficient deep transformers](https://arxiv.org/pdf/2505.01618)
*Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, Joel Hestness*

Main category: cs.LG

TL;DR: The paper introduces CompleteP, a parameterization method for LLM training that ensures hyperparameter transfer across model sizes and avoids lazy learning, improving compute efficiency by 12-34%.


<details>
  <summary>Details</summary>
Motivation: Current parameterization methods for LLM training often fail to transfer optimal hyperparameters across model sizes, leading to expensive re-tuning or sub-optimal performance. Some methods also trap models in lazy learning, limiting depth and nonlinearity utilization.

Method: The study analyzes different parameterization rules for adjusting hyperparameters as model size changes. It develops theory to identify lazy learning regimes and proposes CompleteP, a parameterization that ensures hyperparameter transfer and non-lazy learning.

Result: CompleteP achieves depth-wise hyperparameter transfer and avoids lazy learning, enabling compute-efficient model shapes. It improves compute efficiency by 12-34% over prior methods.

Conclusion: CompleteP is a superior parameterization method for LLM training, offering both hyperparameter transfer and non-lazy learning, leading to significant compute efficiency gains.

Abstract: We study compute efficiency of LLM training when using different
parameterizations, i.e., rules for adjusting model and optimizer
hyperparameters (HPs) as model size changes. Some parameterizations fail to
transfer optimal base HPs (such as learning rate) across changes in model
depth, requiring practitioners to either re-tune these HPs as they scale up
(expensive), or accept sub-optimal training when re-tuning is prohibitive. Even
when they achieve HP transfer, we develop theory to show parameterizations may
still exist in the lazy learning regime where layers learn only features close
to their linearization, preventing effective use of depth and nonlinearity.
Finally, we identify and adopt the unique parameterization we call CompleteP
that achieves both depth-wise HP transfer and non-lazy learning in all layers.
CompleteP enables a wider range of model width/depth ratios to remain
compute-efficient, unlocking shapes better suited for different hardware
settings and operational contexts. Moreover, CompleteP enables 12-34\% compute
efficiency improvements over the prior state-of-the-art.

</details>


### [374] [Skill-based Safe Reinforcement Learning with Risk Planning](https://arxiv.org/pdf/2505.01619)
*Hanping Zhang, Yuhong Guo*

Main category: cs.LG

TL;DR: A novel Safe Skill Planning (SSkP) approach enhances safe RL by using offline data for risk prediction and planning, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in RL is critical to avoid costly or severe consequences from improper actions in real-world environments.

Method: SSkP involves a two-stage process: learning a skill risk predictor from offline data using PU learning, then using it for risk planning to guide online RL safely.

Result: Experiments in robotic simulations show SSkP consistently outperforms state-of-the-art safe RL methods.

Conclusion: SSkP effectively improves safe RL by leveraging offline data for risk-aware planning and policy learning.

Abstract: Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent
conducts learning by interacting with real-world environments where improper
actions can induce high costs or lead to severe consequences. In this paper, we
propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe
RL by exploiting auxiliary offline demonstration data. SSkP involves a
two-stage process. First, we employ PU learning to learn a skill risk predictor
from the offline demonstration data. Then, based on the learned skill risk
predictor, we develop a novel risk planning process to enhance online safe RL
and learn a risk-averse safe policy efficiently through interactions with the
online RL environment, while simultaneously adapting the skill risk predictor
to the environment. We conduct experiments in several benchmark robotic
simulation environments. The experimental results demonstrate that the proposed
approach consistently outperforms previous state-of-the-art safe RL methods.

</details>


### [375] [A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components](https://arxiv.org/pdf/2505.01627)
*Fatemeh Elhambakhsh, Daniele Grandi, Hyunwoong Ko*

Main category: cs.LG

TL;DR: The paper proposes an LLM-based domain adaptation framework for automating the classification of mechanical assembly parts' functions, improving accuracy and consistency in functional modeling during early design phases.


<details>
  <summary>Details</summary>
Motivation: The lack of well-structured functional data hinders function-based design and early decision-making. LLMs offer potential to automate and improve this process.

Method: A novel LLM-based domain adaptation framework using fine-tuning (e.g., GPT-3.5 Turbo) on domain-specific datasets (OSDR) for automated function classification.

Result: The domain-adapted LLM generates high-quality functional data, enhancing semantic representation and supporting effective design exploration.

Conclusion: LLM-based domain adaptation improves functional modeling accuracy and consistency, benefiting early-phase engineering design.

Abstract: The conceptual design phase represents a critical early stage in the product
development process, where designers generate potential solutions that meet
predefined design specifications based on functional requirements. Functional
modeling, a foundational aspect of this phase, enables designers to reason
about product functions before specific structural details are determined. A
widely adopted approach to functional modeling is the
Function-Behavior-Structure (FBS) framework, which supports the transformation
of functional intent into behavioral and structural descriptions. However, the
effectiveness of function-based design is often hindered by the lack of
well-structured and comprehensive functional data. This scarcity can negatively
impact early design decision-making and hinder the development of accurate
behavioral models. Recent advances in Large Language Models (LLMs), such as
those based on GPT architectures, offer a promising avenue to address this gap.
LLMs have demonstrated significant capabilities in language understanding and
natural language processing (NLP), making them suitable for automated
classification tasks. This study proposes a novel LLM-based domain adaptation
(DA) framework using fine-tuning for the automated classification of mechanical
assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the
traditionally manual and subjective process of function annotation can be
improved in both accuracy and consistency. A case study demonstrates
fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository
(OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the
domain-adapted LLM can generate high-quality functional data, enhancing the
semantic representation of mechanical parts and supporting more effective
design exploration in early-phase engineering.

</details>


### [376] [Causally Fair Node Classification on Non-IID Graph Data](https://arxiv.org/pdf/2505.01652)
*Yucong Dai, Lu Zhang, Yaowei Hu, Susan Gauch, Yongkai Wu*

Main category: cs.LG

TL;DR: The paper introduces a causality-based fairness method for graph data, addressing biases in non-IID settings using the NSCM framework and MPVA model.


<details>
  <summary>Details</summary>
Motivation: Existing fairness methods neglect causal relationships in graph data, assuming IID conditions. This work tackles non-IID, interconnected data to improve fairness.

Method: The paper uses the NSCM framework with Decomposability and Graph Independence assumptions, developing MPVA for causally fair node classification.

Result: MPVA outperforms conventional methods in approximating interventional distributions and reducing bias, validated on semi-synthetic and real-world datasets.

Conclusion: The work highlights causality's role in fairness for complex ML, suggesting future research to relax initial assumptions for broader applicability.

Abstract: Fair machine learning seeks to identify and mitigate biases in predictions
against unfavorable populations characterized by demographic attributes, such
as race and gender. Recently, a few works have extended fairness to graph data,
such as social networks, but most of them neglect the causal relationships
among data instances. This paper addresses the prevalent challenge in
fairness-aware ML algorithms, which typically assume Independent and
Identically Distributed (IID) data. We tackle the overlooked domain of non-IID,
graph-based settings where data instances are interconnected, influencing the
outcomes of fairness interventions. We base our research on the Network
Structural Causal Model (NSCM) framework and posit two main assumptions:
Decomposability and Graph Independence, which enable the computation of
interventional distributions in non-IID settings using the $do$-calculus. Based
on that, we develop the Message Passing Variational Autoencoder for Causal
Inference (MPVA) to compute interventional distributions and facilitate
causally fair node classification through estimated interventional
distributions. Empirical evaluations on semi-synthetic and real-world datasets
demonstrate that MPVA outperforms conventional methods by effectively
approximating interventional distributions and mitigating bias. The
implications of our findings underscore the potential of causality-based
fairness in complex ML applications, setting the stage for further research
into relaxing the initial assumptions to enhance model fairness.

</details>


### [377] [Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification](https://arxiv.org/pdf/2505.01660)
*Sicong Li, Qianqian Xu, Zhiyong Yang, Zitai Wang, Linchao Zhang, Xiaochun Cao, Qingming Huang*

Main category: cs.LG

TL;DR: Focal-SAM improves generalization in long-tailed datasets by assigning class-wise sharpness penalties, balancing efficiency and control without extra backpropagations.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational efficiency and loss landscape control in existing long-tail SAM variants (ImbSAM, CC-SAM).

Method: Introduces Focal-SAM, which assigns different penalties to class-wise sharpness, avoiding extra backpropagations.

Result: Achieves fine-grained control efficiently, validated by experiments on traditional and foundation models.

Conclusion: Focal-SAM offers a better balance of efficiency and control, with theoretical and empirical support.

Abstract: Real-world datasets often follow a long-tailed distribution, making
generalization to tail classes difficult. Recent methods resorted to long-tail
variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to
improve generalization by flattening the loss landscape. However, these
attempts face a trade-off between computational efficiency and control over the
loss landscape. On the one hand, ImbSAM is efficient but offers only coarse
control as it excludes head classes from the SAM process. On the other hand,
CC-SAM provides fine-grained control through class-dependent perturbations but
at the cost of efficiency due to multiple backpropagations. Seeing this
dilemma, we introduce Focal-SAM, which assigns different penalties to
class-wise sharpness, achieving fine-grained control without extra
backpropagations, thus maintaining efficiency. Furthermore, we theoretically
analyze Focal-SAM's generalization ability and derive a sharper generalization
bound. Extensive experiments on both traditional and foundation models validate
the effectiveness of Focal-SAM.

</details>


### [378] [Adaptively Point-weighting Curriculum Learning](https://arxiv.org/pdf/2505.01665)
*Wensheng Li, Hao Wang, Ruifeng Zhou, Hanting Guan, Chao Zhang, Dacheng Tao*

Main category: cs.LG

TL;DR: APW curriculum learning adaptively weights samples based on training error and network state, improving training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance deep network training by mimicking human learning, focusing on easy samples first and hard samples later.

Method: Developed the APW algorithm, which adjusts sample weights dynamically during training phases.

Result: APW improves training effectiveness, stability, and generalization, supported by numerical experiments.

Conclusion: APW is a superior curriculum learning method with validated theoretical and practical benefits.

Abstract: Curriculum learning (CL) is referred to as a training strategy that makes
easy samples learned first and then fits hard samples. It imitates the process
of humans learning knowledge, and has become a potential manner of effectively
training deep networks. In this study, we develop the adaptively
point-weighting (APW) curriculum learning algorithm, which adaptively assigns
the weight to every training sample not only based on its training error but
also considering the current training state of the network. Specifically, in
the early training phase, it increases the weights of easy samples to make the
network rapidly capture the overall characteristics of the dataset; and in the
later training phase, the weights of hard points rise to improve the fitting
performance on the discrete local regions. Moreover, we also present the
theoretical analysis on the properties of APW including training effectiveness,
training feasibility, training stability, and generalization performance. The
numerical experiments support the superiority of APW and demonstrate the
validity of our theoretical findings.

</details>


### [379] [PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking](https://arxiv.org/pdf/2505.01700)
*Yize Jiang, Xinze Li, Yuanyuan Zhang, Jin Han, Youjun Xu, Ayush Pandit, Zaixi Zhang, Mengdi Wang, Mengyang Wang, Chong Liu, Guang Yang, Yejin Choi, Wu-Jun Li, Tianfan Fu, Fang Wu, Junhong Liu*

Main category: cs.LG

TL;DR: PoseX is an open-source benchmark for evaluating protein-ligand docking methods, focusing on self-docking and cross-docking, with curated datasets, 22 diverse methods, and a relaxation post-processing technique.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for protein-ligand docking lack practical evaluation setups or involve heavy frameworks, making efficient assessment challenging.

Method: PoseX curates a dataset (718 self-docking, 1,312 cross-docking entries), incorporates 22 docking methods (physics-based, AI docking, AI co-folding), and introduces a relaxation post-processing method.

Result: AI-based methods outperform traditional physics-based ones in accuracy; relaxation improves stereochemical deficiencies; AI co-folding struggles with ligand chirality.

Conclusion: PoseX provides a practical, comprehensive benchmark, highlighting AI advancements and limitations in docking, with open-source resources for community use.

Abstract: Recently, significant progress has been made in protein-ligand docking,
especially in modern deep learning methods, and some benchmarks were proposed,
e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical
evaluation setups (e.g., blind docking, self docking), or heavy framework that
involves training, raising challenges to assess docking methods efficiently. To
fill this gap, we proposed PoseX, an open-source benchmark focusing on
self-docking and cross-docking, to evaluate the algorithmic advances
practically and comprehensively. Specifically, first, we curate a new
evaluation dataset with 718 entries for self docking and 1,312 for cross
docking; second, we incorporate 22 docking methods across three methodological
categories, including (1) traditional physics-based methods (e.g.,
Schr\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI
co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as
post-processing to minimize conformation energy and refine binding pose;
fourth, we released a leaderboard to rank submitted models in real time. We
draw some key insights via extensive experiments: (1) AI-based approaches have
already surpassed traditional physics-based approaches in overall docking
accuracy (RMSD). The longstanding generalization issues that have plagued AI
molecular docking have been significantly alleviated in the latest models. (2)
The stereochemical deficiencies of AI-based approaches can be greatly
alleviated with post-processing relaxation. Combining AI docking methods with
the enhanced relaxation method achieves the best performance to date. (3) AI
co-folding methods commonly face ligand chirality issues, which cannot be
resolved by relaxation. The code, curated dataset and leaderboard are released
at https://github.com/CataAI/PoseX.

</details>


### [380] [PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems](https://arxiv.org/pdf/2505.01736)
*Han Wan, Rui Zhang, Qi Wang, Yang Liu, Hao Sun*

Main category: cs.LG

TL;DR: PeSANet integrates physics and machine learning to forecast complex systems with limited data and incomplete physics, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods and machine learning struggle with incomplete physics and scarce data in modeling PDE-governed systems.

Method: PeSANet combines physics-encoded blocks for local differential operators and spectral-enhanced blocks with a novel spectral attention mechanism for global dependencies.

Result: PeSANet excels in long-term forecasting accuracy, surpassing existing methods across all metrics.

Conclusion: PeSANet offers a robust solution for simulating complex systems with limited data and incomplete physics.

Abstract: Accurately modeling and forecasting complex systems governed by partial
differential equations (PDEs) is crucial in various scientific and engineering
domains. However, traditional numerical methods struggle in real-world
scenarios due to incomplete or unknown physical laws. Meanwhile, machine
learning approaches often fail to generalize effectively when faced with scarce
observational data and the challenge of capturing local and global features. To
this end, we propose the Physics-encoded Spectral Attention Network (PeSANet),
which integrates local and global information to forecast complex systems with
limited data and incomplete physical priors. The model consists of two key
components: a physics-encoded block that uses hard constraints to approximate
local differential operators from limited data, and a spectral-enhanced block
that captures long-range global dependencies in the frequency domain.
Specifically, we introduce a novel spectral attention mechanism to model
inter-spectrum relationships and learn long-range spatial features.
Experimental results demonstrate that PeSANet outperforms existing methods
across all metrics, particularly in long-term forecasting accuracy, providing a
promising solution for simulating complex systems with limited data and
incomplete physics.

</details>


### [381] [Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients](https://arxiv.org/pdf/2505.01744)
*Yezhen Wang, Zhouhao Yang, Brian K Chen, Fanyi Pu, Bo Li, Tianyu Gao, Kenji Kawaguchi*

Main category: cs.LG

TL;DR: VLoRP extends LoRP by introducing finer-grained gradient projections, improving stability and efficiency under fixed memory. ProjFactor, an adaptive optimizer, reduces memory usage while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing LoRP methods lack exploration of projection granularity, limiting trade-offs between memory efficiency and performance.

Method: VLoRP introduces finer-grained projections and ProjFactor, a memory-efficient optimizer, to optimize performance under memory constraints.

Result: Finer-grained projections enhance stability and efficiency. ProjFactor reduces memory usage without compromising performance.

Conclusion: VLoRP and ProjFactor offer a balanced trade-off between memory efficiency and performance, validated across multiple tasks.

Abstract: Building upon the success of low-rank adapter (LoRA), low-rank gradient
projection (LoRP) has emerged as a promising solution for memory-efficient
fine-tuning. However, existing LoRP methods typically treat each row of the
gradient matrix as the default projection unit, leaving the role of projection
granularity underexplored. In this work, we propose a novel framework, VLoRP,
that extends low-rank gradient projection by introducing an additional degree
of freedom for controlling the trade-off between memory efficiency and
performance, beyond the rank hyper-parameter. Through this framework, we
systematically explore the impact of projection granularity, demonstrating that
finer-grained projections lead to enhanced stability and efficiency even under
a fixed memory budget. Regarding the optimization for VLoRP, we present
ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces
memory requirement while ensuring competitive performance, even in the presence
of gradient accumulation. Additionally, we provide a theoretical analysis of
VLoRP, demonstrating the descent and convergence of its optimization trajectory
under both SGD and ProjFactor. Extensive experiments are conducted to validate
our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.

</details>


### [382] [Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition](https://arxiv.org/pdf/2505.01783)
*Amirmohammad Farzaneh, Osvaldo Simeone*

Main category: cs.LG

TL;DR: C-PP-COAD is a new framework for online anomaly detection that uses synthetic and real calibration data to ensure reliable performance with limited real data, maintaining strict FDR control.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of limited real calibration data in anomaly detection while ensuring reliable false discovery rate (FDR) control.

Method: Leverages synthetic calibration data and integrates real data contextually, using conformal p-values, active p-value statistics, and online FDR control.

Result: Reduces dependency on real calibration data without compromising FDR control, as shown in experiments on synthetic and real datasets.

Conclusion: C-PP-COAD offers a robust solution for online anomaly detection with limited real data, ensuring reliable performance and FDR guarantees.

Abstract: Online anomaly detection is essential in fields such as cybersecurity,
healthcare, and industrial monitoring, where promptly identifying deviations
from expected behavior can avert critical failures or security breaches. While
numerous anomaly scoring methods based on supervised or unsupervised learning
have been proposed, current approaches typically rely on a continuous stream of
real-world calibration data to provide assumption-free guarantees on the false
discovery rate (FDR). To address the inherent challenges posed by limited real
calibration data, we introduce context-aware prediction-powered conformal
online anomaly detection (C-PP-COAD). Our framework strategically leverages
synthetic calibration data to mitigate data scarcity, while adaptively
integrating real data based on contextual cues. C-PP-COAD utilizes conformal
p-values, active p-value statistics, and online FDR control mechanisms to
maintain rigorous and reliable anomaly detection performance over time.
Experiments conducted on both synthetic and real-world datasets demonstrate
that C-PP-COAD significantly reduces dependency on real calibration data
without compromising guaranteed FDR control.

</details>


### [383] [Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning](https://arxiv.org/pdf/2505.01788)
*Md. Tanzib Hosain, Asif Zaman, Md. Shahriar Sajid, Shadman Sakeeb Khan, Shanjida Akter*

Main category: cs.LG

TL;DR: The paper analyzes privacy-preserving AI, focusing on Federated Personalized Learning (PPMLFPL), and evaluates two methods (APPLE+DP and APPLE+HE) for balancing personalization and data privacy.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about data privacy in AI-driven systems necessitate privacy-preserving methods like Federated Learning.

Method: The study evaluates PPMLFPL, specifically comparing APPLE+DP (Differential Privacy) and APPLE+HE (Homomorphic Encryption) for personalized ML models.

Result: APPLE+HE is recommended for privacy-preserving tasks, while APPLE+DP offers efficient execution.

Conclusion: The findings highlight PPMLFPL's potential for advancing privacy-conscious AI technologies.

Abstract: The widespread adoption of Artificial Intelligence (AI) has been driven by
significant advances in intelligent system research. However, this progress has
raised concerns about data privacy, leading to a growing awareness of the need
for privacy-preserving AI. In response, there has been a seismic shift in
interest towards the leading paradigm for training Machine Learning (ML) models
on decentralized data silos while maintaining data privacy, Federated Learning
(FL). This research paper presents a comprehensive performance analysis of a
cutting-edge approach to personalize ML model while preserving privacy achieved
through Privacy Preserving Machine Learning with the innovative framework of
Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns
about data privacy, this study evaluates the effectiveness of PPMLFPL
addressing the critical balance between personalized model refinement and
maintaining the confidentiality of individual user data. According to our
analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential
Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the
Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption
(APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated
personalized learning settings is strongly suggested. The results offer
valuable insights creating it a promising scope for future advancements in the
field of privacy-conscious data-driven technologies.

</details>


### [384] [Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees](https://arxiv.org/pdf/2505.01810)
*Zhiyi Zhou, Hexin Peng, Hongyu Long*

Main category: cs.LG

TL;DR: The paper applies conformal prediction (CP) to deep learning-based indoor positioning to address challenges like poor generalization and overfitting, achieving high accuracy and statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: High-precision indoor positioning is crucial for IoT-based LBS, but existing methods suffer from issues like poor generalization and lack of interpretability.

Method: CP transforms model uncertainty into non-conformity scores, constructs prediction sets, and introduces conformal risk control for navigation tasks. Lightweight models like MobileNetV1 and ResNet50 are tested.

Result: The model achieved ~100% training accuracy and 85% testing accuracy, with CP effectively approximating target coverage and managing FDR/FNR.

Conclusion: CP enhances deep learning-based indoor positioning by providing statistical guarantees and improving generalization, as demonstrated on the UJIIndoLoc dataset.

Abstract: With the advancement of Internet of Things (IoT) technologies, high-precision
indoor positioning has become essential for Location-Based Services (LBS) in
complex indoor environments. Fingerprint-based localization is popular, but
traditional algorithms and deep learning-based methods face challenges such as
poor generalization, overfitting, and lack of interpretability. This paper
applies conformal prediction (CP) to deep learning-based indoor positioning. CP
transforms the uncertainty of the model into a non-conformity score, constructs
prediction sets to ensure correctness coverage, and provides statistical
guarantees. We also introduce conformal risk control for path navigation tasks
to manage the false discovery rate (FDR) and the false negative rate (FNR).The
model achieved an accuracy of approximately 100% on the training dataset and
85% on the testing dataset, effectively demonstrating its performance and
generalization capability. Furthermore, we also develop a conformal p-value
framework to control the proportion of position-error points. Experiments on
the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19,
MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction
technique can effectively approximate the target coverage, and different models
have different performance in terms of prediction set size and uncertainty
quantification.

</details>


### [385] [An LSTM-PINN Hybrid Method to the specific problem of population forecasting](https://arxiv.org/pdf/2505.01819)
*Ze Tao*

Main category: cs.LG

TL;DR: The paper proposes two physics-informed deep learning models (PINN and LSTM-PINN) to simulate age-structured population dynamics under fertility policy changes, demonstrating their effectiveness in capturing policy-sensitive demographic shifts.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling age-structured population dynamics under policy-driven fertility changes is challenging due to the lack of integration between domain knowledge and long-term temporal dependencies.

Method: Two frameworks are introduced: PINN (enforcing governing equations via collocation-based training) and LSTM-PINN (adding sequential memory mechanisms for long-range dependencies).

Result: The models successfully simulate population evolution under three fertility policy scenarios, reflecting policy-sensitive demographic shifts.

Conclusion: The study offers a novel framework for demographic forecasting under policy interventions, aiding long-term planning for population challenges.

Abstract: Deep learning has emerged as a powerful tool in scientific modeling,
particularly for complex dynamical systems; however, accurately capturing
age-structured population dynamics under policy-driven fertility changes
remains a significant challenge due to the lack of effective integration
between domain knowledge and long-term temporal dependencies. To address this
issue, we propose two physics-informed deep learning frameworks--PINN and
LSTM-PINN--that incorporate policy-aware fertility functions into a
transport-reaction partial differential equation to simulate population
evolution from 2024 to 2054. The standard PINN model enforces the governing
equation and boundary conditions via collocation-based training, enabling
accurate learning of underlying population dynamics and ensuring stable
convergence. Building on this, the LSTM-PINN framework integrates sequential
memory mechanisms to effectively capture long-range dependencies in the
age-time domain, achieving robust training performance across multiple loss
components. Simulation results under three distinct fertility policy
scenarios-the Three-child policy, the Universal two-child policy, and the
Separate two-child policy--demonstrate the models' ability to reflect
policy-sensitive demographic shifts and highlight the effectiveness of
integrating domain knowledge into data-driven forecasting. This study provides
a novel and extensible framework for modeling age-structured population
dynamics under policy interventions, offering valuable insights for
data-informed demographic forecasting and long-term policy planning in the face
of emerging population challenges.

</details>


### [386] [Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning](https://arxiv.org/pdf/2505.01822)
*Jifeng Hu, Sili Huang, Zhejian Yang, Shengchao Hu, Li Shen, Hechang Chen, Lichao Sun, Yi Chang, Dacheng Tao*

Main category: cs.LG

TL;DR: AEPO addresses the challenge of estimating intermediate energy in diffusion models for RL by providing a closed-form solution and training an energy neural network, outperforming baselines in D4RL benchmarks.


<details>
  <summary>Details</summary>
Motivation: The intractability of intermediate energy estimation in diffusion models for RL motivates the development of AEPO.

Method: AEPO analyzes the conditional Gaussian transformation, derives a closed-form solution for intermediate guidance, and trains an energy neural network to estimate log-expectation.

Result: AEPO surpasses baselines in 30+ offline RL tasks, demonstrating effectiveness in D4RL benchmarks.

Conclusion: AEPO successfully tackles the energy estimation challenge, proving competitive in offline RL tasks.

Abstract: Conditional decision generation with diffusion models has shown powerful
competitiveness in reinforcement learning (RL). Recent studies reveal the
relation between energy-function-guidance diffusion models and constrained RL
problems. The main challenge lies in estimating the intermediate energy, which
is intractable due to the log-expectation formulation during the generation
process. To address this issue, we propose the Analytic Energy-guided Policy
Optimization (AEPO). Specifically, we first provide a theoretical analysis and
the closed-form solution of the intermediate guidance when the diffusion model
obeys the conditional Gaussian transformation. Then, we analyze the posterior
Gaussian distribution in the log-expectation formulation and obtain the target
estimation of the log-expectation under mild assumptions. Finally, we train an
intermediate energy neural network to approach the target estimation of
log-expectation formulation. We apply our method in 30+ offline RL tasks to
demonstrate the effectiveness of our method. Extensive experiments illustrate
that our method surpasses numerous representative baselines in D4RL offline
reinforcement learning benchmarks.

</details>


### [387] [Towards Trustworthy Federated Learning with Untrusted Participants](https://arxiv.org/pdf/2505.01874)
*Youssef Allouah, Rachid Guerraoui, John Stephan*

Main category: cs.LG

TL;DR: CafCor enables privacy and robustness in distributed learning without a trusted server by using shared randomness between workers and correlated noise injection.


<details>
  <summary>Details</summary>
Motivation: To achieve trustworthy distributed learning with resilience against malicious parties and data privacy without relying on a trusted central server.

Method: Proposes CafCor, integrating robust gradient aggregation with correlated noise injection using shared randomness between workers.

Result: CafCor outperforms local DP methods and approaches central DP utility, validated by empirical results on benchmarks.

Conclusion: Privacy and robustness can coexist in distributed systems without trusting the server or sacrificing utility.

Abstract: Resilience against malicious parties and data privacy are essential for
trustworthy distributed learning, yet achieving both with good utility
typically requires the strong assumption of a trusted central server. This
paper shows that a significantly weaker assumption suffices: each pair of
workers shares a randomness seed unknown to others. In a setting where
malicious workers may collude with an untrusted server, we propose CafCor, an
algorithm that integrates robust gradient aggregation with correlated noise
injection, leveraging shared randomness between workers. We prove that CafCor
achieves strong privacy-utility trade-offs, significantly outperforming local
differential privacy (DP) methods, which do not make any trust assumption,
while approaching central DP utility, where the server is fully trusted.
Empirical results on standard benchmarks validate CafCor's practicality,
showing that privacy and robustness can coexist in distributed systems without
sacrificing utility or trusting the server.

</details>


### [388] [OODTE: A Differential Testing Engine for the ONNX Optimizer](https://arxiv.org/pdf/2505.01892)
*Nikolaos Louloudakis, Ajitha Rajan*

Main category: cs.LG

TL;DR: OODTE is a tool to test the ONNX Optimizer's accuracy preservation, revealing 15 issues (14 new) and significant accuracy deviations in models.


<details>
  <summary>Details</summary>
Motivation: The ONNX Optimizer lacks rigorous accuracy preservation testing, prompting the need for OODTE.

Method: OODTE uses differential testing on ONNX models, comparing original and optimized versions to detect issues.

Result: Found 15 issues (14 new), with 9.2% model crashes and 30% accuracy deviations in classification models.

Conclusion: OODTE effectively identifies ONNX Optimizer flaws, highlighting the need for better accuracy preservation.

Abstract: With $700$ stars on GitHub and part of the official ONNX repository, the ONNX
Optimizer consists of the standard method to apply graph-based optimizations on
ONNX models. However, its ability to preserve model accuracy across
optimizations, has not been rigorously explored. We propose OODTE, a utility to
automatically and thoroughly assess the correctness of the ONNX Optimizer.
OODTE follows a simple, yet effective differential testing and evaluation
approach that can be easily adopted to other compiler optimizers. In
particular, OODTE utilizes a number of ONNX models, then optimizes them and
executes both the original and the optimized variants across a user-defined set
of inputs, while automatically logging any issues with the optimization
process. Finally, for successfully optimized models, OODTE compares the
results, and, if any accuracy deviations are observed, it iteratively repeats
the process for each pass of the ONNX Optimizer, to localize the root cause of
the differences observed. Using OODTE, we sourced well-known $130$ models from
the official ONNX Model Hub, used for a wide variety of tasks (classification,
object detection, semantic segmentation, text summarization, question and
answering, sentiment analysis) from the official ONNX model hub. We detected 15
issues, 14 of which were previously unknown, associated with optimizer crashes
and accuracy deviations. We also observed $9.2$% of all model instances
presenting issues leading into the crash of the optimizer, or the generation of
an invalid model while using the primary optimizer strategies. In addition,
$30$% of the classification models presented accuracy differences across the
original and the optimized model variants, while $16.6$% of semantic
segmentation and object detection models are also affected, at least to a
limited extent.

</details>


### [389] [From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup](https://arxiv.org/pdf/2505.01902)
*Ali Al-Bustami, Zaid Ghazal*

Main category: cs.LG

TL;DR: A machine learning framework for predicting FIFA World Cup match outcomes using team and player data outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate match predictions are valuable for analysts, coaches, bettors, and fans.

Method: Integrates team-level and player-specific data, uses classification with dimensionality reduction and hyperparameter optimization.

Result: Superior accuracy in predicting FIFA 2022 World Cup outcomes compared to baseline methods.

Conclusion: Player-centric data enhances predictions, suggesting future use of advanced models like graph neural networks.

Abstract: Accurate prediction of FIFA World Cup match outcomes holds significant value
for analysts, coaches, bettors, and fans. This paper presents a machine
learning framework specifically designed to forecast match winners in FIFA
World Cup. By integrating both team-level historical data and player-specific
performance metrics such as goals, assists, passing accuracy, and tackles, we
capture nuanced interactions often overlooked by traditional aggregate models.
Our methodology processes multi-year data to create year-specific team profiles
that account for evolving rosters and player development. We employ
classification techniques complemented by dimensionality reduction and
hyperparameter optimization, to yield robust predictive models. Experimental
results on data from the FIFA 2022 World Cup demonstrate our approach's
superior accuracy compared to baseline method. Our findings highlight the
importance of incorporating individual player attributes and team-level
composition to enhance predictive performance, offering new insights into
player synergy, strategic match-ups, and tournament progression scenarios. This
work underscores the transformative potential of rich, player-centric data in
sports analytics, setting a foundation for future exploration of advanced
learning architectures such as graph neural networks to model complex team
interactions.

</details>


### [390] [LookAlike: Consistent Distractor Generation in Math MCQs](https://arxiv.org/pdf/2505.01903)
*Nisarg Parikh, Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan*

Main category: cs.LG

TL;DR: LookAlike improves distractor generation for math MCQs by using preference optimization and inconsistency mining, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating distractors in MCQs lack consistency with common student errors.

Method: LookAlike mines synthetic preference pairs from model inconsistencies and alternates SFT with DPO for stable training.

Result: Achieves 51.6% accuracy in distractor generation and 57.2% in error generation, outperforming prior methods.

Conclusion: Preference-based regularization and inconsistency mining effectively generate consistent distractors at scale.

Abstract: Large language models (LLMs) are increasingly used to generate distractors
for multiple-choice questions (MCQs), especially in domains like math
education. However, existing approaches are limited in ensuring that the
generated distractors are consistent with common student errors. We propose
LookAlike, a method that improves error-distractor consistency via preference
optimization. Our two main innovations are: (a) mining synthetic preference
pairs from model inconsistencies, and (b) alternating supervised fine-tuning
(SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike
prior work that relies on heuristics or manually annotated preference data,
LookAlike uses its own generation inconsistencies as dispreferred samples, thus
enabling scalable and stable training. Evaluated on a real-world dataset of
1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation
and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an
existing state-of-the-art method (45.6% / 47.7%). These improvements highlight
the effectiveness of preference-based regularization and inconsistency mining
for generating consistent math MCQ distractors at scale.

</details>


### [391] [BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models](https://arxiv.org/pdf/2505.01912)
*Evan R. Antoniuk, Shehtab Zaman, Tal Ben-Nun, Peggy Li, James Diffenderfer, Busra Demirci, Obadiah Smolenski, Tim Hsu, Anna M. Hiszpanski, Kenneth Chiu, Bhavya Kailkhura, Brian Van Essen*

Main category: cs.LG

TL;DR: BOOM benchmarks OOD performance of molecular property prediction models, revealing no model achieves strong OOD generalization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic benchmarks for molecular OOD prediction and evaluate ML models' generalization capabilities.

Method: Evaluated 140+ model-task combinations, focusing on OOD performance, and conducted ablation studies on factors like data generation and model architecture.

Result: No model performed strongly OOD; top models had 3x higher OOD error than in-distribution. High inductive bias models excelled in simple tasks, but foundation models lacked OOD extrapolation.

Conclusion: OOD generalization is a key challenge in chemical ML; BOOM provides an open-source benchmark for future research.

Abstract: Advances in deep learning and generative modeling have driven interest in
data-driven molecule discovery pipelines, whereby machine learning (ML) models
are used to filter and design novel molecules without requiring prohibitively
expensive first-principles simulations. Although the discovery of novel
molecules that extend the boundaries of known chemistry requires accurate
out-of-distribution (OOD) predictions, ML models often struggle to generalize
OOD. Furthermore, there are currently no systematic benchmarks for molecular
OOD prediction tasks. We present BOOM, $\boldsymbol{b}$enchmarks for
$\boldsymbol{o}$ut-$\boldsymbol{o}$f-distribution $\boldsymbol{m}$olecular
property predictions -- a benchmark study of property-based out-of-distribution
models for common molecular property prediction models. We evaluate more than
140 combinations of models and property prediction tasks to benchmark deep
learning models on their OOD performance. Overall, we do not find any existing
models that achieve strong OOD generalization across all tasks: even the top
performing model exhibited an average OOD error 3x larger than in-distribution.
We find that deep learning models with high inductive bias can perform well on
OOD tasks with simple, specific properties. Although chemical foundation models
with transfer and in-context learning offer a promising solution for limited
training data scenarios, we find that current foundation models do not show
strong OOD extrapolation capabilities. We perform extensive ablation
experiments to highlight how OOD performance is impacted by data generation,
pre-training, hyperparameter optimization, model architecture, and molecular
representation. We propose that developing ML models with strong OOD
generalization is a new frontier challenge in chemical ML model development.
This open-source benchmark will be made available on Github.

</details>


### [392] [Unemployment Dynamics Forecasting with Machine Learning Regression Models](https://arxiv.org/pdf/2505.01933)
*Kyungsu Kim*

Main category: cs.LG

TL;DR: The paper compares seven regression and machine learning models for forecasting U.S. unemployment, finding tree-based ensembles and LSTM outperform others, with job openings and consumer sentiment as key predictors.


<details>
  <summary>Details</summary>
Motivation: To enhance real-time unemployment forecasting by evaluating modern machine-learning techniques against traditional methods.

Method: Compared seven models (Linear Regression, SGDRegressor, Random Forest, XGBoost, CatBoost, SVR, LSTM) using macro, labor, financial, and consumer sentiment features, with hyperparameter tuning and cross-validation.

Result: Tree-based ensembles (especially CatBoost) and LSTM outperformed linear models, with job openings and consumer sentiment being the most influential predictors.

Conclusion: Modern machine-learning techniques, particularly ensembles and deep learning, improve unemployment forecasting, offering valuable insights for policymakers.

Abstract: In this paper, I explored how a range of regression and machine learning
techniques can be applied to monthly U.S. unemployment data to produce timely
forecasts. I compared seven models: Linear Regression, SGDRegressor, Random
Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network,
training each on a historical span of data and then evaluating on a later
hold-out period. Input features include macro indicators (GDP growth, CPI),
labor market measures (job openings, initial claims), financial variables
(interest rates, equity indices), and consumer sentiment.
  I tuned model hyperparameters via cross-validation and assessed performance
with standard error metrics and the ability to predict the correct unemployment
direction. Across the board, tree-based ensembles (and CatBoost in particular)
deliver noticeably better forecasts than simple linear approaches, while the
LSTM captures underlying temporal patterns more effectively than other
nonlinear methods. SVR and SGDRegressor yield modest gains over standard
regression but don't match the consistency of the ensemble and deep-learning
models.
  Interpretability tools ,feature importance rankings and SHAP values, point to
job openings and consumer sentiment as the most influential predictors across
all methods. By directly comparing linear, ensemble, and deep-learning
approaches on the same dataset, our study shows how modern machine-learning
techniques can enhance real-time unemployment forecasting, offering economists
and policymakers richer insights into labor market trends.
  In the comparative evaluation of the models, I employed a dataset comprising
thirty distinct features over the period from January 2020 through December
2024.

</details>


### [393] [Multi-Scale Graph Learning for Anti-Sparse Downscaling](https://arxiv.org/pdf/2505.01948)
*Yingda Fan, Runlong Yu, Janet R. Barclay, Alison P. Appling, Yiming Sun, Yiqun Xie, Xiaowei Jia*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Scale Graph Learning (MSGL) method to predict stream water temperature at fine spatial scales, addressing data scarcity by leveraging coarse-scale data and introducing cross-scale interpolation learning.


<details>
  <summary>Details</summary>
Motivation: Accurate fine-scale water temperature prediction is crucial for water quality and habitat protection, but existing models lack sufficient fine-scale data.

Method: MSGL uses a multi-task learning framework, integrating coarse-scale data to enhance fine-scale predictions. It introduces cross-scale interpolation learning and an asynchronous variant (ASYNC-MSGL).

Result: The method achieves state-of-the-art performance in downscaling daily stream temperatures in the Delaware River Basin.

Conclusion: MSGL and ASYNC-MSGL show promise for improving water resources monitoring and management.

Abstract: Water temperature can vary substantially even across short distances within
the same sub-watershed. Accurate prediction of stream water temperature at fine
spatial resolutions (i.e., fine scales, $\leq$ 1 km) enables precise
interventions to maintain water quality and protect aquatic habitats. Although
spatiotemporal models have made substantial progress in spatially coarse time
series modeling, challenges persist in predicting at fine spatial scales due to
the lack of data at that scale.To address the problem of insufficient
fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This
method employs a multi-task learning framework where coarse-scale graph
learning, bolstered by larger datasets, simultaneously enhances fine-scale
graph learning. Although existing multi-scale or multi-resolution methods
integrate data from different spatial scales, they often overlook the spatial
correspondences across graph structures at various scales. To address this, our
MSGL introduces an additional learning task, cross-scale interpolation
learning, which leverages the hydrological connectedness of stream locations
across coarse- and fine-scale graphs to establish cross-scale connections,
thereby enhancing overall model performance. Furthermore, we have broken free
from the mindset that multi-scale learning is limited to synchronous training
by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL).
Extensive experiments demonstrate the state-of-the-art performance of our
method for anti-sparse downscaling of daily stream temperatures in the Delaware
River Basin, USA, highlighting its potential utility for water resources
monitoring and management.

</details>


### [394] [Semantic Probabilistic Control of Language Models](https://arxiv.org/pdf/2505.01954)
*Kareem Ahmed, Catarina G Belem, Padhraic Smyth, Sameer Singh*

Main category: cs.LG

TL;DR: The paper introduces a method for semantic control of language models (LMs) using verifier gradients to steer generations towards non-lexical constraints like toxicity or sentiment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LM control either handle only syntactic constraints or rely on inefficient sampling for low-probability events, limiting their effectiveness for semantic attributes.

Method: The approach leverages verifier gradients to approximate the conditional LM distribution, reweighing next-token probabilities based on expected sentence embeddings informed by verifier evaluations.

Result: The method achieves high-probability (>95%) control over toxicity, sentiment, and topic-adherence without degrading generation quality.

Conclusion: The proposed gradient-based approach effectively enables precise semantic control of LMs, addressing limitations of prior methods.

Abstract: Semantic control entails steering LM generations towards satisfying subtle
non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes
that can be captured by a sequence-level verifier. It can thus be viewed as
sampling from the LM distribution conditioned on the target attribute, a
computationally intractable problem due to the non-decomposable nature of the
verifier. Existing approaches to LM control either only deal with syntactic
constraints which cannot capture the aforementioned attributes, or rely on
sampling to explore the conditional LM distribution, an ineffective estimator
for low-probability events. In this work, we leverage a verifier's gradient
information to efficiently reason over all generations that satisfy the target
attribute, enabling precise steering of LM generations by reweighing the
next-token distribution. Starting from an initial sample, we create a local LM
distribution favoring semantically similar sentences. This approximation
enables the tractable computation of an expected sentence embedding. We use
this expected embedding, informed by the verifier's evaluation at the initial
sample, to estimate the probability of satisfying the constraint, which
directly informs the update to the next-token distribution. We evaluated the
effectiveness of our approach in controlling the toxicity, sentiment, and
topic-adherence of LMs yielding generations satisfying the constraint with high
probability (>95%) without degrading their quality.

</details>


### [395] [EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting](https://arxiv.org/pdf/2505.01959)
*Leyi Yan, Linda Wang, Sihang Liu, Yi Ding*

Main category: cs.LG

TL;DR: EnsembleCI, an adaptive ensemble learning method, outperforms CarbonCast in carbon intensity (CI) forecasting by addressing regional variability and improving accuracy by 19.58% on average.


<details>
  <summary>Details</summary>
Motivation: Current methods like CarbonCast lack regional adaptability and accuracy in CI predictions, which are critical for managing environmental impact.

Method: EnsembleCI uses weighted predictions from multiple sublearners for flexible and regionally adaptable CI forecasting.

Result: Evaluations across 11 grids show EnsembleCI achieves lower MAPE than CarbonCast, with 19.58% better accuracy and reduced variability.

Conclusion: EnsembleCI is a more accurate, robust, and interpretable solution for CI forecasting, with practical relevance and open-source availability.

Abstract: Carbon intensity (CI) measures the average carbon emissions generated per
unit of electricity, making it a crucial metric for quantifying and managing
the environmental impact. Accurate CI predictions are vital for minimizing
carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due
to its inability to address regional variability and lack of adaptability. To
address these limitations, we introduce EnsembleCI, an adaptive, end-to-end
ensemble learning-based approach for CI forecasting. EnsembleCI combines
weighted predictions from multiple sublearners, offering enhanced flexibility
and regional adaptability. In evaluations across 11 regional grids, EnsembleCI
consistently surpasses CarbonCast, achieving the lowest mean absolute
percentage error (MAPE) in almost all grids and improving prediction accuracy
by an average of 19.58%. While performance still varies across grids due to
inherent regional diversity, EnsembleCI reduces variability and exhibits
greater robustness in long-term forecasting compared to CarbonCast and
identifies region-specific key features, underscoring its interpretability and
practical relevance. These findings position EnsembleCI as a more accurate and
reliable solution for CI forecasting. EnsembleCI source code and data used in
this paper are available at https://github.com/emmayly/EnsembleCI.

</details>


### [396] [D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection](https://arxiv.org/pdf/2505.01979)
*Chenran Zhao, Dianxi Shi, Mengzhu Wang, Jianqiang Xia, Huanhuan Yang, Songchang Jin, Shaowu Yang, Chunping Qiu*

Main category: cs.LG

TL;DR: D3HRL is a causal HRL method addressing delay effects and spurious correlations via distributed causal discovery and conditional independence testing, improving decision-making in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current HRL algorithms struggle with delay effects and spurious correlations, limiting their effectiveness in long-horizon tasks.

Method: D3HRL models delay effects as causal relationships, uses distributed causal discovery, and applies conditional independence testing to eliminate spurious correlations. Hierarchical policies are trained based on identified causal relationships.

Result: Experiments in 2D-MineCraft and MiniGrid show D3HRL's superior sensitivity to delay effects and accurate causal relationship identification.

Conclusion: D3HRL enhances decision-making in complex environments by iteratively exploring causal chains and eliminating spurious correlations.

Abstract: Current Hierarchical Reinforcement Learning (HRL) algorithms excel in
long-horizon sequential decision-making tasks but still face two challenges:
delay effects and spurious correlations. To address them, we propose a causal
HRL approach called D3HRL. First, D3HRL models delayed effects as causal
relationships across different time spans and employs distributed causal
discovery to learn these relationships. Second, it employs conditional
independence testing to eliminate spurious correlations. Finally, D3HRL
constructs and trains hierarchical policies based on the identified true causal
relationships. These three steps are iteratively executed, gradually exploring
the complete causal chain of the task. Experiments conducted in 2D-MineCraft
and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects
and accurately identifies causal relationships, leading to reliable
decision-making in complex environments.

</details>


### [397] [Always Skip Attention](https://arxiv.org/pdf/2505.01996)
*Yiping Ji, Hemanth Saratchandran, Peyman Moghaddam, Simon Lucey*

Main category: cs.LG

TL;DR: Modern Vision Transformers (ViTs) critically depend on skip connections for training self-attention, unlike other components or older architectures like CNNs. The paper explains this phenomenon theoretically and introduces Token Graying as a complementary solution.


<details>
  <summary>Details</summary>
Motivation: To understand why self-attention in ViTs fails without skip connections, unlike other components or traditional architectures, and to propose a solution.

Method: Theoretical analysis of self-attention's ill-conditioning and introduction of Token Graying to improve input token conditioning. Validation in supervised and self-supervised training.

Result: Self-attention in ViTs is uniquely dependent on skip connections due to fundamental ill-conditioning. Token Graying effectively complements skip connections.

Conclusion: Skip connections are essential for ViTs' self-attention, and Token Graying enhances their stability and performance.

Abstract: We highlight a curious empirical result within modern Vision Transformers
(ViTs). Specifically, self-attention catastrophically fails to train unless it
is used in conjunction with a skip connection. This is in contrast to other
elements of a ViT that continue to exhibit good performance (albeit suboptimal)
when skip connections are removed. Further, we show that this critical
dependence on skip connections is a relatively new phenomenon, with previous
deep architectures (\eg, CNNs) exhibiting good performance in their absence. In
this paper, we theoretically characterize that the self-attention mechanism is
fundamentally ill-conditioned and is, therefore, uniquely dependent on skip
connections for regularization. Additionally, we propose Token Graying -- a
simple yet effective complement (to skip connections) that further improves the
condition of input tokens. We validate our approach in both supervised and
self-supervised training methods.

</details>


### [398] [Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach](https://arxiv.org/pdf/2505.01997)
*Jiancong Xiao, Bojian Hou, Zhanliang Wang, Ruochen Jin, Qi Long, Weijie J. Su, Li Shen*

Main category: cs.LG

TL;DR: Preference alignment in LLMs causes poor calibration. The paper investigates this issue, identifies preference collapse as the cause, and proposes domain-specific fine-tuning and calibration-aware methods to address it.


<details>
  <summary>Details</summary>
Motivation: To understand why preference alignment in LLMs leads to poor calibration and to develop solutions to mitigate this issue without compromising model performance.

Method: Analyzes preference collapse, categorizes models into calibratable and non-calibratable regimes, and proposes calibration-aware fine-tuning and EM-algorithm-based ECE regularization.

Result: Demonstrates that domain-specific fine-tuning and calibration-aware methods effectively address poor calibration while maintaining performance.

Conclusion: The proposed methods successfully mitigate calibration issues in LLMs post-preference alignment, validated through extensive experiments.

Abstract: One of the key technologies for the success of Large Language Models (LLMs)
is preference alignment. However, a notable side effect of preference alignment
is poor calibration: while the pre-trained models are typically
well-calibrated, LLMs tend to become poorly calibrated after alignment with
human preferences. In this paper, we investigate why preference alignment
affects calibration and how to address this issue. For the first question, we
observe that the preference collapse issue in alignment undesirably generalizes
to the calibration scenario, causing LLMs to exhibit overconfidence and poor
calibration. To address this, we demonstrate the importance of fine-tuning with
domain-specific knowledge to alleviate the overconfidence issue. To further
analyze whether this affects the model's performance, we categorize models into
two regimes: calibratable and non-calibratable, defined by bounds of Expected
Calibration Error (ECE). In the calibratable regime, we propose a
calibration-aware fine-tuning approach to achieve proper calibration without
compromising LLMs' performance. However, as models are further fine-tuned for
better performance, they enter the non-calibratable regime. For this case, we
develop an EM-algorithm-based ECE regularization for the fine-tuning loss to
maintain low calibration error. Extensive experiments validate the
effectiveness of the proposed methods.

</details>


### [399] [CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting](https://arxiv.org/pdf/2505.02011)
*Minhyuk Lee, HyeKyung Yoon, MyungJoo Kang*

Main category: cs.LG

TL;DR: The paper introduces CASA, a CNN Autoencoder-based Score Attention mechanism, to improve Transformer variants for multivariate long-term time series forecasting, reducing computational costs and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in time complexity, computational resources, and cross-dimensional interactions in existing Transformer-based forecasting methods.

Method: Proposes CASA, a model-agnostic attention mechanism integrated into Transformers, leveraging CNN Autoencoder for efficiency.

Result: CASA reduces computational resources by 77.7%, speeds up inference by 44.0%, and achieves state-of-the-art performance on eight datasets.

Conclusion: CASA effectively enhances Transformer-based forecasting models, offering significant efficiency and performance improvements.

Abstract: Multivariate long-term time series forecasting is critical for applications
such as weather prediction, and traffic analysis. In addition, the
implementation of Transformer variants has improved prediction accuracy.
Following these variants, different input data process approaches also enhanced
the field, such as tokenization techniques including point-wise, channel-wise,
and patch-wise tokenization. However, previous studies still have limitations
in time complexity, computational resources, and cross-dimensional
interactions. To address these limitations, we introduce a novel CNN
Autoencoder-based Score Attention mechanism (CASA), which can be introduced in
diverse Transformers model-agnosticically by reducing memory and leading to
improvement in model performance. Experiments on eight real-world datasets
validate that CASA decreases computational resources by up to 77.7%,
accelerates inference by 44.0%, and achieves state-of-the-art performance,
ranking first in 87.5% of evaluated metrics.

</details>


### [400] [Wide & Deep Learning for Node Classification](https://arxiv.org/pdf/2505.02020)
*Yancheng Chen, Wenguo Yang, Zhipeng Jiang*

Main category: cs.LG

TL;DR: The paper introduces GCNIII, a framework combining Wide & Deep architecture with three techniques to improve GCN performance in node classification, addressing issues like heterophily and expressiveness.


<details>
  <summary>Details</summary>
Motivation: To address limitations in GCNs (e.g., heterophily, expressiveness) by leveraging the Wide & Deep architecture and enhancing node feature utilization.

Method: Proposes GCNIII, incorporating Intersect memory, Initial residual, and Identity mapping, and explores LLMs for node feature engineering.

Result: GCNIII effectively balances over-fitting and over-generalization, improving performance in semi- and full-supervised tasks.

Conclusion: GCNIII offers a flexible solution for node classification, with potential for cross-domain applications via LLM-enhanced feature engineering.

Abstract: Wide & Deep, a simple yet effective learning architecture for recommendation
systems developed by Google, has had a significant impact in both academia and
industry due to its combination of the memorization ability of generalized
linear models and the generalization ability of deep models. Graph
convolutional networks (GCNs) remain dominant in node classification tasks;
however, recent studies have highlighted issues such as heterophily and
expressiveness, which focus on graph structure while seemingly neglecting the
potential role of node features. In this paper, we propose a flexible framework
GCNIII, which leverages the Wide & Deep architecture and incorporates three
techniques: Intersect memory, Initial residual and Identity mapping. We provide
comprehensive empirical evidence showing that GCNIII can more effectively
balance the trade-off between over-fitting and over-generalization on various
semi- and full- supervised tasks. Additionally, we explore the use of large
language models (LLMs) for node feature engineering to enhance the performance
of GCNIII in cross-domain node classification tasks. Our implementation is
available at https://github.com/CYCUCAS/GCNIII.

</details>


### [401] [NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks](https://arxiv.org/pdf/2505.02022)
*Yiming Zhang, Koji Tsuda*

Main category: cs.LG

TL;DR: NbBench is introduced as the first comprehensive benchmark for nanobody representation learning, evaluating 11 models across 8 tasks, revealing no single model excels in all areas.


<details>
  <summary>Details</summary>
Motivation: Nanobody-specific modeling lacks a unified benchmark despite their therapeutic and diagnostic potential, prompting the creation of NbBench.

Method: NbBench spans 8 tasks across 9 datasets, evaluating 11 models (protein, antibody, and nanobody-specific LMs) in a frozen setting.

Result: Antibody LMs perform well in antigen-related tasks, but regression tasks like thermostability and affinity remain challenging. No model consistently outperforms others.

Conclusion: NbBench standardizes evaluation for nanobody modeling, providing a reproducible foundation for future advancements.

Abstract: Nanobodies, single-domain antibody fragments derived from camelid
heavy-chain-only antibodies, exhibit unique advantages such as compact size,
high stability, and strong binding affinity, making them valuable tools in
therapeutics and diagnostics. While recent advances in pretrained protein and
antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular
understanding, nanobody-specific modeling remains underexplored and lacks a
unified benchmark. To address this gap, we introduce NbBench, the first
comprehensive benchmark suite for nanobody representation learning. Spanning
eight biologically meaningful tasks across nine curated datasets, NbBench
encompasses structure annotation, binding prediction, and developability
assessment. We systematically evaluate eleven representative models--including
general-purpose protein LMs, antibody-specific LMs, and nanobody-specific
LMs--in a frozen setting. Our analysis reveals that antibody language models
excel in antigen-related tasks, while performance on regression tasks such as
thermostability and affinity remains challenging across all models. Notably, no
single model consistently outperforms others across all tasks. By standardizing
datasets, task definitions, and evaluation protocols, NbBench offers a
reproducible foundation for assessing and advancing nanobody modeling.

</details>


### [402] [GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning](https://arxiv.org/pdf/2505.02027)
*Rui Lv, Zaixi Zhang, Kai Zhang, Qi Liu, Weibo Gao, Jiawei Liu, Jiaxia Yan, Linan Yue, Fangzhou Yao*

Main category: cs.LG

TL;DR: GraphPrompter enhances graph in-context learning by optimizing prompt generation, selection, and augmentation, outperforming baselines by 8%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for graph in-context learning suffer from noisy prompts and performance degradation when testing graphs have more classes than training.

Method: GraphPrompter uses a multi-stage approach: Prompt Generator highlights informative edges, Prompt Selector dynamically chooses samples, and Prompt Augmenter improves generalization.

Result: GraphPrompter outperforms state-of-the-art baselines by over 8% on average.

Conclusion: The proposed method effectively improves in-context learning for graph models, addressing key challenges in prompt optimization.

Abstract: Graph In-Context Learning, with the ability to adapt pre-trained graph models
to novel and diverse downstream graphs without updating any parameters, has
gained much attention in the community. The key to graph in-context learning is
to perform downstream graphs conditioned on chosen prompt examples. Existing
methods randomly select subgraphs or edges as prompts, leading to noisy graph
prompts and inferior model performance. Additionally, due to the gap between
pre-training and testing graphs, when the number of classes in the testing
graphs is much greater than that in the training, the in-context learning
ability will also significantly deteriorate. To tackle the aforementioned
challenges, we develop a multi-stage adaptive prompt optimization method
GraphPrompter, which optimizes the entire process of generating, selecting, and
using graph prompts for better in-context learning capabilities. Firstly,
Prompt Generator introduces a reconstruction layer to highlight the most
informative edges and reduce irrelevant noise for graph prompt construction.
Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest
neighbors algorithm and pre-trained selection layers to dynamically choose
appropriate samples and minimize the influence of irrelevant prompts. Finally,
we leverage a Prompt Augmenter with a cache replacement strategy to enhance the
generalization capability of the pre-trained model on new datasets. Extensive
experiments show that GraphPrompter effectively enhances the in-context
learning ability of graph models. On average across all the settings, our
approach surpasses the state-of-the-art baselines by over 8%. Our code is
released at https://github.com/karin0018/GraphPrompter.

</details>


### [403] [Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles](https://arxiv.org/pdf/2505.02033)
*Emine Akpinar, Batuhan Hangun, Murat Oduncuoglu, Oguz Altun, Onder Eyecioglu, Zeynel Yalcin*

Main category: cs.LG

TL;DR: The paper proposes 'Deep VQC,' a quantum AI model, to classify brain tumors using gene expression data, outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: Classical AI methods face limitations in handling high-dimensional gene data, prompting exploration of quantum computing for more efficient analysis.

Method: The study introduces 'Deep VQC,' a Variational Quantum Classifier, applied to microarray data with 54,676 gene features to classify four brain tumor types.

Result: Deep VQC achieved high accuracy in tumor classification, performing comparably or better than classical ML algorithms.

Conclusion: Quantum AI methods like Deep VQC show promise for analyzing complex gene expression data and improving tumor classification.

Abstract: DNA microarray technology enables the simultaneous measurement of expression
levels of thousands of genes, thereby facilitating the understanding of the
molecular mechanisms underlying complex diseases such as brain tumors and the
identification of diagnostic genetic signatures. To derive meaningful
biological insights from the high-dimensional and complex gene features
obtained through this technology and to analyze gene properties in detail,
classical AI-based approaches such as machine learning and deep learning are
widely employed. However, these methods face various limitations in managing
high-dimensional vector spaces and modeling the intricate relationships among
genes. In particular, challenges such as hyperparameter tuning, computational
costs, and high processing power requirements can hinder their efficiency. To
overcome these limitations, quantum computing and quantum AI approaches are
gaining increasing attention. Leveraging quantum properties such as
superposition and entanglement, quantum methods enable more efficient parallel
processing of high-dimensional data and offer faster and more effective
solutions to problems that are computationally demanding for classical methods.
In this study, a novel model called "Deep VQC" is proposed, based on the
Variational Quantum Classifier approach. Developed using microarray data
containing 54,676 gene features, the model successfully classified four
different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and
pilocytic astrocytoma-alongside healthy samples with high accuracy.
Furthermore, compared to classical ML algorithms, our model demonstrated either
superior or comparable classification performance. These results highlight the
potential of quantum AI methods as an effective and promising approach for the
analysis and classification of complex structures such as brain tumors based on
gene expression features.

</details>


### [404] [Secrets of GFlowNets' Learning Behavior: A Theoretical Study](https://arxiv.org/pdf/2505.02035)
*Tianshu Yu*

Main category: cs.LG

TL;DR: The paper provides a theoretical analysis of GFlowNets' learning behavior, covering convergence, sample complexity, implicit regularization, and robustness, aiming to enhance their design and deployment.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of GFlowNets' learning dynamics and their strengths/limitations.

Method: Rigorous theoretical investigation focusing on four key dimensions: convergence, sample complexity, implicit regularization, and robustness.

Result: Findings offer insights into GFlowNet performance factors and guidelines for their effective use.

Conclusion: The study bridges a theoretical gap, advancing GFlowNets as a reliable and interpretable generative modeling framework.

Abstract: Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for
generating composite structures, demonstrating considerable promise across
diverse applications. While substantial progress has been made in exploring
their modeling validity and connections to other generative frameworks, the
theoretical understanding of their learning behavior remains largely uncharted.
In this work, we present a rigorous theoretical investigation of GFlowNets'
learning behavior, focusing on four fundamental dimensions: convergence, sample
complexity, implicit regularization, and robustness. By analyzing these
aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's
learning dynamics, shedding light on its strengths and limitations. Our
findings contribute to a deeper understanding of the factors influencing
GFlowNet performance and provide insights into principled guidelines for their
effective design and deployment. This study not only bridges a critical gap in
the theoretical landscape of GFlowNets but also lays the foundation for their
evolution as a reliable and interpretable framework for generative modeling.
Through this, we aspire to advance the theoretical frontiers of GFlowNets and
catalyze their broader adoption in the AI community.

</details>


### [405] [Neural Logistic Bandits](https://arxiv.org/pdf/2505.02069)
*Seoungbin Bae, Dabeen Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of neural logistic bandits, where the main task is to
learn an unknown reward function within a logistic link function using a neural
network. Existing approaches either exhibit unfavorable dependencies on
$\kappa$, where $1/\kappa$ represents the minimum variance of reward
distributions, or suffer from direct dependence on the feature dimension $d$,
which can be huge in neural network-based settings. In this work, we introduce
a novel Bernstein-type inequality for self-normalized vector-valued martingales
that is designed to bypass a direct dependence on the ambient dimension. This
lets us deduce a regret upper bound that grows with the effective dimension
$\widetilde{d}$, not the feature dimension, while keeping a minimal dependence
on $\kappa$. Based on the concentration inequality, we propose two algorithms,
NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of
order $\widetilde{O}(\widetilde{d}\sqrt{\kappa T})$ and
$\widetilde{O}(\widetilde{d}\sqrt{T/\kappa})$, respectively, improving on the
existing results. Lastly, we report numerical results on both synthetic and
real datasets to validate our theoretical findings.

</details>


### [406] [Lightweight Defense Against Adversarial Attacks in Time Series Classification](https://arxiv.org/pdf/2505.02073)
*Yi Han*

Main category: cs.LG

TL;DR: The paper introduces five efficient data augmentation-based defense methods for time series classification (TSC) against adversarial attacks, outperforming adversarial training (AT) in performance and computational cost.


<details>
  <summary>Details</summary>
Motivation: Adversarial defense in TSC is understudied compared to CV, and existing methods like AT are computationally expensive. This work aims to provide efficient alternatives.

Method: Developed five data augmentation-based defense methods, including an ensemble approach, tailored for TSC. These methods are computationally efficient and easy to deploy.

Result: The ensemble method outperforms PGD-based AT in defense performance and generalization, using less than one-third of the computational resources.

Conclusion: The proposed methods advance robust TSC and offer insights for integrating data augmentation-based defense with large-scale pre-trained models.

Abstract: As time series classification (TSC) gains prominence, ensuring robust TSC
models against adversarial attacks is crucial. While adversarial defense is
well-studied in Computer Vision (CV), the TSC field has primarily relied on
adversarial training (AT), which is computationally expensive. In this paper,
five data augmentation-based defense methods tailored for time series are
developed, with the most computationally intensive method among them increasing
the computational resources by only 14.07% compared to the original TSC model.
Moreover, the deployment process for these methods is straightforward. By
leveraging these advantages of our methods, we create two combined methods. One
of these methods is an ensemble of all the proposed techniques, which not only
provides better defense performance than PGD-based AT but also enhances the
generalization ability of TSC models. Moreover, the computational resources
required for our ensemble are less than one-third of those required for
PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as
foundation models are increasingly explored for time series feature learning,
our work provides insights into integrating data augmentation-based adversarial
defense with large-scale pre-trained models in future research.

</details>


### [407] [Learning Local Causal World Models with State Space Models and Attention](https://arxiv.org/pdf/2505.02074)
*Francesco Petri, Luigi Asprino, Aldo Gangemi*

Main category: cs.LG

TL;DR: SSMs outperform Transformers in learning causal models for world dynamics.


<details>
  <summary>Details</summary>
Motivation: To explore the intersection of causality theory and neural world modelling, addressing the gap in learning causal representations.

Method: Assess the causal discovery potential of State Space Models (SSMs) compared to Transformers in a simple environment.

Result: SSMs model dynamics and learn causal models with equivalent or better performance than Transformers.

Conclusion: SSMs show promise for causal-aware world modelling, encouraging further research.

Abstract: World modelling, i.e. building a representation of the rules that govern the
world so as to predict its evolution, is an essential ability for any agent
interacting with the physical world. Despite their impressive performance, many
solutions fail to learn a causal representation of the environment they are
trying to model, which would be necessary to gain a deep enough understanding
of the world to perform complex tasks. With this work, we aim to broaden the
research in the intersection of causality theory and neural world modelling by
assessing the potential for causal discovery of the State Space Model (SSM)
architecture, which has been shown to have several advantages over the
widespread Transformer. We show empirically that, compared to an equivalent
Transformer, a SSM can model the dynamics of a simple environment and learn a
causal model at the same time with equivalent or better performance, thus
paving the way for further experiments that lean into the strength of SSMs and
further enhance them with causal awareness.

</details>


### [408] [SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations](https://arxiv.org/pdf/2505.02094)
*Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, Qifeng Chen*

Main category: cs.LG

TL;DR: The paper addresses noise and coverage issues in RLID by proposing data augmentation techniques (STG and STF) and adaptive learning strategies (ATS and historical encoding) to improve skill acquisition and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing RLID methods suffer from noisy, sparse demonstrations, limiting their ability to capture skill variations and transitions.

Method: The authors introduce Stitched Trajectory Graph (STG) and State Transition Field (STF) for data augmentation, along with Adaptive Trajectory Sampling (ATS) and historical encoding for learning.

Result: Experiments show significant improvements in convergence stability, generalization, and robustness compared to state-of-the-art methods.

Conclusion: The proposed approach effectively bridges gaps in demonstrations, enabling robust and generalizable skill acquisition in RLID.

Abstract: We address a fundamental challenge in Reinforcement Learning from Interaction
Demonstration (RLID): demonstration noise and coverage limitations. While
existing data collection approaches provide valuable interaction
demonstrations, they often yield sparse, disconnected, and noisy trajectories
that fail to capture the full spectrum of possible skill variations and
transitions. Our key insight is that despite noisy and sparse demonstrations,
there exist infinite physically feasible trajectories that naturally bridge
between demonstrated skills or emerge from their neighboring states, forming a
continuous space of possible skill variations and transitions. Building upon
this insight, we present two data augmentation techniques: a Stitched
Trajectory Graph (STG) that discovers potential transitions between
demonstration skills, and a State Transition Field (STF) that establishes
unique connections for arbitrary states within the demonstration neighborhood.
To enable effective RLID with augmented data, we develop an Adaptive Trajectory
Sampling (ATS) strategy for dynamic curriculum generation and a historical
encoding mechanism for memory-dependent skill learning. Our approach enables
robust skill acquisition that significantly generalizes beyond the reference
demonstrations. Extensive experiments across diverse interaction tasks
demonstrate substantial improvements over state-of-the-art methods in terms of
convergence stability, generalization capability, and recovery robustness.

</details>


### [409] [Deep Representation Learning for Electronic Design Automation](https://arxiv.org/pdf/2505.02105)
*Pratik Shrestha, Saran Phatharodom, Alec Aversa, David Blankenship, Zhengfeng Wu, Ioannis Savidis*

Main category: cs.LG

TL;DR: Representation learning in EDA improves feature extraction from complex data (images, grids, graphs) to address circuit complexity and PPA requirements, enhancing tasks like timing prediction and routing.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges of increasing circuit complexity and strict PPA requirements in electronic design automation (EDA).

Method: Utilizes image-based, graph-based, and hybrid multimodal representation learning techniques.

Result: Enhances efficiency, accuracy, and scalability in tasks like timing prediction, routability analysis, and placement.

Conclusion: Representation learning shows significant potential to improve current integrated circuit design workflows.

Abstract: Representation learning has become an effective technique utilized by
electronic design automation (EDA) algorithms, which leverage the natural
representation of workflow elements as images, grids, and graphs. By addressing
challenges related to the increasing complexity of circuits and stringent
power, performance, and area (PPA) requirements, representation learning
facilitates the automatic extraction of meaningful features from complex data
formats, including images, grids, and graphs. This paper examines the
application of representation learning in EDA, covering foundational concepts
and analyzing prior work and case studies on tasks that include timing
prediction, routability analysis, and automated placement. Key techniques,
including image-based methods, graph-based approaches, and hybrid multimodal
solutions, are presented to illustrate the improvements provided in routing,
timing, and parasitic prediction. The provided advancements demonstrate the
potential of representation learning to enhance efficiency, accuracy, and
scalability in current integrated circuit design flows.

</details>


### [410] [GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code](https://arxiv.org/pdf/2505.02124)
*Samidha Verma, Arushi Goyal, Ananya Mathur, Ankit Anand, Sayan Ranu*

Main category: cs.LG

TL;DR: GRAIL introduces a novel approach using LLMs and prompt tuning to generate programs for computing GED, addressing limitations of neural methods like data dependency and interpretability.


<details>
  <summary>Details</summary>
Motivation: Neural methods for GED computation face challenges like data dependency, lack of interpretability, and poor cross-domain generalization.

Method: GRAIL uses large language models and automated prompt tuning to generate programs for GED computation, avoiding direct prediction.

Result: GRAIL outperforms state-of-the-art methods in prediction quality and achieves robust cross-domain generalization.

Conclusion: GRAIL offers a scalable, interpretable, and generalizable solution for GED computation, overcoming key limitations of existing approaches.

Abstract: Graph Edit Distance (GED) is a widely used metric for measuring similarity
between two graphs. Computing the optimal GED is NP-hard, leading to the
development of various neural and non-neural heuristics. While neural methods
have achieved improved approximation quality compared to non-neural approaches,
they face significant challenges: (1) They require large amounts of ground
truth data, which is itself NP-hard to compute. (2) They operate as black
boxes, offering limited interpretability. (3) They lack cross-domain
generalization, necessitating expensive retraining for each new dataset. We
address these limitations with GRAIL, introducing a paradigm shift in this
domain. Instead of training a neural model to predict GED, GRAIL employs a
novel combination of large language models (LLMs) and automated prompt tuning
to generate a program that is used to compute GED. This shift from predicting
GED to generating programs imparts various advantages, including end-to-end
interpretability and an autonomous self-evolutionary learning mechanism without
ground-truth supervision. Extensive experiments on seven datasets confirm that
GRAIL not only surpasses state-of-the-art GED approximation methods in
prediction quality but also achieves robust cross-domain generalization across
diverse graph distributions.

</details>


### [411] [Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation](https://arxiv.org/pdf/2505.02138)
*Chenxi Liu, Hao Miao, Qianxiong Xu, Shaowen Zhou, Cheng Long, Yan Zhao, Ziyue Li, Rui Zhao*

Main category: cs.LG

TL;DR: TimeKD is an efficient MTSF framework using calibrated language models and knowledge distillation to improve forecasting while addressing LLM inefficiency.


<details>
  <summary>Details</summary>
Motivation: To tackle the low efficiency of LLMs in MTSF inference, TimeKD leverages knowledge distillation and cross-modality learning.

Method: Uses a cross-modality teacher model with calibrated language models and subtractive cross attention, plus privileged knowledge distillation for the student model.

Result: Demonstrates effectiveness, efficiency, and scalability in real-world experiments.

Conclusion: TimeKD successfully enhances MTSF performance by combining CLMs and knowledge distillation.

Abstract: Multivariate time series forecasting (MTSF) endeavors to predict future
observations given historical data, playing a crucial role in time series data
management systems. With advancements in large language models (LLMs), recent
studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.
However, the deployment of LLMs often suffers from low efficiency during the
inference phase. To address this problem, we introduce TimeKD, an efficient
MTSF framework that leverages the calibrated language models and privileged
knowledge distillation. TimeKD aims to generate high-quality future
representations from the proposed cross-modality teacher model and cultivate an
effective student model. The cross-modality teacher model adopts calibrated
language models (CLMs) with ground truth prompts, motivated by the paradigm of
Learning Under Privileged Information (LUPI). In addition, we design a
subtractive cross attention (SCA) mechanism to refine these representations. To
cultivate an effective student model, we propose an innovative privileged
knowledge distillation (PKD) mechanism including correlation and feature
distillation. PKD enables the student to replicate the teacher's behavior while
minimizing their output discrepancy. Extensive experiments on real data offer
insight into the effectiveness, efficiency, and scalability of the proposed
TimeKD.

</details>


### [412] [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/pdf/2505.02206)
*Lei Mao, Yuanhe Tian, Yan Song*

Main category: cs.LG

TL;DR: DNAZEN is a genomic representation framework that learns from gene sequence granularities (small polymers and G-grams) using a Transformer-based encoder and whole G-gram masking, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing gene sequence models mimic language modeling but overlook intrinsic granularity-based information organization, limiting representation effectiveness.

Method: DNAZEN extracts G-grams unsupervisedly, uses a Transformer-based encoder for representation, and employs whole G-gram masking for training.

Result: DNAZEN shows superior performance on benchmark datasets for downstream tasks.

Conclusion: DNAZEN advances genomic representation by leveraging multi-granularity learning and innovative masking, proving effective for gene sequence analysis.

Abstract: Genome modeling conventionally treats gene sequence as a language, reflecting
its structured motifs and long-range dependencies analogous to linguistic units
and organization principles such as words and syntax. Recent studies utilize
advanced neural networks, ranging from convolutional and recurrent models to
Transformer-based models, to capture contextual information of gene sequence,
with the primary goal of obtaining effective gene sequence representations and
thus enhance the models' understanding of various running gene samples.
However, these approaches often directly apply language modeling techniques to
gene sequences and do not fully consider the intrinsic information organization
in them, where they do not consider how units at different granularities
contribute to representation. In this paper, we propose DNAZEN, an enhanced
genomic representation framework designed to learn from various granularities
in gene sequences, including small polymers and G-grams that are combinations
of several contiguous polymers. Specifically, we extract the G-grams from
large-scale genomic corpora through an unsupervised approach to construct the
G-gram vocabulary, which is used to provide G-grams in the learning process of
DNA sequences through dynamically matching from running gene samples. A
Transformer-based G-gram encoder is also proposed and the matched G-grams are
fed into it to compute their representations and integrated into the encoder
for basic unit (E4BU), which is responsible for encoding small units and
maintaining the learning and inference process. To further enhance the learning
process, we propose whole G-gram masking to train DNAZEN, where the model
largely favors the selection of each entire G-gram to mask rather than an
ordinary masking mechanism performed on basic units. Experiments on benchmark
datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.

</details>


### [413] [Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora](https://arxiv.org/pdf/2505.02147)
*Prajwal Thapa, Mridul Sharma, Jinu Nyachhyon, Yagya Raj Pandeya*

Main category: cs.LG

TL;DR: A deep learning approach using CNNs and transfer learning is proposed for classifying 60 herb species in Nepal, with DenseNet121 outperforming other models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of herb classification in biodiversity-rich regions like Nepal to preserve traditional knowledge and promote sustainable herb use.

Method: Employed CNNs and transfer learning with architectures like DenseNet121, ResNet50, VGG16, InceptionV3, EfficientNetV2, and VIT, using a dataset of 12,000 herb images. Data augmentation and regularization were applied.

Result: DenseNet121 showed superior performance among the tested models.

Conclusion: The study advances herb classification techniques, supporting botanical research and sustainable herb utilization.

Abstract: Herb classification presents a critical challenge in botanical research,
particularly in regions with rich biodiversity such as Nepal. This study
introduces a novel deep learning approach for classifying 60 different herb
species using Convolutional Neural Networks (CNNs) and transfer learning
techniques. Using a manually curated dataset of 12,000 herb images, we
developed a robust machine learning model that addresses existing limitations
in herb recognition methodologies. Our research employed multiple model
architectures, including DenseNet121, 50-layer Residual Network (ResNet50),
16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,
and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating
superior performance. Data augmentation and regularization techniques were
applied to mitigate overfitting and enhance the generalizability of the model.
This work advances herb classification techniques, preserving traditional
botanical knowledge and promoting sustainable herb utilization.

</details>


### [414] [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/pdf/2505.02309)
*Sanjay Surendranath Girija, Shashank Kapoor, Lakshit Arora, Dipen Pradhan, Aman Raj, Ankit Shetgaonkar*

Main category: cs.LG

TL;DR: A survey on compressing Large Language Models (LLMs) for efficient edge deployment, covering Knowledge Distillation, Model Quantization, and Model Pruning.


<details>
  <summary>Details</summary>
Motivation: LLMs' high resource demands hinder deployment on mobile and edge devices, necessitating compression techniques.

Method: Examines Knowledge Distillation, Model Quantization, and Model Pruning, along with complementary strategies like mixture-of-experts and early-exit.

Result: Provides a comprehensive overview of compression techniques, their variants, and successful applications.

Conclusion: Highlights future directions for optimizing LLMs in resource-constrained environments, serving as a resource for researchers and practitioners.

Abstract: Large Language Models (LLMs) have revolutionized many areas of artificial
intelligence (AI), but their substantial resource requirements limit their
deployment on mobile and edge devices. This survey paper provides a
comprehensive overview of techniques for compressing LLMs to enable efficient
inference in resource-constrained environments. We examine three primary
approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For
each technique, we discuss the underlying principles, present different
variants, and provide examples of successful applications. We also briefly
discuss complementary techniques such as mixture-of-experts and early-exit
strategies. Finally, we highlight promising future directions, aiming to
provide a valuable resource for both researchers and practitioners seeking to
optimize LLMs for edge deployment.

</details>


### [415] [Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning](https://arxiv.org/pdf/2505.02181)
*Shengyu Duan, Marcos L. L. Sartori, Rishad Shafik, Alex Yakovlev, Emre Ozer*

Main category: cs.LG

TL;DR: The paper proposes a time-domain approach using programmable delay lines (PDLs) and arbiters to optimize popcount and argmax operations in Tsetlin Machines (TMs), achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: TMs are lightweight for ML but suffer from high computational costs of popcount and argmax operations, limiting performance.

Method: A time-domain implementation using PDLs and arbiters to manage popcount and argmax efficiently, with an FPGA design flow to address delay skew.

Result: Up to 38% latency reduction, 43.1% dynamic power reduction, and 15% resource savings compared to synchronous TMs.

Conclusion: The time-domain approach effectively accelerates and optimizes TM operations, enhancing performance and efficiency.

Abstract: Population count (popcount) is a crucial operation for many low-complexity
machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising
new ML method, particularly well-suited for solving classification tasks. The
inference mechanism in TM consists of propositional logic-based structures
within each class, followed by a majority voting scheme, which makes the
classification decision. In TM, the voters are the outputs of Boolean clauses.
The voting mechanism comprises two operations: popcount for each class and
determining the class with the maximum vote by means of an argmax operation.
  While TMs offer a lightweight ML alternative, their performance is often
limited by the high computational cost of popcount and comparison required to
produce the argmax result. In this paper, we propose an innovative approach to
accelerate and optimize these operations by performing them in the time domain.
Our time-domain implementation uses programmable delay lines (PDLs) and
arbiters to efficiently manage these tasks through delay-based mechanisms. We
also present an FPGA design flow for practical implementation of the
time-domain popcount, addressing delay skew and ensuring that the behavior
matches that of the model's intended functionality. By leveraging the natural
compatibility of the proposed popcount with asynchronous architectures, we
demonstrate significant improvements in an asynchronous TM, including up to 38%
reduction in latency, 43.1% reduction in dynamic power, and 15% savings in
resource utilization, compared to synchronous TMs using adder-based popcount.

</details>


### [416] [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/pdf/2505.02391)
*Jiarui Yao, Yifan Hao, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang*

Main category: cs.LG

TL;DR: GVM-RAFT introduces a dynamic sample allocation strategy to optimize computational resources in CoT reasoning, improving speed and accuracy over static methods like RAFT.


<details>
  <summary>Details</summary>
Motivation: Prior methods like RAFT use uniform inference budgets, ignoring variability in prompt difficulty and convergence, leading to inefficient training.

Method: GVM-RAFT dynamically allocates computational resources by monitoring prompt acceptance rates and gradient norms to minimize stochastic gradient variance.

Result: Experiments show GVM-RAFT achieves 2-4x speedup and better accuracy in mathematical reasoning compared to RAFT.

Conclusion: The dynamic sampling strategy is generalizable, enhancing convergence and accuracy in reinforcement learning algorithms like GRPO.

Abstract: Chain-of-thought (CoT) reasoning in large language models (LLMs) can be
formalized as a latent variable problem, where the model needs to generate
intermediate reasoning steps. While prior approaches such as iterative
reward-ranked fine-tuning (RAFT) have relied on such formulations, they
typically apply uniform inference budgets across prompts, which fails to
account for variability in difficulty and convergence behavior. This work
identifies the main bottleneck in CoT training as inefficient stochastic
gradient estimation due to static sampling strategies. We propose GVM-RAFT, a
prompt-specific Dynamic Sample Allocation Strategy designed to minimize
stochastic gradient variance under a computational budget constraint. The
method dynamically allocates computational resources by monitoring prompt
acceptance rates and stochastic gradient norms, ensuring that the resulting
gradient variance is minimized. Our theoretical analysis shows that the
proposed dynamic sampling strategy leads to accelerated convergence guarantees
under suitable conditions. Experiments on mathematical reasoning show that
GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over
vanilla RAFT. The proposed dynamic sampling strategy is general and can be
incorporated into other reinforcement learning algorithms, such as GRPO,
leading to similar improvements in convergence and test accuracy. Our code is
available at https://github.com/RLHFlow/GVM.

</details>


### [417] [Exogenous Isomorphism for Counterfactual Identifiability](https://arxiv.org/pdf/2505.02212)
*Yikang Chen, Dehui Du*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates $\sim_{\mathcal{L}_3}$-identifiability, a form of
complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH)
framework, ensuring that all Structural Causal Models (SCMs) satisfying the
given assumptions provide consistent answers to all causal questions. To
simplify this problem, we introduce exogenous isomorphism and propose
$\sim_{\mathrm{EI}}$-identifiability, reflecting the strength of model
identifiability required for $\sim_{\mathcal{L}_3}$-identifiability. We explore
sufficient assumptions for achieving $\sim_{\mathrm{EI}}$-identifiability in
two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual
transport, and Triangular Monotonic SCMs (TM-SCMs), which extend
$\sim_{\mathcal{L}_2}$-identifiability. Our results unify and generalize
existing theories, providing theoretical guarantees for practical applications.
Finally, we leverage neural TM-SCMs to address the consistency problem in
counterfactual reasoning, with experiments validating both the effectiveness of
our method and the correctness of the theory.

</details>


### [418] [Bielik v3 Small: Technical Report](https://arxiv.org/pdf/2505.02550)
*Krzysztof Ociepa, Łukasz Flis, Remigiusz Kinas, Krzysztof Wróbel, Adrian Gwoździej*

Main category: cs.LG

TL;DR: Bielik v3 introduces parameter-efficient generative text models (1.5B and 4.5B) for Polish, achieving performance comparable to larger models with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To optimize Polish language processing with smaller, efficient models, making high-quality AI accessible for resource-constrained applications.

Method: Uses a custom Polish tokenizer (APT4), Weighted Instruction Cross-Entropy Loss, and Adaptive Learning Rate, trained on 292B tokens from 303M documents.

Result: The 4.5B model competes with models 2-3 times its size; the 1.5B model performs well despite its compact size.

Conclusion: Bielik v3 sets new benchmarks for parameter-efficient language modeling in less-represented languages like Polish.

Abstract: We introduce Bielik v3, a series of parameter-efficient generative text
models (1.5B and 4.5B) optimized for Polish language processing. These models
demonstrate that smaller, well-optimized architectures can achieve performance
comparable to much larger counterparts while requiring substantially fewer
computational resources. Our approach incorporates several key innovations: a
custom Polish tokenizer (APT4) that significantly improves token efficiency,
Weighted Instruction Cross-Entropy Loss to balance learning across instruction
types, and Adaptive Learning Rate that dynamically adjusts based on training
progress. Trained on a meticulously curated corpus of 292 billion tokens
spanning 303 million documents, these models excel across multiple benchmarks,
including the Open PL LLM Leaderboard, Complex Polish Text Understanding
Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter
model achieves results competitive with models 2-3 times its size, while the
1.5B model delivers strong performance despite its extremely compact profile.
These advances establish new benchmarks for parameter-efficient language
modeling in less-represented languages, making high-quality Polish language AI
more accessible for resource-constrained applications.

</details>


### [419] [An Empirical Study of Qwen3 Quantization](https://arxiv.org/pdf/2505.02214)
*Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, Xianglong Liu*

Main category: cs.LG

TL;DR: The study evaluates Qwen3's performance under low-bit quantization, revealing competitive results at moderate bit-widths but degradation at ultra-low precision, highlighting challenges in LLM compression.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of quantization on Qwen3's performance and identify opportunities and challenges for efficient deployment in resource-constrained environments.

Method: Systematic evaluation of Qwen3 using 5 post-training quantization techniques across bit-widths (1-8 bits) on multiple datasets.

Result: Qwen3 performs well at moderate bit-widths but suffers notable degradation in linguistic tasks under ultra-low precision.

Conclusion: Further research is needed to mitigate performance loss in extreme quantization scenarios, with insights provided for advancing quantization methods for Qwen3 and future LLMs.

Abstract: The Qwen series has emerged as a leading family of open-source Large Language
Models (LLMs), demonstrating remarkable capabilities in natural language
understanding tasks. With the recent release of Qwen3, which exhibits superior
performance across diverse benchmarks, there is growing interest in deploying
these models efficiently in resource-constrained environments. Low-bit
quantization presents a promising solution, yet its impact on Qwen3's
performance remains underexplored. This study conducts a systematic evaluation
of Qwen3's robustness under various quantization settings, aiming to uncover
both opportunities and challenges in compressing this state-of-the-art model.
We rigorously assess 5 existing classic post-training quantization techniques
applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their
effectiveness across multiple datasets. Our findings reveal that while Qwen3
maintains competitive performance at moderate bit-widths, it experiences
notable degradation in linguistic tasks under ultra-low precision, underscoring
the persistent hurdles in LLM compression. These results emphasize the need for
further research to mitigate performance loss in extreme quantization
scenarios. We anticipate that this empirical analysis will provide actionable
insights for advancing quantization methods tailored to Qwen3 and future LLMs,
ultimately enhancing their practicality without compromising accuracy. Our
project is released on https://github.com/Efficient-ML/Qwen3-Quantization and
https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.

</details>


### [420] [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/pdf/2505.02639)
*Xuan Lin, Qingrui Liu, Hongxin Xiang, Daojian Zeng, Xiangxiang Zeng*

Main category: cs.LG

TL;DR: ChemDual is a novel LLM framework for chemical synthesis, addressing data and task correlation challenges by creating a large instruction dataset and using dual-task learning. It outperforms existing methods in reaction and retrosynthesis prediction.


<details>
  <summary>Details</summary>
Motivation: Challenges in applying LLMs to chemical synthesis include lack of large-scale datasets and ignoring task correlations. ChemDual aims to solve these issues.

Method: ChemDual treats reaction and retrosynthesis as related processes, constructs a 4.4M instruction dataset, and enhances LLaMA with multi-scale tokenizer and dual-task learning.

Result: ChemDual achieves state-of-the-art performance on Mol-Instruction and USPTO-50K datasets, generating compounds with strong protein binding affinity.

Conclusion: ChemDual demonstrates strong potential in drug design, outperforming single-task approaches and general LLMs.

Abstract: Chemical reaction and retrosynthesis prediction are fundamental tasks in drug
discovery. Recently, large language models (LLMs) have shown potential in many
domains. However, directly applying LLMs to these tasks faces two major
challenges: (i) lacking a large-scale chemical synthesis-related instruction
dataset; (ii) ignoring the close correlation between reaction and
retrosynthesis prediction for the existing fine-tuning strategies. To address
these challenges, we propose ChemDual, a novel LLM framework for accurate
chemical synthesis. Specifically, considering the high cost of data acquisition
for reaction and retrosynthesis, ChemDual regards the
reaction-and-retrosynthesis of molecules as a related
recombination-and-fragmentation process and constructs a large-scale of 4.4
million instruction dataset. Furthermore, ChemDual introduces an enhanced
LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,
to jointly optimize the process of recombination and fragmentation as well as
the tasks between reaction and retrosynthesis prediction. Extensive experiments
on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves
state-of-the-art performance in both predictions of reaction and
retrosynthesis, outperforming the existing conventional single-task approaches
and the general open-source LLMs. Through molecular docking analysis, ChemDual
generates compounds with diverse and strong protein binding affinity, further
highlighting its strong potential in drug design.

</details>


### [421] [Practical Efficiency of Muon for Pretraining](https://arxiv.org/pdf/2505.02222)
*Essential AI, :, Ishaan Shah, Anthony M. Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh J Shah, Khoi Nguyen, Kurt Smith, Michael Callahan, Michael Pust, Mohit Parmar, Peter Rushton, Platon Mazarakis, Ritvik Kapila, Saurabh Srivastava, Somanshu Singla, Tim Romanski, Yash Vanjani, Ashish Vaswani*

Main category: cs.LG

TL;DR: Muon, a second-order optimizer, outperforms AdamW in data efficiency at large batch sizes, enabling economical training. Combined with muP, it simplifies hyperparameter transfer with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: To improve training efficiency and data retention at large batch sizes beyond the critical batch size, while maintaining computational efficiency.

Method: Muon, a second-order optimizer, is combined with maximal update parameterization (muP) and a telescoping algorithm to account for errors in muP.

Result: Muon retains data efficiency better than AdamW at large batch sizes, validated with models up to 4B parameters.

Conclusion: Muon and muP enable efficient, economical training with minimal overhead, expanding the Pareto frontier over AdamW.

Abstract: We demonstrate that Muon, the simplest instantiation of a second-order
optimizer, explicitly expands the Pareto frontier over AdamW on the
compute-time tradeoff. We find that Muon is more effective than AdamW in
retaining data efficiency at large batch sizes, far beyond the so-called
critical batch size, while remaining computationally efficient, thus enabling
more economical training. We study the combination of Muon and the maximal
update parameterization (muP) for efficient hyperparameter transfer and present
a simple telescoping algorithm that accounts for all sources of error in muP
while introducing only a modest overhead in resources. We validate our findings
through extensive experiments with model sizes up to four billion parameters
and ablations on the data distribution and architecture.

</details>


### [422] [Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning](https://arxiv.org/pdf/2505.02228)
*Shangzhe Li, Zhiao Huang, Hao Su*

Main category: cs.LG

TL;DR: A novel online imitation learning method using RND-based reward model improves stability and achieves expert-level performance in diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing IL methods face instability, especially with adversarial rewards in world models. This work aims to address these limitations.

Method: Proposes a reward model based on random network distillation (RND) for density estimation, jointly estimating expert and behavioral distributions in latent space.

Result: Evaluated on DMControl, Meta-World, and ManiSkill2, the method shows stable performance and expert-level results in locomotion and manipulation.

Conclusion: The approach outperforms adversarial methods in stability while maintaining expert-level performance.

Abstract: Imitation Learning (IL) has achieved remarkable success across various
domains, including robotics, autonomous driving, and healthcare, by enabling
agents to learn complex behaviors from expert demonstrations. However, existing
IL methods often face instability challenges, particularly when relying on
adversarial reward or value formulations in world model frameworks. In this
work, we propose a novel approach to online imitation learning that addresses
these limitations through a reward model based on random network distillation
(RND) for density estimation. Our reward model is built on the joint estimation
of expert and behavioral distributions within the latent space of the world
model. We evaluate our method across diverse benchmarks, including DMControl,
Meta-World, and ManiSkill2, showcasing its ability to deliver stable
performance and achieve expert-level results in both locomotion and
manipulation tasks. Our approach demonstrates improved stability over
adversarial methods while maintaining expert-level performance.

</details>


### [423] [Federated Causal Inference in Healthcare: Methods, Challenges, and Applications](https://arxiv.org/pdf/2505.02238)
*Haoyang Li, Jie Xu, Kyra Gan, Fei Wang, Chengxi Zang*

Main category: cs.LG

TL;DR: The paper reviews federated causal inference for multi-site treatment effect estimation, addressing data heterogeneity challenges and comparing methods like weight-based and optimization-based frameworks. It highlights FedProx-style regularization as optimal and discusses future directions.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving treatment effect estimation across heterogeneous sites without sharing individual data, addressing biases and inefficiencies in federated causal inference.

Method: Classifies methods into weight-based and optimization-based strategies, examines federated Cox and Aalen-Johansen models for time-to-event outcomes, and analyzes bias-variance trade-offs.

Result: FedProx-style regularization offers near-optimal bias-variance trade-offs compared to naive averaging and meta-analysis.

Conclusion: Outlines opportunities and challenges for scalable, fair, and trustworthy federated causal inference in distributed healthcare systems.

Abstract: Federated causal inference enables multi-site treatment effect estimation
without sharing individual-level data, offering a privacy-preserving solution
for real-world evidence generation. However, data heterogeneity across sites,
manifested in differences in covariate, treatment, and outcome, poses
significant challenges for unbiased and efficient estimation. In this paper, we
present a comprehensive review and theoretical analysis of federated causal
effect estimation across both binary/continuous and time-to-event outcomes. We
classify existing methods into weight-based strategies and optimization-based
frameworks and further discuss extensions including personalized models,
peer-to-peer communication, and model decomposition. For time-to-event
outcomes, we examine federated Cox and Aalen-Johansen models, deriving
asymptotic bias and variance under heterogeneity. Our analysis reveals that
FedProx-style regularization achieves near-optimal bias-variance trade-offs
compared to naive averaging and meta-analysis. We review related software tools
and conclude by outlining opportunities, challenges, and future directions for
scalable, fair, and trustworthy federated causal inference in distributed
healthcare systems.

</details>


### [424] [RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation](https://arxiv.org/pdf/2505.02247)
*Jingxiang Qu, Wenhan Gao, Jiaxing Zhang, Xufeng Liu, Hua Wei, Haibin Ling, Yi Liu*

Main category: cs.LG

TL;DR: A novel method for explaining 3D GNNs by localizing explanations to node neighborhoods, enhancing interpretability for molecular data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited interpretability of 3D GNNs in scientific applications, especially for molecular data.

Method: Introduces a localized explanation method for 3D GNNs, assigning each node a radius of influence to capture spatial and structural interactions.

Result: Enhances interpretability by aligning explanations with physical and structural dependencies in 3D graphs.

Conclusion: The method improves transparency and reliability of 3D GNNs for applications like molecular learning.

Abstract: 3D Geometric Graph Neural Networks (GNNs) have emerged as transformative
tools for modeling molecular data. Despite their predictive power, these models
often suffer from limited interpretability, raising concerns for scientific
applications that require reliable and transparent insights. While existing
methods have primarily focused on explaining molecular substructures in 2D
GNNs, the transition to 3D GNNs introduces unique challenges, such as handling
the implicit dense edge structures created by a cut-off radius. To tackle this,
we introduce a novel explanation method specifically designed for 3D GNNs,
which localizes the explanation to the immediate neighborhood of each node
within the 3D space. Each node is assigned an radius of influence, defining the
localized region within which message passing captures spatial and structural
interactions crucial for the model's predictions. This method leverages the
spatial and geometric characteristics inherent in 3D graphs. By constraining
the subgraph to a localized radius of influence, the approach not only enhances
interpretability but also aligns with the physical and structural dependencies
typical of 3D graph applications, such as molecular learning.

</details>


### [425] [Epistemic Wrapping for Uncertainty Quantification](https://arxiv.org/pdf/2505.02277)
*Maryam Sultana, Neil Yorke-Smith, Kaizheng Wang, Shireen Kudukkil Manchingal, Muhammad Mubashar, Fabio Cuzzolin*

Main category: cs.LG

TL;DR: A novel 'Epistemic Wrapping' method improves uncertainty estimation in classification by transforming Bayesian Neural Network outputs into belief function posteriors, enhancing generalization and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Uncertainty estimation is crucial for robust and reliable machine learning models, especially in classification tasks.

Method: Uses Bayesian Neural Networks (BNNs) as a baseline and transforms their outputs into belief function posteriors to capture epistemic uncertainty.

Result: Experiments on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 show significant improvement in generalization and uncertainty quantification.

Conclusion: The Epistemic Wrapper is an efficient and general methodology for enhancing uncertainty estimation in classification tasks.

Abstract: Uncertainty estimation is pivotal in machine learning, especially for
classification tasks, as it improves the robustness and reliability of models.
We introduce a novel `Epistemic Wrapping' methodology aimed at improving
uncertainty estimation in classification. Our approach uses Bayesian Neural
Networks (BNNs) as a baseline and transforms their outputs into belief function
posteriors, effectively capturing epistemic uncertainty and offering an
efficient and general methodology for uncertainty quantification. Comprehensive
experiments employing a Bayesian Neural Network (BNN) baseline and an Interval
Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and
CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly
enhances generalisation and uncertainty quantification.

</details>


### [426] [Universal Approximation Theorem of Deep Q-Networks](https://arxiv.org/pdf/2505.02288)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper analyzes Deep Q-Networks (DQNs) in continuous-time using stochastic control and FBSDEs, showing DQNs can approximate optimal Q-functions accurately and analyzing Q-learning convergence.


<details>
  <summary>Details</summary>
Motivation: To bridge deep reinforcement learning and stochastic control, providing insights for applications like physical systems or high-frequency data.

Method: Uses stochastic control, FBSDEs, and a continuous-time MDP to analyze DQN approximation and Q-learning convergence.

Result: DQNs can approximate optimal Q-functions with high accuracy on compact sets, and Q-learning convergence is analyzed.

Conclusion: The work connects deep RL and stochastic control, highlighting DQN performance in continuous-time settings.

Abstract: We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs)
via stochastic control and Forward-Backward Stochastic Differential Equations
(FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by
a square-integrable martingale, we analyze DQN approximation properties. We
show that DQNs can approximate the optimal Q-function on compact sets with
arbitrary accuracy and high probability, leveraging residual network
approximation theorems and large deviation bounds for the state-action process.
We then analyze the convergence of a general Q-learning algorithm for training
DQNs in this setting, adapting stochastic approximation theorems. Our analysis
emphasizes the interplay between DQN layer count, time discretization, and the
role of viscosity solutions (primarily for the value function $V^*$) in
addressing potential non-smoothness of the optimal Q-function. This work
bridges deep reinforcement learning and stochastic control, offering insights
into DQNs in continuous-time settings, relevant for applications with physical
systems or high-frequency data.

</details>


### [427] [Entropy-Guided Sampling of Flat Modes in Discrete Spaces](https://arxiv.org/pdf/2505.02296)
*Pinaki Mohanty, Riddhiman Bhattacharya, Ruqi Zhang*

Main category: cs.LG

TL;DR: Proposes Entropic Discrete Langevin Proposal (EDLP) for sampling from flat modes in discrete spaces, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Flat modes are robust solutions but underexplored; existing methods struggle to sample them effectively.

Method: EDLP incorporates local entropy via a continuous auxiliary variable under a joint distribution.

Result: EDLP provides non-asymptotic convergence guarantees and outperforms traditional methods in empirical tasks.

Conclusion: EDLP effectively addresses the challenge of sampling from flat modes in discrete spaces.

Abstract: Sampling from flat modes in discrete spaces is a crucial yet underexplored
problem. Flat modes represent robust solutions and have broad applications in
combinatorial optimization and discrete generative modeling. However, existing
sampling algorithms often overlook the mode volume and struggle to capture flat
modes effectively. To address this limitation, we propose \emph{Entropic
Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the
sampling process through a continuous auxiliary variable under a joint
distribution. The local entropy term guides the discrete sampler toward flat
modes with a small overhead. We provide non-asymptotic convergence guarantees
for EDLP in locally log-concave discrete distributions. Empirically, our method
consistently outperforms traditional approaches across tasks that require
sampling from flat basins, including Bernoulli distribution, restricted
Boltzmann machines, combinatorial optimization, and binary neural networks.

</details>


### [428] [Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection](https://arxiv.org/pdf/2505.02299)
*Daisuke Yamada, Harit Vishwakarma, Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: A human-in-the-loop framework is proposed to dynamically update ML scoring functions and thresholds for OOD detection, ensuring high TPR while strictly controlling FPR.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods lack adaptivity and FPR control, posing risks in safety-critical applications.

Method: A human-in-the-loop framework updates scoring functions and thresholds in real-time based on observed OOD inputs.

Result: The approach outperforms existing methods, achieving higher TPR while maintaining strict FPR control.

Conclusion: The proposed framework provides adaptive and safe OOD detection with theoretical guarantees and empirical validation.

Abstract: Machine Learning (ML) models are trained on in-distribution (ID) data but
often encounter out-of-distribution (OOD) inputs during deployment -- posing
serious risks in safety-critical domains. Recent works have focused on
designing scoring functions to quantify OOD uncertainty, with score thresholds
typically set based solely on ID data to achieve a target true positive rate
(TPR), since OOD data is limited before deployment. However, these TPR-based
thresholds leave false positive rates (FPR) uncontrolled, often resulting in
high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring
functions and thresholds lack the adaptivity needed to handle newly observed,
evolving OOD inputs, leading to sub-optimal performance. To address these
challenges, we propose a human-in-the-loop framework that \emph{safely updates
both scoring functions and thresholds on the fly} based on real-world OOD
inputs. Our method maximizes TPR while strictly controlling FPR at all times,
even as the system adapts over time. We provide theoretical guarantees for FPR
control under stationary conditions and present extensive empirical evaluations
on OpenOOD benchmarks to demonstrate that our approach outperforms existing
methods by achieving higher TPRs while maintaining FPR control.

</details>


### [429] [Enabling Local Neural Operators to perform Equation-Free System-Level Analysis](https://arxiv.org/pdf/2505.02308)
*Gianluca Fabiani, Hannes Vandecasteele, Somdatta Goswami, Constantinos Siettos, Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: Neural Operators (NOs) are extended beyond temporal simulations to perform system-level tasks like stability and bifurcation analysis, integrating with Krylov subspace methods for efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore NOs' potential for rigorous numerical system-level tasks (e.g., fixed-point, stability, bifurcation analysis) beyond dynamical behavior prediction.

Method: Proposes a framework combining local NOs with Krylov subspace methods for stability and bifurcation analysis, also using local in space and space-time NOs.

Result: Demonstrates effectiveness via nonlinear PDE benchmarks (Allen-Cahn, Liouville-Bratu-Gelfand, FitzHugh-Nagumo), showcasing bifurcation and stability analysis.

Conclusion: The framework successfully extends NOs' utility to system-level analysis, enabling efficient study of complex dynamical systems.

Abstract: Neural Operators (NOs) provide a powerful framework for computations
involving physical laws that can be modelled by (integro-) partial differential
equations (PDEs), directly learning maps between infinite-dimensional function
spaces that bypass both the explicit equation identification and their
subsequent numerical solving. Still, NOs have so far primarily been employed to
explore the dynamical behavior as surrogates of brute-force temporal
simulations/predictions. Their potential for systematic rigorous numerical
system-level tasks, such as fixed-point, stability, and bifurcation analysis -
crucial for predicting irreversible transitions in real-world phenomena -
remains largely unexplored. Toward this aim, inspired by the Equation-Free
multiscale framework, we propose and implement a framework that integrates
(local) NOs with advanced iterative numerical methods in the Krylov subspace,
so as to perform efficient system-level stability and bifurcation analysis of
large-scale dynamical systems. Beyond fixed point, stability, and bifurcation
analysis enabled by local in time NOs, we also demonstrate the usefulness of
local in space as well as in space-time ("patch") NOs in accelerating the
computer-aided analysis of spatiotemporal dynamics. We illustrate our framework
via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes
multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE,
which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN)
model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node
bifurcations.

</details>


### [430] [Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training](https://arxiv.org/pdf/2505.02360)
*Fares B. Mehouachi, Saif Eddin Jabari*

Main category: cs.LG

TL;DR: Proposes controlling the $l^p$ training norm to mitigate Catastrophic Overfitting (CO) in adversarial training, avoiding noise or regularization.


<details>
  <summary>Details</summary>
Motivation: CO is more prevalent under $l^{\infty}$ norm than $l^2$, suggesting norm control as a solution.

Method: Develops $l^p$-FGSM attacks and adaptive tuning of training norm based on gradient concentration.

Result: Achieves strong robustness without extra regularization or noise.

Conclusion: Offers a principled solution to CO by adapting the training norm.

Abstract: Adversarial training is a cornerstone of robust deep learning, but fast
methods like the Fast Gradient Sign Method (FGSM) often suffer from
Catastrophic Overfitting (CO), where models become robust to single-step
attacks but fail against multi-step variants. While existing solutions rely on
noise injection, regularization, or gradient clipping, we propose a novel
solution that purely controls the $l^p$ training norm to mitigate CO.
  Our study is motivated by the empirical observation that CO is more prevalent
under the $l^{\infty}$ norm than the $l^2$ norm. Leveraging this insight, we
develop a framework for generalized $l^p$ attack as a fixed point problem and
craft $l^p$-FGSM attacks to understand the transition mechanics from $l^2$ to
$l^{\infty}$. This leads to our core insight: CO emerges when highly
concentrated gradients where information localizes in few dimensions interact
with aggressive norm constraints. By quantifying gradient concentration through
Participation Ratio and entropy measures, we develop an adaptive $l^p$-FGSM
that automatically tunes the training norm based on gradient information.
Extensive experiments demonstrate that this approach achieves strong robustness
without requiring additional regularization or noise injection, providing a
novel and theoretically-principled pathway to mitigate the CO problem.

</details>


### [431] [Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks](https://arxiv.org/pdf/2505.02369)
*Juyoung Yun*

Main category: cs.LG

TL;DR: ZSharp improves SAM by using layer-wise Z-score normalization and percentile-based filtering to focus on statistically significant gradient directions, enhancing generalization without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often converge to sharp minima, degrading robustness. SAM seeks flatter minima but includes insignificant gradient directions, which ZSharp addresses.

Method: ZSharp applies layer-wise Z-score normalization and percentile-based filtering to retain only significant gradient components, aligning updates with curvature-sensitive directions.

Result: ZSharp outperforms SAM and its variants in test accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet, especially for deeper and transformer-based models.

Conclusion: ZSharp is a lightweight, principled improvement for sharpness-aware optimization, enhancing generalization without requiring architectural changes.

Abstract: Generalizing well in deep neural networks remains a core challenge,
particularly due to their tendency to converge to sharp minima that degrade
robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking
flatter minima but perturbs parameters using the full gradient, which can
include statistically insignificant directions. We propose ZSharp, a simple yet
effective extension to SAM that applies layer-wise Z-score normalization
followed by percentile-based filtering to retain only statistically significant
gradient components. This selective perturbation aligns updates with
curvature-sensitive directions, enhancing generalization without requiring
architectural changes. ZSharp introduces only one additional hyperparameter,
the percentile threshold, and remains fully compatible with existing SAM
variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,
VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and
its variants in test accuracy, particularly on deeper and transformer-based
models. These results demonstrate that ZSharp is a principled and lightweight
improvement for sharpness-aware optimization.

</details>


### [432] [EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices](https://arxiv.org/pdf/2505.02380)
*Arnab Sanyal, Prithwish Mukherjee, Gourav Datta, Sandeep P. Chinchali*

Main category: cs.LG

TL;DR: EntroLLM is a compression framework for LLMs using mixed quantization and entropy coding, reducing storage by up to 65% while maintaining accuracy and improving inference speed on edge devices.


<details>
  <summary>Details</summary>
Motivation: Large storage and computational demands of LLMs hinder their deployment on edge devices, necessitating efficient compression methods.

Method: Layer-wise mixed quantization (symmetric/asymmetric) combined with Huffman encoding for lossless compression and parallel decoding for efficient inference.

Result: Achieves 30-65% storage reduction, preserves perplexity/accuracy, and improves inference throughput by 31.9-146.6% on edge devices.

Conclusion: EntroLLM offers a practical, re-training-free solution for deploying LLMs on edge devices with minimal performance impact.

Abstract: Large Language Models (LLMs) demonstrate exceptional performance across
various tasks, but their large storage and computational requirements constrain
their deployment on edge devices. To address this, we propose EntroLLM, a novel
compression framework that integrates mixed quantization with entropy coding to
reduce storage overhead while maintaining model accuracy. Our method applies a
layer-wise mixed quantization scheme - choosing between symmetric and
asymmetric quantization based on individual layer weight distributions - to
optimize compressibility. We then employ Huffman encoding for lossless
compression of the quantized weights, significantly reducing memory bandwidth
requirements. Furthermore, we introduce parallel Huffman decoding, which
enables efficient retrieval of encoded weights during inference, ensuring
minimal latency impact. Our experiments on edge-compatible LLMs, including
smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,
demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to
uint8 models and up to $65%$ storage reduction compared to uint4 models, while
preserving perplexity and accuracy, on language benchmark tasks. We further
show that our method enables $31.9%$ - $146.6%$ faster inference throughput on
memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing
the required data movement. The proposed approach requires no additional
re-training and is fully compatible with existing post-training quantization
methods, making it a practical solution for edge LLMs.

</details>


### [433] [Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret](https://arxiv.org/pdf/2505.02383)
*Bingshan Hu, Zhiming Huang, Tianyue H. Zhang, Mathias Lécuyer, Nidhi Hegde*

Main category: cs.LG

TL;DR: The paper introduces DP-TS-UCB, a differentially private bandit algorithm balancing privacy (GDP) and regret, with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To explore connections between Thompson Sampling, Gaussian mechanisms, and GDP in private bandit problems.

Method: Proposes DP-TS-UCB, a parametrized algorithm leveraging Gaussian priors and anti-concentration bounds.

Result: DP-TS-UCB achieves $\tilde{O}(T^{0.25(1-\alpha)})$-GDP and $O(K\ln^{\alpha+1}(T)/\Delta)$ regret.

Conclusion: The algorithm bridges Thompson Sampling and UCB methods, offering a flexible privacy-regret trade-off.

Abstract: We address differentially private stochastic bandit problems from the angles
of exploring the deep connections among Thompson Sampling with Gaussian priors,
Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose
DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade
off privacy and regret. DP-TS-UCB satisfies $ \tilde{O}
\left(T^{0.25(1-\alpha)}\right)$-GDP and enjoys an $O
\left(K\ln^{\alpha+1}(T)/\Delta \right)$ regret bound, where $\alpha \in [0,1]$
controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB
relies on anti-concentration bounds of Gaussian distributions and links
exploration mechanisms in Thompson Sampling-based algorithms and Upper
Confidence Bound-based algorithms, which may be of independent interest.

</details>


### [434] [Quantitative Analysis of Performance Drop in DeepSeek Model Quantization](https://arxiv.org/pdf/2505.02390)
*Enbo Zhao, Yi Shen, Shuming Shi, Jieyun Huang, Zhihao Chen, Ning Wang, Siqi Xiao, Jian Zhang, Kai Wang, Shiguo Lian*

Main category: cs.LG

TL;DR: The paper evaluates multi-bitwidth quantization for DeepSeek-R1 and V3, showing 4-bit quantization maintains performance and enables single-machine deployment. A dynamic 3-bit method (DQ3_K_M) outperforms traditional variants and matches 4-bit performance.


<details>
  <summary>Details</summary>
Motivation: High demand for local deployment due to service issues and privacy concerns, but memory limits hinder single-machine deployment.

Method: Quantization evaluation across DeepSeek models, proposing DQ3_K_M, a dynamic 3-bit method.

Result: 4-bit quantization works well; DQ3_K_M outperforms traditional 3-bit and matches 4-bit performance, enabling deployment on standard GPUs.

Conclusion: DQ3_K_M is a viable solution for efficient local deployment of large models with minimal performance loss.

Abstract: Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally,
possibly because the official service often suffers from being busy and some
organizations have data privacy concerns. While single-machine deployment
offers infrastructure simplicity, the models' 671B FP8 parameter configuration
exceeds the practical memory limits of a standard 8-GPU machine. Quantization
is a widely used technique that helps reduce model memory consumption. However,
it is unclear what the performance of DeepSeek-R1 and V3 will be after being
quantized. This technical report presents the first quantitative evaluation of
multi-bitwidth quantization across the complete DeepSeek model spectrum. Key
findings reveal that 4-bit quantization maintains little performance
degradation versus FP8 while enabling single-machine deployment on standard
NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization
method that significantly outperforms traditional Q3_K_M variant on various
benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach
in most tasks. Moreover, DQ3_K_M supports single-machine deployment
configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of
DQ3\_K\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing
optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.

</details>


### [435] [A probabilistic view on Riemannian machine learning models for SPD matrices](https://arxiv.org/pdf/2505.02402)
*Thibault de Surrel, Florian Yger, Fabien Lotte, Sylvain Chevallier*

Main category: cs.LG

TL;DR: Uniting machine learning tools on SPD matrices under a probabilistic framework using Gaussian distributions.


<details>
  <summary>Details</summary>
Motivation: To unify various machine learning tools on the Riemannian manifold of SPD matrices under a probabilistic framework.

Method: Define Gaussian distributions on SPD matrices, reinterpret classifiers as Bayes Classifiers, and use distributions for outlier detection and dimension reduction.

Result: Demonstrates the pervasiveness of these distributions in tools for SPD matrices, enabling extension of other ML tools.

Conclusion: The probabilistic framework unifies and extends machine learning tools for SPD matrices.

Abstract: The goal of this paper is to show how different machine learning tools on the
Riemannian manifold $\mathcal{P}_d$ of Symmetric Positive Definite (SPD)
matrices can be united under a probabilistic framework. For this, we will need
several Gaussian distributions defined on $\mathcal{P}_d$. We will show how
popular classifiers on $\mathcal{P}_d$ can be reinterpreted as Bayes
Classifiers using these Gaussian distributions. These distributions will also
be used for outlier detection and dimension reduction. By showing that those
distributions are pervasive in the tools used on $\mathcal{P}_d$, we allow for
other machine learning tools to be extended to $\mathcal{P}_d$.

</details>


### [436] [T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models](https://arxiv.org/pdf/2505.02417)
*Yunfeng Ge, Jiawei Li, Yiji Zhao, Haomin Wen, Zhao Li, Meikang Qiu, Hongyan Li, Ming Jin, Shirui Pan*

Main category: cs.LG

TL;DR: The paper introduces Text-to-Series (T2S), a diffusion-based framework for generating time series from text, addressing limitations in generalization and arbitrary-length generation.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like data sparsity and domain-specific limitations in time series generation, and to extend diffusion models' success to time series.

Method: Proposes T2S, which uses a length-adaptive variational autoencoder and aligns text with latent embeddings via Flow Matching and Diffusion Transformer.

Result: T2S achieves state-of-the-art performance across 13 datasets in 12 domains.

Conclusion: T2S effectively bridges the gap between natural language and time series, enabling flexible and high-quality generation.

Abstract: Text-to-Time Series generation holds significant potential to address
challenges such as data sparsity, imbalance, and limited availability of
multimodal time series datasets across domains. While diffusion models have
achieved remarkable success in Text-to-X (e.g., vision and audio data)
generation, their use in time series generation remains in its nascent stages.
Existing approaches face two critical limitations: (1) the lack of systematic
exploration of general-proposed time series captions, which are often
domain-specific and struggle with generalization; and (2) the inability to
generate time series of arbitrary lengths, limiting their applicability to
real-world scenarios. In this work, we first categorize time series captions
into three levels: point-level, fragment-level, and instance-level.
Additionally, we introduce a new fragment-level dataset containing over 600,000
high-resolution time series-text pairs. Second, we propose Text-to-Series
(T2S), a diffusion-based framework that bridges the gap between natural
language and time series in a domain-agnostic manner. T2S employs a
length-adaptive variational autoencoder to encode time series of varying
lengths into consistent latent embeddings. On top of that, T2S effectively
aligns textual representations with latent embeddings by utilizing Flow
Matching and employing Diffusion Transformer as the denoiser. We train T2S in
an interleaved paradigm across multiple lengths, allowing it to generate
sequences of any desired length. Extensive evaluations demonstrate that T2S
achieves state-of-the-art performance across 13 datasets spanning 12 domains.

</details>


### [437] [Towards One-shot Federated Learning: Advances, Challenges, and Future Directions](https://arxiv.org/pdf/2505.02426)
*Flora Amato, Lingyu Qiu, Mohammad Tanveer, Salvatore Cuomo, Fabio Giampaolo, Francesco Piccialli*

Main category: cs.LG

TL;DR: One-shot FL enables single-round collaborative training, ideal for resource-constrained and privacy-sensitive applications. This survey reviews its framework, methodologies, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, privacy-preserving collaborative training in resource-limited settings, avoiding iterative communication.

Method: Systematic categorization of methodologies, focusing on client model initialization, aggregation techniques, and handling heterogeneous data distributions.

Result: Identifies advancements and limitations, particularly in scalability and non-IID data generalization.

Conclusion: The survey serves as a reference for designing One-shot FL systems, promoting its real-world adoption in constrained scenarios.

Abstract: One-shot FL enables collaborative training in a single round, eliminating the
need for iterative communication, making it particularly suitable for use in
resource-constrained and privacy-sensitive applications. This survey offers a
thorough examination of One-shot FL, highlighting its distinct operational
framework compared to traditional federated approaches. One-shot FL supports
resource-limited devices by enabling single-round model aggregation while
maintaining data locality. The survey systematically categorizes existing
methodologies, emphasizing advancements in client model initialization,
aggregation techniques, and strategies for managing heterogeneous data
distributions. Furthermore, we analyze the limitations of current approaches,
particularly in terms of scalability and generalization in non-IID settings. By
analyzing cutting-edge techniques and outlining open challenges, this survey
aspires to provide a comprehensive reference for researchers and practitioners
aiming to design and implement One-shot FL systems, advancing the development
and adoption of One-shot FL solutions in a real-world, resource-constrained
scenario.

</details>


### [438] [FairPO: Robust Preference Optimization for Fair Multi-Label Learning](https://arxiv.org/pdf/2505.02433)
*Soumen Kumar Mondal, Akshit Varmora, Prateek Chanda, Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: FairPO is a framework for fair multi-label classification, optimizing preference signals to reduce bias between privileged and non-privileged label groups while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address fairness in multi-label classification by mitigating bias between label groups and ensuring equitable treatment.

Method: Uses a preference-based loss (inspired by DPO) to differentiate true positives from negatives in privileged groups, with robust optimization to adjust training focus dynamically.

Result: FairPO aims to reduce bias and improve fairness across label groups while preserving baseline performance.

Conclusion: The framework shows promise for fair classification and plans extensions like alternative loss formulations and multilabel generation.

Abstract: We propose FairPO, a novel framework designed to promote fairness in
multi-label classification by directly optimizing preference signals with a
group robustness perspective. In our framework, the set of labels is
partitioned into privileged and non-privileged groups, and a preference-based
loss inspired by Direct Preference Optimization (DPO) is employed to more
effectively differentiate true positive labels from confusing negatives within
the privileged group, while preserving baseline classification performance for
non-privileged labels. By framing the learning problem as a robust optimization
over groups, our approach dynamically adjusts the training emphasis toward
groups with poorer performance, thereby mitigating bias and ensuring a fairer
treatment across diverse label categories. In addition, we outline plans to
extend this approach by investigating alternative loss formulations such as
Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization
(CPO) to exploit reference-free reward formulations and contrastive training
signals. Furthermore, we plan to extend FairPO with multilabel generation
capabilities, enabling the model to dynamically generate diverse and coherent
label sets for ambiguous inputs.

</details>


### [439] [A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability](https://arxiv.org/pdf/2505.02435)
*Pouria Fatemi, Ehsan Sharifian, Mohammad Hossein Yassaee*

Main category: cs.LG

TL;DR: Proposes an efficient backtracking counterfactual method with causal reasoning for actionable explanations, addressing limitations of traditional and newer approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional counterfactual methods lack causal relationships, leading to unrealistic examples, while newer causal methods are computationally expensive.

Method: Introduces a backtracking counterfactual approach incorporating causal reasoning to generate actionable explanations efficiently.

Result: Demonstrates deeper insights into model outputs and generalizes previous techniques in specific scenarios.

Conclusion: The proposed method efficiently integrates causality for realistic and actionable counterfactual explanations.

Abstract: Counterfactual explanations enhance interpretability by identifying
alternative inputs that produce different outputs, offering localized insights
into model decisions. However, traditional methods often neglect causal
relationships, leading to unrealistic examples. While newer approaches
integrate causality, they are computationally expensive. To address these
challenges, we propose an efficient method based on backtracking
counterfactuals that incorporates causal reasoning to generate actionable
explanations. We first examine the limitations of existing methods and then
introduce our novel approach and its features. We also explore the relationship
between our method and previous techniques, demonstrating that it generalizes
them in specific scenarios. Finally, experiments show that our method provides
deeper insights into model outputs.

</details>


### [440] [SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning](https://arxiv.org/pdf/2505.02486)
*Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, Horace Ho Shing Ip, Sam Kwong*

Main category: cs.LG

TL;DR: MCIT addresses catastrophic forgetting in MLLMs by distinguishing superficial (format deviation) and essential (factual loss) forgetting. ASD standardizes answer styles to prevent superficial forgetting, while RegLoRA mitigates essential forgetting via regularization. SEFE achieves top performance.


<details>
  <summary>Details</summary>
Motivation: To enable MLLMs to learn new tasks incrementally without forgetting prior knowledge, addressing both superficial (style shifts) and essential (knowledge loss) forgetting.

Method: 1. ASD paradigm standardizes answer styles across tasks to prevent superficial forgetting. 2. RegLoRA applies regularization to stabilize key parameters, mitigating essential forgetting.

Result: SEFE (the combined method) achieves state-of-the-art performance in preventing forgetting.

Conclusion: The proposed approach effectively addresses both types of forgetting in MCIT, with SEFE demonstrating superior results.

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal
Large Language Models (MLLMs) to incrementally learn new tasks without
catastrophic forgetting. In this paper, we explore forgetting in this context,
categorizing it into superficial forgetting and essential forgetting.
Superficial forgetting refers to cases where the model's knowledge may not be
genuinely lost, but its responses to previous tasks deviate from expected
formats due to the influence of subsequent tasks' answer styles, making the
results unusable. By contrast, essential forgetting refers to situations where
the model provides correctly formatted but factually inaccurate answers,
indicating a true loss of knowledge. Assessing essential forgetting
necessitates addressing superficial forgetting first, as severe superficial
forgetting can obscure the model's knowledge state. Hence, we first introduce
the Answer Style Diversification (ASD) paradigm, which defines a standardized
process for transforming data styles across different tasks, unifying their
training sets into similarly diversified styles to prevent superficial
forgetting caused by style shifts. Building on this, we propose RegLoRA to
mitigate essential forgetting. RegLoRA stabilizes key parameters where prior
knowledge is primarily stored by applying regularization, enabling the model to
retain existing competencies. Experimental results demonstrate that our overall
method, SEFE, achieves state-of-the-art performance.

</details>


### [441] [Bayesian Robust Aggregation for Federated Learning](https://arxiv.org/pdf/2505.02490)
*Aleksandr Karakulev, Usama Zafar, Salman Toor, Prashant Singh*

Main category: cs.LG

TL;DR: An adaptive Bayesian inference-based method for robust aggregation in Federated Learning, resilient to unknown and varying numbers of adversarial clients.


<details>
  <summary>Details</summary>
Motivation: Federated Learning is vulnerable to adversarial attacks from compromised clients, with the number and extent of attacks often unknown or dynamic.

Method: Proposes a Bayesian inference approach for robust aggregation, defining the mean update via marginalized likelihood over client honesty probabilities.

Result: Outperforms other aggregation methods (e.g., Krum) on benchmark datasets, handling static and dynamic adversarial clients effectively.

Conclusion: The method combines simplicity with robustness, achieving state-of-the-art performance without needing prior knowledge of attack scale.

Abstract: Federated Learning enables collaborative training of machine learning models
on decentralized data. This scheme, however, is vulnerable to adversarial
attacks, when some of the clients submit corrupted model updates. In real-world
scenarios, the total number of compromised clients is typically unknown, with
the extent of attacks potentially varying over time. To address these
challenges, we propose an adaptive approach for robust aggregation of model
updates based on Bayesian inference. The mean update is defined by the maximum
of the likelihood marginalized over probabilities of each client to be
`honest'. As a result, the method shares the simplicity of the classical
average estimators (e.g., sample mean or geometric median), being independent
of the number of compromised clients. At the same time, it is as effective
against attacks as methods specifically tailored to Federated Learning, such as
Krum. We compare our approach with other aggregation schemes in federated
setting on three benchmark image classification data sets. The proposed method
consistently achieves state-of-the-art performance across various attack types
with static and varying number of malicious clients.

</details>


### [442] [Exploring Design Choices for Autoregressive Deep Learning Climate Models](https://arxiv.org/pdf/2505.02506)
*Florian Gallusser, Simon Hentschel, Anna Krause, Andreas Hotho*

Main category: cs.LG

TL;DR: The paper compares three deep learning models for weather prediction, identifying configurations for stable long-term forecasts.


<details>
  <summary>Details</summary>
Motivation: Deep learning models excel in short-term weather prediction but struggle with long-term stability, unlike some atmospheric models. This study aims to uncover design choices for stable long-term forecasts.

Method: The study evaluates three DL-MWP architectures (FourCastNet, SFNO, ClimaX) trained on ERA5 data, assessing autoregressive training, model capacity, and prognostic variables.

Result: SFNO shows the most robustness, but all models can become unstable depending on random seeds and prognostic variables. Stable 10-year rollouts are achievable with specific configurations.

Conclusion: The study identifies key factors for stable long-term weather predictions, highlighting SFNO's robustness but noting instability risks across models.

Abstract: Deep Learning models have achieved state-of-the-art performance in
medium-range weather prediction but often fail to maintain physically
consistent rollouts beyond 14 days. In contrast, a few atmospheric models
demonstrate stability over decades, though the key design choices enabling this
remain unclear. This study quantitatively compares the long-term stability of
three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained
on ERA5 reanalysis data at 5.625{\deg} resolution. We systematically assess the
impact of autoregressive training steps, model capacity, and choice of
prognostic variables, identifying configurations that enable stable 10-year
rollouts while preserving the statistical properties of the reference dataset.
Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter
choices, yet all models can experience instability depending on the random seed
and the set of prognostic variables

</details>


### [443] [Uncovering Population PK Covariates from VAE-Generated Latent Spaces](https://arxiv.org/pdf/2505.02514)
*Diego Perazzolo, Chiara Castellani, Enrico Grisan*

Main category: cs.LG

TL;DR: A data-driven framework combining VAEs and LASSO regression is proposed for covariate selection in PopPK modeling, identifying key covariates like SNP, age, albumin, and hemoglobin for tacrolimus.


<details>
  <summary>Details</summary>
Motivation: Traditional PopPK methods struggle with complex, nonlinear covariate relationships, prompting the need for a more robust, data-driven approach.

Method: Integrates VAEs to compress PK signals into a latent space and LASSO regression for sparse covariate selection.

Result: Achieves accurate PK signal reconstruction (MAPE 2.26%) and identifies clinically relevant covariates consistently.

Conclusion: The VAE-LASSO framework is scalable, interpretable, and effective for precision pharmacotherapy.

Abstract: Population pharmacokinetic (PopPK) modelling is a fundamental tool for
understanding drug behaviour across diverse patient populations and enabling
personalized dosing strategies to improve therapeutic outcomes. A key challenge
in PopPK analysis lies in identifying and modelling covariates that influence
drug absorption, as these relationships are often complex and nonlinear.
Traditional methods may fail to capture hidden patterns within the data. In
this study, we propose a data-driven, model-free framework that integrates
Variational Autoencoders (VAEs) deep learning model and LASSO regression to
uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles.
The VAE compresses high-dimensional PK signals into a structured latent space,
achieving accurate reconstruction with a mean absolute percentage error (MAPE)
of 2.26%. LASSO regression is then applied to map patient-specific covariates
to the latent space, enabling sparse feature selection through L1
regularization. This approach consistently identifies clinically relevant
covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are
retained across the tested regularization strength levels, while effectively
discarding non-informative features. The proposed VAE-LASSO methodology offers
a scalable, interpretable, and fully data-driven solution for covariate
selection, with promising applications in drug development and precision
pharmacotherapy.

</details>


### [444] [FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization](https://arxiv.org/pdf/2505.02515)
*Hongze Li, Zesheng Zhou, Zhenbiao Cao, Xinhui Li, Wei Chen, Xiaojin Zhang*

Main category: cs.LG

TL;DR: FedSDAF introduces a federated framework for domain generalization by leveraging source domain-specific features, outperforming existing methods by 5.2-13.8%.


<details>
  <summary>Details</summary>
Motivation: Addressing the oversight of source domain-specific characteristics in federated domain generalization, which limits model performance.

Method: Combines Domain-Invariant Adapter and Domain-Aware Adapter with MHSA, plus bidirectional knowledge distillation for privacy-preserving knowledge sharing.

Result: Achieves significant accuracy improvements (5.2-13.8%) over state-of-the-art methods on benchmarks like OfficeHome, PACS, VLCS, and DomainNet.

Conclusion: FedSDAF effectively enhances federated domain generalization by systematically utilizing source domain-aware features, setting a new performance standard.

Abstract: Traditional domain generalization approaches predominantly focus on
leveraging target domain-aware features while overlooking the critical role of
source domain-specific characteristics, particularly in federated settings with
inherent data isolation. To address this gap, we propose the Federated Source
Domain Awareness Framework (FedSDAF), the first method to systematically
exploit source domain-aware features for enhanced federated domain
generalization (FedDG). The FedSDAF framework consists of two synergistic
components: the Domain-Invariant Adapter, which preserves critical
domain-invariant features, and the Domain-Aware Adapter, which extracts and
integrates source domain-specific knowledge using a Multihead Self-Attention
mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge
distillation mechanism that fosters knowledge sharing among clients while
safeguarding privacy. Our approach represents the first systematic exploitation
of source domain-aware features, resulting in significant advancements in model
generalization capability.Extensive experiments on four standard benchmarks
(OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently
surpasses state-of-the-art federated domain generalization approaches, with
accuracy gains of 5.2-13.8%. The source code is available at
https://github.com/pizzareapers/FedSDAF.

</details>


### [445] [Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations](https://arxiv.org/pdf/2505.02537)
*Davide Sartor, Alberto Sinigaglia, Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper generalizes theoretical results on MLPs with non-negative weight constraints and alternating saturation activations, proving their universality for monotonic functions. It introduces an equivalence between activation saturation and weight sign, extends universality to convex monotone activations with non-positive weights, and proposes a weight-sign-adjusted activation method to ease optimization.


<details>
  <summary>Details</summary>
Motivation: To address optimization challenges in conventional monotonic MLPs and generalize theoretical foundations for their universality.

Method: Generalizes theoretical proofs for MLPs with non-negative weights and alternating saturation activations, introduces equivalence between activation saturation and weight sign, and proposes a weight-sign-adjusted activation method.

Result: Proves universality for MLPs with convex monotone activations and non-positive weights, and shows improved optimization stability with the proposed method.

Conclusion: The work provides theoretical grounding for empirical observations, simplifies architectures, and improves optimization, validated by experiments.

Abstract: Conventional techniques for imposing monotonicity in MLPs by construction
involve the use of non-negative weight constraints and bounded activation
functions, which pose well-known optimization challenges. In this work, we
generalize previous theoretical results, showing that MLPs with non-negative
weight constraint and activations that saturate on alternating sides are
universal approximators for monotonic functions. Additionally, we show an
equivalence between the saturation side in the activations and the sign of the
weight constraint. This connection allows us to prove that MLPs with convex
monotone activations and non-positive constrained weights also qualify as
universal approximators, in contrast to their non-negative constrained
counterparts. Our results provide theoretical grounding to the empirical
effectiveness observed in previous works while leading to possible
architectural simplification. Moreover, to further alleviate the optimization
difficulties, we propose an alternative formulation that allows the network to
adjust its activations according to the sign of the weights. This eliminates
the requirement for weight reparameterization, easing initialization and
improving training stability. Experimental evaluation reinforces the validity
of the theoretical results, showing that our novel approach compares favourably
to traditional monotonic architectures.

</details>


### [446] [Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data](https://arxiv.org/pdf/2505.02540)
*Ljubomir Rokvic, Panayiotis Danassis, Boi Faltings*

Main category: cs.LG

TL;DR: Proposes pFedLIA, a personalized federated learning framework using 'Lazy Influence' to cluster clients and improve model performance in non-IID data settings.


<details>
  <summary>Details</summary>
Motivation: Addresses performance drop in global models due to heterogeneous client data distributions (e.g., user-specific language patterns, medical images, driving data).

Method: Uses 'Lazy Influence' approximation to cluster clients distributively, then trains cluster-specific models.

Result: Recovers global model performance drop, matches Oracle clustering, and outperforms baselines (e.g., 17% improvement on CIFAR100).

Conclusion: pFedLIA effectively handles non-IID data in federated learning, enhancing personalized model performance.

Abstract: In Federated Learning, heterogeneity in client data distributions often means
that a single global model does not have the best performance for individual
clients. Consider for example training a next-word prediction model for
keyboards: user-specific language patterns due to demographics (dialect, age,
etc.), language proficiency, and writing style result in a highly non-IID
dataset across clients. Other examples are medical images taken with different
machines, or driving data from different vehicle types. To address this, we
propose a simple yet effective personalized federated learning framework
(pFedLIA) that utilizes a computationally efficient influence approximation,
called `Lazy Influence', to cluster clients in a distributed manner before
model aggregation. Within each cluster, data owners collaborate to jointly
train a model that captures the specific data patterns of the clients. Our
method has been shown to successfully recover the global model's performance
drop due to the non-IID-ness in various synthetic and real-world settings,
specifically a next-word prediction task on the Nordic languages as well as
several benchmark tasks. It matches the performance of a hypothetical Oracle
clustering, and significantly improves on existing baselines, e.g., an
improvement of 17% on CIFAR100.

</details>


### [447] [Robustness questions the interpretability of graph neural networks: what to do?](https://arxiv.org/pdf/2505.02566)
*Kirill Lukyanov, Georgii Sazonov, Serafim Boyarsky, Ilya Makarov*

Main category: cs.LG

TL;DR: This paper benchmarks the impact of robustness defenses on GNN interpretability, revealing trade-offs and providing a foundation for developing trustworthy GNNs.


<details>
  <summary>Details</summary>
Motivation: The interplay between GNN interpretability and robustness under adversarial attacks is poorly understood, necessitating systematic analysis.

Method: Six GNN architectures were evaluated across five datasets using four interpretability metrics (Fidelity, Stability, Consistency, Sparsity) under poisoning and evasion defenses.

Result: Interpretability varies significantly with defense methods and model architectures, highlighting robustness-interpretability trade-offs.

Conclusion: The benchmark aids in developing GNNs that are both robust and interpretable, ensuring trust in sensitive applications.

Abstract: Graph Neural Networks (GNNs) have become a cornerstone in graph-based data
analysis, with applications in diverse domains such as bioinformatics, social
networks, and recommendation systems. However, the interplay between model
interpretability and robustness remains poorly understood, especially under
adversarial scenarios like poisoning and evasion attacks. This paper presents a
comprehensive benchmark to systematically analyze the impact of various factors
on the interpretability of GNNs, including the influence of
robustness-enhancing defense mechanisms.
  We evaluate six GNN architectures based on GCN, SAGE, GIN, and GAT across
five datasets from two distinct domains, employing four interpretability
metrics: Fidelity, Stability, Consistency, and Sparsity. Our study examines how
defenses against poisoning and evasion attacks, applied before and during model
training, affect interpretability and highlights critical trade-offs between
robustness and interpretability. The framework will be published as open
source.
  The results reveal significant variations in interpretability depending on
the chosen defense methods and model architecture characteristics. By
establishing a standardized benchmark, this work provides a foundation for
developing GNNs that are both robust to adversarial threats and interpretable,
facilitating trust in their deployment in sensitive applications.

</details>


### [448] [Rethinking Federated Graph Learning: A Data Condensation Perspective](https://arxiv.org/pdf/2505.02573)
*Hao Zhang, Xunkai Li, Yinlin Zhu, Lianglin Hu*

Main category: cs.LG

TL;DR: FedGM introduces condensed graphs to address data heterogeneity in federated graph learning, reducing privacy risks and communication costs while outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing federated graph learning methods struggle with data heterogeneity and privacy risks due to excessive parameter sharing.

Method: FedGM uses condensed graphs as optimization carriers, employing generalized condensation graph consensus for knowledge aggregation with minimal communication.

Result: FedGM outperforms state-of-the-art baselines on six public datasets.

Conclusion: FedGM offers a novel, efficient, and privacy-preserving paradigm for federated graph learning.

Abstract: Federated graph learning is a widely recognized technique that promotes
collaborative training of graph neural networks (GNNs) by multi-client
graphs.However, existing approaches heavily rely on the communication of model
parameters or gradients for federated optimization and fail to adequately
address the data heterogeneity introduced by intricate and diverse graph
distributions. Although some methods attempt to share additional messages among
the server and clients to improve federated convergence during communication,
they introduce significant privacy risks and increase communication overhead.
To address these issues, we introduce the concept of a condensed graph as a
novel optimization carrier to address FGL data heterogeneity and propose a new
FGL paradigm called FedGM. Specifically, we utilize a generalized condensation
graph consensus to aggregate comprehensive knowledge from distributed graphs,
while minimizing communication costs and privacy risks through a single
transmission of the condensed data. Extensive experiments on six public
datasets consistently demonstrate the superiority of FedGM over
state-of-the-art baselines, highlighting its potential for a novel FGL
paradigm.

</details>


### [449] [Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era](https://arxiv.org/pdf/2505.02583)
*Chenxi Liu, Shaowen Zhou, Qianxiong Xu, Hao Miao, Cheng Long, Ziyue Li, Rui Zhao*

Main category: cs.LG

TL;DR: A survey on LLM-based cross-modality modeling for time series analytics, covering taxonomy, strategies, applications, experiments, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rise of edge devices and time series data, combined with the potential of LLMs, motivates exploring their integration despite the cross-modality gap.

Method: Classifies approaches into four groups based on textual data types, summarizes cross-modality strategies (e.g., alignment, fusion), and tests combinations on multimodal datasets.

Result: Identifies effective textual data and strategy combinations for enhancing time series analytics.

Conclusion: Highlights promising future research directions and targets professionals interested in LLM-based time series modeling.

Abstract: The proliferation of edge devices has generated an unprecedented volume of
time series data across different domains, motivating various well-customized
methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm
for time series analytics by leveraging the shared sequential nature of textual
data and time series. However, a fundamental cross-modality gap between time
series and LLMs exists, as LLMs are pre-trained on textual corpora and are not
inherently optimized for time series. Many recent proposals are designed to
address this issue. In this survey, we provide an up-to-date overview of
LLMs-based cross-modality modeling for time series analytics. We first
introduce a taxonomy that classifies existing approaches into four groups based
on the type of textual data employed for time series modeling. We then
summarize key cross-modality strategies, e.g., alignment and fusion, and
discuss their applications across a range of downstream tasks. Furthermore, we
conduct experiments on multimodal datasets from different application domains
to investigate effective combinations of textual data and cross-modality
strategies for enhancing time series analytics. Finally, we suggest several
promising directions for future research. This survey is designed for a range
of professionals, researchers, and practitioners interested in LLM-based time
series modeling.

</details>


### [450] [Low-Loss Space in Neural Networks is Continuous and Fully Connected](https://arxiv.org/pdf/2505.02604)
*Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee*

Main category: cs.LG

TL;DR: The paper explores the existence of continuous low-loss paths in neural network parameter spaces, challenging the notion of isolated minima.


<details>
  <summary>Details</summary>
Motivation: To understand the connectivity of low-loss regions in neural networks and its implications for over-parameterization and model generalization.

Method: A new algorithm is proposed to investigate low-loss paths in the full parameter space, tested on LeNet5, ResNet18, and Compact Convolutional Transformer architectures.

Result: Experiments confirm the existence of continuous low-loss paths, suggesting the low-loss region is fully connected and continuous.

Conclusion: The findings highlight parameter redundancy within models and offer insights for improving generalization by exploring low-loss spaces.

Abstract: Visualizations of the loss landscape in neural networks suggest that minima
are isolated points. However, both theoretical and empirical studies indicate
that it is possible to connect two different minima with a path consisting of
intermediate points that also have low loss. In this study, we propose a new
algorithm which investigates low-loss paths in the full parameter space, not
only between two minima. Our experiments on LeNet5, ResNet18, and Compact
Convolutional Transformer architectures consistently demonstrate the existence
of such continuous paths in the parameter space. These results suggest that the
low-loss region is a fully connected and continuous space in the parameter
space. Our findings provide theoretical insight into neural network
over-parameterization, highlighting that parameters collectively define a
high-dimensional low-loss space, implying parameter redundancy exists only
within individual models and not throughout the entire low-loss space.
Additionally, our work also provides new visualization methods and
opportunities to improve model generalization by exploring the low-loss space
that is closer to the origin.

</details>


### [451] [Mirror Mean-Field Langevin Dynamics](https://arxiv.org/pdf/2505.02621)
*Anming Gu, Juno Kim*

Main category: cs.LG

TL;DR: The paper introduces mirror mean-field Langevin dynamics (MMFLD) to optimize probability measures on constrained domains, extending MFLD with convergence guarantees and chaos propagation results.


<details>
  <summary>Details</summary>
Motivation: Existing mean-field algorithms fail to handle constrained domains due to global diffusion, limiting their applicability to problems like interacting particle systems.

Method: Proposes MMFLD, an extension of MFLD using the mirror Langevin framework, for constrained convex subsets of ℝᵈ.

Result: Achieves linear convergence via a uniform log-Sobolev inequality and uniform-in-time propagation of chaos for discretized versions.

Conclusion: MMFLD effectively addresses constrained optimization in Wasserstein space, with theoretical guarantees for convergence and chaos propagation.

Abstract: The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized
nonlinear convex functional on the Wasserstein space over $\mathbb{R}^d$, and
has gained attention recently as a model for the gradient descent dynamics of
interacting particle systems such as infinite-width two-layer neural networks.
However, many problems of interest have constrained domains, which are not
solved by existing mean-field algorithms due to the global diffusion term. We
study the optimization of probability measures constrained to a convex subset
of $\mathbb{R}^d$ by proposing the \emph{mirror mean-field Langevin dynamics}
(MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain
linear convergence guarantees for the continuous MMFLD via a uniform
log-Sobolev inequality, and uniform-in-time propagation of chaos results for
its time- and particle-discretized counterpart.

</details>


### [452] [A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition](https://arxiv.org/pdf/2505.02627)
*Yuanpeng Li*

Main category: cs.LG

TL;DR: The paper derives a necessary and sufficient condition for compositional generalization in neural networks, involving computational graph alignment and component information encoding.


<details>
  <summary>Details</summary>
Motivation: To address the lack of compositional generalization in most deep learning models by identifying governing conditions.

Method: Derives a theoretical condition supported by mathematical proofs, involving architecture design, regularization, and training data properties.

Result: A minimal example validates the condition, and its potential for pre-training assessment is discussed.

Conclusion: The work provides a fundamental theoretical understanding of compositional generalization in neural networks.

Abstract: Compositional generalization is a crucial property in artificial
intelligence, enabling models to handle novel combinations of known components.
While most deep learning models lack this capability, certain models succeed in
specific tasks, suggesting the existence of governing conditions. This paper
derives a necessary and sufficient condition for compositional generalization
in neural networks. Conceptually, it requires that (i) the computational graph
matches the true compositional structure, and (ii) components encode just
enough information in training. The condition is supported by mathematical
proofs. This criterion combines aspects of architecture design, regularization,
and training data properties. A carefully designed minimal example illustrates
an intuitive understanding of the condition. We also discuss the potential of
the condition for assessing compositional generalization before training. This
work is a fundamental theoretical study of compositional generalization in
neural networks.

</details>


### [453] [Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning](https://arxiv.org/pdf/2505.02634)
*David Ramos, Lucas Lacasa, Eusebio Valero, Gonzalo Rubio*

Main category: cs.LG

TL;DR: A transfer learning-enhanced DRL method optimizes airfoil geometry for aerodynamic and structural goals, outperforming PSO in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To optimize airfoil geometry using multi-objective criteria (aerodynamic and structural) more efficiently than traditional methods.

Method: Uses deep reinforcement learning (DRL) with transfer learning (TL) strategies, comparing performance against Particle Swarm Optimization (PSO).

Result: DRL outperforms PSO in computational efficiency and optimization performance; TL-enhanced DRL saves resources while matching DRL performance.

Conclusion: The proposed DRL method is effective for multi-objective airfoil optimization, with TL further enhancing computational efficiency.

Abstract: The main objective of this paper is to introduce a transfer
learning-enhanced, multi-objective, deep reinforcement learning (DRL)
methodology that is able to optimise the geometry of any airfoil based on
concomitant aerodynamic and structural criteria. To showcase the method, we aim
to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural
integrity of the airfoil -- as modelled by its maximum thickness -- and train
the DRL agent using a list of different transfer learning (TL) strategies. The
performance of the DRL agent is compared with Particle Swarm Optimisation
(PSO), a traditional gradient-free optimisation method. Results indicate that
DRL agents are able to perform multi-objective shape optimisation, that the DRL
approach outperforms PSO in terms of computational efficiency and shape
optimisation performance, and that the TL-enhanced DRL agent achieves
performance comparable to the DRL one, while further saving substantial
computational resources.

</details>


### [454] [Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints](https://arxiv.org/pdf/2505.02640)
*Shubham Vaishnav, Praveen Kumar Donta, Sindri Magnússon*

Main category: cs.LG

TL;DR: A Budgeted Multi-Armed Bandit framework for IoT with dynamic constraints, using a decaying violation budget and Budgeted UCB algorithm, achieves sublinear regret and better constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Current IoT systems struggle with dynamic resource constraints and evolving operational limits, requiring adaptive solutions.

Method: Proposes a Budgeted Multi-Armed Bandit framework with a decaying violation budget and Budgeted UCB algorithm for balancing performance and compliance.

Result: Theoretical guarantees show sublinear regret and logarithmic constraint violations; simulations confirm faster adaptation and better constraint satisfaction.

Conclusion: The framework is promising for adaptive, resource-aware IoT systems.

Abstract: Internet of Things (IoT) systems increasingly operate in environments where
devices must respond in real time while managing fluctuating resource
constraints, including energy and bandwidth. Yet, current approaches often fall
short in addressing scenarios where operational constraints evolve over time.
To address these limitations, we propose a novel Budgeted Multi-Armed Bandit
framework tailored for IoT applications with dynamic operational limits. Our
model introduces a decaying violation budget, which permits limited constraint
violations early in the learning process and gradually enforces stricter
compliance over time. We present the Budgeted Upper Confidence Bound (UCB)
algorithm, which adaptively balances performance optimization and compliance
with time-varying constraints. We provide theoretical guarantees showing that
Budgeted UCB achieves sublinear regret and logarithmic constraint violations
over the learning horizon. Extensive simulations in a wireless communication
setting show that our approach achieves faster adaptation and better constraint
satisfaction than standard online learning methods. These results highlight the
framework's potential for building adaptive, resource-aware IoT systems.

</details>


### [455] [SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2505.02655)
*Shiwei Guo, Ziang Chen, Yupeng Ma, Yunfei Han, Yi Wang*

Main category: cs.LG

TL;DR: SCFormer enhances Transformer models for time series forecasting by adding temporal constraints and using HiPPO for historical data.


<details>
  <summary>Details</summary>
Motivation: Existing Transformers lack temporal constraints and fail to use cumulative historical data effectively.

Method: SCFormer adds temporal constraints to linear transformations and employs HiPPO for historical series.

Result: SCFormer outperforms baselines in experiments.

Conclusion: SCFormer improves time series forecasting by addressing temporal and historical data limitations.

Abstract: The Transformer model has shown strong performance in multivariate time
series forecasting by leveraging channel-wise self-attention. However, this
approach lacks temporal constraints when computing temporal features and does
not utilize cumulative historical series effectively.To address these
limitations, we propose the Structured Channel-wise Transformer with Cumulative
Historical state (SCFormer). SCFormer introduces temporal constraints to all
linear transformations, including the query, key, and value matrices, as well
as the fully connected layers within the Transformer. Additionally, SCFormer
employs High-order Polynomial Projection Operators (HiPPO) to deal with
cumulative historical time series, allowing the model to incorporate
information beyond the look-back window during prediction. Extensive
experiments on multiple real-world datasets demonstrate that SCFormer
significantly outperforms mainstream baselines, highlighting its effectiveness
in enhancing time series forecasting. The code is publicly available at
https://github.com/ShiweiGuo1995/SCFormer

</details>


### [456] [A Note on Statistically Accurate Tabular Data Generation Using Large Language Models](https://arxiv.org/pdf/2505.02659)
*Andrey Sidorenko*

Main category: cs.LG

TL;DR: A probability-driven prompting method improves LLM-generated tabular data by preserving complex feature dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for synthetic tabular data generation with LLMs fail to maintain complex feature dependencies, especially among categorical variables.

Method: Introduces a probability-driven prompting approach that uses LLMs to estimate conditional distributions for data synthesis.

Result: The method enhances the statistical fidelity of LLM-generated tabular data.

Conclusion: Probability-driven prompting shows promise for scalable and accurate synthetic data generation with LLMs.

Abstract: Large language models (LLMs) have shown promise in synthetic tabular data
generation, yet existing methods struggle to preserve complex feature
dependencies, particularly among categorical variables. This work introduces a
probability-driven prompting approach that leverages LLMs to estimate
conditional distributions, enabling more accurate and scalable data synthesis.
The results highlight the potential of prompting probobility distributions to
enhance the statistical fidelity of LLM-generated tabular data.

</details>


### [457] [Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework](https://arxiv.org/pdf/2505.02712)
*Andrzej Mizera, Jakub Zarzycki*

Main category: cs.LG

TL;DR: The paper proposes a deep reinforcement learning (DRL) approach to control Boolean network models for cellular reprogramming, improving scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional wet-lab experiments for discovering reprogramming strategies are time-consuming and costly, prompting the need for computational alternatives.

Method: The study uses DRL to control Boolean network models under asynchronous updates, incorporating graph neural networks for scalability and pseudo-attractor identification.

Result: Experiments on real-world biological networks show the approach is scalable and effective.

Conclusion: The DRL-based framework offers a promising computational solution for cellular reprogramming challenges.

Abstract: Cellular reprogramming, the artificial transformation of one cell type into
another, has been attracting increasing research attention due to its
therapeutic potential for complex diseases. However, discovering reprogramming
strategies through classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we explore the use of deep
reinforcement learning (DRL) to control Boolean network models of complex
biological systems, such as gene regulatory networks and signalling pathway
networks. We formulate a novel control problem for Boolean network models under
the asynchronous update mode in the context of cellular reprogramming. To
facilitate scalability, we consider our previously introduced concept of a
pseudo-attractor and we improve our procedure for effective identification of
pseudo-attractor states. Finally, we devise a computational framework to solve
the control problem. To leverage the structure of biological systems, we
incorporate graph neural networks with graph convolutions into the artificial
neural network approximator for the action-value function learned by the DRL
agent. Experiments on a number of large real-world biological networks from
literature demonstrate the scalability and effectiveness of our approach.

</details>


### [458] [Less is More: Efficient Weight Farcasting with 1-Layer Neural Network](https://arxiv.org/pdf/2505.02714)
*Xiao Shou, Debarun Bhattacharjya, Yanna Ding, Chen Zhao, Rui Li, Jianxi Gao*

Main category: cs.LG

TL;DR: A novel framework using long-term time series forecasting for training large-scale deep neural networks, focusing on initial and final weight values, with a tailored regularizer for improved performance.


<details>
  <summary>Details</summary>
Motivation: Addressing computational challenges in training large-scale deep neural networks as model sizes grow, seeking alternatives to traditional methods like gradient descent.

Method: Leverages long-term time series forecasting techniques, using only initial and final weight values, and introduces a novel regularizer.

Result: Superior forecasting accuracy and computational efficiency demonstrated on synthetic and real-world architectures like DistilBERT, with minimal overhead.

Conclusion: The framework offers a promising, efficient alternative for accelerating training across diverse tasks and architectures.

Abstract: Addressing the computational challenges inherent in training large-scale deep
neural networks remains a critical endeavor in contemporary machine learning
research. While previous efforts have focused on enhancing training efficiency
through techniques such as gradient descent with momentum, learning rate
scheduling, and weight regularization, the demand for further innovation
continues to burgeon as model sizes keep expanding. In this study, we introduce
a novel framework which diverges from conventional approaches by leveraging
long-term time series forecasting techniques. Our method capitalizes solely on
initial and final weight values, offering a streamlined alternative for complex
model architectures. We also introduce a novel regularizer that is tailored to
enhance the forecasting performance of our approach. Empirical evaluations
conducted on synthetic weight sequences and real-world deep learning
architectures, including the prominent large language model DistilBERT,
demonstrate the superiority of our method in terms of forecasting accuracy and
computational efficiency. Notably, our framework showcases improved performance
while requiring minimal additional computational overhead, thus presenting a
promising avenue for accelerating the training process across diverse tasks and
architectures.

</details>


### [459] [Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation](https://arxiv.org/pdf/2505.02737)
*Pons Gerard, Bilalli Besim, Queralt Anna*

Main category: cs.LG

TL;DR: The paper proposes using Knowledge Graphs (KGs) to enhance Large Language Models (LLMs) for zero-shot Entity Disambiguation (ED), improving performance and adaptability over non-enhanced or description-only LLMs.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like hallucination and outdated knowledge, which are hard to fix via retraining. KGs offer structured external knowledge to mitigate these issues.

Method: Leverages hierarchical class representation and entity descriptions from KGs to prune candidate space and enrich input prompts for LLMs.

Result: Outperforms non-enhanced and description-only LLMs on ED datasets, showing higher adaptability than task-specific models.

Conclusion: KGs effectively enhance LLMs for ED, with KG semantic expressivity impacting performance; error analysis provides insights for future improvements.

Abstract: Recent advances in Large Language Models (LLMs) have positioned them as a
prominent solution for Natural Language Processing tasks. Notably, they can
approach these problems in a zero or few-shot manner, thereby eliminating the
need for training or fine-tuning task-specific models. However, LLMs face some
challenges, including hallucination and the presence of outdated knowledge or
missing information from specific domains in the training data. These problems
cannot be easily solved by retraining the models with new data as it is a
time-consuming and expensive process. To mitigate these issues, Knowledge
Graphs (KGs) have been proposed as a structured external source of information
to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for
zero-shot Entity Disambiguation (ED). For that purpose, we leverage the
hierarchical representation of the entities' classes in a KG to gradually prune
the candidate space as well as the entities' descriptions to enrich the input
prompt with additional factual knowledge. Our evaluation on popular ED datasets
shows that the proposed method outperforms non-enhanced and description-only
enhanced LLMs, and has a higher degree of adaptability than task-specific
models. Furthermore, we conduct an error analysis and discuss the impact of the
leveraged KG's semantic expressivity on the ED performance.

</details>


### [460] [Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties](https://arxiv.org/pdf/2505.02743)
*Jiaxiang Yi, Miguel A. Bessa*

Main category: cs.LG

TL;DR: A method combining variance networks with Bayesian neural networks to disentangle aleatoric and epistemic uncertainties while improving mean estimation.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of existing methods (MVE networks and Bayesian neural networks) in handling aleatoric and epistemic uncertainties.

Method: Cooperatively training a variance network with a Bayesian neural network.

Result: Effective disentanglement of uncertainties and improved mean estimation across diverse datasets.

Conclusion: The method is robust, scalable, and adaptable to various architectures.

Abstract: Real-world data contains aleatoric uncertainty - irreducible noise arising
from imperfect measurements or from incomplete knowledge about the data
generation process. Mean variance estimation (MVE) networks can learn this type
of uncertainty but require ad-hoc regularization strategies to avoid
overfitting and are unable to predict epistemic uncertainty (model
uncertainty). Conversely, Bayesian neural networks predict epistemic
uncertainty but are notoriously difficult to train due to the approximate
nature of Bayesian inference. We propose to cooperatively train a variance
network with a Bayesian neural network and demonstrate that the resulting model
disentangles aleatoric and epistemic uncertainties while improving the mean
estimation. We demonstrate the effectiveness and scalability of this method
across a diverse range of datasets, including a time-dependent heteroscedastic
regression dataset we created where the aleatoric uncertainty is known. The
proposed method is straightforward to implement, robust, and adaptable to
various model architectures.

</details>


### [461] [HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models](https://arxiv.org/pdf/2505.02795)
*Zheng Lin, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Praneeth Vepakomma, Wei Ni, Jun Luo, Yue Gao*

Main category: cs.LG

TL;DR: HSplitLoRA is a framework for efficient fine-tuning of large language models (LLMs) on heterogeneous devices, combining split learning and LoRA to reduce computing costs and improve performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs with private data is resource-intensive, and federated learning (FL) faces challenges due to high computing costs and device heterogeneity.

Method: HSplitLoRA identifies important weights, configures LoRA adapter ranks dynamically, and uses a noise-free aggregation mechanism for heterogeneous devices.

Result: HSplitLoRA achieves better training accuracy and convergence speed than existing benchmarks.

Conclusion: HSplitLoRA effectively addresses the challenges of fine-tuning LLMs on heterogeneous devices, offering a practical solution for real-world applications.

Abstract: Recently, large language models (LLMs) have achieved remarkable
breakthroughs, revolutionizing the natural language processing domain and
beyond. Due to immense parameter sizes, fine-tuning these models with private
data for diverse downstream tasks has become mainstream. Though federated
learning (FL) offers a promising solution for fine-tuning LLMs without sharing
raw data, substantial computing costs hinder its democratization. Moreover, in
real-world scenarios, private client devices often possess heterogeneous
computing resources, further complicating LLM fine-tuning. To combat these
challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient
fine-tuning (PEFT) framework built on split learning (SL) and low-rank
adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on
heterogeneous client devices. HSplitLoRA first identifies important weights
based on their contributions to LLM training. It then dynamically configures
the decomposition ranks of LoRA adapters for selected weights and determines
the model split point according to varying computing budgets of client devices.
Finally, a noise-free adapter aggregation mechanism is devised to support
heterogeneous adapter aggregation without introducing noise. Extensive
experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks
in training accuracy and convergence speed.

</details>


### [462] [Towards Quantifying the Hessian Structure of Neural Networks](https://arxiv.org/pdf/2505.02809)
*Zhaorui Dong, Yushun Zhang, Zhi-Quan Luo, Jianfeng Yao, Ruoyu Sun*

Main category: cs.LG

TL;DR: The paper explains the near-block-diagonal structure of the Hessian matrix in neural networks, attributing it to static (architecture) and dynamic (training) forces. It focuses on static forces, showing that the block-diagonal structure emerges as the number of classes (C) grows large.


<details>
  <summary>Details</summary>
Motivation: To theoretically understand the near-block-diagonal Hessian structure in neural networks, which empirical studies observed but lacked theoretical grounding.

Method: Analyzes linear models and 1-hidden-layer networks with MSE and CE loss, using random matrix theory to compare diagonal and off-diagonal Hessian blocks.

Result: The block-diagonal structure arises as the number of classes (C) approaches infinity, identifying C as a key factor.

Conclusion: The findings provide theoretical insights into Hessian structures, particularly relevant for large language models with high C values.

Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs)
exhibits a near-block-diagonal structure, yet its theoretical foundation
remains unclear. In this work, we reveal two forces that shape the Hessian
structure: a ``static force'' rooted in the architecture design, and a
``dynamic force'' arisen from training. We then provide a rigorous theoretical
analysis of ``static force'' at random initialization. We study linear models
and 1-hidden-layer networks with the mean-square (MSE) loss and the
Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix
theory, we compare the limit distributions of the diagonal and off-diagonal
Hessian blocks and find that the block-diagonal structure arises as $C
\rightarrow \infty$, where $C$ denotes the number of classes. Our findings
reveal that $C$ is a primary driver of the near-block-diagonal structure. These
results may shed new light on the Hessian structure of large language models
(LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.

</details>


### [463] [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection](https://arxiv.org/pdf/2309.15670)
*Sumit Kumar Banshal, Sajal Das, Shumaiya Akter Shammi, Narayan Ranjan Chakraborty, Aulia Luqman Aziz, Mohammed Aljuaid, Fazla Rabby, Rohit Bansal*

Main category: cs.LG

TL;DR: The paper addresses the gap in multi-label emotion recognition in Bangla by creating an annotated corpus from Facebook data and using BERT for superior results, demonstrated via a web app.


<details>
  <summary>Details</summary>
Motivation: Bangla's structural complexity and lack of research in multi-label sentiment analysis motivate the study.

Method: Constructed an annotated corpus from Facebook data using a context-based approach and employed BERT for emotion recognition.

Result: BERT outperformed other methods in multi-label emotion recognition in Bangla.

Conclusion: The study successfully bridges gaps in Bangla emotion recognition and demonstrates BERT's effectiveness through a web application.

Abstract: In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have
been increasingly popular in the Bangla language, which is the seventh most
spoken language throughout the entire world. However, the language is
structurally complicated, which makes this field arduous to extract emotions in
an accurate manner. Several distinct approaches such as the extraction of
positive and negative sentiments as well as multiclass emotions, have been
implemented in this field of study. Nevertheless, the extraction of multiple
sentiments is an almost untouched area in this language. Which involves
identifying several feelings based on a single piece of text. Therefore, this
study demonstrates a thorough method for constructing an annotated corpus based
on scrapped data from Facebook to bridge the gaps in this subject area to
overcome the challenges. To make this annotation more fruitful, the
context-based approach has been used. Bidirectional Encoder Representations
from Transformers (BERT), a well-known methodology of transformers, have been
shown the best results of all methods implemented. Finally, a web application
has been developed to demonstrate the performance of the pre-trained
top-performer model (BERT) for multi-label ER in Bangla.

</details>


### [464] [Impact of Noisy Supervision in Foundation Model Learning](https://arxiv.org/pdf/2403.06869)
*Hao Chen, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie, Masashi Sugiyama, Bhiksha Raj, Jindong Wang*

Main category: cs.LG

TL;DR: The paper investigates noise in pre-training datasets, showing it harms out-of-domain performance, and proposes NMTune to mitigate its effects.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the adverse effects of label noise in large-scale pre-training datasets on downstream tasks.

Method: Conducts experiments on synthetic noisy datasets (ImageNet-1K, YFCC15M, CC12M) and proposes NMTune for feature space adjustment.

Result: Pre-training noise harms OOD performance but may slightly benefit in-domain performance. NMTune effectively mitigates noise impacts.

Conclusion: The study highlights the importance of addressing noise in pre-training datasets and introduces a practical solution for improving model generalization.

Abstract: Foundation models are usually pre-trained on large-scale datasets and then
adapted to downstream tasks through tuning. However, the large-scale
pre-training datasets, often inaccessible or too expensive to handle, can
contain label noise that may adversely affect the generalization of the model
and pose unexpected risks. This paper stands out as the first work to
comprehensively understand and analyze the nature of noise in pre-training
datasets and then effectively mitigate its impacts on downstream tasks.
Specifically, through extensive experiments of fully-supervised and image-text
contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M
datasets, we demonstrate that, while slight noise in pre-training can benefit
in-domain (ID) performance, where the training and testing data share a similar
distribution, it always deteriorates out-of-domain (OOD) performance, where
training and testing distributions are significantly different. These
observations are agnostic to scales of pre-training datasets, pre-training
noise types, model architectures, pre-training objectives, downstream tuning
methods, and downstream applications. We empirically ascertain that the reason
behind this is that the pre-training noise shapes the feature space
differently. We then propose a tuning method (NMTune) to affine the feature
space to mitigate the malignant effect of noise and improve generalization,
which is applicable in both parameter-efficient and black-box tuning manners.
We additionally conduct extensive experiments on popular vision and language
models, including APIs, which are supervised and self-supervised pre-trained on
realistic noisy data for evaluation. Our analysis and results demonstrate the
importance of this novel and fundamental research direction, which we term as
Noisy Model Learning.

</details>


### [465] [Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization](https://arxiv.org/pdf/2502.04667)
*Xinhao Yao, Ruifeng Ren, Yun Liao, Yong Liu*

Main category: cs.LG

TL;DR: CoT training improves LLM reasoning by internalizing a two-stage generalizing circuit and enhancing OOD generalization through mastered subtasks and reasoning compositions.


<details>
  <summary>Details</summary>
Motivation: To understand how CoT training reshapes model representations and why it improves reasoning generalization in both ID and OOD scenarios.

Method: Controlled experiments and theoretical analysis, including structural advantage and information-theoretic generalization bounds.

Result: CoT training accelerates convergence, enhances generalization (ID and OOD), and maintains robust performance with noise.

Conclusion: CoT strategies can enhance LLM reasoning robustness by leveraging identified mechanisms.

Abstract: The integration of explicit Chain-of-Thought (CoT) reasoning into training
large language models (LLMs) has advanced their reasoning capabilities, yet the
mechanisms by which CoT enhances generalization remain poorly understood. This
work investigates (1) \textit{how} CoT training reshapes internal model
representations and (2) \textit{why} it improves both in-distribution (ID) and
out-of-distribution (OOD) reasoning generalization. Through controlled
experiments and theoretical analysis, we derive the following key insights.
\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a
two-stage generalizing circuit, where the number of stages corresponds to the
explicit reasoning steps during training. Notably, CoT-trained models resolve
intermediate results at shallower layers compared to non-CoT counterparts,
freeing up deeper layers to specialize in subsequent reasoning steps.
\textbf{2)} Theoretical Analysis: the information-theoretic generalization
bounds via distributional divergence can be decomposed into ID and OOD
components. While ID error diminishes with sufficient training regardless of
CoT, OOD error critically depends on CoT: Non-CoT training fails to generalize
to OOD samples due to unseen reasoning patterns, whereas CoT training achieves
near-perfect OOD generalization by mastering subtasks and reasoning
compositions during training. The identified mechanisms explain our
experimental results: CoT training accelerates convergence and enhances
generalization from ID to both ID and OOD scenarios while maintaining robust
performance even with tolerable noise. These findings are further validated on
complex real-world datasets. This paper offers valuable insights for designing
CoT strategies to enhance LLM reasoning robustness.

</details>


### [466] [One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data](https://arxiv.org/pdf/2302.06375)
*Simone Luetto, Fabrizio Garuti, Enver Sangineto, Lorenzo Forni, Rita Cucchiara*

Main category: cs.LG

TL;DR: A Transformer architecture is proposed for heterogeneous time-dependent tabular data, using frequency functions for numerical features and a unified loss function.


<details>
  <summary>Details</summary>
Motivation: Growing interest in applying Deep Learning to tabular data, especially time-dependent cases like financial transactions, but challenges arise from mixed categorical and numerical data.

Method: Transformer architecture with frequency functions for numerical features and a single loss function for uniform training.

Result: The method addresses the heterogeneity issue in tabular data with time dependence.

Conclusion: The proposed architecture effectively handles mixed data types in time-dependent tabular settings.

Abstract: There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.

</details>


### [467] [When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions](https://arxiv.org/pdf/2306.15546)
*Weiming Zhuang, Chen Chen, Jingtao Li, Chaochao Chen, Yaochu Jin, Lingjuan Lyu*

Main category: cs.LG

TL;DR: The paper explores the synergy between Foundation Models (FM) and Federated Learning (FL), highlighting mutual benefits like enhanced data diversity, collaborative development, and improved performance, while addressing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of FM (pre-trained knowledge, performance) and FL (collaborative learning, data privacy) to address real-world challenges and unlock new applications.

Method: Examines the interplay between FM and FL, discussing how FM can enhance FL (e.g., synthetic data generation) and how FL can support FM (e.g., avoiding monopoly).

Result: Identifies mutual benefits and challenges, proposing a collaborative framework for future research.

Conclusion: The paper lays a foundation for advancing both FM and FL by fostering their integration and inspiring further research.

Abstract: The intersection of Foundation Model (FM) and Federated Learning (FL)
presents a unique opportunity to unlock new possibilities for real-world
applications. On the one hand, FL, as a collaborative learning paradigm, help
address challenges in FM development by expanding data availability, enabling
computation sharing, facilitating the collaborative development of FMs,
tackling continuous data update, avoiding FM monopoly, response delay and FM
service down. On the other hand, FM, equipped with pre-trained knowledge and
exceptional performance, can serve as a robust starting point for FL. It can
also generate synthetic data to enrich data diversity and enhance overall
performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task
and multi-modality capabilities for FL. By examining the interplay between FL
and FM, this paper presents the motivations, challenges, and future directions
of empowering FL with FM and empowering FM with FL. We hope that this work
provides a good foundation to inspire future research efforts to drive
advancements in both fields.

</details>


### [468] [A simple connection from loss flatness to compressed neural representations](https://arxiv.org/pdf/2310.01770)
*Shirui Chen, Stefano Recanatesi, Eric Shea-Brown*

Main category: cs.LG

TL;DR: The paper explores the relationship between sharpness (a measure of loss landscape flatness) and neural network behavior, introducing three compression metrics (LVR, MLS, Local Dimensionality) and showing their correlation with sharpness.


<details>
  <summary>Details</summary>
Motivation: To clarify the unclear significance of sharpness in neural networks and its connection to generalization by studying its influence on local geometric features in feature space.

Method: Introduces three compression metrics (LVR, MLS, Local Dimensionality) and derives mathematical inequalities linking them to sharpness, validated empirically across various architectures.

Result: LVR and MLS correlate with loss flatness, with flatter losses corresponding to lower compression metrics. Inequalities predict positive correlation between compression and sharpness.

Conclusion: Sharpness influences neural representation compression, with derived inequalities providing a theoretical and empirical basis for this relationship.

Abstract: Sharpness, a geometric measure in the parameter space that reflects the
flatness of the loss landscape, has long been studied for its potential
connections to neural network behavior. While sharpness is often associated
with generalization, recent work highlights inconsistencies in this
relationship, leaving its true significance unclear. In this paper, we
investigate how sharpness influences the local geometric features of neural
representations in feature space, offering a new perspective on its role. We
introduce this problem and study three measures for compression: the Local
Volumetric Ratio (LVR), based on volume compression, the Maximum Local
Sensitivity (MLS), based on sensitivity to input changes, and the Local
Dimensionality, based on how uniform the sensitivity is on different
directions. We show that LVR and MLS correlate with the flatness of the loss
around the local minima; and that this correlation is predicted by a relatively
simple mathematical relationship: a flatter loss corresponds to a lower upper
bound on the compression metrics of neural representations. Our work builds
upon the linear stability insight by Ma and Ying, deriving inequalities between
various compression metrics and quantities involving sharpness. Our
inequalities readily extend to reparametrization-invariant sharpness as well.
Through empirical experiments on various feedforward, convolutional, and
transformer architectures, we find that our inequalities predict a consistently
positive correlation between local representation compression and sharpness.

</details>


### [469] [Supercharging Graph Transformers with Advective Diffusion](https://arxiv.org/pdf/2310.06417)
*Qitian Wu, Chenxiao Yang, Kaipeng Zeng, Michael Bronstein*

Main category: cs.LG

TL;DR: AdvDIFFormer, a physics-inspired graph Transformer, addresses generalization under topological shifts in non-Euclidean data, outperforming graph diffusion models.


<details>
  <summary>Details</summary>
Motivation: Prior studies neglect how machine learning models generalize under topological shifts in non-Euclidean data like graphs.

Method: AdvDIFFormer is derived from advective diffusion equations, enabling continuous message passing with observed and latent topological structures.

Result: The model provably controls generalization error under topological shifts and outperforms in tasks like information networks, molecular screening, and protein interactions.

Conclusion: AdvDIFFormer effectively addresses topological shift challenges, demonstrating superior generalization in diverse predictive tasks.

Abstract: The capability of generalization is a cornerstone for the success of modern
learning systems. For non-Euclidean data, e.g., graphs, that particularly
involves topological structures, one important aspect neglected by prior
studies is how machine learning models generalize under topological shifts.
This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model
designed to address this challenge. The model is derived from advective
diffusion equations which describe a class of continuous message passing
process with observed and latent topological structures. We show that
AdvDIFFormer has provable capability for controlling generalization error with
topological shifts, which in contrast cannot be guaranteed by graph diffusion
models. Empirically, the model demonstrates superiority in various predictive
tasks across information networks, molecular screening and protein
interactions.

</details>


### [470] [Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off](https://arxiv.org/pdf/2402.07002)
*Yuecheng Li, Lele Fu, Tong Wang, Jian Lou, Bin Chen, Lei Yang, Jian Shen, Zibin Zheng, Chuan Chen*

Main category: cs.LG

TL;DR: FedCEO is a novel federated learning framework balancing model utility and privacy by client collaboration, using tensor low-rank optimization to recover semantic integrity disrupted by differential privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy leakage in federated learning caused by differential privacy noise, which disrupts model semantics over time.

Method: Clients collaborate; server performs tensor low-rank proximal optimization on stacked local models to truncate high-frequency spectral components.

Result: Improved utility-privacy trade-off by √d; significant performance gains and strict privacy guarantees in experiments.

Conclusion: FedCEO effectively balances privacy and utility, recovering semantic integrity and outperforming existing methods.

Abstract: To defend against privacy leakage of user data, differential privacy is
widely used in federated learning, but it is not free. The addition of noise
randomly disrupts the semantic integrity of the model and this disturbance
accumulates with increased communication rounds. In this paper, we introduce a
novel federated learning framework with rigorous privacy guarantees, named
FedCEO, designed to strike a trade-off between model utility and user privacy
by letting clients ''Collaborate with Each Other''. Specifically, we perform
efficient tensor low-rank proximal optimization on stacked local model
parameters at the server, demonstrating its capability to flexibly truncate
high-frequency components in spectral space. This capability implies that our
FedCEO can effectively recover the disrupted semantic information by smoothing
the global semantic space for different privacy settings and continuous
training processes. Moreover, we improve the SOTA utility-privacy trade-off
bound by order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate
our theoretical results with experiments on representative datasets and observe
significant performance improvements and strict privacy guarantees under
different privacy settings. The code is available at
https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other.

</details>


### [471] [Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization](https://arxiv.org/pdf/2402.15290)
*Tongyi Liang, Han-Xiong Li*

Main category: cs.LG

TL;DR: Proposes efficient SSM (eSSM) to improve computational efficiency in state space models (SSMs) by reducing parameters and speeding up training.


<details>
  <summary>Details</summary>
Motivation: Existing SSMs struggle with balancing performance and computational efficiency, especially with large parameters.

Method: Uses MIMO SSM, diagonalization, fast tensor convolution, and block diagonalization to enhance efficiency.

Result: eSSM matches S4 performance, outperforms Transformers/LSTM, reduces parameters (12.89% of LSTM), and speeds up training (3.94x faster than LSTM).

Conclusion: eSSM is a highly efficient SSM variant with superior performance and computational advantages.

Abstract: Existing models encounter bottlenecks in balancing performance and
computational efficiency when modeling long sequences. Although the state space
model (SSM) has achieved remarkable success in handling long sequence tasks, it
still faces the problem of large number of parameters. In order to further
improve the efficiency of SSM, we propose a new state space layer based on
multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is
built on the convolutional representation of multi-input and multi-input (MIMO)
SSM. We propose a variety of effective strategies to improve the computational
efficiency. The diagonalization of the system matrix first decouples the
original system. Then a fast tensor convolution is proposed based on the fast
Fourier transform. In addition, the block diagonalization of the SSM further
reduces the model parameters and improves the model flexibility. Extensive
experimental results show that the performance of the proposed model on
multiple databases matches the performance of state-of-the-art models, such as
S4, and is significantly better than Transformers and LSTM. In the model
efficiency benchmark, the parameters of eSSM are only 12.89\% of LSTM and
13.24\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and
1.35 times faster than Mamba. Code is available at:
\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.

</details>


### [472] [REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories](https://arxiv.org/pdf/2402.16310)
*Bangchao Deng, Bingqing Qu, Pengyang Wang, Dingqi Yang, Benjamin Fankhauser, Philippe Cudre-Mauroux*

Main category: cs.LG

TL;DR: REPLAY, an RNN architecture, improves location prediction by capturing time-varying temporal regularities in human mobility, outperforming state-of-the-art methods by 7.7%-10.5%.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture time-varying temporal regularities in human mobility, which are crucial for accurate location prediction.

Method: REPLAY incorporates spatiotemporal distances and timestamp embeddings with learnable bandwidths to adapt to temporal regularities.

Result: REPLAY outperforms state-of-the-art methods by 7.7%-10.5% in location prediction tasks.

Conclusion: REPLAY effectively captures time-varying temporal regularities, enhancing location prediction accuracy and revealing mobility patterns.

Abstract: Location prediction forecasts a user's location based on historical user
mobility traces. To tackle the intrinsic sparsity issue of real-world user
mobility traces, spatiotemporal contexts have been shown as significantly
useful. Existing solutions mostly incorporate spatiotemporal distances between
locations in mobility traces, either by feeding them as additional inputs to
Recurrent Neural Networks (RNNs) or by using them to search for informative
past hidden states for prediction. However, such distance-based methods fail to
capture the time-varying temporal regularities of human mobility, where human
mobility is often more regular in the morning than in other periods, for
example; this suggests the usefulness of the actual timestamps besides the
temporal distances. Against this background, we propose REPLAY, a general RNN
architecture learning to capture the time-varying temporal regularities for
location prediction. Specifically, REPLAY not only resorts to the
spatiotemporal distances in sparse trajectories to search for the informative
past hidden states, but also accommodates the time-varying temporal
regularities by incorporating smoothed timestamp embeddings using Gaussian
weighted averaging with timestamp-specific learnable bandwidths, which can
flexibly adapt to the temporal regularities of different strengths across
different timestamps. Our extensive evaluation compares REPLAY against a
sizable collection of state-of-the-art techniques on two real-world datasets.
Results show that REPLAY consistently and significantly outperforms
state-of-the-art methods by 7.7\%-10.5\% in the location prediction task, and
the bandwidths reveal interesting patterns of the time-varying temporal
regularities.

</details>


### [473] [Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models](https://arxiv.org/pdf/2405.03869)
*Anshuman Chhabra, Bo Li, Jian Chen, Prasant Mohapatra, Hongfu Liu*

Main category: cs.LG

TL;DR: The paper proposes a Hessian-free method using outlier gradient detection to identify detrimental training samples, offering computational efficiency and insights into gradient roles.


<details>
  <summary>Details</summary>
Motivation: High computational costs of influence functions due to Hessian matrix inversion limit their use in large models. The paper aims to provide a simpler, efficient alternative.

Method: The approach transforms influence function-based sample identification into outlier gradient detection, avoiding Hessian calculations.

Result: Validated on synthetic datasets, the method effectively detects mislabeled samples in vision models and improves NLP transformer performance. It also aids fine-tuning Large Language Models.

Conclusion: The outlier gradient method is a practical, efficient alternative to influence functions for identifying detrimental training samples across various applications.

Abstract: A core data-centric learning challenge is the identification of training
samples that are detrimental to model performance. Influence functions serve as
a prominent tool for this task and offer a robust framework for assessing
training data influence on model predictions. Despite their widespread use,
their high computational cost associated with calculating the inverse of the
Hessian matrix pose constraints, particularly when analyzing large-sized deep
models. In this paper, we establish a bridge between identifying detrimental
training samples via influence functions and outlier gradient detection. This
transformation not only presents a straightforward and Hessian-free formulation
but also provides insights into the role of the gradient in sample impact.
Through systematic empirical evaluations, we first validate the hypothesis of
our proposed outlier gradient analysis approach on synthetic datasets. We then
demonstrate its effectiveness in detecting mislabeled samples in vision models
and selecting data samples for improving performance of natural language
processing transformer models. We also extend its use to influential sample
identification for fine-tuning Large Language Models.

</details>


### [474] [A Comprehensive Survey on Data Augmentation](https://arxiv.org/pdf/2405.09591)
*Zaitian Wang, Pengfei Wang, Kunpeng Liu, Pengyang Wang, Yanjie Fu, Chang-Tien Lu, Charu C. Aggarwal, Jian Pei, Yuanchun Zhou*

Main category: cs.LG

TL;DR: The paper proposes a new taxonomy for data augmentation techniques across multiple modalities, focusing on intrinsic relationships between data samples.


<details>
  <summary>Details</summary>
Motivation: Existing surveys lack a consistent summary of data augmentation methods across modalities, limiting understanding of how data samples serve the augmentation process.

Method: The survey introduces a modality-independent taxonomy, categorizing methods by intrinsic relationships (single-wise, pair-wise, population-wise) and unifying five data modalities.

Result: A more enlightening taxonomy is provided, enhancing comprehension of data augmentation techniques across diverse modalities.

Conclusion: The proposed taxonomy bridges gaps in existing literature, offering a unified perspective on data augmentation for improved AI model generalization.

Abstract: Data augmentation is a series of techniques that generate high-quality
artificial data by manipulating existing data samples. By leveraging data
augmentation techniques, AI models can achieve significantly improved
applicability in tasks involving scarce or imbalanced datasets, thereby
substantially enhancing AI models' generalization capabilities. Existing
literature surveys only focus on a certain type of specific modality data, and
categorize these methods from modality-specific and operation-centric
perspectives, which lacks a consistent summary of data augmentation methods
across multiple modalities and limits the comprehension of how existing data
samples serve the data augmentation process. To bridge this gap, we propose a
more enlightening taxonomy that encompasses data augmentation techniques for
different common data modalities. Specifically, from a data-centric
perspective, this survey proposes a modality-independent taxonomy by
investigating how to take advantage of the intrinsic relationship between data
samples, including single-wise, pair-wise, and population-wise sample data
augmentation methods. Additionally, we categorize data augmentation methods
across five data modalities through a unified inductive approach.

</details>


### [475] [Automatic Input Feature Relevance via Spectral Neural Networks](https://arxiv.org/pdf/2406.01183)
*Lorenzo Chicchi, Lorenzo Buffoni, Diego Febbe, Lorenzo Giambagli, Raffaele Marino, Duccio Fanelli*

Main category: cs.LG

TL;DR: A novel spectral-based method for ranking input feature importance in Deep Neural Networks, achieved during training without extra steps.


<details>
  <summary>Details</summary>
Motivation: To identify key input features for efficiency and decision-making insights in machine learning.

Method: Spectral re-parametrization of optimization, using eigenvalues of input nodes as relevance proxies.

Result: Successful application on synthetic and real data, providing automatic feature ranking.

Conclusion: The method efficiently ranks input features during training, aiding compact datasets and decision-making.

Abstract: In machine learning practice it is often useful to identify relevant input
features, so as to obtain compact dataset for more efficient numerical
handling. On the other hand, by isolating key input elements, ranked according
their respective degree of relevance, can help to elaborate on the process of
decision making. Here, we propose a novel method to estimate the relative
importance of the input components for a Deep Neural Network. This is achieved
by leveraging on a spectral re-parametrization of the optimization process.
Eigenvalues associated to input nodes provide in fact a robust proxy to gauge
the relevance of the supplied entry features. Notably, the spectral features
ranking is performed automatically, as a byproduct of the network training,
with no additional processing to be carried out. The technique is successfully
challenged against both synthetic and real data.

</details>


### [476] [Parallel Split Learning with Global Sampling](https://arxiv.org/pdf/2407.15738)
*Mohammad Kohankhaki, Ahmad Ayad, Mahdi Barhoush, Anke Schmeink*

Main category: cs.LG

TL;DR: A server-driven sampling strategy fixes global batch size by adjusting client-side batch sizes, improving scalability and generalization in distributed deep learning.


<details>
  <summary>Details</summary>
Motivation: Address scalability and generalization challenges in distributed deep learning due to large batch sizes and non-identical client data distributions.

Method: Introduce a server-driven sampling strategy to dynamically adjust client-side batch sizes, ensuring fixed global batch size and better data distribution reflection.

Result: Tighter deviation guarantees, improved model accuracy, training efficiency, and convergence stability on benchmark datasets.

Conclusion: The method offers a scalable solution for edge learning by decoupling batch size from device count and enhancing data representation.

Abstract: Distributed deep learning in resource-constrained environments faces
scalability and generalization challenges due to large effective batch sizes
and non-identically distributed client data. We introduce a server-driven
sampling strategy that maintains a fixed global batch size by dynamically
adjusting client-side batch sizes. This decouples the effective batch size from
the number of participating devices and ensures that global batches better
reflect the overall data distribution. Using standard concentration bounds, we
establish tighter deviation guarantees compared to existing approaches.
Empirical results on a benchmark dataset confirm that the proposed method
improves model accuracy, training efficiency, and convergence stability,
offering a scalable solution for learning at the network edge.

</details>


### [477] [FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models](https://arxiv.org/pdf/2408.10276)
*Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, Fenglong Ma*

Main category: cs.LG

TL;DR: FedKIM introduces a federated learning approach with knowledge injection to scale medical foundation models while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations in developing medical foundation models due to privacy constraints and lack of diverse modalities.

Method: Uses lightweight local models and an adaptive M3OE module to integrate knowledge into a centralized model.

Result: Effective across twelve tasks in seven modalities, demonstrating scalability without direct data access.

Conclusion: FedKIM offers a privacy-preserving solution to enhance medical foundation models.

Abstract: Foundation models have demonstrated remarkable capabilities in handling
diverse modalities and tasks, outperforming conventional artificial
intelligence (AI) approaches that are highly task-specific and
modality-reliant. In the medical domain, however, the development of
comprehensive foundation models is constrained by limited access to diverse
modalities and stringent privacy regulations. To address these constraints,
this study introduces a novel knowledge injection approach, FedKIM, designed to
scale the medical foundation model within a federated learning framework.
FedKIM leverages lightweight local models to extract healthcare knowledge from
private data and integrates this knowledge into a centralized foundation model
using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE)
module. This method not only preserves privacy but also enhances the model's
ability to handle complex medical tasks involving multiple modalities. Our
extensive experiments across twelve tasks in seven modalities demonstrate the
effectiveness of FedKIM in various settings, highlighting its potential to
scale medical foundation models without direct access to sensitive data.

</details>


### [478] [FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition](https://arxiv.org/pdf/2408.17090)
*Chen Hu, Hanchi Ren, Jingjing Deng, Xianghua Xie, Xiaoke Ma*

Main category: cs.LG

TL;DR: FissionVAE improves federated learning for non-IID data by decoupling latent spaces and customizing decoders for client groups, outperforming baseline VAEs.


<details>
  <summary>Details</summary>
Motivation: Address challenges of non-IID data in federated learning, particularly for VAEs, which lack attention compared to GANs.

Method: Introduces FissionVAE, decoupling latent spaces and using tailored decoder branches for client groups, with hierarchical VAEs and heterogeneous architectures.

Result: FissionVAE significantly enhances generation quality on composite datasets (MNIST/FashionMNIST and diverse RGB images) over baseline federated VAEs.

Conclusion: FissionVAE effectively handles non-IID data in federated learning, offering improved generation quality and tailored learning for diverse client groups.

Abstract: Federated learning is a machine learning paradigm that enables decentralized
clients to collaboratively learn a shared model while keeping all the training
data local. While considerable research has focused on federated image
generation, particularly Generative Adversarial Networks, Variational
Autoencoders have received less attention. In this paper, we address the
challenges of non-IID (independently and identically distributed) data
environments featuring multiple groups of images of different types. Non-IID
data distributions can lead to difficulties in maintaining a consistent latent
space and can also result in local generators with disparate texture features
being blended during aggregation. We thereby introduce FissionVAE that
decouples the latent space and constructs decoder branches tailored to
individual client groups. This method allows for customized learning that
aligns with the unique data distributions of each group. Additionally, we
incorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder
architectures within FissionVAE. We also explore strategies for setting the
latent prior distributions to enhance the decoupling process. To evaluate our
approach, we assemble two composite datasets: the first combines MNIST and
FashionMNIST; the second comprises RGB datasets of cartoon and human faces,
wild animals, marine vessels, and remote sensing images. Our experiments
demonstrate that FissionVAE greatly improves generation quality on these
datasets compared to baseline federated VAE models.

</details>


### [479] [Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain](https://arxiv.org/pdf/2403.06432)
*Jungwon Choi, Hyungi Lee, Byung-Hoon Kim, Juho Lee*

Main category: cs.LG

TL;DR: ST-JEMA, a self-supervised learning method for dynamic functional connectivity, outperforms previous methods in phenotype prediction using fMRI data, even with limited labeled samples.


<details>
  <summary>Details</summary>
Motivation: Labeled clinical data for training GNNs is scarce and resource-intensive, necessitating the use of unlabeled data for representation learning in dynamic functional connectivity.

Method: ST-JEMA, inspired by JEPA, reconstructs dynamic graphs to learn high-level semantic representations with temporal perspectives, applied to fMRI data.

Result: ST-JEMA shows superior performance in phenotype prediction and psychiatric diagnoses across eight fMRI datasets, even with limited samples.

Conclusion: ST-JEMA is a robust representation learning method for label-scarce fMRI data, demonstrating effectiveness in temporal reconstruction and phenotype prediction.

Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic
functional connectivity for distinguishing phenotypes from human brain
networks. However, obtaining extensive labeled clinical data for training is
often resource-intensive, making practical application difficult. Leveraging
unlabeled data thus becomes crucial for representation learning in a
label-scarce setting. Although generative self-supervised learning techniques,
especially masked autoencoders, have shown promising results in representation
learning in various domains, their application to dynamic graphs for dynamic
functional connectivity remains underexplored, facing challenges in capturing
high-level semantic representations. Here, we introduce the Spatio-Temporal
Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the
Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA
employs a JEPA-inspired strategy for reconstructing dynamic graphs, which
enables the learning of higher-level semantic representations considering
temporal perspectives, addressing the challenges in fMRI data representation
learning. Utilizing the large-scale UK Biobank dataset for self-supervised
learning, ST-JEMA shows exceptional representation learning performance on
dynamic functional connectivity demonstrating superiority over previous methods
in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI
datasets even with limited samples and effectiveness of temporal reconstruction
on missing data scenarios. These findings highlight the potential of our
approach as a robust representation learning method for leveraging label-scarce
fMRI data.

</details>


### [480] [Generative-Contrastive Heterogeneous Graph Neural Network](https://arxiv.org/pdf/2404.02810)
*Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang, Xindong Wu*

Main category: cs.LG

TL;DR: GC-HGNN enhances contrastive learning for heterogeneous graphs with generative methods, addressing data augmentation and sampling bias, outperforming baselines in node classification and link prediction.


<details>
  <summary>Details</summary>
Motivation: Existing CL-based HGNNs face limitations in data augmentation and suffer from sampling bias and lack of local heterogeneous information.

Method: Proposes a generative-contrastive approach with masked autoencoder augmentation, position/semantics-aware sampling, and hierarchical contrastive learning.

Result: Outperforms 17 baselines on 8 datasets for node classification and link prediction.

Conclusion: GC-HGNN effectively addresses limitations and improves performance in heterogeneous graph tasks.

Abstract: Heterogeneous Graphs (HGs) effectively model complex relationships in the
real world through multi-type nodes and edges. In recent years, inspired by
self-supervised learning (SSL), contrastive learning (CL)-based Heterogeneous
Graphs Neural Networks (HGNNs) have shown great potential in utilizing data
augmentation and contrastive discriminators for downstream tasks. However, data
augmentation remains limited due to the graph data's integrity. Furthermore,
the contrastive discriminators suffer from sampling bias and lack local
heterogeneous information. To tackle the above limitations, we propose a novel
Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN).
Specifically, we propose a heterogeneous graph generative learning method that
enhances CL-based paradigm. This paradigm includes: 1) A contrastive view
augmentation strategy using a masked autoencoder. 2) Position-aware and
semantics-aware positive sample sampling strategy for generating hard negative
samples. 3) A hierarchical contrastive learning strategy aimed at capturing
local and global information. Furthermore, the hierarchical contrastive
learning and sampling strategies aim to constitute an enhanced contrastive
discriminator under the generative-contrastive perspective. Finally, we compare
our model with seventeen baselines on eight real-world datasets. Our model
outperforms the latest baselines on node classification and link prediction
tasks.

</details>


### [481] [M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the JEPA framework](https://arxiv.org/pdf/2409.05929)
*Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji*

Main category: cs.LG

TL;DR: M3-Jepa introduces a scalable multimodal alignment framework using a multi-directional mixture of experts (MoE) to optimize alignment in latent space, achieving state-of-the-art performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current multimodal alignment methods may cause information bias by working on original token space. JEPA addresses this but lacks multimodal application.

Method: M3-Jepa uses a multi-directional MoE predictor to align modalities in latent space, alternating optimization between uni-directional tasks.

Result: Achieves state-of-the-art performance, generalizes to unseen datasets, and is computationally efficient.

Conclusion: M3-Jepa offers a new paradigm for self-supervised learning and open-world modeling.

Abstract: Current multimodal alignment strategies primarily use single or unified
modality encoders, while optimizing the alignment on the original token space.
Such a framework is easy to implement and incorporate with the pretrained
knowledge, but might result in information bias. To deal with such issues, the
joint encoding predictive architecture (JEPA) learns the alignment loss on the
latent space, with a predictor to convert the input encoding to the output
latent space. However, the application of JEPA in multimodal scenarios is
limited so far. In this paper, we introduce M3-Jepa, a scalable multimodal
alignment framework, with the predictor implemented by a multi-directional
mixture of experts (MoE). We demonstrate the framework can maximize the mutual
information with information theory derivations, by alternating the
optimization between different uni-directional tasks. By thoroughly designed
experiments, we show that M3-Jepa can obtain state-of-the-art performance on
different modalities and tasks, generalize to unseen datasets and domains, and
is computationally efficient in training and inference. Our study indicates
that M3-Jepa might provide a new paradigm to self-supervised learning and
open-world modeling.

</details>


### [482] [tPARAFAC2: Tracking evolving patterns in (incomplete) temporal data](https://arxiv.org/pdf/2407.01356)
*Christos Chatzis, Carla Schenker, Max Pfeffer, Evrim Acar*

Main category: cs.LG

TL;DR: The paper introduces tPARAFAC2, a method for tracking evolving patterns in time-evolving tensor data, addressing limitations of existing methods by incorporating temporal smoothness regularization and handling missing data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for tracking evolving patterns in tensor data either impose overly constrained structural requirements or lack uniqueness, which is critical for interpretation. The paper aims to address these gaps.

Method: The tPARAFAC2 model uses temporal smoothness regularization on evolving factors and extends an existing algorithmic framework (AO and ADMM) to handle partially observed data.

Result: Numerical experiments show tPARAFAC2 accurately extracts evolving patterns even with high noise and missing data, outperforming state-of-the-art methods. Real datasets validate its effectiveness.

Conclusion: tPARAFAC2 is effective for revealing evolving patterns in tensor data, especially with missing data, though the paper acknowledges its limitations and compares various approaches for handling missing data.

Abstract: Tensor factorizations have been widely used for the task of uncovering
patterns in various domains. Often, the input is time-evolving, shifting the
goal to tracking the evolution of the underlying patterns instead. To adapt to
this more complex setting, existing methods incorporate temporal regularization
but they either have overly constrained structural requirements or lack
uniqueness which is crucial for interpretation. In this paper, in order to
capture the underlying evolving patterns, we introduce t(emporal)PARAFAC2,
which utilizes temporal smoothness regularization on the evolving factors.
Previously, Alternating Optimization (AO) and Alternating Direction Method of
Multipliers (ADMM)-based algorithmic approach has been introduced to fit the
PARAFAC2 model to fully observed data. In this paper, we extend this
algorithmic framework to the case of partially observed data and use it to fit
the tPARAFAC2 model to complete and incomplete datasets with the goal of
revealing evolving patterns. Our numerical experiments on simulated datasets
demonstrate that tPARAFAC2 can extract the underlying evolving patterns more
accurately compared to the state-of-the-art in the presence of high amounts of
noise and missing data. Using two real datasets, we also demonstrate the
effectiveness of the algorithmic approach in terms of handling missing data and
tPARAFAC2 model in terms of revealing evolving patterns. The paper provides an
extensive comparison of different approaches for handling missing data within
the proposed framework, and discusses both the advantages and limitations of
tPARAFAC2 model.

</details>


### [483] [SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series Imputation](https://arxiv.org/pdf/2410.03954)
*Amir Eskandari, Aman Anand, Drishti Sharma, Farhana Zulkernine*

Main category: cs.LG

TL;DR: Proposes SDA-GRIN, a method for imputing missing data in multivariate time series by capturing dynamic spatial dependencies using graph structures and attention mechanisms, showing improved performance on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Missing data in multivariate time series disrupts systems; existing methods ignore dynamic spatial dependencies.

Method: Uses a multi-head attention mechanism to adapt graph structures over time, modeling time series as temporal graphs with recurrent message-passing.

Result: Improves MSE by 9.51% (AQI), 9.40% (AQI-36), and 1.94% (PEMS-BAY). Ablation studies validate the impact of window sizes and missing data.

Conclusion: SDA-GRIN effectively addresses dynamic spatial dependencies for missing data imputation, outperforming existing methods.

Abstract: In various applications, the multivariate time series often suffers from
missing data. This issue can significantly disrupt systems that rely on the
data. Spatial and temporal dependencies can be leveraged to impute the missing
samples. Existing imputation methods often ignore dynamic changes in spatial
dependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation
Network (SDA-GRIN) which is capable of capturing dynamic changes in spatial
dependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph
structures with time. SDA-GRIN models multivariate time series as a sequence of
temporal graphs and uses a recurrent message-passing architecture for
imputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves
MSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it
achieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the
effect of window sizes and missing data on the performance of the method.
Project page:https://ameskandari.github.io/sda-grin/

</details>


### [484] [A Pattern Language for Machine Learning Tasks](https://arxiv.org/pdf/2407.02424)
*Benjamin Rodatz, Ian Fan, Tuomas Laakkonen, Neil John Ortega, Thomas Hoffmann, Vincent Wang-Mascianica*

Main category: cs.LG

TL;DR: The paper formalizes objective functions as equality constraints ('tasks') on learner composites, proposing a graphical framework to unify ML approaches, optimize behaviors model-agnostically, and bridge theory with practice. A 'manipulator' task demonstrates practical impact.


<details>
  <summary>Details</summary>
Motivation: To unify diverse ML approaches, optimize model behaviors without dependency on specific architectures, and integrate theoretical insights into practical applications.

Method: Develops a graphical mathematics for tasks, enabling model-agnostic design and optimization. Introduces a 'manipulator' task as proof-of-concept.

Result: Achieves end-to-end data manipulation with desired attributes, avoiding custom architectures, adversarial training, or data interventions.

Conclusion: The framework offers a unified, practical, and stable approach to model behavior design, bridging theory and application in ML.

Abstract: We formalise the essential data of objective functions as equality
constraints on composites of learners. We call these constraints "tasks", and
we investigate the idealised view that such tasks determine model behaviours.
We develop a flowchart-like graphical mathematics for tasks that allows us to;
(1) offer a unified perspective of approaches in machine learning across
domains; (2) design and optimise desired behaviours model-agnostically; and (3)
import insights from theoretical computer science into practical machine
learning. As a proof-of-concept of the potential practical impact of our
theoretical framework, we exhibit and implement a novel "manipulator" task that
minimally edits input data to have a desired attribute. Our model-agnostic
approach achieves this end-to-end, and without the need for custom
architectures, adversarial training, random sampling, or interventions on the
data, hence enabling capable, small-scale, and training-stable models.

</details>


### [485] [Hard-Constrained Neural Networks with Universal Approximation Guarantees](https://arxiv.org/pdf/2410.10807)
*Youngjae Min, Navid Azizan*

Main category: cs.LG

TL;DR: HardNet is a framework for neural networks that enforces hard constraints during training without sacrificing performance, ensuring safety-critical compliance.


<details>
  <summary>Details</summary>
Motivation: Existing methods use soft constraints, which lack guarantees for safety-critical applications, while hard constraints may reduce model performance.

Method: HardNet appends a differentiable enforcement layer to ensure hard constraints are met during end-to-end training, maintaining model capacity.

Result: HardNet efficiently enforces multiple input-dependent constraints, retains universal approximation, and improves performance in various applications.

Conclusion: HardNet provides a practical solution for hard constraint satisfaction in neural networks, enhancing safety and performance in critical tasks.

Abstract: Incorporating prior knowledge or specifications of input-output relationships
into machine learning models has gained significant attention, as it enhances
generalization from limited data and leads to conforming outputs. However, most
existing approaches use soft constraints by penalizing violations through
regularization, which offers no guarantee of constraint satisfaction--an
essential requirement in safety-critical applications. On the other hand,
imposing hard constraints on neural networks may hinder their representational
power, adversely affecting performance. To address this, we propose HardNet, a
practical framework for constructing neural networks that inherently satisfy
hard constraints without sacrificing model capacity. Unlike approaches that
modify outputs only at inference time, HardNet enables end-to-end training with
hard constraint guarantees, leading to improved performance. To the best of our
knowledge, HardNet is the first method with an efficient forward pass to
enforce more than one input-dependent inequality constraint. It allows
unconstrained optimization of the network parameters using standard algorithms
by appending a differentiable closed-form enforcement layer to the network's
output. Furthermore, we show that HardNet retains the universal approximation
capabilities of neural networks. We demonstrate the versatility and
effectiveness of HardNet across various applications: learning with piecewise
constraints, learning optimization solvers, optimizing control policies in
safety-critical systems, and learning safe decision logic for aircraft systems.

</details>


### [486] [FIVB ranking: Misstep in the right direction](https://arxiv.org/pdf/2408.01603)
*Salma Tenni, Daniel Gomes de Pinho Zanco, Leszek Szczecinski*

Main category: cs.LG

TL;DR: The paper evaluates FIVB's 2020 ranking algorithm, highlighting its probabilistic model for match outcomes. It suggests improvements like adding home-field advantage and adjusting parameters.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness and optimality of FIVB's probabilistic ranking algorithm, which is novel for multi-level outcomes in sports.

Method: Analytical and numerical methods to evaluate the algorithm's parameters and performance, including testing home-field advantage and match result weighting.

Result: Current thresholds fit the data well, but adding home-field advantage improves the model. Adjusting parameters enhances performance, while match result weighting is counterproductive.

Conclusion: The FIVB ranking algorithm is effective but could benefit from incorporating home-field advantage and revised parameters, while avoiding match result weighting.

Abstract: This work presents and evaluates the ranking algorithm that has been used by
Federation Internationale de Volleyball (FIVB) since 2020. The prominent
feature of the FIVB ranking is the use of the probabilistic model, which
explicitly calculates the probabilities of the future matches results using the
estimated teams' strengths. Such explicit modeling is new in the context of
official sport rankings, especially for multi-level outcomes, and we study the
optimality of its parameters using both analytical and numerical methods. We
conclude that from the modeling perspective, the current thresholds fit well
the data but adding the home-field advantage (HFA) would be beneficial.
Regarding the algorithm itself, we explain the rationale behind the
approximations currently used and show a simple method to find new parameters
(numerical score) which improve the performance. We also show that the
weighting of the match results is counterproductive.

</details>


### [487] [Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds](https://arxiv.org/pdf/2408.06996)
*Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: The paper explores lower bounds for approximating Sobolev functions on manifolds, showing the required statistical complexity depends only on intrinsic manifold properties like curvature and volume.


<details>
  <summary>Details</summary>
Motivation: To address the practical implications of the manifold hypothesis by deriving dimension-independent bounds for approximation tasks.

Method: Analyzing optimal uniform approximations of bounded Sobolev functions on compact manifolds, focusing on lower bounds.

Result: Demonstrates that the statistical complexity for approximation is bounded from below and depends solely on intrinsic manifold properties.

Conclusion: The findings highlight the fundamental difficulty of approximation on manifolds, with implications for theoretical guarantees in high-dimensional data analysis.

Abstract: The manifold hypothesis says that natural high-dimensional data lie on or
around a low-dimensional manifold. The recent success of statistical and
learning-based methods in very high dimensions empirically supports this
hypothesis, suggesting that typical worst-case analysis does not provide
practical guarantees. A natural step for analysis is thus to assume the
manifold hypothesis and derive bounds that are independent of any ambient
dimensions that the data may be embedded in. Theoretical implications in this
direction have recently been explored in terms of generalization of ReLU
networks and convergence of Langevin methods. In this work, we consider optimal
uniform approximations with functions of finite statistical complexity. While
upper bounds on uniform approximation exist in the literature using ReLU neural
networks, we consider the opposite: lower bounds to quantify the fundamental
difficulty of approximation on manifolds. In particular, we demonstrate that
the statistical complexity required to approximate a class of bounded Sobolev
functions on a compact manifold is bounded from below, and moreover that this
bound is dependent only on the intrinsic properties of the manifold, such as
curvature, volume, and injectivity radius.

</details>


### [488] [Enhanced BPINN Training Convergence in Solving General and Multi-scale Elliptic PDEs with Noise](https://arxiv.org/pdf/2408.09340)
*Yilong Hou, Xi'an Li, Jinran Wu, You-Gan Wang*

Main category: cs.LG

TL;DR: The paper proposes a robust multi-scale BPINN (MBPINN) method to improve Hamiltonian Monte Carlo (HMC) convergence in Bayesian Physics Informed Neural Networks (BPINN) for solving multi-scale PDEs.


<details>
  <summary>Details</summary>
Motivation: HMC in BPINN often performs poorly and converges inadequately for given step sizes, limiting its practicality for multi-scale PDEs.

Method: MBPINN integrates multi-scale deep neural networks (MscaleDNN) and BPINN, reframing HMC with Stochastic Gradient Descent (SGD) and using Fourier feature mapping-induced MscaleDNN.

Result: MBPINN outperforms HMC in robustness, computational cost, and flexibility, successfully solving Poisson and multi-scale elliptic problems in 1D/2D spaces.

Conclusion: MBPINN shows promise for physics-informed machine learning, especially for ill-posed problems in parameter estimation and solution recovery.

Abstract: Bayesian Physics Informed Neural Networks (BPINN) have attracted considerable
attention for inferring the system states and physical parameters of
differential equations according to noisy observations. However, in practice,
Hamiltonian Monte Carlo (HMC) used to estimate the internal parameters of the
solver for BPINN often encounters these troubles including poor performance and
awful convergence for a given step size used to adjust the momentum of those
parameters. To address the convergence of HMC for the BPINN method and extend
its application scope to multi-scale partial differential equations (PDE), we
develop a robust multi-scale BPINN (dubbed MBPINN) method by integrating
multi-scale deep neural networks (MscaleDNN) and the BPINN framework. In this
newly proposed MBPINN method, we reframe HMC with Stochastic Gradient Descent
(SGD) to ensure the most ``likely'' estimation is always provided, and we
configure its solver as a Fourier feature mapping-induced MscaleDNN. This novel
method offers several key advantages: (1) it is more robust than HMC, (2) it
incurs less computational cost than HMC, and (3) it is more flexible for
complex problems. We demonstrate the applicability and performance of the
proposed method through some general Poisson and multi-scale elliptic problems
in one and two-dimensional Euclidean spaces. Our findings indicate that the
proposed method can avoid HMC failures and provide valid results. Additionally,
our method is capable of handling complex elliptic PDE and producing comparable
results for general elliptic PDE under the case of lower signal-to-noise rate.
These findings suggest that our proposed approach has great potential for
physics-informed machine learning for parameter estimation and solution
recovery in the case of ill-posed problems.

</details>


### [489] [Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning](https://arxiv.org/pdf/2408.12307)
*Yen-Ru Lai, Fu-Chieh Chang, Pei-Yuan Wu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Offline reinforcement learning (RL) learns policies from a fixed dataset, but
often requires large amounts of data. The challenge arises when labeled
datasets are expensive, especially when rewards have to be provided by human
labelers for large datasets. In contrast, unlabelled data tends to be less
expensive. This situation highlights the importance of finding effective ways
to use unlabelled data in offline RL, especially when labelled data is limited
or expensive to obtain. In this paper, we present the algorithm to utilize the
unlabeled data in the offline RL method with kernel function approximation and
give the theoretical guarantee. We present various eigenvalue decay conditions
of $\mathcal{H}_k$ which determine the complexity of the algorithm. In summary,
our work provides a promising approach for exploiting the advantages offered by
unlabeled data in offline RL, whilst maintaining theoretical assurances.

</details>


### [490] [CoDiCast: Conditional Diffusion Model for Global Weather Prediction with Uncertainty Quantification](https://arxiv.org/pdf/2409.05975)
*Jimeng Shi, Bowen Jin, Jiawei Han, Sundararaman Gopalakrishnan, Giri Narasimhan*

Main category: cs.LG

TL;DR: CoDiCast is a conditional diffusion model for accurate, efficient, and uncertainty-quantified global weather forecasting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing weather forecasting methods lack a balance of accuracy, uncertainty quantification, and computational efficiency. Ensemble NWP is computationally heavy, while MLWP lacks uncertainty capture.

Method: CoDiCast uses a conditional diffusion model to generate ensemble forecasts by simulating a reverse denoising process, conditioned on recent observations.

Result: CoDiCast achieves high accuracy, generates 6-day forecasts at 6-hour steps, and runs efficiently on a commodity GPU.

Conclusion: CoDiCast successfully addresses the limitations of current methods, offering a scalable and accurate solution for weather forecasting with uncertainty quantification.

Abstract: Accurate weather forecasting is critical for science and society. Yet,
existing methods have not managed to simultaneously have the properties of high
accuracy, low uncertainty, and high computational efficiency. On one hand, to
quantify the uncertainty in weather predictions, the strategy of ensemble
forecast (i.e., generating a set of diverse predictions) is often employed.
However, traditional ensemble numerical weather prediction (NWP) is
computationally intensive. On the other hand, most existing machine
learning-based weather prediction (MLWP) approaches are efficient and accurate.
Nevertheless, they are deterministic and cannot capture the uncertainty of
weather forecasting. In this work, we propose CoDiCast, a conditional diffusion
model to generate accurate global weather prediction, while achieving
uncertainty quantification with ensemble forecasts and modest computational
cost. The key idea is to simulate a conditional version of the reverse
denoising process in diffusion models, which starts from pure Gaussian noise to
generate realistic weather scenarios for a future time point. Each denoising
step is conditioned on observations from the recent past. Ensemble forecasts
are achieved by repeatedly sampling from stochastic Gaussian noise to represent
uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis
data from the European Centre for Medium-Range Weather Forecasts (ECMWF).
Experimental results demonstrate that our approach outperforms several existing
data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can
generate 6-day global weather forecasts, at 6-hour steps and $5.625^\circ$
latitude-longitude resolution, for over 5 variables, in about 12 minutes on a
commodity A100 GPU machine with 80GB memory. The open-souced code is provided
at https://github.com/JimengShi/CoDiCast.

</details>


### [491] [Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling](https://arxiv.org/pdf/2409.11529)
*Lukas Schynol, Marius Pesavento*

Main category: cs.LG

TL;DR: The paper proposes a robust tensor decomposition and deep unrolling approach for anomaly detection in network flows, addressing data efficiency, domain adaptation, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection is crucial for resilient communication systems, but deep learning faces challenges like data efficiency and interpretability.

Method: Uses a block-successive convex approximation algorithm with low-rank tensor modeling for normal flows and sparse anomalies, augmented for efficiency. Deep unrolling creates a learnable network architecture with online adaptation.

Result: The proposed method shows high training data efficiency, outperforms reference methods, and adapts to varying topologies.

Conclusion: The approach effectively addresses key challenges in anomaly detection, offering a scalable and interpretable solution.

Abstract: Anomaly detection (AD) is increasingly recognized as a key component for
ensuring the resilience of future communication systems. While deep learning
has shown state-of-the-art AD performance, its application in critical systems
is hindered by concerns regarding training data efficiency, domain adaptation
and interpretability. This work considers AD in network flows using incomplete
measurements, leveraging a robust tensor decomposition approach and deep
unrolling techniques to address these challenges. We first propose a novel
block-successive convex approximation algorithm based on a regularized
model-fitting objective where the normal flows are modeled as low-rank tensors
and anomalies as sparse. An augmentation of the objective is introduced to
decrease the computational cost. We apply deep unrolling to derive a novel deep
network architecture based on our proposed algorithm, treating the
regularization parameters as learnable weights. Inspired by Bayesian
approaches, we extend the model architecture to perform online adaptation to
per-flow and per-time-step statistics, improving AD performance while
maintaining a low parameter count and preserving the problem's permutation
equivariances. To optimize the deep network weights for detection performance,
we employ a homotopy optimization approach based on an efficient approximation
of the area under the receiver operating characteristic curve. Extensive
experiments on synthetic and real-world data demonstrate that our proposed deep
network architecture exhibits a high training data efficiency, outperforms
reference methods, and adapts seamlessly to varying network topologies.

</details>


### [492] [CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction](https://arxiv.org/pdf/2501.01010)
*Mohammad Shahab Sepehri, Asal Mehradfar, Mahdi Soltanolkotabi, Salman Avestimehr*

Main category: cs.LG

TL;DR: CryptoMamba, a Mamba-based State Space Model, outperforms traditional models like ARIMA and LSTMs in predicting Bitcoin prices by capturing long-range dependencies and regime shifts.


<details>
  <summary>Details</summary>
Motivation: Bitcoin price prediction is challenging due to volatility and non-linear dynamics. Existing models (ARIMA, GARCH, LSTMs) fail to capture regime shifts and long-range dependencies.

Method: Proposes CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture, designed for financial time-series data.

Result: CryptoMamba provides more accurate predictions and better generalizability across market conditions, outperforming previous models.

Conclusion: SSMs, like CryptoMamba, show significant potential for stock and cryptocurrency price forecasting, offering practical utility in trading.

Abstract: Predicting Bitcoin price remains a challenging problem due to the high
volatility and complex non-linear dynamics of cryptocurrency markets.
Traditional time-series models, such as ARIMA and GARCH, and recurrent neural
networks, like LSTMs, have been widely applied to this task but struggle to
capture the regime shifts and long-range dependencies inherent in the data. In
this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM)
architecture designed to effectively capture long-range dependencies in
financial time-series data. Our experiments show that CryptoMamba not only
provides more accurate predictions but also offers enhanced generalizability
across different market conditions, surpassing the limitations of previous
models. Coupled with trading algorithms for real-world scenarios, CryptoMamba
demonstrates its practical utility by translating accurate forecasts into
financial outcomes. Our findings signal a huge advantage for SSMs in stock and
cryptocurrency price forecasting tasks.

</details>


### [493] [Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport](https://arxiv.org/pdf/2410.00844)
*Zhenyi Zhang, Tiejun Li, Peijie Zhou*

Main category: cs.LG

TL;DR: A deep learning method for reconstructing dynamics from sparse snapshots using regularized unbalanced optimal transport (RUOT), applicable to stochastic processes without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of inferring continuous dynamics from sparsely time-resolved snapshots in natural sciences and machine learning.

Method: Introduces a deep learning approach for RUOT, modeling dynamics without prior knowledge of growth/death processes, leveraging connections to the Schrödinger bridge problem.

Result: Demonstrated effectiveness on synthetic gene networks, Gaussian Mixture Models, and single-cell RNA-seq data, accurately identifying growth/transition patterns and eliminating false transitions.

Conclusion: The method successfully reconstructs dynamics and constructs developmental landscapes, outperforming other approaches.

Abstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots
is an important problem in both natural sciences and machine learning. Here, we
introduce a new deep learning approach for solving regularized unbalanced
optimal transport (RUOT) and inferring continuous unbalanced stochastic
dynamics from observed snapshots. Based on the RUOT form, our method models
these dynamics without requiring prior knowledge of growth and death processes
or additional information, allowing them to be learned directly from data.
Theoretically, we explore the connections between the RUOT and Schr\"odinger
bridge problem and discuss the key challenges and potential solutions. The
effectiveness of our method is demonstrated with a synthetic gene regulatory
network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data
from blood development. Compared with other methods, our approach accurately
identifies growth and transition patterns, eliminates false transitions, and
constructs the Waddington developmental landscape. Our code is available at:
https://github.com/zhenyiizhang/DeepRUOT.

</details>


### [494] [HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR](https://arxiv.org/pdf/2410.02597)
*Hainan Xu, Travis M. Bartley, Vladimir Bataev, Boris Ginsburg*

Main category: cs.LG

TL;DR: HAINAN is a flexible speech recognition model combining autoregressive and non-autoregressive inference, achieving efficiency and accuracy improvements over existing models.


<details>
  <summary>Details</summary>
Motivation: To enhance speech recognition by balancing accuracy and speed, addressing limitations of existing models like TDT and CTC.

Method: Extends TDT with masked predictor training, supporting autoregressive, non-autoregressive, and semi-autoregressive inference.

Result: HAINAN matches CTC/TDT efficiency, outperforms TDT/RNN-T in accuracy, and semi-autoregressive mode further improves results.

Conclusion: HAINAN is a versatile and efficient solution for real-world speech recognition, excelling in both accuracy and speed.

Abstract: We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel
architecture for speech recognition that extends the Token-and-Duration
Transducer (TDT) model. Trained with randomly masked predictor network outputs,
HAINAN supports both autoregressive inference with all network components and
non-autoregressive inference without the predictor. Additionally, we propose a
novel semi-autoregressive inference paradigm that first generates an initial
hypothesis using non-autoregressive inference, followed by refinement steps
where each token prediction is regenerated using parallelized autoregression on
the initial hypothesis. Experiments on multiple datasets across different
languages demonstrate that HAINAN achieves efficiency parity with CTC in
non-autoregressive mode and with TDT in autoregressive mode. In terms of
accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while
non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive
inference further enhances the model's accuracy with minimal computational
overhead, and even outperforms TDT results in some cases. These results
highlight HAINAN's flexibility in balancing accuracy and speed, positioning it
as a strong candidate for real-world speech recognition applications.

</details>


### [495] [Understanding Large Language Models in Your Pockets: Performance Study on COTS Mobile Devices](https://arxiv.org/pdf/2410.03613)
*Jie Xiao, Qianyi Huang, Xu Chen, Chen Tian*

Main category: cs.LG

TL;DR: The paper evaluates the performance of lightweight LLMs on mobile devices, focusing on user experience metrics and developer-critical factors, while comparing mobile SoCs.


<details>
  <summary>Details</summary>
Motivation: Growing privacy concerns drive local LLM deployment, but performance on mobile devices is unclear, necessitating a comprehensive study.

Method: A measurement study on mobile devices, evaluating token throughput, latency, battery consumption, resource utilization, DVFS strategies, and inference engines.

Result: Detailed analysis of hardware and system impacts on LLM performance, with comparisons across major mobile SoCs.

Conclusion: The study offers insights for improving on-device LLMs and future mobile system designs.

Abstract: As large language models (LLMs) increasingly integrate into every aspect of
our work and daily lives, there are growing concerns about user privacy, which
push the trend toward local deployment of these models. There are a number of
lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on
smartphones, providing users with greater control over their personal data. As
a rapidly emerging application, we are concerned about their performance on
commercial-off-the-shelf mobile devices. To fully understand the current
landscape of LLM deployment on mobile platforms, we conduct a comprehensive
measurement study on mobile devices. We evaluate both metrics that affect user
experience, including token throughput, latency, and battery consumption, as
well as factors critical to developers, such as resource utilization, DVFS
strategies, and inference engines. In addition, we provide a detailed analysis
of how these hardware capabilities and system dynamics affect on-device LLM
performance, which may help developers identify and address bottlenecks for
mobile LLM applications. We also provide comprehensive comparisons across the
mobile system-on-chips (SoCs) from major vendors, highlighting their
performance differences in handling LLM workloads. We hope that this study can
provide insights for both the development of on-device LLMs and the design for
future mobile system architecture.

</details>


### [496] [T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data](https://arxiv.org/pdf/2410.05016)
*Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan*

Main category: cs.LG

TL;DR: T-JEPA is an augmentation-free SSL method for tabular data using latent space predictions, improving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Self-supervised learning for tabular data is challenging due to the difficulty of constructing augmentations. T-JEPA addresses this by avoiding augmentations altogether.

Method: T-JEPA uses a Joint Embedding Predictive Architecture (JEPA) to predict latent representations of feature subsets within the same sample, learning rich representations without augmentations.

Result: T-JEPA outperforms models trained on original data and matches or exceeds traditional methods like Gradient Boosted Decision Trees in classification and regression tasks.

Conclusion: T-JEPA is effective for tabular data SSL, learning meaningful representations and identifying relevant features without labels, aided by novel regularization tokens.

Abstract: Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data.

</details>


### [497] [Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling](https://arxiv.org/pdf/2502.06491)
*Shenghong He*

Main category: cs.LG

TL;DR: RT, a new MORL algorithm, ensures reliable trajectory generation by incorporating historical information and cumulative reliability, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MORL methods neglect historical information, leading to unreliable trajectories. RT addresses this gap by leveraging cumulative reliability and high-reward action sampling.

Method: RT uses a weighted variational distance to measure trajectory reliability and samples high-reward actions to generate high-return trajectories.

Result: RT theoretically guarantees performance and empirically outperforms state-of-the-art MORL methods on benchmark tasks.

Conclusion: RT improves MORL by ensuring reliable trajectory generation and efficient policy learning, validated by theory and experiments.

Abstract: Model-based offline reinforcement learning (MORL) aims to learn a policy by
exploiting a dynamics model derived from an existing dataset. Applying
conservative quantification to the dynamics model, most existing works on MORL
generate trajectories that approximate the real data distribution to facilitate
policy learning by using current information (e.g., the state and action at
time step $t$). However, these works neglect the impact of historical
information on environmental dynamics, leading to the generation of unreliable
trajectories that may not align with the real data distribution. In this paper,
we propose a new MORL algorithm \textbf{R}eliability-guaranteed
\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by
calculating the cumulative reliability of the generated trajectory (i.e., using
a weighted variational distance away from the real data). Moreover, by sampling
candidate actions with high rewards, RT can efficiently generate high-return
trajectories from the existing offline data. We theoretically prove the
performance guarantees of RT in policy learning, and empirically demonstrate
its effectiveness against state-of-the-art model-based methods on several
benchmark tasks.

</details>


### [498] [AI-based particle track identification in scintillating fibres read out with imaging sensors](https://arxiv.org/pdf/2410.10519)
*Noemi Bührer, Saúl Alonso-Monsalve, Matthew Franks, Till Dieminger, Davide Sgalaberna*

Main category: cs.LG

TL;DR: An AI-based method using a variational autoencoder (VAE) for particle track identification in scintillating fibres, achieving efficient signal-background separation and fast real-time processing.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of filtering and identifying particle tracks from large datasets generated by SPAD array sensors, leveraging AI for efficient anomaly detection.

Method: A VAE model trained on background frames to distinguish particle tracks from noise, validated with experimental data.

Result: The VAE demonstrated high capability in identifying relevant events quickly, suitable for real-time deployment.

Conclusion: Combining advanced sensors with machine learning enhances particle detection, showing promise for real-time applications.

Abstract: This paper presents the development and application of an AI-based method for
particle track identification using scintillating fibres read out with imaging
sensors. We propose a variational autoencoder (VAE) to efficiently filter and
identify frames containing signal from the substantial data generated by SPAD
array sensors. Our VAE model, trained on purely background frames, demonstrated
a high capability to distinguish frames containing particle tracks from
background noise. The performance of the VAE-based anomaly detection was
validated with experimental data, demonstrating the method's ability to
efficiently identify relevant events with rapid processing time, suggesting a
solid prospect for deployment as a fast inference tool on hardware for
real-time anomaly detection. This work highlights the potential of combining
advanced sensor technology with machine learning techniques to enhance particle
detection and tracking.

</details>


### [499] [MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning](https://arxiv.org/pdf/2410.11226)
*Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K. Gilson, Rose Yu*

Main category: cs.LG

TL;DR: MF-LAL is a generative modeling framework that integrates multi-fidelity oracles for drug discovery, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current generative models rely on docking scores, which often fail to predict real-world activity. More accurate methods are too expensive.

Method: MF-LAL combines generative and multi-fidelity surrogate models into one framework, using active learning to reduce costs.

Result: MF-LAL achieves ~50% improvement in mean binding free energy scores compared to other approaches.

Conclusion: MF-LAL offers a practical solution for generating active compounds with better accuracy and efficiency.

Abstract: Current generative models for drug discovery primarily use molecular docking
as an oracle to guide the generation of active compounds. However, such models
are often not useful in practice because even compounds with high docking
scores do not consistently show real-world experimental activity. More accurate
methods for activity prediction exist, such as molecular dynamics based binding
free energy calculations, but they are too computationally expensive to use in
a generative model. To address this challenge, we propose Multi-Fidelity Latent
space Active Learning (MF-LAL), a generative modeling framework that integrates
a set of oracles with varying cost-accuracy tradeoffs. We train a surrogate
model for each oracle and use these surrogates to guide generation of compounds
with high predicted activity. Unlike previous approaches that separately learn
the surrogate model and generative model, MF-LAL combines the generative and
multi-fidelity surrogate models into a single framework, allowing for more
accurate activity prediction and higher quality samples. We train MF-LAL with a
novel active learning algorithm to further reduce computational cost. Our
experiments on two disease-relevant proteins show that MF-LAL produces
compounds with significantly better binding free energy scores than other
single and multi-fidelity approaches (~50% improvement in mean binding free
energy score).

</details>


### [500] [Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination](https://arxiv.org/pdf/2411.01642)
*Md Abrar Jahin, Md. Akmol Masud, M. F. Mridha, Nilanjan Dey, Zeyar Aung*

Main category: cs.LG

TL;DR: QRGCL, a quantum rationale-aware graph contrastive learning framework, improves quark-gluon jet tagging with fewer labeled samples and parameters, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in particle jet tagging, such as complex data structures and limited labeled samples, while improving computational efficiency and feature extraction.

Method: Integrates a quantum rationale generator (QRG) into a contrastive learning framework (QRGCL) to enhance feature extraction and reduce labeled data dependency.

Result: Achieves 77.53% AUC on quark-gluon jet dataset with only 45 QRG parameters, outperforming classical, quantum, and hybrid benchmarks.

Conclusion: QRGCL shows promise for advancing jet tagging and similar tasks in high-energy physics by balancing efficiency and performance.

Abstract: In high-energy physics, particle jet tagging plays a pivotal role in
distinguishing quark from gluon jets using data from collider experiments.
While graph-based deep learning methods have advanced this task beyond
traditional feature-engineered approaches, the complex data structure and
limited labeled samples present ongoing challenges. However, existing
contrastive learning (CL) frameworks struggle to leverage rationale-aware
augmentations effectively, often lacking supervision signals that guide the
extraction of salient features and facing computational efficiency issues such
as high parameter counts. In this study, we demonstrate that integrating a
quantum rationale generator (QRG) within our proposed Quantum Rationale-aware
Graph Contrastive Learning (QRGCL) framework significantly enhances jet
discrimination performance, reducing reliance on labeled data and capturing
discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL
achieves an AUC score of $77.53\%$ while maintaining a compact architecture of
only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and
GNN benchmarks. These results highlight QRGCL's potential to advance jet
tagging and other complex classification tasks in high-energy physics, where
computational efficiency and feature extraction limitations persist.

</details>


### [501] [Infinite Width Limits of Self Supervised Neural Networks](https://arxiv.org/pdf/2411.11176)
*Maximilian Fleissner, Gautham Govind Anil, Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: The paper bridges the gap between the Neural Tangent Kernel (NTK) and self-supervised learning, proving the NTK of Barlow Twins becomes constant for wide networks and providing generalization bounds.


<details>
  <summary>Details</summary>
Motivation: To address the open question of whether the NTK connection to self-supervised learning is mathematically sound, particularly for wide neural networks trained under the Barlow Twins loss.

Method: Focuses on two-layer neural networks trained with Barlow Twins loss, proving the NTK becomes constant as width approaches infinity, and derives generalization error bounds.

Result: The NTK of Barlow Twins becomes constant for infinite-width networks, justifying the use of kernel theory in self-supervised learning. Generalization bounds are also derived.

Conclusion: The work provides a mathematical foundation for applying kernel theory to self-supervised learning, with implications for understanding wide neural networks.

Abstract: The NTK is a widely used tool in the theoretical analysis of deep learning,
allowing us to look at supervised deep neural networks through the lenses of
kernel regression. Recently, several works have investigated kernel models for
self-supervised learning, hypothesizing that these also shed light on the
behavior of wide neural networks by virtue of the NTK. However, it remains an
open question to what extent this connection is mathematically sound -- it is a
commonly encountered misbelief that the kernel behavior of wide neural networks
emerges irrespective of the loss function it is trained on. In this paper, we
bridge the gap between the NTK and self-supervised learning, focusing on
two-layer neural networks trained under the Barlow Twins loss. We prove that
the NTK of Barlow Twins indeed becomes constant as the width of the network
approaches infinity. Our analysis technique is a bit different from previous
works on the NTK and may be of independent interest. Overall, our work provides
a first justification for the use of classic kernel theory to understand
self-supervised learning of wide neural networks. Building on this result, we
derive generalization error bounds for kernelized Barlow Twins and connect them
to neural networks of finite width.

</details>


### [502] [Conformal Transformations for Symmetric Power Transformers](https://arxiv.org/pdf/2503.03269)
*Saurabh Kumar, Jacob Buckman, Carles Gelada, Sean Zhang*

Main category: cs.LG

TL;DR: The conformal-sympow transformer improves upon sympow transformers by addressing their finite capacity issue, achieving robust performance across scaled contexts.


<details>
  <summary>Details</summary>
Motivation: Sympow transformers, while efficient, suffer from limited recurrent state capacity, degrading performance with longer contexts.

Method: Proposes conformal-sympow transformer using data-dependent gating and rotary embeddings to dynamically manage capacity.

Result: Preliminary experiments show conformal-sympow outperforms sympow transformers on LongCrawl64, especially in scaled contexts.

Conclusion: Conformal-sympow effectively addresses sympow's limitations, offering robust performance for longer sequences.

Abstract: Transformers with linear attention offer significant computational advantages
over softmax-based transformers but often suffer from degraded performance. The
symmetric power (sympow) transformer, a particular type of linear transformer,
addresses some of this performance gap by leveraging symmetric tensor
embeddings, achieving comparable performance to softmax transformers. However,
the finite capacity of the recurrent state in sympow transformers limits their
ability to retain information, leading to performance degradation when scaling
the training or evaluation context length. To address this issue, we propose
the conformal-sympow transformer, which dynamically frees up capacity using
data-dependent multiplicative gating and adaptively stores information using
data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64
dataset demonstrate that conformal-sympow overcomes the limitations of sympow
transformers, achieving robust performance across scaled training and
evaluation contexts.

</details>


### [503] [Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors](https://arxiv.org/pdf/2411.11764)
*Shovito Barua Soumma, S M Raihanul Alam, Rudmila Rahman, Umme Niraj Mahi, Abdullah Mamun, Sayyed Mostafa Mostafavi, Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: FOGSense is a novel FOG detection system for Parkinson's disease, using a single sensor and federated learning to improve accuracy, reduce failure points, and preserve privacy in real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Current FOG detection methods struggle with variability, privacy, and real-world applicability, necessitating a more robust and deployable solution.

Method: FOGSense employs GAF transformations and federated deep learning to analyze gait patterns with a single sensor, tested in free-living environments.

Result: FOGSense outperforms existing methods with 10.4% higher accuracy, 22.2% better F1-score, and 74.53% lower false positive rate.

Conclusion: FOGSense is a promising, privacy-preserving solution for real-time, long-term FOG monitoring in uncontrolled settings.

Abstract: Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease that
impairs mobility and safety by increasing the risk of falls. An effective FOG
detection system must be accurate, real-time, and deployable in free-living
environments to enable timely interventions. However, existing detection
methods face challenges due to (1) intra- and inter-patient variability, (2)
subject-specific training, (3) using multiple sensors in FOG dominant locations
(e.g., ankles) leading to high failure points, (4) centralized, non-adaptive
learning frameworks that sacrifice patient privacy and prevent collaborative
model refinement across populations and disease progression, and (5) most
systems are tested in controlled settings, limiting their real-world
applicability for continuous in-home monitoring. Addressing these gaps, we
present FOGSense, a real-world deployable FOG detection system designed for
uncontrolled, free-living conditions using only a single sensor. FOGSense uses
Gramian Angular Field (GAF) transformations and privacy-preserving federated
deep learning to capture temporal and spatial gait patterns missed by
traditional methods with a low false positive rate. We evaluated our system
using a public Parkinson's dataset collected in a free-living environment.
FOGSense improves accuracy by 10.4% over a single-axis accelerometer, reduces
failure points compared to multi-sensor systems, and demonstrates robustness to
missing values. The federated architecture allows personalized model adaptation
and efficient smartphone synchronization during off-peak hours, making it
effective for long-term monitoring as symptoms evolve. Overall, FOGSense
achieved a 22.2% improvement in F1-score and a 74.53% reduction in false
positive rate compared to state-of-the-art methods, along with enhanced
sensitivity for FOG episode detection.

</details>


### [504] [M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation](https://arxiv.org/pdf/2412.04134)
*Tao Zhang, Zhenhai Liu, Feipeng Qi, Yongjun Jiao, Tailin Wu*

Main category: cs.LG

TL;DR: M2PDE uses diffusion models to improve multiphysics and multi-component simulations, outperforming existing methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in integrating specialized solvers and handling complex structures in multiphysics and multi-component simulations.

Method: Proposes M2PDE, a diffusion-based model that learns energy functions for conditional probabilities of physical processes/components and samples joint distributions for solutions.

Result: M2PDE achieves higher accuracy in multiphysics tasks and scales effectively in multi-component simulations, outperforming existing approaches.

Conclusion: M2PDE offers a robust solution for complex simulations, demonstrating superior performance and scalability.

Abstract: Multiphysics simulation, which models the interactions between multiple
physical processes, and multi-component simulation of complex structures are
critical in fields like nuclear and aerospace engineering. Previous studies use
numerical solvers or ML-based surrogate models for these simulations. However,
multiphysics simulations typically require integrating multiple specialized
solvers-each for a specific physical process-into a coupled program, which
introduces significant development challenges. Furthermore, existing numerical
algorithms struggle with highly complex large-scale structures in
multi-component simulations. Here we propose compositional Multiphysics and
Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these
challenges. During diffusion-based training, M2PDE learns energy functions
modeling the conditional probability of one physical process/component
conditioned on other processes/components. In inference, M2PDE generates
coupled multiphysics and multi-component solutions by sampling from the joint
probability distribution. We evaluate M2PDE on two multiphysics
tasks-reaction-diffusion and nuclear thermal coupling-where it achieves more
accurate predictions than surrogate models in challenging scenarios. We then
apply it to a multi-component prismatic fuel element problem, demonstrating
that M2PDE scales from single-component training to a 64-component structure
and outperforms existing domain-decomposition and graph-based approaches. The
code is available at https://github.com/AI4Science-WestlakeU/M2PDE.

</details>


### [505] [Truthful mechanisms for linear bandit games with private contexts](https://arxiv.org/pdf/2501.03865)
*Yiting Hu, Lingjie Duan*

Main category: cs.LG

TL;DR: The paper addresses the issue of private context misreporting in contextual bandit problems, proposing a truthful mechanism with low regret.


<details>
  <summary>Details</summary>
Motivation: In healthcare and recommendation systems, agents may misreport private contexts (e.g., symptoms) to manipulate outcomes, undermining system effectiveness.

Method: The authors propose a linear program-based mechanism to ensure truthful reporting while minimizing deviation from Thompson sampling.

Result: The mechanism achieves $O(\ln T)$ frequentist regret, outperforming traditional methods like UCB and ETC.

Conclusion: The proposed solution effectively balances truthfulness and performance, validated by numerical experiments.

Abstract: The contextual bandit problem, where agents arrive sequentially with personal
contexts and the system adapts its arm allocation decisions accordingly, has
recently garnered increasing attention for enabling more personalized outcomes.
However, in many healthcare and recommendation applications, agents have
private profiles and may misreport their contexts to gain from the system. For
example, in adaptive clinical trials, where hospitals sequentially recruit
volunteers to test multiple new treatments and adjust plans based on
volunteers' reported profiles such as symptoms and interim data, participants
may misreport severe side effects like allergy and nausea to avoid perceived
suboptimal treatments. We are the first to study this issue of private context
misreporting in a stochastic contextual bandit game between the system and
non-repeated agents. We show that traditional low-regret algorithms, such as
UCB family algorithms and Thompson sampling, fail to ensure truthful reporting
and can result in linear regret in the worst case, while traditional truthful
algorithms like explore-then-commit (ETC) and $\epsilon$-greedy algorithm incur
sublinear but high regret. We propose a mechanism that uses a linear program to
ensure truthfulness while minimizing deviation from Thompson sampling, yielding
an $O(\ln T)$ frequentist regret. Our numerical experiments further demonstrate
strong performance in multiple contexts and across other distribution families.

</details>


### [506] [Rethinking the Bias of Foundation Model under Long-tailed Distribution](https://arxiv.org/pdf/2501.15955)
*Jiahao Chen, Bin Qin, Jiangmeng Li, Hao Chen, Bing Su*

Main category: cs.LG

TL;DR: The paper investigates how imbalances in pre-training data affect long-tailed downstream tasks, identifying parameter and data imbalances. It proposes a causal learning-based backdoor adjustment method to address these issues, achieving a 1.67% performance boost.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the biases introduced by imbalanced pre-training data in foundation models, particularly for long-tailed learning tasks.

Method: The study identifies parameter and data imbalances, then proposes a novel backdoor adjustment method based on causal learning to address these imbalances.

Result: The method improves performance by an average of 1.67% on each dataset, effectively tackling parameter imbalance, which existing techniques fail to address.

Conclusion: The proposed causal learning approach successfully mitigates biases in long-tailed learning, outperforming current re-balancing strategies.

Abstract: Long-tailed learning has garnered increasing attention due to its practical
significance. Among the various approaches, the fine-tuning paradigm has gained
considerable interest with the advent of foundation models. However, most
existing methods primarily focus on leveraging knowledge from these models,
overlooking the inherent biases introduced by the imbalanced training data they
rely on. In this paper, we examine how such imbalances from pre-training affect
long-tailed downstream tasks. Specifically, we find the imbalance biases
inherited in foundation models on downstream task as parameter imbalance and
data imbalance. During fine-tuning, we observe that parameter imbalance plays a
more critical role, while data imbalance can be mitigated using existing
re-balancing strategies. Moreover, we find that parameter imbalance cannot be
effectively addressed by current re-balancing techniques, such as adjusting the
logits, during training, unlike data imbalance. To tackle both imbalances
simultaneously, we build our method on causal learning and view the incomplete
semantic factor as the confounder, which brings spurious correlations between
input samples and labels. To resolve the negative effects of this, we propose a
novel backdoor adjustment method that learns the true causal effect between
input samples and labels, rather than merely fitting the correlations in the
data. Notably, we achieve an average performance increase of about $1.67\%$ on
each dataset.

</details>


### [507] [PAC Learning is just Bipartite Matching (Sort of)](https://arxiv.org/pdf/2502.00607)
*Shaddin Dughmi*

Main category: cs.LG

TL;DR: The paper links PAC learning to bipartite matching via transductive learning and one-inclusion graphs, also connecting to recreational hat puzzles.


<details>
  <summary>Details</summary>
Motivation: To show the relationship between PAC learning and bipartite matching, and to explore transductive learning's role in learning theory.

Method: Uses transductive learning models and one-inclusion graphs, generalizing recreational hat puzzles.

Result: Demonstrates a connection between PAC learning and bipartite matching, highlighting transductive learning's relevance.

Conclusion: The paper bridges PAC learning and bipartite matching, offering insights into transductive learning's applications in theory.

Abstract: The main goal of this article is to convince you, the reader, that supervised
learning in the Probably Approximately Correct (PAC) model is closely related
to -- of all things -- bipartite matching! En-route from PAC learning to
bipartite matching, I will overview a particular transductive model of
learning, and associated one-inclusion graphs, which can be viewed as a
generalization of some of the hat puzzles that are popular in recreational
mathematics. Whereas this transductive model is far from new, it has recently
seen a resurgence of interest as a tool for tackling deep questions in learning
theory. A secondary purpose of this article could be as a (biased) tutorial on
the connections between the PAC and transductive models of learning.

</details>


### [508] [LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records](https://arxiv.org/pdf/2502.14259)
*Sujeong Im, Jungwoo Oh, Edward Choi*

Main category: cs.LG

TL;DR: LabTOP is a unified model using language modeling on EHR data to predict continuous lab test outcomes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Frequent lab testing is burdensome and results may be delayed, necessitating a predictive solution.

Method: LabTOP leverages language modeling on EHR data for continuous numerical predictions across diverse lab tests.

Result: Outperforms traditional ML and large language models on three EHR datasets, validated by ablation studies.

Conclusion: LabTOP is accurate and generalizable, useful for clinical decision support and early detection.

Abstract: Lab tests are fundamental for diagnosing diseases and monitoring patient
conditions. However, frequent testing can be burdensome for patients, and test
results may not always be immediately available. To address these challenges,
we propose LabTOP, a unified model that predicts lab test outcomes by
leveraging a language modeling approach on EHR data. Unlike conventional
methods that estimate only a subset of lab tests or classify discrete value
ranges, LabTOP performs continuous numerical predictions for a diverse range of
lab items. We evaluate LabTOP on three publicly available EHR datasets and
demonstrate that it outperforms existing methods, including traditional machine
learning models and state-of-the-art large language models. We also conduct
extensive ablation studies to confirm the effectiveness of our design choices.
We believe that LabTOP will serve as an accurate and generalizable framework
for lab test outcome prediction, with potential applications in clinical
decision support and early detection of critical conditions.

</details>


### [509] [VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning](https://arxiv.org/pdf/2504.08837)
*Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen*

Main category: cs.LG

TL;DR: Slow-thinking models like GPT-o1 outperform fast-thinking models in math/science but lag in multimodal reasoning. This paper enhances slow-thinking via RL, introducing Selective Sample Replay and Forced Rethinking, achieving SOTA on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve slow-thinking capabilities in vision-language models, addressing their underperformance in multimodal reasoning compared to fast-thinking models.

Method: Uses reinforcement learning (GRPO algorithm with Selective Sample Replay) and introduces Forced Rethinking to enforce self-reflection.

Result: VL-Rethinker achieves SOTA scores on MathVista (80.4%) and MathVerse (63.5%), and performs well on other benchmarks.

Conclusion: The combined techniques effectively enhance slow-thinking, narrowing the gap with top models like OpenAI-o1.

Abstract: Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated
great potential in solving challenging problems through explicit reflection.
They significantly outperform the best fast-thinking models, such as GPT-4o, on
various math and science benchmarks. However, their multimodal reasoning
capabilities remain on par with fast-thinking models. For instance, GPT-o1's
performance on benchmarks like MathVista, MathVerse, and MathVision is similar
to fast-thinking models. In this paper, we aim to enhance the slow-thinking
capabilities of vision-language models using reinforcement learning (without
relying on distillation) to advance the state of the art. First, we adapt the
GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to
address the vanishing advantages problem. While this approach yields strong
performance, the resulting RL-trained models exhibit limited self-reflection or
self-verification. To further encourage slow-thinking, we introduce Forced
Rethinking, which appends a rethinking trigger token to the end of rollouts in
RL training, explicitly enforcing a self-reflection reasoning step. By
combining these two techniques, our model, VL-Rethinker, advances
state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5%
respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary
benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the
gap with OpenAI-o1. Our empirical results show the effectiveness of our
approaches.

</details>


### [510] [Frankenstein Optimizer: Harnessing the Potential by Revisiting Optimization Tricks](https://arxiv.org/pdf/2503.02147)
*Chia-Wei Hsu, Nien-Ti Tsou, Yu-Cheng Chen, Yang Jeong Park, Ju Li*

Main category: cs.LG

TL;DR: The paper introduces the Frankenstein optimizer, which dynamically adjusts momentum coefficients to combine the strengths of adaptive algorithms and SGD, improving convergence and generalization.


<details>
  <summary>Details</summary>
Motivation: Adaptive algorithms accelerate training but often fail to find flat minima, leading to suboptimal generalization compared to SGD. This work aims to merge the advantages of both approaches.

Method: The Frankenstein optimizer dynamically adjusts first- and second-momentum coefficients based on the optimizer's state to maintain consistent learning dynamics and respond to gradient changes.

Result: Experiments in computer vision, NLP, few-shot learning, and scientific simulations show Frankenstein outperforms existing adaptive algorithms and SGD in convergence speed and generalization.

Conclusion: The Frankenstein optimizer successfully combines adaptive and SGD benefits, enhancing performance and deepening understanding of adaptive algorithms through kernel alignment and loss landscape analysis.

Abstract: Gradient-based optimization drives the unprecedented performance of modern
deep neural network models across diverse applications. Adaptive algorithms
have accelerated neural network training due to their rapid convergence rates;
however, they struggle to find ``flat minima" reliably, resulting in suboptimal
generalization compared to stochastic gradient descent (SGD). By revisiting
various adaptive algorithms' mechanisms, we propose the Frankenstein optimizer,
which combines their advantages. The proposed Frankenstein dynamically adjusts
first- and second-momentum coefficients according to the optimizer's current
state to directly maintain consistent learning dynamics and immediately reflect
sudden gradient changes. Extensive experiments across several research domains
such as computer vision, natural language processing, few-shot learning, and
scientific simulations show that Frankenstein surpasses existing adaptive
algorithms and SGD empirically regarding convergence speed and generalization
performance. Furthermore, this research deepens our understanding of adaptive
algorithms through centered kernel alignment analysis and loss landscape
visualization during the learning process. Code is available at
https://github.com/acctouhou/Frankenstein_optimizer

</details>


### [511] [Entropic Time Schedulers for Generative Diffusion Models](https://arxiv.org/pdf/2504.13612)
*Dejan Stancevic, Luca Ambrogioni*

Main category: cs.LG

TL;DR: The paper introduces an entropy-based time scheduler for diffusion models, improving inference performance without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: Current noise scheduling in generative diffusion models lacks efficiency in information contribution per sampling point.

Method: Proposes an entropy-based time scheduler and a rescaled version, derived from training loss, to optimize sampling points.

Result: Experiments show improved image quality (FID, FD-DINO scores) in models like EDM2, especially with fewer function evaluations.

Conclusion: Entropic time reparameterization enhances model performance efficiently, particularly in low-compute scenarios.

Abstract: The practical performance of generative diffusion models depends on the
appropriate choice of the noise scheduling function, which can also be
equivalently expressed as a time reparameterization. In this paper, we present
a time scheduler that selects sampling points based on entropy rather than
uniform time spacing, ensuring that each point contributes an equal amount of
information to the final generation. We prove that this time reparameterization
does not depend on the initial choice of time. Furthermore, we provide a
tractable exact formula to estimate this \emph{entropic time} for a trained
model using the training loss without substantial overhead. Alongside the
entropic time, inspired by the optimality results, we introduce a rescaled
entropic time. In our experiments with mixtures of Gaussian distributions and
ImageNet, we show that using the (rescaled) entropic times greatly improves the
inference performance of trained models. In particular, we found that the image
quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can
be substantially increased by the rescaled entropic time reparameterization
without increasing the number of function evaluations, with greater
improvements in the few NFEs regime.

</details>


### [512] [Artificial Intelligence in Reactor Physics: Current Status and Future Prospects](https://arxiv.org/pdf/2503.02440)
*Ruizhi Zhang, Shengfeng Zhu, Kan Wang, Ding She, Jean-Philippe Argaud, Bertrand Bouriquet, Qing Li, Helin Gong*

Main category: cs.LG

TL;DR: The paper reviews AI and ML applications in reactor physics, covering scenarios, challenges, and future directions, with a focus on efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: To explore how AI, particularly ML, can enhance reactor physics by improving deterministic methods and addressing uncertainties.

Method: A comprehensive review of ML methods applied to reactor physics, including steady-state, transient, and combustion problems.

Result: ML enhances efficiency and corrects uncertainties in reactor physics models, but generalization remains a challenge.

Conclusion: Future research should address theoretical challenges and improve industrial applications like surrogate models and digital twins.

Abstract: Reactor physics is the study of neutron properties, focusing on using models
to examine the interactions between neutrons and materials in nuclear reactors.
Artificial intelligence (AI) has made significant contributions to reactor
physics, e.g., in operational simulations, safety design, real-time monitoring,
core management and maintenance. This paper presents a comprehensive review of
AI approaches in reactor physics, especially considering the category of
Machine Learning (ML), with the aim of describing the application scenarios,
frontier topics, unsolved challenges and future research directions. From
equation solving and state parameter prediction to nuclear industry
applications, this paper provides a step-by-step overview of ML methods applied
to steady-state, transient and combustion problems. Most literature works
achieve industry-demanded models by enhancing the efficiency of deterministic
methods or correcting uncertainty methods, which leads to successful
applications. However, research on ML methods in reactor physics is somewhat
fragmented, and the ability to generalize models needs to be strengthened.
Progress is still possible, especially in addressing theoretical challenges and
enhancing industrial applications such as building surrogate models and digital
twins.

</details>


### [513] [Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation](https://arxiv.org/pdf/2503.06337)
*Mohit Pandey, Gopeshh Subbaraj, Artem Cherkasov, Martin Ester, Emmanuel Bengio*

Main category: cs.LG

TL;DR: A-GFNs use atoms as building blocks for diverse molecular generation, outperforming baselines in drug design tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of predefined fragments in GFlowNets by exploring chemical space more comprehensively with atoms.

Method: Unsupervised pre-training with drug-like datasets and goal-conditioned finetuning for specific properties.

Result: A-GFNs effectively generate molecules with desirable pharmacological properties, outperforming baselines.

Conclusion: A-GFNs offer a robust approach for diverse and high-quality molecular generation in drug design.

Abstract: Generative Flow Networks (GFlowNets) have recently emerged as a suitable
framework for generating diverse and high-quality molecular structures by
learning from rewards treated as unnormalized distributions. Previous works in
this framework often restrict exploration by using predefined molecular
fragments as building blocks, limiting the chemical space that can be accessed.
In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative
model leveraging individual atoms as building blocks to explore drug-like
chemical space more comprehensively. We propose an unsupervised pre-training
approach using drug-like molecule datasets, which teaches A-GFNs about
inexpensive yet informative molecular descriptors such as drug-likeliness,
topological polar surface area, and synthetic accessibility scores. These
properties serve as proxy rewards, guiding A-GFNs towards regions of chemical
space that exhibit desirable pharmacological properties. We further implement a
goal-conditioned finetuning process, which adapts A-GFNs to optimize for
specific target properties. In this work, we pretrain A-GFN on a subset of ZINC
dataset, and by employing robust evaluation metrics we show the effectiveness
of our approach when compared to other relevant baseline methods for a wide
range of drug design tasks.

</details>


### [514] [An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization](https://arxiv.org/pdf/2503.08555)
*Jannis O. Luebsen, Annika Eichler*

Main category: cs.LG

TL;DR: The paper proposes a method to integrate additional information sources into Bayesian optimization while maintaining safety constraints, using an unknown correlation matrix and adjusted error bounds.


<details>
  <summary>Details</summary>
Motivation: To enhance Bayesian optimization by incorporating extra information sources without violating safety constraints.

Method: Model interdependencies with an unknown correlation matrix and adjust uniform error bounds using confidence intervals from data.

Result: Experiments show improved sample efficiency, validating the method for optimizing costly functions.

Conclusion: The approach effectively integrates additional information while ensuring safety, making it suitable for expensive-to-evaluate functions.

Abstract: This paper addresses the integration of additional information sources into a
Bayesian optimization framework while ensuring that safety constraints are
satisfied. The interdependencies between these information sources are modeled
using an unknown correlation matrix. We explore how uniform error bounds must
be adjusted to maintain constraint satisfaction throughout the optimization
process, considering both Bayesian and frequentist statistical perspectives.
This is achieved by appropriately scaling the error bounds based on a
confidence interval that can be estimated from the data. Furthermore, the
efficacy of the proposed approach is demonstrated through experiments on two
benchmark functions and a controller parameter optimization problem. Our
results highlight a significant improvement in sample efficiency, demonstrating
the methods suitability for optimizing expensive-to-evaluate functions.

</details>


### [515] [UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows](https://arxiv.org/pdf/2503.23236)
*Ismaël Zighed, Nicolas Thome, Patrick Gallinari, Taraneh Sayadi*

Main category: cs.LG

TL;DR: A nonlinear reduction strategy for transient flows using a VAE with uncertainty quantification and attention mechanisms for improved robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust, generalizable ROMs in fluid mechanics with confidence measures in predictions.

Method: Uses a VAE with variational inference for confidence and a latent space transformer with attention mechanisms for dynamics prediction.

Result: Enhanced generalization and robustness in predictions, with cost-effective parameter space sampling.

Conclusion: The strategy improves ROM applicability by combining prediction confidence and attention-based dynamics learning.

Abstract: Reduced order models (ROMs) play a critical role in fluid mechanics by
providing low-cost predictions, making them an attractive tool for engineering
applications. However, for ROMs to be widely applicable, they must not only
generalise well across different regimes, but also provide a measure of
confidence in their predictions. While recent data-driven approaches have begun
to address nonlinear reduction techniques to improve predictions in transient
environments, challenges remain in terms of robustness and parametrisation. In
this work, we present a nonlinear reduction strategy specifically designed for
transient flows that incorporates parametrisation and uncertainty
quantification. Our reduction strategy features a variational auto-encoder
(VAE) that uses variational inference for confidence measurement. We use a
latent space transformer that incorporates recent advances in attention
mechanisms to predict dynamical systems. Attention's versatility in learning
sequences and capturing their dependence on external parameters enhances
generalisation across a wide range of dynamics. Prediction, coupled with
confidence, enables more informed decision making and addresses the need for
more robust models. In addition, this confidence is used to cost-effectively
sample the parameter space, improving model performance a priori across the
entire parameter space without requiring evaluation data for the entire domain.

</details>


### [516] [Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study](https://arxiv.org/pdf/2504.18267)
*Prajwal Chauhan, Salah Eddine Choutri, Mohamed Ghattassi, Nader Masmoudi, Saif Eddin Jabari*

Main category: cs.LG

TL;DR: Neural operators struggle with complex Hughes model scenarios, especially with discontinuities and dynamic boundaries, due to unintended smoothing effects.


<details>
  <summary>Details</summary>
Motivation: To evaluate neural operators' performance in learning solutions for the Hughes model, a challenging hyperbolic system with shocks and discontinuities.

Method: Assessed three neural operators (Fourier, Wavelet, Multiwavelet) under discontinuous/Gaussian initial conditions and diverse boundary conditions, using various numerical schemes.

Result: Neural operators perform well in simple scenarios but fail in complex ones, introducing smoothing effects and losing key physical features like shocks.

Conclusion: Current neural operator architectures may lack the ability to handle discontinuities, raising concerns for applications like traffic modeling where shock preservation is critical.

Abstract: This paper investigates the limitations of neural operators in learning
solutions for a Hughes model, a first-order hyperbolic conservation law system
for crowd dynamics. The model couples a Fokker-Planck equation representing
pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes
model belongs to the class of nonlinear hyperbolic systems that often exhibit
complex solution structures, including shocks and discontinuities. In this
study, we assess the performance of three state-of-the-art neural operators
(Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural
Operator) in various challenging scenarios. Specifically, we consider (1)
discontinuous and Gaussian initial conditions and (2) diverse boundary
conditions, while also examining the impact of different numerical schemes.
  Our results show that these neural operators perform well in easy scenarios
with fewer discontinuities in the initial condition, yet they struggle in
complex scenarios with multiple initial discontinuities and dynamic boundary
conditions, even when trained specifically on such complex samples. The
predicted solutions often appear smoother, resulting in a reduction in total
variation and a loss of important physical features. This smoothing behavior is
similar to issues discussed by Daganzo (1995), where models that introduce
artificial diffusion were shown to miss essential features such as shock waves
in hyperbolic systems. These results suggest that current neural operator
architectures may introduce unintended regularization effects that limit their
ability to capture transport dynamics governed by discontinuities. They also
raise concerns about generalizing these methods to traffic applications where
shock preservation is essential.

</details>


### [517] [GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments](https://arxiv.org/pdf/2504.00711)
*Enjun Du, Xunkai Li, Tian Jin, Zhihan Zhang, Rong-Hua Li, Guoren Wang*

Main category: cs.LG

TL;DR: GraphMaster is a multi-agent framework for graph data synthesis, addressing limitations of traditional methods and LLMs by ensuring semantic coherence and structural integrity.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale graph corpora and the limitations of existing synthesis techniques and LLMs in handling graph data motivate the need for GraphMaster.

Method: GraphMaster uses four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) for iterative refinement of graph synthesis.

Result: GraphMaster outperforms traditional methods on new data-limited benchmarks, validated by a novel interpretability framework.

Conclusion: GraphMaster advances Graph Foundation Models in data-scarce environments by improving synthesis quality and interpretability.

Abstract: The era of foundation models has revolutionized AI research, yet Graph
Foundation Models (GFMs) remain constrained by the scarcity of large-scale
graph corpora. Traditional graph data synthesis techniques primarily focus on
simplistic structural operations, lacking the capacity to generate semantically
rich nodes with meaningful textual attributes: a critical limitation for
real-world applications. While large language models (LLMs) demonstrate
exceptional text generation capabilities, their direct application to graph
synthesis is impeded by context window limitations, hallucination phenomena,
and structural consistency challenges. To address these issues, we introduce
GraphMaster, the first multi-agent framework specifically designed for graph
data synthesis in data-limited environments. GraphMaster orchestrates four
specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that
collaboratively optimize the synthesis process through iterative refinement,
ensuring both semantic coherence and structural integrity. To rigorously
evaluate our approach, we create new data-limited "Sub" variants of six
standard graph benchmarks, specifically designed to test synthesis capabilities
under realistic constraints. Additionally, we develop a novel interpretability
assessment framework that combines human evaluation with a principled
Grassmannian manifold-based analysis, providing both qualitative and
quantitative measures of semantic coherence. Experimental results demonstrate
that GraphMaster significantly outperforms traditional synthesis methods across
multiple datasets, establishing a strong foundation for advancing GFMs in
data-scarce environments.

</details>


### [518] [TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network](https://arxiv.org/pdf/2504.06559)
*Ali Eslamian, Alireza Afzal Aghaei, Qiang Cheng*

Main category: cs.LG

TL;DR: TabKAN introduces Kolmogorov-Arnold Networks (KANs) for tabular data, improving interpretability and efficiency, outperforming traditional methods in supervised and transfer learning.


<details>
  <summary>Details</summary>
Motivation: Address challenges in tabular data analysis (heterogeneous features, missing values) and bridge the gap between traditional ML and deep learning.

Method: Proposes modular KAN-based architectures, a transfer learning framework, and model-specific interpretability for tabular data.

Result: TabKAN outperforms classical and Transformer-based models in supervised and transfer learning tasks.

Conclusion: KAN-based architectures excel in knowledge transfer and narrow the gap between traditional ML and deep learning for structured data.

Abstract: Tabular data analysis presents unique challenges due to its heterogeneous
feature types, missing values, and complex interactions. While traditional
machine learning methods, such as gradient boosting, often outperform deep
learning approaches, recent advancements in neural architectures offer
promising alternatives. This paper introduces TabKAN, a novel framework that
advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike
conventional deep learning models, KANs leverge learnable activation functions
on edges, which improve both interpretability and training efficiency. Our
contributions include: (1) the introduction of modular KAN-based architectures
for tabular data analysis, (2) the development of a transfer learning framework
for KAN models that supports knowledge transfer between domains, (3) the
development of model-specific interpretability for tabular data learning, which
reduces dependence on post hoc and model-agnostic analysis, and (4)
comprehensive evaluation of vanilla supervised learning across binary and
multi-class classification tasks. Through extensive benchmarking on diverse
public datasets, TabKAN demonstrates superior performance in supervised
learning while significantly outperforming classical and Transformer-based
models in transfer learning scenarios. Our findings highlight the advantage of
KAN-based architectures in transferring knowledge across domains and narrowing
the gap between traditional machine learning and deep learning for structured
data.

</details>


### [519] [Inference-friendly Graph Compression for Graph Neural Networks](https://arxiv.org/pdf/2504.13034)
*Yangxin Fan, Haolai Che, Yinghui Wu*

Main category: cs.LG

TL;DR: The paper proposes IFGC, a graph compression method to accelerate GNN inference by preserving results with minimal decompression cost.


<details>
  <summary>Details</summary>
Motivation: GNN inference is costly for large graphs, limiting applications. IFGC aims to address this by compressing graphs while maintaining inference accuracy.

Method: IFGC uses inference equivalence relations and introduces three compression schemes: SPGC, (α, r)-compression, and anchored compression, each with tailored algorithms.

Result: Experiments on large-scale graphs confirm IFGC's effectiveness and efficiency in accelerating GNN inference.

Conclusion: IFGC provides a practical solution for efficient GNN inference on large graphs through targeted compression schemes.

Abstract: Graph Neural Networks (GNNs) have demonstrated promising performance in graph
analysis. Nevertheless, the inference process of GNNs remains costly, hindering
their applications for large graphs. This paper proposes inference-friendly
graph compression (IFGC), a graph compression scheme to accelerate GNNs
inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed
graph $G_c$, to best preserve the inference results of $M$ over $G$, such that
the result can be directly inferred by accessing $G_c$ with no or little
decompression cost. (1) We characterize IFGC with a class of inference
equivalence relation. The relation captures the node pairs in $G$ that are not
distinguishable for GNN inference. (2) We introduce three practical
specifications of IFGC for representative GNNs: structural preserving
compression (SPGC), which computes $G_c$ that can be directly processed by GNN
inference without decompression; ($\alpha$, $r$)-compression, that allows for a
configurable trade-off between compression ratio and inference quality, and
anchored compression that preserves inference results for specific nodes of
interest. For each scheme, we introduce compression and inference algorithms
with guarantees of efficiency and quality of the inferred results. We conduct
extensive experiments on diverse sets of large-scale graphs, which verifies the
effectiveness and efficiency of our graph compression approaches.

</details>


### [520] [Observability conditions for neural state-space models with eigenvalues and their roots of unity](https://arxiv.org/pdf/2504.15758)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: The paper explores observability in neural state-space models and Mamba architecture using ODEs and control theory, proposing efficient methods to enforce observability with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address observability in neural state-space models, particularly in learning contexts with high-dimensional hidden states, leveraging control theory principles.

Method: The authors develop strategies to enforce observability, emphasizing eigenvalues, roots of unity, and permutations. They use Fourier transforms, nonlinear mappings, and Vandermonde matrices, and propose a shared-parameter Mamba system for efficiency.

Result: Five key results are presented: observability via permutations, Fourier-based observability with high probability, a Hautus-type condition for Mamba, and a shared-parameter Mamba system. A training algorithm is also introduced.

Conclusion: The methods achieve computational efficiency in enforcing observability, with a training algorithm meeting Robbins-Monro conditions under orthogonality, outperforming classical approaches.

Abstract: We operate through the lens of ordinary differential equations and control
theory to study the concept of observability in the context of neural
state-space models and the Mamba architecture. We develop strategies to enforce
observability, which are tailored to a learning context, specifically where the
hidden states are learnable at initial time, in conjunction to over its
continuum, and high-dimensional. We also highlight our methods emphasize
eigenvalues, roots of unity, or both. Our methods effectuate computational
efficiency when enforcing observability, sometimes at great scale. We formulate
observability conditions in machine learning based on classical control theory
and discuss their computational complexity. Our nontrivial results are
fivefold. We discuss observability through the use of permutations in neural
applications with learnable matrices without high precision. We present two
results built upon the Fourier transform that effect observability with high
probability up to the randomness in the learning. These results are worked with
the interplay of representations in Fourier space and their eigenstructure,
nonlinear mappings, and the observability matrix. We present a result for Mamba
that is similar to a Hautus-type condition, but instead employs an argument
using a Vandermonde matrix instead of eigenvectors. Our final result is a
shared-parameter construction of the Mamba system, which is computationally
efficient in high exponentiation. We develop a training algorithm with this
coupling, showing it satisfies a Robbins-Monro condition under certain
orthogonality, while a more classical training procedure fails to satisfy a
contraction with high Lipschitz constant.

</details>


### [521] [Token-Efficient RL for LLM Reasoning](https://arxiv.org/pdf/2504.20834)
*Alan Lee, Harry Tong*

Main category: cs.LG

TL;DR: Proposes RL strategies for LLMs under memory/compute limits, introducing S-GRPO and T-SPMO, improving accuracy on SVAMP and multi-digit multiplication.


<details>
  <summary>Details</summary>
Motivation: Address challenges of reasoning in LLMs under strict memory and compute constraints, focusing on LoRA fine-tuning compatibility.

Method: Critic-free RL methods targeting small, informative token subsets; introduces S-GRPO (stochastic GRPO) and T-SPMO (token-level prefix matching).

Result: Accuracy on SVAMP rises from 46% to 70%; strong multi-digit multiplication performance. Full-token GRPO under LoRA fails to improve base model.

Conclusion: Selective token-level optimization may act as an implicit regularizer in low-parameter training, showing promise for efficient RL in LLMs.

Abstract: We propose reinforcement learning (RL) strategies tailored for reasoning in
large language models (LLMs) under strict memory and compute limits, with a
particular focus on compatibility with LoRA fine-tuning. Rather than relying on
full-sequence updates or separate critic networks, we design critic-free
methods that operate on a small, informative subset of output tokens to reduce
memory usage and stabilize training. We introduce S-GRPO, a stochastic variant
of Group Relative Policy Optimization, and T-SPMO, a token-level prefix
matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B,
our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show
strong performance on multi-digit multiplication. Surprisingly, full-token GRPO
under LoRA fails to improve over the base model, suggesting that selective
token-level optimization may act as an implicit regularizer in low-parameter
training regimes.

</details>


### [522] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/pdf/2504.16693)
*Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu*

Main category: cs.LG

TL;DR: PIN-WM, a Physics-INformed World Model, learns 3D rigid body dynamics for non-prehensile manipulation using differentiable physics simulation and Gaussian Splatting, achieving robust Sim2Real transfer.


<details>
  <summary>Details</summary>
Motivation: Non-prehensile manipulation is challenging due to sensitivity to physical interactions like friction and restitution, requiring robust policy learning and generalization.

Method: PIN-WM uses differentiable physics simulation and Gaussian Splatting for few-shot, task-agnostic learning of 3D dynamics without state estimation. Physics-aware randomizations create Digital Cousins for Sim2Real bridging.

Result: PIN-WM outperforms Real2Sim2Real methods in simulation and real-world tests, enabling robust non-prehensile manipulation skills.

Conclusion: PIN-WM, enhanced with Digital Cousins, effectively bridges Sim2Real gaps and advances non-prehensile manipulation learning.

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [523] [Hierarchical Uncertainty-Aware Graph Neural Network](https://arxiv.org/pdf/2504.19820)
*Yoonhyuk Choi, Jiho Choi, Taewook Ko, Chong-Kwon Kim*

Main category: cs.LG

TL;DR: HU-GNN integrates hierarchical learning and uncertainty estimation in GNNs, improving robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored synergy between local uncertainty and graph hierarchies in GNNs.

Method: Introduces HU-GNN, combining multi-scale representation, uncertainty estimation, and self-supervised diversity in an end-to-end framework.

Result: Achieves state-of-the-art robustness and interpretability on semi-supervised tasks.

Conclusion: HU-GNN effectively unifies uncertainty and hierarchy, offering theoretical and practical advancements.

Abstract: Recent research on graph neural networks (GNNs) has explored mechanisms for
capturing local uncertainty and exploiting graph hierarchies to mitigate data
sparsity and leverage structural properties. However, the synergistic
integration of these two approaches remains underexplored. This work introduces
a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network
(HU-GNN), which unifies multi-scale representation learning, principled
uncertainty estimation, and self-supervised embedding diversity within a single
end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and
estimates uncertainty at multiple structural scales from individual nodes to
higher levels. These uncertainty estimates guide a robust message-passing
mechanism and attention weighting, effectively mitigating noise and adversarial
perturbations while preserving predictive accuracy on semi-supervised
classification tasks. We also offer key theoretical contributions, including a
probabilistic formulation, rigorous uncertainty-calibration guarantees, and
formal robustness bounds. Extensive experiments on standard benchmarks
demonstrate that our model achieves state-of-the-art robustness and
interpretability.

</details>


### [524] [Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture](https://arxiv.org/pdf/2505.00316)
*Tien Comlekoglu, J. Quetzalcóatl Toledo-Marín, Tina Comlekoglu, Douglas W. DeSimone, Shayn M. Peirce, Geoffrey Fox, James A. Glazier*

Main category: cs.LG

TL;DR: A CNN surrogate model using U-Net accelerates Cellular-Potts model (CPM) simulations by 590x, capturing emergent behaviors like vasculogenesis.


<details>
  <summary>Details</summary>
Motivation: CPMs are computationally expensive due to explicit modeling of interactions and PDEs, limiting scalability.

Method: Developed a U-Net CNN surrogate model to predict 100 Monte-Carlo steps ahead, trained on CPM data.

Result: Achieved 590x speedup, accurately capturing vessel sprouting, extension, anastomosis, and lacunae contraction.

Conclusion: Deep learning can efficiently replace CPMs, enabling faster, larger-scale biological simulations.

Abstract: The Cellular-Potts model is a powerful and ubiquitous framework for
developing computational models for simulating complex multicellular biological
systems. Cellular-Potts models (CPMs) are often computationally expensive due
to the explicit modeling of interactions among large numbers of individual
model agents and diffusive fields described by partial differential equations
(PDEs). In this work, we develop a convolutional neural network (CNN) surrogate
model using a U-Net architecture that accounts for periodic boundary
conditions. We use this model to accelerate the evaluation of a mechanistic CPM
previously used to investigate \textit{in vitro} vasculogenesis. The surrogate
model was trained to predict 100 computational steps ahead (Monte-Carlo steps,
MCS), accelerating simulation evaluations by a factor of 590 times compared to
CPM code execution. Over multiple recursive evaluations, our model effectively
captures the emergent behaviors demonstrated by the original Cellular-Potts
model of such as vessel sprouting, extension and anastomosis, and contraction
of vascular lacunae. This approach demonstrates the potential for deep learning
to serve as efficient surrogate models for CPM simulations, enabling faster
evaluation of computationally expensive CPM of biological processes at greater
spatial and temporal scales.

</details>


### [525] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/pdf/2504.20110)
*Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara*

Main category: cs.LG

TL;DR: A self-supervised method for learning fine-scale geometric features from 3D models, improving surrogate modeling accuracy in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of existing methods in preserving fine-scale geometric details for surrogate modeling due to scarce labeled datasets.

Method: Decouples geometric feature extraction from physics tasks, using near-zero level sampling and a batch-adaptive attention-weighted loss for detailed feature encoding.

Result: Demonstrates strong performance in capturing design features and enabling accurate few-shot physics predictions in structural mechanics.

Conclusion: Bridges the gap between geometric and physics-based representations, offering an effective surrogate modeling solution for data-scarce applications.

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>


### [526] [Variational OOD State Correction for Offline Reinforcement Learning](https://arxiv.org/pdf/2505.00503)
*Ke Jiang, Wen Jiang, Masahiro Fujisawa, Xiaoyang Tan*

Main category: cs.LG

TL;DR: Proposes Density-Aware Safety Perception (DASP) for OOD state correction in offline RL, prioritizing high-density outcomes for safety.


<details>
  <summary>Details</summary>
Motivation: Addresses state distributional shift in offline RL by correcting OOD states to ensure safer decision-making.

Method: Uses a variational framework to optimize actions based on outcome density, promoting in-distribution operations.

Result: Validated effectiveness on MuJoCo and AntMaze suites, showing improved safety and performance.

Conclusion: DASP successfully mitigates OOD issues by leveraging density-aware decision-making.

Abstract: The performance of Offline reinforcement learning is significantly impacted
by the issue of state distributional shift, and out-of-distribution (OOD) state
correction is a popular approach to address this problem. In this paper, we
propose a novel method named Density-Aware Safety Perception (DASP) for OOD
state correction. Specifically, our method encourages the agent to prioritize
actions that lead to outcomes with higher data density, thereby promoting its
operation within or the return to in-distribution (safe) regions. To achieve
this, we optimize the objective within a variational framework that
concurrently considers both the potential outcomes of decision-making and their
density, thus providing crucial contextual information for safe
decision-making. Finally, we validate the effectiveness and feasibility of our
proposed method through extensive experimental evaluations on the offline
MuJoCo and AntMaze suites.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [527] [Safe and Efficient CAV Lane Changing using Decentralised Safety Shields](https://arxiv.org/pdf/2505.01453)
*Bharathkumar Hegde, Melanie Bouroche*

Main category: cs.MA

TL;DR: Proposes a Hybrid Safety Shield (HSS) for safe lane-changing in CAVs, integrating it with MARL (MARL-HSS) to balance efficiency and safety. Evaluated in simulated traffic, it ensures zero crashes and stable policies.


<details>
  <summary>Details</summary>
Motivation: Lane-changing in CAVs must balance efficiency and safety, but existing MARL methods struggle with safety guarantees.

Method: Combines optimisation and rule-based HSS with MARL (MARL-HSS), using control barrier functions for safe maneuvers.

Result: HSS enforces safety constraints, achieving zero crashes and stable policies in light/moderate traffic.

Conclusion: MARL-HSS successfully balances safety and efficiency, outperforming baseline MARL without safety measures.

Abstract: Lane changing is a complex decision-making problem for Connected and
Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with
safety. Although traffic efficiency can be improved by using vehicular
communication for training lane change controllers using Multi-Agent
Reinforcement Learning (MARL), ensuring safety is difficult. To address this
issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines
optimisation and a rule-based approach to guarantee safety. Our method applies
control barrier functions to constrain longitudinal and lateral control inputs
of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to
integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while
ensuring safety. We evaluate MARL-HSS using a gym-like environment that
simulates an on-ramp merging scenario with two levels of traffic densities,
such as light and moderate densities. The results show that HSS provides a
safety guarantee by strictly enforcing a dynamic safety constraint defined on a
time headway, even in moderate traffic density that offers challenging lane
change scenarios. Moreover, the proposed method learns stable policies compared
to the baseline, a state-of-the-art MARL lane change controller without a
safety shield. Further policy evaluation shows that our method achieves a
balance between safety and traffic efficiency with zero crashes and comparable
average speeds in light and moderate traffic densities.

</details>


### [528] [Pathfinders in the Sky: Formal Decision-Making Models for Collaborative Air Traffic Control in Convective Weather](https://arxiv.org/pdf/2505.01804)
*Jimin Choi, Kartikeya Anand, Husni R. Idris, Huy T. Tran, Max Z. Li*

Main category: cs.MA

TL;DR: The paper addresses the lack of research on multi-agent decision-making in pathfinder operations for air traffic disrupted by weather, proposing models to analyze operational dynamics and resilience.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in understanding the multi-agent decision-making problem in pathfinder operations, which are routine but under-researched in air traffic control.

Method: Formulates decision models, including a Markov chain for operational state transitions, flight-specific acceptance models, and pathfinder selection strategies. Analyzes steady-state behavior and conducts worst-case scenario analysis.

Result: Empirical FAA data validates the significance of pathfinder operations. The models highlight risks like collective rejection and explore resilience factors like selfless behavior and uncertainty.

Conclusion: The study provides foundational models for pathfinder operations, offering insights into system dynamics and resilience, with practical implications for air traffic management.

Abstract: Air traffic can be significantly disrupted by weather. Pathfinder operations
involve assigning a designated aircraft to assess whether airspace that was
previously impacted by weather can be safely traversed through. Despite
relatively routine use in air traffic control, there is little research on the
underlying multi-agent decision-making problem. We seek to address this gap
herein by formulating decision models to capture the operational dynamics and
implications of pathfinders. Specifically, we construct a Markov chain to
represent the stochastic transitions between key operational states (e.g.,
pathfinder selection). We then analyze its steady-state behavior to understand
long-term system dynamics. We also propose models to characterize
flight-specific acceptance behaviors (based on utility trade-offs) and
pathfinder selection strategies (based on sequential offer allocations). We
then conduct a worst-case scenario analysis that highlights risks from
collective rejection and explores how selfless behavior and uncertainty affect
system resilience. Empirical analysis of data from the US Federal Aviation
Administration demonstrates the real-world significance of pathfinder
operations and informs future model calibration.

</details>


### [529] [Act Natural! Extending Naturalistic Projection to Multimodal Behavior Scenarios](https://arxiv.org/pdf/2505.01945)
*Hamzah I. Khan, David Fridovich-Keil*

Main category: cs.MA

TL;DR: The paper proposes a method to enhance autonomous agents' predictability and naturalistic behavior by modeling multimodal human actions using multiple convex sets, improving fidelity in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents need to behave predictably and naturally in public spaces, but existing methods fail to capture all human motivations or require excessive data.

Method: Extends unimodal naturalistic behavior modeling with multiple convex sets for multimodal behavior, using an optimization-based filter to project trajectories into these sets.

Result: Demonstrated on real-world human driving data (inD and rounD datasets), showing improved naturalistic behavior modeling.

Conclusion: The approach effectively models multimodal human behavior, enhancing autonomous agents' predictability and naturalism in complex scenarios.

Abstract: Autonomous agents operating in public spaces must consider how their
behaviors might affect the humans around them, even when not directly
interacting with them. To this end, it is often beneficial to be predictable
and appear naturalistic. Existing methods for this purpose use human actor
intent modeling or imitation learning techniques, but these approaches rarely
capture all possible motivations for human behavior and/or require significant
amounts of data. Our work extends a technique for modeling unimodal
naturalistic behaviors with an explicit convex set representation, to account
for multimodal behavior by using multiple convex sets. This more flexible
representation provides a higher degree of fidelity in data-driven modeling of
naturalistic behavior that arises in real-world scenarios in which human
behavior is, in some sense, discrete, e.g. whether or not to yield at a
roundabout. Equipped with this new set representation, we develop an
optimization-based filter to project arbitrary trajectories into the set so
that they appear naturalistic to humans in the scene, while also satisfying
vehicle dynamics, actuator limits, etc. We demonstrate our methods on
real-world human driving data from the inD (intersection) and rounD
(roundabout) datasets.

</details>


### [530] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/pdf/2504.20903)
*Prothit Sen, Sai Mihir Jakkaraju*

Main category: cs.MA

TL;DR: The paper explores AI-human collaboration in tasks, showing its effectiveness depends on task structure (modular vs. sequenced) rather than context. AI substitutes humans in modular tasks, while sequenced tasks reveal complementarities, especially when humans initiate and AI refines.


<details>
  <summary>Details</summary>
Motivation: To understand how AI and humans can collaborate effectively in organizational tasks, focusing on task structure as a key determinant.

Method: Agent-based simulation using an NK model to compare heuristic-based human adaptation and rule-based AI search in modular and sequenced tasks.

Result: In modular tasks, AI outperforms humans unless human expertise is high. In sequenced tasks, human-initiated AI refinement maximizes performance, while AI-led processes can suffer from excessive human refinement. Even flawed AI aids low-capability humans.

Conclusion: Task structure, not context, dictates AI-human collaboration success. The model offers a general framework for strategic decision-making in diverse settings.

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [531] [TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing](https://arxiv.org/pdf/2505.02096)
*Yaru Chen, Peiliang Zhang, Fei Li, Faegheh Sardari, Ruohao Guo, Zhenbo Li, Wenwu Wang*

Main category: cs.MM

TL;DR: The paper proposes TeMTG, a multimodal optimization framework for Audio-Visual Video Parsing (AVVP), enhancing semantic representation and modeling temporal dependencies to improve event parsing accuracy under weak supervision.


<details>
  <summary>Details</summary>
Motivation: Existing AVVP methods lack explicit modeling of semantic relationships and temporal dependencies, leading to ambiguous event boundaries and inaccurate parsing under weak supervision.

Method: TeMTG combines text enhancement (using pre-trained models for modality-specific text embeddings) and multi-hop temporal graph modeling to improve feature representation and capture temporal continuity.

Result: The method achieves state-of-the-art performance on the LLP dataset.

Conclusion: TeMTG effectively addresses the limitations of existing AVVP methods by enhancing semantic representation and explicitly modeling temporal dependencies.

Abstract: Audio-Visual Video Parsing (AVVP) task aims to parse the event categories and
occurrence times from audio and visual modalities in a given video. Existing
methods usually focus on implicitly modeling audio and visual features through
weak labels, without mining semantic relationships for different modalities and
explicit modeling of event temporal dependencies. This makes it difficult for
the model to accurately parse event information for each segment under weak
supervision, especially when high similarity between segmental modal features
leads to ambiguous event boundaries. Hence, we propose a multimodal
optimization framework, TeMTG, that combines text enhancement and multi-hop
temporal graph modeling. Specifically, we leverage pre-trained multimodal
models to generate modality-specific text embeddings, and fuse them with
audio-visual features to enhance the semantic representation of these features.
In addition, we introduce a multi-hop temporal graph neural network, which
explicitly models the local temporal relationships between segments, capturing
the temporal continuity of both short-term and long-range events. Experimental
results demonstrate that our proposed method achieves state-of-the-art (SOTA)
performance in multiple key indicators in the LLP dataset.

</details>


### [532] [From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking](https://arxiv.org/pdf/2008.06255)
*Seung-Hun Nam, Jihyeon Kang, Daesik Kim, Namhyuk Ahn, Wonhyuk Ahn*

Main category: cs.MM

TL;DR: The paper introduces a watermarking attack network (WAN) to benchmark and exploit vulnerabilities in multi-bit watermarking (MW) systems, improving robustness and visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing benchmark tools for MW systems lack focus on visual quality and fail to exploit MW-specific vulnerabilities.

Method: Proposes WAN, a trainable benchmark tool using residual dense blocks to attack MW systems while maintaining visual quality.

Result: WAN effectively undermines MW systems and enhances imperceptibility and robustness, as shown by empirical results.

Conclusion: WAN serves as both a benchmarking tool and an add-on mechanism, improving MW systems without needing detailed context.

Abstract: Multi-bit watermarking (MW) has been designed to enhance resistance against
watermarking attacks, such as signal processing operations and geometric
distortions. Various benchmark tools exist to assess this robustness through
simulated attacks on watermarked images. However, these tools often fail to
capitalize on the unique attributes of the targeted MW and typically neglect
the aspect of visual quality, a critical factor in practical applications. To
overcome these shortcomings, we introduce a watermarking attack network (WAN),
a fully trainable watermarking benchmark tool designed to exploit
vulnerabilities within MW systems and induce watermark bit inversions,
significantly diminishing watermark extractability. The proposed WAN employs an
architecture based on residual dense blocks, which is adept at both local and
global feature learning, thereby maintaining high visual quality while
obstructing the extraction of embedded information. Our empirical results
demonstrate that the WAN effectively undermines various block-based MW systems
while minimizing visual degradation caused by attacks. This is facilitated by
our novel watermarking attack loss, which is specifically crafted to compromise
these systems. The WAN functions not only as a benchmarking tool but also as an
add-on watermarking (AoW) mechanism, augmenting established universal
watermarking schemes by enhancing robustness or imperceptibility without
requiring detailed method context and adapting to dynamic watermarking
requirements. Extensive experimental results show that AoW complements the
performance of the targeted MW system by independently enhancing both
imperceptibility and robustness.

</details>


### [533] [Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline](https://arxiv.org/pdf/2504.21772)
*Minwoo Oh, Minsu Park, Eunil Park*

Main category: cs.MM

TL;DR: A novel pipeline combining Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR) effectively addresses copyright issues in short videos by separating and restoring original soundtracks (OST) from background music (BGM).


<details>
  <summary>Details</summary>
Motivation: Short video platforms struggle with copyright compliance due to infringers using arbitrary BGM to obscure OSTs, evading originality detection.

Method: The proposed pipeline integrates MSS and CMVMR to separate BGM from OST and restore authentic audio tracks, supported by two datasets: OASD-20K and OSVAR-160.

Result: The pipeline accurately removes BGM and restores OSTs, ensuring content integrity, as demonstrated by experiments.

Conclusion: This approach offers an ethical and scalable solution to copyright challenges in user-generated short videos.

Abstract: Short video platforms like YouTube Shorts and TikTok face significant
copyright compliance challenges, as infringers frequently embed arbitrary
background music (BGM) to obscure original soundtracks (OST) and evade content
originality detection. To tackle this issue, we propose a novel pipeline that
integrates Music Source Separation (MSS) and cross-modal video-music retrieval
(CMVMR). Our approach effectively separates arbitrary BGM from the original
OST, enabling the restoration of authentic video audio tracks. To support this
work, we introduce two domain-specific datasets: OASD-20K for audio separation
and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips
featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset
comprising 1,121 video and mixed-audio pairs, specifically designed for short
video restoration tasks. Experimental results demonstrate that our pipeline not
only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring
content integrity. This approach provides an ethical and scalable solution to
copyright challenges in user-generated content on short video platforms.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [534] [Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments](https://arxiv.org/pdf/2505.01632)
*Noussaiba Djeffal, Djamel Addou, Hamza Kheddar, Sid Ahmed Selouani*

Main category: eess.AS

TL;DR: The paper proposes a neural framework with a robust frontend for ASR systems, improving accuracy in noisy environments using ResNet-based transfer learning.


<details>
  <summary>Details</summary>
Motivation: To address the persistent challenge of non-stationary noise in ASR systems, leveraging data-driven supervised approaches like deep neural networks.

Method: Introduces a neural framework incorporating a robust frontend, evaluated using the Aurora-2 database with ResNet-based transfer learning for Mel-frequency features.

Result: Achieved 98.94% accuracy in clean and 91.21% in noisy environments, outperforming CNN and LSTM networks.

Conclusion: The proposed ResNet-based framework significantly enhances ASR performance in both clean and noisy conditions.

Abstract: Addressing the detrimental impact of non-stationary environmental noise on
automatic speech recognition (ASR) has been a persistent and significant
research focus. Despite advancements, this challenge continues to be a major
concern. Recently, data-driven supervised approaches, such as deep neural
networks, have emerged as promising alternatives to traditional unsupervised
methods. With extensive training, these approaches have the potential to
overcome the challenges posed by diverse real-life acoustic environments. In
this light, this paper introduces a novel neural framework that incorporates a
robust frontend into ASR systems in both clean and noisy environments.
Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness
of an acoustic feature set for Mel-frequency, employing the approach of
transfer learning based on Residual neural network (ResNet). The experimental
results demonstrate a significant improvement in recognition accuracy compared
to convolutional neural networks (CNN) and long short-term memory (LSTM)
networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.

</details>


### [535] [Low-Complexity Acoustic Scene Classification with Device Information in the DCASE 2025 Challenge](https://arxiv.org/pdf/2505.01747)
*Florian Schmid, Paul Primus, Toni Heittola, Annamaria Mesaros, Irene Martín-Morató, Gerhard Widmer*

Main category: eess.AS

TL;DR: The paper introduces the DCASE 2025 Challenge's low-complexity acoustic scene classification task, emphasizing device-specific models using provided device info at inference. The baseline system achieves 50.72% accuracy (device-general) and 51.89% (device-specific).


<details>
  <summary>Details</summary>
Motivation: To address real-world deployment scenarios by leveraging device information for improved acoustic scene classification, focusing on low-complexity models and data efficiency.

Method: Uses a baseline system with device-general and device-specific models, trained on a 25% subset of DCASE 2024 data, allowing external data for transfer learning.

Result: Baseline achieves 50.72% accuracy (device-general) and 51.89% (device-specific) on a ten-class problem.

Conclusion: Providing device information at inference improves model performance, highlighting its potential for real-world applications.

Abstract: This paper presents the Low-Complexity Acoustic Scene Classification with
Device Information Task of the DCASE 2025 Challenge and its baseline system.
Continuing the focus on low-complexity models, data efficiency, and device
mismatch from previous editions (2022--2024), this year's task introduces a key
change: recording device information is now provided at inference time. This
enables the development of device-specific models that leverage device
characteristics -- reflecting real-world deployment scenarios in which a model
is designed with awareness of the underlying hardware. The training set matches
the 25% subset used in the corresponding DCASE 2024 challenge, with no
restrictions on external data use, highlighting transfer learning as a central
topic. The baseline achieves 50.72% accuracy on this ten-class problem with a
device-general model, improving to 51.89% when using the available device
information.

</details>


### [536] [FLOWER: Flow-Based Estimated Gaussian Guidance for General Speech Restoration](https://arxiv.org/pdf/2505.01750)
*Da-Hee Yang, Jaeuk Lee, Joon-Hyuk Chang*

Main category: eess.AS

TL;DR: FLOWER is a new conditioning method for speech restoration using Gaussian guidance in generative frameworks, improving performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance speech restoration by integrating Gaussian guidance into generative models for better control and performance.

Method: Transforms clean speech into a Gaussian distribution via a normalizing flow network, using this to guide generative models at each block.

Result: FLOWER effectively improves performance in general speech restoration tasks.

Conclusion: FLOWER offers a promising approach for precise and effective speech restoration.

Abstract: We introduce FLOWER, a novel conditioning method designed for speech
restoration that integrates Gaussian guidance into generative frameworks. By
transforming clean speech into a predefined prior distribution (e.g., Gaussian
distribution) using a normalizing flow network, FLOWER extracts critical
information to guide generative models. This guidance is incorporated into each
block of the generative network, enabling precise restoration control.
Experimental results demonstrate the effectiveness of FLOWER in improving
performance across various general speech restoration tasks.

</details>


### [537] [Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction in open-plan offices](https://arxiv.org/pdf/2501.15744)
*Manuj Yadav, Jungsoo Kim, Valtteri Hongisto, Densil Cabrera, Richard de Dear*

Main category: eess.AS

TL;DR: The study models acoustic dissatisfaction in open-plan offices using room acoustics, sound environment, and occupant surveys, finding lack of privacy more impactful than noise disturbance. SPL-based metrics outperformed distraction distance, contradicting prior findings. Dissatisfaction varied with office size and parameters but not seating flexibility.


<details>
  <summary>Details</summary>
Motivation: To understand and predict acoustic dissatisfaction in open-plan offices by analyzing room acoustics, sound environment, and occupant perceptions.

Method: Combined measurements of room acoustics, sound environment during occupancy, and occupant surveys (n=349) across 28 diverse offices.

Result: Lack of privacy was 25% more impactful than noise disturbance. SPL-based metrics were better predictors than distraction distance. Dissatisfaction varied with office size and parameters but not seating flexibility.

Conclusion: The study reveals complexities in linking instrumental acoustic measurements to occupant perceptions, challenging some ISO 3382-3 expectations.

Abstract: Open-plan offices are well-known to be adversely affected by acoustic issues.
This study aims to model acoustic dissatisfaction using measurements of room
acoustics, sound environment during occupancy, and occupant surveys (n = 349)
in 28 offices representing a diverse range of workplace parameters. As latent
factors, the contribution of $\textit{lack of privacy}$ (LackPriv) was 25%
higher than $\textit{noise disturbance}$ (NseDstrb) in predicting
$\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on
sound pressure level (SPL) decay of speech ($L_{\text{p,A,s,4m}}$ and
$r_{\text{C}}$) were better in predicting these factors than distraction
distance ($r_{\text{D}}$) based on speech transmission index. This contradicts
previous findings, and the trends for SPL-based metrics in predicting AcDsat
and LackPriv go against expectations based on ISO 3382-3. For sound during
occupation, $L_{\text{A,90}}$ and psychoacoustic loudness ($N_{\text{90}}$)
predicted AcDsat, and a SPL fluctuation metric ($M_{\text{A,eq}}$) predicted
LackPriv. However, these metrics were weaker predictors than ISO 3382-3
metrics. Medium-sized offices exhibited higher dissatisfaction than larger
($\geq$50 occupants) offices. Dissatisfaction varied substantially across
parameters including ceiling heights, number of workstations, and years of
work, but not between offices with fixed seating compared to more flexible and
activity-based working configurations. Overall, these findings highlight the
complexities in characterizing occupants' perceptions using instrumental
acoustic measurements.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [538] [CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering](https://arxiv.org/pdf/2505.01476)
*Zhe Zhang, Mingxiu Cai, Hanxiao Wang, Gaochang Wu, Tianyou Chai, Xiatian Zhu*

Main category: eess.IV

TL;DR: CostFilter-AD introduces cost filtering from classical matching tasks to improve anomaly detection in images by refining matching cost volumes.


<details>
  <summary>Details</summary>
Motivation: Existing UAD methods rely on inaccurate image- or feature-level matching, leading to sub-optimal detection.

Method: Constructs a matching cost volume and refines it using a cost volume filtering network guided by input observation.

Result: Validated on MVTec-AD and VisA benchmarks, showing benefits for single- and multi-class UAD tasks.

Conclusion: CostFilter-AD is a versatile plug-in that enhances both reconstruction- and embedding-based UAD methods.

Abstract: Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an
input image with respect to normal samples. Either by reconstructing normal
counterparts (reconstruction-based) or by learning an image feature embedding
space (embedding-based), existing approaches fundamentally rely on image-level
or feature-level matching to derive anomaly scores. Often, such a matching
process is inaccurate yet overlooked, leading to sub-optimal detection. To
address this issue, we introduce the concept of cost filtering, borrowed from
classical matching tasks, such as depth and flow estimation, into the UAD
problem. We call this approach {\em CostFilter-AD}. Specifically, we first
construct a matching cost volume between the input and normal samples,
comprising two spatial dimensions and one matching dimension that encodes
potential matches. To refine this, we propose a cost volume filtering network,
guided by the input observation as an attention query across multiple feature
layers, which effectively suppresses matching noise while preserving edge
structures and capturing subtle anomalies. Designed as a generic
post-processing plug-in, CostFilter-AD can be integrated with either
reconstruction-based or embedding-based methods. Extensive experiments on
MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for
both single- and multi-class UAD tasks. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [539] [Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth](https://arxiv.org/pdf/2505.01638)
*Michael Marinaccio, Fatemeh Afghah*

Main category: eess.IV

TL;DR: SAM-TIFF enables wildfire temperature prediction and segmentation using only RGB input, eliminating the need for thermal sensors by distilling knowledge from a multimodal teacher to a unimodal student network.


<details>
  <summary>Details</summary>
Motivation: To reduce hardware costs and power consumption in wildfire monitoring by avoiding the need for multimodal sensing (RGB and thermal imagery).

Method: Uses a teacher-student distillation framework where a multimodal teacher (trained on RGB-Thermal data) teaches a unimodal RGB student. Segmentation is supervised via SAM-guided mask generation, TOPSIS selection, Canny edge detection, and Otsu's thresholding.

Result: Achieves per-pixel temperature regression from RGB UAV data, showing strong generalization on the FLAME 3 dataset.

Conclusion: Paves the way for lightweight, cost-effective UAV wildfire monitoring without thermal sensors.

Abstract: High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)
typically requires multimodal sensing - especially RGB and thermal imagery -
which increases hardware cost and power consumption. This paper introduces
SAM-TIFF, a novel teacher-student distillation framework for pixel-level
wildfire temperature prediction and segmentation using RGB input only. A
multimodal teacher network trained on paired RGB-Thermal imagery and
radiometric TIFF ground truth distills knowledge to a unimodal RGB student
network, enabling thermal-sensor-free inference. Segmentation supervision is
generated using a hybrid approach of segment anything (SAM)-guided mask
generation, and selection via TOPSIS, along with Canny edge detection and
Otsu's thresholding pipeline for automatic point prompt selection. Our method
is the first to perform per-pixel temperature regression from RGB UAV data,
demonstrating strong generalization on the recent FLAME 3 dataset. This work
lays the foundation for lightweight, cost-effective UAV-based wildfire
monitoring systems without thermal sensors.

</details>


### [540] [A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans](https://arxiv.org/pdf/2505.01644)
*Jun Li, Yijue Zhang, Haibo Shi, Minhong Li, Qiwei Li, Xiaohua Qian*

Main category: eess.IV

TL;DR: A dual-task framework combining pixel-level classification and regression improves pancreatic lesion segmentation and generalization across diverse imaging datasets.


<details>
  <summary>Details</summary>
Motivation: Pancreatic cancer's high mortality and variability in imaging necessitate accurate lesion delineation, but current methods lack generalizability due to tissue heterogeneity.

Method: The framework synergizes segmentation and regression tasks, using dual self-supervised learning to enhance feature and output space stability.

Result: Achieves 84.07% Dice score in-domain and improves cross-lesion segmentation by 9.51%.

Conclusion: The model provides robust support for pancreatic disease management and broader medical applications, with code publicly available.

Abstract: Pancreatic cancer, characterized by its notable prevalence and mortality
rates, demands accurate lesion delineation for effective diagnosis and
therapeutic interventions. The generalizability of extant methods is frequently
compromised due to the pronounced variability in imaging and the heterogeneous
characteristics of pancreatic lesions, which may mimic normal tissues and
exhibit significant inter-patient variability. Thus, we propose a
generalization framework that synergizes pixel-level classification and
regression tasks, to accurately delineate lesions and improve model stability.
This framework not only seeks to align segmentation contours with actual
lesions but also uses regression to elucidate spatial relationships between
diseased and normal tissues, thereby improving tumor localization and
morphological characterization. Enhanced by the reciprocal transformation of
task outputs, our approach integrates additional regression supervision within
the segmentation context, bolstering the model's generalization ability from a
dual-task perspective. Besides, dual self-supervised learning in feature spaces
and output spaces augments the model's representational capability and
stability across different imaging views. Experiments on 594 samples composed
of three datasets with significant imaging differences demonstrate that our
generalized pancreas segmentation results comparable to mainstream in-domain
validation performance (Dice: 84.07%). More importantly, it successfully
improves the results of the highly challenging cross-lesion generalized
pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a
resilient and efficient foundational technological support for pancreatic
disease management and wider medical applications. The codes will be released
at https://github.com/SJTUBME-QianLab/Dual-Task-Seg.

</details>


### [541] [Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations](https://arxiv.org/pdf/2505.01670)
*Christos Zangos, Danish Ebadulla, Thomas Christopher Sprague, Ambuj Singh*

Main category: eess.IV

TL;DR: A novel fMRI-based visual image reconstruction method uses a subject-agnostic common representation space, improving efficiency and performance in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in traditional fMRI-based visual image reconstruction methods by leveraging a common representation space for brain signal alignment.

Method: Aligns brain signals of subjects in a common space during training, using lightweight modules for subject-specific adaptation.

Result: Demonstrates superior efficiency and performance, especially in low-data settings, and is effective across subjects and datasets.

Conclusion: The proposed common space approach is a robust and efficient alternative to traditional methods for fMRI-based visual image reconstruction.

Abstract: This work introduces a novel approach to fMRI-based visual image
reconstruction using a subject-agnostic common representation space. We show
that the brain signals of the subjects can be aligned in this common space
during training to form a semantically aligned common brain. This is leveraged
to demonstrate that aligning subject-specific lightweight modules to a
reference subject is significantly more efficient than traditional end-to-end
training methods. Our approach excels in low-data scenarios. We evaluate our
methods on different datasets, demonstrating that the common space is subject
and dataset-agnostic.

</details>


### [542] [CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification](https://arxiv.org/pdf/2505.01741)
*Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea*

Main category: eess.IV

TL;DR: A novel CNN training method, CLOG-CD, combines curriculum learning and class decomposition to improve medical image classification, achieving high accuracy on imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Data irregularities in medical imaging make classification challenging. Curriculum learning and class decomposition can address this by improving model performance and training efficiency.

Method: CLOG-CD uses curriculum learning and class decomposition, training from descending to ascending order (anti-curriculum). Evaluated on four imbalanced medical datasets using ResNet-50 and DenseNet-121.

Result: High accuracy achieved: 96.08% (CXR), 96.91% (brain tumour), 79.76% (knee X-ray), 99.17% (CRC) with ResNet-50; similar results with DenseNet-121.

Conclusion: CLOG-CD effectively improves medical image classification performance, demonstrating the value of combining curriculum learning and class decomposition.

Abstract: Curriculum learning strategies have been proven to be effective in various
applications and have gained significant interest in the field of machine
learning. It has the ability to improve the final model's performance and
accelerate the training process. However, in the medical imaging domain, data
irregularities can make the recognition task more challenging and usually
result in misclassification between the different classes in the dataset.
Class-decomposition approaches have shown promising results in solving such a
problem by learning the boundaries within the classes of the data set. In this
paper, we present a novel convolutional neural network (CNN) training method
based on the curriculum learning strategy and the class decomposition approach,
which we call CLOG-CD, to improve the performance of medical image
classification. We evaluated our method on four different imbalanced medical
image datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray,
and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights
from the decomposition granularity of the classes, and the training is
accomplished from descending to ascending order (i.e., anti-curriculum
technique). We also investigated the classification performance of our proposed
method based on different acceleration factors and pace function curricula. We
used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for
CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to
improve classification performance with an accuracy of 96.08% for the CXR
dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee
X-ray, and 99.17% for the CRC dataset, compared to other training strategies.
In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%,
and 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets,
respectively

</details>


### [543] [Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs](https://arxiv.org/pdf/2505.01742)
*Yu Mao, Jingzong Li, Jun Wang, Hong Xu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue*

Main category: eess.IV

TL;DR: Easz is a transformer-based edge-compute-free image coding framework that shifts computational overhead to servers, using patch-erase and lightweight reconstruction for efficient edge-device compression.


<details>
  <summary>Details</summary>
Motivation: Neural image compression is hindered by heavy encode-decode structures and inflexibility in switching compression levels, making it unsuitable for edge devices.

Method: Easz employs a patch-erase algorithm with a conditional uniform-based sampler and a lightweight transformer-based reconstruction framework.

Result: Easz outperforms existing methods in adaptability, computational efficiency, and image reconstruction quality.

Conclusion: Easz effectively addresses edge-device compression challenges by offloading computation to servers and optimizing reconstruction.

Abstract: Neural image compression, necessary in various machine-to-machine
communication scenarios, suffers from its heavy encode-decode structures and
inflexibility in switching between different compression levels. Consequently,
it raises significant challenges in applying the neural image compression to
edge devices that are developed for powerful servers with high computational
and storage capacities. We take a step to solve the challenges by proposing a
new transformer-based edge-compute-free image coding framework called Easz.
Easz shifts the computational overhead to the server, and hence avoids the
heavy encoding and model switching overhead on the edge. Easz utilizes a
patch-erase algorithm to selectively remove image contents using a conditional
uniform-based sampler. The erased pixels are reconstructed on the receiver side
through a transformer-based framework. To further reduce the computational
overhead on the receiver, we then introduce a lightweight transformer-based
reconstruction structure to reduce the reconstruction load on the receiver
side. Extensive evaluations conducted on a real-world testbed demonstrate
multiple advantages of Easz over existing compression approaches, in terms of
adaptability to different compression levels, computational efficiency, and
image reconstruction quality.

</details>


### [544] [LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction](https://arxiv.org/pdf/2505.01755)
*Jiesong Bai, Yuhao Yin, Yihang Dong, Xiaofeng Zhang, Chi-Man Pun, Xuhang Chen*

Main category: eess.IV

TL;DR: LensNet is a deep learning framework for lensless imaging, combining spatial and frequency domains with a learnable PSF estimator, outperforming traditional methods in reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Traditional lensless imaging relies on rigid PSF models, limiting adaptability to real-world challenges like noise and dynamic scenes. LensNet aims to overcome these limitations.

Method: LensNet integrates spatial and frequency domains, using a learnable Coded Mask Simulator (CMS) for dynamic PSF estimation and Wiener filtering for detail restoration.

Result: LensNet achieves superior reconstruction quality, preserving high-frequency details and reducing noise better than state-of-the-art methods.

Conclusion: LensNet bridges physics-based modeling and data-driven learning, offering a flexible, accurate solution for lensless imaging in diverse applications.

Abstract: Lensless imaging stands out as a promising alternative to conventional
lens-based systems, particularly in scenarios demanding ultracompact form
factors and cost-effective architectures. However, such systems are
fundamentally governed by the Point Spread Function (PSF), which dictates how a
point source contributes to the final captured signal. Traditional lensless
techniques often require explicit calibrations and extensive pre-processing,
relying on static or approximate PSF models. These rigid strategies can result
in limited adaptability to real-world challenges, including noise, system
imperfections, and dynamic scene variations, thus impeding high-fidelity
reconstruction. In this paper, we propose LensNet, an end-to-end deep learning
framework that integrates spatial-domain and frequency-domain representations
in a unified pipeline. Central to our approach is a learnable Coded Mask
Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during
training, effectively mitigating the shortcomings of fixed or sparsely
calibrated kernels. By embedding a Wiener filtering component, LensNet refines
global structure and restores fine-scale details, thus alleviating the
dependency on multiple handcrafted pre-processing steps. Extensive experiments
demonstrate LensNet's robust performance and superior reconstruction quality
compared to state-of-the-art methods, particularly in preserving high-frequency
details and attenuating noise. The proposed framework establishes a novel
convergence between physics-based modeling and data-driven learning, paving the
way for more accurate, flexible, and practical lensless imaging solutions for
applications ranging from miniature sensors to medical diagnostics. The link of
code is https://github.com/baijiesong/Lensnet.

</details>


### [545] [Continuous Filtered Backprojection by Learnable Interpolation Network](https://arxiv.org/pdf/2505.01768)
*Hui Lin, Dong Zeng, Qi Xie, Zerui Mao, Jianhua Ma, Deyu Meng*

Main category: eess.IV

TL;DR: Proposes LInFBP, a deep learning model to improve CT image reconstruction by replacing fixed interpolation in FBP with learnable interpolation.


<details>
  <summary>Details</summary>
Motivation: Address interpolation errors in conventional FBP methods, which degrade CT image quality.

Method: Uses a deep network to predict coefficients for a linear combination of basis functions, enabling learnable interpolation in FBP.

Result: LInFBP enhances image quality, offers plug-and-play ability, and shows strong generalization across CT scenarios.

Conclusion: LInFBP effectively reduces interpolation errors in FBP, improving CT reconstruction accuracy.

Abstract: Accurate reconstruction of computed tomography (CT) images is crucial in
medical imaging field. However, there are unavoidable interpolation errors in
the backprojection step of the conventional reconstruction methods, i.e.,
filtered-back-projection based methods, which are detrimental to the accurate
reconstruction. In this study, to address this issue, we propose a novel deep
learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to
enhance the reconstructed CT image quality, which achieves learnable
interpolation in the backprojection step of filtered backprojection (FBP) and
alleviates the interpolation errors. Specifically, in the proposed LInFBP, we
formulate every local piece of the latent continuous function of discrete
sinogram data as a linear combination of selected basis functions, and learn
this continuous function by exploiting a deep network to predict the linear
combination coefficients. Then, the learned latent continuous function is
exploited for interpolation in backprojection step, which first time takes the
advantage of deep learning for the interpolation in FBP. Extensive experiments,
which encompass diverse CT scenarios, demonstrate the effectiveness of the
proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play
ability and generalization capability.

</details>


### [546] [Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement](https://arxiv.org/pdf/2505.01831)
*Haofan Wu, Yin Huang, Yuqing Wu, Qiuyu Yang, Bingfang Wang, Li Zhang, Muhammad Fahadullah Khan, Ali Zia, M. Saleh Memon, Syed Sohail Bukhari, Abdul Fattah Memon, Daizong Ji, Ya Zhang, Ghulam Mustafa, Yin Fang*

Main category: eess.IV

TL;DR: The paper proposes MTRL-FIE, a multi-scale target-aware framework for fundus image enhancement, addressing limitations in existing methods by combining structural detail restoration and pathological region enhancement.


<details>
  <summary>Details</summary>
Motivation: Fundus images often suffer from low resolution and noise due to hardware and operational issues. Existing methods lack a unified approach for multi-scale enhancement and fail to focus on lesions, crucial for diagnosis.

Method: The framework includes a multi-scale feature encoder (MFE) using wavelet decomposition, a structure-preserving hierarchical decoder (SHD) for feature fusion, and a target-aware feature aggregation (TFA) module for lesion enhancement.

Result: MTRL-FIE outperforms state-of-the-art methods in enhancement performance with a lightweight architecture and generalizes to other ophthalmic tasks without fine-tuning.

Conclusion: MTRL-FIE is effective, generalizable, and clinically promising for fundus image enhancement and other ophthalmic applications.

Abstract: High-quality fundus images provide essential anatomical information for
clinical screening and ophthalmic disease diagnosis. Yet, due to hardware
limitations, operational variability, and patient compliance, fundus images
often suffer from low resolution and signal-to-noise ratio. Recent years have
witnessed promising progress in fundus image enhancement. However, existing
works usually focus on restoring structural details or global characteristics
of fundus images, lacking a unified image enhancement framework to recover
comprehensive multi-scale information. Moreover, few methods pinpoint the
target of image enhancement, e.g., lesions, which is crucial for medical
image-based diagnosis. To address these challenges, we propose a multi-scale
target-aware representation learning framework (MTRL-FIE) for efficient fundus
image enhancement. Specifically, we propose a multi-scale feature encoder (MFE)
that employs wavelet decomposition to embed both low-frequency structural
information and high-frequency details. Next, we design a structure-preserving
hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real
fundus image restoration. SHD integrates hierarchical fusion and group
attention mechanisms to achieve adaptive feature fusion while retaining local
structural smoothness. Meanwhile, a target-aware feature aggregation (TFA)
module is used to enhance pathological regions and reduce artifacts.
Experimental results on multiple fundus image datasets demonstrate the
effectiveness and generalizability of MTRL-FIE for fundus image enhancement.
Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement
performance with a more lightweight architecture. Furthermore, our approach
generalizes to other ophthalmic image processing tasks without supervised
fine-tuning, highlighting its potential for clinical applications.

</details>


### [547] [Impact of Chirality on the Properties of Two-Dimensional Images Propagating Through a Chiral Dispersive Thick Lens](https://arxiv.org/pdf/2505.01836)
*Salaheddeen Bugoffa, Hussin Ragb*

Main category: eess.IV

TL;DR: The paper derives dual image formation for a 2D object using a chiral-dispersive thick lens, analyzing bimodal propagation via circular polarizations and dispersion effects.


<details>
  <summary>Details</summary>
Motivation: To explore imaging through chiral-dispersive thick lenses, considering frequency-dependent material dispersion and chirality, and to understand the resulting dual-image formation.

Method: Derives two sets of ABCD matrices for right- and left-circularly polarized modes under paraxial and meridional conditions, and examines image transmission and dispersion effects.

Result: Demonstrates dual-image formation due to bimodal propagation, with defocusing effects influenced by dispersion and chirality bands.

Conclusion: The study highlights the unique imaging properties of chiral thick lenses, particularly the role of chirality and dispersion in dual-image formation.

Abstract: Dual image formation for a two-dimensional object via bimodal propagation
through chiral-dispersive thick lens is derived. In this article, first-order
frequency-dependent material dispersion of the dielectric permittivity and the
lens material being chiral are considered. In addition, the thick lens is
configured in a uniform background. A salient feature of a chiral thick lens is
the inherent bimodal propagation via circular polarizations. Under chirality,
two sets of ABCD frequency dependent matrices are derived for right- and
left-circularly polarized modes based on standard paraxial and meridional
conditions. For imaging purposes, a simple2D colored transparency is placed as
an object before the thick lens. The image transmission across the lens
examined via the ABCD matrix parameters and defocusing effects due to
dispersion under different chirality bands.

</details>


### [548] [Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2](https://arxiv.org/pdf/2505.01854)
*Yuwen Chen, Zafer Yildiz, Qihang Li, Yaqian Chen, Haoyu Dong, Hanxue Gu, Nicholas Konz, Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: SLM-SAM 2 improves medical image annotation by integrating short-term and long-term memory banks, outperforming SAM 2 in accuracy and reducing error propagation.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of medical images is labor-intensive. SAM 2's performance varies, especially in boundary regions, prompting the need for a better solution.

Method: Proposes SLM-SAM 2, which uses distinct short-term and long-term memory banks with separate attention modules to enhance segmentation accuracy.

Result: SLM-SAM 2 outperforms SAM 2, improving Dice Similarity Coefficient by 0.14 and 0.11 in different scenarios, and resists over-propagation.

Conclusion: SLM-SAM 2 advances automated medical image annotation, offering higher accuracy and robustness for segmentation model development.

Abstract: Manual annotation of volumetric medical images, such as magnetic resonance
imaging (MRI) and computed tomography (CT), is a labor-intensive and
time-consuming process. Recent advancements in foundation models for video
object segmentation, such as Segment Anything Model 2 (SAM 2), offer a
potential opportunity to significantly speed up the annotation process by
manually annotating one or a few slices and then propagating target masks
across the entire volume. However, the performance of SAM 2 in this context
varies. Our experiments show that relying on a single memory bank and attention
module is prone to error propagation, particularly at boundary regions where
the target is present in the previous slice but absent in the current one. To
address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel
architecture that integrates distinct short-term and long-term memory banks
with separate attention modules to improve segmentation accuracy. We evaluate
SLM-SAM 2 on three public datasets covering organs, bones, and muscles across
MRI and CT modalities. We show that the proposed method markedly outperforms
the default SAM 2, achieving average Dice Similarity Coefficient improvement of
0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for
the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger
resistance to over-propagation, making a notable step toward more accurate
automated annotation of medical images for segmentation model development.

</details>


### [549] [Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images](https://arxiv.org/pdf/2505.01884)
*Siddharth Kothari, Srinivasan Murali, Sankalp Kothari, Ujjwal Verma, Jaya Sreevalsan-Nair*

Main category: eess.IV

TL;DR: The paper studies the robustness of U-Net for inland water body segmentation in SAR images against manual annotation errors, simulated as adversarial attacks. Results show U-Net tolerates some corruption, emphasizing annotation quality's importance.


<details>
  <summary>Details</summary>
Motivation: Differentiating water bodies in SAR images is challenging due to complex geometry and manual annotation errors. The study aims to assess U-Net's robustness to such errors.

Method: Simulated manual annotation errors as adversarial attacks on U-Net, evaluating its performance under corrupted ground truth.

Result: U-Net tolerates a certain level of corruption before performance drops significantly, highlighting annotation quality's impact.

Conclusion: Manual annotation quality is crucial for segmentation model effectiveness. The work provides a dataset and adversarial examples for robust training.

Abstract: Inland water body segmentation from Synthetic Aperture Radar (SAR) images is
an important task needed for several applications, such as flood mapping. While
SAR sensors capture data in all-weather conditions as high-resolution images,
differentiating water and water-like surfaces from SAR images is not
straightforward. Inland water bodies, such as large river basins, have complex
geometry, which adds to the challenge of segmentation. U-Net is a widely used
deep learning model for land-water segmentation of SAR images. In practice,
manual annotation is often used to generate the corresponding water masks as
ground truth. Manual annotation of the images is prone to label noise owing to
data poisoning attacks, especially due to complex geometry. In this work, we
simulate manual errors in the form of adversarial attacks on the U-Net model
and study the robustness of the model to human errors in annotation. Our
results indicate that U-Net can tolerate a certain level of corruption before
its performance drops significantly. This finding highlights the crucial role
that the quality of manual annotations plays in determining the effectiveness
of the segmentation model. The code and the new dataset, along with adversarial
examples for robust training, are publicly available. (Github link -
https://github.com/GVCL/IWSeg-SAR-Poison.git)

</details>


### [550] [UNet-3D with Adaptive TverskyCE Loss for Pancreas Medical Image Segmentation](https://arxiv.org/pdf/2505.01951)
*Xubei Zhang, Mikhail Y. Shalaginov, Tingying Helen Zeng*

Main category: eess.IV

TL;DR: The paper proposes an adaptive TverskyCE loss for pancreas segmentation in CT scans, improving accuracy over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Pancreatic cancer diagnosis relies on CT scans, but pancreas segmentation is challenging due to its small size and obscured position. Current DL models need improvement.

Method: Combines Tversky loss with cross-entropy loss using learnable weights, enabling automatic adjustment of loss contribution during training. Evaluated on UNet-3D and Dilated UNet-3D.

Result: Achieved a DSC of 85.59%, with peak performance up to 95.24%, improving baseline by 9.47%.

Conclusion: The adaptive TverskyCE loss enhances pancreas segmentation accuracy, demonstrating significant improvement over existing methods.

Abstract: Pancreatic cancer, which has a low survival rate, is the most intractable one
among all cancers. Most diagnoses of this cancer heavily depend on abdominal
computed tomography (CT) scans. Therefore, pancreas segmentation is crucial but
challenging. Because of the obscure position of the pancreas, surrounded by
other large organs, and its small area, the pancreas has often been impeded and
difficult to detect. With these challenges , the segmentation results based on
Deep Learning (DL) models still need to be improved. In this research, we
propose a novel adaptive TverskyCE loss for DL model training, which combines
Tversky loss with cross-entropy loss using learnable weights. Our method
enables the model to adjust the loss contribution automatically and find the
best objective function during training. All experiments were conducted on the
National Institutes of Health (NIH) Pancreas-CT dataset. We evaluated the
adaptive TverskyCE loss on the UNet-3D and Dilated UNet-3D, and our method
achieved a Dice Similarity Coefficient (DSC) of 85.59%, with peak performance
up to 95.24%, and the score of 85.14%. DSC and the score were improved by 9.47%
and 8.98% respectively compared with the baseline UNet-3D with Tversky loss for
pancreas segmentation.
  Keywords: Pancreas segmentation, Tversky loss, Cross-entropy loss, UNet-3D,
Dilated UNet-3D

</details>


### [551] [Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework](https://arxiv.org/pdf/2505.02001)
*Vineesh Kumar Reddy Mondem*

Main category: eess.IV

TL;DR: HIRQM is a hybrid image quality metric combining statistical, multi-scale, and deep learning methods, outperforming traditional metrics like MSE and SSIM.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics often fail to reflect perceptual quality under complex distortions, necessitating a more comprehensive approach.

Method: HIRQM integrates three components: Probability Density Function, Multi-scale Feature Similarity, and Hierarchical Deep Image Features (VGG16), with dynamic weighting based on image characteristics.

Result: HIRQM achieves Pearson and Spearman correlations of 0.92 and 0.90 on TID2013 and LIVE datasets, excelling in handling noise, blur, and compression artifacts.

Conclusion: HIRQM is a flexible, unified metric that aligns better with human perception, making it valuable for image processing applications.

Abstract: Traditional image quality assessment metrics like Mean Squared Error and
Structural Similarity Index often fail to reflect perceptual quality under
complex distortions. We propose the Hybrid Image Resolution Quality Metric
(HIRQM), integrating statistical, multi-scale, and deep learning-based methods
for a comprehensive quality evaluation. HIRQM combines three components:
Probability Density Function for local pixel distribution analysis, Multi-scale
Feature Similarity for structural integrity across resolutions, and
Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic
alignment with human perception. A dynamic weighting mechanism adapts component
contributions based on image characteristics like brightness and variance,
enhancing flexibility across distortion types. Our contributions include a
unified metric and dynamic weighting for better perceptual alignment. Evaluated
on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations
of 0.92 and 0.90, outperforming traditional metrics. It excels in handling
noise, blur, and compression artifacts, making it valuable for image processing
applications like compression and restoration.

</details>


### [552] [EMulator: Rapid Estimation of Complex-valued Electric Fields using a U-Net Architecture](https://arxiv.org/pdf/2505.02095)
*Fatima Ahsan, Lorenzo Luzi, Richard G. Barainuk, Sameer A. Sheth, Wayne Goodman, Behnaam Aazhang*

Main category: eess.IV

TL;DR: EMulator, a U-Net-based model, accelerates electric field estimation in brain stimulation, achieving high accuracy and speed compared to traditional simulators.


<details>
  <summary>Details</summary>
Motivation: Traditional physics-based simulators for electric field estimation are computationally expensive, hindering optimization of brain stimulation parameters.

Method: Developed EMulator, a U-Net regression model, trained on electric fields from 43 antennas around 14 brain models to predict fields from segmented brain inputs.

Result: Achieved a complex correlation coefficient of 0.978 and computation time of 4.4 ms, ~1200x faster than COMSOL.

Conclusion: EMulator enables real-time electric field calculation, facilitating optimization of stimulation parameters via gradient descent.

Abstract: A common factor across electromagnetic methodologies of brain stimulation is
the optimization of essential dosimetry parameters, like amplitude, phase, and
location of one or more transducers, which controls the stimulation strength
and targeting precision. Since obtaining in-vivo measurements for the electric
field distribution inside the biological tissue is challenging, physics-based
simulators are used. However, these simulators are computationally expensive
and time-consuming, making repeated calculations of electric fields for
optimization purposes computationally prohibitive. To overcome this issue, we
developed EMulator, a U-Net architecture-based regression model, for fast and
robust complex electric field estimation. We trained EMulator using electric
fields generated by 43 antennas placed around 14 segmented human brain models.
Once trained, EMulator uses a segmented human brain model with an antenna
location as an input and outputs the corresponding electric field. A
representative result of our study is that, at 1.5 GHz, on the validation
dataset consisting of 6 subjects, we can estimate the electric field with the
magnitude of complex correlation coefficient of 0.978. Additionally, we could
calculate the electric field with a mean time of 4.4 ms. On average, this is at
least x1200 faster than the time required by state-of-the-art physics-based
simulator COMSOL. The significance of this work is that it shows the
possibility of real-time calculation of the electric field from the segmented
human head model and antenna location, making it possible to optimize the
amplitude, phase, and location of several different transducers with stochastic
gradient descent since our model is almost everywhere differentiable.

</details>


### [553] [CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images](https://arxiv.org/pdf/2505.02211)
*Peiqi Li, Yincheng Gao, Renxing Li, Haojie Yang, Yunyun Liu, Boji Liu, Jiahui Ni, Ying Zhang, Yulu Wu, Xiaowei Fang, Lehang Guo, Liping Sun, Jiangang Chen*

Main category: eess.IV

TL;DR: A multitask learning framework (CSASN) is proposed for rare thyroid carcinoma classification, combining EfficientNet and ViT with attention refinement, achieving superior performance under data imbalance.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous morphological features and data imbalance in ultrasound imaging make rare thyroid carcinoma classification challenging.

Method: Proposes CSASN, integrating EfficientNet for local spatial encoding and ViT for global semantic modeling, with channel-spatial attention refinement, residual multiscale classifier, and dynamically weighted loss.

Result: Outperforms single-stream CNN or Transformer models, balancing precision and recall, especially for rare subtypes like FTC and MTC.

Conclusion: CSASN offers a promising AI-assisted strategy for thyroid cancer diagnosis, addressing data imbalance and feature heterogeneity.

Abstract: Heterogeneous morphological features and data imbalance pose significant
challenges in rare thyroid carcinoma classification using ultrasound imaging.
To address this issue, we propose a novel multitask learning framework,
Channel-Spatial Attention Synergy Network (CSASN), which integrates a
dual-branch feature extractor - combining EfficientNet for local spatial
encoding and ViT for global semantic modeling, with a cascaded channel-spatial
attention refinement module. A residual multiscale classifier and dynamically
weighted loss function further enhance classification stability and accuracy.
Trained on a multicenter dataset comprising more than 2000 patients from four
clinical institutions, our framework leverages a residual multiscale classifier
and dynamically weighted loss function to enhance classification stability and
accuracy. Extensive ablation studies demonstrate that each module contributes
significantly to model performance, particularly in recognizing rare subtypes
such as FTC and MTC carcinomas. Experimental results show that CSASN
outperforms existing single-stream CNN or Transformer-based models, achieving a
superior balance between precision and recall under class-imbalanced
conditions. This framework provides a promising strategy for AI-assisted
thyroid cancer diagnosis.

</details>


### [554] [OASIS: Optimized Lightweight Autoencoder System for Distributed In-Sensor computing](https://arxiv.org/pdf/2505.02256)
*Chengwei Zhou, Sreetama Sarkar, Yuming Li, Arnab Sanyal, Gourav Datta*

Main category: eess.IV

TL;DR: The paper proposes a dual-branch autoencoder-based vision architecture for in-sensor computing to reduce data bandwidth and energy consumption while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: In-sensor computing addresses bandwidth bottlenecks in high-resolution, high-frame-rate video applications by integrating computation within the sensor. However, deploying advanced DNNs on limited compute and memory resources is challenging.

Method: A dual-branch autoencoder architecture is introduced, with a lightweight encoder on the logic chip and task-specific networks off-chip. The encoder is trained using a triple loss function for accuracy, compactness, and visual information preservation.

Result: The approach reduces output activation dimensionality by four orders of magnitude, cuts energy consumption by 2-4.5×, and achieves state-of-the-art accuracy with up to 22.7 TOPS/W efficiency.

Conclusion: The proposed architecture effectively balances bandwidth reduction and energy efficiency while maintaining high accuracy, making it suitable for machine vision applications like AR/VR and smart home systems.

Abstract: In-sensor computing, which integrates computation directly within the sensor,
has emerged as a promising paradigm for machine vision applications such as
AR/VR and smart home systems. By processing data on-chip before transmission,
it alleviates the bandwidth bottleneck caused by high-resolution,
high-frame-rate image transmission, particularly in video applications. We
envision a system architecture that integrates a CMOS image sensor (CIS) with a
logic chip via advanced packaging, where the logic chip processes early-stage
deep neural network (DNN) layers. However, its limited compute and memory make
deploying advanced DNNs challenging. A simple solution is to split the model,
executing the first part on the logic chip and the rest off-chip. However,
modern DNNs require multiple layers before dimensionality reduction, limiting
their ability to achieve the primary goal of in-sensor computing: minimizing
data bandwidth. To address this, we propose a dual-branch autoencoder-based
vision architecture that deploys a lightweight encoder on the logic chip while
the task-specific network runs off-chip. The encoder is trained using a triple
loss function: (1) task-specific loss to optimize accuracy, (2) entropy loss to
enforce compact and compressible representations, and (3) reconstruction loss
(mean-square error) to preserve essential visual information. This design
enables a four-order-of-magnitude reduction in output activation dimensionality
compared to input images, resulting in a $2{-}4.5\times$ decrease in energy
consumption, as validated by our hardware-backed semi-analytical energy models.
We evaluate our approach on CNN and ViT-based models across applications in
smart home and augmented reality domains, achieving state-of-the-art accuracy
with energy efficiency of up to 22.7 TOPS/W.

</details>


### [555] [An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation](https://arxiv.org/pdf/2505.02385)
*Lei Xie, Huajun Zhou, Junxiong Huang, Jiahao Huang, Qingrun Zeng, Jianzhong He, Jiawei Zhang, Baohua Fan, Mingchu Li, Guoqiang Xie, Hao Chen, Yuanjing Feng*

Main category: eess.IV

TL;DR: CNTSeg-v2 is a novel arbitrary-modal fusion network for cranial nerve tract segmentation, improving accuracy by leveraging T1w images as the primary modality and integrating auxiliary modalities effectively.


<details>
  <summary>Details</summary>
Motivation: Current multimodal CNs tract segmentation methods require complete multimodal data, which is often impractical in clinical settings due to equipment, privacy, or workflow constraints.

Method: Proposes CNTSeg-v2, using T1w images as the primary modality to supervise auxiliary modalities via an Arbitrary-Modal Collaboration Module (ACM) and a Deep Distance-guided Multi-stage (DDM) decoder for error correction.

Result: Achieves state-of-the-art segmentation performance on HCP and MDM datasets, outperforming existing methods.

Conclusion: CNTSeg-v2 offers a practical and accurate solution for CNs tract segmentation with flexible modality combinations, addressing clinical data limitations.

Abstract: The segmentation of cranial nerves (CNs) tract provides a valuable
quantitative tool for the analysis of the morphology and trajectory of
individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which
combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have
achieved promising segmentation performance. However, it is laborious or even
infeasible to collect complete multimodal data in clinical practice due to
limitations in equipment, user privacy, and working conditions. In this work,
we propose a novel arbitrary-modal fusion network for volumetric CNs tract
segmentation, called CNTSeg-v2, which trains one model to handle different
combinations of available modalities. Instead of directly combining all the
modalities, we select T1-weighted (T1w) images as the primary modality due to
its simplicity in data acquisition and contribution most to the results, which
supervises the information selection of other auxiliary modalities. Our model
encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to
effectively extract informative features from other auxiliary modalities,
guided by the supervision of T1w images. Meanwhile, we construct a Deep
Distance-guided Multi-stage (DDM) decoder to correct small errors and
discontinuities through signed distance maps to improve segmentation accuracy.
We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the
clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental
results show that our CNTSeg-v2 achieves state-of-the-art segmentation
performance, outperforming all competing methods.

</details>


### [556] [Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch](https://arxiv.org/pdf/2505.02396)
*Kennard Norbert Sudiardjo, Islam Nur Alam, Wilson Wijaya, Lili Ayu Wulandhari*

Main category: eess.IV

TL;DR: The paper explores CNN models (MobileNetV2 and a scratch-built model) for pneumonia diagnosis, highlighting trade-offs between stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Pneumonia diagnosis is often uncertain due to atypical presentations and diagnostic tool limitations, prompting the need for improved methods.

Method: Used MobileNetV2 (pre-trained) and a scratch-built CNN with ResNet101V2 architecture, trained on Kaggle datasets.

Result: MobileNetV2 showed stability with 84.87% training accuracy (later 78.95%), while the scratch model had higher accuracy but instability and overfitting.

Conclusion: MobileNetV2 offers stability, while the scratch model provides higher accuracy; ResNet101V2 balances both.

Abstract: Pneumonia Diagnosis, though it is crucial for an effective treatment, it can
be hampered by uncertainty. This uncertainty starts to arise due to some
factors like atypical presentations, limitations of diagnostic tools such as
chest X-rays, and the presence of co-existing respiratory conditions. This
research proposes one of the supervised learning methods, CNN. Using
MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using
Keras API as the built from scratch model, for identifying lung diseases
especially pneumonia. The datasets used in this research were obtained from the
website through Kaggle. The result shows that by implementing CNN MobileNetV2
and CNN from scratch the result is promising. While validating data,
MobileNetV2 performs with stability and minimal overfitting, while the training
accuracy increased to 84.87% later it slightly decreased to 78.95%, with
increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is
more stable. Although it takes more time to train each epoch. Meanwhile, after
the 10th epoch, the Scratch model displayed more instability and overfitting
despite having higher validation accuracy, training accuracy decreased
significantly to 78.12% and the validation loss increased from 0.5698 to
1.1809. With these results, ResNet101V2 offers stability, and the Scratch model
offers high accuracy.

</details>


### [557] [Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems](https://arxiv.org/pdf/2505.02470)
*Tal Oved, Beatrice Lena, Chloé F. Najac, Sheng Shen, Matthew S. Rosen, Andrew Webb, Efrat Shimron*

Main category: eess.IV

TL;DR: A deep learning method (ViT-Fuser) enhances low-field MRI scans using features from past high-field scans, improving image quality and reducing costs.


<details>
  <summary>Details</summary>
Motivation: High costs and limited accessibility of high-field MRI hinder longitudinal care, while low-field MRI suffers from poor image quality and long scan times.

Method: ViT-Fuser, a feature-fusion vision transformer, leverages past high-field MRI scans to enhance low-field scans, requiring only one prior scan regardless of vendor or field strength.

Result: ViT-Fuser outperforms state-of-the-art methods, providing diagnostic-quality images from accelerated low-field scans, even with out-of-distribution data.

Conclusion: The framework enables affordable, rapid, and high-quality MRI imaging, broadening healthcare accessibility.

Abstract: Magnetic resonance imaging (MRI) offers superb-quality images, but its
accessibility is limited by high costs, posing challenges for patients
requiring longitudinal care. Low-field MRI provides affordable imaging with
low-cost devices but is hindered by long scans and degraded image quality,
including low signal-to-noise ratio (SNR) and tissue contrast. We propose a
novel healthcare paradigm: using deep learning to extract personalized features
from past standard high-field MRI scans and harnessing them to enable
accelerated, enhanced-quality follow-up scans with low-cost systems. To
overcome the SNR and contrast differences, we introduce ViT-Fuser, a
feature-fusion vision transformer that learns features from past scans, e.g.
those stored in standard DICOM CDs. We show that \textit{a single prior scan is
sufficient}, and this scan can come from various MRI vendors, field strengths,
and pulse sequences. Experiments with four datasets, including glioblastoma
data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that
ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality
images from accelerated low-field scans, with robustness to out-of-distribution
data. Our freely available framework thus enables rapid, diagnostic-quality,
low-cost imaging for wide healthcare applications.

</details>


### [558] [Lane-Wise Highway Anomaly Detection](https://arxiv.org/pdf/2505.02613)
*Mei Qiu, William Lorenz Reindl, Yaobin Chen, Stanley Chien, Shu Hu*

Main category: eess.IV

TL;DR: A scalable, interpretable framework for lane-wise highway traffic anomaly detection using AI-powered vision models and multi-modal data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional sensor-dependent methods by leveraging vision models for cost-effective, hardware-free anomaly detection in highway traffic.

Method: Uses AI-powered vision models to extract lane-specific features (vehicle count, occupancy, truck percentage) and integrates deep learning, rule-based logic, and machine learning for anomaly detection.

Result: Outperforms state-of-the-art methods in precision, recall, and F1-score, validated on a novel dataset of 73,139 lane-wise samples.

Conclusion: Provides a cost-effective, scalable solution for real-world intelligent transportation systems, enhancing robustness and precision in anomaly detection.

Abstract: This paper proposes a scalable and interpretable framework for lane-wise
highway traffic anomaly detection, leveraging multi-modal time series data
extracted from surveillance cameras. Unlike traditional sensor-dependent
methods, our approach uses AI-powered vision models to extract lane-specific
features, including vehicle count, occupancy, and truck percentage, without
relying on costly hardware or complex road modeling. We introduce a novel
dataset containing 73,139 lane-wise samples, annotated with four classes of
expert-validated anomalies: three traffic-related anomalies (lane blockage and
recovery, foreign object intrusion, and sustained congestion) and one
sensor-related anomaly (camera angle shift). Our multi-branch detection system
integrates deep learning, rule-based logic, and machine learning to improve
robustness and precision. Extensive experiments demonstrate that our framework
outperforms state-of-the-art methods in precision, recall, and F1-score,
providing a cost-effective and scalable solution for real-world intelligent
transportation systems.

</details>


### [559] [DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction](https://arxiv.org/pdf/2505.02628)
*Yiqun Lin, Hualiang Wang, Jixiang Chen, Jiewen Yang, Jiarong Guo, Xiaomeng Li*

Main category: eess.IV

TL;DR: DeepSparse is a foundation model for sparse-view CBCT reconstruction, combining 2D and 3D features via DiCE and pretraining with HyViP, achieving superior image quality with reduced radiation.


<details>
  <summary>Details</summary>
Motivation: High radiation in CBCT poses risks; sparse-view reconstruction reduces exposure but faces computational and generalizability challenges.

Method: Proposes DeepSparse with DiCE for feature integration and HyViP for pretraining, followed by two-step finetuning.

Result: DeepSparse outperforms state-of-the-art methods in reconstruction quality.

Conclusion: DeepSparse enables safer, more efficient CBCT imaging with high-quality sparse-view reconstruction.

Abstract: Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in
the medical field, while the high radiation exposure required for high-quality
imaging raises significant concerns, particularly for vulnerable populations.
Sparse-view reconstruction reduces radiation by using fewer X-ray projections
while maintaining image quality, yet existing methods face challenges such as
high computational demands and poor generalizability to different datasets. To
overcome these limitations, we propose DeepSparse, the first foundation model
for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional
Cross-Scale Embedding), a novel network that integrates multi-view 2D features
and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View
Sampling Pretraining) framework, which pretrains the model on large datasets
with both sparse-view and dense-view projections, and a two-step finetuning
strategy to adapt and refine the model for new datasets. Extensive experiments
and ablation studies demonstrate that our proposed DeepSparse achieves superior
reconstruction quality compared to state-of-the-art methods, paving the way for
safer and more efficient CBCT imaging.

</details>


### [560] [Multi-View Learning with Context-Guided Receptance for Image Denoising](https://arxiv.org/pdf/2505.02705)
*Binghong Chen, Tingting Chai, Wei Jiang, Yuanrong Xu, Guanglu Zhou, Xiangqian Wu*

Main category: eess.IV

TL;DR: Proposes a Context-guided Receptance Weighted Key-Value (M) model for image denoising, combining multi-view feature integration, CTS paradigm, FMix module, and BiWKV mechanism, achieving better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex noise patterns and high computational costs, especially with Transformer-based models.

Method: Introduces CTS for spatial dependencies, FMix for frequency-domain noise isolation, and BiWKV for efficient sequence modeling.

Result: Outperforms state-of-the-art methods, reduces inference time by 40%, and restores fine details effectively.

Conclusion: The proposed model offers a robust and efficient solution for real-world image denoising.

Abstract: Image denoising is essential in low-level vision applications such as
photography and automated driving. Existing methods struggle with
distinguishing complex noise patterns in real-world scenes and consume
significant computational resources due to reliance on Transformer-based
models. In this work, the Context-guided Receptance Weighted Key-Value (\M)
model is proposed, combining enhanced multi-view feature integration with
efficient sequence modeling. Our approach introduces the Context-guided Token
Shift (CTS) paradigm, which effectively captures local spatial dependencies and
enhance the model's ability to model real-world noise distributions.
Additionally, the Frequency Mix (FMix) module extracting frequency-domain
features is designed to isolate noise in high-frequency spectra, and is
integrated with spatial representations through a multi-view learning process.
To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is
adopted, enabling full pixel-sequence interaction with linear complexity while
overcoming the causal selection constraints. The model is validated on multiple
real-world image denoising datasets, outperforming the existing
state-of-the-art methods quantitatively and reducing inference time up to 40\%.
Qualitative results further demonstrate the ability of our model to restore
fine details in various scenes.

</details>


### [561] [The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement](https://arxiv.org/pdf/2202.09738)
*Danni Huang, Lingyu Zhu, Zihao Lin, Hanwei Zhu, Shiqi Wang, Baoliang Chen*

Main category: eess.IV

TL;DR: The paper proposes a loop enhancement framework for optimizing low-light image enhancement based on perceptual quality, introduces a large-scale database (QUOTE-LOL) for quality assessment, and iteratively improves enhancement models.


<details>
  <summary>Details</summary>
Motivation: The gap between low-light image enhancement and quality assessment needs addressing to optimize visual quality.

Method: A loop framework combining enhancement and optimization, supported by the QUOTE-LOL database for quality assessment.

Result: The framework successfully enhances low-light images iteratively, validated across various scenes.

Conclusion: The proposed scheme bridges enhancement and assessment, optimizing perceptual quality effectively.

Abstract: There is an increasing consensus that the design and optimization of low
light image enhancement methods need to be fully driven by perceptual quality.
With numerous approaches proposed to enhance low-light images, much less work
has been dedicated to quality assessment and quality optimization of low-light
enhancement. In this paper, to close the gap between enhancement and
assessment, we propose a loop enhancement framework that produces a clear
picture of how the enhancement of low-light images could be optimized towards
better visual quality. In particular, we create a large-scale database for
QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as
the foundation in studying and developing objective quality assessment
measures. The objective quality assessment measure plays a critical bridging
role between visual quality and enhancement and is further incorporated in the
optimization in learning the enhancement model towards perceptual optimally.
Finally, we iteratively perform the enhancement and optimization tasks,
enhancing the low-light images continuously. The superiority of the proposed
scheme is validated based on various low-light scenes.

</details>


### [562] [GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology](https://arxiv.org/pdf/2501.04206)
*Raktim Kumar Mondol, Ewan K. A. Millar, Peter H. Graham, Lois Browne, Arcot Sowmya, Erik Meijering*

Main category: eess.IV

TL;DR: GRAPHITE is a post-hoc explainable AI framework for breast cancer TMA analysis, outperforming traditional XAI methods with high performance metrics and clinical utility.


<details>
  <summary>Details</summary>
Motivation: Enhancing interpretability and clinical trustworthiness of deep learning models in cancer diagnosis, addressing the black-box nature of such models.

Method: GRAPHITE uses a multiscale approach with hierarchical graphs and graph attention networks (GAT) with scalewise attention (SAN) for breast cancer tissue analysis.

Result: Achieved mAP of 0.56, AUROC of 0.94, ThR of 0.70, and highest AUDC of 4.17e+5, indicating robust performance and clinical utility.

Conclusion: GRAPHITE is a clinically valuable tool for computational pathology, providing interpretable visualizations aligned with pathologists' reasoning.

Abstract: Explainable AI (XAI) in medical histopathology is essential for enhancing the
interpretability and clinical trustworthiness of deep learning models in cancer
diagnosis. However, the black-box nature of these models often limits their
clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue
Examination), a post-hoc explainable framework designed for breast cancer
tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach,
extracting patches at various magnification levels, constructing an
hierarchical graph, and utilising graph attention networks (GAT) with scalewise
attention (SAN) to capture scale-dependent features. We trained the model on
140 tumour TMA cores and four benign whole slide images from which 140 benign
samples were created, and tested it on 53 pathologist-annotated TMA samples.
GRAPHITE outperformed traditional XAI methods, achieving a mean average
precision (mAP) of 0.56, an area under the receiver operating characteristic
curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating
that the model maintains high performance across a wide range of thresholds. In
clinical utility, GRAPHITE achieved the highest area under the decision curve
(AUDC) of 4.17e+5, indicating reliable decision support across thresholds.
These results highlight GRAPHITE's potential as a clinically valuable tool in
computational pathology, providing interpretable visualisations that align with
the pathologists' diagnostic reasoning and support precision medicine.

</details>


### [563] [Make Both Ends Meet: A Synergistic Optimization Infrared Small Target Detection with Streamlined Computational Overhead](https://arxiv.org/pdf/2504.21581)
*Yuxin Jing, Yuchen Zheng, Jufeng Zhao, Guangmang Cui, Tianpei Zhang*

Main category: eess.IV

TL;DR: LE-IRSTD is a lightweight, efficient framework for infrared small target detection, improving accuracy and reducing computational overhead by innovating backbone and convolution structures.


<details>
  <summary>Details</summary>
Motivation: Address blurred target boundaries and excessive computational overhead in existing IRSTD methods.

Method: Uses MBConvblock and BSblock in YOLOv8n backbone, introduces AVCStem with VKConv, and employs GSConv for feature shuffling.

Result: Achieves superior accuracy and lightweight performance compared to state-of-the-art methods.

Conclusion: LE-IRSTD effectively balances efficiency and performance for IRSTD tasks.

Abstract: Infrared small target detection(IRSTD) is widely recognized as a challenging
task due to the inherent limitations of infrared imaging, including low
signal-to-noise ratios, lack of texture details, and complex background
interference. While most existing methods model IRSTD as a semantic
segmentation task, but they suffer from two critical drawbacks: (1)blurred
target boundaries caused by long-distance imaging dispersion; and (2) excessive
computational overhead due to indiscriminate feature stackin. To address these
issues, we propose the Lightweight Efficiency Infrared Small Target Detection
(LE-IRSTD), a lightweight and efficient framework based on YOLOv8n, with
following key innovations. Firstly, we identify that the multiple bottleneck
structures within the C2f component of the YOLOv8-n backbone contribute to an
increased computational burden. Therefore, we implement the Mobile Inverted
Bottleneck Convolution block (MBConvblock) and Bottleneck Structure block
(BSblock) in the backbone, effectively balancing the trade-off between
computational efficiency and the extraction of deep semantic information.
Secondly, we introduce the Attention-based Variable Convolution Stem (AVCStem)
structure, substituting the final convolution with Variable Kernel Convolution
(VKConv), which allows for adaptive convolutional kernels that can transform
into various shapes, facilitating the receptive field for the extraction of
targets. Finally, we employ Global Shuffle Convolution (GSConv) to shuffle the
channel dimension features obtained from different convolutional approaches,
thereby enhancing the robustness and generalization capabilities of our method.
Experimental results demonstrate that our LE-IRSTD method achieves compelling
results in both accuracy and lightweight performance, outperforming several
state-of-the-art deep learning methods.

</details>


### [564] [AI-Driven Segmentation and Analysis of Microbial Cells](https://arxiv.org/pdf/2505.00578)
*Shuang Zhang, Carleton Coffin, Karyn L. Rogers, Catherine Ann Royer, Ge Wang*

Main category: eess.IV

TL;DR: An AI-driven image analysis system for microbial cell segmentation and feature analysis, improving accuracy with denoising and post-processing.


<details>
  <summary>Details</summary>
Motivation: To study microbial growth and metabolism in harsh environments for biotechnology applications.

Method: Four-module system: denoising, SAM for segmentation, post-processing, and quantitative feature extraction.

Result: Improved segmentation accuracy; automated, high-accuracy cellular parameter calculation.

Conclusion: Enables efficient analysis of microbial adaptations via fluorescence microscopy.

Abstract: Studying the growth and metabolism of microbes provides critical insights
into their evolutionary adaptations to harsh environments, which are essential
for microbial research and biotechnology applications. In this study, we
developed an AI-driven image analysis system to efficiently segment individual
cells and quantitatively analyze key cellular features. This system is
comprised of four main modules. First, a denoising algorithm enhances contrast
and suppresses noise while preserving fine cellular details. Second, the
Segment Anything Model (SAM) enables accurate, zero-shot segmentation of cells
without additional training. Third, post-processing is applied to refine
segmentation results by removing over-segmented masks. Finally, quantitative
analysis algorithms extract essential cellular features, including average
intensity, length, width, and volume. The results show that denoising and
post-processing significantly improved the segmentation accuracy of SAM in this
new domain. Without human annotations, the AI-driven pipeline automatically and
efficiently outlines cellular boundaries, indexes them, and calculates key
cellular parameters with high accuracy. This framework will enable efficient
and automated quantitative analysis of high-resolution fluorescence microscopy
images to advance research into microbial adaptations to grow and metabolism
that allow extremophiles to thrive in their harsh habitats.

</details>


### [565] [Regression s all you need for medical image translation](https://arxiv.org/pdf/2505.02048)
*Sebastian Rassmann, David Kügler, Christian Ewert, Martin Reuter*

Main category: eess.IV

TL;DR: YODA, a 2.5D diffusion-based framework for medical image translation, combines diffusion and regression to produce accurate outputs, outperforming GANs and DMs while challenging their assumed advantages.


<details>
  <summary>Details</summary>
Motivation: Enhancing medical datasets with synthetic images requires high anatomical accuracy, which GANs and DMs often fail to deliver due to noise and hallucination.

Method: YODA integrates diffusion and regression paradigms and introduces ExpA-sampling to suppress noise, tested on multi-modal datasets.

Result: YODA outperforms GANs and DMs, generating images interchangeable or superior to real acquisitions for downstream tasks.

Conclusion: YODA challenges DM advantages in MIT, enabling practical medical imaging applications.

Abstract: The acquisition of information-rich images within a limited time budget is
crucial in medical imaging. Medical image translation (MIT) can help enhance
and supplement existing datasets by generating synthetic images from acquired
data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have
achieved remarkable success in natural image generation, their benefits -
creativity and image realism - do not necessarily transfer to medical
applications where highly accurate anatomical information is required. In fact,
the imitation of acquisition noise or content hallucination hinder clinical
utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel
2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and
regression paradigms to produce realistic or noise-free outputs. Furthermore,
we propose Expectation-Approximation (ExpA) DM sampling, which draws
inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise
and, thus, eliminates noise from biasing the evaluation of image quality.
Through extensive experiments on four diverse multi-modal datasets - comprising
multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and
regression sampling yield similar results in practice. As such, the
computational overhead of diffusion sampling does not provide systematic
benefits in medical information translation. Building on these insights, we
demonstrate that YODA outperforms several state-of-the-art GAN and DM methods.
Notably, YODA-generated images are shown to be interchangeable with, or even
superior to, physical acquisitions for several downstream tasks. Our findings
challenge the presumed advantages of DMs in MIT and pave the way for the
practical application of MIT in medical imaging.

</details>


### [566] [RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction](https://arxiv.org/pdf/2505.02529)
*Aiman Farooq, Azad Singh, Deepak Mishra, Santanu Chaudhury*

Main category: eess.IV

TL;DR: RobSurv, a robust deep-learning framework, improves cancer survival prediction by leveraging vector quantization and a dual-path architecture for noise-resistant multi-modal feature learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of noise and protocol variations in multi-modal medical imaging for cancer survival prediction, which limits clinical applicability.

Method: Introduces RobSurv with a dual-path architecture: one for noise-resistant discrete codebook mapping and another for continuous feature processing, integrated via patch-wise fusion and Transformer-based global context.

Result: Achieves superior performance (concordance indices of 0.771, 0.742, 0.734) and robustness under noise (3.8-4.5% degradation vs. 8-12% in baselines).

Conclusion: RobSurv is a reliable solution for clinical prognosis, enhancing treatment planning and patient care across diverse cancer types and imaging protocols.

Abstract: Cancer survival prediction using multi-modal medical imaging presents a
critical challenge in oncology, mainly due to the vulnerability of deep
learning models to noise and protocol variations across imaging centers.
Current approaches struggle to extract consistent features from heterogeneous
CT and PET images, limiting their clinical applicability. We address these
challenges by introducing RobSurv, a robust deep-learning framework that
leverages vector quantization for resilient multi-modal feature learning. The
key innovation of our approach lies in its dual-path architecture: one path
maps continuous imaging features to learned discrete codebooks for
noise-resistant representation, while the parallel path preserves fine-grained
details through continuous feature processing. This dual representation is
integrated through a novel patch-wise fusion mechanism that maintains local
spatial relationships while capturing global context via Transformer-based
processing. In extensive evaluations across three diverse datasets (HECKTOR,
H\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,
achieving concordance index of 0.771, 0.742, and 0.734 respectively -
significantly outperforming existing methods. Most notably, our model maintains
robust performance even under severe noise conditions, with performance
degradation of only 3.8-4.5\% compared to 8-12\% in baseline methods. These
results, combined with strong generalization across different cancer types and
imaging protocols, establish RobSurv as a promising solution for reliable
clinical prognosis that can enhance treatment planning and patient care.

</details>


### [567] [Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data](https://arxiv.org/pdf/2505.02677)
*Saeed Shurrab, Aadim Nepal, Terrence J. Lee-St. John, Nicola G. Ghazi, Bartlomiej Piechowski-Jozwiak, Farah E. Shamout*

Main category: eess.IV

TL;DR: The paper proposes a multimodal deep neural network using retinal images and clinical data for stroke detection and risk prediction, showing improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Stroke is a major health issue, and current diagnostic methods rely on expensive imaging. Retinal imaging offers a cost-effective alternative due to shared pathways with the brain.

Method: A multimodal deep neural network processes OCT and infrared retinal scans alongside clinical data. Pretrained with self-supervised learning on 37k scans, then fine-tuned on labeled data.

Result: Achieved 5% AUROC improvement over image-only baselines and 8% over state-of-the-art models, demonstrating predictive ability for stroke effects and risk.

Conclusion: Retinal imaging has potential for identifying high-risk stroke patients and improving outcomes.

Abstract: Stroke is a major public health problem, affecting millions worldwide. Deep
learning has recently demonstrated promise for enhancing the diagnosis and risk
prediction of stroke. However, existing methods rely on costly medical imaging
modalities, such as computed tomography. Recent studies suggest that retinal
imaging could offer a cost-effective alternative for cerebrovascular health
assessment due to the shared clinical pathways between the retina and the
brain. Hence, this study explores the impact of leveraging retinal images and
clinical data for stroke detection and risk prediction. We propose a multimodal
deep neural network that processes Optical Coherence Tomography (OCT) and
infrared reflectance retinal scans, combined with clinical data, such as
demographics, vital signs, and diagnosis codes. We pretrained our model using a
self-supervised learning framework using a real-world dataset consisting of
$37$ k scans, and then fine-tuned and evaluated the model using a smaller
labeled subset. Our empirical findings establish the predictive ability of the
considered modalities in detecting lasting effects in the retina associated
with acute stroke and forecasting future risk within a specific time horizon.
The experimental results demonstrate the effectiveness of our proposed
framework by achieving $5$\% AUROC improvement as compared to the unimodal
image-only baseline, and $8$\% improvement compared to an existing
state-of-the-art foundation model. In conclusion, our study highlights the
potential of retinal imaging in identifying high-risk patients and improving
long-term outcomes.

</details>


### [568] [Platelet enumeration in dense aggregates](https://arxiv.org/pdf/2505.02751)
*H. Martin Gillis, Yogeshwar Shendye, Paul Hollensen, Alan Fine, Thomas Trappenberg*

Main category: eess.IV

TL;DR: The paper explores improving platelet identification in blood samples using CNNs, addressing challenges like size variability and aggregation by optimizing convolutional kernels and class designations.


<details>
  <summary>Details</summary>
Motivation: Accurate platelet identification is crucial but challenging due to size variability and aggregation. Existing methods like class-weighted loss functions are insufficient.

Method: Used semantic segmentation with U-Net architectures, assigned separate classes for single platelets and aggregates, and evaluated counting methods (pixel area vs. connected component analysis).

Result: Proposed method improved platelet identification, showing pixel area-based counting overestimates counts.

Conclusion: Optimizing convolutional operations and class designations enhances platelet identification accuracy.

Abstract: Identifying and counting blood components such as red blood cells, various
types of white blood cells, and platelets is a critical task for healthcare
practitioners. Deep learning approaches, particularly convolutional neural
networks (CNNs) using supervised learning strategies, have shown considerable
success for such tasks. However, CNN based architectures such as U-Net, often
struggles to accurately identify platelets due to their sizes and high
variability of features. To address these challenges, researchers have commonly
employed strategies such as class weighted loss functions, which have
demonstrated some success. However, this does not address the more significant
challenge of platelet variability in size and tendency to form aggregates and
associations with other blood components. In this study, we explored an
alternative approach by investigating the role of convolutional kernels in
mitigating these issues. We also assigned separate classes to singular
platelets and platelet aggregates and performed semantic segmentation using
various U-Net architectures for identifying platelets. We then evaluated and
compared two common methods (pixel area method and connected component
analysis) for counting platelets and proposed an alternative approach
specialized for single platelets and platelet aggregates. Our experiments
provided results that showed significant improvements in the identification of
platelets, highlighting the importance of optimizing convolutional operations
and class designations. We show that the common practice of pixel area-based
counting often over estimate platelet counts, whereas the proposed method
presented in this work offers significant improvements. We discuss in detail
about these methods from segmentation masks.

</details>
