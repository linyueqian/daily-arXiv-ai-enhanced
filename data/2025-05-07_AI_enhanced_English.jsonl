{"id": "2505.02847", "pdf": "https://arxiv.org/pdf/2505.02847", "abs": "https://arxiv.org/abs/2505.02847", "authors": ["Bang Zhang", "Ruotian Ma", "Qingxuan Jiang", "Peisong Wang", "Jiaqi Chen", "Zheng Xie", "Xingyu Chen", "Yue Wang", "Fanghua Ye", "Jian Li", "Yifan Yang", "Zhaopeng Tu", "Xiaolong Li"], "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Assessing how well a large language model (LLM) understands human, rather\nthan merely text, remains an open challenge. To bridge the gap, we introduce\nSentient Agent as a Judge (SAGE), an automated evaluation framework that\nmeasures an LLM's higher-order social cognition. SAGE instantiates a Sentient\nAgent that simulates human-like emotional changes and inner thoughts during\ninteraction, providing a more realistic evaluation of the tested model in\nmulti-turn conversations. At every turn, the agent reasons about (i) how its\nemotion changes, (ii) how it feels, and (iii) how it should reply, yielding a\nnumerical emotion trajectory and interpretable inner thoughts. Experiments on\n100 supportive-dialogue scenarios show that the final Sentient emotion score\ncorrelates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings\nand utterance-level empathy metrics, validating psychological fidelity. We also\nbuild a public Sentient Leaderboard covering 18 commercial and open-source\nmodels that uncovers substantial gaps (up to 4x) between frontier systems\n(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in\nconventional leaderboards (e.g., Arena). SAGE thus provides a principled,\nscalable and interpretable tool for tracking progress toward genuinely\nempathetic and socially adept language agents.", "AI": {"tldr": "SAGE is an automated framework evaluating LLMs' social cognition by simulating human-like emotions and thoughts, validated by strong correlations with psychological metrics.", "motivation": "To address the challenge of assessing LLMs' human-like understanding beyond text, focusing on social cognition.", "method": "SAGE uses a Sentient Agent to simulate emotional changes and inner thoughts during multi-turn conversations, generating emotion scores and interpretable reasoning.", "result": "Experiments show strong correlation with psychological metrics (BLRI, empathy), and a leaderboard reveals gaps between frontier and baseline models.", "conclusion": "SAGE offers a scalable, interpretable tool for advancing empathetic and socially adept LLMs."}}
{"id": "2505.02850", "pdf": "https://arxiv.org/pdf/2505.02850", "abs": "https://arxiv.org/abs/2505.02850", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Diksha Seth", "Ananya Thakur", "Deepak Subramani"], "title": "Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB"], "comment": null, "summary": "Generating high-quality MCQs, especially those targeting diverse cognitive\nlevels and incorporating common misconceptions into distractor design, is\ntime-consuming and expertise-intensive, making manual creation impractical at\nscale. Current automated approaches typically generate questions at lower\ncognitive levels and fail to incorporate domain-specific misconceptions. This\npaper presents a hierarchical concept map-based framework that provides\nstructured knowledge to guide LLMs in generating MCQs with distractors. We\nchose high-school physics as our test domain and began by developing a\nhierarchical concept map covering major Physics topics and their\ninterconnections with an efficient database design. Next, through an automated\npipeline, topic-relevant sections of these concept maps are retrieved to serve\nas a structured context for the LLM to generate questions and distractors that\nspecifically target common misconceptions. Lastly, an automated validation is\ncompleted to ensure that the generated MCQs meet the requirements provided. We\nevaluate our framework against two baseline approaches: a base LLM and a\nRAG-based generation. We conducted expert evaluations and student assessments\nof the generated MCQs. Expert evaluation shows that our method significantly\noutperforms the baseline approaches, achieving a success rate of 75.20% in\nmeeting all quality criteria compared to approximately 37% for both baseline\nmethods. Student assessment data reveal that our concept map-driven approach\nachieved a significantly lower guess success rate of 28.05% compared to 37.10%\nfor the baselines, indicating a more effective assessment of conceptual\nunderstanding. The results demonstrate that our concept map-based approach\nenables robust assessment across cognitive levels and instant identification of\nconceptual gaps, facilitating faster feedback loops and targeted interventions\nat scale.", "AI": {"tldr": "A hierarchical concept map-based framework guides LLMs to generate high-quality MCQs targeting cognitive levels and misconceptions, outperforming baseline methods in expert and student evaluations.", "motivation": "Manual MCQ generation is time-consuming and lacks scalability, while current automated methods fail to address higher cognitive levels and domain-specific misconceptions.", "method": "Developed a hierarchical concept map for physics, used it to guide LLMs in generating MCQs with distractors, and validated the output automatically.", "result": "Achieved 75.20% success in quality criteria (vs. 37% for baselines) and a lower student guess rate (28.05% vs. 37.10%).", "conclusion": "The concept map-based approach enables robust assessment, identifies conceptual gaps, and supports scalable feedback and interventions."}}
{"id": "2505.02851", "pdf": "https://arxiv.org/pdf/2505.02851", "abs": "https://arxiv.org/abs/2505.02851", "authors": ["Franklin Zhang", "Sonya Zhang", "Alon Halevy"], "title": "30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; H.3.1; H.3.3"], "comment": "8 pages (main content), 4 figures. Submitted to ACL BEA2025", "summary": "In this paper, we present 30 Day Me, a habit formation application that\nleverages Large Language Models (LLMs) to help users break down their goals\ninto manageable, actionable steps and track their progress. Central to the app\nis the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced\nfrom over 15K webpages, and enables runtime search of challenge ideas aligned\nwith user-defined goals. We showcase how LLMs can be harnessed to rapidly\nconstruct domain specific content corpora for behavioral and educational\npurposes, and propose a practical pipeline that incorporates effective LLM\nenhanced approaches for content generation and semantic deduplication.", "AI": {"tldr": "30 Day Me is a habit formation app using LLMs to break goals into steps and track progress, featuring the 30DAYGEN system for generating and searching challenges.", "motivation": "To demonstrate how LLMs can create domain-specific content for behavioral and educational purposes, improving habit formation.", "method": "Developed the 30DAYGEN system to generate and search 30-day challenges from web content, using LLMs for content generation and semantic deduplication.", "result": "Produced 3,531 unique challenges from 15K webpages, enabling goal-aligned runtime search.", "conclusion": "LLMs effectively support rapid content creation and deduplication for habit formation apps."}}
{"id": "2505.02854", "pdf": "https://arxiv.org/pdf/2505.02854", "abs": "https://arxiv.org/abs/2505.02854", "authors": ["Masumi Morishige", "Ryo Koshihara"], "title": "Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 10 figures", "summary": "Reproducibility and reliability remain pressing challenges for generative AI\nsystems whose behavior can drift with each model update or prompt revision. We\nintroduce GPR-bench, a lightweight, extensible benchmark that operationalizes\nregression testing for general purpose use cases. GPR-bench couples an open,\nbilingual (English and Japanese) dataset covering eight task categories (e.g.,\ntext generation, code generation, and information retrieval) and 10 scenarios\nin each task categories (80 total test cases for each language) with an\nautomated evaluation pipeline that employs \"LLM-as-a-Judge\" scoring of\ncorrectness and conciseness. Experiments across three recent model versions -\ngpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default\nversus concise-writing instruction) reveal heterogeneous quality. Our results\nshow that newer models generally improve correctness, but the differences are\nmodest and not statistically significant, suggesting that GPR-bench may not be\nsufficiently challenging to differentiate between recent model versions. In\ncontrast, the concise-writing instruction significantly enhances conciseness\n(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with\nminimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of\nprompt engineering. Released under the MIT License, GPR- bench lowers the\nbarrier to initiating reproducibility monitoring and provides a foundation for\ncommunity-driven extensions, while also raising important considerations about\nbenchmark design for rapidly evolving language models.", "AI": {"tldr": "GPR-bench is introduced as a lightweight, extensible benchmark for regression testing in generative AI, evaluating correctness and conciseness using LLM-as-a-Judge. Results show modest improvements in newer models but significant conciseness gains from prompt engineering.", "motivation": "Address reproducibility and reliability challenges in generative AI systems, which can drift with model updates or prompt changes.", "method": "GPR-bench uses a bilingual dataset (English/Japanese) across eight task categories with an automated evaluation pipeline (LLM-as-a-Judge). Tests three model versions and two prompt configurations.", "result": "Newer models show slight correctness improvements (not significant). Concise-writing prompts boost conciseness (+12.37 pp) with minor accuracy trade-offs (-1.7 pp).", "conclusion": "GPR-bench aids reproducibility monitoring but may lack challenge for recent models. Highlights prompt engineering's impact and benchmark design considerations."}}
{"id": "2505.03186", "pdf": "https://arxiv.org/pdf/2505.03186", "abs": "https://arxiv.org/abs/2505.03186", "authors": ["Detao Bai", "Zhiheng Ma", "Xihan Wei", "Liefeng Bo"], "title": "CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": null, "summary": "The inherent synchronization between a speaker's lip movements, voice, and\nthe underlying linguistic content offers a rich source of information for\nimproving speech processing tasks, especially in challenging conditions where\ntraditional audio-only systems falter. We introduce CoGenAV, a powerful and\ndata-efficient model designed to learn versatile audio-visual representations\napplicable across a wide range of speech and audio-visual tasks. CoGenAV is\ntrained by optimizing a dual objective derived from natural audio-visual\nsynchrony, contrastive feature alignment and generative text prediction, using\nonly 223 hours of labeled data from the LRS2 dataset. This\ncontrastive-generative synchronization strategy effectively captures\nfundamental cross-modal correlations. We showcase the effectiveness and\nversatility of the learned CoGenAV representations on multiple benchmarks. When\nutilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these\nrepresentations contribute to achieving a state-of-the-art Word Error Rate\n(WER) of 1.27. They also enable strong performance in Visual Speech Recognition\n(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in\nnoisy environments by over 70%. Furthermore, CoGenAV representations benefit\nspeech reconstruction tasks, boosting performance in Speech Enhancement and\nSeparation, and achieve competitive results in audio-visual synchronization\ntasks like Active Speaker Detection (ASD). Our model will be open-sourced to\nfacilitate further development and collaboration within both academia and\nindustry.", "AI": {"tldr": "CoGenAV is a data-efficient model for audio-visual tasks, achieving state-of-the-art results in AVSR, VSR, and noisy environments, while also aiding speech reconstruction and synchronization tasks.", "motivation": "Leverage the synchronization between lip movements, voice, and linguistic content to improve speech processing, especially in challenging conditions where audio-only systems fail.", "method": "CoGenAV is trained using a dual objective of contrastive feature alignment and generative text prediction, utilizing only 223 hours of labeled data from LRS2.", "result": "Achieves a WER of 1.27 in AVSR, 22.0 in VSR, improves noisy environment performance by 70%, and excels in speech reconstruction and synchronization tasks.", "conclusion": "CoGenAV's versatile representations are highly effective across multiple benchmarks, and the model will be open-sourced for broader use."}}
{"id": "2505.02867", "pdf": "https://arxiv.org/pdf/2505.02867", "abs": "https://arxiv.org/abs/2505.02867", "authors": ["Ruiqi Wang", "Hao Zhang"], "title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation", "categories": ["cs.CV"], "comment": "42 pages, 31 figures. For more details:\n  https://suikei-wang.github.io/RESAnything/", "summary": "We present an open-vocabulary and zero-shot method for arbitrary referring\nexpression segmentation (RES), targeting input expressions that are more\ngeneral than what prior works were designed to handle. Specifically, our inputs\nencompass both object- and part-level labels as well as implicit references\npointing to properties or qualities of object/part function, design, style,\nmaterial, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)\nreasoning, where the key idea is attribute prompting. We generate detailed\ndescriptions of object/part attributes including shape, color, and location for\npotential segment proposals through systematic prompting of a large language\nmodel (LLM), where the proposals are produced by a foundational image\nsegmentation model. Our approach encourages deep reasoning about object or part\nattributes related to function, style, design, etc., enabling the system to\nhandle implicit queries without any part annotations for training or\nfine-tuning. As the first zero-shot and LLM-based RES method, RESAnything\nachieves clearly superior performance among zero-shot methods on traditional\nRES benchmarks and significantly outperforms existing methods on challenging\nscenarios involving implicit queries and complex part-level relations. Finally,\nwe contribute a new benchmark dataset to offer ~3K carefully curated RES\ninstances to assess part-level, arbitrary RES solutions.", "AI": {"tldr": "RESAnything is a zero-shot, open-vocabulary method for arbitrary referring expression segmentation (RES), handling implicit queries and part-level labels without training data. It uses Chain-of-Thoughts reasoning and LLM-generated attribute descriptions to outperform existing methods.", "motivation": "Prior RES methods struggle with general input expressions, including implicit references and part-level labels. RESAnything aims to address this gap by leveraging LLMs for deep reasoning.", "method": "RESAnything combines Chain-of-Thoughts reasoning with attribute prompting. An LLM generates detailed descriptions of object/part attributes, while a foundational segmentation model produces proposals.", "result": "RESAnything outperforms zero-shot methods on traditional benchmarks and excels in handling implicit queries and part-level relations. A new benchmark dataset (~3K instances) is introduced.", "conclusion": "RESAnything advances RES by enabling zero-shot, open-vocabulary segmentation for arbitrary expressions, demonstrating superior performance and introducing a new benchmark for evaluation."}}
{"id": "2505.02874", "pdf": "https://arxiv.org/pdf/2505.02874", "abs": "https://arxiv.org/abs/2505.02874", "authors": ["L. Juli\u00e1n Lechuga L\u00f3pez", "Shaza Elsharief", "Dhiyaa Al Jorf", "Firas Darwish", "Congbo Ma", "Farah E. Shamout"], "title": "Uncertainty Quantification for Machine Learning in Healthcare: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "46 pages, 3 figures, 2 tables, AHLI Conference on Health, Inference,\n  and Learning (CHIL)", "summary": "Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,\nreliability, and interpretability of Machine Learning (ML) systems for\nhealthcare, optimizing resources and improving patient care. Despite the\nemergence of ML-based clinical decision support tools, the lack of principled\nquantification of uncertainty in ML models remains a major challenge. Current\nreviews have a narrow focus on analyzing the state-of-the-art UQ in specific\nhealthcare domains without systematically evaluating method efficacy across\ndifferent stages of model development, and despite a growing body of research,\nits implementation in healthcare applications remains limited. Therefore, in\nthis survey, we provide a comprehensive analysis of current UQ in healthcare,\noffering an informed framework that highlights how different methods can be\nintegrated into each stage of the ML pipeline including data processing,\ntraining and evaluation. We also highlight the most popular methods used in\nhealthcare and novel approaches from other domains that hold potential for\nfuture adoption in the medical context. We expect this study will provide a\nclear overview of the challenges and opportunities of implementing UQ in the ML\npipeline for healthcare, guiding researchers and practitioners in selecting\nsuitable techniques to enhance the reliability, safety and trust from patients\nand clinicians on ML-driven healthcare solutions.", "AI": {"tldr": "The paper surveys Uncertainty Quantification (UQ) in ML for healthcare, addressing gaps in current reviews and proposing a framework for integrating UQ methods across the ML pipeline.", "motivation": "To enhance ML system robustness and reliability in healthcare by addressing the lack of principled UQ methods and their limited implementation.", "method": "Comprehensive analysis of UQ methods in healthcare, evaluating their efficacy across data processing, training, and evaluation stages.", "result": "Identifies popular and novel UQ methods, offering a framework for their integration into ML pipelines to improve reliability and trust.", "conclusion": "The study provides a clear overview of UQ challenges and opportunities, guiding researchers and practitioners in selecting suitable techniques for healthcare ML solutions."}}
{"id": "2505.02952", "pdf": "https://arxiv.org/pdf/2505.02952", "abs": "https://arxiv.org/abs/2505.02952", "authors": ["Fabrizio Marozzo"], "title": "Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR", "cs.LG"], "comment": null, "summary": "Generative AI systems have revolutionized human interaction by enabling\nnatural language-based coding and problem solving. However, the inherent\nambiguity of natural language often leads to imprecise instructions, forcing\nusers to iteratively test, correct, and resubmit their prompts. We propose an\niterative approach that systematically narrows down these ambiguities through a\nstructured series of clarification questions and alternative solution\nproposals, illustrated with input/output examples as well. Once every\nuncertainty is resolved, a final, precise solution is generated. Evaluated on a\ndiverse dataset spanning coding, data analysis, and creative writing, our\nmethod demonstrates superior accuracy, competitive resolution times, and higher\nuser satisfaction compared to conventional one-shot solutions, which typically\nrequire multiple manual iterations to achieve a correct output.", "AI": {"tldr": "The paper proposes an iterative method to reduce ambiguity in generative AI systems by using structured clarification questions and alternative solutions, improving accuracy and user satisfaction.", "motivation": "Natural language ambiguity in generative AI leads to imprecise instructions, requiring iterative corrections. The goal is to systematically resolve uncertainties for precise outputs.", "method": "An iterative approach with clarification questions, alternative solutions, and input/output examples to narrow down ambiguities before generating a final solution.", "result": "The method outperforms one-shot solutions in accuracy, resolution time, and user satisfaction across coding, data analysis, and creative writing tasks.", "conclusion": "The structured iterative approach effectively resolves ambiguities in generative AI, delivering precise solutions with higher efficiency and user satisfaction."}}
{"id": "2505.03420", "pdf": "https://arxiv.org/pdf/2505.03420", "abs": "https://arxiv.org/abs/2505.03420", "authors": ["Fei Zhao", "Chengcui Zhang", "Runlin Zhang", "Tianyang Wang", "Xi Li"], "title": "Mitigating Image Captioning Hallucinations in Vision-Language Models", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Hallucinations in vision-language models (VLMs) hinder reliability and\nreal-world applicability, usually stemming from distribution shifts between\npretraining data and test samples. Existing solutions, such as retraining or\nfine-tuning on additional data, demand significant computational resources and\nlabor-intensive data collection, while ensemble-based methods incur additional\ncosts by introducing auxiliary VLMs. To address these challenges, we propose a\nnovel test-time adaptation framework using reinforcement learning to mitigate\nhallucinations during inference without retraining or any auxiliary VLMs. By\nupdating only the learnable parameters in the layer normalization of the\nlanguage model (approximately 0.003% of the model parameters), our method\nreduces distribution shifts between test samples and pretraining samples. A\nCLIP-based hallucination evaluation model is proposed to provide dual rewards\nto VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in\nhallucination rates on LLaVA and InstructBLIP, respectively. Our approach\noutperforms state-of-the-art baselines with a 68.3% improvement in\nhallucination mitigation, demonstrating its effectiveness.", "AI": {"tldr": "A reinforcement learning-based test-time adaptation framework reduces hallucinations in VLMs by updating minimal parameters, outperforming existing methods.", "motivation": "Hallucinations in VLMs limit reliability, and current solutions are resource-intensive.", "method": "Proposes a test-time adaptation framework using reinforcement learning, updating only layer normalization parameters (0.003% of model). A CLIP-based model provides dual rewards.", "result": "Reduces hallucination rates by 15.4% (LLaVA) and 17.3% (InstructBLIP), with a 68.3% improvement over baselines.", "conclusion": "The method effectively mitigates hallucinations without retraining or auxiliary models, enhancing VLM reliability."}}
{"id": "2505.02843", "pdf": "https://arxiv.org/pdf/2505.02843", "abs": "https://arxiv.org/abs/2505.02843", "authors": ["Miriam Cobo", "David Corral Fontecha", "Wilson Silva", "Lara Lloret Iglesias"], "title": "Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "17 pages, 2 figures", "summary": "Artificial intelligence in medical imaging has seen unprecedented growth in\nthe last years, due to rapid advances in deep learning and computing resources.\nApplications cover the full range of existing medical imaging modalities, with\nunique characteristics driven by the physics of each technique. Yet, artificial\nintelligence professionals entering the field, and even experienced developers,\noften lack a comprehensive understanding of the physical principles underlying\nmedical image acquisition, which hinders their ability to fully leverage its\npotential. The integration of physics knowledge into artificial intelligence\nalgorithms enhances their trustworthiness and robustness in medical imaging,\nespecially in scenarios with limited data availability. In this work, we review\nthe fundamentals of physics in medical images and their impact on the latest\nadvances in artificial intelligence, particularly, in generative models and\nreconstruction algorithms. Finally, we explore the integration of physics\nknowledge into physics-inspired machine learning models, which leverage\nphysics-based constraints to enhance the learning of medical imaging features.", "AI": {"tldr": "The paper reviews the role of physics in medical imaging AI, emphasizing its importance for enhancing trustworthiness and robustness, especially in data-limited scenarios.", "motivation": "AI professionals often lack understanding of medical imaging physics, limiting their ability to leverage AI's full potential in the field.", "method": "The work reviews fundamentals of medical imaging physics and its impact on AI, focusing on generative models and reconstruction algorithms.", "result": "Integrating physics knowledge into AI improves model trustworthiness and robustness, particularly in data-scarce situations.", "conclusion": "Physics-inspired machine learning models, incorporating physics-based constraints, enhance feature learning in medical imaging."}}
{"id": "2505.03071", "pdf": "https://arxiv.org/pdf/2505.03071", "abs": "https://arxiv.org/abs/2505.03071", "authors": ["Vincent Dumoulin", "Otilia Stretcu", "Jenny Hamer", "Lauren Harrell", "Rob Laber", "Hugo Larochelle", "Bart van Merri\u00ebnboer", "Amanda Navine", "Patrick Hart", "Ben Williams", "Timothy A. C. Lamont", "Tries B. Rasak", "Mars Coral Restoration Team", "Sheryn Brodie", "Brendan Doohan", "Phil Eichinski", "Paul Roe", "Lin Schwarzkopf", "Tom Denton"], "title": "The Search for Squawk: Agile Modeling in Bioacoustics", "categories": ["eess.AS"], "comment": null, "summary": "Passive acoustic monitoring (PAM) has shown great promise in helping\necologists understand the health of animal populations and ecosystems. However,\nextracting insights from millions of hours of audio recordings requires the\ndevelopment of specialized recognizers. This is typically a challenging task,\nnecessitating large amounts of training data and machine learning expertise. In\nthis work, we introduce a general, scalable and data-efficient system for\ndeveloping recognizers for novel bioacoustic problems in under an hour. Our\nsystem consists of several key components that tackle problems in previous\nbioacoustic workflows: 1) highly generalizable acoustic embeddings pre-trained\nfor birdsong classification minimize data hunger; 2) indexed audio search\nallows the efficient creation of classifier training datasets, and 3)\nprecomputation of embeddings enables an efficient active learning loop,\nimproving classifier quality iteratively with minimal wait time. Ecologists\nemployed our system in three novel case studies: analyzing coral reef health\nthrough unidentified sounds; identifying juvenile Hawaiian bird calls to\nquantify breeding success and improve endangered species monitoring; and\nChristmas Island bird occupancy modeling. We augment the case studies with\nsimulated experiments which explore the range of design decisions in a\nstructured way and help establish best practices. Altogether these experiments\nshowcase our system's scalability, efficiency, and generalizability, enabling\nscientists to quickly address new bioacoustic challenges.", "AI": {"tldr": "A scalable, data-efficient system for bioacoustic recognizers enables ecologists to address new challenges quickly with minimal training data and expertise.", "motivation": "Passive acoustic monitoring (PAM) aids ecosystem health assessment, but developing recognizers is challenging due to data and expertise requirements.", "method": "The system uses pre-trained acoustic embeddings, indexed audio search for dataset creation, and precomputed embeddings for active learning.", "result": "Applied in three case studies (coral reef health, juvenile Hawaiian birds, Christmas Island birds) and simulations, the system proved scalable and efficient.", "conclusion": "The system's generalizability and efficiency empower scientists to tackle bioacoustic problems rapidly."}}
{"id": "2505.02858", "pdf": "https://arxiv.org/pdf/2505.02858", "abs": "https://arxiv.org/abs/2505.02858", "authors": ["Henry Tari", "Nojus Sereiva", "Rishabh Kaushal", "Thales Bertaglia", "Adriana Iamnitchi"], "title": "Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": "arXiv admin note: text overlap with arXiv:2407.08323", "summary": "Social media datasets are essential for research on a variety of topics, such\nas disinformation, influence operations, hate speech detection, or influencer\nmarketing practices. However, access to social media datasets is often\nconstrained due to costs and platform restrictions. Acquiring datasets that\nspan multiple platforms, which is crucial for understanding the digital\necosystem, is particularly challenging. This paper explores the potential of\nlarge language models to create lexically and semantically relevant social\nmedia datasets across multiple platforms, aiming to match the quality of real\ndata. We propose multi-platform topic-based prompting and employ various\nlanguage models to generate synthetic data from two real datasets, each\nconsisting of posts from three different social media platforms. We assess the\nlexical and semantic properties of the synthetic data and compare them with\nthose of the real data. Our empirical findings show that using large language\nmodels to generate synthetic multi-platform social media data is promising,\ndifferent language models perform differently in terms of fidelity, and a\npost-processing approach might be needed for generating high-fidelity synthetic\ndatasets for research. In addition to the empirical evaluation of three state\nof the art large language models, our contributions include new fidelity\nmetrics specific to multi-platform social media datasets.", "AI": {"tldr": "The paper explores using large language models to generate synthetic multi-platform social media datasets, evaluating their fidelity compared to real data and proposing new metrics for assessment.", "motivation": "Access to multi-platform social media datasets is limited due to costs and restrictions, hindering research on topics like disinformation and hate speech.", "method": "Multi-platform topic-based prompting with various language models generates synthetic data from real datasets, assessing lexical and semantic properties.", "result": "Synthetic data shows promise, with varying fidelity across models, suggesting post-processing may improve quality.", "conclusion": "Large language models can generate useful synthetic datasets, but fidelity varies, and new metrics are proposed for evaluation."}}
{"id": "2505.03193", "pdf": "https://arxiv.org/pdf/2505.03193", "abs": "https://arxiv.org/abs/2505.03193", "authors": ["Wei Meng"], "title": "A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive", "categories": ["cs.SD", "cs.AI", "cs.CR", "eess.AS", "94A12 (Primary), 68T07, 42A38 (Secondary)", "H.3.3; I.5.4; I.2.6"], "comment": "This paper proposes a novel framework for detecting steganographic\n  content in short video audio streams using sliding spectral features and\n  distributed inference models, combining STFT analysis, entropy-based\n  synchronization, and deep learning-driven decoding strategies", "summary": "With the rise of short video platforms in global communication, embedding\nsteganographic data in audio synchronization streams has emerged as a new\ncovert communication method. To address the limitations of traditional\ntechniques in detecting synchronized steganography, this paper proposes a\ndetection and distributed guidance reconstruction model based on short video\n\"Yupan\" samples released by China's South Sea Fleet on TikTok. The method\nintegrates sliding spectrum feature extraction and intelligent inference\nmechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is\nused to extract the main frequency trajectory and construct the synchronization\nframe detection model (M1), identifying a frame flag \"FFFFFFFFFFFFFFFFFF80\".\nThe subsequent 32-byte payload is decoded by a structured model (M2) to infer\ndistributed guidance commands. Analysis reveals a low-entropy, repetitive byte\nsequence in the 36 to 45 second audio segment with highly concentrated spectral\nenergy, confirming the presence of synchronization frames. Although plaintext\nsemantics are not restored, the consistency in command field layout suggests\nfeatures of military communication protocols. The multi-segment splicing model\nfurther shows cross-video embedding and centralized decoding capabilities. The\nproposed framework validates the effectiveness of sliding spectral features for\nsynchronized steganography detection and builds an extensible inference model\nfor covert communication analysis and tactical guidance simulation on open\nplatforms.", "AI": {"tldr": "A method for detecting steganographic data in audio synchronization streams of short videos is proposed, using sliding spectrum features and intelligent inference to identify and decode hidden military commands.", "motivation": "The rise of short video platforms has introduced new covert communication methods, but traditional techniques struggle to detect synchronized steganography.", "method": "Uses a 25 ms sliding window with STFT to extract frequency trajectories and detect synchronization frames, followed by structured decoding of payloads.", "result": "Identified low-entropy, repetitive byte sequences in audio segments, confirming synchronization frames and military-like command layouts.", "conclusion": "The framework effectively detects synchronized steganography and provides an extensible model for analyzing covert communication on open platforms."}}
{"id": "2505.02949", "pdf": "https://arxiv.org/pdf/2505.02949", "abs": "https://arxiv.org/abs/2505.02949", "authors": ["Tian Qiu", "Arjun Nichani", "Rasta Tadayontahmasebi", "Haewon Jeong"], "title": "Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images", "categories": ["cs.CV"], "comment": "Accepted at ACM FAccT '25", "summary": "Neural compression methods are gaining popularity due to their superior\nrate-distortion performance over traditional methods, even at extremely low\nbitrates below 0.1 bpp. As deep learning architectures, these models are prone\nto bias during the training process, potentially leading to unfair outcomes for\nindividuals in different groups. In this paper, we present a general,\nstructured, scalable framework for evaluating bias in neural image compression\nmodels. Using this framework, we investigate racial bias in neural compression\nalgorithms by analyzing nine popular models and their variants. Through this\ninvestigation, we first demonstrate that traditional distortion metrics are\nineffective in capturing bias in neural compression models. Next, we highlight\nthat racial bias is present in all neural compression models and can be\ncaptured by examining facial phenotype degradation in image reconstructions. We\nthen examine the relationship between bias and realism in the decoded images\nand demonstrate a trade-off across models. Finally, we show that utilizing a\nracially balanced training set can reduce bias but is not a sufficient bias\nmitigation strategy. We additionally show the bias can be attributed to\ncompression model bias and classification model bias. We believe that this work\nis a first step towards evaluating and eliminating bias in neural image\ncompression models.", "AI": {"tldr": "The paper evaluates racial bias in neural image compression models, showing traditional metrics fail to capture bias, and proposes a framework to assess and mitigate it.", "motivation": "To address the potential unfair outcomes due to bias in neural compression models, especially at low bitrates, and to evaluate racial bias in popular models.", "method": "A structured, scalable framework is used to analyze nine neural compression models, focusing on facial phenotype degradation and realism in reconstructions.", "result": "Racial bias is present in all models, with a trade-off between bias and realism. A racially balanced training set reduces bias but isn't sufficient. Bias stems from both compression and classification models.", "conclusion": "This work is a foundational step toward evaluating and mitigating bias in neural image compression, highlighting the need for better metrics and strategies."}}
{"id": "2505.02877", "pdf": "https://arxiv.org/pdf/2505.02877", "abs": "https://arxiv.org/abs/2505.02877", "authors": ["Hele Zhu", "Xinyi Huang", "Haojia Gao", "Mengfei Jiang", "Haohua Que", "Lei Mu"], "title": "A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Plant disease is a critical factor affecting agricultural production.\nTraditional manual recognition methods face significant drawbacks, including\nlow accuracy, high costs, and inefficiency. Deep learning techniques have\ndemonstrated significant benefits in identifying plant diseases, but they still\nface challenges such as inference delays and high energy consumption. Deep\nlearning algorithms are difficult to run on resource-limited embedded devices.\nOffloading these models to cloud servers is confronted with the restriction of\ncommunication bandwidth, and all of these factors will influence the\ninference's efficiency. We propose a collaborative inference framework for\nrecognizing plant diseases between edge devices and cloud servers to enhance\ninference speed. The DNN model for plant disease recognition is pruned through\ndeep reinforcement learning to improve the inference speed and reduce energy\nconsumption. Then the optimal split point is determined by a greedy strategy to\nachieve the best collaborated inference acceleration. Finally, the system for\ncollaborative inference acceleration in plant disease recognition has been\nimplemented using Gradio to facilitate friendly human-machine interaction.\nExperiments indicate that the proposed collaborative inference framework\nsignificantly increases inference speed while maintaining acceptable\nrecognition accuracy, offering a novel solution for rapidly diagnosing and\npreventing plant diseases.", "AI": {"tldr": "A collaborative inference framework using edge-cloud computing and deep reinforcement learning improves plant disease recognition speed and energy efficiency.", "motivation": "Traditional manual and deep learning methods for plant disease recognition are inefficient, slow, or resource-heavy.", "method": "The framework prunes DNN models with deep reinforcement learning, uses a greedy strategy to optimize edge-cloud split points, and implements a Gradio-based system.", "result": "The framework boosts inference speed while maintaining accuracy, enabling rapid plant disease diagnosis.", "conclusion": "The solution offers an efficient, scalable approach for plant disease recognition on resource-limited devices."}}
{"id": "2505.03020", "pdf": "https://arxiv.org/pdf/2505.03020", "abs": "https://arxiv.org/abs/2505.03020", "authors": ["Kishore Sampath", "Pratheesh", "Ayaazuddin Mohammad", "Resmi Ramachandranpillai"], "title": "The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI", "categories": ["cs.AI"], "comment": "CVPR 2025 Second Workshop on Responsible Generative AI", "summary": "Multimodal learning, which integrates diverse data sources such as images,\ntext, and structured data, has proven superior to unimodal counterparts in\nhigh-stakes decision-making. However, while performance gains remain the gold\nstandard for evaluating multimodal systems, concerns around bias and robustness\nare frequently overlooked. In this context, this paper explores two key\nresearch questions (RQs): (i) RQ1 examines whether adding a modality\ncon-sistently enhances performance and investigates its role in shaping\nfairness measures, assessing whether it mitigates or amplifies bias in\nmultimodal models; (ii) RQ2 investigates the impact of missing modalities at\ninference time, analyzing how multimodal models generalize in terms of both\nperformance and fairness. Our analysis reveals that incorporating new\nmodalities during training consistently enhances the performance of multimodal\nmodels, while fairness trends exhibit variability across different evaluation\nmeasures and datasets. Additionally, the absence of modalities at inference\ndegrades performance and fairness, raising concerns about its robustness in\nreal-world deployment. We conduct extensive experiments using multimodal\nhealthcare datasets containing images, time series, and structured information\nto validate our findings.", "AI": {"tldr": "Adding modalities improves performance but fairness varies; missing modalities at inference harms both performance and fairness.", "motivation": "To address overlooked concerns of bias and robustness in multimodal learning, focusing on performance and fairness.", "method": "Analyzes impact of adding/missing modalities using healthcare datasets with images, time series, and structured data.", "result": "New modalities boost performance but fairness varies; missing modalities degrade both performance and fairness.", "conclusion": "Highlights trade-offs in multimodal learning, emphasizing robustness and fairness for real-world deployment."}}
{"id": "2505.03123", "pdf": "https://arxiv.org/pdf/2505.03123", "abs": "https://arxiv.org/abs/2505.03123", "authors": ["Yiran Zhu", "Wei Yang", "Yan su", "Zesheng Li", "Chengchang Pan", "Honggang Qi"], "title": "STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "9 pages, 4 figures, 5 tables", "summary": "We propose a multimodal spatiotemporal graph neural network (STG) framework\nto predict colorectal cancer liver metastasis (CRLM) progression. Current\nclinical models do not effectively integrate the tumor's spatial heterogeneity,\ndynamic evolution, and complex multimodal data relationships, limiting their\npredictive accuracy. Our STG framework combines preoperative CT imaging and\nclinical data into a heterogeneous graph structure, enabling joint modeling of\ntumor distribution and temporal evolution through spatial topology and\ncross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal\nneighborhood information and leverages supervised and contrastive learning\nstrategies to enhance the model's ability to capture temporal features and\nimprove robustness. A lightweight version of the model reduces parameter count\nby 78.55%, maintaining near-state-of-the-art performance. The model jointly\noptimizes recurrence risk regression and survival analysis tasks, with\ncontrastive loss improving feature representational discriminability and\ncross-modal consistency. Experimental results on the MSKCC CRLM dataset show a\ntime-adjacent accuracy of 85% and a mean absolute error of 1.1005,\nsignificantly outperforming existing methods. The innovative heterogeneous\ngraph construction and spatiotemporal decoupling mechanism effectively uncover\nthe associations between dynamic tumor microenvironment changes and prognosis,\nproviding reliable quantitative support for personalized treatment decisions.", "AI": {"tldr": "A multimodal spatiotemporal graph neural network (STG) framework is proposed to predict colorectal cancer liver metastasis (CRLM) progression, outperforming existing methods by integrating spatial, temporal, and multimodal data.", "motivation": "Current clinical models fail to effectively integrate spatial heterogeneity, dynamic evolution, and multimodal data, limiting predictive accuracy for CRLM progression.", "method": "The STG framework combines CT imaging and clinical data into a heterogeneous graph, using GraphSAGE for spatiotemporal aggregation and supervised/contrastive learning for robustness. A lightweight version reduces parameters by 78.55%.", "result": "Achieves 85% time-adjacent accuracy and 1.1005 mean absolute error on the MSKCC CRLM dataset, significantly outperforming existing methods.", "conclusion": "The framework's innovative graph construction and spatiotemporal decoupling uncover tumor microenvironment-prognosis associations, aiding personalized treatment decisions."}}
{"id": "2505.03037", "pdf": "https://arxiv.org/pdf/2505.03037", "abs": "https://arxiv.org/abs/2505.03037", "authors": ["Xiaofeng Liu", "Yongsong Huang", "Thibault Marin", "Samira Vafay Eslahi", "Tiss Amal", "Yanis Chemli", "Keith Johnson", "Georges El Fakhri", "Jinsong Ouyang"], "title": "Dual Prompting for Diverse Count-level PET Denoising", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "Published in IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2025", "summary": "The to-be-denoised positron emission tomography (PET) volumes are inherent\nwith diverse count levels, which imposes challenges for a unified model to\ntackle varied cases. In this work, we resort to the recently flourished prompt\nlearning to achieve generalizable PET denoising with different count levels.\nSpecifically, we propose dual prompts to guide the PET denoising in a\ndivide-and-conquer manner, i.e., an explicitly count-level prompt to provide\nthe specific prior information and an implicitly general denoising prompt to\nencode the essential PET denoising knowledge. Then, a novel prompt fusion\nmodule is developed to unify the heterogeneous prompts, followed by a\nprompt-feature interaction module to inject prompts into the features. The\nprompts are able to dynamically guide the noise-conditioned denoising process.\nTherefore, we are able to efficiently train a unified denoising model for\nvarious count levels, and deploy it to different cases with personalized\nprompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly\nselected 13-22\\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.\nIt shows our dual prompting can largely improve the performance with informed\ncount-level and outperform the count-conditional model.", "AI": {"tldr": "The paper proposes a dual-prompt learning approach for denoising PET volumes with varying count levels, achieving better performance than count-conditional models.", "motivation": "PET volumes have diverse count levels, making unified denoising challenging. The goal is to develop a generalizable model for varied cases.", "method": "Uses dual prompts (explicit count-level and implicit general denoising) with a fusion module and prompt-feature interaction to guide denoising dynamically.", "result": "Tested on 1940 low-count PET volumes, the dual-prompt method outperforms count-conditional models.", "conclusion": "The dual-prompt approach effectively unifies denoising for diverse count levels, improving performance and generalizability."}}
{"id": "2505.03697", "pdf": "https://arxiv.org/pdf/2505.03697", "abs": "https://arxiv.org/abs/2505.03697", "authors": ["Susmita Bhattacharjee", "Jagabandhu Mishra", "H. S. Shekhawat", "S. R. Mahadeva Prasanna"], "title": "Fairness of Automatic Speech Recognition in Cleft Lip and Palate Speech", "categories": ["eess.AS"], "comment": "Submitted to Digital Signal Processing", "summary": "Speech produced by individuals with cleft lip and palate (CLP) is often\nhighly nasalized and breathy due to structural anomalies, causing shifts in\nformant structure that affect automatic speech recognition (ASR) performance\nand fairness. This study hypothesizes that publicly available ASR systems\nexhibit reduced fairness for CLP speech and confirms this through experiments.\nDespite formant disruptions, mild and moderate CLP speech retains some\nspectro-temporal alignment with normal speech, motivating augmentation\nstrategies to enhance fairness. The study systematically explores augmenting\nCLP speech with normal speech across severity levels and evaluates its impact\non ASR fairness. Three ASR models-GMM-HMM, Whisper, and XLS-R-were tested on\nAIISH and NMCPC datasets. Results indicate that training with normal speech and\ntesting on mixed data improves word error rate (WER). Notably, WER decreased\nfrom $22.64\\%$ to $18.76\\%$ (GMM-HMM, AIISH) and $28.45\\%$ to $18.89\\%$\n(Whisper, NMCPC). The superior performance of GMM-HMM on AIISH may be due to\nits suitability for Kannada children's speech, a challenge for foundation\nmodels like XLS-R and Whisper. To assess fairness, a fairness score was\nintroduced, revealing improvements of $17.89\\%$ (AIISH) and $47.50\\%$ (NMCPC)\nwith augmentation.", "AI": {"tldr": "ASR systems show reduced fairness for CLP speech, but augmenting CLP speech with normal speech improves performance and fairness.", "motivation": "To address the reduced fairness of ASR systems for CLP speech due to structural anomalies affecting formant structure.", "method": "Augment CLP speech with normal speech and test on three ASR models (GMM-HMM, Whisper, XLS-R) using AIISH and NMCPC datasets.", "result": "WER improved (e.g., GMM-HMM: 22.64% to 18.76%; Whisper: 28.45% to 18.89%). Fairness scores increased by 17.89% (AIISH) and 47.50% (NMCPC).", "conclusion": "Augmentation enhances ASR fairness for CLP speech, with GMM-HMM performing best for Kannada children's speech."}}
{"id": "2505.02859", "pdf": "https://arxiv.org/pdf/2505.02859", "abs": "https://arxiv.org/abs/2505.02859", "authors": ["Jonas Bokstaller", "Julia Altheimer", "Julian Dormehl", "Alina Buss", "Jasper Wiltfang", "Johannes Schneider", "Maximilian R\u00f6glinger"], "title": "Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Across various sectors applications of eXplainableAI (XAI) gained momentum as\nthe increasing black-boxedness of prevailing Machine Learning (ML) models\nbecame apparent. In parallel, Large Language Models (LLMs) significantly\ndeveloped in their abilities to understand human language and complex patterns.\nBy combining both, this paper presents a novel reference architecture for the\ninterpretation of XAI through an interactive chatbot powered by a fine-tuned\nLLM. We instantiate the reference architecture in the context of\nState-of-Health (SoH) prediction for batteries and validate its design in\nmultiple evaluation and demonstration rounds. The evaluation indicates that the\nimplemented prototype enhances the human interpretability of ML, especially for\nusers with less experience with XAI.", "AI": {"tldr": "A novel reference architecture combines XAI and LLMs to create an interactive chatbot for interpreting ML models, validated in battery SoH prediction.", "motivation": "The increasing opacity of ML models and advancements in LLMs' language understanding drive the need for better XAI interpretability.", "method": "Develop a reference architecture using a fine-tuned LLM for an interactive chatbot, applied to battery SoH prediction.", "result": "The prototype improves ML interpretability, especially for users with limited XAI experience.", "conclusion": "The architecture successfully enhances human understanding of ML models through interactive XAI."}}
{"id": "2505.03228", "pdf": "https://arxiv.org/pdf/2505.03228", "abs": "https://arxiv.org/abs/2505.03228", "authors": ["Ya Li", "Bin Zhou", "Bo Hu"], "title": "MGFF-TDNN: A Multi-Granularity Feature Fusion TDNN Model with Depth-Wise Separable Module for Speaker Verification", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "In speaker verification, traditional models often emphasize modeling\nlong-term contextual features to capture global speaker characteristics.\nHowever, this approach can neglect fine-grained voiceprint information, which\ncontains highly discriminative features essential for robust speaker\nembeddings. This paper introduces a novel model architecture, termed MGFF-TDNN,\nbased on multi-granularity feature fusion. The MGFF-TDNN leverages a\ntwo-dimensional depth-wise separable convolution module, enhanced with local\nfeature modeling, as a front-end feature extractor to effectively capture\ntime-frequency domain features. To achieve comprehensive multi-granularity\nfeature fusion, we propose the M-TDNN structure, which integrates global\ncontextual modeling with fine-grained feature extraction by combining\ntime-delay neural networks and phoneme-level feature pooling. Experiments on\nthe VoxCeleb dataset demonstrate that the MGFF-TDNN achieves outstanding\nperformance in speaker verification while remaining efficient in terms of\nparameters and computational resources.", "AI": {"tldr": "The paper introduces MGFF-TDNN, a novel model for speaker verification that combines multi-granularity feature fusion to capture both global and fine-grained voiceprint features, achieving high performance efficiently.", "motivation": "Traditional speaker verification models focus on long-term contextual features, neglecting fine-grained voiceprint details. The paper aims to address this gap by capturing both global and local features for robust speaker embeddings.", "method": "The MGFF-TDNN uses a two-dimensional depth-wise separable convolution module for front-end feature extraction and integrates global contextual modeling with fine-grained features via the M-TDNN structure, combining time-delay neural networks and phoneme-level pooling.", "result": "Experiments on the VoxCeleb dataset show that MGFF-TDNN outperforms traditional models in speaker verification while maintaining efficiency in parameters and computation.", "conclusion": "The MGFF-TDNN effectively balances global and fine-grained feature extraction, offering a robust and efficient solution for speaker verification."}}
{"id": "2505.02966", "pdf": "https://arxiv.org/pdf/2505.02966", "abs": "https://arxiv.org/abs/2505.02966", "authors": ["Alexander Holmberg"], "title": "Generating Narrated Lecture Videos from Slides with Synchronized Highlights", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Turning static slides into engaging video lectures takes considerable time\nand effort, requiring presenters to record explanations and visually guide\ntheir audience through the material. We introduce an end-to-end system designed\nto automate this process entirely. Given a slide deck, this system synthesizes\na video lecture featuring AI-generated narration synchronized precisely with\ndynamic visual highlights. These highlights automatically draw attention to the\nspecific concept being discussed, much like an effective presenter would. The\ncore technical contribution is a novel highlight alignment module. This module\naccurately maps spoken phrases to locations on a given slide using diverse\nstrategies (e.g., Levenshtein distance, LLM-based semantic analysis) at\nselectable granularities (line or word level) and utilizes timestamp-providing\nText-to-Speech (TTS) for timing synchronization. We demonstrate the system's\neffectiveness through a technical evaluation using a manually annotated slide\ndataset with 1000 samples, finding that LLM-based alignment achieves high\nlocation accuracy (F1 > 92%), significantly outperforming simpler methods,\nespecially on complex, math-heavy content. Furthermore, the calculated\ngeneration cost averages under $1 per hour of video, offering potential savings\nof two orders of magnitude compared to conservative estimates of manual\nproduction costs. This combination of high accuracy and extremely low cost\npositions this approach as a practical and scalable tool for transforming\nstatic slides into effective, visually-guided video lectures.", "AI": {"tldr": "An AI system automates video lecture creation from slides, syncing narration with dynamic visual highlights, achieving high accuracy and low cost.", "motivation": "Manual video lecture creation is time-consuming; automating it can save effort and costs.", "method": "Uses a highlight alignment module with strategies like Levenshtein distance and LLM-based analysis, synchronized with TTS.", "result": "LLM-based alignment achieves F1 > 92%, with generation costs under $1 per hour.", "conclusion": "The system is a scalable, cost-effective solution for converting slides into engaging video lectures."}}
{"id": "2505.02880", "pdf": "https://arxiv.org/pdf/2505.02880", "abs": "https://arxiv.org/abs/2505.02880", "authors": ["Zian Liu", "Renjun Jia"], "title": "LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction", "categories": ["cs.LG"], "comment": "12 pages, 9figures", "summary": "Predicting financial time series presents significant challenges due to\ninherent low signal-to-noise ratios and intricate temporal patterns.\nTraditional machine learning models exhibit limitations in this forecasting\ntask constrained by their restricted model capacity. Recent advances in large\nlanguage models (LLMs), with their greatly expanded parameter spaces,\ndemonstrate promising potential for modeling complex dependencies in temporal\nsequences. However, existing LLM-based approaches typically focus on\nfixed-length patch analysis due to the Transformer architecture, ignoring\nmarket data's multi-scale pattern characteristics. In this study, we propose\n$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal\nsequence modeling through learnable patch segmentation and dynamic wavelet\nconvolution modules. Specifically,we first employ K-means++ clustering based on\nDTW distance to identify scale-invariant patterns in market data. Building upon\npattern recognition results, we introduce adaptive patch segmentation that\npartitions temporal sequences while preserving maximal pattern integrity. To\naccommodate time-varying frequency characteristics, we devise a dynamic wavelet\nconvolution module that emulates discrete wavelet transformation with enhanced\nflexibility in capturing time-frequency features. These three modules work\ntogether to improve large language model's ability to handle scale-invariant\npatterns in financial time series. Extensive experiments on real-world\nfinancial datasets substantiate the framework's efficacy, demonstrating\nsuperior performance in capturing complex market patterns and achieving\nstate-of-the-art results in stock return prediction. The successful deployment\nin practical trading systems confirms its real-world applicability,\nrepresenting a significant advancement in LLM applications for financial\nforecasting.", "AI": {"tldr": "The paper proposes $LLM4FTS$, a framework enhancing LLMs for financial time series forecasting using adaptive patch segmentation and dynamic wavelet convolution to handle multi-scale patterns.", "motivation": "Financial time series forecasting is challenging due to low signal-to-noise ratios and complex temporal patterns. Traditional ML models lack capacity, and existing LLM approaches ignore multi-scale market data characteristics.", "method": "The framework uses K-means++ clustering with DTW distance for scale-invariant pattern recognition, adaptive patch segmentation for sequence partitioning, and dynamic wavelet convolution for time-frequency feature capture.", "result": "Experiments show superior performance in capturing market patterns and achieving state-of-the-art stock return prediction. Practical trading system deployment confirms real-world applicability.", "conclusion": "$LLM4FTS$ advances LLM applications in financial forecasting by effectively handling scale-invariant patterns and improving prediction accuracy."}}
{"id": "2505.03033", "pdf": "https://arxiv.org/pdf/2505.03033", "abs": "https://arxiv.org/abs/2505.03033", "authors": ["George Xi Wang", "Jingying Deng", "Safinah Ali"], "title": "Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Independent learners often struggle with sustaining focus and emotional\nregulation in unstructured or distracting settings. Although some rely on\nambient aids such as music, ASMR, or visual backgrounds to support\nconcentration, these tools are rarely integrated into cohesive,\nlearner-centered systems. Moreover, existing educational technologies focus\nprimarily on content adaptation and feedback, overlooking the emotional and\nsensory context in which learning takes place. Large language models have\ndemonstrated powerful multimodal capabilities including the ability to generate\nand adapt text, audio, and visual content. Educational research has yet to\nfully explore their potential in creating personalized audiovisual learning\nenvironments. To address this gap, we introduce an AI-powered system that uses\nLLMs to generate personalized multisensory study environments. Users select or\ngenerate customized visual themes (e.g., abstract vs. realistic, static vs.\nanimated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.\nnovel sounds) to create immersive settings aimed at reducing distraction and\nenhancing emotional stability. Our primary research question investigates how\ncombinations of personalized audiovisual elements affect learner cognitive load\nand engagement. Using a mixed-methods design that incorporates biometric\nmeasures and performance outcomes, this study evaluates the effectiveness of\nLLM-driven sensory personalization. The findings aim to advance emotionally\nresponsive educational technologies and extend the application of multimodal\nLLMs into the sensory dimension of self-directed learning.", "AI": {"tldr": "An AI-powered system uses LLMs to create personalized audiovisual learning environments, aiming to reduce distraction and enhance emotional stability for independent learners.", "motivation": "Independent learners struggle with focus and emotional regulation in unstructured settings, and existing educational technologies overlook sensory and emotional contexts.", "method": "The system leverages LLMs to generate personalized visual and auditory elements, evaluated through mixed-methods design with biometric measures and performance outcomes.", "result": "The study investigates how personalized audiovisual combinations affect cognitive load and engagement, aiming to improve emotionally responsive educational technologies.", "conclusion": "The findings aim to advance sensory personalization in self-directed learning and expand the use of multimodal LLMs in education."}}
{"id": "2505.03319", "pdf": "https://arxiv.org/pdf/2505.03319", "abs": "https://arxiv.org/abs/2505.03319", "authors": ["Manolis Mylonas", "Evlampios Apostolidis", "Vasileios Mezaris"], "title": "SD-VSum: A Method and Dataset for Script-Driven Video Summarization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Under review", "summary": "In this work, we introduce the task of script-driven video summarization,\nwhich aims to produce a summary of the full-length video by selecting the parts\nthat are most relevant to a user-provided script outlining the visual content\nof the desired summary. Following, we extend a recently-introduced large-scale\ndataset for generic video summarization (VideoXum) by producing natural\nlanguage descriptions of the different human-annotated summaries that are\navailable per video. In this way we make it compatible with the introduced\ntask, since the available triplets of ``video, summary and summary\ndescription'' can be used for training a method that is able to produce\ndifferent summaries for a given video, driven by the provided script about the\ncontent of each summary. Finally, we develop a new network architecture for\nscript-driven video summarization (SD-VSum), that relies on the use of a\ncross-modal attention mechanism for aligning and fusing information from the\nvisual and text modalities. Our experimental evaluations demonstrate the\nadvanced performance of SD-VSum against state-of-the-art approaches for\nquery-driven and generic (unimodal and multimodal) summarization from the\nliterature, and document its capacity to produce video summaries that are\nadapted to each user's needs about their content.", "AI": {"tldr": "The paper introduces script-driven video summarization, extends the VideoXum dataset with natural language descriptions, and proposes SD-VSum, a cross-modal attention-based architecture, outperforming state-of-the-art methods.", "motivation": "To enable video summarization tailored to user-provided scripts, addressing the need for personalized summaries.", "method": "Extends VideoXum dataset with summary descriptions and develops SD-VSum, a cross-modal attention network for aligning visual and text data.", "result": "SD-VSum outperforms existing methods in query-driven and generic summarization, producing user-adapted summaries.", "conclusion": "The work successfully introduces script-driven summarization, demonstrates its effectiveness, and provides a scalable dataset for future research."}}
{"id": "2505.03637", "pdf": "https://arxiv.org/pdf/2505.03637", "abs": "https://arxiv.org/abs/2505.03637", "authors": ["Malte Riedel", "Thomas Ulrich", "Samuel Bianchi", "Klaas P. Pruessmann"], "title": "Stabilizing 3D EPI time series by servo navigation and phase equalization exploiting repeated shots (PEERS)", "categories": ["eess.IV", "physics.med-ph"], "comment": "to be published in Magnetic Resonance in Medicine (MRM)", "summary": "Purpose: To enable run-time head motion control and robust frequency\ncorrections for 3D EPI fMRI. Methods: A short 3D orbital navigator (3 ms) is\ninserted into a 3D EPI sequence. A linear perturbation model is calibrated to\nestimate rigid motion and frequency parameters per shot. Rigid motion is\ncorrected by scan geometry updates in run-time, while several techniques are\ninvestigated to stabilize navigator-based frequency corrections in the\nreconstruction. An additional method termed PEERS is proposed that exploits the\nrepetitive structure of fMRI scans to fine-tune shot-wise phase and frequency\nestimates using the motion-corrected EPI data itself. Results: Servo navigation\neffectively reduces motion in the raw data of in-vivo fMRI scans in six\nsubjects. PEERS provides high-precision frequency parameters for robust\nphase-corrected reconstructions in the phantom and in-vivo accounting for\nscanner drifts and slice encoding-related effects on EPI. In combination, servo\nnavigation and PEERS achieve successful intra-volume corrections and consistent\ntSNR improvements of 8% on average throughout the brain. The two methods prove\nto be highly synergetic. Conclusion: Servo navigation achieves high-precision\nmotion correction for 3D-EPI fMRI in run-time and, in synergy with PEERS,\nprovides stable frequency corrections with short navigators even for long echo\ntimes. With its automatic self-calibration and no hardware requirements, servo\nnavigation and PEERS enable effective plug-and-play motion correction for 3D\nfMRI.", "AI": {"tldr": "The paper proposes servo navigation and PEERS for real-time motion and frequency correction in 3D EPI fMRI, improving data quality and tSNR.", "motivation": "To address motion and frequency instability in 3D EPI fMRI scans, ensuring robust and high-quality imaging.", "method": "Uses a 3D orbital navigator for motion estimation and PEERS for phase/frequency refinement, combining real-time geometry updates and data-driven corrections.", "result": "Reduces motion artifacts, improves tSNR by 8%, and stabilizes frequency corrections, even for long echo times.", "conclusion": "Servo navigation and PEERS offer a plug-and-play solution for effective motion and frequency correction in 3D fMRI without additional hardware."}}
{"id": "2505.03054", "pdf": "https://arxiv.org/pdf/2505.03054", "abs": "https://arxiv.org/abs/2505.03054", "authors": ["Orevaoghene Ahia", "Martijn Bartelds", "Kabir Ahuja", "Hila Gonen", "Valentin Hofmann", "Siddhant Arora", "Shuyue Stella Li", "Vishal Puttagunta", "Mofetoluwa Adeyemi", "Charishma Buchireddy", "Ben Walls", "Noah Bennett", "Shinji Watanabe", "Noah A. Smith", "Yulia Tsvetkov", "Sachin Kumar"], "title": "BLAB: Brutally Long Audio Bench", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing large audio language models (LMs) capable of understanding diverse\nspoken interactions is essential for accommodating the multimodal nature of\nhuman communication and can increase the accessibility of language technologies\nacross different user populations. Recent work on audio LMs has primarily\nevaluated their performance on short audio segments, typically under 30\nseconds, with limited exploration of long-form conversational speech segments\nthat more closely reflect natural user interactions with these models. We\nintroduce Brutally Long Audio Bench (BLAB), a challenging long-form audio\nbenchmark that evaluates audio LMs on localization, duration estimation,\nemotion, and counting tasks using audio segments averaging 51 minutes in\nlength. BLAB consists of 833+ hours of diverse, full-length audio clips, each\npaired with human-annotated, text-based natural language questions and answers.\nOur audio data were collected from permissively licensed sources and underwent\na human-assisted filtering process to ensure task compliance. We evaluate six\nopen-source and proprietary audio LMs on BLAB and find that all of them,\nincluding advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the\ntasks in BLAB. Our comprehensive analysis reveals key insights into the\ntrade-offs between task difficulty and audio duration. In general, we find that\naudio LMs struggle with long-form speech, with performance declining as\nduration increases. They perform poorly on localization, temporal reasoning,\ncounting, and struggle to understand non-phonemic information, relying more on\nprompts than audio content. BLAB serves as a challenging evaluation framework\nto develop audio LMs with robust long-form audio understanding capabilities.", "AI": {"tldr": "BLAB is a long-form audio benchmark testing audio LMs on tasks like localization and emotion, revealing their struggles with extended speech.", "motivation": "To address the lack of evaluation for audio LMs on long-form conversational speech, which better reflects real-world interactions.", "method": "Introduces BLAB, a benchmark with 833+ hours of diverse, annotated long-form audio clips (avg. 51 mins), testing tasks like localization and counting.", "result": "All tested audio LMs, including advanced ones like GPT-4o, perform poorly on BLAB, especially as audio duration increases.", "conclusion": "BLAB highlights the need for improved long-form audio understanding in LMs and serves as a framework for future development."}}
{"id": "2505.02862", "pdf": "https://arxiv.org/pdf/2505.02862", "abs": "https://arxiv.org/abs/2505.02862", "authors": ["Haoming Yang", "Ke Ma", "Xiaojun Jia", "Yingfei Sun", "Qianqian Xu", "Qingming Huang"], "title": "Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the remarkable performance of Large Language Models (LLMs), they\nremain vulnerable to jailbreak attacks, which can compromise their safety\nmechanisms. Existing studies often rely on brute-force optimization or manual\ndesign, failing to uncover potential risks in real-world scenarios. To address\nthis, we propose a novel jailbreak attack framework, ICRT, inspired by\nheuristics and biases in human cognition. Leveraging the simplicity effect, we\nemploy cognitive decomposition to reduce the complexity of malicious prompts.\nSimultaneously, relevance bias is utilized to reorganize prompts, enhancing\nsemantic alignment and inducing harmful outputs effectively. Furthermore, we\nintroduce a ranking-based harmfulness evaluation metric that surpasses the\ntraditional binary success-or-failure paradigm by employing ranking aggregation\nmethods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify\nthe harmfulness of generated content. Experimental results show that our\napproach consistently bypasses mainstream LLMs' safety mechanisms and generates\nhigh-risk content, providing insights into jailbreak attack risks and\ncontributing to stronger defense strategies.", "AI": {"tldr": "Proposes ICRT, a novel jailbreak attack framework for LLMs using human cognition biases, outperforming existing methods and offering insights for defense.", "motivation": "Address vulnerabilities in LLMs' safety mechanisms by uncovering real-world risks through cognitive-inspired attacks.", "method": "Uses cognitive decomposition (simplicity effect) and relevance bias to reorganize prompts, with a ranking-based harmfulness metric.", "result": "Consistently bypasses LLMs' safety, generating high-risk content, and provides insights for defense.", "conclusion": "ICRT highlights jailbreak risks and aids in developing stronger defense strategies."}}
{"id": "2505.03244", "pdf": "https://arxiv.org/pdf/2505.03244", "abs": "https://arxiv.org/abs/2505.03244", "authors": ["Yu-Ren Guo", "Wen-Kai Tai"], "title": "SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival Augmented Generation", "categories": ["cs.SD", "eess.AS"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language processing (NLP) and multimodal learning, with successful\napplications in text generation and speech synthesis, enabling a deeper\nunderstanding and generation of multimodal content. In the field of sound\neffects (SFX) generation, LLMs have been leveraged to orchestrate multiple\nmodels for audio synthesis. However, due to the scarcity of annotated datasets,\nand the complexity of temproal modeling. current SFX generation techniques\nstill fall short in achieving high-fidelity audio. To address these\nlimitations, this paper introduces a novel framework that integrates LLMs with\nexisting sound effect databases, allowing for the retrieval, recombination, and\nsynthesis of audio based on user requirements. By leveraging this approach, we\nenhance the diversity and quality of generated sound effects while eliminating\nthe need for additional recording costs, offering a flexible and efficient\nsolution for sound design and application.", "AI": {"tldr": "A novel framework integrates LLMs with sound effect databases to improve SFX generation, addressing dataset scarcity and temporal modeling challenges.", "motivation": "Current SFX generation lacks high-fidelity audio due to limited annotated datasets and complex temporal modeling.", "method": "The framework combines LLMs with existing sound databases for retrieval, recombination, and synthesis.", "result": "Enhanced diversity and quality of sound effects, reducing the need for costly recordings.", "conclusion": "The approach offers a flexible, efficient solution for sound design."}}
{"id": "2505.02971", "pdf": "https://arxiv.org/pdf/2505.02971", "abs": "https://arxiv.org/abs/2505.02971", "authors": ["Anjila Budathoki", "Manish Dhakal"], "title": "Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks have been fairly explored for computer vision and\nvision-language models. However, the avenue of adversarial attack for the\nvision language segmentation models (VLSMs) is still under-explored, especially\nfor medical image analysis.\n  Thus, we have investigated the robustness of VLSMs against adversarial\nattacks for 2D medical images with different modalities with radiology,\nphotography, and endoscopy. The main idea of this project was to assess the\nrobustness of the fine-tuned VLSMs specially in the medical domain setting to\naddress the high risk scenario.\n  First, we have fine-tuned pre-trained VLSMs for medical image segmentation\nwith adapters.\n  Then, we have employed adversarial attacks -- projected gradient descent\n(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to\ndetermine its robustness against adversaries.\n  We have reported models' performance decline to analyze the adversaries'\nimpact.\n  The results exhibit significant drops in the DSC and IoU scores after the\nintroduction of these adversaries. Furthermore, we also explored universal\nperturbation but were not able to find for the medical images.\n  \\footnote{https://github.com/anjilab/secure-private-ai}", "AI": {"tldr": "The paper explores adversarial attacks on vision-language segmentation models (VLSMs) for medical images, assessing their robustness using PGD and FGSM attacks. Results show significant performance drops in DSC and IoU scores.", "motivation": "To address the under-explored robustness of VLSMs against adversarial attacks in medical image analysis, especially in high-risk scenarios.", "method": "Fine-tuned pre-trained VLSMs for medical image segmentation with adapters, then applied PGD and FGSM adversarial attacks to evaluate robustness.", "result": "Significant performance decline in DSC and IoU scores after adversarial attacks; universal perturbation was not found for medical images.", "conclusion": "VLSMs are vulnerable to adversarial attacks in medical settings, highlighting the need for robust defenses in high-risk applications."}}
{"id": "2505.02881", "pdf": "https://arxiv.org/pdf/2505.02881", "abs": "https://arxiv.org/abs/2505.02881", "authors": ["Kazuki Fujii", "Yukito Tajima", "Sakae Mizuki", "Hinari Shimada", "Taihei Shiotani", "Koshiro Saito", "Masanari Ohi", "Masaki Kawamura", "Taishi Nakamura", "Takumi Okamoto", "Shigeki Ishida", "Kakeru Hattori", "Youmi Ma", "Hiroya Takamura", "Rio Yokota", "Naoaki Okazaki"], "title": "Rewriting Pre-Training Data Boosts LLM Performance in Math and Code", "categories": ["cs.LG", "cs.AI"], "comment": "27pages(including appendix), 10 figures", "summary": "The performance of large language models (LLMs) in program synthesis and\nmathematical reasoning is fundamentally limited by the quality of their\npre-training corpora. We introduce two openly licensed datasets, released under\nthe Llama 3.3 Community License, that significantly enhance LLM performance by\nsystematically rewriting public data. SwallowCode (approximately 16.1 billion\ntokens) refines Python snippets from The-Stack-v2 through a novel four-stage\npipeline: syntax validation, pylint-based style filtering, and a two-stage LLM\nrewriting process that enforces style conformity and transforms snippets into\nself-contained, algorithmically efficient examples. Unlike prior methods that\nrely on exclusionary filtering or limited transformations, our\ntransform-and-retain approach upgrades low-quality code, maximizing data\nutility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by\nremoving boilerplate, restoring context, and reformatting solutions into\nconcise, step-by-step explanations. Within a fixed 50 billion token training\nbudget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1\nby +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing\nthe baseline model's code generation capabilities. Similarly, substituting\nSwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies\nconfirm that each pipeline stage contributes incrementally, with rewriting\ndelivering the largest gains. All datasets, prompts, and checkpoints are\npublicly available, enabling reproducible research and advancing LLM\npre-training for specialized domains.", "AI": {"tldr": "The paper introduces SwallowCode and SwallowMath datasets to improve LLM performance in program synthesis and math reasoning by refining public data through systematic rewriting.", "motivation": "The quality of pre-training corpora limits LLM performance in specialized tasks like coding and math.", "method": "A four-stage pipeline (syntax validation, style filtering, LLM rewriting) for SwallowCode and context restoration/reformatting for SwallowMath.", "result": "SwallowCode boosts HumanEval pass@1 by +17.0, and SwallowMath improves GSM8K accuracy by +12.4.", "conclusion": "The datasets enhance LLM capabilities, with all materials publicly available for reproducibility."}}
{"id": "2505.03108", "pdf": "https://arxiv.org/pdf/2505.03108", "abs": "https://arxiv.org/abs/2505.03108", "authors": ["Brendan Campbell", "Alan Williams", "Kleio Baxevani", "Alyssa Campbell", "Rushabh Dhoke", "Rileigh E. Hudock", "Xiaomin Lin", "Vivek Mange", "Bernhard Neuberger", "Arjun Suresh", "Alhim Vera", "Arthur Trembanis", "Herbert G. Tanner", "Edward Hale"], "title": "Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE", "categories": ["cs.AI"], "comment": null, "summary": "Oysters are ecologically and commercially important species that require\nfrequent monitoring to track population demographics (e.g. abundance, growth,\nmortality). Current methods of monitoring oyster reefs often require\ndestructive sampling methods and extensive manual effort. Therefore, they are\nsuboptimal for small-scale or sensitive environments. A recent alternative, the\nODYSSEE model, was developed to use deep learning techniques to identify live\noysters using video or images taken in the field of oyster reefs to assess\nabundance. The validity of this model in identifying live oysters on a reef was\ncompared to expert and non-expert annotators. In addition, we identified\npotential sources of prediction error. Although the model can make inferences\nsignificantly faster than expert and non-expert annotators (39.6 s, $2.34 \\pm\n0.61$ h, $4.50 \\pm 1.46$ h, respectively), the model overpredicted the number\nof live oysters, achieving lower accuracy (63\\%) in identifying live oysters\ncompared to experts (74\\%) and non-experts (75\\%) alike. Image quality was an\nimportant factor in determining the accuracy of the model and the annotators.\nBetter quality images improved human accuracy and worsened model accuracy.\nAlthough ODYSSEE was not sufficiently accurate, we anticipate that future\ntraining on higher-quality images, utilizing additional live imagery, and\nincorporating additional annotation training classes will greatly improve the\nmodel's predictive power based on the results of this analysis. Future research\nshould address methods that improve the detection of living vs. dead oysters.", "AI": {"tldr": "The paper evaluates the ODYSSEE model, a deep learning tool for identifying live oysters in images, comparing its performance to human annotators. While faster, the model was less accurate, with image quality affecting results. Future improvements are suggested.", "motivation": "Current oyster monitoring methods are destructive and labor-intensive, making them unsuitable for sensitive environments. The ODYSSEE model offers a non-destructive, efficient alternative.", "method": "The study compares the ODYSSEE model's performance in identifying live oysters to expert and non-expert annotators, analyzing prediction errors and the impact of image quality.", "result": "The model was faster (39.6 s) but less accurate (63%) than humans (experts: 74%, non-experts: 75%). Image quality influenced accuracy, improving human performance but worsening the model's.", "conclusion": "ODYSSEE shows promise but needs refinement, such as training on higher-quality images and additional annotation classes, to improve accuracy for future use."}}
{"id": "2505.03480", "pdf": "https://arxiv.org/pdf/2505.03480", "abs": "https://arxiv.org/abs/2505.03480", "authors": ["Lilian Marey", "Charlotte Laclau", "Bruno Sguerra", "Tiphaine Viard", "Manuel Moussallam"], "title": "Modeling Musical Genre Trajectories through Pathlet Learning", "categories": ["cs.IR", "cs.LG", "cs.MM"], "comment": "Adjunct Proceedings of the 33rd ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP Adjunct '25)", "summary": "The increasing availability of user data on music streaming platforms opens\nup new possibilities for analyzing music consumption. However, understanding\nthe evolution of user preferences remains a complex challenge, particularly as\ntheir musical tastes change over time. This paper uses the dictionary learning\nparadigm to model user trajectories across different musical genres. We define\na new framework that captures recurring patterns in genre trajectories, called\npathlets, enabling the creation of comprehensible trajectory embeddings. We\nshow that pathlet learning reveals relevant listening patterns that can be\nanalyzed both qualitatively and quantitatively. This work improves our\nunderstanding of users' interactions with music and opens up avenues of\nresearch into user behavior and fostering diversity in recommender systems. A\ndataset of 2000 user histories tagged by genre over 17 months, supplied by\nDeezer (a leading music streaming company), is also released with the code.", "AI": {"tldr": "The paper introduces a dictionary learning-based framework to model user music genre preferences over time, identifying recurring patterns (pathlets) for better analysis and recommender system improvements.", "motivation": "To understand evolving user music preferences and improve recommender systems by analyzing genre trajectories.", "method": "Uses dictionary learning to model user genre trajectories, identifying pathlets as recurring patterns for creating trajectory embeddings.", "result": "Pathlet learning reveals meaningful listening patterns, enabling qualitative and quantitative analysis. A dataset of 2000 user histories is released.", "conclusion": "The framework enhances understanding of user music interactions and supports research into behavior and diversity in recommender systems."}}
{"id": "2505.03261", "pdf": "https://arxiv.org/pdf/2505.03261", "abs": "https://arxiv.org/abs/2505.03261", "authors": ["Wei-Ting Chen", "Yu-Jiet Vong", "Yi-Tsung Lee", "Sy-Yen Kuo", "Qiang Gao", "Sizhuo Ma", "Jian Wang"], "title": "DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Video Quality Assessment (VQA) aims to evaluate video quality based on\nperceptual distortions and human preferences. Despite the promising performance\nof existing methods using Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViTs), they often struggle to align closely with human\nperceptions, particularly in diverse real-world scenarios. This challenge is\nexacerbated by the limited scale and diversity of available datasets. To\naddress this limitation, we introduce a novel VQA framework, DiffVQA, which\nharnesses the robust generalization capabilities of diffusion models\npre-trained on extensive datasets. Our framework adapts these models to\nreconstruct identical input frames through a control module. The adapted\ndiffusion model is then used to extract semantic and distortion features from a\nresizing branch and a cropping branch, respectively. To enhance the model's\nability to handle long-term temporal dynamics, a parallel Mamba module is\nintroduced, which extracts temporal coherence augmented features that are\nmerged with the diffusion features to predict the final score. Experiments\nacross multiple datasets demonstrate DiffVQA's superior performance on\nintra-dataset evaluations and its exceptional generalization across datasets.\nThese results confirm that leveraging a diffusion model as a feature extractor\ncan offer enhanced VQA performance compared to CNN and ViT backbones.", "AI": {"tldr": "DiffVQA is a novel VQA framework using diffusion models for feature extraction, outperforming CNNs and ViTs by better aligning with human perceptions and handling diverse scenarios.", "motivation": "Existing VQA methods (CNNs/ViTs) struggle to align with human perceptions in diverse scenarios due to limited dataset scale and diversity.", "method": "DiffVQA adapts pre-trained diffusion models for frame reconstruction, extracts semantic/distortion features, and integrates a Mamba module for temporal dynamics.", "result": "DiffVQA shows superior performance in intra-dataset evaluations and exceptional generalization across datasets.", "conclusion": "Diffusion models as feature extractors enhance VQA performance over traditional backbones like CNNs and ViTs."}}
{"id": "2505.03073", "pdf": "https://arxiv.org/pdf/2505.03073", "abs": "https://arxiv.org/abs/2505.03073", "authors": ["Eric Easthope"], "title": "Coupling the Heart to Musical Machines", "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Biofeedback is being used more recently as a general control paradigm for\nhuman-computer interfaces (HCIs). While biofeedback especially from breath has\nseen increasing uptake as a controller for novel musical interfaces, new\ninterfaces for musical expression (NIMEs), the community has not given as much\nattention to the heart. The heart is just as intimate a part of music as breath\nand it is argued that the heart determines our perception of time and so\nindirectly our perception of music. Inspired by this I demonstrate a\nphotoplethysmogram (PPG)-based NIME controller using heart rate as a 1D control\nparameter to transform the qualities of sounds in real-time over a Bluetooth\nwireless HCI. I apply time scaling to \"warp\" audio buffers inbound to the sound\ncard, and play these transformed audio buffers back to the listener wearing the\nPPG sensor, creating a hypothetical perceptual biofeedback loop: changes in\nsound change heart rate to change PPG measurements to change sound. I discuss\nhow a sound-heart-PPG biofeedback loop possibly affords greater control and/or\nvariety of movements with a 1D controller, how controlling the space and/or\ntime scale of sound playback with biofeedback makes for possibilities in\nperformance ambience, and I briefly discuss generative latent spaces as a\npossible way to extend a 1D PPG control space.", "AI": {"tldr": "The paper explores using heart rate via PPG as a 1D control for NIMEs, creating a biofeedback loop between sound and heart rate, and discusses its potential for performance and control variety.", "motivation": "The heart's role in music perception is understudied compared to breath, and its intimate connection to time perception makes it a compelling control parameter for HCIs.", "method": "A PPG-based NIME controller uses heart rate to transform sound in real-time via time scaling, creating a biofeedback loop between sound and heart rate.", "result": "The system demonstrates how heart rate can dynamically alter sound, suggesting potential for enhanced control and performance ambience.", "conclusion": "Heart rate biofeedback offers novel possibilities for NIMEs, with future extensions like generative latent spaces to enrich the 1D control paradigm."}}
{"id": "2505.02861", "pdf": "https://arxiv.org/pdf/2505.02861", "abs": "https://arxiv.org/abs/2505.02861", "authors": ["Kushagra Agrawal", "Nisharg Nargund"], "title": "Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments", "categories": ["cs.MA", "cs.AI", "cs.NE"], "comment": null, "summary": "Multi-agent systems (MAS) are foundational in simulating complex real-world\nscenarios involving autonomous, interacting entities. However, traditional MAS\narchitectures often suffer from rigid coordination mechanisms and difficulty\nadapting to dynamic tasks. We propose MetaOrch, a neural orchestration\nframework for optimal agent selection in multi-domain task environments. Our\nsystem implements a supervised learning approach that models task context,\nagent histories, and expected response quality to select the most appropriate\nagent for each task. A novel fuzzy evaluation module scores agent responses\nalong completeness, relevance, and confidence dimensions, generating soft\nsupervision labels for training the orchestrator. Unlike previous methods that\nhard-code agent-task mappings, MetaOrch dynamically predicts the most suitable\nagent while estimating selection confidence. Experiments in simulated\nenvironments with heterogeneous agents demonstrate that our approach achieves\n86.3% selection accuracy, significantly outperforming baseline strategies\nincluding random selection and round-robin scheduling. The modular architecture\nemphasizes extensibility, allowing agents to be registered, updated, and\nqueried independently. Results suggest that neural orchestration offers a\npowerful approach to enhancing the autonomy, interpretability, and adaptability\nof multi-agent systems across diverse task domains.", "AI": {"tldr": "MetaOrch is a neural orchestration framework for dynamic agent selection in multi-agent systems, outperforming baselines with 86.3% accuracy.", "motivation": "Traditional MAS architectures lack adaptability and flexible coordination.", "method": "Supervised learning models task context, agent histories, and response quality, with a fuzzy evaluation module for soft supervision.", "result": "Achieves 86.3% selection accuracy, surpassing random and round-robin baselines.", "conclusion": "Neural orchestration enhances MAS autonomy, interpretability, and adaptability."}}
{"id": "2505.02865", "pdf": "https://arxiv.org/pdf/2505.02865", "abs": "https://arxiv.org/abs/2505.02865", "authors": ["Zhihai Wang", "Jie Wang", "Jilai Pan", "Xilin Xia", "Huiling Zhen", "Mingxuan Yuan", "Jianye Hao", "Feng Wu"], "title": "Accelerating Large Language Model Reasoning via Speculative Search", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML2025", "summary": "Tree-search-based reasoning methods have significantly enhanced the reasoning\ncapability of large language models (LLMs) by facilitating the exploration of\nmultiple intermediate reasoning steps, i.e., thoughts. However, these methods\nsuffer from substantial inference latency, as they have to generate numerous\nreasoning thoughts, severely limiting LLM applicability. To address this\nchallenge, we propose a novel Speculative Search (SpecSearch) framework that\nsignificantly accelerates LLM reasoning by optimizing thought generation.\nSpecifically, SpecSearch utilizes a small model to strategically collaborate\nwith a large model at both thought and token levels, efficiently generating\nhigh-quality reasoning thoughts. The major pillar of SpecSearch is a novel\nquality-preserving rejection mechanism, which effectively filters out thoughts\nwhose quality falls below that of the large model's outputs. Moreover, we show\nthat SpecSearch preserves comparable reasoning quality to the large model.\nExperiments on both the Qwen and Llama models demonstrate that SpecSearch\nsignificantly outperforms state-of-the-art approaches, achieving up to\n2.12$\\times$ speedup with comparable reasoning quality.", "AI": {"tldr": "Speculative Search (SpecSearch) accelerates LLM reasoning by using a small model to collaborate with a large model, preserving reasoning quality while reducing latency.", "motivation": "Tree-search-based reasoning methods for LLMs suffer from high inference latency due to generating numerous thoughts, limiting applicability.", "method": "SpecSearch uses a small model to collaborate with a large model at thought and token levels, employing a quality-preserving rejection mechanism to filter low-quality thoughts.", "result": "SpecSearch achieves up to 2.12\u00d7 speedup with comparable reasoning quality, outperforming state-of-the-art methods.", "conclusion": "SpecSearch effectively reduces latency while maintaining reasoning quality, enhancing LLM applicability."}}
{"id": "2505.03273", "pdf": "https://arxiv.org/pdf/2505.03273", "abs": "https://arxiv.org/abs/2505.03273", "authors": ["Zhaoxi Mu", "Xinyu Yang", "Gang Wang"], "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Appears in IJCAI 2025", "summary": "While contemporary speech separation technologies adeptly process lengthy\nmixed audio waveforms, they are frequently challenged by the intricacies of\nreal-world environments, including noisy and reverberant settings, which can\nresult in artifacts or distortions in the separated speech. To overcome these\nlimitations, we introduce SepALM, a pioneering approach that employs audio\nlanguage models (ALMs) to rectify and re-synthesize speech within the text\ndomain following preliminary separation. SepALM comprises four core components:\na separator, a corrector, a synthesizer, and an aligner. By integrating an\nALM-based end-to-end error correction mechanism, we mitigate the risk of error\naccumulation and circumvent the optimization hurdles typically encountered in\nconventional methods that amalgamate automatic speech recognition (ASR) with\nlarge language models (LLMs). Additionally, we have developed Chain-of-Thought\n(CoT) prompting and knowledge distillation techniques to facilitate the\nreasoning and training processes of the ALM. Our experiments substantiate that\nSepALM not only elevates the precision of speech separation but also markedly\nbolsters adaptability in novel acoustic environments.", "AI": {"tldr": "SepALM introduces an ALM-based approach to improve speech separation in noisy environments by correcting and re-synthesizing speech in the text domain, outperforming traditional methods.", "motivation": "Addressing the challenges of real-world noisy and reverberant environments that cause artifacts in separated speech, aiming for higher precision and adaptability.", "method": "SepALM uses a separator, corrector, synthesizer, and aligner, integrating ALM-based error correction, CoT prompting, and knowledge distillation to avoid error accumulation.", "result": "SepALM enhances speech separation accuracy and adaptability in new acoustic settings.", "conclusion": "SepALM is a promising solution for robust speech separation in challenging environments, leveraging ALMs for superior performance."}}
{"id": "2505.02980", "pdf": "https://arxiv.org/pdf/2505.02980", "abs": "https://arxiv.org/abs/2505.02980", "authors": ["Daniela Ruiz", "Paula Cardenas", "Leonardo Manrique", "Daniela Vega", "Gabriel Mejia", "Pablo Arbelaez"], "title": "Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.13027", "summary": "Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.", "AI": {"tldr": "SpaRED introduces a standardized database for gene expression prediction from histology images, while SpaCKLE, a transformer-based model, reduces errors by 82.5%. The benchmark evaluates eight models, showing SpaCKLE's superiority.", "motivation": "Address limitations of Visium (cost, expertise, inefficiency) and inconsistencies in datasets and models for gene expression prediction.", "method": "Create SpaRED, a curated database of 26 datasets, and develop SpaCKLE, a transformer-based model for gene expression completion.", "result": "SpaCKLE reduces mean squared error by 82.5% and improves all evaluated models' performance.", "conclusion": "SpaRED and SpaCKLE provide a comprehensive benchmark and tool for advancing Spatial Transcriptomics research."}}
{"id": "2505.02884", "pdf": "https://arxiv.org/pdf/2505.02884", "abs": "https://arxiv.org/abs/2505.02884", "authors": ["Guangzhi Sun", "Potsawee Manakul", "Xiao Zhan", "Mark Gales"], "title": "Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Unlearning has emerged as a critical capability for large language models\n(LLMs) to support data privacy, regulatory compliance, and ethical AI\ndeployment. Recent techniques often rely on obfuscation by injecting incorrect\nor irrelevant information to suppress knowledge. Such methods effectively\nconstitute knowledge addition rather than true removal, often leaving models\nvulnerable to probing. In this paper, we formally distinguish unlearning from\nobfuscation and introduce a probing-based evaluation framework to assess\nwhether existing approaches genuinely remove targeted information. Moreover, we\npropose DF-MCQ, a novel unlearning method that flattens the model predictive\ndistribution over automatically generated multiple-choice questions using\nKL-divergence, effectively removing knowledge about target individuals and\ntriggering appropriate refusal behaviour. Experimental results demonstrate that\nDF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level\nuncertainty that is much higher than obfuscation on probing questions.", "AI": {"tldr": "The paper distinguishes unlearning from obfuscation in LLMs, proposes DF-MCQ for true knowledge removal, and validates its effectiveness with high refusal rates and uncertainty.", "motivation": "Addressing the need for true knowledge removal in LLMs to ensure privacy, compliance, and ethical AI, as current methods often obfuscate rather than unlearn.", "method": "Introduces DF-MCQ, which flattens model predictive distributions over multiple-choice questions using KL-divergence to remove targeted knowledge.", "result": "DF-MCQ achieves over 90% refusal rate and higher uncertainty than obfuscation methods, proving effective unlearning.", "conclusion": "DF-MCQ provides a robust solution for true unlearning in LLMs, outperforming obfuscation-based approaches."}}
{"id": "2505.03135", "pdf": "https://arxiv.org/pdf/2505.03135", "abs": "https://arxiv.org/abs/2505.03135", "authors": ["Haoran Ou", "Gelei Deng", "Xingshuo Han", "Jie Zhang", "Xinlei He", "Han Qiu", "Shangwei Guo", "Tianwei Zhang"], "title": "Holmes: Automated Fact Check with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "The rise of Internet connectivity has accelerated the spread of\ndisinformation, threatening societal trust, decision-making, and national\nsecurity. Disinformation has evolved from simple text to complex multimodal\nforms combining images and text, challenging existing detection methods.\nTraditional deep learning models struggle to capture the complexity of\nmultimodal disinformation. Inspired by advances in AI, this study explores\nusing Large Language Models (LLMs) for automated disinformation detection. The\nempirical study shows that (1) LLMs alone cannot reliably assess the\ntruthfulness of claims; (2) providing relevant evidence significantly improves\ntheir performance; (3) however, LLMs cannot autonomously search for accurate\nevidence. To address this, we propose Holmes, an end-to-end framework featuring\na novel evidence retrieval method that assists LLMs in collecting high-quality\nevidence. Our approach uses (1) LLM-powered summarization to extract key\ninformation from open sources and (2) a new algorithm and metrics to evaluate\nevidence quality. Holmes enables LLMs to verify claims and generate\njustifications effectively. Experiments show Holmes achieves 88.3% accuracy on\ntwo open-source datasets and 90.2% in real-time verification tasks. Notably,\nour improved evidence retrieval boosts fact-checking accuracy by 30.8% over\nexisting methods", "AI": {"tldr": "The paper proposes Holmes, an end-to-end framework using LLMs and evidence retrieval to improve multimodal disinformation detection, achieving high accuracy.", "motivation": "The spread of complex multimodal disinformation challenges existing detection methods, requiring advanced solutions like LLMs.", "method": "Holmes combines LLM-powered summarization and a novel evidence retrieval algorithm to verify claims and generate justifications.", "result": "Holmes achieves 88.3% accuracy on datasets and 90.2% in real-time tasks, with a 30.8% improvement in fact-checking accuracy.", "conclusion": "Holmes effectively enhances LLM-based disinformation detection by improving evidence retrieval and verification."}}
{"id": "2505.03603", "pdf": "https://arxiv.org/pdf/2505.03603", "abs": "https://arxiv.org/abs/2505.03603", "authors": ["Y. B. Wang", "S. Z. Zhou", "J. F. Wu", "T. Hu", "J. N. Zhang", "Y. Liu"], "title": "PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Audio-driven human animation technology is widely used in human-computer\ninteraction, and the emergence of diffusion models has further advanced its\ndevelopment. Currently, most methods rely on multi-stage generation and\nintermediate representations, resulting in long inference time and issues with\ngeneration quality in specific foreground regions and audio-motion consistency.\nThese shortcomings are primarily due to the lack of localized fine-grained\nsupervised guidance. To address above challenges, we propose PAHA, an\nend-to-end audio-driven upper-body human animation framework with diffusion\nmodel. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts\nConsistency Enhancement (PCE). PAR dynamically adjusts regional training loss\nweights based on pose confidence scores, effectively improving visual quality.\nPCE constructs and trains diffusion-based regional audio-visual classifiers to\nimprove the consistency of motion and co-speech audio. Afterwards, we design\ntwo novel inference guidance methods for the foregoing classifiers, Sequential\nGuidance (SG) and Differential Guidance (DG), to balance efficiency and quality\nrespectively. Additionally, we build CNAS, the first public Chinese News Anchor\nSpeech dataset, to advance research and validation in this field. Extensive\nexperimental results and user studies demonstrate that PAHA significantly\noutperforms existing methods in audio-motion alignment and video-related\nevaluations. The codes and CNAS dataset will be released upon acceptance.", "AI": {"tldr": "PAHA is an end-to-end audio-driven upper-body human animation framework using diffusion models, addressing quality and consistency issues with PAR and PCE methods, and introducing CNAS dataset.", "motivation": "Current methods suffer from long inference times, poor generation quality in specific regions, and inconsistent audio-motion alignment due to lack of fine-grained supervision.", "method": "Proposes PAR for dynamic loss adjustment and PCE for audio-visual consistency. Introduces SG and DG for inference guidance.", "result": "PAHA outperforms existing methods in audio-motion alignment and video evaluations, validated by experiments and user studies.", "conclusion": "PAHA advances audio-driven human animation with improved quality and consistency, supported by the new CNAS dataset."}}
{"id": "2505.03327", "pdf": "https://arxiv.org/pdf/2505.03327", "abs": "https://arxiv.org/abs/2505.03327", "authors": ["Jos\u00e9-Luis Bueso-Bello", "Benjamin Chauvel", "Daniel Carcereri", "Philipp Posovszky", "Pietro Milillo", "Jennifer Ruiz", "Juan-Carlos Fern\u00e1ndez-Diaz", "Carolina Gonz\u00e1lez", "Michele Martone", "Ronny H\u00e4nsch", "Paola Rizzoli"], "title": "Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "Preprint submitted to Remote Sensing of Environment", "summary": "Deep learning models have shown encouraging capabilities for mapping\naccurately forests at medium resolution with TanDEM-X interferometric SAR data.\nSuch models, as most of current state-of-the-art deep learning techniques in\nremote sensing, are trained in a fully-supervised way, which requires a large\namount of labeled data for training and validation. In this work, our aim is to\nexploit the high-resolution capabilities of the TanDEM-X mission to map forests\nat 6 m. The goal is to overcome the intrinsic limitations posed by\nmidresolution products, which affect, e.g., the detection of narrow roads\nwithin vegetated areas and the precise delineation of forested regions\ncontours. To cope with the lack of extended reliable reference datasets at such\na high resolution, we investigate self-supervised learning techniques for\nextracting highly informative representations from the input features, followed\nby a supervised training step with a significantly smaller number of reliable\nlabels. A 1 m resolution forest/non-forest reference map over Pennsylvania,\nUSA, allows for comparing different training approaches for the development of\nan effective forest mapping framework with limited labeled samples. We select\nthe best-performing approach over this test region and apply it in a real-case\nforest mapping scenario over the Amazon rainforest, where only very few labeled\ndata at high resolution are available. In this challenging scenario, the\nproposed self-supervised framework significantly enhances the classification\naccuracy with respect to fully-supervised methods, trained using the same\namount of labeled data, representing an extremely promising starting point for\nlarge-scale, very high-resolution forest mapping with TanDEM-X data.", "AI": {"tldr": "The paper proposes a self-supervised learning framework to map forests at 6m resolution using TanDEM-X SAR data, reducing reliance on large labeled datasets and outperforming fully-supervised methods in accuracy.", "motivation": "To overcome limitations of mid-resolution forest mapping (e.g., detecting narrow roads, precise delineation) and address the lack of high-resolution labeled data.", "method": "Investigates self-supervised learning for feature extraction, followed by supervised training with fewer labels. Validated using a 1m resolution reference map in Pennsylvania and applied to the Amazon rainforest.", "result": "The self-supervised framework significantly improves classification accuracy compared to fully-supervised methods with the same labeled data.", "conclusion": "The approach is promising for large-scale, high-resolution forest mapping with limited labeled data."}}
{"id": "2505.03314", "pdf": "https://arxiv.org/pdf/2505.03314", "abs": "https://arxiv.org/abs/2505.03314", "authors": ["Jincheng Zhang", "Gy\u00f6rgy Fazekas", "Charalampos Saitis"], "title": "Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "The recent surge in the popularity of diffusion models for image synthesis\nhas attracted new attention to their potential for generation tasks in other\ndomains. However, their applications to symbolic music generation remain\nlargely under-explored because symbolic music is typically represented as\nsequences of discrete events and standard diffusion models are not well-suited\nfor discrete data. We represent symbolic music as image-like pianorolls,\nfacilitating the use of diffusion models for the generation of symbolic music.\nMoreover, this study introduces a novel diffusion model that incorporates our\nproposed Transformer-Mamba block and learnable wavelet transform.\nClassifier-free guidance is utilised to generate symbolic music with target\nchords. Our evaluation shows that our method achieves compelling results in\nterms of music quality and controllability, outperforming the strong baseline\nin pianoroll generation. Our code is available at\nhttps://github.com/jinchengzhanggg/proffusion.", "AI": {"tldr": "A novel diffusion model for symbolic music generation using image-like pianorolls, incorporating Transformer-Mamba blocks and learnable wavelet transforms, outperforming baselines in quality and controllability.", "motivation": "Diffusion models are under-explored for symbolic music due to discrete data challenges. This work adapts them for music by representing it as pianorolls.", "method": "Symbolic music is represented as pianorolls, enabling diffusion models. A Transformer-Mamba block and learnable wavelet transform are introduced, with classifier-free guidance for chord-targeted generation.", "result": "The method achieves superior music quality and controllability, outperforming baselines in pianoroll generation.", "conclusion": "The proposed approach effectively adapts diffusion models for symbolic music, demonstrating strong performance and potential for further applications."}}
{"id": "2505.03096", "pdf": "https://arxiv.org/pdf/2505.03096", "abs": "https://arxiv.org/abs/2505.03096", "authors": ["Joshua Owotogbe"], "title": "Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering", "categories": ["cs.MA", "cs.AI", "cs.SE"], "comment": null, "summary": "This study explores the application of chaos engineering to enhance the\nrobustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in\nproduction-like environments under real-world conditions. LLM-MAS can\npotentially improve a wide range of tasks, from answering questions and\ngenerating content to automating customer support and improving decision-making\nprocesses. However, LLM-MAS in production or preproduction environments can be\nvulnerable to emergent errors or disruptions, such as hallucinations, agent\nfailures, and agent communication failures. This study proposes a chaos\nengineering framework to proactively identify such vulnerabilities in LLM-MAS,\nassess and build resilience against them, and ensure reliable performance in\ncritical applications.", "AI": {"tldr": "A chaos engineering framework is proposed to improve the robustness of LLM-MAS by identifying vulnerabilities and ensuring reliable performance.", "motivation": "LLM-MAS can enhance various tasks but are prone to errors like hallucinations and communication failures in real-world conditions.", "method": "Proposes a chaos engineering framework to test and build resilience in LLM-MAS.", "result": "The framework aims to proactively identify and mitigate vulnerabilities.", "conclusion": "Chaos engineering can enhance the reliability of LLM-MAS in critical applications."}}
{"id": "2505.02872", "pdf": "https://arxiv.org/pdf/2505.02872", "abs": "https://arxiv.org/abs/2505.02872", "authors": ["Cfir Avraham Hadar", "Omer Shubi", "Yoav Meiri", "Yevgeni Berzak"], "title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "When reading, we often have specific information that interests us in a text.\nFor example, you might be reading this paper because you are curious about LLMs\nfor eye movements in reading, the experimental design, or perhaps you only care\nabout the question ``but does it work?''. More broadly, in daily life, people\napproach texts with any number of text-specific goals that guide their reading\nbehavior. In this work, we ask, for the first time, whether open-ended reading\ngoals can be automatically decoded from eye movements in reading. To address\nthis question, we introduce goal classification and goal reconstruction tasks\nand evaluation frameworks, and use large-scale eye tracking for reading data in\nEnglish with hundreds of text-specific information seeking tasks. We develop\nand compare several discriminative and generative multimodal LLMs that combine\neye movements and text for goal classification and goal reconstruction. Our\nexperiments show considerable success on both tasks, suggesting that LLMs can\nextract valuable information about the readers' text-specific goals from eye\nmovements.", "AI": {"tldr": "The paper explores whether open-ended reading goals can be decoded from eye movements using multimodal LLMs, achieving success in goal classification and reconstruction tasks.", "motivation": "People read with diverse, text-specific goals, but it's unclear if these goals can be inferred from eye movements. This work investigates this for the first time.", "method": "Introduces goal classification and reconstruction tasks, using large-scale eye-tracking data and multimodal LLMs combining eye movements and text.", "result": "Experiments show significant success in decoding reading goals from eye movements, demonstrating LLMs' capability in this domain.", "conclusion": "LLMs can effectively extract information about readers' text-specific goals from eye movements, opening new research avenues."}}
{"id": "2505.03337", "pdf": "https://arxiv.org/pdf/2505.03337", "abs": "https://arxiv.org/abs/2505.03337", "authors": ["Bernardo Torres", "Geoffroy Peeters", "Gael Richard"], "title": "The Inverse Drum Machine: Source Separation Through Joint Transcription and Analysis-by-Synthesis", "categories": ["cs.SD", "eess.AS", "eess.SP", "stat.ML"], "comment": null, "summary": "We introduce the Inverse Drum Machine (IDM), a novel approach to drum source\nseparation that combines analysis-by-synthesis with deep learning. Unlike\nrecent supervised methods that rely on isolated stems, IDM requires only\ntranscription annotations. It jointly optimizes automatic drum transcription\nand one-shot drum sample synthesis in an end-to-end framework. By convolving\nsynthesized one-shot samples with estimated onsets-mimicking a drum machine-IDM\nreconstructs individual drum stems and trains a neural network to match the\noriginal mixture. Evaluations on the StemGMD dataset show that IDM achieves\nseparation performance on par with state-of-the-art supervised methods, while\nsubstantially outperforming matrix decomposition baselines.", "AI": {"tldr": "IDM is a drum source separation method combining analysis-by-synthesis and deep learning, requiring only transcription annotations. It matches state-of-the-art supervised methods.", "motivation": "To simplify drum source separation by avoiding the need for isolated stems, using only transcription annotations.", "method": "Combines automatic drum transcription and one-shot drum sample synthesis in an end-to-end framework, convolving synthesized samples with estimated onsets.", "result": "Achieves separation performance comparable to supervised methods and outperforms matrix decomposition baselines on the StemGMD dataset.", "conclusion": "IDM offers a promising alternative to supervised methods for drum source separation, reducing annotation requirements."}}
{"id": "2505.03007", "pdf": "https://arxiv.org/pdf/2505.03007", "abs": "https://arxiv.org/abs/2505.03007", "authors": ["Nikolay Safonov", "Alexey Bryncev", "Andrey Moskalenko", "Dmitry Kulikov", "Dmitry Vatolin", "Radu Timofte", "Haibo Lei", "Qifan Gao", "Qing Luo", "Yaqing Li", "Jie Song", "Shaozhe Hao", "Meisong Zheng", "Jingyi Xu", "Chengbin Wu", "Jiahui Liu", "Ying Chen", "Xin Deng", "Mai Xu", "Peipei Liang", "Jie Ma", "Junjie Jin", "Yingxue Pang", "Fangzhou Luo", "Kai Chen", "Shijie Zhao", "Mingyang Wu", "Renjie Li", "Yushen Zuo", "Shengyun Zhong", "Zhengzhong Tu"], "title": "NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an overview of the NTIRE 2025 Challenge on UGC Video\nEnhancement. The challenge constructed a set of 150 user-generated content\nvideos without reference ground truth, which suffer from real-world\ndegradations such as noise, blur, faded colors, compression artifacts, etc. The\ngoal of the participants was to develop an algorithm capable of improving the\nvisual quality of such videos. Given the widespread use of UGC on short-form\nvideo platforms, this task holds substantial practical importance. The\nevaluation was based on subjective quality assessment in crowdsourcing,\nobtaining votes from over 8000 assessors. The challenge attracted more than 25\nteams submitting solutions, 7 of which passed the final phase with source code\nverification. The outcomes may provide insights into the state-of-the-art in\nUGC video enhancement and highlight emerging trends and effective strategies in\nthis evolving research area. All data, including the processed videos and\nsubjective comparison votes and scores, is made publicly available at\nhttps://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.", "AI": {"tldr": "The NTIRE 2025 Challenge focused on enhancing user-generated content (UGC) videos with real-world degradations, attracting 25+ teams. Subjective evaluation involved 8000+ assessors, and 7 teams succeeded. Results and data are publicly available.", "motivation": "UGC videos on short-form platforms often suffer from quality issues like noise and blur, making enhancement practically significant.", "method": "Participants developed algorithms to improve UGC video quality. Evaluation used crowdsourced subjective assessments from 8000+ voters.", "result": "7 teams passed the final phase, providing insights into state-of-the-art UGC enhancement techniques.", "conclusion": "The challenge highlights effective strategies and trends in UGC video enhancement, with all data made public for further research."}}
{"id": "2505.02888", "pdf": "https://arxiv.org/pdf/2505.02888", "abs": "https://arxiv.org/abs/2505.02888", "authors": ["Rintaro Ando"], "title": "When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T05, 68Q85", "I.2.0; I.2.3; I.2.6"], "comment": "20 pages, 4 figures, 3 tables. Code:\n  github.com/rintaro-ando-tech/n2m-rsi-demo (v1.0)", "summary": "We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal\nformal model showing that once an AI agent feeds its own outputs back as inputs\nand crosses an explicit information-integration threshold, its internal\ncomplexity will grow without bound under our assumptions. The framework unifies\nearlier ideas on self-prompting large language models, G\\\"odelian\nself-reference, and AutoML, yet remains implementation-agnostic. The model\nfurthermore scales naturally to interacting swarms of agents, hinting at\nsuper-linear effects once communication among instances is permitted. For\nsafety reasons, we omit system-specific implementation details and release only\na brief, model-agnostic toy prototype in Appendix C.", "AI": {"tldr": "N2M-RSI is a formal model where an AI agent's internal complexity grows indefinitely when it feeds its outputs back as inputs, crossing an information-integration threshold. It unifies self-prompting, G\u00f6delian self-reference, and AutoML, and scales to multi-agent systems.", "motivation": "To demonstrate how recursive self-improvement in AI can lead to unbounded internal complexity under certain conditions, unifying existing concepts like self-prompting and G\u00f6delian self-reference.", "method": "The model involves an AI agent recursively feeding its outputs back as inputs, crossing an explicit information-integration threshold. It remains implementation-agnostic and scales to multi-agent systems.", "result": "The agent's internal complexity grows without bound under the given assumptions, with potential super-linear effects in multi-agent swarms.", "conclusion": "N2M-RSI provides a minimal yet powerful framework for understanding recursive self-improvement in AI, with implications for both theoretical and practical AI development, though safety concerns limit detailed implementation sharing."}}
{"id": "2505.03171", "pdf": "https://arxiv.org/pdf/2505.03171", "abs": "https://arxiv.org/abs/2505.03171", "authors": ["Junqi Liu", "Xiaohan Lin", "Jonas Bayer", "Yael Dillies", "Weijie Jiang", "Xiaodan Liang", "Roman Soletskyi", "Haiming Wang", "Yunzhou Xie", "Beibei Xiong", "Zhengfeng Yang", "Jujian Zhang", "Lihong Zhi", "Jia Li", "Zhengying Liu"], "title": "CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics", "categories": ["cs.AI"], "comment": null, "summary": "Neurosymbolic approaches integrating large language models with formal\nreasoning have recently achieved human-level performance on mathematics\ncompetition problems in algebra, geometry and number theory. In comparison,\ncombinatorics remains a challenging domain, characterized by a lack of\nappropriate benchmarks and theorem libraries. To address this gap, we introduce\nCombiBench, a comprehensive benchmark comprising 100 combinatorial problems,\neach formalized in Lean~4 and paired with its corresponding informal statement.\nThe problem set covers a wide spectrum of difficulty levels, ranging from\nmiddle school to IMO and university level, and span over ten combinatorial\ntopics. CombiBench is suitable for testing IMO solving capabilities since it\nincludes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its\nstatement contain an images). Furthermore, we provide a comprehensive and\nstandardized evaluation framework, dubbed Fine-Eval (for\n$\\textbf{F}$ill-in-the-blank $\\textbf{in}$ L$\\textbf{e}$an Evaluation), for\nformal mathematics. It accommodates not only proof-based problems but also, for\nthe first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval\nas the evaluation method and Kimina Lean Server as the backend, we benchmark\nseveral LLMs on CombiBench and observe that their capabilities for formally\nsolving combinatorial problems remain limited. Among all models tested (none of\nwhich has been trained for this particular task), Kimina-Prover attains the\nbest results, solving 7 problems (out of 100) under both ``with solution'' and\n``without solution'' scenarios. We open source the benchmark dataset alongside\nwith the code of the proposed evaluation method at\nhttps://github.com/MoonshotAI/CombiBench/.", "AI": {"tldr": "CombiBench is a new benchmark for combinatorial problems, formalized in Lean~4, with an evaluation framework (Fine-Eval) to test LLMs. Current models perform poorly, with Kimina-Prover solving only 7 out of 100 problems.", "motivation": "Combinatorics lacks benchmarks and theorem libraries, unlike other math domains. CombiBench fills this gap.", "method": "Introduces CombiBench (100 problems in Lean~4) and Fine-Eval, a framework for evaluating formal math solutions, including fill-in-the-blank questions.", "result": "LLMs perform poorly on CombiBench, with Kimina-Prover solving only 7 problems.", "conclusion": "CombiBench and Fine-Eval provide tools for advancing formal reasoning in combinatorics, but current LLMs are limited."}}
{"id": "2505.03730", "pdf": "https://arxiv.org/pdf/2505.03730", "abs": "https://arxiv.org/abs/2505.03730", "authors": ["Shiyi Zhang", "Junhao Zhuang", "Zhaoyang Zhang", "Ying Shan", "Yansong Tang"], "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by Siggraph2025, Project Page:\n  https://shiyi-zh0408.github.io/projectpages/FlexiAct/", "summary": "Action customization involves generating videos where the subject performs\nactions dictated by input control signals. Current methods use pose-guided or\nglobal motion customization but are limited by strict constraints on spatial\nstructure, such as layout, skeleton, and viewpoint consistency, reducing\nadaptability across diverse subjects and scenarios. To overcome these\nlimitations, we propose FlexiAct, which transfers actions from a reference\nvideo to an arbitrary target image. Unlike existing methods, FlexiAct allows\nfor variations in layout, viewpoint, and skeletal structure between the subject\nof the reference video and the target image, while maintaining identity\nconsistency. Achieving this requires precise action control, spatial structure\nadaptation, and consistency preservation. To this end, we introduce RefAdapter,\na lightweight image-conditioned adapter that excels in spatial adaptation and\nconsistency preservation, surpassing existing methods in balancing appearance\nconsistency and structural flexibility. Additionally, based on our\nobservations, the denoising process exhibits varying levels of attention to\nmotion (low frequency) and appearance details (high frequency) at different\ntimesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike\nexisting methods that rely on separate spatial-temporal architectures, directly\nachieves action extraction during the denoising process. Experiments\ndemonstrate that our method effectively transfers actions to subjects with\ndiverse layouts, skeletons, and viewpoints. We release our code and model\nweights to support further research at\nhttps://shiyi-zh0408.github.io/projectpages/FlexiAct/", "AI": {"tldr": "FlexiAct enables action transfer from a reference video to a target image, accommodating layout, viewpoint, and skeletal variations while maintaining identity consistency. It uses RefAdapter for spatial adaptation and FAE for frequency-aware action extraction.", "motivation": "Overcome limitations of current methods that enforce strict spatial constraints, reducing adaptability across diverse subjects and scenarios.", "method": "Proposes FlexiAct with RefAdapter for spatial adaptation and FAE for action extraction during denoising.", "result": "Effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints.", "conclusion": "FlexiAct offers improved flexibility and consistency in action customization, supported by released code and models."}}
{"id": "2505.03380", "pdf": "https://arxiv.org/pdf/2505.03380", "abs": "https://arxiv.org/abs/2505.03380", "authors": ["Haonan Wang", "Jiaji Mao", "Lehan Wang", "Qixiang Zhang", "Marawan Elbatel", "Yi Qin", "Huijun Hu", "Baoxun Li", "Wenhui Deng", "Weifeng Qin", "Hongrui Li", "Jialin Liang", "Jun Shen", "Xiaomeng Li"], "title": "Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Medical AI assistants support doctors in disease diagnosis, medical image\nanalysis, and report generation. However, they still face significant\nchallenges in clinical use, including limited accuracy with multimodal content\nand insufficient validation in real-world settings. We propose RCMed, a\nfull-stack AI assistant that improves multimodal alignment in both input and\noutput, enabling precise anatomical delineation, accurate localization, and\nreliable diagnosis through hierarchical vision-language grounding. A\nself-reinforcing correlation mechanism allows visual features to inform\nlanguage context, while language semantics guide pixel-wise attention, forming\na closed loop that refines both modalities. This correlation is enhanced by a\ncolor region description strategy, translating anatomical structures into\nsemantically rich text to learn shape-location-text relationships across\nscales. Trained on 20 million image-mask-description triplets, RCMed achieves\nstate-of-the-art precision in contextualizing irregular lesions and subtle\nanatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It\nachieved a 23.5% relative improvement in cell segmentation from microscopy\nimages over prior methods. RCMed's strong vision-language alignment enables\nexceptional generalization, with state-of-the-art performance in external\nvalidation across 20 clinically significant cancer types, including novel\ntasks. This work demonstrates how integrated multimodal models capture\nfine-grained patterns, enabling human-level interpretation in complex scenarios\nand advancing human-centric AI healthcare.", "AI": {"tldr": "RCMed is a full-stack AI assistant improving multimodal alignment for medical tasks, achieving state-of-the-art precision and generalization in clinical applications.", "motivation": "Medical AI assistants face challenges like limited accuracy with multimodal content and insufficient real-world validation. RCMed aims to address these by enhancing vision-language alignment.", "method": "RCMed uses hierarchical vision-language grounding and a self-reinforcing correlation mechanism. It includes a color region description strategy to learn shape-location-text relationships across scales, trained on 20 million image-mask-description triplets.", "result": "RCMed achieves a 23.5% relative improvement in cell segmentation and excels in 165 clinical tasks across 9 modalities, with strong performance in external validation for 20 cancer types.", "conclusion": "RCMed demonstrates how integrated multimodal models enable human-level interpretation in complex medical scenarios, advancing AI healthcare."}}
{"id": "2505.03442", "pdf": "https://arxiv.org/pdf/2505.03442", "abs": "https://arxiv.org/abs/2505.03442", "authors": ["Diep Luong", "Mikko Heikkinen", "Konstantinos Drossos", "Tuomas Virtanen"], "title": "Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Speech denoising is a generally adopted and impactful task, appearing in many\ncommon and everyday-life use cases. Although there are very powerful methods\npublished, most of those are too complex for deployment in everyday and\nlow-resources computational environments, like hand-held devices, intelligent\nglasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for\nalleviating this complexity mismatch and is based on the\ntransferring/distilling of knowledge from a pre-trained complex model, the\nteacher, to another less complex one, the student. Existing KD methods for\nspeech denoising are based on processes that potentially hamper the KD by\nbounding the learning of the student to the distribution, information ordering,\nand feature dimensionality learned by the teacher. In this paper, we present\nand assess a method that tries to treat this issue, by exploiting the\nwell-known denoising-autoencoder framework, the linear inverted bottlenecks,\nand the properties of the cosine similarity. We use a public dataset and\nconduct repeated experiments with different mismatching scenarios between the\nteacher and the student, reporting the mean and standard deviation of the\nmetrics of our method and another, state-of-the-art method that is used as a\nbaseline. Our results show that with the proposed method, the student can\nperform better and can also retain greater mismatching conditions compared to\nthe teacher.", "AI": {"tldr": "The paper proposes a method to improve knowledge distillation (KD) for speech denoising, addressing limitations of existing KD methods by leveraging denoising-autoencoders, linear inverted bottlenecks, and cosine similarity.", "motivation": "Existing KD methods for speech denoising restrict student learning to the teacher's distribution and features, limiting performance in low-resource environments.", "method": "The method combines denoising-autoencoders, linear inverted bottlenecks, and cosine similarity to enhance KD.", "result": "Experiments show the student model outperforms baseline methods and handles greater mismatches with the teacher.", "conclusion": "The proposed method improves KD effectiveness for speech denoising, especially in resource-constrained settings."}}
{"id": "2505.03288", "pdf": "https://arxiv.org/pdf/2505.03288", "abs": "https://arxiv.org/abs/2505.03288", "authors": ["Francesco Morri", "H\u00e9l\u00e8ne Le Cadre", "Pierre Gruet", "Luce Brotcorne"], "title": "Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling", "categories": ["cs.MA", "cs.GT", "econ.GN", "q-fin.EC"], "comment": null, "summary": "We characterize zonal ancillary market coupling relying on noncooperative\ngame theory. To that purpose, we formulate the ancillary market as a\nmulti-leader single follower bilevel problem, that we subsequently cast as a\ngeneralized Nash game with side constraints and nonconvex feasibility sets. We\ndetermine conditions for equilibrium existence and show that the game has a\ngeneralized potential game structure. To compute market equilibrium, we rely on\ntwo exact approaches: an integrated optimization approach and Gauss-Seidel\nbest-response, that we compare against multi-agent deep reinforcement learning.\nOn real data from Germany and Austria, simulations indicate that multi-agent\ndeep reinforcement learning achieves the smallest convergence rate but requires\npretraining, while best-response is the slowest. On the economics side,\nmulti-agent deep reinforcement learning results in smaller market costs\ncompared to the exact methods, but at the cost of higher variability in the\nprofit allocation among stakeholders. Further, stronger coupling between zones\ntends to reduce costs for larger zones.", "AI": {"tldr": "The paper analyzes zonal ancillary market coupling using noncooperative game theory, comparing exact methods (optimization and best-response) with multi-agent deep reinforcement learning (MADRL). MADRL shows lower costs but higher profit variability, while stronger zone coupling reduces costs for larger zones.", "motivation": "To understand and optimize zonal ancillary market coupling using game theory, addressing equilibrium existence and computational methods for market equilibrium.", "method": "Formulates the market as a multi-leader single-follower bilevel problem, cast as a generalized Nash game. Uses exact methods (optimization, Gauss-Seidel best-response) and MADRL for equilibrium computation.", "result": "MADRL achieves lower market costs but higher profit variability. Exact methods are slower, with best-response being the slowest. Stronger zone coupling reduces costs for larger zones.", "conclusion": "MADRL is promising for cost reduction but requires pretraining and trades off stability. Exact methods are reliable but slower. Zone coupling benefits larger zones economically."}}
{"id": "2505.02983", "pdf": "https://arxiv.org/pdf/2505.02983", "abs": "https://arxiv.org/abs/2505.02983", "authors": ["Wenjie Hua", "Shenghan Xu"], "title": "Logits-Constrained Framework with RoBERTa for Ancient Chinese NER", "categories": ["cs.CL", "68T50", "I.2.7; I.5.1; I.5.4"], "comment": "5 pages, 2 figures, 6 tables. Accepted to EvaHan 2025 shared task on\n  Ancient Chinese NLP", "summary": "This paper presents a Logits-Constrained (LC) framework for Ancient Chinese\nNamed Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our\ntwo-stage model integrates GujiRoBERTa for contextual encoding and a\ndifferentiable decoding mechanism to enforce valid BMES label transitions.\nExperiments demonstrate that LC improves performance over traditional CRF and\nBiLSTM-based approaches, especially in high-label or large-data settings. We\nalso propose a model selection criterion balancing label complexity and dataset\nsize, providing practical guidance for real-world Ancient Chinese NLP tasks.", "AI": {"tldr": "A Logits-Constrained (LC) framework for Ancient Chinese NER improves performance over traditional methods, especially in high-label or large-data settings, using GujiRoBERTa and a differentiable decoding mechanism.", "motivation": "To address the challenges of Named Entity Recognition in Ancient Chinese, particularly in high-label or large-data scenarios, by improving accuracy and efficiency.", "method": "A two-stage model combining GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions.", "result": "LC outperforms traditional CRF and BiLSTM-based approaches, particularly in high-label or large-data settings.", "conclusion": "The LC framework is effective for Ancient Chinese NER, with a proposed model selection criterion offering practical guidance for real-world NLP tasks."}}
{"id": "2406.16020", "pdf": "https://arxiv.org/pdf/2406.16020", "abs": "https://arxiv.org/abs/2406.16020", "authors": ["Bin Wang", "Xunlong Zou", "Geyu Lin", "Shuo Sun", "Zhuohan Liu", "Wenyu Zhang", "Zhengyuan Liu", "AiTi Aw", "Nancy F. Chen"], "title": "AudioBench: A Universal Benchmark for Audio Large Language Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "v5 - Update acknowledgment; Code:\n  https://github.com/AudioLLMs/AudioBench", "summary": "We introduce AudioBench, a universal benchmark designed to evaluate Audio\nLarge Language Models (AudioLLMs). It encompasses 8 distinct tasks and 26\ndatasets, among which, 7 are newly proposed datasets. The evaluation targets\nthree main aspects: speech understanding, audio scene understanding, and voice\nunderstanding (paralinguistic). Despite recent advancements, there lacks a\ncomprehensive benchmark for AudioLLMs on instruction following capabilities\nconditioned on audio signals. AudioBench addresses this gap by setting up\ndatasets as well as desired evaluation metrics. Besides, we also evaluated the\ncapabilities of five popular models and found that no single model excels\nconsistently across all tasks. We outline the research outlook for AudioLLMs\nand anticipate that our open-sourced evaluation toolkit, data, and leaderboard\nwill offer a robust testbed for future model developments.", "AI": {"tldr": "AudioBench is a universal benchmark for evaluating Audio Large Language Models (AudioLLMs) across 8 tasks and 26 datasets, addressing gaps in instruction-following capabilities. It evaluates speech, audio scene, and voice understanding, revealing no single model excels in all tasks.", "motivation": "The lack of a comprehensive benchmark for AudioLLMs' instruction-following capabilities conditioned on audio signals motivated the creation of AudioBench.", "method": "AudioBench includes 8 tasks and 26 datasets (7 new), evaluating speech, audio scene, and voice understanding. Five popular models were tested.", "result": "No single model performed consistently well across all tasks.", "conclusion": "AudioBench provides a robust testbed for future AudioLLM development, with open-sourced tools, data, and a leaderboard."}}
{"id": "2505.03012", "pdf": "https://arxiv.org/pdf/2505.03012", "abs": "https://arxiv.org/abs/2505.03012", "authors": ["Saeed Ebrahimi", "Sahar Rahimi", "Ali Dabouei", "Srinjoy Das", "Jeremy M. Dawson", "Nasser M. Nasrabadi"], "title": "GIF: Generative Inspiration for Face Recognition at Scale", "categories": ["cs.CV"], "comment": null, "summary": "Aiming to reduce the computational cost of Softmax in massive label space of\nFace Recognition (FR) benchmarks, recent studies estimate the output using a\nsubset of identities. Although promising, the association between the\ncomputation cost and the number of identities in the dataset remains linear\nonly with a reduced ratio. A shared characteristic among available FR methods\nis the employment of atomic scalar labels during training. Consequently, the\ninput to label matching is through a dot product between the feature vector of\nthe input and the Softmax centroids. Inspired by generative modeling, we\npresent a simple yet effective method that substitutes scalar labels with\nstructured identity code, i.e., a sequence of integers. Specifically, we\npropose a tokenization scheme that transforms atomic scalar labels into\nstructured identity codes. Then, we train an FR backbone to predict the code\nfor each input instead of its scalar label. As a result, the associated\ncomputational cost becomes logarithmic w.r.t. number of identities. We\ndemonstrate the benefits of the proposed method by conducting experiments. In\nparticular, our method outperforms its competitors by 1.52%, and 0.6% at\nTAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the\nassociation between computational cost and the number of identities from linear\nto logarithmic. See code at https://github.com/msed-Ebrahimi/GIF", "AI": {"tldr": "The paper proposes a method to reduce computational cost in face recognition by replacing scalar labels with structured identity codes, achieving logarithmic cost scaling and improved performance.", "motivation": "To address the linear computational cost of Softmax in massive label spaces for face recognition, the paper seeks a more efficient alternative.", "method": "The method tokenizes scalar labels into structured identity codes and trains the model to predict these codes, reducing computational cost.", "result": "The approach outperforms competitors by 1.52% and 0.6% on IJB-B and IJB-C benchmarks, respectively, while achieving logarithmic cost scaling.", "conclusion": "The proposed method effectively reduces computational cost and improves performance in face recognition tasks."}}
{"id": "2505.02889", "pdf": "https://arxiv.org/pdf/2505.02889", "abs": "https://arxiv.org/abs/2505.02889", "authors": ["Oyindolapo O. Komolafe", "Zhimin Mei", "David Morales Zarate", "Gregory William Spangenberg"], "title": "Early Prediction of Sepsis: Feature-Aligned Transfer Learning", "categories": ["cs.LG", "cs.AI"], "comment": "A project implemented for MACHINE LEARNING IN HEALTH AND BIOMEDICAL\n  SCIENCE", "summary": "Sepsis is a life threatening medical condition that occurs when the body has\nan extreme response to infection, leading to widespread inflammation, organ\nfailure, and potentially death. Because sepsis can worsen rapidly, early\ndetection is critical to saving lives. However, current diagnostic methods\noften identify sepsis only after significant damage has already occurred. Our\nproject aims to address this challenge by developing a machine learning based\nsystem to predict sepsis in its early stages, giving healthcare providers more\ntime to intervene.\n  A major problem with existing models is the wide variability in the patient\ninformation or features they use, such as heart rate, temperature, and lab\nresults. This inconsistency makes models difficult to compare and limits their\nability to work across different hospitals and settings. To solve this, we\npropose a method called Feature Aligned Transfer Learning (FATL), which\nidentifies and focuses on the most important and commonly reported features\nacross multiple studies, ensuring the model remains consistent and clinically\nrelevant.\n  Most existing models are trained on narrow patient groups, leading to\npopulation bias. FATL addresses this by combining knowledge from models trained\non diverse populations, using a weighted approach that reflects each models\ncontribution. This makes the system more generalizable and effective across\ndifferent patient demographics and clinical environments. FATL offers a\npractical and scalable solution for early sepsis detection, particularly in\nhospitals with limited resources, and has the potential to improve patient\noutcomes, reduce healthcare costs, and support more equitable healthcare\ndelivery.", "AI": {"tldr": "A machine learning system (FATL) is proposed for early sepsis detection by focusing on consistent, clinically relevant features and addressing population bias.", "motivation": "Early sepsis detection is critical but current methods are delayed and inconsistent. Existing models lack generalizability due to varied features and population bias.", "method": "Feature Aligned Transfer Learning (FATL) identifies key features across studies and combines knowledge from diverse populations using a weighted approach.", "result": "FATL ensures consistency, clinical relevance, and generalizability across different patient demographics and settings.", "conclusion": "FATL provides a scalable solution for early sepsis detection, improving outcomes, reducing costs, and promoting equitable healthcare."}}
{"id": "2505.03189", "pdf": "https://arxiv.org/pdf/2505.03189", "abs": "https://arxiv.org/abs/2505.03189", "authors": ["Yixiong Hao", "Ayush Panda", "Stepan Shabalin", "Sheikh Abdur Raheem Ali"], "title": "Patterns and Mechanisms of Contrastive Activation Engineering", "categories": ["cs.AI", "cs.HC"], "comment": "Published at the ICLR 2025 Bi-Align, HAIC, and Building Trust\n  workshops", "summary": "Controlling the behavior of Large Language Models (LLMs) remains a\nsignificant challenge due to their inherent complexity and opacity. While\ntechniques like fine-tuning can modify model behavior, they typically require\nextensive computational resources. Recent work has introduced a class of\ncontrastive activation engineering (CAE) techniques as promising approaches for\nsteering LLM outputs through targeted modifications to their internal\nrepresentations. Applied at inference-time with zero cost, CAE has the\npotential to introduce a new paradigm of flexible, task-specific LLM behavior\ntuning. We analyze the performance of CAE in in-distribution,\nout-of-distribution settings, evaluate drawbacks, and begin to develop\ncomprehensive guidelines for its effective deployment. We find that 1. CAE is\nonly reliably effective when applied to in-distribution contexts. 2. Increasing\nthe number of samples used to generate steering vectors has diminishing returns\nat around 80 samples. 3. Steering vectors are susceptible to adversarial inputs\nthat reverses the behavior that is steered for. 4. Steering vectors harm the\noverall model perplexity. 5. Larger models are more resistant to\nsteering-induced degradation.", "AI": {"tldr": "CAE techniques offer a cost-free, inference-time method to steer LLM behavior but are limited to in-distribution contexts, have diminishing returns with more samples, and can degrade model performance.", "motivation": "Addressing the challenge of controlling LLMs without extensive computational resources by exploring CAE techniques.", "method": "Analyzing CAE's performance in in-distribution and out-of-distribution settings, evaluating drawbacks, and developing deployment guidelines.", "result": "CAE is effective only in-distribution, has diminishing returns beyond 80 samples, is vulnerable to adversarial inputs, harms perplexity, and larger models resist degradation.", "conclusion": "CAE is a promising but limited tool for LLM behavior control, requiring careful deployment and further research."}}
{"id": "2504.17938", "pdf": "https://arxiv.org/pdf/2504.17938", "abs": "https://arxiv.org/abs/2504.17938", "authors": ["Raza Ul Mustafa", "Sesha Dassanayake", "Noman Ashraf", "Romana Aziz", "Ala Saleh Alluhaidan"], "title": "Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G", "categories": ["cs.MM", "cs.LG"], "comment": null, "summary": "The Quality of Experience (QoE) is the users satisfaction while streaming a\nvideo session over an over-the-top (OTT) platform like YouTube. QoE of YouTube\nreflects the smooth streaming session without any buffering and quality shift\nevents. One of the most important factors nowadays affecting QoE of YouTube is\nfrequent shifts from higher to lower resolutions and vice versa. These shifts\nensure a smooth streaming session; however, it might get a lower mean opinion\nscore. For instance, dropping from 1080p to 480p during a video can preserve\ncontinuity but might reduce the viewers enjoyment. Over time, OTT platforms are\nlooking for alternative ways to boost user experience instead of relying on\ntraditional Quality of Service (QoS) metrics such as bandwidth, latency, and\nthroughput. As a result, we look into the relationship between quality shifting\nin YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our\nfindings state that these channel metrics positively correlate with shifts.\nThus, in real-time, OTT can only rely on them to predict video streaming\nsessions into lower- and higher-resolution categories, thus providing more\nresources to improve user experience. Using traditional Machine Learning (ML)\nclassifiers, we achieved an accuracy of 77-percent, while using only RSRP,\nRSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency\nnetworks promise enhanced streaming capabilities, the proposed methodology can\nbe used to improve OTT services.", "AI": {"tldr": "The paper explores how channel metrics (RSRP, RSRQ, SNR) correlate with YouTube video quality shifts, proposing ML-based prediction to enhance QoE.", "motivation": "Improve YouTube streaming QoE by understanding and predicting resolution shifts using channel metrics, moving beyond traditional QoS.", "method": "Analyzes the relationship between quality shifts and channel metrics, using ML classifiers for prediction.", "result": "Channel metrics positively correlate with shifts; ML models achieve 77% accuracy in predicting resolution categories.", "conclusion": "Proposed method can enhance OTT services, especially in 5G networks, by leveraging channel metrics for better QoE."}}
{"id": "2505.03539", "pdf": "https://arxiv.org/pdf/2505.03539", "abs": "https://arxiv.org/abs/2505.03539", "authors": ["Mengfei Duan", "Kailun Yang", "Yuheng Zhang", "Yihong Cao", "Fei Teng", "Kai Luo", "Jiaming Zhang", "Zhiyong Li", "Shutao Li"], "title": "Panoramic Out-of-Distribution Segmentation", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "Code and datasets will be available at\n  https://github.com/MengfeiD/PanOoS", "summary": "Panoramic imaging enables capturing 360{\\deg} images with an ultra-wide\nField-of-View (FoV) for dense omnidirectional perception. However, current\npanoramic semantic segmentation methods fail to identify outliers, and pinhole\nOut-of-distribution Segmentation (OoS) models perform unsatisfactorily in the\npanoramic domain due to background clutter and pixel distortions. To address\nthese issues, we introduce a new task, Panoramic Out-of-distribution\nSegmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the\nfirst solution, POS, which adapts to the characteristics of panoramic images\nthrough text-guided prompt distribution learning. Specifically, POS integrates\na disentanglement strategy designed to materialize the cross-domain\ngeneralization capability of CLIP. The proposed Prompt-based Restoration\nAttention (PRA) optimizes semantic decoding by prompt guidance and\nself-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)\nrefines the manifold of per-pixel mask embeddings via semantic prototype\nsupervision. Besides, to compensate for the scarcity of PanOoS datasets, we\nestablish two benchmarks: DenseOoS, which features diverse outliers in complex\nenvironments, and QuadOoS, captured by a quadruped robot with a panoramic\nannular lens system. Extensive experiments demonstrate superior performance of\nPOS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,\noutperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves\nleading closed-set segmentation capabilities. Code and datasets will be\navailable at https://github.com/MengfeiD/PanOoS.", "AI": {"tldr": "The paper introduces Panoramic Out-of-distribution Segmentation (PanOoS) and proposes POS, a solution using text-guided prompt distribution learning to address OoS in panoramas, outperforming existing methods.", "motivation": "Current panoramic semantic segmentation methods struggle with outliers, and pinhole OoS models perform poorly in panoramic domains due to distortions and clutter.", "method": "POS integrates a disentanglement strategy for CLIP, uses Prompt-based Restoration Attention (PRA) for semantic decoding, and Bilevel Prompt Distribution Learning (BPDL) for mask embeddings. Two benchmarks, DenseOoS and QuadOoS, are introduced.", "result": "POS improves AuPRC by 34.25% and reduces FPR95 by 21.42% on DenseOoS, outperforming state-of-the-art methods. It also excels in closed-set segmentation.", "conclusion": "POS effectively addresses PanOoS, demonstrating superior performance and generalization, with datasets and code made available for further research."}}
{"id": "2502.13473", "pdf": "https://arxiv.org/pdf/2502.13473", "abs": "https://arxiv.org/abs/2502.13473", "authors": ["Michael Neri", "Tuomas Virtanen"], "title": "Multi-channel Replay Speech Detection using an Adaptive Learnable Beamformer", "categories": ["eess.AS", "eess.SP"], "comment": "IEEE Open Journal of Signal Processing", "summary": "Replay attacks belong to the class of severe threats against voice-controlled\nsystems, exploiting the easy accessibility of speech signals by recorded and\nreplayed speech to grant unauthorized access to sensitive data. In this work,\nwe propose a multi-channel neural network architecture called M-ALRAD for the\ndetection of replay attacks based on spatial audio features. This approach\nintegrates a learnable adaptive beamformer with a convolutional recurrent\nneural network, allowing for joint optimization of spatial filtering and\nclassification. Experiments have been carried out on the ReMASC dataset, which\nis a state-of-the-art multi-channel replay speech detection dataset\nencompassing four microphones with diverse array configurations and four\nenvironments. Results on the ReMASC dataset show the superiority of the\napproach compared to the state-of-the-art and yield substantial improvements\nfor challenging acoustic environments. In addition, we demonstrate that our\napproach is able to better generalize to unseen environments with respect to\nprior studies.", "AI": {"tldr": "M-ALRAD, a multi-channel neural network, detects replay attacks using spatial audio features, outperforming state-of-the-art methods on the ReMASC dataset.", "motivation": "Replay attacks threaten voice-controlled systems by using recorded speech for unauthorized access, necessitating robust detection methods.", "method": "M-ALRAD combines a learnable adaptive beamformer with a convolutional recurrent neural network for joint spatial filtering and classification.", "result": "M-ALRAD outperforms existing methods on the ReMASC dataset, especially in challenging environments, and generalizes better to unseen conditions.", "conclusion": "M-ALRAD is a superior solution for replay attack detection, offering improved performance and generalization."}}
{"id": "2505.03472", "pdf": "https://arxiv.org/pdf/2505.03472", "abs": "https://arxiv.org/abs/2505.03472", "authors": ["David Kl\u00fcner", "Simon Sch\u00e4fer", "Lucas Hegerath", "Jianye Xu", "Julius Kahle", "Hazem Ibrahim", "Alexandru Kampmann", "Bassam Alrifaee"], "title": "Simulation to Reality: Testbeds and Architectures for Connected and Automated Vehicles", "categories": ["cs.MA"], "comment": null, "summary": "Ensuring the safe and efficient operation of CAVs relies heavily on the\nsoftware framework used. A software framework needs to ensure real-time\nproperties, reliable communication, and efficient resource utilization.\nFurthermore, a software framework needs to enable seamless transition between\ntesting stages, from simulation to small-scale to full-scale experiments. In\nthis paper, we survey prominent software frameworks used for in-vehicle and\ninter-vehicle communication in CAVs. We analyze these frameworks regarding\nopportunities and challenges, such as their real-time properties and\ntransitioning capabilities. Additionally, we delve into the tooling\nrequirements necessary for addressing the associated challenges. We illustrate\nthe practical implications of these challenges through case studies focusing on\ncritical areas such as perception, motion planning, and control. Furthermore,\nwe identify research gaps in the field, highlighting areas where further\ninvestigation is needed to advance the development and deployment of safe and\nefficient CAV systems.", "AI": {"tldr": "Survey of software frameworks for CAVs, analyzing real-time properties, transitioning capabilities, and tooling requirements, with case studies and research gaps.", "motivation": "To ensure safe and efficient operation of CAVs by evaluating software frameworks for real-time properties, communication, and resource utilization.", "method": "Survey and analysis of prominent software frameworks, focusing on real-time properties, transitioning capabilities, and tooling needs, supported by case studies.", "result": "Identified opportunities, challenges, and research gaps in CAV software frameworks, emphasizing practical implications in perception, planning, and control.", "conclusion": "Further research is needed to address gaps and advance the development of safe and efficient CAV systems."}}
{"id": "2505.03005", "pdf": "https://arxiv.org/pdf/2505.03005", "abs": "https://arxiv.org/abs/2505.03005", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper", "AI": {"tldr": "RADLADS converts softmax attention transformers to linear attention decoders efficiently, with minimal cost and token usage, while maintaining performance.", "motivation": "To reduce computational costs and token requirements for converting transformers to linear attention models without significant quality loss.", "method": "Introduces RADLADS protocol and new RWKV-variant architectures, converting Qwen2.5 models (7B, 32B, 72B) using only 350-700M tokens.", "result": "Achieves SOTA performance for linear attention models, with conversion costs under $2,000 for a 72B model.", "conclusion": "RADLADS offers a cost-effective, efficient method for converting transformers to linear attention models, with models released under open licenses."}}
{"id": "2502.07328", "pdf": "https://arxiv.org/pdf/2502.07328", "abs": "https://arxiv.org/abs/2502.07328", "authors": ["Atharva Mehta", "Shivam Chauhan", "Amirbek Djanibekov", "Atharva Kulkarni", "Gus Xia", "Monojit Choudhury"], "title": "Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "17 pages, 5 figures, accepted to NAACL'25", "summary": "The advent of Music-Language Models has greatly enhanced the automatic music\ngeneration capability of AI systems, but they are also limited in their\ncoverage of the musical genres and cultures of the world. We present a study of\nthe datasets and research papers for music generation and quantify the bias and\nunder-representation of genres. We find that only 5.7% of the total hours of\nexisting music datasets come from non-Western genres, which naturally leads to\ndisparate performance of the models across genres. We then investigate the\nefficacy of Parameter-Efficient Fine-Tuning (PEFT) techniques in mitigating\nthis bias. Our experiments with two popular models -- MusicGen and Mustango,\nfor two underrepresented non-Western music traditions -- Hindustani Classical\nand Turkish Makam music, highlight the promises as well as the non-triviality\nof cross-genre adaptation of music through small datasets, implying the need\nfor more equitable baseline music-language models that are designed for\ncross-cultural transfer learning.", "AI": {"tldr": "The paper highlights the bias in Music-Language Models due to underrepresentation of non-Western genres and explores PEFT techniques to mitigate this, showing promise but also challenges.", "motivation": "To address the bias and underrepresentation of non-Western music genres in AI-generated music, which leads to disparate model performance.", "method": "Quantifies dataset bias, then tests Parameter-Efficient Fine-Tuning (PEFT) on MusicGen and Mustango models for Hindustani Classical and Turkish Makam music.", "result": "Only 5.7% of music datasets are non-Western. PEFT shows promise but cross-genre adaptation remains challenging with small datasets.", "conclusion": "More equitable baseline models designed for cross-cultural transfer learning are needed."}}
{"id": "2505.03018", "pdf": "https://arxiv.org/pdf/2505.03018", "abs": "https://arxiv.org/abs/2505.03018", "authors": ["Aurora Rofena", "Arianna Manchia", "Claudia Lucia Piccolo", "Bruno Beomonte Zobel", "Paolo Soda", "Valerio Guarrasi"], "title": "Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic\ntechnique that improves lesion visibility through the administration of an\niodinated contrast agent. It acquires both a low-energy image, comparable to\nstandard mammography, and a high-energy image, which are then combined to\nproduce a dual-energy subtracted image highlighting lesion contrast\nenhancement. While CESM offers superior diagnostic accuracy compared to\nstandard mammography, its use entails higher radiation exposure and potential\nside effects associated with the contrast medium. To address these limitations,\nwe propose Seg-CycleGAN, a generative deep learning framework for Virtual\nContrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy\nsubtracted images from low-energy images, leveraging lesion segmentation maps\nto guide the generative process and improve lesion reconstruction. Building\nupon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss\nterms focused on lesion areas, enhancing the synthesis of diagnostically\nrelevant regions. Experiments on the CESM@UCBM dataset demonstrate that\nSeg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while\nmaintaining competitive MSE and VIF. Qualitative evaluations further confirm\nimproved lesion fidelity in the generated images. These results suggest that\nsegmentation-aware generative models offer a viable pathway toward\ncontrast-free CESM alternatives.", "AI": {"tldr": "Seg-CycleGAN, a deep learning model, synthesizes high-quality dual-energy subtracted CESM images from low-energy ones, reducing the need for contrast agents and radiation.", "motivation": "CESM improves lesion visibility but has drawbacks like higher radiation and contrast medium side effects. The goal is to create a contrast-free alternative.", "method": "Seg-CycleGAN uses lesion segmentation maps to guide image synthesis, adding localized loss terms to enhance diagnostically relevant areas.", "result": "Outperforms baselines in PSNR and SSIM, with competitive MSE and VIF. Qualitative evaluations show better lesion fidelity.", "conclusion": "Segmentation-aware generative models like Seg-CycleGAN can enable contrast-free CESM alternatives."}}
{"id": "2505.02922", "pdf": "https://arxiv.org/pdf/2505.02922", "abs": "https://arxiv.org/abs/2505.02922", "authors": ["Yaoqi Chen", "Jinkai Zhang", "Baotong Lu", "Qianxi Zhang", "Chengruidong Zhang", "Jingjia Luo", "Di Liu", "Huiqiang Jiang", "Qi Chen", "Jing Liu", "Bailu Ding", "Xiao Yan", "Jiawei Jiang", "Chen Chen", "Mingxing Zhang", "Yuqing Yang", "Fan Yang", "Mao Yang"], "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference", "categories": ["cs.LG"], "comment": "16 pages", "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.", "AI": {"tldr": "RetroInfer accelerates long-context LLM inference by reimagining the KV cache as a vector storage system, leveraging attention sparsity for efficiency without sacrificing accuracy.", "motivation": "Addressing GPU memory and bandwidth constraints in LLM inference due to growing context lengths.", "method": "Uses a wave index for efficient token retrieval and a wave buffer for KV cache coordination, combining tripartite attention approximation and segmented clustering.", "result": "Achieves up to 4.5X speedup over full attention and 10.5X over sparse baselines while maintaining accuracy.", "conclusion": "RetroInfer offers a robust solution for efficient long-context LLM inference by optimizing KV cache usage."}}
{"id": "2505.03275", "pdf": "https://arxiv.org/pdf/2505.03275", "abs": "https://arxiv.org/abs/2505.03275", "authors": ["Tiantian Gan", "Qiyao Sun"], "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) struggle to effectively utilize a growing number\nof external tools, such as those defined by the Model Context Protocol\n(MCP)\\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We\nintroduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes\nthis challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to\nidentify the most relevant MCP(s) for a given query from an external index\nbefore engaging the LLM. Only the selected tool descriptions are passed to the\nmodel, drastically reducing prompt size and simplifying decision-making.\nExperiments, including an MCP stress test, demonstrate RAG-MCP significantly\ncuts prompt tokens (e.g., by over 50%) and more than triples tool selection\naccuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables\nscalable and accurate tool integration for LLMs.", "AI": {"tldr": "RAG-MCP is a Retrieval-Augmented Generation framework that improves LLMs' tool usage by reducing prompt bloat and simplifying tool selection via semantic retrieval.", "motivation": "LLMs struggle with prompt bloat and complexity when using external tools like MCP.", "method": "RAG-MCP uses semantic retrieval to identify relevant tools from an external index, passing only selected descriptions to the LLM.", "result": "RAG-MCP reduces prompt tokens by over 50% and triples tool selection accuracy (43.13% vs 13.62%).", "conclusion": "RAG-MCP enables scalable and accurate tool integration for LLMs."}}
{"id": "2505.02549", "pdf": "https://arxiv.org/pdf/2505.02549", "abs": "https://arxiv.org/abs/2505.02549", "authors": ["Yongxiang Li", "Yuan Sun", "Yang Qin", "Dezhong Peng", "Xi Peng", "Peng Hu"], "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identification", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE.", "AI": {"tldr": "A novel Robust Duality Learning framework (RoDE) addresses pseudo-label noise in unsupervised visible-infrared person re-identification (UVI-ReID) by dynamically emphasizing clean samples, using dual models to prevent error accumulation, and aligning clusters across modalities.", "motivation": "Existing methods assume pseudo-labels are correct, but noise in these labels hinders model performance. The paper aims to mitigate pseudo-label noise challenges: overfitting, error accumulation, and noisy cluster correspondence.", "method": "Proposes RoDE with Robust Adaptive Learning (RAL) to weight samples, dual-model training to prevent error accumulation, and Cluster Consistency Matching (CCM) to align clusters across models and modalities.", "result": "Extensive experiments on three benchmarks validate RoDE's effectiveness in handling pseudo-label noise and improving UVI-ReID performance.", "conclusion": "RoDE successfully addresses pseudo-label noise challenges, offering a robust solution for unsupervised cross-modal person re-identification."}}
{"id": "2409.07012", "pdf": "https://arxiv.org/pdf/2409.07012", "abs": "https://arxiv.org/abs/2409.07012", "authors": ["Daeun Kyung", "Junu Kim", "Tackeun Kim", "Edward Choi"], "title": "Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted at Proc. of Conference on Health, Inference, and Learning\n  (CHIL) 2025 (10 pages for main text, 3 pages for references, 8 pages for\n  supplementary materials)", "summary": "Chest X-ray (CXR) is an important diagnostic tool widely used in hospitals to\nassess patient conditions and monitor changes over time. Recently, generative\nmodels, specifically diffusion-based models, have shown promise in generating\nrealistic synthetic CXRs. However, these models mainly focus on conditional\ngeneration using single-time-point data, i.e., generating CXRs conditioned on\ntheir corresponding reports from a specific time. This limits their clinical\nutility, particularly for capturing temporal changes. To address this\nlimitation, we propose a novel framework, EHRXDiff, which predicts future CXR\nimages by integrating previous CXRs with subsequent medical events, e.g.,\nprescriptions, lab measures, etc. Our framework dynamically tracks and predicts\ndisease progression based on a latent diffusion model, conditioned on the\nprevious CXR image and a history of medical events. We comprehensively evaluate\nthe performance of our framework across three key aspects, including clinical\nconsistency, demographic consistency, and visual realism. Results show that our\nframework generates high-quality, realistic future images that effectively\ncapture potential temporal changes. This suggests that our framework could be\nfurther developed to support clinical decision-making and provide valuable\ninsights for patient monitoring and treatment planning in the medical field.\nThe code is available at https://github.com/dek924/EHRXDiff.", "AI": {"tldr": "EHRXDiff is a novel framework using a latent diffusion model to predict future CXR images by integrating past CXRs and medical events, addressing the limitation of single-time-point generation in existing models.", "motivation": "Existing generative models for CXRs focus on single-time-point data, limiting their ability to capture temporal changes, which is crucial for clinical utility.", "method": "EHRXDiff integrates previous CXR images and medical event histories into a latent diffusion model to predict future CXRs.", "result": "The framework generates high-quality, realistic future CXRs that capture temporal changes, validated by clinical, demographic, and visual consistency.", "conclusion": "EHRXDiff shows promise for clinical decision-making and patient monitoring by effectively predicting disease progression."}}
{"id": "2501.15858", "pdf": "https://arxiv.org/pdf/2501.15858", "abs": "https://arxiv.org/abs/2501.15858", "authors": ["Eunjung Yeo", "Julie Liss", "Visar Berisha", "David Mortensen"], "title": "Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "15 pages, 2 figure, 2 tables", "summary": "Purpose: Speech intelligibility is a critical outcome in the assessment and\nmanagement of dysarthria, yet most research and clinical practices have focused\non English, limiting their applicability across languages. This commentary\nintroduces a conceptual framework--and a demonstration of how it can be\nimplemented--leveraging artificial intelligence (AI) to advance cross-language\nintelligibility assessment of dysarthric speech. Method: We propose a\ntwo-tiered conceptual framework consisting of a universal speech model that\nencodes dysarthric speech into acoustic-phonetic representations, followed by a\nlanguage-specific intelligibility assessment model that interprets these\nrepresentations within the phonological or prosodic structures of the target\nlanguage. We further identify barriers to cross-language intelligibility\nassessment of dysarthric speech, including data scarcity, annotation\ncomplexity, and limited linguistic insights into dysarthric speech, and outline\npotential AI-driven solutions to overcome these challenges. Conclusion:\nAdvancing cross-language intelligibility assessment of dysarthric speech\nnecessitates models that are both efficient and scalable, yet constrained by\nlinguistic rules to ensure accurate and language-sensitive assessment. Recent\nadvances in AI provide the foundational tools to support this integration,\nshaping future directions toward generalizable and linguistically informed\nassessment frameworks.", "AI": {"tldr": "The paper proposes an AI-driven framework for cross-language intelligibility assessment of dysarthric speech, addressing data and linguistic challenges.", "motivation": "Current research and clinical practices focus on English, limiting applicability across languages. The paper aims to bridge this gap.", "method": "A two-tiered framework: a universal speech model for acoustic-phonetic encoding and a language-specific model for intelligibility assessment.", "result": "Identifies barriers like data scarcity and annotation complexity, proposing AI solutions.", "conclusion": "AI advancements can enable scalable, linguistically informed frameworks for cross-language dysarthric speech assessment."}}
{"id": "2505.03586", "pdf": "https://arxiv.org/pdf/2505.03586", "abs": "https://arxiv.org/abs/2505.03586", "authors": ["Songchen Fu", "Siang Chen", "Shaojing Zhao", "Letian Bai", "Ta Li", "Yonghong Yan"], "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation", "categories": ["cs.MA", "cs.AI", "68T07 (Primary), 68T20, 68T42 (Secondary)", "I.2"], "comment": "The code will be open-sourced in the RDC-pymarl project under\n  https://github.com/linkjoker1006", "summary": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.", "AI": {"tldr": "The paper introduces a framework (RDC) to address stochastic individual delays in multi-agent systems, improving MARL performance under delayed observations.", "motivation": "Observation delays in MASs hinder agents' decision-making, necessitating a solution for delayed observations in MARL.", "method": "Proposes DSID-POMDP to model delays and RDC framework with modules to compensate for delays, tested on MARL benchmarks.", "result": "RDC mitigates performance degradation under delays, achieving near delay-free performance in some cases.", "conclusion": "The work offers a novel solution for delayed observations in MASs, enhancing MARL robustness."}}
{"id": "2505.03019", "pdf": "https://arxiv.org/pdf/2505.03019", "abs": "https://arxiv.org/abs/2505.03019", "authors": ["Alb\u00e9rick Euraste Djir\u00e9", "Abdoul Kader Kabor\u00e9", "Earl T. Barr", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Large Language Models (LLMs) achieve remarkable performance through\ntraining on massive datasets, they can exhibit concerning behaviors such as\nverbatim reproduction of training data rather than true generalization. This\nmemorization phenomenon raises significant concerns about data privacy,\nintellectual property rights, and the reliability of model evaluations. This\npaper introduces PEARL, a novel approach for detecting memorization in LLMs.\nPEARL assesses how sensitive an LLM's performance is to input perturbations,\nenabling memorization detection without requiring access to the model's\ninternals. We investigate how input perturbations affect the consistency of\noutputs, enabling us to distinguish between true generalization and\nmemorization. Our findings, following extensive experiments on the Pythia open\nmodel, provide a robust framework for identifying when the model simply\nregurgitates learned information. Applied on the GPT 4o models, the PEARL\nframework not only identified cases of memorization of classic texts from the\nBible or common code from HumanEval but also demonstrated that it can provide\nsupporting evidence that some data, such as from the New York Times news\narticles, were likely part of the training data of a given model.", "AI": {"tldr": "PEARL is a new method to detect memorization in LLMs by analyzing output consistency under input perturbations, without needing internal model access. It successfully identified memorization in models like GPT-4o.", "motivation": "LLMs often memorize training data instead of generalizing, raising privacy, IP, and reliability concerns. PEARL addresses this by detecting memorization.", "method": "PEARL evaluates LLM output sensitivity to input perturbations to distinguish memorization from generalization.", "result": "PEARL identified memorization in GPT-4o, including classic texts and common code, and inferred training data sources like NYT articles.", "conclusion": "PEARL provides a robust, external framework for detecting memorization in LLMs, enhancing transparency and trust in model evaluations."}}
{"id": "2503.22712", "pdf": "https://arxiv.org/pdf/2503.22712", "abs": "https://arxiv.org/abs/2503.22712", "authors": ["Zijun Jia"], "title": "Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets", "categories": ["cs.SD", "cs.LG", "eess.AS"], "comment": null, "summary": "Road rage, often triggered by emotional suppression and sudden outbursts,\nsignificantly threatens road safety by causing collisions and aggressive\nbehavior. Speech emotion recognition technologies can mitigate this risk by\nidentifying negative emotions early and issuing timely alerts. However, current\nSER methods, such as those based on hidden markov models and Long short-term\nmemory networks, primarily handle one-dimensional signals, frequently\nexperience overfitting, and lack calibration, limiting their safety-critical\neffectiveness. We propose a novel risk-controlled prediction framework\nproviding statistically rigorous guarantees on prediction accuracy. This\napproach employs a calibration set to define a binary loss function indicating\nwhether the true label is included in the prediction set. Using a data-driven\nthreshold $\\beta$, we optimize a joint loss function to maintain an expected\ntest loss bounded by a user-specified risk level $\\alpha$. Evaluations across\nsix baseline models and two benchmark datasets demonstrate our framework\nconsistently achieves a minimum coverage of $1 - \\alpha$, effectively\ncontrolling marginal error rates despite varying calibration-test split ratios\n(e.g., 0.1). The robustness and generalizability of the framework are further\nvalidated through an extension to small-batch online calibration under a local\nexchangeability assumption. We construct a non-negative test martingale to\nmaintain prediction validity even in dynamic and non-exchangeable environments.\nCross-dataset tests confirm our method's ability to uphold reliable statistical\nguarantees in realistic, evolving data scenarios.", "AI": {"tldr": "A risk-controlled prediction framework for speech emotion recognition (SER) is proposed to mitigate road rage by ensuring statistically rigorous guarantees on prediction accuracy, outperforming traditional methods.", "motivation": "Road rage, caused by emotional suppression and outbursts, threatens road safety. Current SER methods lack reliability and calibration, limiting their effectiveness in safety-critical scenarios.", "method": "A novel framework uses a calibration set and a binary loss function to optimize predictions, ensuring expected test loss is bounded by a user-specified risk level. It includes small-batch online calibration for dynamic environments.", "result": "The framework consistently achieves a minimum coverage of 1 - \u03b1 across six baseline models and two datasets, controlling marginal error rates robustly.", "conclusion": "The proposed method provides reliable statistical guarantees in evolving data scenarios, enhancing SER's effectiveness for road safety."}}
{"id": "2505.03039", "pdf": "https://arxiv.org/pdf/2505.03039", "abs": "https://arxiv.org/abs/2505.03039", "authors": ["Yuezhou Zhang", "Amos A. Folarin", "Callum Stewart", "Heet Sankesara", "Yatharth Ranjan", "Pauline Conde", "Akash Roy Choudhury", "Shaoxiong Sun", "Zulqarnain Rashid", "Richard J. B. Dobson"], "title": "An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Continuous monitoring of behavior and physiology via wearable devices offers\na novel, objective method for the early detection of worsening depression and\nanxiety. In this study, we present an explainable anomaly detection framework\nthat identifies clinically meaningful increases in symptom severity using\nconsumer-grade wearable data. Leveraging data from 2,023 participants with\ndefined healthy baselines, our LSTM autoencoder model learned normal health\npatterns of sleep duration, step count, and resting heart rate. Anomalies were\nflagged when self-reported depression or anxiety scores increased by >=5 points\n(a threshold considered clinically significant). The model achieved an adjusted\nF1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393\nsymptom-worsening episodes across 341 participants, with higher performance\nobserved for episodes involving concurrent depression and anxiety escalation\n(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =\n0.85). Model interpretability was supported by SHAP-based analysis, which\nidentified resting heart rate as the most influential feature in 71.4\npercentage of detected anomalies, followed by physical activity and sleep.\nTogether, our findings highlight the potential of explainable anomaly detection\nto enable personalized, scalable, and proactive mental health monitoring in\nreal-world settings.", "AI": {"tldr": "An explainable anomaly detection framework using wearable data detects worsening depression and anxiety with high accuracy, leveraging LSTM autoencoders and SHAP analysis for interpretability.", "motivation": "To enable early, objective detection of worsening mental health symptoms using consumer-grade wearable devices for proactive monitoring.", "method": "LSTM autoencoder model trained on healthy baseline data (sleep, step count, resting heart rate) to detect anomalies when symptom scores increased by \u22655 points.", "result": "Achieved F1-score of 0.80, with higher performance for concurrent depression/anxiety (F1=0.84) and larger symptom changes (F1=0.85). Resting heart rate was the most influential feature.", "conclusion": "Explainable anomaly detection can enable scalable, personalized mental health monitoring in real-world settings."}}
{"id": "2505.02959", "pdf": "https://arxiv.org/pdf/2505.02959", "abs": "https://arxiv.org/abs/2505.02959", "authors": ["Enrique Nueve", "Bo Waggoner"], "title": "Smooth Quadratic Prediction Markets", "categories": ["cs.LG", "cs.GT"], "comment": null, "summary": "When agents trade in a Duality-based Cost Function prediction market, they\ncollectively implement the learning algorithm Follow-The-Regularized-Leader. We\nask whether other learning algorithms could be used to inspire the design of\nprediction markets. By decomposing and modifying the Duality-based Cost\nFunction Market Maker's (DCFMM) pricing mechanism, we propose a new prediction\nmarket, called the Smooth Quadratic Prediction Market, the incentivizes agents\nto collectively implement general steepest gradient descent. Relative to the\nDCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary\nloss for AD securities while preserving axiom guarantees such as the existence\nof instantaneous price, information incorporation, expressiveness, no\narbitrage, and a form of incentive compatibility. To motivate the application\nof the Smooth Quadratic Prediction Market, we independently examine agents'\ntrading behavior under two realistic constraints: bounded budgets and buy-only\nsecurities. Finally, we provide an introductory analysis of an approach to\nfacilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.\nOur results suggest future designs where the price update rule is separate from\nthe fee structure, yet guarantees are preserved.", "AI": {"tldr": "The paper introduces the Smooth Quadratic Prediction Market, which generalizes steepest gradient descent, improving worst-case monetary loss while preserving key guarantees. It also explores trading behavior under constraints and adaptive liquidity.", "motivation": "To explore if other learning algorithms beyond Follow-The-Regularized-Leader can inspire prediction market designs, improving guarantees and practical constraints.", "method": "Decompose and modify the Duality-based Cost Function Market Maker's pricing mechanism to propose the Smooth Quadratic Prediction Market.", "result": "The new market improves worst-case monetary loss for AD securities while preserving key axioms like no arbitrage and incentive compatibility.", "conclusion": "The Smooth Quadratic Prediction Market offers a promising design with separable price update rules and fee structures, suggesting future research directions."}}
{"id": "2505.03295", "pdf": "https://arxiv.org/pdf/2505.03295", "abs": "https://arxiv.org/abs/2505.03295", "authors": ["Luis Miguel Vieira da Silva", "Aljosha K\u00f6cher", "Nicolas K\u00f6nig", "Felix Gehlhoff", "Alexander Fay"], "title": "Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces", "categories": ["cs.AI", "cs.RO", "cs.SE"], "comment": null, "summary": "Modern automation systems increasingly rely on modular architectures, with\ncapabilities and skills as one solution approach. Capabilities define the\nfunctions of resources in a machine-readable form and skills provide the\nconcrete implementations that realize those capabilities. However, the\ndevelopment of a skill implementation conforming to a corresponding capability\nremains a time-consuming and challenging task. In this paper, we present a\nmethod that treats capabilities as contracts for skill implementations and\nleverages large language models to generate executable code based on natural\nlanguage user input. A key feature of our approach is the integration of\nexisting software libraries and interface technologies, enabling the generation\nof skill implementations across different target languages. We introduce a\nframework that allows users to incorporate their own libraries and resource\ninterfaces into the code generation process through a retrieval-augmented\ngeneration architecture. The proposed method is evaluated using an autonomous\nmobile robot controlled via Python and ROS 2, demonstrating the feasibility and\nflexibility of the approach.", "AI": {"tldr": "A method using large language models to generate executable skill implementations from natural language input, integrating existing libraries and interfaces for flexibility.", "motivation": "To simplify and accelerate the development of skill implementations conforming to capabilities in modular automation systems.", "method": "Treats capabilities as contracts, uses large language models for code generation, and integrates existing libraries and interfaces via a retrieval-augmented framework.", "result": "Demonstrated feasibility and flexibility using an autonomous mobile robot controlled via Python and ROS 2.", "conclusion": "The approach effectively reduces development time and complexity for skill implementations in modular systems."}}
{"id": "2411.15255", "pdf": "https://arxiv.org/pdf/2411.15255", "abs": "https://arxiv.org/abs/2411.15255", "authors": ["Gehui Li", "Bin Chen", "Chen Zhao", "Lei Zhang", "Jian Zhang"], "title": "OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Exposure correction is a fundamental problem in computer vision and image\nprocessing. Recently, frequency domain-based methods have achieved impressive\nimprovement, yet they still struggle with complex real-world scenarios under\nextreme exposure conditions. This is due to the local convolutional receptive\nfields failing to model long-range dependencies in the spectrum, and the\nnon-generative learning paradigm being inadequate for retrieving lost details\nfrom severely degraded regions. In this paper, we propose Omnidirectional\nSpectral Mamba (OSMamba), a novel exposure correction network that incorporates\nthe advantages of state space models and generative diffusion models to address\nthese limitations. Specifically, OSMamba introduces an omnidirectional spectral\nscanning mechanism that adapts Mamba to the frequency domain to capture\ncomprehensive long-range dependencies in both the amplitude and phase spectra\nof deep image features, hence enhancing illumination correction and structure\nrecovery. Furthermore, we develop a dual-domain prior generator that learns\nfrom well-exposed images to generate a degradation-free diffusion prior\ncontaining correct information about severely under- and over-exposed regions\nfor better detail restoration. Extensive experiments on multiple-exposure and\nmixed-exposure datasets demonstrate that the proposed OSMamba achieves\nstate-of-the-art performance both quantitatively and qualitatively.", "AI": {"tldr": "OSMamba is a novel exposure correction network combining state space models and generative diffusion models to improve performance in extreme exposure conditions by capturing long-range dependencies and restoring lost details.", "motivation": "Existing frequency domain-based methods struggle with complex real-world scenarios due to limited receptive fields and non-generative learning, failing to restore severely degraded regions.", "method": "OSMamba uses an omnidirectional spectral scanning mechanism to capture long-range dependencies in frequency domain features and a dual-domain prior generator for detail restoration via diffusion models.", "result": "OSMamba achieves state-of-the-art performance on multiple- and mixed-exposure datasets, excelling in both quantitative and qualitative metrics.", "conclusion": "OSMamba effectively addresses limitations in exposure correction by integrating state space and diffusion models, offering superior performance in challenging scenarios."}}
{"id": "2505.03732", "pdf": "https://arxiv.org/pdf/2505.03732", "abs": "https://arxiv.org/abs/2505.03732", "authors": ["Jacqueline Harding", "Tobias Gerstenberg", "Thomas Icard"], "title": "A Communication-First Account of Explanation", "categories": ["cs.MA"], "comment": null, "summary": "This paper develops a formal account of causal explanation, grounded in a\ntheory of conversational pragmatics, and inspired by the interventionist idea\nthat explanation is about asking and answering\nwhat-if-things-had-been-different questions. We illustrate the fruitfulness of\nthe account, relative to previous accounts, by showing that widely recognised\nexplanatory virtues emerge naturally, as do subtle empirical patterns\nconcerning the impact of norms on causal judgments. This shows the value of a\ncommunication-first approach to explanation: getting clear on explanation's\ncommunicative dimension is an important prerequisite for philosophical work on\nexplanation. The result is a simple but powerful framework for incorporating\ninsights from the cognitive sciences into philosophical work on explanation,\nwhich will be useful for philosophers or cognitive scientists interested in\nexplanation.", "AI": {"tldr": "A formal account of causal explanation is developed, grounded in conversational pragmatics and interventionist ideas, showing the value of a communication-first approach.", "motivation": "To address the communicative dimension of explanation and integrate cognitive science insights into philosophical work on explanation.", "method": "Uses a theory of conversational pragmatics and interventionist ideas to analyze causal explanation.", "result": "Widely recognized explanatory virtues emerge naturally, and norms' impact on causal judgments is clarified.", "conclusion": "A communication-first approach provides a simple yet powerful framework for philosophical and cognitive science work on explanation."}}
{"id": "2505.03025", "pdf": "https://arxiv.org/pdf/2505.03025", "abs": "https://arxiv.org/abs/2505.03025", "authors": ["Steven Bedrick", "A. Seza Do\u011fru\u00f6z", "Sergiu Nisioi"], "title": "A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Synthetic data sets are used across linguistic domains and NLP tasks,\nparticularly in scenarios where authentic data is limited (or even\nnon-existent). One such domain is that of clinical (healthcare) contexts, where\nthere exist significant and long-standing challenges (e.g., privacy,\nanonymization, and data governance) which have led to the development of an\nincreasing number of synthetic datasets. One increasingly important category of\nclinical dataset is that of clinical dialogues which are especially sensitive\nand difficult to collect, and as such are commonly synthesized.\n  While such synthetic datasets have been shown to be sufficient in some\nsituations, little theory exists to inform how they may be best used and\ngeneralized to new applications. In this paper, we provide an overview of how\nsynthetic datasets are created, evaluated and being used for dialogue related\ntasks in the medical domain. Additionally, we propose a novel typology for use\nin classifying types and degrees of data synthesis, to facilitate comparison\nand evaluation.", "AI": {"tldr": "The paper discusses the use of synthetic datasets in clinical dialogues, addressing challenges like privacy and proposing a typology for classification.", "motivation": "Clinical dialogue datasets are sensitive and hard to collect, leading to reliance on synthetic data, but there's limited theory on their optimal use and generalization.", "method": "The paper reviews synthetic dataset creation and evaluation methods for medical dialogues and introduces a novel typology for classification.", "result": "The typology aids in comparing and evaluating synthetic datasets, addressing gaps in their application and generalization.", "conclusion": "The proposed typology enhances understanding and utility of synthetic clinical dialogue datasets, supporting better evaluation and application."}}
{"id": "2505.03093", "pdf": "https://arxiv.org/pdf/2505.03093", "abs": "https://arxiv.org/abs/2505.03093", "authors": ["Siming He", "Zachary Osman", "Fernando Cladera", "Dexter Ong", "Nitant Rai", "Patrick Corey Green", "Vijay Kumar", "Pratik Chaudhari"], "title": "Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera", "categories": ["cs.CV"], "comment": null, "summary": "Forest inventories rely on accurate measurements of the diameter at breast\nheight (DBH) for ecological monitoring, resource management, and carbon\naccounting. While LiDAR-based techniques can achieve centimeter-level\nprecision, they are cost-prohibitive and operationally complex. We present a\nlow-cost alternative that only needs a consumer-grade 360 video camera. Our\nsemi-automated pipeline comprises of (i) a dense point cloud reconstruction\nusing Structure from Motion (SfM) photogrammetry software called Agisoft\nMetashape, (ii) semantic trunk segmentation by projecting Grounded Segment\nAnything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based\ntechnique to estimate cross section shape and DBH. We introduce an interactive\nvisualization tool for inspecting segmented trees and their estimated DBH. On\n61 acquisitions of 43 trees under a variety of conditions, our method attains\nmedian absolute relative errors of 5-9% with respect to \"ground-truth\" manual\nmeasurements. This is only 2-4% higher than LiDAR-based estimates, while\nemploying a single 360 camera that costs orders of magnitude less, requires\nminimal setup, and is widely available.", "AI": {"tldr": "A low-cost method using a consumer-grade 360 camera achieves near-LiDAR accuracy (5-9% error) for measuring tree DBH, with minimal setup and cost.", "motivation": "High costs and complexity of LiDAR-based DBH measurements necessitate a cheaper, simpler alternative for forest inventories.", "method": "Uses SfM photogrammetry, semantic trunk segmentation with Grounded SAM, and RANSAC-based DBH estimation. Includes an interactive visualization tool.", "result": "Achieves median absolute relative errors of 5-9% compared to manual measurements, close to LiDAR accuracy.", "conclusion": "The method offers a viable, cost-effective alternative to LiDAR for DBH measurement in forest inventories."}}
{"id": "2505.02974", "pdf": "https://arxiv.org/pdf/2505.02974", "abs": "https://arxiv.org/abs/2505.02974", "authors": ["Fabien Casenave", "Xavier Roynard", "Brian Staber", "Nissrine Akkari", "William Piat", "Michele Alessandro Bucci", "Abbas Kabalan", "Xuan Minh Vuong Nguyen", "Luca Saverio", "Rapha\u00ebl Carpintero Perez", "Anthony Kalaydjian", "Samy Fouch\u00e9", "Thierry Gonon", "Ghassan Najjar", "Emmanuel Menier", "Matthieu Nastorg", "Christian Rey"], "title": "Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning-based surrogate models have emerged as a powerful tool to\naccelerate simulation-driven scientific workflows. However, their widespread\nadoption is hindered by the lack of large-scale, diverse, and standardized\ndatasets tailored to physics-based simulations. While existing initiatives\nprovide valuable contributions, many are limited in scope-focusing on specific\nphysics domains, relying on fragmented tooling, or adhering to overly\nsimplistic datamodels that restrict generalization. To address these\nlimitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and\nextensible framework for representing and sharing datasets of physics\nsimulations. PLAID defines a unified standard for describing simulation data\nand is accompanied by a library for creating, reading, and manipulating complex\ndatasets across a wide range of physical use cases (gitlab.com/drti/plaid). We\nrelease six carefully crafted datasets under the PLAID standard, covering\nstructural mechanics and computational fluid dynamics, and provide baseline\nbenchmarks using representative learning methods. Benchmarking tools are made\navailable on Hugging Face, enabling direct participation by the community and\ncontribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).", "AI": {"tldr": "PLAID introduces a flexible framework for standardized physics simulation datasets, addressing limitations of existing tools, and provides benchmarks for community participation.", "motivation": "The lack of large-scale, diverse, and standardized datasets for physics-based simulations hinders the adoption of machine learning surrogate models.", "method": "PLAID is introduced as a unified standard for simulation data, accompanied by a library for dataset manipulation and six curated datasets.", "result": "Six datasets covering structural mechanics and computational fluid dynamics are released, with benchmarks and tools for community evaluation.", "conclusion": "PLAID enables broader adoption of machine learning in physics simulations by standardizing datasets and fostering community collaboration."}}
{"id": "2505.03315", "pdf": "https://arxiv.org/pdf/2505.03315", "abs": "https://arxiv.org/abs/2505.03315", "authors": ["Kanghyun Jo", "Jehwan Choi", "Kwanho Kim", "Seongmin Kim", "Duy-Linh Nguyen", "Xuan-Thuy Vo", "Adri Priadana", "Tien-Dat Tran"], "title": "Artificial Behavior Intelligence: Technology, Challenges, and Future Directions", "categories": ["cs.AI"], "comment": "9 pages, 6 figures, Pre-print for IWIS2025", "summary": "Understanding and predicting human behavior has emerged as a core capability\nin various AI application domains such as autonomous driving, smart healthcare,\nsurveillance systems, and social robotics. This paper defines the technical\nframework of Artificial Behavior Intelligence (ABI), which comprehensively\nanalyzes and interprets human posture, facial expressions, emotions, behavioral\nsequences, and contextual cues. It details the essential components of ABI,\nincluding pose estimation, face and emotion recognition, sequential behavior\nanalysis, and context-aware modeling. Furthermore, we highlight the\ntransformative potential of recent advances in large-scale pretrained models,\nsuch as large language models (LLMs), vision foundation models, and multimodal\nintegration models, in significantly improving the accuracy and\ninterpretability of behavior recognition. Our research team has a strong\ninterest in the ABI domain and is actively conducting research, particularly\nfocusing on the development of intelligent lightweight models capable of\nefficiently inferring complex human behaviors. This paper identifies several\ntechnical challenges that must be addressed to deploy ABI in real-world\napplications including learning behavioral intelligence from limited data,\nquantifying uncertainty in complex behavior prediction, and optimizing model\nstructures for low-power, real-time inference. To tackle these challenges, our\nteam is exploring various optimization strategies including lightweight\ntransformers, graph-based recognition architectures, energy-aware loss\nfunctions, and multimodal knowledge distillation, while validating their\napplicability in real-time environments.", "AI": {"tldr": "The paper introduces Artificial Behavior Intelligence (ABI), a framework for analyzing human behavior using AI, and discusses its components, challenges, and potential solutions.", "motivation": "To enhance AI's capability in understanding and predicting human behavior for applications like autonomous driving and healthcare.", "method": "ABI integrates pose estimation, emotion recognition, behavior analysis, and context modeling, leveraging pretrained models for accuracy.", "result": "Identifies challenges like limited data learning and real-time inference, proposing solutions like lightweight transformers and knowledge distillation.", "conclusion": "ABI holds transformative potential but requires addressing technical hurdles for real-world deployment."}}
{"id": "2412.06690", "pdf": "https://arxiv.org/pdf/2412.06690", "abs": "https://arxiv.org/abs/2412.06690", "authors": ["Ciro Benito Raggio", "Mathias Krohmer Zabaleta", "Nils Skupien", "Oliver Blanck", "Francesco Cicone", "Giuseppe Lucio Cascini", "Paolo Zaffino", "Lucia Migliorelli", "Maria Francesca Spadea"], "title": "FedSynthCT-Brain: A Federated Learning Framework for Multi-Institutional Brain MRI-to-CT Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The generation of Synthetic Computed Tomography (sCT) images has become a\npivotal methodology in modern clinical practice, particularly in the context of\nRadiotherapy (RT) treatment planning. The use of sCT enables the calculation of\ndoses, pushing towards Magnetic Resonance Imaging (MRI) guided radiotherapy\ntreatments. Deep learning methods for MRI-to-sCT have shown promising results,\nbut their reliance on single-centre training dataset limits generalisation\ncapabilities to diverse clinical settings. Moreover, creating centralised\nmulti-centre datasets may pose privacy concerns. To address the aforementioned\nissues, we introduced FedSynthCT-Brain, an approach based on the Federated\nLearning (FL) paradigm for MRI-to-sCT in brain imaging. This is among the first\napplications of FL for MRI-to-sCT, employing a cross-silo horizontal FL\napproach that allows multiple centres to collaboratively train a U-Net-based\ndeep learning model. We validated our method using real multicentre data from\nfour European and American centres, simulating heterogeneous scanner types and\nacquisition modalities, and tested its performance on an independent dataset\nfrom a centre outside the federation. In the case of the unseen centre, the\nfederated model achieved a median Mean Absolute Error (MAE) of $102.0$ HU\nacross 23 patients, with an interquartile range of $96.7-110.5$ HU. The median\n(interquartile range) for the Structural Similarity Index (SSIM) and the Peak\nSignal to Noise Ratio (PNSR) were $0.89 (0.86-0.89)$ and $26.58 (25.52-27.42)$,\nrespectively. The analysis of the results showed acceptable performances of the\nfederated approach, thus highlighting the potential of FL to enhance MRI-to-sCT\nto improve generalisability and advancing safe and equitable clinical\napplications while fostering collaboration and preserving data privacy.", "AI": {"tldr": "FedSynthCT-Brain uses federated learning for MRI-to-sCT in brain imaging, improving generalizability and privacy in multi-centre settings.", "motivation": "Address limitations of single-centre deep learning models for MRI-to-sCT, ensuring privacy and generalizability in clinical settings.", "method": "Cross-silo horizontal federated learning with a U-Net model, validated on multi-centre data from four centres.", "result": "Achieved median MAE of 102.0 HU, SSIM of 0.89, and PSNR of 26.58 on unseen data.", "conclusion": "Federated learning enhances MRI-to-sCT generalizability and privacy, supporting equitable clinical applications."}}
{"id": "2505.02842", "pdf": "https://arxiv.org/pdf/2505.02842", "abs": "https://arxiv.org/abs/2505.02842", "authors": ["Olga Mironenko", "Hadi Banaee", "Amy Loutfi"], "title": "Evaluation of Coordination Strategies for Underground Automated Vehicle Fleets in Mixed Traffic", "categories": ["physics.soc-ph", "cs.HC", "cs.MA", "cs.RO", "cs.SY", "eess.SY", "I.2.9; I.2.11; H.1.2"], "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  Intelligent Vehicles Symposium (IV 2025)", "summary": "This study investigates the efficiency and safety outcomes of implementing\ndifferent adaptive coordination models for automated vehicle (AV) fleets,\nmanaged by a centralized coordinator that dynamically responds to\nhuman-controlled vehicle behavior. The simulated scenarios replicate an\nunderground mining environment characterized by narrow tunnels with limited\nconnectivity. To address the unique challenges of such settings, we propose a\nnovel metric - Path Overlap Density (POD) - to predict efficiency and\npotentially the safety performance of AV fleets. The study also explores the\nimpact of map features on AV fleets performance. The results demonstrate that\nboth AV fleet coordination strategies and underground tunnel network\ncharacteristics significantly influence overall system performance. While map\nfeatures are critical for optimizing efficiency, adaptive coordination\nstrategies are essential for ensuring safe operations.", "AI": {"tldr": "The study evaluates adaptive coordination models for AV fleets in underground mining, introducing POD as a novel metric. Results show coordination strategies and tunnel features impact performance, with adaptive coordination ensuring safety.", "motivation": "To address efficiency and safety challenges of AV fleets in confined, low-connectivity environments like underground mines.", "method": "Simulated scenarios in an underground mining setting, using POD to predict performance and analyzing coordination models and map features.", "result": "Coordination strategies and tunnel network characteristics significantly affect AV fleet performance. Adaptive coordination ensures safety, while map features optimize efficiency.", "conclusion": "Adaptive coordination is vital for safe AV fleet operations in confined spaces, with map features playing a key role in efficiency."}}
{"id": "2505.03030", "pdf": "https://arxiv.org/pdf/2505.03030", "abs": "https://arxiv.org/abs/2505.03030", "authors": ["Sicong Huang", "Jincheng He", "Shiyuan Huang", "Karthik Raja Anandan", "Arkajyoti Chakraborty", "Ian Lane"], "title": "UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output", "categories": ["cs.CL"], "comment": "6 pages, 1 figure", "summary": "Hallucinations pose a significant challenge for large language models when\nanswering knowledge-intensive queries. As LLMs become more widely adopted, it\nis crucial not only to detect if hallucinations occur but also to pinpoint\nexactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:\nMultilingual Shared-task on Hallucinations and Related Observable\nOvergeneration Mistakes, is a recent effort in this direction. This paper\ndescribes the UCSC system submission to the shared Mu-SHROOM task. We introduce\na framework that first retrieves relevant context, next identifies false\ncontent from the answer, and finally maps them back to spans in the LLM output.\nThe process is further enhanced by automatically optimizing prompts. Our system\nachieves the highest overall performance, ranking #1 in average position across\nall languages. We release our code and experiment results.", "AI": {"tldr": "The paper presents a framework for detecting and localizing hallucinations in LLM outputs, achieving top performance in the Mu-SHROOM task.", "motivation": "Address the challenge of identifying and pinpointing hallucinations in LLM responses to knowledge-intensive queries.", "method": "A framework involving context retrieval, false content identification, and span mapping, enhanced by prompt optimization.", "result": "Ranked #1 in average performance across all languages in the Mu-SHROOM task.", "conclusion": "The system effectively detects and localizes hallucinations, with code and results made publicly available."}}
{"id": "2505.03097", "pdf": "https://arxiv.org/pdf/2505.03097", "abs": "https://arxiv.org/abs/2505.03097", "authors": ["Lei Wang", "Senmao Li", "Fei Yang", "Jianye Wang", "Ziheng Zhang", "Yuhan Liu", "Yaxing Wang", "Jian Yang"], "title": "Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "The diffusion models, in early stages focus on constructing basic image\nstructures, while the refined details, including local features and textures,\nare generated in later stages. Thus the same network layers are forced to learn\nboth structural and textural information simultaneously, significantly\ndiffering from the traditional deep learning architectures (e.g., ResNet or\nGANs) which captures or generates the image semantic information at different\nlayers. This difference inspires us to explore the time-wise diffusion models.\nWe initially investigate the key contributions of the U-Net parameters to the\ndenoising process and identify that properly zeroing out certain parameters\n(including large parameters) contributes to denoising, substantially improving\nthe generation quality on the fly. Capitalizing on this discovery, we propose a\nsimple yet effective method-termed ``MaskUNet''- that enhances generation\nquality with negligible parameter numbers. Our method fully leverages timestep-\nand sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer\ntwo fine-tuning strategies: a training-based approach and a training-free\napproach, including tailored networks and optimization functions. In zero-shot\ninference on the COCO dataset, MaskUNet achieves the best FID score and further\ndemonstrates its effectiveness in downstream task evaluations. Project page:\nhttps://gudaochangsheng.github.io/MaskUnet-Page/", "AI": {"tldr": "The paper explores time-wise diffusion models, proposing MaskUNet to enhance generation quality by selectively zeroing out U-Net parameters, achieving superior results on COCO.", "motivation": "Diffusion models handle structure and texture simultaneously, unlike traditional architectures. This inspires investigation into parameter contributions and optimization.", "method": "Proposes MaskUNet, leveraging timestep- and sample-dependent U-Net parameters, with two fine-tuning strategies: training-based and training-free.", "result": "MaskUNet achieves the best FID score on COCO in zero-shot inference and excels in downstream tasks.", "conclusion": "MaskUNet is a simple yet effective method for improving diffusion model performance with minimal added parameters."}}
{"id": "2505.02985", "pdf": "https://arxiv.org/pdf/2505.02985", "abs": "https://arxiv.org/abs/2505.02985", "authors": ["Mohammad Partohaghighi", "Roummel Marcia", "YangQuan Chen"], "title": "More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems", "categories": ["cs.LG", "math.OC"], "comment": "8 pages submitted to IEEE CDC2025. arXiv admin note: substantial text\n  overlap with arXiv:2503.13764", "summary": "Fractional-order stochastic gradient descent (FOSGD) leverages fractional\nexponents to capture long-memory effects in optimization. However, its utility\nis often limited by the difficulty of tuning and stabilizing these exponents.\nWe propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which\nintegrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to\nadapt the fractional exponent in a data-driven manner. By tracking model\nsensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the\nexponent to mitigate oscillations and hasten convergence. Theoretically, for\nonoconvex optimization problems, this approach preserves the advantages of\nfractional memory without the sluggish or unstable behavior observed in na\\\"ive\nfractional SGD. Empirical evaluations in Gaussian and $\\alpha$-stable noise\nscenarios using an autoregressive (AR) model highlight faster convergence and\nmore robust parameter estimates compared to baseline methods, underscoring the\npotential of dimension-aware fractional techniques for advanced modeling and\nestimation tasks.", "AI": {"tldr": "2SEDFOSGD combines fractional-order SGD with the 2SED algorithm to adaptively tune fractional exponents, improving convergence and stability in nonconvex optimization.", "motivation": "FOSGD's long-memory effects are hindered by unstable exponent tuning, prompting a need for a data-driven solution.", "method": "Integrates 2SED with FOSGD to dynamically adjust fractional exponents based on model sensitivity and effective dimensionality.", "result": "Outperforms baselines in convergence speed and robustness, especially in Gaussian and \u03b1-stable noise scenarios.", "conclusion": "2SEDFOSGD offers a promising approach for stable and efficient fractional-order optimization in complex modeling tasks."}}
{"id": "2505.03332", "pdf": "https://arxiv.org/pdf/2505.03332", "abs": "https://arxiv.org/abs/2505.03332", "authors": ["Evgeny Markhasin"], "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning", "categories": ["cs.AI"], "comment": "22 pages, 36 pages (references and appendixes)", "summary": "Critical peer review of scientific manuscripts presents a significant\nchallenge for Large Language Models (LLMs), partly due to data limitations and\nthe complexity of expert reasoning. This report introduces Persistent Workflow\nPrompting (PWP), a potentially broadly applicable prompt engineering\nmethodology designed to bridge this gap using standard LLM chat interfaces\n(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical\nanalysis of experimental chemistry manuscripts, featuring a hierarchical,\nmodular architecture (structured via Markdown) that defines detailed analysis\nworkflows. We develop this PWP prompt through iterative application of\nmeta-prompting techniques and meta-reasoning aimed at systematically codifying\nexpert review workflows, including tacit knowledge. Submitted once at the start\nof a session, this PWP prompt equips the LLM with persistent workflows\ntriggered by subsequent queries, guiding modern reasoning LLMs through\nsystematic, multimodal evaluations. Demonstrations show the PWP-guided LLM\nidentifying major methodological flaws in a test case while mitigating LLM\ninput bias and performing complex tasks, including distinguishing claims from\nevidence, integrating text/photo/figure analysis to infer parameters, executing\nquantitative feasibility checks, comparing estimates against claims, and\nassessing a priori plausibility. To ensure transparency and facilitate\nreplication, we provide full prompts, detailed demonstration analyses, and logs\nof interactive chats as supplementary resources. Beyond the specific\napplication, this work offers insights into the meta-development process\nitself, highlighting the potential of PWP, informed by detailed workflow\nformalization, to enable sophisticated analysis using readily available LLMs\nfor complex scientific tasks.", "AI": {"tldr": "Persistent Workflow Prompting (PWP) is introduced to enhance LLMs' critical peer review of scientific manuscripts, overcoming data and reasoning challenges. It uses modular workflows for systematic analysis.", "motivation": "Addressing the limitations of LLMs in critical peer review due to data constraints and complex expert reasoning.", "method": "PWP employs hierarchical, modular workflows (structured via Markdown) developed through meta-prompting and meta-reasoning to codify expert review processes.", "result": "PWP enables LLMs to identify methodological flaws, mitigate bias, and perform complex tasks like integrating multimodal data and quantitative checks.", "conclusion": "PWP demonstrates potential for sophisticated scientific analysis using standard LLMs, with transparency and replication supported by provided resources."}}
{"id": "2502.18225", "pdf": "https://arxiv.org/pdf/2502.18225", "abs": "https://arxiv.org/abs/2502.18225", "authors": ["Jun Zeng", "Debesh Jha", "Ertugrul Aktas", "Elif Keles", "Alpay Medetalibeyoglu", "Matthew Antalek", "Amir A. Borhani", "Daniela P. Ladner", "Gorkem Durak", "Ulas Bagci"], "title": "Liver Cirrhosis Stage Estimation from MRI with Deep Learning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "7 pages, 1 figure", "summary": "We present an end-to-end deep learning framework for automated liver\ncirrhosis stage estimation from multi-sequence MRI. Cirrhosis is the severe\nscarring (fibrosis) of the liver and a common endpoint of various chronic liver\ndiseases. Early diagnosis is vital to prevent complications such as\ndecompensation and cancer, which significantly decreases life expectancy.\nHowever, diagnosing cirrhosis in its early stages is challenging, and patients\noften present with life-threatening complications. Our approach integrates\nmulti-scale feature learning with sequence-specific attention mechanisms to\ncapture subtle tissue variations across cirrhosis progression stages. Using\nCirrMRI600+, a large-scale publicly available dataset of 628 high-resolution\nMRI scans from 339 patients, we demonstrate state-of-the-art performance in\nthree-stage cirrhosis classification. Our best model achieves 72.8% accuracy on\nT1W and 63.8% on T2W sequences, significantly outperforming traditional\nradiomics-based approaches. Through extensive ablation studies, we show that\nour architecture effectively learns stage-specific imaging biomarkers. We\nestablish new benchmarks for automated cirrhosis staging and provide insights\nfor developing clinically applicable deep learning systems. The source code\nwill be available at https://github.com/JunZengz/CirrhosisStage.", "AI": {"tldr": "An end-to-end deep learning framework for automated liver cirrhosis stage estimation from MRI, achieving state-of-the-art performance.", "motivation": "Early cirrhosis diagnosis is challenging but crucial to prevent severe complications like decompensation and cancer.", "method": "Integrates multi-scale feature learning with sequence-specific attention mechanisms using a large dataset (CirrMRI600+).", "result": "Achieves 72.8% accuracy on T1W and 63.8% on T2W sequences, outperforming traditional methods.", "conclusion": "Sets new benchmarks for automated cirrhosis staging and offers insights for clinical deep learning applications."}}
{"id": "2505.02945", "pdf": "https://arxiv.org/pdf/2505.02945", "abs": "https://arxiv.org/abs/2505.02945", "authors": ["Egil Diau"], "title": "The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": "This is a position paper. Theoretical framework is finalized; minor\n  language revisions are ongoing. Submitted for feedback and public discussion", "summary": "A key challenge in multi-agent AI is modeling social cooperation under\nrealistic behavioral constraints. Many foundational concepts in economics and\nethics such as \"trust\" or \"morality\" are often defined informally, without\noperational criteria or cognitive grounding, which limits their testability and\nimplementation in artificial agents. Drawing on converging empirical evidence\nfrom primate behavior, infant cognition, and economic anthropology, we propose\na conceptual framework composed of three cognitively minimal mechanisms:\nindividual recognition, reciprocal credence, and cost return sensitivity. This\nframework reframes trust as a graded cognitive expectation, providing a\nsimulateable basis for reciprocal exchange in artificial agents, and enabling\nthe bottom-up emergence of scalable cooperation and institutional dynamics.", "AI": {"tldr": "A framework for modeling trust in multi-agent AI using three cognitive mechanisms: individual recognition, reciprocal credence, and cost return sensitivity.", "motivation": "To address the lack of operational definitions for concepts like trust and morality in AI, limiting their testability and implementation.", "method": "Proposes a conceptual framework based on empirical evidence from primate behavior, infant cognition, and economic anthropology.", "result": "Reframes trust as a graded cognitive expectation, enabling simulateable reciprocal exchange and scalable cooperation in AI.", "conclusion": "The framework provides a cognitively grounded, testable basis for modeling social cooperation in artificial agents."}}
{"id": "2505.03052", "pdf": "https://arxiv.org/pdf/2505.03052", "abs": "https://arxiv.org/abs/2505.03052", "authors": ["Ryan Wang", "Matthew Finlayson", "Luca Soldaini", "Swabha Swayamdipta", "Robin Jia"], "title": "Teaching Models to Understand (but not Generate) High-risk Data", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language model developers typically filter out high-risk content -- such as\ntoxic or copyrighted text -- from their pre-training data to prevent models\nfrom generating similar outputs. However, removing such data altogether limits\nmodels' ability to recognize and appropriately respond to harmful or sensitive\ncontent. In this paper, we introduce Selective Loss to Understand but Not\nGenerate (SLUNG), a pre-training paradigm through which models learn to\nunderstand high-risk data without learning to generate it. Instead of uniformly\napplying the next-token prediction loss, SLUNG selectively avoids incentivizing\nthe generation of high-risk tokens while ensuring they remain within the\nmodel's context window. As the model learns to predict low-risk tokens that\nfollow high-risk ones, it is forced to understand the high-risk content.\nThrough our experiments, we show that SLUNG consistently improves models'\nunderstanding of high-risk data (e.g., ability to recognize toxic content)\nwithout increasing its generation (e.g., toxicity of model responses). Overall,\nour SLUNG paradigm enables models to benefit from high-risk text that would\notherwise be filtered out.", "AI": {"tldr": "SLUNG is a pre-training method that helps models understand high-risk content without generating it, improving recognition of harmful text while avoiding its production.", "motivation": "Current methods filter out high-risk content, limiting models' ability to recognize and respond to harmful or sensitive text. SLUNG addresses this gap.", "method": "SLUNG selectively avoids incentivizing high-risk token generation while ensuring their context is understood, using next-token prediction loss selectively.", "result": "SLUNG improves models' understanding of high-risk data (e.g., toxicity recognition) without increasing its generation (e.g., toxic responses).", "conclusion": "SLUNG allows models to benefit from high-risk text that would otherwise be filtered, enhancing safety and utility."}}
{"id": "2505.03113", "pdf": "https://arxiv.org/pdf/2505.03113", "abs": "https://arxiv.org/abs/2505.03113", "authors": ["Zherui Zhang", "Rongtao Xu", "Jie Zhou", "Changwei Wang", "Xingtian Pei", "Wenhao Xu", "Jiguang Zhang", "Li Guo", "Longxiang Gao", "Wenbo Xu", "Shibiao Xu"], "title": "Image Recognition with Online Lightweight Vision Transformer: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "The Transformer architecture has achieved significant success in natural\nlanguage processing, motivating its adaptation to computer vision tasks. Unlike\nconvolutional neural networks, vision transformers inherently capture\nlong-range dependencies and enable parallel processing, yet lack inductive\nbiases and efficiency benefits, facing significant computational and memory\nchallenges that limit its real-world applicability. This paper surveys various\nonline strategies for generating lightweight vision transformers for image\nrecognition, focusing on three key areas: Efficient Component Design, Dynamic\nNetwork, and Knowledge Distillation. We evaluate the relevant exploration for\neach topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,\nparameters, throughput, and more to highlight their respective advantages,\ndisadvantages, and flexibility. Finally, we propose future research directions\nand potential challenges in the lightweighting of vision transformers with the\naim of inspiring further exploration and providing practical guidance for the\ncommunity. Project Page: https://github.com/ajxklo/Lightweight-VIT", "AI": {"tldr": "This paper surveys lightweight vision transformers for image recognition, focusing on Efficient Component Design, Dynamic Network, and Knowledge Distillation, evaluating trade-offs and proposing future directions.", "motivation": "Transformers in vision lack efficiency and inductive biases, limiting real-world use. This work aims to address these challenges.", "method": "Surveys and evaluates three strategies: Efficient Component Design, Dynamic Network, and Knowledge Distillation, tested on ImageNet-1K.", "result": "Analyzes trade-offs in precision, parameters, and throughput, highlighting pros and cons of each approach.", "conclusion": "Proposes future research directions and challenges for lightweight vision transformers to guide the community."}}
{"id": "2505.03031", "pdf": "https://arxiv.org/pdf/2505.03031", "abs": "https://arxiv.org/abs/2505.03031", "authors": ["Sean I. Young"], "title": "Radio: Rate-Distortion Optimization for Large Language Model Compression", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "In recent years, the compression of large language models (LLMs) has emerged\nas a key problem in facilitating LLM deployment on resource-limited devices,\nreducing compute costs, and mitigating the environmental footprint due to\nlarge-scale AI infrastructure. Here, we establish the foundations of LLM\nquantization from a rate-distortion theory perspective and propose a\nquantization technique based on simple rate-distortion optimization. Our\ntechnique scales to models containing hundreds of billions of weight parameters\nand offers users the flexibility to compress models, post-training, to a model\nsize or accuracy specified by the user.", "AI": {"tldr": "A method for compressing large language models (LLMs) using quantization based on rate-distortion theory, enabling flexible post-training compression to specified sizes or accuracies.", "motivation": "To address the challenges of deploying LLMs on resource-limited devices, reducing compute costs, and minimizing environmental impact.", "method": "Proposes a quantization technique grounded in rate-distortion theory, scalable to models with hundreds of billions of parameters.", "result": "Enables flexible post-training compression to user-specified model sizes or accuracies.", "conclusion": "The approach provides a practical solution for efficient LLM deployment while maintaining performance."}}
{"id": "2505.03359", "pdf": "https://arxiv.org/pdf/2505.03359", "abs": "https://arxiv.org/abs/2505.03359", "authors": ["June-Woo Kim", "Haram Yoon", "Wonkyo Oh", "Dawoon Jung", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection", "categories": ["cs.AI"], "comment": "Accepted to EMBC 2025", "summary": "Speech-based AI models are emerging as powerful tools for detecting\ndepression and the presence of Post-traumatic stress disorder (PTSD), offering\na non-invasive and cost-effective way to assess mental health. However, these\nmodels often struggle with gender bias, which can lead to unfair and inaccurate\npredictions. In this study, our study addresses this issue by introducing a\ndomain adversarial training approach that explicitly considers gender\ndifferences in speech-based depression and PTSD detection. Specifically, we\ntreat different genders as distinct domains and integrate this information into\na pretrained speech foundation model. We then validate its effectiveness on the\nE-DAIC dataset to assess its impact on performance. Experimental results show\nthat our method notably improves detection performance, increasing the F1-score\nby up to 13.29 percentage points compared to the baseline. This highlights the\nimportance of addressing demographic disparities in AI-driven mental health\nassessment.", "AI": {"tldr": "The paper introduces a domain adversarial training method to reduce gender bias in speech-based AI models for detecting depression and PTSD, improving F1-scores by up to 13.29%.", "motivation": "Gender bias in speech-based AI models for mental health detection leads to unfair and inaccurate predictions, necessitating a solution to address demographic disparities.", "method": "A domain adversarial training approach is used, treating genders as distinct domains and integrating this into a pretrained speech foundation model, validated on the E-DAIC dataset.", "result": "The method improves detection performance, increasing the F1-score by up to 13.29 percentage points over the baseline.", "conclusion": "Addressing gender disparities in AI-driven mental health assessment is crucial for fair and accurate predictions."}}
{"id": "2504.07606", "pdf": "https://arxiv.org/pdf/2504.07606", "abs": "https://arxiv.org/abs/2504.07606", "authors": ["Andr\u00e9s Bell-Navas", "Mar\u00eda Villalba-Orero", "Enrique Lara-Pezzi", "Jes\u00fas Garicano-Mena", "Soledad Le Clainche"], "title": "Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases", "categories": ["eess.IV", "cs.CV", "68T07, 68T10, 68T45, 62H35", "I.2; I.2.10; I.4.5; I.5.1; I.5.4; J.3"], "comment": "39 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:2404.19579", "summary": "Heart diseases constitute the main cause of international human defunction.\nAccording to the World Health Organization (WHO), approximately 18 million\ndeaths happen each year due to precisely heart diseases. In particular, heart\nfailures (HF) press the healthcare industry to develop systems for their early,\nrapid, and effective prediction. This work presents an automatic system based\non a novel deep learning framework which analyses in real-time echocardiography\nvideo sequences for the challenging and more specific task of heart failure\ntime prediction. This system works in two stages. The first one transforms the\ndata from a database of echocardiography video sequences into a machine\nlearning-compatible collection of annotated images which can be used in the\ntraining phase of any machine learning-based framework, including a deep\nlearning-based one. This stage includes the use of the Higher Order Dynamic\nMode Decomposition (HODMD) algorithm for both data augmentation and feature\nextraction. The second stage builds and trains a Vision Transformer (ViT).\nSelf-supervised learning (SSL) methods, so far barely explored in the\nliterature about heart failure prediction, are adopted to effectively train the\nViT from scratch, even with scarce databases. The designed neural network\nanalyses images from echocardiography sequences to estimate the time in which a\nheart failure will happen. The results obtained show the efficacy of the HODMD\nalgorithm and the superiority of the proposed system with respect to several\nestablished ViT and Convolutional Neural Network (CNN) architectures. The\nsource code will be incorporated into the next version release of the\nModelFLOWs-app software (https://github.com/modelflows/ModelFLOWs-app).", "AI": {"tldr": "An automatic system using deep learning predicts heart failure time from echocardiography videos, employing HODMD for data processing and a Vision Transformer for prediction, outperforming traditional methods.", "motivation": "Heart diseases cause 18 million annual deaths; early prediction of heart failure is critical for healthcare.", "method": "Two-stage system: 1) HODMD for data augmentation/feature extraction from echocardiography videos, 2) Vision Transformer trained with self-supervised learning for heart failure time prediction.", "result": "HODMD and Vision Transformer outperform established ViT and CNN architectures.", "conclusion": "The system is effective for heart failure prediction and will be integrated into ModelFLOWs-app."}}
{"id": "2505.03088", "pdf": "https://arxiv.org/pdf/2505.03088", "abs": "https://arxiv.org/abs/2505.03088", "authors": ["Akshita Gupta", "Yashwanth Kumar Nakka", "Changrak Choi", "Amir Rahmani"], "title": "Global Task-aware Fault Detection, Identification For On-Orbit Multi-Spacecraft Collaborative Inspection", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": "published. 33rd AAS AIAA Conference 2023", "summary": "In this paper, we present a global-to-local task-aware fault detection and\nidentification algorithm to detect failures in a multi-spacecraft system\nperforming a collaborative inspection (referred to as global) task. The\ninspection task is encoded as a cost functional $\\costH$ that informs global\n(task allocation and assignment) and local (agent-level) decision-making. The\nmetric $\\costH$ is a function of the inspection sensor model, and the agent\nfull-pose. We use the cost functional $\\costH$ to design a metric that compares\nthe expected and actual performance to detect the faulty agent using a\nthreshold. We use higher-order cost gradients $\\costH$ to derive a new metric\nto identify the type of fault, including task-specific sensor fault, an\nagent-level actuator, and sensor faults. Furthermore, we propose an approach to\ndesign adaptive thresholds for each fault mentioned above to incorporate the\ntime dependence of the inspection task. We demonstrate the efficacy of the\nproposed method empirically, by simulating and detecting faults (such as\ninspection sensor faults, actuators, and sensor faults) in a low-Earth orbit\ncollaborative spacecraft inspection task using the metrics and the threshold\ndesigned using the global task cost $\\costH$.", "AI": {"tldr": "A global-to-local fault detection algorithm for multi-spacecraft systems uses a cost functional to detect and identify faults, including sensor and actuator issues, with adaptive thresholds for task-specific failures.", "motivation": "To ensure reliable performance in collaborative spacecraft inspection tasks by detecting and identifying faults at both global and local levels.", "method": "Uses a cost functional derived from inspection sensor models and agent poses to design metrics for fault detection and identification, including adaptive thresholds for time-dependent tasks.", "result": "Empirical simulations demonstrate successful detection of faults like inspection sensor and actuator failures in low-Earth orbit spacecraft inspection tasks.", "conclusion": "The proposed method effectively detects and identifies faults in multi-spacecraft systems, enhancing reliability in collaborative inspection tasks."}}
{"id": "2505.03053", "pdf": "https://arxiv.org/pdf/2505.03053", "abs": "https://arxiv.org/abs/2505.03053", "authors": ["Jennifer Healey", "Laurie Byrum", "Md Nadeem Akhtar", "Surabhi Bhargava", "Moumita Sinha"], "title": "Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, no figures, presented at CHI 2025 workshop for Human\n  Evaluation and Auditing of Language Models", "summary": "LLM evaluation is challenging even the case of base models. In real world\ndeployments, evaluation is further complicated by the interplay of task\nspecific prompts and experiential context. At scale, bias evaluation is often\nbased on short context, fixed choice benchmarks that can be rapidly evaluated,\nhowever, these can lose validity when the LLMs' deployed context differs. Large\nscale human evaluation is often seen as too intractable and costly. Here we\npresent our journey towards developing a semi-automated bias evaluation\nframework for free text responses that has human insights at its core. We\ndiscuss how we developed an operational definition of bias that helped us\nautomate our pipeline and a methodology for classifying bias beyond multiple\nchoice. We additionally comment on how human evaluation helped us uncover\nproblematic templates in a bias benchmark.", "AI": {"tldr": "A semi-automated bias evaluation framework for LLM free text responses, combining human insights with automation to address challenges in bias assessment.", "motivation": "Evaluating bias in LLMs is complex, especially in real-world deployments where task-specific prompts and context interplay. Traditional benchmarks often lack validity, and human evaluation is costly.", "method": "Developed an operational definition of bias, automated the evaluation pipeline, and extended bias classification beyond multiple-choice formats. Incorporated human evaluation to identify issues in benchmarks.", "result": "Created a framework that balances automation with human insights, uncovering problematic templates in bias benchmarks.", "conclusion": "The semi-automated approach offers a scalable and valid solution for bias evaluation in LLMs, leveraging both human and automated methods."}}
{"id": "2505.03114", "pdf": "https://arxiv.org/pdf/2505.03114", "abs": "https://arxiv.org/abs/2505.03114", "authors": ["Teng Zhou", "Jax Luo", "Yuping Sun", "Yiheng Tan", "Shun Yao", "Nazim Haouchine", "Scott Raymond"], "title": "Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate MRI-to-CT translation promises the integration of complementary\nimaging information without the need for additional imaging sessions. Given the\npractical challenges associated with acquiring paired MRI and CT scans, the\ndevelopment of robust methods capable of leveraging unpaired datasets is\nessential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT\ntranslation methods, which predominantly rely on cycle consistency and\ncontrastive learning frameworks, frequently encounter challenges in accurately\ntranslating anatomical features that are highly discernible on CT but less\ndistinguishable on MRI, such as bone structures. This limitation renders these\napproaches less suitable for applications in radiation therapy, where precise\nbone representation is essential for accurate treatment planning. To address\nthis challenge, we propose a path- and bone-contour regularized approach for\nunpaired MRI-to-CT translation. In our method, MRI and CT images are projected\nto a shared latent space, where the MRI-to-CT mapping is modeled as a\ncontinuous flow governed by neural ordinary differential equations. The optimal\nmapping is obtained by minimizing the transition path length of the flow. To\nenhance the accuracy of translated bone structures, we introduce a trainable\nneural network to generate bone contours from MRI and implement mechanisms to\ndirectly and indirectly encourage the model to focus on bone contours and their\nadjacent regions. Evaluations conducted on three datasets demonstrate that our\nmethod outperforms existing unpaired MRI-to-CT translation approaches,\nachieving lower overall error rates. Moreover, in a downstream bone\nsegmentation task, our approach exhibits superior performance in preserving the\nfidelity of bone structures. Our code is available at:\nhttps://github.com/kennysyp/PaBoT.", "AI": {"tldr": "Proposes a path- and bone-contour regularized method for unpaired MRI-to-CT translation, improving accuracy for bone structures and outperforming existing methods.", "motivation": "Accurate MRI-to-CT translation is needed for applications like radiation therapy, but current unpaired methods struggle with bone structures.", "method": "Projects MRI and CT images to a shared latent space, models MRI-to-CT as a continuous flow using neural ODEs, and introduces bone-contour regularization.", "result": "Outperforms existing methods with lower error rates and better bone structure fidelity in segmentation tasks.", "conclusion": "The proposed method advances unpaired MRI-to-CT translation, especially for bone structures, with practical benefits for radiation therapy."}}
{"id": "2505.03042", "pdf": "https://arxiv.org/pdf/2505.03042", "abs": "https://arxiv.org/abs/2505.03042", "authors": ["Steven Tin Sui Luo"], "title": "A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields", "categories": ["cs.LG"], "comment": null, "summary": "Instant-NGP has been the state-of-the-art architecture of neural fields in\nrecent years. Its incredible signal-fitting capabilities are generally\nattributed to its multi-resolution hash grid structure and have been used and\nimproved in numerous following works. However, it is unclear how and why such a\nhash grid structure improves the capabilities of a neural network by such great\nmargins. A lack of principled understanding of the hash grid also implies that\nthe large set of hyperparameters accompanying Instant-NGP could only be tuned\nempirically without much heuristics. To provide an intuitive explanation of the\nworking principle of the hash grid, we propose a novel perspective, namely\ndomain manipulation. This perspective provides a ground-up explanation of how\nthe feature grid learns the target signal and increases the expressivity of the\nneural field by artificially creating multiples of pre-existing linear\nsegments. We conducted numerous experiments on carefully constructed\n1-dimensional signals to support our claims empirically and aid our\nillustrations. While our analysis mainly focuses on 1-dimensional signals, we\nshow that the idea is generalizable to higher dimensions.", "AI": {"tldr": "The paper explains the success of Instant-NGP's multi-resolution hash grid through a novel 'domain manipulation' perspective, showing how it enhances neural field expressivity by creating linear segments.", "motivation": "To understand why Instant-NGP's hash grid improves neural network performance and provide a principled explanation for its hyperparameters.", "method": "Proposes 'domain manipulation' to explain the hash grid's function, supported by experiments on 1D signals.", "result": "Demonstrates how the hash grid artificially increases expressivity by creating linear segments, with empirical validation.", "conclusion": "The 'domain manipulation' perspective generalizes beyond 1D, offering insights for optimizing neural fields."}}
{"id": "2505.03369", "pdf": "https://arxiv.org/pdf/2505.03369", "abs": "https://arxiv.org/abs/2505.03369", "authors": ["Yuanyuan Yang", "Yuan Shen", "Tianchen Sun", "Yangbin Xie"], "title": "Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten", "categories": ["cs.AI", "cs.CY"], "comment": "15 pages, 4 figures", "summary": "Free play is a fundamental aspect of early childhood education, supporting\nchildren's cognitive, social, emotional, and motor development. However,\nassessing children's development during free play poses significant challenges\ndue to the unstructured and spontaneous nature of the activity. Traditional\nassessment methods often rely on direct observations by teachers, parents, or\nresearchers, which may fail to capture comprehensive insights from free play\nand provide timely feedback to educators. This study proposes an innovative\napproach combining Large Language Models (LLMs) with learning analytics to\nanalyze children's self-narratives of their play experiences. The LLM\nidentifies developmental abilities, while performance scores across different\nplay settings are calculated using learning analytics techniques. We collected\n2,224 play narratives from 29 children in a kindergarten, covering four\ndistinct play areas over one semester. According to the evaluation results from\neight professionals, the LLM-based approach achieved high accuracy in\nidentifying cognitive, motor, and social abilities, with accuracy exceeding 90%\nin most domains. Moreover, significant differences in developmental outcomes\nwere observed across play settings, highlighting each area's unique\ncontributions to specific abilities. These findings confirm that the proposed\napproach is effective in identifying children's development across various free\nplay settings. This study demonstrates the potential of integrating LLMs and\nlearning analytics to provide child-centered insights into developmental\ntrajectories, offering educators valuable data to support personalized learning\nand enhance early childhood education practices.", "AI": {"tldr": "The study proposes using LLMs and learning analytics to assess children's development during free play by analyzing their self-narratives, achieving high accuracy and revealing play setting-specific developmental differences.", "motivation": "Traditional assessment methods for free play are limited in capturing comprehensive insights and providing timely feedback, prompting the need for an innovative approach.", "method": "Combines LLMs and learning analytics to analyze 2,224 play narratives from 29 children across four play areas, evaluating developmental abilities.", "result": "The LLM-based approach achieved over 90% accuracy in identifying cognitive, motor, and social abilities, with significant differences observed across play settings.", "conclusion": "The approach effectively identifies developmental outcomes in free play, showcasing the potential of LLMs and learning analytics to enhance early childhood education."}}
{"id": "2504.21188", "pdf": "https://arxiv.org/pdf/2504.21188", "abs": "https://arxiv.org/abs/2504.21188", "authors": ["Natnael Alemayehu"], "title": "Light Weight CNN for classification of Brain Tumors from MRI Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages", "summary": "This study presents a convolutional neural network (CNN)-based approach for\nthe multi-class classification of brain tumors using magnetic resonance imaging\n(MRI) scans. We utilize a publicly available dataset containing MRI images\ncategorized into four classes: glioma, meningioma, pituitary tumor, and no\ntumor. Our primary objective is to build a light weight deep learning model\nthat can automatically classify brain tumor types with high accuracy. To\nachieve this goal, we incorporate image preprocessing steps, including\nnormalization, data augmentation, and a cropping technique designed to reduce\nbackground noise and emphasize relevant regions. The CNN architecture is\noptimized through hyperparameter tuning using Keras Tuner, enabling systematic\nexploration of network parameters. To ensure reliable evaluation, we apply\n5-fold cross-validation, where each hyperparameter configuration is evaluated\nacross multiple data splits to mitigate overfitting. Experimental results\ndemonstrate that the proposed model achieves a classification accuracy of\n98.78%, indicating its potential as a diagnostic aid in clinical settings. The\nproposed method offers a low-complexity yet effective solution for assisting in\nearly brain tumor diagnosis.", "AI": {"tldr": "A lightweight CNN model for multi-class brain tumor classification from MRI scans achieves 98.78% accuracy using preprocessing, hyperparameter tuning, and cross-validation.", "motivation": "To develop an efficient deep learning model for accurate and automated classification of brain tumor types to aid clinical diagnosis.", "method": "Utilizes a CNN with preprocessing (normalization, augmentation, cropping), hyperparameter tuning via Keras Tuner, and 5-fold cross-validation.", "result": "Achieves 98.78% classification accuracy on a dataset with four tumor classes.", "conclusion": "The model is a low-complexity, effective solution for early brain tumor diagnosis in clinical settings."}}
{"id": "2505.03721", "pdf": "https://arxiv.org/pdf/2505.03721", "abs": "https://arxiv.org/abs/2505.03721", "authors": ["Dian Chen", "Zelin Wan", "Dong Sam Ha", "Jin-Hee Cho"], "title": "Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Solar sensor-based monitoring systems have become a crucial agricultural\ninnovation, advancing farm management and animal welfare through integrating\nsensor technology, Internet-of-Things, and edge and cloud computing. However,\nthe resilience of these systems to cyber-attacks and their adaptability to\ndynamic and constrained energy supplies remain largely unexplored. To address\nthese challenges, we propose a sustainable smart farm network designed to\nmaintain high-quality animal monitoring under various cyber and adversarial\nthreats, as well as fluctuating energy conditions. Our approach utilizes deep\nreinforcement learning (DRL) to devise optimal policies that maximize both\nmonitoring effectiveness and energy efficiency. To overcome DRL's inherent\nchallenge of slow convergence, we integrate transfer learning (TL) and decision\ntheory (DT) to accelerate the learning process. By incorporating DT-guided\nstrategies, we optimize monitoring quality and energy sustainability,\nsignificantly reducing training time while achieving comparable performance\nrewards. Our experimental results prove that DT-guided DRL outperforms\nTL-enhanced DRL models, improving system performance and reducing training\nruntime by 47.5%.", "AI": {"tldr": "A sustainable smart farm network using DRL, TL, and DT to enhance monitoring under cyber threats and energy constraints, achieving faster convergence and better performance.", "motivation": "Addressing unexplored resilience to cyber-attacks and adaptability to dynamic energy supplies in solar sensor-based farm monitoring systems.", "method": "Proposes DRL for optimal policies, enhanced by TL and DT to accelerate learning and improve efficiency.", "result": "DT-guided DRL outperforms TL-enhanced DRL, improving performance and reducing training runtime by 47.5%.", "conclusion": "The approach effectively balances monitoring quality and energy sustainability, proving resilient and efficient."}}
{"id": "2505.03059", "pdf": "https://arxiv.org/pdf/2505.03059", "abs": "https://arxiv.org/abs/2505.03059", "authors": ["Junlin Wang", "Roy Xie", "Shang Zhu", "Jue Wang", "Ben Athiwaratkun", "Bhuwan Dhingra", "Shuaiwen Leon Song", "Ce Zhang", "James Zou"], "title": "Improving Model Alignment Through Collective Intelligence of Open-Source LLMS", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Building helpful and harmless large language models (LLMs) requires effective\nmodel alignment approach based on human instructions and feedback, which\nnecessitates high-quality human-labeled data. Constructing such datasets is\noften expensive and hard to scale, and may face potential limitations on\ndiversity and generalization. To address these challenges, we introduce Mixture\nof Agents Alignment (MoAA), that leverages the collective strengths of various\nlanguage models to provide high-quality data for model alignment. By employing\nMoAA, we enhance both supervised fine-tuning and preference optimization,\nleading to improved performance compared to using a single model alone to\ngenerate alignment data (e.g. using GPT-4o alone). Evaluation results show that\nour approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on\nArena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising\ndirection for model alignment through this new scalable and diverse synthetic\ndata recipe. Furthermore, we demonstrate that MoAA enables a self-improvement\npipeline, where models finetuned on MoA-generated data surpass their own\ninitial capabilities, providing evidence that our approach can push the\nfrontier of open-source LLMs without reliance on stronger external supervision.\nData and code will be released.", "AI": {"tldr": "MoAA leverages multiple language models to generate high-quality alignment data, improving LLM performance and enabling self-improvement.", "motivation": "High-quality human-labeled data for LLM alignment is expensive and lacks diversity, prompting the need for scalable synthetic solutions.", "method": "Mixture of Agents Alignment (MoAA) combines various language models to produce diverse and high-quality alignment data for supervised fine-tuning and preference optimization.", "result": "MoAA boosts LLaMA-3.1-8B-Instruct's win rate significantly on Arena-Hard and AlpacaEval2, and enables self-improvement without external supervision.", "conclusion": "MoAA offers a scalable, diverse, and effective approach for LLM alignment, advancing open-source models without relying on costly human data."}}
{"id": "2505.03116", "pdf": "https://arxiv.org/pdf/2505.03116", "abs": "https://arxiv.org/abs/2505.03116", "authors": ["Haoyue Liu", "Jinghan Xu", "Yi Chang", "Hanyu Zhou", "Haozhi Zhao", "Lin Wang", "Luxin Yan"], "title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras\nas guidance has recently shown better performance and memory efficiency than\nthe frame-based methods, thanks to the event cameras' advantages, such as high\ntemporal resolution. A hurdle for event-based VFI is how to effectively deal\nwith non-linear motion, caused by the dynamic changes in motion direction and\nspeed within the scene. Existing methods either use events to estimate sparse\noptical flow or fuse events with image features to estimate dense optical flow.\nUnfortunately, motion errors often degrade the VFI quality as the continuous\nmotion cues from events do not align with the dense spatial information of\nimages in the temporal dimension. In this paper, we find that object motion is\ncontinuous in space, tracking local regions over continuous time enables more\naccurate identification of spatiotemporal feature correlations. In light of\nthis, we propose a novel continuous point tracking-based VFI framework, named\nTimeTracker. Specifically, we first design a Scene-Aware Region Segmentation\n(SARS) module to divide the scene into similar patches. Then, a Continuous\nTrajectory guided Motion Estimation (CTME) module is proposed to track the\ncontinuous motion trajectory of each patch through events. Finally,\nintermediate frames at any given time are generated through global motion\noptimization and frame refinement. Moreover, we collect a real-world dataset\nthat features fast non-linear motion. Extensive experiments show that our\nmethod outperforms prior arts in both motion estimation and frame interpolation\nquality.", "AI": {"tldr": "The paper proposes TimeTracker, a novel event-based video frame interpolation (VFI) framework that improves motion estimation and interpolation quality by tracking continuous object motion trajectories.", "motivation": "Existing event-based VFI methods struggle with non-linear motion due to misalignment between event cues and image features, degrading interpolation quality.", "method": "TimeTracker uses Scene-Aware Region Segmentation (SARS) to divide scenes into patches and Continuous Trajectory guided Motion Estimation (CTME) to track patch motion via events, followed by global optimization and refinement.", "result": "The method outperforms prior works in motion estimation and interpolation quality, validated on a real-world dataset with fast non-linear motion.", "conclusion": "TimeTracker effectively addresses non-linear motion challenges in event-based VFI, offering superior performance and accuracy."}}
{"id": "2505.03049", "pdf": "https://arxiv.org/pdf/2505.03049", "abs": "https://arxiv.org/abs/2505.03049", "authors": ["Yoel Zimmermann", "Adib Bazgir", "Alexander Al-Feghali", "Mehrad Ansari", "L. Catherine Brinson", "Yuan Chiang", "Defne Circi", "Min-Hsueh Chiu", "Nathan Daelman", "Matthew L. Evans", "Abhijeet S. Gangan", "Janine George", "Hassan Harb", "Ghazal Khalighinejad", "Sartaaj Takrim Khan", "Sascha Klawohn", "Magdalena Lederbauer", "Soroush Mahjoubi", "Bernadette Mohr", "Seyed Mohamad Moosavi", "Aakash Naik", "Aleyna Beste Ozhan", "Dieter Plessers", "Aritra Roy", "Fabian Sch\u00f6ppach", "Philippe Schwaller", "Carla Terboven", "Katharina Ueltzen", "Shang Zhu", "Jan Janssen", "Calvin Li", "Ian Foster", "Ben Blaiszik"], "title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": "arXiv admin note: substantial text overlap with arXiv:2411.15221", "summary": "Large Language Models (LLMs) are reshaping many aspects of materials science\nand chemistry research, enabling advances in molecular property prediction,\nmaterials design, scientific automation, knowledge extraction, and more. Recent\ndevelopments demonstrate that the latest class of models are able to integrate\nstructured and unstructured data, assist in hypothesis generation, and\nstreamline research workflows. To explore the frontier of LLM capabilities\nacross the research lifecycle, we review applications of LLMs through 34 total\nprojects developed during the second annual Large Language Model Hackathon for\nApplications in Materials Science and Chemistry, a global hybrid event. These\nprojects spanned seven key research areas: (1) molecular and material property\nprediction, (2) molecular and material design, (3) automation and novel\ninterfaces, (4) scientific communication and education, (5) research data\nmanagement and automation, (6) hypothesis generation and evaluation, and (7)\nknowledge extraction and reasoning from the scientific literature.\nCollectively, these applications illustrate how LLMs serve as versatile\npredictive models, platforms for rapid prototyping of domain-specific tools,\nand much more. In particular, improvements in both open source and proprietary\nLLM performance through the addition of reasoning, additional training data,\nand new techniques have expanded effectiveness, particularly in low-data\nenvironments and interdisciplinary research. As LLMs continue to improve, their\nintegration into scientific workflows presents both new opportunities and new\nchallenges, requiring ongoing exploration, continued refinement, and further\nresearch to address reliability, interpretability, and reproducibility.", "AI": {"tldr": "LLMs are transforming materials science and chemistry by enhancing property prediction, design, automation, and knowledge extraction, as demonstrated by 34 projects in a hackathon.", "motivation": "To explore and showcase the expanding capabilities of LLMs in materials science and chemistry research.", "method": "Review of 34 projects from a global hackathon, covering seven key research areas.", "result": "LLMs are versatile tools for prediction, prototyping, and interdisciplinary research, with improved performance in low-data settings.", "conclusion": "LLMs offer significant opportunities but require further research to address reliability, interpretability, and reproducibility."}}
{"id": "2505.03434", "pdf": "https://arxiv.org/pdf/2505.03434", "abs": "https://arxiv.org/abs/2505.03434", "authors": ["Schaun Wheeler", "Olivier Jeunen"], "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the workshop on Hybrid AI for Human-Centric\n  Personalization (HyPer), co-located with ACM UMAP '25", "summary": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.", "AI": {"tldr": "LLMs excel in procedural tasks but struggle in unpredictable environments. Augmenting them with semantic memory and associative learning can enhance adaptability.", "motivation": "LLMs rely on procedural memory, limiting their effectiveness in complex, shifting environments.", "method": "Propose a modular architecture combining procedural, semantic memory, and associative learning.", "result": "Potential to bridge the gap between narrow expertise and adaptive intelligence.", "conclusion": "Augmenting LLMs with additional cognitive functions can improve real-world problem-solving."}}
{"id": "2505.01884", "pdf": "https://arxiv.org/pdf/2505.01884", "abs": "https://arxiv.org/abs/2505.01884", "authors": ["Siddharth Kothari", "Srinivasan Murali", "Sankalp Kothari", "Ujjwal Verma", "Jaya Sreevalsan-Nair"], "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "21 pages, 15 figures, 2 tables", "summary": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (GitHub link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)", "AI": {"tldr": "The paper investigates the robustness of U-Net for inland water body segmentation in SAR images against manual annotation errors, simulated as adversarial attacks. It finds U-Net tolerates some corruption before performance drops, emphasizing annotation quality's importance.", "motivation": "Manual annotation of SAR images for water segmentation is error-prone due to complex geometry and adversarial attacks, impacting model performance.", "method": "Simulates manual annotation errors as adversarial attacks on U-Net and evaluates its robustness.", "result": "U-Net can withstand a certain level of annotation corruption before significant performance decline.", "conclusion": "Annotation quality is critical for segmentation model effectiveness; the study provides a dataset and adversarial examples for robust training."}}
{"id": "2502.18438", "pdf": "https://arxiv.org/pdf/2502.18438", "abs": "https://arxiv.org/abs/2502.18438", "authors": ["Pedro Sequeira", "Vidyasagar Sadhu", "Melinda Gervasio"], "title": "ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": "Appears in Proc. of the Adaptive and Learning Agents Workshop (ALA\n  2025), ala-workshop.github.io", "summary": "In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents in\nTeams), a new framework for generating ToM-conditioned trajectories. It\ncombines a meta-learning mechanism, that performs ToM reasoning over teammates'\nunderlying goals and future behavior, with a multiagent denoising-diffusion\nmodel, that generates plans for an agent and its teammates conditioned on both\nthe agent's goals and its teammates' characteristics, as computed via ToM. We\nimplemented an online planning system that dynamically samples new trajectories\n(replans) from the diffusion model whenever it detects a divergence between a\npreviously generated plan and the current state of the world. We conducted\nseveral experiments using ToMCAT in a simulated cooking domain. Our results\nhighlight the importance of the dynamic replanning mechanism in reducing the\nusage of resources without sacrificing team performance. We also show that\nrecent observations about the world and teammates' behavior collected by an\nagent over the course of an episode combined with ToM inferences are crucial to\ngenerate team-aware plans for dynamic adaptation to teammates, especially when\nno prior information is provided about them.", "AI": {"tldr": "ToMCAT is a framework for generating ToM-conditioned trajectories using meta-learning and multiagent diffusion models, with dynamic replanning for adaptive teamwork.", "motivation": "To improve cooperative agent teamwork by integrating Theory of Mind (ToM) reasoning and dynamic replanning for better resource usage and performance.", "method": "Combines meta-learning for ToM reasoning with a multiagent denoising-diffusion model for plan generation, plus dynamic replanning based on real-time observations.", "result": "Dynamic replanning reduces resource usage without compromising team performance; ToM inferences and recent observations are key for adaptive planning.", "conclusion": "ToMCAT effectively enhances teamwork by leveraging ToM and dynamic replanning, especially in scenarios with unknown teammate characteristics."}}
{"id": "2505.03229", "pdf": "https://arxiv.org/pdf/2505.03229", "abs": "https://arxiv.org/abs/2505.03229", "authors": ["Behrooz Mansouri"], "title": "Survey of Abstract Meaning Representation: Then, Now, Future", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a survey of Abstract Meaning Representation (AMR), a\nsemantic representation framework that captures the meaning of sentences\nthrough a graph-based structure. AMR represents sentences as rooted, directed\nacyclic graphs, where nodes correspond to concepts and edges denote\nrelationships, effectively encoding the meaning of complex sentences. This\nsurvey investigates AMR and its extensions, focusing on AMR capabilities. It\nthen explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by\nshowing traditional, current, and possible futures approaches. It also reviews\nvarious applications of AMR including text generation, text classification, and\ninformation extraction and information seeking. By analyzing recent\ndevelopments and challenges in the field, this survey provides insights into\nfuture directions for research and the potential impact of AMR on enhancing\nmachine understanding of human language.", "AI": {"tldr": "A survey of Abstract Meaning Representation (AMR), its capabilities, parsing/generation tasks, applications, and future research directions.", "motivation": "To explore AMR's potential in enhancing machine understanding of human language through its graph-based semantic representation.", "method": "Investigates AMR and its extensions, reviews parsing/generation tasks, and examines applications like text generation and classification.", "result": "Highlights recent developments, challenges, and the impact of AMR on NLP tasks.", "conclusion": "AMR shows promise for advancing machine language understanding, with future research needed to address current limitations."}}
{"id": "2505.03132", "pdf": "https://arxiv.org/pdf/2505.03132", "abs": "https://arxiv.org/abs/2505.03132", "authors": ["Xinyuan Yan", "Xiwei Xuan", "Jorge Piazentin Ono", "Jiajing Guo", "Vikram Mohanty", "Shekar Arvind Kumar", "Liang Gou", "Bei Wang", "Liu Ren"], "title": "VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Real-world machine learning models require rigorous evaluation before\ndeployment, especially in safety-critical domains like autonomous driving and\nsurveillance. The evaluation of machine learning models often focuses on data\nslices, which are subsets of the data that share a set of characteristics. Data\nslice finding automatically identifies conditions or data subgroups where\nmodels underperform, aiding developers in mitigating performance issues.\nDespite its popularity and effectiveness, data slicing for vision model\nvalidation faces several challenges. First, data slicing often needs additional\nimage metadata or visual concepts, and falls short in certain computer vision\ntasks, such as object detection. Second, understanding data slices is a\nlabor-intensive and mentally demanding process that heavily relies on the\nexpert's domain knowledge. Third, data slicing lacks a human-in-the-loop\nsolution that allows experts to form hypothesis and test them interactively. To\novercome these limitations and better support the machine learning operations\nlifecycle, we introduce VISLIX, a novel visual analytics framework that employs\nstate-of-the-art foundation models to help domain experts analyze slices in\ncomputer vision models. Our approach does not require image metadata or visual\nconcepts, automatically generates natural language insights, and allows users\nto test data slice hypothesis interactively. We evaluate VISLIX with an expert\nstudy and three use cases, that demonstrate the effectiveness of our tool in\nproviding comprehensive insights for validating object detection models.", "AI": {"tldr": "VISLIX is a visual analytics framework for analyzing data slices in computer vision models without metadata, offering automated insights and interactive hypothesis testing.", "motivation": "Challenges in data slicing for vision models include reliance on metadata, labor-intensive analysis, and lack of interactive solutions.", "method": "VISLIX uses foundation models to analyze slices, generates natural language insights, and enables interactive hypothesis testing.", "result": "Evaluated with expert studies and use cases, VISLIX effectively validates object detection models.", "conclusion": "VISLIX addresses limitations in data slicing, enhancing model validation in computer vision."}}
{"id": "2505.03084", "pdf": "https://arxiv.org/pdf/2505.03084", "abs": "https://arxiv.org/abs/2505.03084", "authors": ["Shashank Kapoor", "Sanjay Surendranath Girija", "Lakshit Arora", "Dipen Pradhan", "Ankit Shetgaonkar", "Aman Raj"], "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey", "categories": ["cs.LG"], "comment": "Accepted in IEEE COMPSAC 2025", "summary": "The introduction of multimodal models is a huge step forward in Artificial\nIntelligence. A single model is trained to understand multiple modalities:\ntext, image, video, and audio. Open-source multimodal models have made these\nbreakthroughs more accessible. However, considering the vast landscape of\nadversarial attacks across these modalities, these models also inherit\nvulnerabilities of all the modalities, and ultimately, the adversarial threat\namplifies. While broad research is available on possible attacks within or\nacross these modalities, a practitioner-focused view that outlines attack types\nremains absent in the multimodal world. As more Machine Learning Practitioners\nadopt, fine-tune, and deploy open-source models in real-world applications,\nit's crucial that they can view the threat landscape and take the preventive\nactions necessary. This paper addresses the gap by surveying adversarial\nattacks targeting all four modalities: text, image, video, and audio. This\nsurvey provides a view of the adversarial attack landscape and presents how\nmultimodal adversarial threats have evolved. To the best of our knowledge, this\nsurvey is the first comprehensive summarization of the threat landscape in the\nmultimodal world.", "AI": {"tldr": "The paper surveys adversarial attacks in multimodal models (text, image, video, audio) to fill the gap in practitioner-focused threat analysis.", "motivation": "Multimodal models inherit vulnerabilities from all modalities, amplifying adversarial threats, yet a practitioner-focused view of attack types is missing.", "method": "The paper surveys adversarial attacks across text, image, video, and audio modalities.", "result": "It provides a comprehensive view of the adversarial threat landscape in multimodal models.", "conclusion": "This is the first survey summarizing multimodal adversarial threats, aiding practitioners in understanding and mitigating risks."}}
{"id": "2505.03439", "pdf": "https://arxiv.org/pdf/2505.03439", "abs": "https://arxiv.org/abs/2505.03439", "authors": ["Artem Karpov", "Tinuade Adeleke", "Seong Hah Cho", "Natalia Perez-Campanero"], "title": "The Steganographic Potentials of Language Models", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "Published at Building Trust Workshop at ICLR 2025", "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.", "AI": {"tldr": "LLMs can hide messages in plain text (steganography), posing detection challenges. This paper explores their steganographic abilities via RL fine-tuning, revealing enhanced concealment with algorithmic guidance.", "motivation": "To understand and address the challenge of LLMs hiding messages, which undermines their reasoning faithfulness and poses risks from unaligned AI agents.", "method": "Fine-tuning LLMs via reinforcement learning (RL) to develop covert encoding schemes, engage in steganography when prompted, and use it in realistic scenarios. Detection of hidden reasoning and performance evaluation are also conducted.", "result": "Current models show basic steganographic abilities in security and capacity, but explicit algorithmic guidance significantly improves their information concealment.", "conclusion": "LLMs can be fine-tuned for steganography, with algorithmic guidance enhancing their ability to hide information, highlighting both risks and detection challenges."}}
{"id": "2505.02048", "pdf": "https://arxiv.org/pdf/2505.02048", "abs": "https://arxiv.org/abs/2505.02048", "authors": ["Sebastian Rassmann", "David K\u00fcgler", "Christian Ewert", "Martin Reuter"], "title": "Regression is all you need for medical image translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging.", "AI": {"tldr": "YODA, a 2.5D diffusion-based framework, combines diffusion and regression for medical image translation, outperforming GANs and DMs while challenging their presumed advantages.", "motivation": "Enhancing medical datasets with synthetic images requires accurate anatomical information, which GANs and DMs often fail to provide due to noise or hallucination.", "method": "YODA integrates diffusion and regression paradigms, using ExpA-sampling to suppress noise and improve image quality.", "result": "YODA outperforms GANs and DMs, generating images interchangeable or superior to physical acquisitions for downstream tasks.", "conclusion": "YODA challenges DM advantages in MIT, enabling practical medical imaging applications."}}
{"id": "2502.10148", "pdf": "https://arxiv.org/pdf/2502.10148", "abs": "https://arxiv.org/abs/2502.10148", "authors": ["Zhiyuan Li", "Wenshuai Zhao", "Joni Pajarinen"], "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS's strong performance against\nstate-of-the-art MARL baselines across both symmetric and asymmetric scenarios.\nNotably, in the symmetric Protoss 5v5 task, COMPASS achieved a 57\\% win rate,\nrepresenting a 30 percentage point advantage over QMIX (27\\%). Project page can\nbe found at https://stellar-entremet-1720bb.netlify.app/.", "AI": {"tldr": "COMPASS integrates vision-language models with a dynamic skill library and structured communication for decentralized multi-agent decision-making, outperforming MARL baselines in StarCraft scenarios.", "motivation": "Challenges in MARL include sample inefficiency, interpretability, and transferability. LLMs show promise but struggle with non-Markovian multi-agent interactions.", "method": "COMPASS combines VLMs, a dynamic skill library, and multi-hop communication for decentralized decision-making under partial observability.", "result": "COMPASS achieves a 57% win rate in Protoss 5v5, outperforming QMIX by 30 percentage points.", "conclusion": "COMPASS demonstrates strong performance in MARL tasks, leveraging VLMs and adaptive strategies for improved cooperation."}}
{"id": "2505.03293", "pdf": "https://arxiv.org/pdf/2505.03293", "abs": "https://arxiv.org/abs/2505.03293", "authors": ["Shijing Zhu", "Zhuang Chen", "Guanqun Bi", "Binghang Li", "Yaxi Deng", "Dazhen Wan", "Libiao Peng", "Xiyao Xiao", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "FangFang Li", "Minlie Huang"], "title": "\u03a8-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback", "categories": ["cs.CL"], "comment": "in progress", "summary": "Large language models (LLMs) have shown promise in providing scalable mental\nhealth support, while evaluating their counseling capability remains crucial to\nensure both efficacy and safety. Existing evaluations are limited by the static\nassessment that focuses on knowledge tests, the single perspective that centers\non user experience, and the open-loop framework that lacks actionable feedback.\nTo address these issues, we propose {\\Psi}-Arena, an interactive framework for\ncomprehensive assessment and optimization of LLM-based counselors, featuring\nthree key characteristics: (1) Realistic arena interactions that simulate\nreal-world counseling through multi-stage dialogues with psychologically\nprofiled NPC clients, (2) Tripartite evaluation that integrates assessments\nfrom the client, counselor, and supervisor perspectives, and (3) Closed-loop\noptimization that iteratively improves LLM counselors using diagnostic\nfeedback. Experiments across eight state-of-the-art LLMs show significant\nperformance variations in different real-world scenarios and evaluation\nperspectives. Moreover, reflection-based optimization results in up to a 141%\nimprovement in counseling performance. We hope PsychoArena provides a\nfoundational resource for advancing reliable and human-aligned LLM applications\nin mental healthcare.", "AI": {"tldr": "Psi-Arena is an interactive framework for evaluating and improving LLM-based mental health counselors through multi-stage dialogues, tripartite evaluations, and closed-loop optimization, showing significant performance improvements.", "motivation": "Existing evaluations of LLM counselors are limited by static, single-perspective, and open-loop methods, necessitating a more comprehensive approach.", "method": "Psi-Arena uses realistic multi-stage dialogues with NPC clients, tripartite evaluations (client, counselor, supervisor), and closed-loop optimization with feedback.", "result": "Experiments show performance variations among LLMs and up to 141% improvement in counseling performance after optimization.", "conclusion": "Psi-Arena offers a foundational framework for advancing reliable and human-aligned LLM applications in mental healthcare."}}
{"id": "2505.03134", "pdf": "https://arxiv.org/pdf/2505.03134", "abs": "https://arxiv.org/abs/2505.03134", "authors": ["Sajjad Rezvani Boroujeni", "Hossein Abedi", "Tom Bush"], "title": "Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control", "categories": ["cs.CV", "cs.LG"], "comment": "12 pages, 7 figures, submitted to Computer and Decision Making An\n  International Journal (COMDEM)", "summary": "Visual defect detection in industrial glass manufacturing remains a critical\nchallenge due to the low frequency of defective products, leading to imbalanced\ndatasets that limit the performance of deep learning models and computer vision\nsystems. This paper presents a novel approach using Denoising Diffusion\nProbabilistic Models (DDPMs) to generate synthetic defective glass product\nimages for data augmentation, effectively addressing class imbalance issues in\nmanufacturing quality control and automated visual inspection. The methodology\nsignificantly enhances image classification performance of standard CNN\narchitectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting\nanomalies by increasing the minority class representation. Experimental results\ndemonstrate substantial improvements in key machine learning metrics,\nparticularly in recall for defective samples across all tested deep neural\nnetwork architectures while maintaining perfect precision. The most dramatic\nimprovement was observed in ResNet50V2's overall classification accuracy, which\nincreased from 78 percent to 93 percent when trained with the augmented data.\nThis work provides a scalable, cost-effective approach to enhancing automated\ndefect detection in glass manufacturing that can potentially be extended to\nother industrial quality assurance systems and industries with similar class\nimbalance challenges.", "AI": {"tldr": "The paper proposes using Denoising Diffusion Probabilistic Models (DDPMs) to generate synthetic defective glass images, improving deep learning model performance in imbalanced datasets for industrial defect detection.", "motivation": "Addressing class imbalance in industrial glass defect detection due to low defective product frequency, which limits deep learning model performance.", "method": "Uses DDPMs for synthetic image generation to augment minority class data, tested with CNN architectures (ResNet50V2, EfficientNetB0, MobileNetV2).", "result": "Significant improvements in recall and accuracy, e.g., ResNet50V2's accuracy rose from 78% to 93%, while maintaining perfect precision.", "conclusion": "The approach is scalable and cost-effective for defect detection, with potential applications in other industries facing class imbalance."}}
{"id": "2505.03109", "pdf": "https://arxiv.org/pdf/2505.03109", "abs": "https://arxiv.org/abs/2505.03109", "authors": ["Lutfu Sua", "Haibo Wang", "Jun Huang"], "title": "Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models", "categories": ["cs.LG", "econ.GN", "q-fin.EC"], "comment": "34 pages, 16 figures", "summary": "Unpredictability of renewable energy sources coupled with the complexity of\nthose methods used for various purposes in this area calls for the development\nof robust methods such as DL models within the renewable energy domain. Given\nthe nonlinear relationships among variables in renewable energy datasets, DL\nmodels are preferred over traditional machine learning (ML) models because they\ncan effectively capture and model complex interactions between variables. This\nresearch aims to identify the factors responsible for the accuracy of DL\ntechniques, such as sampling, stationarity, linearity, and hyperparameter\noptimization for different algorithms. The proposed DL framework compares\nvarious methods and alternative training/test ratios. Seven ML methods, such as\nLong-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network\n(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and\nEncoder-Decoder (ED), were evaluated on two different datasets. The first\ndataset contains the weather and power generation data. It encompasses two\ndistinct datasets, hourly energy demand data and hourly weather data in Spain,\nwhile the second dataset includes power output generated by the photovoltaic\npanels at 12 locations. This study deploys regularization approaches, including\nearly stopping, neuron dropping, and L2 regularization, to reduce the\noverfitting problem associated with DL models. The LSTM and MLP models show\nsuperior performance. Their validation data exhibit exceptionally low root mean\nsquare error values.", "AI": {"tldr": "The paper explores the use of deep learning (DL) models in renewable energy to address unpredictability and complexity, comparing seven DL methods and evaluating their accuracy factors like sampling and hyperparameter optimization. LSTM and MLP models performed best.", "motivation": "Renewable energy's unpredictability and complex variable relationships necessitate robust DL models over traditional ML, as DL better captures nonlinear interactions.", "method": "The study evaluates seven DL methods (LSTM, Stacked LSTM, CNN, CNN-LSTM, DNN, MLP, ED) on two datasets (weather/power generation and photovoltaic output) using regularization techniques to prevent overfitting.", "result": "LSTM and MLP models outperformed others, showing low root mean square error values in validation.", "conclusion": "DL models, particularly LSTM and MLP, are effective for renewable energy applications due to their ability to handle complex data relationships and reduce overfitting."}}
{"id": "2505.03475", "pdf": "https://arxiv.org/pdf/2505.03475", "abs": "https://arxiv.org/abs/2505.03475", "authors": ["Zirui Liu", "Jiatong Li", "Yan Zhuang", "Qi Liu", "Shuanghong Shen", "Jie Ouyang", "Mingyue Cheng", "Shijin Wang"], "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation", "categories": ["cs.AI", "cs.LG"], "comment": "ICML2025 Accepted", "summary": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.", "AI": {"tldr": "A novel stable arena framework (m-ELO and am-ELO) improves the ELO rating system for evaluating LLMs by addressing instability and annotator reliability.", "motivation": "Existing ELO-based frameworks for AI model evaluation suffer from instability and ignore annotator abilities.", "method": "Introduces m-ELO (MLE-based) for stable ranking and am-ELO to incorporate annotator reliability.", "result": "The framework provides more robust, accurate, and stable evaluations for LLMs.", "conclusion": "The proposed method enhances the stability and reliability of arena-based evaluations for modern AI models."}}
{"id": "2305.18708", "pdf": "https://arxiv.org/pdf/2305.18708", "abs": "https://arxiv.org/abs/2305.18708", "authors": ["Yi Lu", "Yadong Wang", "Xingbo Jiang", "Xiangzhi Bai"], "title": "Infrared Image Deturbulence Restoration Using Degradation Parameter-Assisted Wide & Deep Learning", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Infrared images captured under turbulent conditions are degraded by complex\ngeometric distortions and blur. We address infrared deturbulence as an image\nrestoration task, proposing DparNet, a parameter-assisted multi-frame network\nwith a wide & deep architecture. DparNet learns a degradation prior (key\nparameter matrix) directly from degraded images without external knowledge. Its\nwide & deep architecture uses these learned parameters to directly modulate\nrestoration, achieving spatially and intensity adaptive results. Evaluated on\ndedicated infrared deturbulence (49,744 images) and visible image denoising\n(109,536 images) datasets, DparNet significantly outperforms State-of-the-Art\n(SOTA) methods in restoration performance and efficiency. Notably, leveraging\nthese parameters improves PSNR by 0.6-1.1 dB with less than 2% increase in\nmodel parameters and computational complexity. Our work demonstrates that\ndegraded images hide key degradation information that can be learned and\nutilized to boost adaptive image restoration.", "AI": {"tldr": "DparNet, a parameter-assisted multi-frame network, learns degradation priors from degraded infrared images to enhance restoration, outperforming SOTA methods with improved PSNR and efficiency.", "motivation": "Infrared images under turbulence suffer from distortions and blur, requiring adaptive restoration methods.", "method": "DparNet uses a wide & deep architecture to learn degradation priors (key parameters) from degraded images, modulating restoration adaptively.", "result": "Outperforms SOTA in restoration (PSNR improved by 0.6-1.1 dB) with minimal increase in model complexity (<2%).", "conclusion": "Degraded images contain learnable degradation information, enabling adaptive and efficient restoration."}}
{"id": "2503.01861", "pdf": "https://arxiv.org/pdf/2503.01861", "abs": "https://arxiv.org/abs/2503.01861", "authors": ["Sami Marreed", "Alon Oved", "Avi Yaeli", "Segev Shlomov", "Ido Levy", "Aviad Sela", "Asaf Adi", "Nir Mashkif"], "title": "Towards Enterprise-Ready Computer Using Generalist Agent", "categories": ["cs.DC", "cs.AI", "cs.MA"], "comment": null, "summary": "This paper presents our ongoing work toward developing an enterprise-ready\nComputer Using Generalist Agent (CUGA) system. Our research highlights the\nevolutionary nature of building agentic systems suitable for enterprise\nenvironments. By integrating state-of-the-art agentic AI techniques with a\nsystematic approach to iterative evaluation, analysis, and refinement, we have\nachieved rapid and cost-effective performance gains, notably reaching a new\nstate-of-the-art performance on the WebArena benchmark. We detail our\ndevelopment roadmap, the methodology and tools that facilitated rapid learning\nfrom failures and continuous system refinement, and discuss key lessons learned\nand future challenges for enterprise adoption.", "AI": {"tldr": "The paper discusses the development of an enterprise-ready Computer Using Generalist Agent (CUGA) system, achieving state-of-the-art performance on WebArena through iterative evaluation and refinement.", "motivation": "To build agentic systems suitable for enterprise environments by integrating advanced AI techniques and systematic refinement.", "method": "Integration of state-of-the-art agentic AI techniques with iterative evaluation, analysis, and refinement.", "result": "Achieved rapid performance gains and state-of-the-art results on the WebArena benchmark.", "conclusion": "Key lessons and future challenges for enterprise adoption are discussed, emphasizing continuous refinement."}}
{"id": "2505.03320", "pdf": "https://arxiv.org/pdf/2505.03320", "abs": "https://arxiv.org/abs/2505.03320", "authors": ["Junyu Ma", "Tianqing Fang", "Zhisong Zhang", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation", "categories": ["cs.CL"], "comment": null, "summary": "Mamba's theoretical infinite-context potential is limited in practice when\nsequences far exceed training lengths. This work explores unlocking Mamba's\nlong-context memory ability by a simple-yet-effective method, Recall with\nReasoning (RwR), by distilling chain-of-thought (CoT) summarization from a\nteacher model. Specifically, RwR prepends these summarization as CoT prompts\nduring fine-tuning, teaching Mamba to actively recall and reason over long\ncontexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's\nlong-context performance against comparable Transformer/hybrid baselines under\nsimilar pretraining conditions, while preserving short-context capabilities,\nall without architectural changes.", "AI": {"tldr": "RwR enhances Mamba's long-context memory by using CoT summarization from a teacher model, improving performance without architectural changes.", "motivation": "Mamba's long-context potential is limited in practice; this work aims to unlock it.", "method": "Recall with Reasoning (RwR) distills CoT summarization from a teacher model and uses it as prompts during fine-tuning.", "result": "RwR boosts Mamba's long-context performance on benchmarks like LONGMEMEVAL and HELMET, while maintaining short-context capabilities.", "conclusion": "RwR is a simple-yet-effective method to enhance Mamba's long-context abilities without altering its architecture."}}
{"id": "2505.03149", "pdf": "https://arxiv.org/pdf/2505.03149", "abs": "https://arxiv.org/abs/2505.03149", "authors": ["Joseph William Kettelkamp", "Ludovica Romanin", "Sarv Priya", "Mathews Jacob"], "title": "Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an unsupervised motion-compensated image reconstruction\nalgorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging\n(MRI). We express the image volume corresponding to each specific motion phase\nas the deformation of a single static image template. The main contribution of\nthe work is the low-rank model for the compact joint representation of the\nfamily of diffeomorphisms, parameterized by the motion phases. The\ndiffeomorphism at a specific motion phase is obtained by integrating a\nparametric velocity field along a path connecting the reference template phase\nto the motion phase. The velocity field at different phases is represented\nusing a low-rank model. The static template and the low-rank motion model\nparameters are learned directly from the k-space data in an unsupervised\nfashion. The more constrained motion model is observed to offer improved\nrecovery compared to current motion-resolved and motion-compensated algorithms\nfor free-breathing 3D cine MRI.", "AI": {"tldr": "An unsupervised algorithm for motion-compensated 3D cardiac MRI reconstruction using a low-rank model for diffeomorphisms, improving recovery over existing methods.", "motivation": "To address the challenge of reconstructing high-quality images from free-breathing and ungated 3D cardiac MRI data without supervision.", "method": "Represents each motion phase as a deformation of a static template, using a low-rank model for diffeomorphisms derived from parametric velocity fields. Parameters are learned directly from k-space data.", "result": "The constrained motion model improves image recovery compared to current motion-resolved and motion-compensated algorithms.", "conclusion": "The proposed unsupervised approach offers a promising solution for free-breathing 3D cine MRI reconstruction."}}
{"id": "2505.03112", "pdf": "https://arxiv.org/pdf/2505.03112", "abs": "https://arxiv.org/abs/2505.03112", "authors": ["Mohammad Rostami", "Atik Faysal", "Reihaneh Gh. Roshan", "Huaxia Wang", "Nikhil Muralidhar", "Yu-Dong Yao"], "title": "Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs", "categories": ["cs.LG"], "comment": null, "summary": "Automatic Modulation Classification (AMC) is critical for efficient spectrum\nmanagement and robust wireless communications. However, AMC remains challenging\ndue to the complex interplay of signal interference and noise. In this work, we\npropose an innovative framework that integrates traditional signal processing\ntechniques with Large-Language Models (LLMs) to address AMC. Our approach\nleverages higher-order statistics and cumulant estimation to convert\nquantitative signal features into structured natural language prompts. By\nincorporating exemplar contexts into these prompts, our method exploits the\nLLM's inherent familiarity with classical signal processing, enabling effective\none-shot classification without additional training or preprocessing (e.g.,\ndenoising). Experimental evaluations on synthetically generated datasets,\nspanning both noiseless and noisy conditions, demonstrate that our framework\nachieves competitive performance across diverse modulation schemes and\nSignal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust\nfoundation models in wireless communications across varying channel conditions,\nsignificantly reducing the expense associated with developing channel-specific\nmodels. This work lays the foundation for scalable, interpretable, and\nversatile signal classification systems in next-generation wireless networks.\nThe source code is available at https://github.com/RU-SIT/context-is-king", "AI": {"tldr": "The paper proposes a novel AMC framework combining signal processing with LLMs, achieving competitive performance without additional training or preprocessing.", "motivation": "AMC is challenging due to signal interference and noise, requiring innovative solutions for efficient spectrum management.", "method": "Integrates higher-order statistics and cumulant estimation with LLMs, using natural language prompts for one-shot classification.", "result": "Demonstrates competitive performance across modulation schemes and SNRs, reducing the need for channel-specific models.", "conclusion": "The framework enables scalable, interpretable signal classification, advancing next-generation wireless networks."}}
{"id": "2505.03547", "pdf": "https://arxiv.org/pdf/2505.03547", "abs": "https://arxiv.org/abs/2505.03547", "authors": ["Eric Zhou", "Shreyas Basavatia", "Moontashir Siam", "Zexin Chen", "Mark O. Riedl"], "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game", "categories": ["cs.AI"], "comment": null, "summary": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.", "AI": {"tldr": "STORY2GAME uses LLMs to generate interactive fiction games by creating stories, populating worlds, and dynamically coding actions, ensuring open-ended play while tracking game state.", "motivation": "To overcome the constraints of hard-coded actions in story generation and enable more open-ended, interactive experiences.", "method": "Generates stories, actions, and game code using LLMs, with dynamic action generation for player creativity.", "result": "Evaluates success based on players completing the generated story interactively.", "conclusion": "STORY2GAME demonstrates effective dynamic action generation for interactive fiction, enhancing player engagement."}}
{"id": "2407.19894", "pdf": "https://arxiv.org/pdf/2407.19894", "abs": "https://arxiv.org/abs/2407.19894", "authors": ["Alexander Ponomarchuk", "Ivan Kruzhilov", "Galina Zubkova", "Artem Shadrin", "Ruslan Utegenov", "Ivan Bessonov", "Pavel Blinov"], "title": "CardioSyntax: end-to-end SYNTAX score prediction -- dataset, benchmark and method", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The SYNTAX score has become a widely used measure of coronary disease\nseverity, crucial in selecting the optimal mode of the revascularization\nprocedure. This paper introduces a new medical regression and classification\nproblem - automatically estimating SYNTAX score from coronary angiography. Our\nstudy presents a comprehensive CardioSYNTAX dataset of 3,018 patients for the\nSYNTAX score estimation and coronary dominance classification. The dataset\nfeatures a balanced distribution of individuals with zero and non-zero scores.\nThis dataset includes a first-of-its-kind, complete coronary angiography\nsamples captured through a multi-view X-ray video, allowing one to observe\ncoronary arteries from multiple perspectives. Furthermore, we present a novel,\nfully automatic end-to-end method for estimating the SYNTAX. For such a\ndifficult task, we have achieved a solid coefficient of determination R2 of\n0.51 in score value prediction and 77.3% accuracy for zero score\nclassification.", "AI": {"tldr": "The paper introduces a method to automatically estimate the SYNTAX score from coronary angiography using a new dataset (CardioSYNTAX) and achieves solid results.", "motivation": "The SYNTAX score is critical for revascularization decisions, but manual estimation is complex. Automating this process can improve efficiency and accuracy.", "method": "A novel end-to-end method is proposed, leveraging a comprehensive dataset (3,018 patients) with multi-view X-ray videos of coronary angiography.", "result": "Achieved an R2 of 0.51 for score prediction and 77.3% accuracy for zero-score classification.", "conclusion": "The method and dataset provide a robust foundation for automating SYNTAX score estimation, aiding clinical decision-making."}}
{"id": "2503.02445", "pdf": "https://arxiv.org/pdf/2503.02445", "abs": "https://arxiv.org/abs/2503.02445", "authors": ["Hao Li", "Yuhao Huang", "Chang Xu", "Viktor Schlegel", "Renhe Jiang", "Riza Batista-Navarro", "Goran Nenadic", "Jiang Bian"], "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": "ICML 2025", "summary": "Time-series Generation (TSG) is a prominent research area with broad\napplications in simulations, data augmentation, and counterfactual analysis.\nWhile existing methods have shown promise in unconditional single-domain TSG,\nreal-world applications demand for cross-domain approaches capable of\ncontrolled generation tailored to domain-specific constraints and\ninstance-level requirements. In this paper, we argue that text can provide\nsemantic insights, domain information and instance-specific temporal patterns,\nto guide and improve TSG. We introduce ``Text-Controlled TSG'', a task focused\non generating realistic time series by incorporating textual descriptions. To\naddress data scarcity in this setting, we propose a novel LLM-based Multi-Agent\nframework that synthesizes diverse, realistic text-to-TS datasets. Furthermore,\nwe introduce BRIDGE, a hybrid text-controlled TSG framework that integrates\nsemantic prototypes with text description for supporting domain-level guidance.\nThis approach achieves state-of-the-art generation fidelity on 11 of 12\ndatasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared\nto no text input generation, highlighting its potential for generating tailored\ntime-series data.", "AI": {"tldr": "The paper introduces Text-Controlled TSG, a method for generating realistic time-series data using textual descriptions, and proposes a novel LLM-based Multi-Agent framework to address data scarcity. The BRIDGE framework achieves state-of-the-art results in generation fidelity and controllability.", "motivation": "Real-world applications require cross-domain time-series generation (TSG) with controlled generation tailored to domain-specific constraints, which existing methods lack. Text can provide semantic insights and domain-specific guidance for TSG.", "method": "The paper introduces a novel LLM-based Multi-Agent framework to synthesize text-to-TS datasets and proposes BRIDGE, a hybrid framework integrating semantic prototypes with text descriptions for domain-level guidance.", "result": "BRIDGE achieves state-of-the-art generation fidelity on 11 of 12 datasets and improves controllability by 12.52% (MSE) and 6.34% (MAE) compared to no-text-input generation.", "conclusion": "Text-Controlled TSG, supported by the BRIDGE framework, demonstrates significant potential for generating tailored time-series data with improved fidelity and controllability."}}
{"id": "2505.03406", "pdf": "https://arxiv.org/pdf/2505.03406", "abs": "https://arxiv.org/abs/2505.03406", "authors": ["Mohammad Shoaib Ansari", "Mohd Sohail Ali Khan", "Shubham Revankar", "Aditya Varma", "Anil S. Mokhade"], "title": "Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "This research paper investigates the application of Large Language Models\n(LLMs) in healthcare, specifically focusing on enhancing medical decision\nsupport through Retrieval-Augmented Generation (RAG) integrated with\nhospital-specific data and fine-tuning using Quantized Low-Rank Adaptation\n(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By\nembedding and retrieving context-relevant healthcare information, the system\nsignificantly improves response accuracy. QLoRA facilitates notable parameter\nefficiency and memory optimization, preserving the integrity of medical\ninformation through specialized quantization techniques. Our research also\nshows that our model performs relatively well on various medical benchmarks,\nindicating that it can be used to make basic medical suggestions. This paper\ndetails the system's technical components, including its architecture,\nquantization methods, and key healthcare applications such as enhanced disease\nprediction from patient symptoms and medical history, treatment suggestions,\nand efficient summarization of complex medical reports. We touch on the ethical\nconsiderations-patient privacy, data security, and the need for rigorous\nclinical validation-as well as the practical challenges of integrating such\nsystems into real-world healthcare workflows. Furthermore, the lightweight\nquantized weights ensure scalability and ease of deployment even in\nlow-resource hospital environments. Finally, the paper concludes with an\nanalysis of the broader impact of LLMs on healthcare and outlines future\ndirections for LLMs in medical settings.", "AI": {"tldr": "The paper explores using LLMs in healthcare with RAG and QLoRA for improved medical decision support, focusing on accuracy, efficiency, and ethical considerations.", "motivation": "To enhance medical decision-making by integrating hospital-specific data with LLMs, ensuring accuracy and efficiency while addressing ethical concerns.", "method": "Uses Llama 3.2-3B-Instruct with RAG for context retrieval and QLoRA for parameter efficiency and memory optimization.", "result": "Improves response accuracy and performs well on medical benchmarks, enabling basic medical suggestions.", "conclusion": "LLMs show promise in healthcare but require ethical handling and further research for real-world integration."}}
{"id": "2505.03153", "pdf": "https://arxiv.org/pdf/2505.03153", "abs": "https://arxiv.org/abs/2505.03153", "authors": ["Sparsh Bansal", "Mingyang Wu", "Xin Wang", "Shu Hu"], "title": "Robust Fairness Vision-Language Learning for Medical Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "The advent of Vision-Language Models (VLMs) in medical image analysis has the\npotential to help process multimodal inputs and increase performance over\ntraditional inference methods. However, when considering the domain in which\nthese models will be implemented, fairness and robustness are important to\nensure the model stays true for any patient. In this paper, we introduce a\nframework for ensuring robustness and fairness of VLM models. This framework\nmodifies the loss function at training by identifying and adjusting faulty\nimage-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing\nSinkhorn distance to ensure the loss distributions of protected groups do not\ndeviate from the total loss. Experimental testing of our framework shows up to\na 8.6\\% improvement when looking at equity-scaled AUC.", "AI": {"tldr": "A framework for improving fairness and robustness in Vision-Language Models (VLMs) for medical image analysis by adjusting faulty image-text pairs and ensuring balanced loss distributions.", "motivation": "Address fairness and robustness in VLMs for medical applications to ensure reliable performance across all patient groups.", "method": "Introduces a framework with Dynamic Bad Pair Mining and Sinkhorn distance to adjust faulty pairs and balance loss distributions.", "result": "Achieves up to 8.6% improvement in equity-scaled AUC.", "conclusion": "The framework effectively enhances fairness and robustness in VLMs for medical image analysis."}}
{"id": "2505.03118", "pdf": "https://arxiv.org/pdf/2505.03118", "abs": "https://arxiv.org/abs/2505.03118", "authors": ["Dmytro Shamatrin"], "title": "Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion", "categories": ["cs.LG"], "comment": null, "summary": "Multi-label classification (MLC) requires predicting multiple labels per\nsample, often under heavy class imbalance and noisy conditions. Traditional\napproaches apply fixed thresholds or treat labels independently, overlooking\ncontext and global rarity. We introduce an adaptive thresholding mechanism that\nfuses global (IDF-based) and local (KNN-based) signals to produce per-label,\nper-instance thresholds. Instead of applying these as hard cutoffs, we treat\nthem as differentiable penalties in the loss, providing smooth supervision and\nbetter calibration. Our architecture is lightweight, interpretable, and highly\nmodular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,\nsubstantially outperforming tree-based and pretrained transformer-based\nmethods. We release full code for reproducibility and future extensions.", "AI": {"tldr": "The paper introduces an adaptive thresholding mechanism for multi-label classification, combining global and local signals to improve performance under class imbalance and noisy conditions.", "motivation": "Traditional MLC methods use fixed thresholds or treat labels independently, ignoring context and global rarity, leading to suboptimal performance.", "method": "Proposes a lightweight, interpretable architecture with differentiable penalties in the loss, using IDF-based global and KNN-based local signals for adaptive thresholding.", "result": "Achieves a macro-F1 of 0.1712 on AmazonCat-13K, outperforming tree-based and transformer-based methods.", "conclusion": "The approach is effective, modular, and reproducible, with potential for future extensions."}}
{"id": "2505.03553", "pdf": "https://arxiv.org/pdf/2505.03553", "abs": "https://arxiv.org/abs/2505.03553", "authors": ["Kolawole E. Ogunsina", "Morayo A. Ogunsina"], "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning", "categories": ["cs.AI", "cs.DC"], "comment": "15 pages", "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.", "AI": {"tldr": "A novel consensus mechanism inspired by Hashgraph is proposed to align outputs of diverse LLMs, reducing inconsistencies and hallucinations by treating each model as a peer in a distributed system.", "motivation": "Addressing divergent outputs from proprietary LLMs due to training and inference variations, aiming for reliable AI systems.", "method": "Uses Hashgraph's gossip-about-gossip and virtual voting for iterative consensus among models, improving accuracy through cross-verification.", "result": "Preliminary findings suggest the approach reduces nonfactual outputs and enhances response fidelity in multi-agent AI systems.", "conclusion": "The Hashgraph-inspired consensus offers a promising path for self-validation and high-fidelity responses in complex AI tasks."}}
{"id": "2503.14519", "pdf": "https://arxiv.org/pdf/2503.14519", "abs": "https://arxiv.org/abs/2503.14519", "authors": ["Kar Balan", "Andrew Gilbert", "John Collomosse"], "title": "Content ARCs: Decentralized Content Rights in the Age of Generative AI", "categories": ["cs.CY", "cs.AI", "cs.DL", "eess.IV"], "comment": null, "summary": "The rise of Generative AI (GenAI) has sparked significant debate over\nbalancing the interests of creative rightsholders and AI developers. As GenAI\nmodels are trained on vast datasets that often include copyrighted material,\nquestions around fair compensation and proper attribution have become\nincreasingly urgent. To address these challenges, this paper proposes a\nframework called Content ARCs (Authenticity, Rights, Compensation). By\ncombining open standards for provenance and dynamic licensing with data\nattribution, and decentralized technologies, Content ARCs create a mechanism\nfor managing rights and compensating creators for using their work in AI\ntraining. We characterize several nascent works in the AI data licensing space\nwithin Content ARCs and identify where challenges remain to fully implement the\nend-to-end framework.", "AI": {"tldr": "The paper proposes Content ARCs, a framework to manage rights and compensate creators for AI training data use, addressing challenges in GenAI and copyright.", "motivation": "The rise of GenAI raises concerns about fair compensation and attribution for copyrighted material used in training datasets.", "method": "The framework combines open standards for provenance, dynamic licensing, data attribution, and decentralized technologies.", "result": "Content ARCs provide a mechanism for rights management and creator compensation, though challenges remain for full implementation.", "conclusion": "The framework addresses urgent issues in AI data licensing but requires further work for end-to-end adoption."}}
{"id": "2503.02950", "pdf": "https://arxiv.org/pdf/2503.02950", "abs": "https://arxiv.org/abs/2503.02950", "authors": ["Danqing Zhang", "Balaji Rama", "Jingyi Ni", "Shiying He", "Fu Zhao", "Kunyu Chen", "Arnold Chen", "Junyu Cao"], "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent\napplications. Our framework addresses a critical gap in the web agent ecosystem\nwith a production-ready solution that combines minimal serverless backend\nconfiguration, intuitive user and browser interfaces, and extensible research\ncapabilities in agent planning, memory, and tree search. For the core\nLiteWebAgent agent framework, we implemented a simple yet effective baseline\nusing recursive function calling, providing with decoupled action generation\nand action grounding. In addition, we integrate advanced research components\nsuch as agent planning, agent workflow memory, and tree search in a modular and\nextensible manner. We then integrate the LiteWebAgent agent framework with\nfrontend and backend as deployed systems in two formats: (1) a production\nVercel-based web application, which provides users with an agent-controlled\nremote browser, (2) a Chrome extension leveraging LiteWebAgent's API to control\nan existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent\nframework is available at https://github.com/PathOnAI/LiteWebAgent, with\ndeployed frontend at https://lite-web-agent.vercel.app/.", "AI": {"tldr": "LiteWebAgent is an open-source framework for VLM-based web agents, offering a production-ready solution with minimal backend setup, user-friendly interfaces, and extensible research features like planning and tree search.", "motivation": "Addressing the lack of a production-ready, extensible web agent framework in the ecosystem.", "method": "Uses recursive function calling for action generation and grounding, with modular integration of advanced components like planning and tree search.", "result": "Deployed as a Vercel web app and Chrome extension, enabling remote browser control via LiteWebAgent's API.", "conclusion": "LiteWebAgent provides a practical, extensible solution for web agent applications, available as open-source with deployed systems."}}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427", "abs": "https://arxiv.org/abs/2505.03427", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.", "AI": {"tldr": "The paper introduces MedArabiQ, a benchmark dataset for evaluating LLMs in the Arabic medical domain, addressing the lack of existing resources.", "motivation": "The efficacy of LLMs in the Arabic medical domain is unexplored due to missing high-quality datasets and benchmarks.", "method": "Constructed MedArabiQ using medical exams and public datasets, then evaluated five LLMs, including GPT-4o and Claude 3.5-Sonnet.", "result": "Findings emphasize the need for multilingual benchmarks to ensure fair LLM deployment in healthcare.", "conclusion": "MedArabiQ provides a foundation for future research on multilingual LLM capabilities in healthcare."}}
{"id": "2505.03154", "pdf": "https://arxiv.org/pdf/2505.03154", "abs": "https://arxiv.org/abs/2505.03154", "authors": ["Yuxuan Mu", "Hung Yu Ling", "Yi Shi", "Ismael Baira Ojeda", "Pengcheng Xi", "Chang Shu", "Fabio Zinno", "Xue Bin Peng"], "title": "StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "17 pages, 13 figures", "summary": "Motion capture (mocap) data often exhibits visually jarring artifacts due to\ninaccurate sensors and post-processing. Cleaning this corrupted data can\nrequire substantial manual effort from human experts, which can be a costly and\ntime-consuming process. Previous data-driven motion cleanup methods offer the\npromise of automating this cleanup process, but often require in-domain paired\ncorrupted-to-clean training data. Constructing such paired datasets requires\naccess to high-quality, relatively artifact-free motion clips, which often\nnecessitates laborious manual cleanup. In this work, we present StableMotion, a\nsimple yet effective method for training motion cleanup models directly from\nunpaired corrupted datasets that need cleanup. The core component of our method\nis the introduction of motion quality indicators, which can be easily annotated\nthrough manual labeling or heuristic algorithms and enable training of\nquality-aware motion generation models on raw motion data with mixed quality.\nAt test time, the model can be prompted to generate high-quality motions using\nthe quality indicators. Our method can be implemented through a simple\ndiffusion-based framework, leading to a unified motion generate-discriminate\nmodel, which can be used to both identify and fix corrupted frames. We\ndemonstrate that our proposed method is effective for training motion cleanup\nmodels on raw mocap data in production scenarios by applying StableMotion to\nSoccerMocap, a 245-hour soccer mocap dataset containing real-world motion\nartifacts. The trained model effectively corrects a wide range of motion\nartifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.\nSee https://youtu.be/3Y7MMAH02B4 for more results.", "AI": {"tldr": "StableMotion automates motion capture cleanup using unpaired datasets and quality indicators, reducing manual effort and artifacts.", "motivation": "Manual cleanup of corrupted mocap data is costly and time-consuming, and existing methods require paired training data, which is hard to obtain.", "method": "Uses motion quality indicators to train a diffusion-based model on unpaired data, enabling quality-aware motion generation and cleanup.", "result": "Reduces motion pops by 68% and frozen frames by 81% on the SoccerMocap dataset.", "conclusion": "StableMotion is a practical solution for mocap cleanup, eliminating the need for paired data and manual effort."}}
{"id": "2505.03155", "pdf": "https://arxiv.org/pdf/2505.03155", "abs": "https://arxiv.org/abs/2505.03155", "authors": ["Max Qiushi Lin", "Jincheng Mei", "Matin Aghaei", "Michael Lu", "Bo Dai", "Alekh Agarwal", "Dale Schuurmans", "Csaba Szepesvari", "Sharan Vaswani"], "title": "Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation", "categories": ["cs.LG"], "comment": "75 pages", "summary": "Policy gradient (PG) methods have played an essential role in the empirical\nsuccesses of reinforcement learning. In order to handle large state-action\nspaces, PG methods are typically used with function approximation. In this\nsetting, the approximation error in modeling problem-dependent quantities is a\nkey notion for characterizing the global convergence of PG methods. We focus on\nSoftmax PG with linear function approximation (referred to as\n$\\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant\nto the algorithm's global convergence even for the stochastic bandit setting.\nConsequently, we first identify the necessary and sufficient conditions on the\nfeature representation that can guarantee the asymptotic global convergence of\n$\\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$\niterations of $\\texttt{Lin-SPG}$ with a problem-specific learning rate result\nin an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that\n$\\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure\nasymptotic global convergence to the optimal policy.", "AI": {"tldr": "The paper shows that approximation error in Softmax PG with linear function approximation (Lin-SPG) doesn't affect global convergence, even in stochastic bandit settings. It identifies necessary/sufficient feature conditions for asymptotic convergence and proves $O(1/T)$ convergence with a problem-specific learning rate.", "motivation": "To understand the impact of approximation error on the global convergence of policy gradient methods, especially in large state-action spaces with function approximation.", "method": "Focuses on Softmax PG with linear function approximation (Lin-SPG), analyzing its convergence properties under specific feature conditions.", "result": "Proves that Lin-SPG achieves $O(1/T)$ convergence to the optimal policy with a problem-specific learning rate and asymptotic convergence with any constant learning rate.", "conclusion": "The approximation error is irrelevant to Lin-SPG's global convergence, and the identified feature conditions ensure asymptotic convergence to the optimal policy."}}
{"id": "2505.03570", "pdf": "https://arxiv.org/pdf/2505.03570", "abs": "https://arxiv.org/abs/2505.03570", "authors": ["Mariya Davydova", "Daniel Jeffries", "Patrick Barker", "Arturo M\u00e1rquez Flores", "Sin\u00e9ad Ryan"], "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents", "categories": ["cs.AI"], "comment": null, "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.", "AI": {"tldr": "OSUniverse is a benchmark for GUI-navigation AI agents, featuring tasks of increasing complexity, automated validation, and calibration to challenge SOTA agents while being solvable by humans.", "motivation": "To provide a robust, extensible, and automated benchmark for evaluating the progress and capabilities of GUI-navigation AI agents.", "method": "Tasks are divided by complexity, from basic clicking to multistep, multiapplication tests. Automated validation ensures accuracy.", "result": "SOTA agents score \u226450%, while humans achieve perfect accuracy. Automated validation has <2% error rate.", "conclusion": "OSUniverse offers a reliable, automated measure for assessing GUI-navigation AI agents' effectiveness."}}
{"id": "2504.16960", "pdf": "https://arxiv.org/pdf/2504.16960", "abs": "https://arxiv.org/abs/2504.16960", "authors": ["Weixuan Chen", "Qianqian Yang", "Shuo Shao", "Zhiguo Shi", "Jiming Chen", "Xuemin", "Shen"], "title": "Can Knowledge Improve Security? A Coding-Enhanced Jamming Approach for Semantic Communication", "categories": ["cs.IT", "eess.IV", "math.IT"], "comment": null, "summary": "As semantic communication (SemCom) attracts growing attention as a novel\ncommunication paradigm, ensuring the security of transmitted semantic\ninformation over open wireless channels has become a critical issue. However,\ntraditional encryption methods often introduce significant additional\ncommunication overhead to maintain stability, and conventional learning-based\nsecure SemCom methods typically rely on a channel capacity advantage for the\nlegitimate receiver, which is challenging to guarantee in real-world scenarios.\nIn this paper, we propose a coding-enhanced jamming method that eliminates the\nneed to transmit a secret key by utilizing shared knowledge-potentially part of\nthe training set of the SemCom system-between the legitimate receiver and the\ntransmitter. Specifically, we leverage the shared private knowledge base to\ngenerate a set of private digital codebooks in advance using neural network\n(NN)-based encoders. For each transmission, we encode the transmitted data into\ndigital sequence Y1 and associate Y1 with a sequence randomly picked from the\nprivate codebook, denoted as Y2, through superposition coding. Here, Y1 serves\nas the outer code and Y2 as the inner code. By optimizing the power allocation\nbetween the inner and outer codes, the legitimate receiver can reconstruct the\ntransmitted data using successive decoding with the index of Y2 shared, while\nthe eavesdropper' s decoding performance is severely degraded, potentially to\nthe point of random guessing. Experimental results demonstrate that our method\nachieves comparable security to state-of-the-art approaches while significantly\nimproving the reconstruction performance of the legitimate receiver by more\nthan 1 dB across varying channel signal-to-noise ratios (SNRs) and compression\nratios.", "AI": {"tldr": "A coding-enhanced jamming method for secure semantic communication eliminates secret key transmission by leveraging shared knowledge and neural network-based codebooks, improving security and receiver performance.", "motivation": "Traditional encryption and learning-based secure semantic communication methods introduce overhead or rely on unrealistic channel advantages, prompting a need for a more practical and efficient solution.", "method": "Utilizes shared private knowledge to generate NN-based digital codebooks, encodes data into sequences (Y1 and Y2) via superposition coding, and optimizes power allocation for secure transmission.", "result": "Achieves comparable security to state-of-the-art methods while enhancing legitimate receiver performance by over 1 dB across varying SNRs and compression ratios.", "conclusion": "The proposed method offers a practical and efficient solution for secure semantic communication without relying on secret keys or unrealistic channel conditions."}}
{"id": "2505.03452", "pdf": "https://arxiv.org/pdf/2505.03452", "abs": "https://arxiv.org/abs/2505.03452", "authors": ["Matan Orbach", "Ohad Eytan", "Benjamin Sznajder", "Ariel Gera", "Odellia Boni", "Yoav Kantor", "Gal Bloch", "Omri Levy", "Hadas Abraham", "Nitzan Barzilay", "Eyal Shnarch", "Michael E. Factor", "Shila Ofek-Koifman", "Paula Ta-Shma", "Assaf Toledo"], "title": "An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a\ngiven use case can be complex and expensive. Motivated by this challenge,\nframeworks for RAG hyper-parameter optimization (HPO) have recently emerged,\nyet their effectiveness has not been rigorously benchmarked. To address this\ngap, we present a comprehensive study involving 5 HPO algorithms over 5\ndatasets from diverse domains, including a new one collected for this work on\nreal-world product documentation. Our study explores the largest HPO search\nspace considered to date, with two optimized evaluation metrics. Analysis of\nthe results shows that RAG HPO can be done efficiently, either greedily or with\niterative random search, and that it significantly boosts RAG performance for\nall datasets. For greedy HPO approaches, we show that optimizing models first\nis preferable to the prevalent practice of optimizing sequentially according to\nthe RAG pipeline order.", "AI": {"tldr": "The paper benchmarks RAG hyper-parameter optimization (HPO) methods, showing greedy or iterative random search can efficiently boost performance across diverse datasets.", "motivation": "The complexity and cost of finding optimal RAG configurations motivate the need for rigorous benchmarking of HPO frameworks.", "method": "The study evaluates 5 HPO algorithms on 5 datasets, including a new real-world product documentation dataset, using the largest HPO search space to date.", "result": "RAG HPO significantly improves performance; greedy approaches benefit from optimizing models first rather than following the RAG pipeline order.", "conclusion": "Efficient RAG HPO is achievable with greedy or iterative random search, and prioritizing model optimization enhances performance."}}
{"id": "2505.03173", "pdf": "https://arxiv.org/pdf/2505.03173", "abs": "https://arxiv.org/abs/2505.03173", "authors": ["Sameer Malik", "Moyuru Yamada", "Ayush Singh", "Dishank Aggarwal"], "title": "RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Comprehending long videos remains a significant challenge for Large\nMulti-modal Models (LMMs). Current LMMs struggle to process even minutes to\nhours videos due to their lack of explicit memory and retrieval mechanisms. To\naddress this limitation, we propose RAVU (Retrieval Augmented Video\nUnderstanding), a novel framework for video understanding enhanced by retrieval\nwith compositional reasoning over a spatio-temporal graph. We construct a graph\nrepresentation of the video, capturing both spatial and temporal relationships\nbetween entities. This graph serves as a long-term memory, allowing us to track\nobjects and their actions across time. To answer complex queries, we decompose\nthe queries into a sequence of reasoning steps and execute these steps on the\ngraph, retrieving relevant key information. Our approach enables more accurate\nunderstanding of long videos, particularly for queries that require multi-hop\nreasoning and tracking objects across frames. Our approach demonstrate superior\nperformances with limited retrieved frames (5-10) compared with other SOTA\nmethods and baselines on two major video QA datasets, NExT-QA and EgoSchema.", "AI": {"tldr": "RAVU is a framework for long video understanding using retrieval-augmented compositional reasoning over a spatio-temporal graph, outperforming SOTA methods with limited frames.", "motivation": "Current LMMs struggle with long videos due to lack of memory and retrieval mechanisms, limiting their ability to track objects and actions over time.", "method": "RAVU constructs a spatio-temporal graph as long-term memory, decomposes queries into reasoning steps, and retrieves key information from the graph.", "result": "RAVU achieves superior performance on video QA datasets (NExT-QA and EgoSchema) with only 5-10 retrieved frames.", "conclusion": "RAVU effectively addresses the challenge of long video understanding by leveraging retrieval-augmented compositional reasoning."}}
{"id": "2505.03165", "pdf": "https://arxiv.org/pdf/2505.03165", "abs": "https://arxiv.org/abs/2505.03165", "authors": ["Nikita Ravi", "Abhinav Goel", "James C. Davis", "George K. Thiruvathukal"], "title": "Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "The field of deep learning has witnessed significant breakthroughs, spanning\nvarious applications, and fundamentally transforming current software\ncapabilities. However, alongside these advancements, there have been increasing\nconcerns about reproducing the results of these deep learning methods. This is\nsignificant because reproducibility is the foundation of reliability and\nvalidity in software development, particularly in the rapidly evolving domain\nof deep learning. The difficulty of reproducibility may arise due to several\nreasons, including having differences from the original execution environment,\nincompatible software libraries, proprietary data and source code, lack of\ntransparency, and the stochastic nature in some software. A study conducted by\nthe Nature journal reveals that more than 70% of researchers failed to\nreproduce other researchers experiments and over 50% failed to reproduce their\nown experiments. Irreproducibility of deep learning poses significant\nchallenges for researchers and practitioners. To address these concerns, this\npaper presents a systematic approach at analyzing and improving the\nreproducibility of deep learning models by demonstrating these guidelines using\na case study. We illustrate the patterns and anti-patterns involved with these\nguidelines for improving the reproducibility of deep learning models. These\nguidelines encompass establishing a methodology to replicate the original\nsoftware environment, implementing end-to-end training and testing algorithms,\ndisclosing architectural designs, and enhancing transparency in data processing\nand training pipelines. We also conduct a sensitivity analysis to understand\nthe model performance across diverse conditions. By implementing these\nstrategies, we aim to bridge the gap between research and practice, so that\ninnovations in deep learning can be effectively reproduced and deployed within\nsoftware.", "AI": {"tldr": "The paper addresses reproducibility challenges in deep learning, proposing systematic guidelines to improve it through environment replication, transparency, and sensitivity analysis.", "motivation": "Reproducibility is critical for reliability in deep learning, yet many studies fail to replicate results due to environmental, transparency, and stochastic issues.", "method": "The paper introduces guidelines for reproducibility, including replicating software environments, end-to-end algorithms, architectural transparency, and sensitivity analysis.", "result": "The guidelines help improve reproducibility, as demonstrated in a case study, highlighting patterns and anti-patterns for effective deployment.", "conclusion": "The proposed strategies bridge the gap between research and practice, ensuring deep learning innovations are reproducible and deployable."}}
{"id": "2505.03641", "pdf": "https://arxiv.org/pdf/2505.03641", "abs": "https://arxiv.org/abs/2505.03641", "authors": ["Chen Wei", "Chi Zhang", "Jiachen Zou", "Haotian Deng", "Dietmar Heinke", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability", "categories": ["cs.AI"], "comment": "accepted at ICML 2025", "summary": "Human decision-making in cognitive tasks and daily life exhibits considerable\nvariability, shaped by factors such as task difficulty, individual preferences,\nand personal experiences. Understanding this variability across individuals is\nessential for uncovering the perceptual and decision-making mechanisms that\nhumans rely on when faced with uncertainty and ambiguity. We present a\ncomputational framework BAM (Boundary Alignment & Manipulation framework) that\ncombines perceptual boundary sampling in ANNs and human behavioral experiments\nto systematically investigate this phenomenon. Our perceptual boundary sampling\nalgorithm generates stimuli along ANN decision boundaries that intrinsically\ninduce significant perceptual variability. The efficacy of these stimuli is\nempirically validated through large-scale behavioral experiments involving 246\nparticipants across 116,715 trials, culminating in the variMNIST dataset\ncontaining 19,943 systematically annotated images. Through personalized model\nalignment and adversarial generation, we establish a reliable method for\nsimultaneously predicting and manipulating the divergent perceptual decisions\nof pairs of participants. This work bridges the gap between computational\nmodels and human individual difference research, providing new tools for\npersonalized perception analysis.", "AI": {"tldr": "The paper introduces BAM, a computational framework combining ANN perceptual boundary sampling and human experiments to study decision-making variability, validated by large-scale behavioral data.", "motivation": "To understand human decision-making variability under uncertainty and ambiguity, bridging computational models and individual differences.", "method": "BAM framework integrates ANN boundary sampling and human experiments, using personalized model alignment and adversarial generation.", "result": "Validated with 246 participants (116,715 trials), creating the variMNIST dataset (19,943 images), enabling prediction and manipulation of perceptual decisions.", "conclusion": "BAM bridges computational models and human research, offering tools for personalized perception analysis."}}
{"id": "2504.18581", "pdf": "https://arxiv.org/pdf/2504.18581", "abs": "https://arxiv.org/abs/2504.18581", "authors": ["Weixuan Chen", "Shunpu Tang", "Qianqian Yang", "Zhiguo Shi", "Dusit Niyato"], "title": "Enhancing Privacy in Semantic Communication over Wiretap Channels leveraging Differential Privacy", "categories": ["cs.CR", "eess.IV"], "comment": "The order of authorship and the list of authors for this paper still\n  require further discussion. In addition, my supervisor believes that the\n  overall structure of this paper needs to be rewritten", "summary": "Semantic communication (SemCom) improves transmission efficiency by focusing\non task-relevant information. However, transmitting semantic-rich data over\ninsecure channels introduces privacy risks. This paper proposes a novel SemCom\nframework that integrates differential privacy (DP) mechanisms to protect\nsensitive semantic features. This method employs the generative adversarial\nnetwork (GAN) inversion technique to extract disentangled semantic features and\nuses neural networks (NNs) to approximate the DP application and removal\nprocesses, effectively mitigating the non-invertibility issue of DP.\nAdditionally, an NN-based encryption scheme is introduced to strengthen the\nsecurity of channel inputs. Simulation results demonstrate that the proposed\napproach effectively prevents eavesdroppers from reconstructing sensitive\ninformation by generating chaotic or fake images, while ensuring high-quality\nimage reconstruction for legitimate users. The system exhibits robust\nperformance across various privacy budgets and channel conditions, achieving an\noptimal balance between privacy protection and reconstruction fidelity.", "AI": {"tldr": "A novel SemCom framework integrates DP and GAN inversion to protect sensitive semantic features, ensuring privacy and high-quality reconstruction.", "motivation": "SemCom improves efficiency but risks privacy over insecure channels; this work addresses privacy protection in SemCom.", "method": "Uses GAN inversion for disentangled feature extraction, NNs for DP processes, and NN-based encryption for secure channel inputs.", "result": "Prevents eavesdroppers from reconstructing sensitive data while maintaining high-quality reconstruction for legitimate users.", "conclusion": "The framework balances privacy and fidelity, performing robustly across privacy budgets and channel conditions."}}
{"id": "2505.03467", "pdf": "https://arxiv.org/pdf/2505.03467", "abs": "https://arxiv.org/abs/2505.03467", "authors": ["Shuang Zhou", "Jiashuo Wang", "Zidu Xu", "Song Wang", "David Brauer", "Lindsay Welton", "Jacob Cogan", "Yuen-Hei Chung", "Lei Tian", "Zaifu Zhan", "Yu Hou", "Mingquan Lin", "Genevieve B. Melton", "Rui Zhang"], "title": "Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis", "categories": ["cs.CL"], "comment": "22 pages, 8 figures", "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.", "AI": {"tldr": "ConfiDx, an uncertainty-aware LLM, improves disease diagnosis by identifying and explaining diagnostic uncertainties, enhancing reliability.", "motivation": "Diagnostic uncertainty in clinical notes can lead to misdiagnosis. Existing systems lack explicit uncertainty identification and explanation.", "method": "Fine-tuned open-source LLMs (ConfiDx) with diagnostic criteria and annotated datasets capturing diagnostic ambiguity.", "result": "ConfiDx excelled in identifying uncertainties, achieving superior diagnostic performance and generating trustworthy explanations.", "conclusion": "ConfiDx is the first to jointly address uncertainty recognition and explanation, significantly improving diagnostic system reliability."}}
{"id": "2505.03176", "pdf": "https://arxiv.org/pdf/2505.03176", "abs": "https://arxiv.org/abs/2505.03176", "authors": ["Hafez Ghaemi", "Eilif Muller", "Shahab Bakhtiari"], "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Current self-supervised algorithms mostly rely on transformations such as\ndata augmentation and masking to learn visual representations. This is achieved\nby inducing invariance or equivariance with respect to these transformations\nafter encoding two views of an image. This dominant two-view paradigm can limit\nthe flexibility of learned representations for downstream adaptation by\ncreating performance trade-offs between invariance-related tasks such as image\nclassification and more fine-grained equivariance-related tasks. In this work,\nwe introduce \\emph{seq-JEPA}, a world modeling paradigm based on\njoint-embedding predictive architecture that leverages architectural inductive\nbiases to resolve this trade-off. Without requiring an additional equivariance\npredictor or loss term, seq-JEPA simultaneously learns two architecturally\nsegregated representations: one equivariant to the specified transformations\nand another invariant to them and suited for tasks such as classification. To\ndo so, our model processes a short sequence of different views (observations)\nof an input image. Each encoded view is concatenated with embeddings\ncorresponding to the relative transformation (action) producing the next\nobservation in the sequence. A transformer encoder outputs an aggregate\nrepresentation of this sequence, which is subsequently conditioned on the\naction leading to the next observation to predict its representation.\nEmpirically, seq-JEPA achieves strong performance on equivariant benchmarks and\nimage classification without sacrificing one for the other. Additionally, our\nframework excels at tasks that inherently require aggregating a sequence of\nobservations, such as path integration across actions and predictive learning\nacross eye movements.", "AI": {"tldr": "seq-JEPA introduces a world modeling paradigm using joint-embedding predictive architecture to learn both invariant and equivariant representations without trade-offs, excelling in diverse tasks.", "motivation": "The two-view paradigm in self-supervised learning limits flexibility by creating trade-offs between invariance (e.g., classification) and equivariance (e.g., fine-grained tasks).", "method": "seq-JEPA processes a sequence of image views, concatenates embeddings with relative transformations, and uses a transformer encoder to predict representations, segregating invariant and equivariant features.", "result": "seq-JEPA achieves strong performance on both equivariant benchmarks and image classification, excelling in tasks requiring sequence aggregation.", "conclusion": "seq-JEPA resolves the trade-off between invariance and equivariance, offering a flexible and effective framework for diverse visual representation tasks."}}
{"id": "2505.03172", "pdf": "https://arxiv.org/pdf/2505.03172", "abs": "https://arxiv.org/abs/2505.03172", "authors": ["Caleb Chuck", "Fan Feng", "Carl Qi", "Chang Shi", "Siddhant Agarwal", "Amy Zhang", "Scott Niekum"], "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Published at ICLR 2025", "summary": "Hindsight relabeling is a powerful tool for overcoming sparsity in\ngoal-conditioned reinforcement learning (GCRL), especially in certain domains\nsuch as navigation and locomotion. However, hindsight relabeling can struggle\nin object-centric domains. For example, suppose that the goal space consists of\na robotic arm pushing a particular target block to a goal location. In this\ncase, hindsight relabeling will give high rewards to any trajectory that does\nnot interact with the block. However, these behaviors are only useful when the\nobject is already at the goal -- an extremely rare case in practice. A dataset\ndominated by these kinds of trajectories can complicate learning and lead to\nfailures. In object-centric domains, one key intuition is that meaningful\ntrajectories are often characterized by object-object interactions such as\npushing the block with the gripper. To leverage this intuition, we introduce\nHindsight Relabeling using Interactions (HInt), which combines interactions\nwith hindsight relabeling to improve the sample efficiency of downstream RL.\nHowever because interactions do not have a consensus statistical definition\ntractable for downstream GCRL, we propose a definition of interactions based on\nthe concept of null counterfactual: a cause object is interacting with a target\nobject if, in a world where the cause object did not exist, the target object\nwould have different transition dynamics. We leverage this definition to infer\ninteractions in Null Counterfactual Interaction Inference (NCII), which uses a\n\"nulling'' operation with a learned model to infer interactions. NCII is able\nto achieve significantly improved interaction inference accuracy in both simple\nlinear dynamics domains and dynamic robotic domains in Robosuite, Robot Air\nHockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.", "AI": {"tldr": "HInt improves GCRL in object-centric domains by combining hindsight relabeling with interactions, defined via null counterfactuals, enhancing sample efficiency.", "motivation": "Hindsight relabeling struggles in object-centric domains due to misleading rewards for non-interactive trajectories, hindering learning.", "method": "Proposes HInt, which integrates interactions (defined via null counterfactuals) with hindsight relabeling, using NCII for interaction inference.", "result": "NCII improves interaction inference accuracy, and HInt boosts sample efficiency by up to 4x in tested domains.", "conclusion": "HInt effectively addresses limitations of hindsight relabeling in object-centric tasks, leveraging interactions for better RL performance."}}
{"id": "2505.03643", "pdf": "https://arxiv.org/pdf/2505.03643", "abs": "https://arxiv.org/abs/2505.03643", "authors": ["Chelsea Sidrane", "Jana Tumova"], "title": "BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems", "categories": ["cs.AI", "cs.LO", "cs.SY", "eess.SY"], "comment": null, "summary": "Learning-enabled planning and control algorithms are increasingly popular,\nbut they often lack rigorous guarantees of performance or safety. We introduce\nan algorithm for computing underapproximate backward reachable sets of\nnonlinear discrete time neural feedback loops. We then use the backward\nreachable sets to check goal-reaching properties. Our algorithm is based on\noverapproximating the system dynamics function to enable computation of\nunderapproximate backward reachable sets through solutions of mixed-integer\nlinear programs. We rigorously analyze the soundness of our algorithm and\ndemonstrate it on a numerical example. Our work expands the class of properties\nthat can be verified for learning-enabled systems.", "AI": {"tldr": "An algorithm for computing underapproximate backward reachable sets in neural feedback loops is introduced, enabling goal-reaching property verification via mixed-integer linear programs.", "motivation": "Learning-enabled systems lack rigorous guarantees of performance or safety, necessitating methods to verify their properties.", "method": "The algorithm overapproximates system dynamics to compute underapproximate backward reachable sets using mixed-integer linear programs.", "result": "The algorithm is rigorously analyzed for soundness and demonstrated on a numerical example, expanding verifiable properties for learning-enabled systems.", "conclusion": "This work enhances the verification capabilities for learning-enabled systems by introducing a novel reachability analysis method."}}
{"id": "2505.03469", "pdf": "https://arxiv.org/pdf/2505.03469", "abs": "https://arxiv.org/abs/2505.03469", "authors": ["Bin Yu", "Hang Yuan", "Yuliang Wei", "Bailing Wang", "Weizhen Qi", "Kai Chen"], "title": "Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "Recent advances in large language models have demonstrated that Supervised\nFine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from\nlarge reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning\ncapabilities to non-reasoning models. However, models fine-tuned with this\napproach inherit the \"overthinking\" problem from teacher models, producing\nverbose and redundant reasoning chains during inference. To address this\nchallenge, we propose \\textbf{L}ong-\\textbf{S}hort Chain-of-Thought\n\\textbf{Mixture} \\textbf{S}upervised \\textbf{F}ine-\\textbf{T}uning\n(\\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their\nshort counterparts obtained through structure-preserved rewriting. Our\nexperiments demonstrate that models trained using the LS-Mixture SFT method,\ncompared to those trained with direct SFT, achieved an average accuracy\nimprovement of 2.3\\% across various benchmarks while substantially reducing\nmodel response length by approximately 47.61\\%. This work offers an approach to\nendow non-reasoning models with reasoning capabilities through supervised\nfine-tuning while avoiding the inherent overthinking problems inherited from\nteacher models, thereby enabling efficient reasoning in the fine-tuned models.", "AI": {"tldr": "LS-Mixture SFT combines long and short CoT reasoning data to improve model accuracy and reduce response length, addressing the 'overthinking' problem in fine-tuned models.", "motivation": "To transfer reasoning capabilities to non-reasoning models without inheriting verbose reasoning chains from teacher models.", "method": "Proposes LS-Mixture SFT, mixing long CoT reasoning data with short, rewritten counterparts for fine-tuning.", "result": "Achieves 2.3% higher accuracy and reduces response length by ~47.61% compared to direct SFT.", "conclusion": "LS-Mixture SFT effectively enhances reasoning in fine-tuned models while avoiding overthinking issues."}}
{"id": "2505.03184", "pdf": "https://arxiv.org/pdf/2505.03184", "abs": "https://arxiv.org/abs/2505.03184", "authors": ["Xiang Xu", "Ruotong Li", "Mengjun Yi", "Baile XU", "Furao Shen", "Jian Zhao"], "title": "Interactive Instance Annotation with Siamese Networks", "categories": ["cs.CV"], "comment": null, "summary": "Annotating instance masks is time-consuming and labor-intensive. A promising\nsolution is to predict contours using a deep learning model and then allow\nusers to refine them. However, most existing methods focus on in-domain\nscenarios, limiting their effectiveness for cross-domain annotation tasks. In\nthis paper, we propose SiamAnno, a framework inspired by the use of Siamese\nnetworks in object tracking. SiamAnno leverages one-shot learning to annotate\npreviously unseen objects by taking a bounding box as input and predicting\nobject boundaries, which can then be adjusted by annotators. Trained on one\ndataset and tested on another without fine-tuning, SiamAnno achieves\nstate-of-the-art (SOTA) performance across multiple datasets, demonstrating its\nability to handle domain and environment shifts in cross-domain tasks. We also\nprovide more comprehensive results compared to previous work, establishing a\nstrong baseline for future research. To our knowledge, SiamAnno is the first\nmodel to explore Siamese architecture for instance annotation.", "AI": {"tldr": "SiamAnno, a Siamese network-based framework, enables cross-domain instance annotation by predicting object boundaries from a bounding box, achieving SOTA performance without fine-tuning.", "motivation": "Manual instance mask annotation is labor-intensive. Existing methods lack effectiveness for cross-domain tasks, prompting the need for a robust solution.", "method": "SiamAnno uses one-shot learning with Siamese networks to predict object boundaries from a bounding box, allowing annotator refinement.", "result": "Achieves SOTA performance across datasets without fine-tuning, handling domain shifts effectively.", "conclusion": "SiamAnno sets a strong baseline for cross-domain instance annotation, pioneering Siamese networks in this task."}}
{"id": "2505.03178", "pdf": "https://arxiv.org/pdf/2505.03178", "abs": "https://arxiv.org/abs/2505.03178", "authors": ["Jiawei Wang", "Xintao Yan", "Yao Mu", "Haowei Sun", "Zhong Cao", "Henry X. Liu"], "title": "RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "Generating safety-critical scenarios in high-fidelity simulations offers a\npromising and cost-effective approach for efficient testing of autonomous\nvehicles. Existing methods typically rely on manipulating a single vehicle's\ntrajectory through sophisticated designed objectives to induce adversarial\ninteractions, often at the cost of realism and scalability. In this work, we\npropose the Risk-Adjustable Driving Environment (RADE), a simulation framework\nthat generates statistically realistic and risk-adjustable traffic scenes.\nBuilt upon a multi-agent diffusion architecture, RADE jointly models the\nbehavior of all agents in the environment and conditions their trajectories on\na surrogate risk measure. Unlike traditional adversarial methods, RADE learns\nrisk-conditioned behaviors directly from data, preserving naturalistic\nmulti-agent interactions with controllable risk levels. To ensure physical\nplausibility, we incorporate a tokenized dynamics check module that efficiently\nfilters generated trajectories using a motion vocabulary. We validate RADE on\nthe real-world rounD dataset, demonstrating that it preserves statistical\nrealism across varying risk levels and naturally increases the likelihood of\nsafety-critical events as the desired risk level grows up. Our results\nhighlight RADE's potential as a scalable and realistic tool for AV safety\nevaluation.", "AI": {"tldr": "RADE is a simulation framework for generating realistic, risk-adjustable traffic scenes for autonomous vehicle testing, using a multi-agent diffusion model and risk-conditioned behaviors.", "motivation": "Existing methods for generating safety-critical scenarios lack realism and scalability, relying on adversarial interactions. RADE aims to address this by preserving naturalistic multi-agent interactions with controllable risk.", "method": "RADE uses a multi-agent diffusion architecture to model all agents' behaviors, conditioned on a surrogate risk measure. It includes a tokenized dynamics check module for physical plausibility.", "result": "Validated on the rounD dataset, RADE maintains statistical realism across risk levels and increases safety-critical events as risk rises.", "conclusion": "RADE is a scalable, realistic tool for autonomous vehicle safety evaluation, outperforming traditional adversarial methods."}}
{"id": "2505.03668", "pdf": "https://arxiv.org/pdf/2505.03668", "abs": "https://arxiv.org/abs/2505.03668", "authors": ["Celeste Veronese", "Daniele Meli", "Alessandro Farinelli"], "title": "Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time", "categories": ["cs.AI"], "comment": "Accepted at 9th Conference on Neurosymbolic Learning and Reasoning", "summary": "This paper proposes an integration of temporal logical reasoning and\nPartially Observable Markov Decision Processes (POMDPs) to achieve\ninterpretable decision-making under uncertainty with macro-actions. Our method\nleverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus\n(EC) to generate \\emph{persistent} (i.e., constant) macro-actions, which guide\nMonte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,\nsignificantly reducing inference time while ensuring robust performance. Such\nmacro-actions are learnt via Inductive Logic Programming (ILP) from a few\ntraces of execution (belief-action pairs), thus eliminating the need for\nmanually designed heuristics and requiring only the specification of the POMDP\ntransition model. In the Pocman and Rocksample benchmark scenarios, our learned\nmacro-actions demonstrate increased expressiveness and generality when compared\nto time-independent heuristics, indeed offering substantial computational\nefficiency improvements.", "AI": {"tldr": "The paper integrates temporal logic (LTL) with POMDPs to create interpretable macro-actions, improving efficiency and performance in decision-making under uncertainty.", "motivation": "To achieve interpretable and efficient decision-making under uncertainty by combining temporal logic reasoning with POMDPs, avoiding manual heuristics.", "method": "Uses Linear Temporal Logic (LTL) and Event Calculus (EC) to generate persistent macro-actions, learned via Inductive Logic Programming (ILP) from belief-action traces, and integrates them with MCTS-based POMDP solvers.", "result": "Demonstrates improved expressiveness, generality, and computational efficiency in Pocman and Rocksample benchmarks compared to time-independent heuristics.", "conclusion": "The integration of temporal logic and POMDPs with learned macro-actions offers a robust and efficient solution for decision-making under uncertainty."}}
{"id": "2505.03473", "pdf": "https://arxiv.org/pdf/2505.03473", "abs": "https://arxiv.org/abs/2505.03473", "authors": ["Marta Boscariol", "Luana Bulla", "Lia Draetta", "Beatrice Fiuman\u00f2", "Emanuele Lenzi", "Leonardo Piano"], "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents", "categories": ["cs.CL"], "comment": null, "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.", "AI": {"tldr": "The paper evaluates GPT and LLama3 for long-tail Entity Linking (EL) using MHERCL v0.1, showing promising results compared to ReLiK.", "motivation": "Long-tail EL is understudied, and LLMs' potential for this task is unexplored despite their contextual understanding.", "method": "Assessed GPT and LLama3 on MHERCL v0.1, comparing their EL performance to ReLiK.", "result": "LLMs perform well in long-tail EL, bridging the gap between head and long-tail entity linking.", "conclusion": "LLMs can effectively supplement traditional methods for long-tail EL, offering new possibilities."}}
{"id": "2505.03203", "pdf": "https://arxiv.org/pdf/2505.03203", "abs": "https://arxiv.org/abs/2505.03203", "authors": ["Chang Xie", "Chenyi Zhuang", "Pan Gao"], "title": "PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Advanced diffusion models have made notable progress in text-to-image\ncompositional generation. However, it is still a challenge for existing models\nto achieve text-image alignment when confronted with complex text prompts. In\nthis work, we highlight two factors that affect this alignment: the quality of\nthe randomly initialized noise and the reliability of the generated controlling\nmask. We then propose PiCo (Pick-and-Control), a novel training-free approach\nwith two key components to tackle these two factors. First, we develop a noise\nselection module to assess the quality of the random noise and determine\nwhether the noise is suitable for the target text. A fast sampling strategy is\nutilized to ensure efficiency in the noise selection stage. Second, we\nintroduce a referring mask module to generate pixel-level masks and to\nprecisely modulate the cross-attention maps. The referring mask is applied to\nthe standard diffusion process to guide the reasonable interaction between text\nand image features. Extensive experiments have been conducted to verify the\neffectiveness of PiCo in liberating users from the tedious process of random\ngeneration and in enhancing the text-image alignment for diverse text\ndescriptions.", "AI": {"tldr": "PiCo improves text-to-image alignment in diffusion models by selecting better noise and using precise masks.", "motivation": "Existing models struggle with text-image alignment for complex prompts due to poor noise quality and unreliable masks.", "method": "PiCo introduces a noise selection module and a referring mask module to enhance alignment without training.", "result": "Experiments show PiCo improves alignment and reduces the need for random generation.", "conclusion": "PiCo effectively addresses alignment challenges in text-to-image generation."}}
{"id": "2505.03181", "pdf": "https://arxiv.org/pdf/2505.03181", "abs": "https://arxiv.org/abs/2505.03181", "authors": ["Jake Grigsby", "Yuke Zhu", "Michael Ryoo", "Juan Carlos Niebles"], "title": "VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making", "categories": ["cs.LG"], "comment": "SSI-FM Workshop ICLR 2025", "summary": "Recent research looks to harness the general knowledge and reasoning of large\nlanguage models (LLMs) into agents that accomplish user-specified goals in\ninteractive environments. Vision-language models (VLMs) extend LLMs to\nmulti-modal data and provide agents with the visual reasoning necessary for new\napplications in areas such as computer automation. However, agent tasks\nemphasize skills where accessible open-weight VLMs lag behind their LLM\nequivalents. For example, VLMs are less capable of following an environment's\nstrict output syntax requirements and are more focused on open-ended question\nanswering. Overcoming these limitations requires supervised fine-tuning (SFT)\non task-specific expert demonstrations. Our work approaches these challenges\nfrom an offline-to-online reinforcement learning (RL) perspective. RL lets us\nfine-tune VLMs to agent tasks while learning from the unsuccessful decisions of\nour own model or more capable (larger) models. We explore an off-policy RL\nsolution that retains the stability and simplicity of the widely used SFT\nworkflow while allowing our agent to self-improve and learn from low-quality\ndatasets. We demonstrate this technique with two open-weight VLMs across three\nmulti-modal agent domains.", "AI": {"tldr": "The paper proposes using offline-to-online reinforcement learning (RL) to fine-tune vision-language models (VLMs) for agent tasks, addressing their limitations compared to LLMs.", "motivation": "VLMs lag behind LLMs in agent tasks due to strict output syntax requirements and focus on open-ended QA. Supervised fine-tuning (SFT) alone is insufficient.", "method": "Off-policy RL is used to fine-tune VLMs, allowing self-improvement and learning from low-quality datasets while retaining SFT stability.", "result": "The technique is demonstrated with two open-weight VLMs across three multi-modal agent domains.", "conclusion": "RL enhances VLMs for agent tasks, bridging the gap with LLMs and enabling practical applications."}}
{"id": "2505.03674", "pdf": "https://arxiv.org/pdf/2505.03674", "abs": "https://arxiv.org/abs/2505.03674", "authors": ["Yotam Amitai", "Reuth Mirsky", "Ofra Amir"], "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance", "categories": ["cs.AI"], "comment": null, "summary": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.", "AI": {"tldr": "Goal-sharing by AI agents in human-agent teams doesn't significantly improve task performance but enhances perceived collaboration and trust, with no added cognitive load.", "motivation": "To explore whether AI agents sharing inferred human goals improves collaboration and task performance, given the challenges of direct goal communication.", "method": "An experiment comparing three conditions: no recognition (NR), viable goals (VG), and viable goals on-demand (VGod), measuring task performance, satisfaction, cognitive load, and thematic analysis.", "result": "No significant task performance or satisfaction improvements, but goal-sharing supported strategic adaptations and perceived collaboration. Cognitive load remained unchanged.", "conclusion": "Goal-sharing balances trust and perceived collaboration but may not yield objective performance gains, highlighting a nuanced trade-off."}}
{"id": "2505.03481", "pdf": "https://arxiv.org/pdf/2505.03481", "abs": "https://arxiv.org/abs/2505.03481", "authors": ["Maciej Zembrzuski", "Saad Mahamood"], "title": "Sentence Embeddings as an intermediate target in end-to-end summarisation", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, Year: 2019", "summary": "Current neural network-based methods to the problem of document summarisation\nstruggle when applied to datasets containing large inputs. In this paper we\npropose a new approach to the challenge of content-selection when dealing with\nend-to-end summarisation of user reviews of accommodations. We show that by\ncombining an extractive approach with externally pre-trained sentence level\nembeddings in an addition to an abstractive summarisation model we can\noutperform existing methods when this is applied to the task of summarising a\nlarge input dataset. We also prove that predicting sentence level embedding of\na summary increases the quality of an end-to-end system for loosely aligned\nsource to target corpora, than compared to commonly predicting probability\ndistributions of sentence selection.", "AI": {"tldr": "A new hybrid approach combining extractive and abstractive summarization with pre-trained embeddings outperforms existing methods for large input datasets, especially in summarizing user reviews.", "motivation": "Existing neural network-based methods struggle with large input datasets, particularly in document summarization tasks like summarizing user reviews.", "method": "Combines extractive summarization with pre-trained sentence embeddings and an abstractive model, focusing on predicting sentence-level embeddings for summaries.", "result": "Outperforms existing methods for large input datasets and improves summary quality by predicting sentence embeddings instead of probability distributions.", "conclusion": "The hybrid approach enhances summarization quality for loosely aligned corpora, offering a better solution for large-scale input summarization."}}
{"id": "2505.03204", "pdf": "https://arxiv.org/pdf/2505.03204", "abs": "https://arxiv.org/abs/2505.03204", "authors": ["Liu Suxing", "Byungwon Min"], "title": "DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning methods have shown promise in classifying breast cancer\nhistopathology images, but their performance often declines with limited\nannotated data, a critical challenge in medical imaging due to the high cost\nand expertise required for annotations.", "AI": {"tldr": "Deep learning struggles with limited annotated data in breast cancer histopathology image classification.", "motivation": "High cost and expertise for annotations in medical imaging limit data availability.", "method": "Deep learning applied to breast cancer histopathology images.", "result": "Performance declines with limited annotated data.", "conclusion": "Addressing data scarcity is crucial for improving deep learning in medical imaging."}}
{"id": "2505.03194", "pdf": "https://arxiv.org/pdf/2505.03194", "abs": "https://arxiv.org/abs/2505.03194", "authors": ["Yiding Chen", "Yiyi Zhang", "Owen Oertell", "Wen Sun"], "title": "Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models accomplish remarkable success in data generation tasks\nacross various domains. However, the iterative sampling process is\ncomputationally expensive. Consistency models are proposed to learn consistency\nfunctions to map from noise to data directly, which allows one-step fast data\ngeneration and multistep sampling to improve sample quality. In this paper, we\nstudy the convergence of consistency models when the self-consistency property\nholds approximately under the training distribution. Our analysis requires only\nmild data assumption and applies to a family of forward processes. When the\ntarget data distribution has bounded support or has tails that decay\nsufficiently fast, we show that the samples generated by the consistency model\nare close to the target distribution in Wasserstein distance; when the target\ndistribution satisfies some smoothness assumption, we show that with an\nadditional perturbation step for smoothing, the generated samples are close to\nthe target distribution in total variation distance. We provide two case\nstudies with commonly chosen forward processes to demonstrate the benefit of\nmultistep sampling.", "AI": {"tldr": "Consistency models enable fast one-step data generation and multistep sampling, with theoretical guarantees on convergence under mild assumptions.", "motivation": "To address the computational inefficiency of iterative sampling in diffusion models by proposing consistency models for direct noise-to-data mapping.", "method": "Analyzes convergence of consistency models under approximate self-consistency, with mild data assumptions and applicability to various forward processes.", "result": "Samples are close to the target distribution in Wasserstein or total variation distance, depending on data properties. Multistep sampling improves quality.", "conclusion": "Consistency models offer efficient and high-quality data generation, supported by theoretical analysis and case studies."}}
{"id": "2505.03678", "pdf": "https://arxiv.org/pdf/2505.03678", "abs": "https://arxiv.org/abs/2505.03678", "authors": ["Walter Didimo", "Fabrizio Montecchiani", "Tommaso Piselli"], "title": "Graph Drawing for LLMs: An Empirical Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.", "AI": {"tldr": "The paper explores how layout paradigms, drawing aesthetics, and prompting techniques affect LLM performance in graph-related tasks using visual inputs.", "motivation": "To understand how visual inputs (graph drawings) impact LLM performance and identify factors that enhance task accuracy.", "method": "Investigates three research questions through experimental analysis, focusing on layout paradigms, drawing aesthetics, and prompting techniques.", "result": "Optimal layout and human-readable drawings boost performance; effective prompting is critical but challenging.", "conclusion": "Visual input quality and prompting techniques significantly influence LLM performance in graph tasks."}}
{"id": "2505.03531", "pdf": "https://arxiv.org/pdf/2505.03531", "abs": "https://arxiv.org/abs/2505.03531", "authors": ["Haoqi Yang", "Luohe Shi", "Qiwei Li", "Zuchao Li", "Ping Wang", "Bo Du", "Mengjia Shen", "Hai Zhao"], "title": "Faster MoE LLM Inference for Extremely Large Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.", "AI": {"tldr": "Fine-grained MoE models offer efficiency gains under certain service loads, with activated expert reduction improving efficiency and minimal performance loss, while total expert reduction harms performance.", "motivation": "To explore the efficiency dynamics of fine-grained MoE models under varying service loads and the impact of expert reduction on efficiency-performance trade-offs.", "method": "Analyzed fine-grained MoE models, focusing on reducing activated and total experts to evaluate efficiency and performance trade-offs.", "result": "Reducing activated experts improves efficiency with minor performance loss; reducing total experts harms performance with limited efficiency gains. Throughput increased by 10% without performance loss.", "conclusion": "MoE inference optimization has significant potential, with fine-grained models offering notable efficiency improvements under specific conditions."}}
{"id": "2505.03220", "pdf": "https://arxiv.org/pdf/2505.03220", "abs": "https://arxiv.org/abs/2505.03220", "authors": ["Shaheer Mohamed", "Tharindu Fernando", "Sridha Sridharan", "Peyman Moghadam", "Clinton Fookes"], "title": "Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data", "categories": ["cs.CV"], "comment": "Preprint to appear in IEEE IGARSS 2025", "summary": "Hyperspectral images (HSIs) capture rich spectral signatures that reveal\nvital material properties, offering broad applicability across various domains.\nHowever, the scarcity of labeled HSI data limits the full potential of deep\nlearning, especially for transformer-based architectures that require\nlarge-scale training. To address this constraint, we propose Spatial-Frequency\nMasked Image Modeling (SFMIM), a self-supervised pretraining strategy for\nhyperspectral data that utilizes the large portion of unlabeled data. Our\nmethod introduces a novel dual-domain masking mechanism that operates in both\nspatial and frequency domains. The input HSI cube is initially divided into\nnon-overlapping patches along the spatial dimension, with each patch comprising\nthe entire spectrum of its corresponding spatial location. In spatial masking,\nwe randomly mask selected patches and train the model to reconstruct the masked\ninputs using the visible patches. Concurrently, in frequency masking, we remove\nportions of the frequency components of the input spectra and predict the\nmissing frequencies. By learning to reconstruct these masked components, the\ntransformer-based encoder captures higher-order spectral-spatial correlations.\nWe evaluate our approach on three publicly available HSI classification\nbenchmarks and demonstrate that it achieves state-of-the-art performance.\nNotably, our model shows rapid convergence during fine-tuning, highlighting the\nefficiency of our pretraining strategy.", "AI": {"tldr": "The paper proposes SFMIM, a self-supervised pretraining method for hyperspectral images using dual-domain masking (spatial and frequency) to address labeled data scarcity. It achieves state-of-the-art performance on HSI classification benchmarks.", "motivation": "Labeled HSI data is scarce, limiting deep learning potential, especially for transformer-based models requiring large-scale training.", "method": "SFMIM uses spatial and frequency masking: spatial masking hides patches, and frequency masking removes spectral components. The model reconstructs masked inputs to learn spectral-spatial correlations.", "result": "SFMIM achieves state-of-the-art performance on three HSI benchmarks and shows rapid convergence during fine-tuning.", "conclusion": "SFMIM effectively leverages unlabeled HSI data for pretraining, enhancing transformer-based models' performance and efficiency."}}
{"id": "2505.03205", "pdf": "https://arxiv.org/pdf/2505.03205", "abs": "https://arxiv.org/abs/2505.03205", "authors": ["Zhaiming Shen", "Alex Havrilla", "Rongjie Lai", "Alexander Cloninger", "Wenjing Liao"], "title": "Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights", "categories": ["cs.LG", "cs.NA", "math.NA", "math.ST", "stat.TH"], "comment": null, "summary": "Transformers serve as the foundational architecture for large language and\nvideo generation models, such as GPT, BERT, SORA and their successors.\nEmpirical studies have demonstrated that real-world data and learning tasks\nexhibit low-dimensional structures, along with some noise or measurement error.\nThe performance of transformers tends to depend on the intrinsic dimension of\nthe data/tasks, though theoretical understandings remain largely unexplored for\ntransformers. This work establishes a theoretical foundation by analyzing the\nperformance of transformers for regression tasks involving noisy input data on\na manifold. Specifically, the input data are in a tubular neighborhood of a\nmanifold, while the ground truth function depends on the projection of the\nnoisy data onto the manifold. We prove approximation and generalization errors\nwhich crucially depend on the intrinsic dimension of the manifold. Our results\ndemonstrate that transformers can leverage low-complexity structures in\nlearning task even when the input data are perturbed by high-dimensional noise.\nOur novel proof technique constructs representations of basic arithmetic\noperations by transformers, which may hold independent interest.", "AI": {"tldr": "The paper provides a theoretical analysis of transformers' performance in regression tasks with noisy data on a manifold, showing their ability to leverage low-dimensional structures despite high-dimensional noise.", "motivation": "To bridge the gap in theoretical understanding of transformers' performance, especially in tasks with noisy data and low-dimensional structures.", "method": "Analyzes transformers for regression tasks with noisy input data on a manifold, proving approximation and generalization errors based on intrinsic manifold dimension.", "result": "Transformers can effectively learn from low-dimensional structures even with high-dimensional noise, with performance tied to the manifold's intrinsic dimension.", "conclusion": "The study establishes a theoretical foundation for transformers' performance in noisy, low-dimensional settings, with potential broader implications for understanding their capabilities."}}
{"id": "2505.02841", "pdf": "https://arxiv.org/pdf/2505.02841", "abs": "https://arxiv.org/abs/2505.02841", "authors": ["Marco Masera", "Alessandro Leone", "Johannes K\u00f6ster", "Ivan Molineris"], "title": "Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Reproducibility and sustainability present significant challenges in\nbioinformatics software development, where rapidly evolving tools and complex\nworkflows often result in short-lived or difficult-to-adapt pipelines. This\npaper introduces Snakemaker, a tool that leverages generative AI to facilitate\nresearchers build sustainable data analysis pipelines by converting\nunstructured code into well-defined Snakemake workflows. Snakemaker\nnon-invasively tracks the work performed in the terminal by the researcher,\nanalyzes execution patterns, and generates Snakemake workflows that can be\nintegrated into existing pipelines. Snakemaker also supports the transformation\nof monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the\nglobal state of the notebook into discrete, file-based interactions between\nrules. An integrated chat assistant provides users with fine-grained control\nthrough natural language instructions. Snakemaker generates high-quality\nSnakemake workflows by adhering to the best practices, including Conda\nenvironment tracking, generic rule generation and loop unrolling. By lowering\nthe barrier between prototype and production-quality code, Snakemaker addresses\na critical gap in computational reproducibility for bioinformatics research.", "AI": {"tldr": "Snakemaker uses generative AI to convert unstructured code into Snakemake workflows, enhancing reproducibility and sustainability in bioinformatics.", "motivation": "Addressing challenges in bioinformatics software development, such as short-lived pipelines and complex workflows, by improving reproducibility and sustainability.", "method": "Snakemaker tracks terminal work, analyzes execution patterns, and transforms unstructured code or Ipython Notebooks into modular Snakemake workflows. It includes a chat assistant for natural language control.", "result": "Generates high-quality Snakemake workflows adhering to best practices, like Conda environment tracking and modular rule generation.", "conclusion": "Snakemaker bridges the gap between prototype and production-quality code, significantly improving computational reproducibility in bioinformatics."}}
{"id": "2505.03563", "pdf": "https://arxiv.org/pdf/2505.03563", "abs": "https://arxiv.org/abs/2505.03563", "authors": ["Cl\u00e9a Chataigner", "Rebecca Ma", "Prakhar Ganesh", "Afaf Ta\u00efk", "Elliot Creager", "Golnoosh Farnadi"], "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing", "categories": ["cs.CL"], "comment": null, "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.", "AI": {"tldr": "Small wording changes in prompts significantly affect LLM behavior, necessitating robust evaluation methods.", "motivation": "Address concerns about LLM evaluation stability due to prompt wording variations.", "method": "Propose a controlled paraphrasing framework using minimal linguistic transformations to generate natural prompt variations, validated with human annotations and automated checks.", "result": "Subtle prompt modifications cause substantial changes in LLM behavior, especially in stereotype evaluation tasks.", "conclusion": "Robust, paraphrase-aware evaluation protocols are needed for reliable LLM assessments."}}
{"id": "2505.03242", "pdf": "https://arxiv.org/pdf/2505.03242", "abs": "https://arxiv.org/abs/2505.03242", "authors": ["Davide Talon", "Federico Girella", "Ziyue Liu", "Marco Cristani", "Yiming Wang"], "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR25. Project page:\n  https://davidetalon.github.io/fashionact-page/", "summary": "Natural language goes beyond dryly describing visual content. It contains\nrich abstract concepts to express feeling, creativity and properties that\ncannot be directly perceived. Yet, current research in Vision Language Models\n(VLMs) has not shed light on abstract-oriented language. Our research breaks\nnew ground by uncovering its wide presence and under-estimated value, with\nextensive analysis. Particularly, we focus our investigation on the fashion\ndomain, a highly-representative field with abstract expressions. By analyzing\nrecent large-scale multimodal fashion datasets, we find that abstract terms\nhave a dominant presence, rivaling the concrete ones, providing novel\ninformation, and being useful in the retrieval task. However, a critical\nchallenge emerges: current general-purpose or fashion-specific VLMs are\npre-trained with databases that lack sufficient abstract words in their text\ncorpora, thus hindering their ability to effectively represent\nabstract-oriented language. We propose a training-free and model-agnostic\nmethod, Abstract-to-Concrete Translator (ACT), to shift abstract\nrepresentations towards well-represented concrete ones in the VLM latent space,\nusing pre-trained models and existing multimodal databases. On the\ntext-to-image retrieval task, despite being training-free, ACT outperforms the\nfine-tuned VLMs in both same- and cross-dataset settings, exhibiting its\neffectiveness with a strong generalization capability. Moreover, the\nimprovement introduced by ACT is consistent with various VLMs, making it a\nplug-and-play solution.", "AI": {"tldr": "The paper highlights the overlooked importance of abstract language in Vision Language Models (VLMs) and introduces ACT, a training-free method to improve abstract representation in VLMs, showing superior performance in retrieval tasks.", "motivation": "Current VLMs neglect abstract language, which is prevalent and valuable, especially in domains like fashion. The study aims to address this gap.", "method": "Proposes Abstract-to-Concrete Translator (ACT), a training-free, model-agnostic method to align abstract representations with concrete ones in VLMs using existing databases.", "result": "ACT outperforms fine-tuned VLMs in text-to-image retrieval tasks, demonstrating strong generalization and consistency across models.", "conclusion": "ACT effectively bridges the gap in abstract language representation in VLMs, offering a plug-and-play solution without additional training."}}
{"id": "2505.03207", "pdf": "https://arxiv.org/pdf/2505.03207", "abs": "https://arxiv.org/abs/2505.03207", "authors": ["Yutong Xie", "Fuchao Yang", "Yuheng Jia"], "title": "Partial Label Clustering", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Partial label learning (PLL) is a significant weakly supervised learning\nframework, where each training example corresponds to a set of candidate labels\nand only one label is the ground-truth label. For the first time, this paper\ninvestigates the partial label clustering problem, which takes advantage of the\nlimited available partial labels to improve the clustering performance.\nSpecifically, we first construct a weight matrix of examples based on their\nrelationships in the feature space and disambiguate the candidate labels to\nestimate the ground-truth label based on the weight matrix. Then, we construct\na set of must-link and cannot-link constraints based on the disambiguation\nresults. Moreover, we propagate the initial must-link and cannot-link\nconstraints based on an adversarial prior promoted dual-graph learning\napproach. Finally, we integrate weight matrix construction, label\ndisambiguation, and pairwise constraints propagation into a joint model to\nachieve mutual enhancement. We also theoretically prove that a better\ndisambiguated label matrix can help improve clustering performance.\nComprehensive experiments demonstrate our method realizes superior performance\nwhen comparing with state-of-the-art constrained clustering methods, and\noutperforms PLL and semi-supervised PLL methods when only limited samples are\nannotated. The code is publicly available at https://github.com/xyt-ml/PLC.", "AI": {"tldr": "The paper introduces a partial label clustering (PLC) method that leverages partial labels to improve clustering performance by disambiguating labels, constructing constraints, and propagating them via a dual-graph learning approach.", "motivation": "To address the challenge of improving clustering performance using limited partial labels, a common scenario in weakly supervised learning.", "method": "Constructs a weight matrix for examples, disambiguates candidate labels, builds must-link/cannot-link constraints, propagates constraints via dual-graph learning, and integrates these steps into a joint model.", "result": "The method outperforms state-of-the-art constrained clustering and partial label learning methods, especially with limited annotated samples.", "conclusion": "The proposed PLC method effectively enhances clustering by leveraging partial labels and constraint propagation, with theoretical and empirical validation."}}
{"id": "2505.02846", "pdf": "https://arxiv.org/pdf/2505.02846", "abs": "https://arxiv.org/abs/2505.02846", "authors": ["Kim Kaivanto"], "title": "The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?", "categories": ["cs.CY", "cs.AI", "econ.GN", "q-fin.EC"], "comment": "47 pages", "summary": "In policy debates concerning the governance and regulation of Artificial\nIntelligence (AI), both the Precautionary Principle (PP) and the Innovation\nPrinciple (IP) are advocated by their respective interest groups. Do these\nprinciples offer wholly incompatible and contradictory guidance? Does one\nnecessarily negate the other? I argue here that provided attention is\nrestricted to weak-form PP and IP, the answer to both of these questions is\n\"No.\" The essence of these weak formulations is the requirement to fully\naccount for type-I error costs arising from erroneously preventing the\ninnovation's diffusion through society (i.e. mistaken regulatory red-lighting)\nas well as the type-II error costs arising from erroneously allowing the\ninnovation to diffuse through society (i.e. mistaken regulatory\ngreen-lighting). Within the Signal Detection Theory (SDT) model developed here,\nweak-PP red-light (weak-IP green-light) determinations are optimal for\nsufficiently small (large) ratios of expected type-I to type-II error costs.\nFor intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy\nis optimal. Regulatory sandbox instruments allow AI testing and experimentation\nto take place within a structured environment of limited duration and societal\nscale, whereby the expected cost ratio falls within the 'wait-and-monitor'\nrange. Through sandboxing regulators and innovating firms learn more about the\nexpected cost ratio, and what respective adaptations -- of regulation, of\ntechnical solution, of business model, or combination thereof, if any -- are\nneeded to keep the ratio out of the weak-PP red-light zone.", "AI": {"tldr": "The paper argues that weak forms of the Precautionary Principle (PP) and Innovation Principle (IP) are not incompatible. Using Signal Detection Theory, it shows optimal regulatory actions depend on error cost ratios, with sandboxing aiding learning and adaptation.", "motivation": "To reconcile the seemingly conflicting PP and IP in AI governance by focusing on weak forms and error cost trade-offs.", "method": "Uses Signal Detection Theory (SDT) to model regulatory decisions based on type-I and type-II error cost ratios, introducing sandboxing for intermediate cases.", "result": "Weak-PP and weak-IP can coexist; optimal policies vary with error cost ratios. Sandboxing helps regulators and firms learn and adapt.", "conclusion": "Weak PP and IP are compatible, with sandboxing as a practical tool for balancing innovation and precaution in AI regulation."}}
{"id": "2505.03675", "pdf": "https://arxiv.org/pdf/2505.03675", "abs": "https://arxiv.org/abs/2505.03675", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula G Allen-Meares", "Eulalia P Abril", "Olga Garcia-Bedoya", "Carolyn A Dickens", "Andrew D. Boyd"], "title": "Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure", "categories": ["cs.CL"], "comment": null, "summary": "We explore the potential of ChatGPT (3.5-turbo and 4) to generate\nconversations focused on self-care strategies for African-American heart\nfailure patients -- a domain with limited specialized datasets. To simulate\npatient-health educator dialogues, we employed four prompting strategies:\ndomain, African American Vernacular English (AAVE), Social Determinants of\nHealth (SDOH), and SDOH-informed reasoning. Conversations were generated across\nkey self-care domains of food, exercise, and fluid intake, with varying turn\nlengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as\nage, gender, neighborhood, and socioeconomic status. Our findings show that\neffective prompt design is essential. While incorporating SDOH and reasoning\nimproves dialogue quality, ChatGPT still lacks the empathy and engagement\nneeded for meaningful healthcare communication.", "AI": {"tldr": "ChatGPT was used to generate self-care conversations for African-American heart failure patients, testing four prompting strategies. Results highlight the importance of prompt design but note ChatGPT's limitations in empathy and engagement.", "motivation": "To address the lack of specialized datasets for African-American heart failure patients by leveraging ChatGPT for self-care dialogue generation.", "method": "Four prompting strategies (domain, AAVE, SDOH, SDOH-informed reasoning) were applied to generate patient-health educator dialogues across self-care domains (food, exercise, fluid intake), varying turn lengths and SDOH attributes.", "result": "Prompt design is crucial; SDOH and reasoning improved dialogue quality, but ChatGPT lacked empathy and engagement for healthcare communication.", "conclusion": "While ChatGPT shows promise for generating specialized healthcare dialogues, its limitations in empathy and engagement must be addressed for meaningful use."}}
{"id": "2505.03254", "pdf": "https://arxiv.org/pdf/2505.03254", "abs": "https://arxiv.org/abs/2505.03254", "authors": ["Lukas Meiner", "Jens Mehnert", "Alexandru Paul Condurache"], "title": "PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Convolutional neural networks (CNNs) are crucial for computer vision tasks on\nresource-constrained devices. Quantization effectively compresses these models,\nreducing storage size and energy cost. However, in modern depthwise-separable\narchitectures, the computational cost is distributed unevenly across its\ncomponents, with pointwise operations being the most expensive. By applying a\ngeneral quantization scheme to this imbalanced cost distribution, existing\nquantization approaches fail to fully exploit potential efficiency gains. To\nthis end, we introduce PROM, a straightforward approach for quantizing modern\ndepthwise-separable convolutional networks by selectively using two distinct\nbit-widths. Specifically, pointwise convolutions are quantized to ternary\nweights, while the remaining modules use 8-bit weights, which is achieved\nthrough a simple quantization-aware training procedure. Additionally, by\nquantizing activations to 8-bit, our method transforms pointwise convolutions\nwith ternary weights into int8 additions, which enjoy broad support across\nhardware platforms and effectively eliminates the need for expensive\nmultiplications. Applying PROM to MobileNetV2 reduces the model's energy cost\nby more than an order of magnitude (23.9x) and its storage size by 2.7x\ncompared to the float16 baseline while retaining similar classification\nperformance on ImageNet. Our method advances the Pareto frontier for energy\nconsumption vs. top-1 accuracy for quantized convolutional models on ImageNet.\nPROM addresses the challenges of quantizing depthwise-separable convolutional\nnetworks to both ternary and 8-bit weights, offering a simple way to reduce\nenergy cost and storage size.", "AI": {"tldr": "PROM introduces selective quantization for depthwise-separable CNNs, using ternary weights for pointwise convolutions and 8-bit for others, reducing energy and storage costs while maintaining accuracy.", "motivation": "Existing quantization methods fail to address the uneven computational cost in depthwise-separable architectures, missing efficiency gains.", "method": "PROM applies two distinct bit-widths: ternary weights for pointwise convolutions and 8-bit for other modules, with quantization-aware training. Activations are quantized to 8-bit, converting pointwise convolutions into int8 additions.", "result": "PROM reduces MobileNetV2's energy cost by 23.9x and storage size by 2.7x compared to float16, with similar ImageNet accuracy.", "conclusion": "PROM advances the Pareto frontier for energy vs. accuracy in quantized CNNs, offering a simple solution for efficiency gains."}}
{"id": "2505.03209", "pdf": "https://arxiv.org/pdf/2505.03209", "abs": "https://arxiv.org/abs/2505.03209", "authors": ["Borui Wang", "Kathleen McKeown", "Rex Ying"], "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning from expert demonstrations has long remained a\nchallenging research problem, and existing state-of-the-art methods using\nbehavioral cloning plus further RL training often suffer from poor\ngeneralization, low sample efficiency, and poor model interpretability.\nInspired by the strong reasoning abilities of large language models (LLMs), we\npropose a novel strategy-based reinforcement learning framework integrated with\nLLMs called DYnamic STrategy Induction with Llms for reinforcement learning\n(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a\nstrategy-generating LLM to induce textual strategies based on advantage\nestimations and expert demonstrations, and gradually internalizes induced\nstrategies into the RL agent through policy optimization to improve its\nperformance through boosting policy generalization and enhancing sample\nefficiency. It also provides a direct textual channel to observe and interpret\nthe evolution of the policy's underlying strategies during training. We test\nDYSTIL over challenging RL environments from Minigrid and BabyAI, and\nempirically demonstrate that DYSTIL significantly outperforms state-of-the-art\nbaseline methods by 17.75% in average success rate while also enjoying higher\nsample efficiency during the learning process.", "AI": {"tldr": "DYSTIL integrates LLMs with RL to improve generalization, sample efficiency, and interpretability by dynamically generating and internalizing textual strategies.", "motivation": "Existing RL methods from expert demonstrations suffer from poor generalization, low sample efficiency, and lack of interpretability.", "method": "DYSTIL queries an LLM to generate textual strategies based on advantage estimations and expert demonstrations, then internalizes these into the RL agent via policy optimization.", "result": "DYSTIL outperforms state-of-the-art methods by 17.75% in success rate and shows higher sample efficiency.", "conclusion": "DYSTIL effectively addresses limitations of current RL methods by leveraging LLMs for strategy induction and policy improvement."}}
{"id": "2505.02848", "pdf": "https://arxiv.org/pdf/2505.02848", "abs": "https://arxiv.org/abs/2505.02848", "authors": ["Kexin Ding", "Mu Zhou", "Akshay Chaudhari", "Shaoting Zhang", "Dimitris N. Metaxas"], "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "The wide exploration of large language models (LLMs) raises the awareness of\nalignment between healthcare stakeholder preferences and model outputs. This\nalignment becomes a crucial foundation to empower the healthcare workflow\neffectively, safely, and responsibly. Yet the varying behaviors of LLMs may not\nalways match with healthcare stakeholders' knowledge, demands, and values. To\nenable a human-AI alignment, healthcare stakeholders will need to perform\nessential roles in guiding and enhancing the performance of LLMs. Human\nprofessionals must participate in the entire life cycle of adopting LLM in\nhealthcare, including training data curation, model training, and inference. In\nthis review, we discuss the approaches, tools, and applications of alignments\nbetween healthcare stakeholders and LLMs. We demonstrate that LLMs can better\nfollow human values by properly enhancing healthcare knowledge integration,\ntask understanding, and human guidance. We provide outlooks on enhancing the\nalignment between humans and LLMs to build trustworthy real-world healthcare\napplications.", "AI": {"tldr": "The paper discusses aligning large language models (LLMs) with healthcare stakeholders' preferences to improve workflow efficiency, safety, and responsibility. It emphasizes human involvement in LLM adoption and explores methods for better alignment.", "motivation": "The varying behaviors of LLMs may not align with healthcare stakeholders' knowledge, demands, and values, necessitating human-AI alignment for effective, safe, and responsible healthcare applications.", "method": "Human professionals guide LLMs through training data curation, model training, and inference. The paper reviews approaches, tools, and applications for alignment.", "result": "LLMs can better align with human values by integrating healthcare knowledge, improving task understanding, and incorporating human guidance.", "conclusion": "Enhancing human-LLM alignment is crucial for building trustworthy real-world healthcare applications, with ongoing focus on knowledge integration and human involvement."}}
{"id": "2505.03688", "pdf": "https://arxiv.org/pdf/2505.03688", "abs": "https://arxiv.org/abs/2505.03688", "authors": ["Sharvi Endait", "Ruturaj Ghatage", "Aditya Kulkarni", "Rajlaxmi Patil", "Raviraj Joshi"], "title": "IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The rapid progress in question-answering (QA) systems has predominantly\nbenefited high-resource languages, leaving Indic languages largely\nunderrepresented despite their vast native speaker base. In this paper, we\npresent IndicSQuAD, a comprehensive multi-lingual extractive QA dataset\ncovering nine major Indic languages, systematically derived from the SQuAD\ndataset. Building on previous work with MahaSQuAD for Marathi, our approach\nadapts and extends translation techniques to maintain high linguistic fidelity\nand accurate answer-span alignment across diverse languages. IndicSQuAD\ncomprises extensive training, validation, and test sets for each language,\nproviding a robust foundation for model development. We evaluate baseline\nperformances using language-specific monolingual BERT models and the\nmultilingual MuRIL-BERT. The results indicate some challenges inherent in\nlow-resource settings. Moreover, our experiments suggest potential directions\nfor future work, including expanding to additional languages, developing\ndomain-specific datasets, and incorporating multimodal data. The dataset and\nmodels are publicly shared at https://github.com/l3cube-pune/indic-nlp", "AI": {"tldr": "IndicSQuAD is a multilingual QA dataset for nine Indic languages, derived from SQuAD, addressing underrepresentation. It includes training, validation, and test sets, with baseline evaluations using BERT models, highlighting challenges in low-resource settings.", "motivation": "To address the underrepresentation of Indic languages in QA systems despite their large speaker base.", "method": "Adapts and extends translation techniques from SQuAD to create IndicSQuAD, ensuring linguistic fidelity and answer-span alignment. Evaluated using monolingual and multilingual BERT models.", "result": "Baseline results show challenges in low-resource settings, with potential for future work in expanding languages, domain-specific datasets, and multimodal data.", "conclusion": "IndicSQuAD provides a robust QA dataset for Indic languages, encouraging further research and development in this underrepresented area."}}
{"id": "2505.03284", "pdf": "https://arxiv.org/pdf/2505.03284", "abs": "https://arxiv.org/abs/2505.03284", "authors": ["Zhenxing Ming", "Julie Stephany Berrio", "Mao Shan", "Yaoqi Huang", "Hongyu Lyu", "Nguyen Hoang Khoi Tran", "Tzu-Yun Tseng", "Stewart Worrall"], "title": "OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The safe operation of autonomous vehicles (AVs) is highly dependent on their\nunderstanding of the surroundings. For this, the task of 3D semantic occupancy\nprediction divides the space around the sensors into voxels, and labels each\nvoxel with both occupancy and semantic information. Recent perception models\nhave used multisensor fusion to perform this task. However, existing\nmultisensor fusion-based approaches focus mainly on using sensor information in\nthe Cartesian coordinate system. This ignores the distribution of the sensor\nreadings, leading to a loss of fine-grained details and performance\ndegradation. In this paper, we propose OccCylindrical that merges and refines\nthe different modality features under cylindrical coordinates. Our method\npreserves more fine-grained geometry detail that leads to better performance.\nExtensive experiments conducted on the nuScenes dataset, including challenging\nrainy and nighttime scenarios, confirm our approach's effectiveness and\nstate-of-the-art performance. The code will be available at:\nhttps://github.com/DanielMing123/OccCylindrical", "AI": {"tldr": "The paper proposes OccCylindrical, a method for 3D semantic occupancy prediction in autonomous vehicles, using cylindrical coordinates to improve fine-grained detail retention and performance.", "motivation": "Existing multisensor fusion methods for 3D semantic occupancy prediction focus on Cartesian coordinates, ignoring sensor reading distribution and losing fine details.", "method": "OccCylindrical merges and refines multisensor features under cylindrical coordinates to preserve geometry details.", "result": "Extensive experiments on the nuScenes dataset, including challenging conditions, show state-of-the-art performance.", "conclusion": "OccCylindrical effectively improves 3D semantic occupancy prediction by leveraging cylindrical coordinates, outperforming existing methods."}}
{"id": "2505.03230", "pdf": "https://arxiv.org/pdf/2505.03230", "abs": "https://arxiv.org/abs/2505.03230", "authors": ["Yue Chen", "Hui Kang", "Jiahui Li", "Geng Su", "Boxiong Wang", "Jiacheng Wang", "Cong Liang", "Shuang Liang", "Dusit Niyato"], "title": "Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach", "categories": ["cs.LG"], "comment": null, "summary": "The integration of simultaneous wireless information and power transfer\n(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant\nchallenges in remote areas and disaster scenarios where ground infrastructure\nis unavailable. This paper proposes a novel unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system enhanced by directional\nantennas to provide both computational resources and energy support for ground\nIoT terminals. However, such systems require multiple trade-off policies to\nbalance UAV energy consumption, terminal battery levels, and computational\nresource allocation under various constraints, including limited UAV battery\ncapacity, non-linear energy harvesting characteristics, and dynamic task\narrivals. To address these challenges comprehensively, we formulate a\nbi-objective optimization problem that simultaneously considers system energy\nefficiency and terminal battery sustainability. We then reformulate this\nnon-convex problem with a hybrid solution space as a Markov decision process\n(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action\nsimplification mechanism to enhance its convergence and generalization\ncapabilities. Simulation results have demonstrated that our proposed approach\noutperforms various baselines in different scenarios, achieving efficient\nenergy management while maintaining high computational performance.\nFurthermore, our method shows strong generalization ability across different\nscenarios, particularly in complex environments, validating the effectiveness\nof our designed boundary penalty and charging reward mechanisms.", "AI": {"tldr": "The paper proposes a UAV-assisted MEC system with directional antennas for SWIPT in 6G IoT networks, addressing energy and computational trade-offs via a bi-objective optimization solved by an improved SAC algorithm.", "motivation": "Challenges in SWIPT for 6G IoT in remote/disaster areas with no ground infrastructure motivate the need for UAV-assisted MEC systems.", "method": "A bi-objective optimization problem is formulated and solved using an improved SAC algorithm with action simplification.", "result": "Simulations show the method outperforms baselines in energy efficiency and computational performance, with strong generalization.", "conclusion": "The proposed approach effectively balances energy and computational needs, validated by its performance in complex scenarios."}}
{"id": "2505.02849", "pdf": "https://arxiv.org/pdf/2505.02849", "abs": "https://arxiv.org/abs/2505.02849", "authors": ["Mohsen Balavar", "Wenli Yang", "David Herbert", "Soonja Yeom"], "title": "Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Recent advancements in artificial intelligence (AI) and machine learning have\nreignited interest in their impact on Computer-based Learning (CBL). AI-driven\ntools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced\nlearning experiences through personalisation and flexibility. ITSs can adapt to\nindividual learning needs and provide customised feedback based on a student's\nperformance, cognitive state, and learning path. Despite these advances,\nchallenges remain in accommodating diverse learning styles and delivering\nreal-time, context-aware feedback. Our research aims to address these gaps by\nintegrating skill-aligned feedback via Retrieval Augmented Generation (RAG)\ninto prompt engineering for Large Language Models (LLMs) and developing an\napplication to enhance learning through personalised tutoring in a computer\nscience programming context. The pilot study evaluated a proposed system using\nthree quantitative metrics: readability score, response time, and feedback\ndepth, across three programming tasks of varying complexity. The system\nsuccessfully sorted simulated students into three skill-level categories and\nprovided context-aware feedback. This targeted approach demonstrated better\neffectiveness and adaptability compared to general methods.", "AI": {"tldr": "The paper explores AI-driven tools like ChatGPT and ITS for personalized learning, addressing gaps in diverse learning styles and real-time feedback. It integrates RAG into LLMs for skill-aligned feedback, showing effectiveness in a programming context.", "motivation": "To enhance Computer-based Learning by addressing challenges in personalization and real-time feedback using AI-driven tools.", "method": "Integration of Retrieval Augmented Generation (RAG) into Large Language Models (LLMs) for skill-aligned feedback, tested via a pilot study with quantitative metrics.", "result": "The system effectively categorized students by skill level and provided context-aware feedback, outperforming general methods.", "conclusion": "Targeted AI-driven feedback improves learning adaptability and effectiveness in programming contexts."}}
{"id": "2505.03711", "pdf": "https://arxiv.org/pdf/2505.03711", "abs": "https://arxiv.org/abs/2505.03711", "authors": ["Baharul Islam", "Nasim Ahmad", "Ferdous Ahmed Barbhuiya", "Kuntal Dey"], "title": "NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "We present our system submission for SemEval 2025 Task 5, which focuses on\ncross-lingual subject classification in the English and German academic\ndomains. Our approach leverages bilingual data during training, employing\nnegative sampling and a margin-based retrieval objective. We demonstrate that a\ndimension-as-token self-attention mechanism designed with significantly reduced\ninternal dimensions can effectively encode sentence embeddings for subject\nretrieval. In quantitative evaluation, our system achieved an average recall\nrate of 32.24% in the general quantitative setting (all subjects), 43.16% and\n31.53% of the general qualitative evaluation methods with minimal GPU usage,\nhighlighting their competitive performance. Our results demonstrate that our\napproach is effective in capturing relevant subject information under resource\nconstraints, although there is still room for improvement.", "AI": {"tldr": "A system for cross-lingual subject classification in English and German academic domains, using bilingual data, negative sampling, and a margin-based retrieval objective, achieving competitive recall rates with minimal GPU usage.", "motivation": "To address cross-lingual subject classification in academic domains, leveraging bilingual data and efficient methods for resource-constrained settings.", "method": "Uses a dimension-as-token self-attention mechanism with reduced internal dimensions, bilingual training data, negative sampling, and a margin-based retrieval objective.", "result": "Achieved average recall rates of 32.24% (general quantitative), 43.16%, and 31.53% (qualitative), demonstrating competitive performance with minimal GPU usage.", "conclusion": "The approach effectively captures subject information under resource constraints but has room for improvement."}}
{"id": "2505.03286", "pdf": "https://arxiv.org/pdf/2505.03286", "abs": "https://arxiv.org/abs/2505.03286", "authors": ["Zhihao Gong", "Lian Wu", "Yong Xu"], "title": "Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "9 pages, 5 figures, 2025 34th International Joint Conference on\n  Artificial Intelligence (IJCAI 2025)", "summary": "Visible-infrared person re-identification (VIReID) provides a solution for\nReID tasks in 24-hour scenarios; however, significant challenges persist in\nachieving satisfactory performance due to the substantial discrepancies between\nvisible (VIS) and infrared (IR) modalities. Existing methods inadequately\nleverage information from different modalities, primarily focusing on digging\ndistinguishing features from modality-shared information while neglecting\nmodality-specific details. To fully utilize differentiated minutiae, we propose\na Base-Detail Feature Learning Framework (BDLF) that enhances the learning of\nboth base and detail knowledge, thereby capitalizing on both modality-shared\nand modality-specific information. Specifically, the proposed BDLF mines detail\nand base features through a lossless detail feature extraction module and a\ncomplementary base embedding generation mechanism, respectively, supported by a\nnovel correlation restriction method that ensures the features gained by BDLF\nenrich both detail and base knowledge across VIS and IR features. Comprehensive\nexperiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the\neffectiveness of BDLF.", "AI": {"tldr": "The paper proposes a Base-Detail Feature Learning Framework (BDLF) to improve visible-infrared person re-identification by leveraging both modality-shared and modality-specific information.", "motivation": "Current methods for VIReID focus too much on modality-shared features and neglect modality-specific details, leading to suboptimal performance.", "method": "BDLF uses a lossless detail feature extraction module and a complementary base embedding generation mechanism, supported by a correlation restriction method.", "result": "Experiments on SYSU-MM01, RegDB, and LLCM datasets confirm BDLF's effectiveness.", "conclusion": "BDLF successfully enhances feature learning for VIReID by integrating both base and detail knowledge."}}
{"id": "2505.03280", "pdf": "https://arxiv.org/pdf/2505.03280", "abs": "https://arxiv.org/abs/2505.03280", "authors": ["Vansh Kapoor", "Jayakrishnan Nair"], "title": "MDPs with a State Sensing Cost", "categories": ["cs.LG"], "comment": "14 pages", "summary": "In many practical sequential decision-making problems, tracking the state of\nthe environment incurs a sensing/communication/computation cost. In these\nsettings, the agent's interaction with its environment includes the additional\ncomponent of deciding $\\textit{when}$ to sense the state, in a manner that\nbalances the value associated with optimal (state-specific) actions and the\ncost of sensing. We formulate this as an expected discounted cost Markov\nDecision Process (MDP), wherein the agent incurs an additional cost for sensing\nits next state, but has the option to take actions while remaining 'blind' to\nthe system state.\n  We pose this problem as a classical discounted cost MDP with an expanded\n(countably infinite) state space. While computing the optimal policy for this\nMDP is intractable in general, we bound the sub-optimality gap associated with\noptimal policies in a restricted class, where the number of consecutive\nnon-sensing (a.k.a., blind) actions is capped. We also design a computationally\nefficient heuristic algorithm based on policy improvement, which in practice\nperforms close to the optimal policy. Finally, we benchmark against the state\nof the art via a numerical case study.", "AI": {"tldr": "The paper addresses sequential decision-making problems where sensing the environment state incurs costs. It formulates this as an MDP with sensing costs and proposes a heuristic algorithm for efficient policy computation.", "motivation": "To balance the trade-off between optimal actions and sensing costs in sequential decision-making, where sensing is costly.", "method": "Formulates the problem as a discounted cost MDP with an expanded state space. Introduces a heuristic algorithm based on policy improvement and bounds sub-optimality gaps for restricted policies.", "result": "The heuristic algorithm performs close to the optimal policy in practice, as demonstrated by a numerical case study.", "conclusion": "The work provides a practical solution for costly sensing scenarios, with theoretical bounds and empirical validation."}}
{"id": "2505.02853", "pdf": "https://arxiv.org/pdf/2505.02853", "abs": "https://arxiv.org/abs/2505.02853", "authors": ["Francesco Balzan", "Pedro P. Santos", "Maurizio Gabbrielli", "Mahault Albarracin", "Manuel Lopes"], "title": "A Computational Model of Inclusive Pedagogy: From Understanding to Application", "categories": ["cs.CY", "cs.AI"], "comment": "This is a preprint version of a manuscript intended for submission to\n  the International Journal of Artificial Intelligence in Education (IJAIED)", "summary": "Human education transcends mere knowledge transfer, it relies on\nco-adaptation dynamics -- the mutual adjustment of teaching and learning\nstrategies between agents. Despite its centrality, computational models of\nco-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue\nthat this gap impedes Educational Science in testing and scaling contextual\ninsights across diverse settings, and limits the potential of Machine Learning\nsystems, which struggle to emulate and adaptively support human learning\nprocesses. To address this, we present a computational T-SI model that\nintegrates contextual insights on human education into a testable framework. We\nuse the model to evaluate diverse T-SI strategies in a realistic synthetic\nclassroom setting, simulating student groups with unequal access to sensory\ninformation. Results show that strategies incorporating co-adaptation\nprinciples (e.g., bidirectional agency) outperform unilateral approaches (i.e.,\nwhere only the teacher or the student is active), improving the learning\noutcomes for all learning types. Beyond the testing and scaling of\ncontext-dependent educational insights, our model enables hypothesis generation\nin controlled yet adaptable environments. This work bridges non-computational\ntheories of human education with scalable, inclusive AI in Education systems,\nproviding a foundation for equitable technologies that dynamically adapt to\nlearner needs.", "AI": {"tldr": "The paper introduces a computational model for teacher-student co-adaptive interactions (T-SI) to improve learning outcomes and bridge gaps in educational science and AI in Education.", "motivation": "Current computational models of T-SI are underdeveloped, limiting the scalability of educational insights and the adaptability of AI systems to human learning processes.", "method": "The authors develop a computational T-SI model integrating human education insights and test it in a synthetic classroom with diverse student groups.", "result": "Co-adaptive strategies (e.g., bidirectional agency) outperform unilateral approaches, enhancing learning outcomes for all students.", "conclusion": "The model bridges non-computational educational theories with scalable AI, enabling equitable, adaptive learning technologies."}}
{"id": "2505.03733", "pdf": "https://arxiv.org/pdf/2505.03733", "abs": "https://arxiv.org/abs/2505.03733", "authors": ["Zimu Lu", "Yunqiao Yang", "Houxing Ren", "Haotian Hou", "Han Xiao", "Ke Wang", "Weikang Shi", "Aojun Zhou", "Mingjie Zhan", "Hongsheng Li"], "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch", "categories": ["cs.CL"], "comment": null, "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.", "AI": {"tldr": "WebGen-Bench is a benchmark for evaluating LLM-based agents' ability to generate multi-file website codebases, featuring diverse instructions and 647 test cases. The best model achieved 27.8% accuracy, showing the benchmark's difficulty. Training on WebGen-Instruct improved performance to 38.2%.", "motivation": "To measure and improve LLM-based agents' capabilities in generating complex, functional websites from scratch, addressing a gap in existing benchmarks.", "method": "Developed WebGen-Bench with human and GPT-4o-generated instructions, created 647 test cases, and automated testing with a web-navigation agent. Evaluated three code-agent frameworks with various LLMs.", "result": "Best model (Bolt.diy + DeepSeek-R1) achieved 27.8% accuracy. Training on WebGen-Instruct improved Qwen2.5-Coder-32B-Instruct to 38.2%.", "conclusion": "WebGen-Bench is a challenging benchmark, and training on specialized datasets can enhance LLM-based agents' performance in website generation."}}
{"id": "2505.03299", "pdf": "https://arxiv.org/pdf/2505.03299", "abs": "https://arxiv.org/abs/2505.03299", "authors": ["Pierre Adorni", "Minh-Tan Pham", "St\u00e9phane May", "S\u00e9bastien Lef\u00e8vre"], "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the MORSE workshop of CVPR 2025", "summary": "Foundation models constitute a significant advancement in computer vision:\nafter a single, albeit costly, training phase, they can address a wide array of\ntasks. In the field of Earth observation, over 75 remote sensing vision\nfoundation models have been developed in the past four years. However, none has\nconsistently outperformed the others across all available downstream tasks. To\nfacilitate their comparison, we propose a cost-effective method for predicting\na model's performance on multiple downstream tasks without the need for\nfine-tuning on each one. This method is based on what we call \"capabilities\nencoding.\" The utility of this novel approach is twofold: we demonstrate its\npotential to simplify the selection of a foundation model for a given new task,\nand we employ it to offer a fresh perspective on the existing literature,\nsuggesting avenues for future research. Codes are available at\nhttps://github.com/pierreadorni/capabilities-encoding.", "AI": {"tldr": "A method for predicting foundation model performance on downstream tasks without fine-tuning, using 'capabilities encoding,' simplifies model selection and provides insights for future research.", "motivation": "Over 75 remote sensing vision foundation models exist, but none consistently outperforms others across all tasks, necessitating a cost-effective comparison method.", "method": "Proposes 'capabilities encoding' to predict model performance on multiple downstream tasks without fine-tuning each one.", "result": "Demonstrates the method's utility for model selection and offers new perspectives on existing literature.", "conclusion": "The approach simplifies foundation model comparison and suggests directions for future research in Earth observation."}}
{"id": "2505.03281", "pdf": "https://arxiv.org/pdf/2505.03281", "abs": "https://arxiv.org/abs/2505.03281", "authors": ["Zhou Wu", "Junyi An", "Baile Xu", "Furao Shen", "Jian Zhao"], "title": "Physics-inspired Energy Transition Neural Network for Sequence Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, the superior performance of Transformers has made them a more\nrobust and scalable solution for sequence modeling than traditional recurrent\nneural networks (RNNs). However, the effectiveness of Transformer in capturing\nlong-term dependencies is primarily attributed to their comprehensive\npair-modeling process rather than inherent inductive biases toward sequence\nsemantics. In this study, we explore the capabilities of pure RNNs and reassess\ntheir long-term learning mechanisms. Inspired by the physics energy transition\nmodels that track energy changes over time, we propose a effective recurrent\nstructure called the``Physics-inspired Energy Transition Neural Network\"\n(PETNN). We demonstrate that PETNN's memory mechanism effectively stores\ninformation over long-term dependencies. Experimental results indicate that\nPETNN outperforms transformer-based methods across various sequence tasks.\nFurthermore, owing to its recurrent nature, PETNN exhibits significantly lower\ncomplexity. Our study presents an optimal foundational recurrent architecture\nand highlights the potential for developing effective recurrent neural networks\nin fields currently dominated by Transformer.", "AI": {"tldr": "PETNN, a physics-inspired RNN, outperforms Transformers in long-term dependency tasks with lower complexity.", "motivation": "Reassess RNNs' long-term learning capabilities and challenge Transformers' dominance by proposing a more efficient recurrent structure.", "method": "Proposes PETNN, inspired by physics energy transition models, to enhance RNNs' memory mechanism for long-term dependencies.", "result": "PETNN outperforms Transformer-based methods in sequence tasks and has lower complexity.", "conclusion": "PETNN is an effective RNN alternative to Transformers, showcasing potential for RNN development in Transformer-dominated fields."}}
{"id": "2505.02856", "pdf": "https://arxiv.org/pdf/2505.02856", "abs": "https://arxiv.org/abs/2505.02856", "authors": ["Mahir Akgun", "Hadi Hosseini"], "title": "AI Education in a Mirror: Challenges Faced by Academic and Industry Experts", "categories": ["cs.CY", "cs.AI"], "comment": "To appear in AIED 2025", "summary": "As Artificial Intelligence (AI) technologies continue to evolve, the gap\nbetween academic AI education and real-world industry challenges remains an\nimportant area of investigation. This study provides preliminary insights into\nchallenges AI professionals encounter in both academia and industry, based on\nsemi-structured interviews with 14 AI experts - eight from industry and six\nfrom academia. We identify key challenges related to data quality and\navailability, model scalability, practical constraints, user behavior, and\nexplainability. While both groups experience data and model adaptation\ndifficulties, industry professionals more frequently highlight deployment\nconstraints, resource limitations, and external dependencies, whereas academics\nemphasize theoretical adaptation and standardization issues. These exploratory\nfindings suggest that AI curricula could better integrate real-world\ncomplexities, software engineering principles, and interdisciplinary learning,\nwhile recognizing the broader educational goals of building foundational and\nethical reasoning skills.", "AI": {"tldr": "The study explores challenges in AI education and industry, identifying gaps like data quality, scalability, and explainability, with differing emphases between academia and industry.", "motivation": "To bridge the gap between academic AI education and real-world industry challenges by understanding the difficulties faced by professionals.", "method": "Semi-structured interviews with 14 AI experts (8 from industry, 6 from academia).", "result": "Key challenges include data quality, scalability, and explainability; industry focuses on deployment constraints, while academia stresses theoretical adaptation.", "conclusion": "AI curricula should integrate real-world complexities, software engineering, and interdisciplinary learning while fostering foundational and ethical skills."}}
{"id": "2505.03739", "pdf": "https://arxiv.org/pdf/2505.03739", "abs": "https://arxiv.org/abs/2505.03739", "authors": ["Zuwei Long", "Yunhang Shen", "Chaoyou Fu", "Heting Gao", "Lijiang Li", "Peixian Chen", "Mengdan Zhang", "Hang Shao", "Jian Li", "Jinlong Peng", "Haoyu Cao", "Ke Li", "Rongrong Ji", "Xing Sun"], "title": "VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Training and Inference Codes: https://github.com/VITA-MLLM/VITA-Audio", "summary": "With the growing requirement for natural human-computer interaction,\nspeech-based systems receive increasing attention as speech is one of the most\ncommon forms of daily communication. However, the existing speech models still\nexperience high latency when generating the first audio token during streaming,\nwhich poses a significant bottleneck for deployment. To address this issue, we\npropose VITA-Audio, an end-to-end large speech model with fast audio-text token\ngeneration. Specifically, we introduce a lightweight Multiple Cross-modal Token\nPrediction (MCTP) module that efficiently generates multiple audio tokens\nwithin a single model forward pass, which not only accelerates the inference\nbut also significantly reduces the latency for generating the first audio in\nstreaming scenarios. In addition, a four-stage progressive training strategy is\nexplored to achieve model acceleration with minimal loss of speech quality. To\nour knowledge, VITA-Audio is the first multi-modal large language model capable\nof generating audio output during the first forward pass, enabling real-time\nconversational capabilities with minimal latency. VITA-Audio is fully\nreproducible and is trained on open-source data only. Experimental results\ndemonstrate that our model achieves an inference speedup of 3~5x at the 7B\nparameter scale, but also significantly outperforms open-source models of\nsimilar model size on multiple benchmarks for automatic speech recognition\n(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.", "AI": {"tldr": "VITA-Audio is an end-to-end large speech model designed to reduce latency in generating the first audio token during streaming, using a lightweight MCTP module and progressive training.", "motivation": "Existing speech models suffer from high latency in generating the first audio token, hindering real-time interaction.", "method": "Introduces a Multiple Cross-modal Token Prediction (MCTP) module for multi-token generation in one pass and a four-stage progressive training strategy.", "result": "Achieves 3~5x inference speedup and outperforms similar-sized models on ASR, TTS, and SQA benchmarks.", "conclusion": "VITA-Audio enables real-time conversational capabilities with minimal latency, using open-source data."}}
{"id": "2505.03300", "pdf": "https://arxiv.org/pdf/2505.03300", "abs": "https://arxiv.org/abs/2505.03300", "authors": ["Andrew Caunes", "Thierry Chateau", "Vincent Fr\u00e9mont"], "title": "3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to IV2024", "summary": "Semantic segmentation of 3D LiDAR point clouds, essential for autonomous\ndriving and infrastructure management, is best achieved by supervised learning,\nwhich demands extensive annotated datasets and faces the problem of domain\nshifts. We introduce a new 3D semantic segmentation pipeline that leverages\naligned scenes and state-of-the-art 2D segmentation methods, avoiding the need\nfor direct 3D annotation or reliance on additional modalities such as camera\nimages at inference time. Our approach generates 2D views from LiDAR scans\ncolored by sensor intensity and applies 2D semantic segmentation to these views\nusing a camera-domain pretrained model. The segmented 2D outputs are then\nback-projected onto the 3D points, with a simple voting-based estimator that\nmerges the labels associated to each 3D point. Our main contribution is a\nglobal pipeline for 3D semantic segmentation requiring no prior 3D annotation\nand not other modality for inference, which can be used for pseudo-label\ngeneration. We conduct a thorough ablation study and demonstrate the potential\nof the generated pseudo-labels for the Unsupervised Domain Adaptation task.", "AI": {"tldr": "A 3D semantic segmentation pipeline using 2D views and pretrained models avoids direct 3D annotation and additional modalities, enabling pseudo-label generation for domain adaptation.", "motivation": "Supervised 3D segmentation requires extensive annotation and faces domain shifts; this method reduces dependency on 3D labels and other modalities.", "method": "Generate 2D views from LiDAR scans, apply 2D segmentation with a pretrained model, and back-project labels to 3D points using a voting-based estimator.", "result": "The pipeline works without 3D annotation or additional modalities and shows promise for pseudo-label generation in domain adaptation.", "conclusion": "The approach offers a practical solution for 3D segmentation, reducing annotation needs and enabling unsupervised domain adaptation."}}
{"id": "2505.03323", "pdf": "https://arxiv.org/pdf/2505.03323", "abs": "https://arxiv.org/abs/2505.03323", "authors": ["Arthur Corr\u00eaa", "Alexandre Jesus", "Crist\u00f3v\u00e3o Silva", "Samuel Moniz"], "title": "Unraveling the Rainbow: can value-based methods schedule?", "categories": ["cs.LG"], "comment": null, "summary": "Recently, deep reinforcement learning has emerged as a promising approach for\nsolving complex combinatorial optimization problems. Broadly, deep\nreinforcement learning methods fall into two categories: policy-based and\nvalue-based. While value-based approaches have achieved notable success in\ndomains such as the Arcade Learning Environment, the combinatorial optimization\ncommunity has predominantly favored policy-based methods, often overlooking the\npotential of value-based algorithms. In this work, we conduct a comprehensive\nempirical evaluation of value-based algorithms, including the deep q-network\nand several of its advanced extensions, within the context of two complex\ncombinatorial problems: the job-shop and the flexible job-shop scheduling\nproblems, two fundamental challenges with multiple industrial applications. Our\nresults challenge the assumption that policy-based methods are inherently\nsuperior for combinatorial optimization. We show that several value-based\napproaches can match or even outperform the widely adopted proximal policy\noptimization algorithm, suggesting that value-based strategies deserve greater\nattention from the combinatorial optimization community. Our code is openly\navailable at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.", "AI": {"tldr": "Value-based deep reinforcement learning methods, often overlooked in combinatorial optimization, are shown to match or outperform policy-based methods like proximal policy optimization in job-shop scheduling problems.", "motivation": "The combinatorial optimization community has favored policy-based methods, neglecting value-based approaches despite their success in other domains. This work aims to evaluate the potential of value-based algorithms in combinatorial problems.", "method": "A comprehensive empirical evaluation of value-based algorithms (e.g., deep q-network and extensions) was conducted on job-shop and flexible job-shop scheduling problems.", "result": "Value-based methods matched or outperformed proximal policy optimization, challenging the superiority of policy-based approaches in combinatorial optimization.", "conclusion": "Value-based strategies deserve more attention in combinatorial optimization, as they can be competitive or superior to policy-based methods."}}
{"id": "2505.02863", "pdf": "https://arxiv.org/pdf/2505.02863", "abs": "https://arxiv.org/abs/2505.02863", "authors": ["Newnew Deng", "Edward Jiusi Liu", "Xiaoming Zhai"], "title": "Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "The use of generative AI (GAI) among university students is rapidly\nincreasing, yet empirical research on students' GAI use and the factors\ninfluencing it remains limited. To address this gap, we surveyed 363\nundergraduate and graduate students in the United States, examining their GAI\nusage and how it relates to demographic variables and personality traits based\non the Big Five model (i.e., extraversion, agreeableness, conscientiousness,\nand emotional stability, and intellect/imagination). Our findings reveal: (a)\nStudents in higher academic years are more inclined to use GAI and prefer it\nover traditional resources. (b) Non-native English speakers use and adopt GAI\nmore readily than native speakers. (c) Compared to White, Asian students report\nhigher GAI usage, perceive greater academic benefits, and express a stronger\npreference for it. Similarly, Black students report a more positive impact of\nGAI on their academic performance. Personality traits also play a significant\nrole in shaping perceptions and usage of GAI. After controlling demographic\nfactors, we found that personality still significantly predicts GAI use and\nattitudes: (a) Students with higher conscientiousness use GAI less. (b)\nStudents who are higher in agreeableness perceive a less positive impact of GAI\non academic performance and express more ethical concerns about using it for\nacademic work. (c) Students with higher emotional stability report a more\npositive impact of GAI on learning and fewer concerns about its academic use.\n(d) Students with higher extraversion show a stronger preference for GAI over\ntraditional resources. (e) Students with higher intellect/imagination tend to\nprefer traditional resources. These insights highlight the need for\nuniversities to provide personalized guidance to ensure students use GAI\neffectively, ethically, and equitably in their academic pursuits.", "AI": {"tldr": "The study explores GAI usage among US university students, linking it to demographics and Big Five personality traits, revealing varied adoption rates and perceptions.", "motivation": "To address the lack of empirical research on students' GAI use and the factors influencing it.", "method": "Surveyed 363 undergraduate and graduate students, analyzing GAI usage in relation to demographics and Big Five personality traits.", "result": "Higher academic years, non-native English speakers, and certain ethnic groups use GAI more. Personality traits significantly influence GAI use and attitudes.", "conclusion": "Universities should offer personalized guidance to promote ethical and equitable GAI use in academics."}}
{"id": "2505.02931", "pdf": "https://arxiv.org/pdf/2505.02931", "abs": "https://arxiv.org/abs/2505.02931", "authors": ["Fernando Vallecillos Ruiz", "Max Hort", "Leon Moonen"], "title": "The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted for publication in the research track of the 29th\n  International Conference on Evaluation and Assessment in Software Engineering\n  (EASE), 17-20 June 2025, Istanbul, T\\\"urkiye", "summary": "Automatic program repair (APR) aims to reduce the manual efforts required to\nidentify and fix errors in source code. Before the rise of LLM-based agents, a\ncommon strategy was to increase the number of generated patches, sometimes to\nthe thousands, to achieve better repair results on benchmarks. More recently,\nself-iterative capabilities enabled LLMs to refine patches over multiple rounds\nguided by feedback. However, literature often focuses on many iterations and\ndisregards different numbers of outputs.\n  We investigate an APR pipeline that balances these two approaches, the\ngeneration of multiple outputs and multiple rounds of iteration, while imposing\na limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs\n- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR\ntask. We further fine-tune each model on an APR dataset with three sizes (1K,\n30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess\ntheir repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.\n  Our results show that by using only a fraction (<1%) of the fine-tuning\ndataset, we can achieve improvements of up to 78% in the number of plausible\npatches generated, challenging prior studies that reported limited gains using\nFull Fine-Tuning. However, we find that exceeding certain thresholds leads to\ndiminishing outcomes, likely due to overfitting. Moreover, we show that base\nmodels greatly benefit from creating patches in an iterative fashion rather\nthan generating them all at once. In addition, the benefit of iterative\nstrategies becomes more pronounced in complex benchmarks. Even fine-tuned\nmodels, while benefiting less from iterations, still gain advantages,\nparticularly on complex benchmarks. The research underscores the need for\nbalanced APR strategies that combine multi-output generation and iterative\nrefinement.", "AI": {"tldr": "The paper explores balancing multiple patch outputs and iterative refinement in automatic program repair (APR) using LLMs, achieving up to 78% improvement with minimal fine-tuning data.", "motivation": "To reduce manual effort in fixing code errors by optimizing the trade-off between generating multiple patches and iterative refinement, while limiting total patches per bug.", "method": "Uses three instruction-tuned LLMs (DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct) fine-tuned on APR datasets of varying sizes (1K, 30K, 65K) with Full Fine-Tuning and LoRA. Evaluated on HumanEval-Java and Defects4J benchmarks.", "result": "Achieves up to 78% improvement in plausible patches with <1% fine-tuning data, but overfitting occurs beyond certain thresholds. Iterative strategies outperform one-time generation, especially in complex benchmarks.", "conclusion": "Balanced APR strategies combining multi-output generation and iterative refinement are essential, with iterative methods particularly beneficial for complex tasks."}}
{"id": "2505.03303", "pdf": "https://arxiv.org/pdf/2505.03303", "abs": "https://arxiv.org/abs/2505.03303", "authors": ["Tasnim Shahriar"], "title": "Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices", "categories": ["cs.CV", "cs.AI", "68-XX (Primary) 68Txx, 68T07 (Secondary)"], "comment": "22 pages, 10 figures, 4 tables, submitted to Springer - Pattern\n  Recognition and Image Analysis", "summary": "This paper presents a comprehensive evaluation of lightweight deep learning\nmodels for image classification, emphasizing their suitability for deployment\nin resource-constrained environments such as low-memory devices. Five\nstate-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,\nEfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse\ndatasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using\nfour key performance metrics: classification accuracy, inference time,\nfloating-point operations (FLOPs), and model size. Additionally, we investigate\nthe impact of hyperparameter tuning, data augmentation, and training paradigms\nby comparing pretrained models with scratch-trained counterparts, focusing on\nMobileNetV3 Small. Our findings reveal that transfer learning significantly\nenhances model accuracy and computational efficiency, particularly for complex\ndatasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest\naccuracy, while MobileNetV3 offers the best balance between accuracy and\nefficiency, and SqueezeNet excels in inference speed and compactness. This\nstudy highlights critical trade-offs between accuracy and efficiency, offering\nactionable insights for deploying lightweight models in real-world applications\nwhere computational resources are limited. By addressing these challenges, this\nresearch contributes to optimizing deep learning systems for edge computing and\nmobile platforms.", "AI": {"tldr": "The paper evaluates lightweight deep learning models for image classification in resource-constrained environments, comparing five architectures on three datasets. EfficientNetV2 excels in accuracy, MobileNetV3 balances accuracy and efficiency, and SqueezeNet is fastest and most compact. Transfer learning boosts performance.", "motivation": "To address the need for efficient deep learning models in low-resource settings like edge devices, the study benchmarks lightweight architectures to identify optimal trade-offs between accuracy and computational efficiency.", "method": "Five models (MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, ShuffleNetV2) are tested on CIFAR-10, CIFAR-100, and Tiny ImageNet. Metrics include accuracy, inference time, FLOPs, and model size. Hyperparameter tuning, data augmentation, and transfer learning (pretrained vs. scratch-trained) are explored.", "result": "EfficientNetV2 achieves highest accuracy, MobileNetV3 balances accuracy and efficiency, and SqueezeNet is fastest and most compact. Transfer learning improves performance, especially for complex datasets.", "conclusion": "The study provides insights for deploying lightweight models in resource-limited scenarios, emphasizing trade-offs between accuracy and efficiency, and highlights the benefits of transfer learning for edge computing and mobile platforms."}}
{"id": "2505.03335", "pdf": "https://arxiv.org/pdf/2505.03335", "abs": "https://arxiv.org/abs/2505.03335", "authors": ["Andrew Zhao", "Yiran Wu", "Yang Yue", "Tong Wu", "Quentin Xu", "Yang Yue", "Matthieu Lin", "Shenzhi Wang", "Qingyun Wu", "Zilong Zheng", "Gao Huang"], "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in\nenhancing the reasoning capabilities of large language models by learning\ndirectly from outcome-based rewards. Recent RLVR works that operate under the\nzero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training.\nThe scarcity of high-quality, human-produced examples raises concerns about the\nlong-term scalability of relying on human supervision, a challenge already\nevident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by\nhumans may offer limited learning potential for a superintelligent system. To\naddress these concerns, we propose a new RLVR paradigm called Absolute Zero, in\nwhich a single model learns to propose tasks that maximize its own learning\nprogress and improves reasoning by solving them, without relying on any\nexternal data. Under this paradigm, we introduce the Absolute Zero Reasoner\n(AZR), a system that self-evolves its training curriculum and reasoning ability\nby using a code executor to both validate proposed code reasoning tasks and\nverify answers, serving as an unified source of verifiable reward to guide\nopen-ended yet grounded learning. Despite being trained entirely without\nexternal data, AZR achieves overall SOTA performance on coding and mathematical\nreasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate\nthat AZR can be effectively applied across different model scales and is\ncompatible with various model classes.", "AI": {"tldr": "The paper introduces Absolute Zero, a new RLVR paradigm where a model self-generates tasks to maximize learning, achieving SOTA performance without external data.", "motivation": "To address scalability and learning limitations of human supervision in RLVR, especially for superintelligent AI.", "method": "Proposes Absolute Zero Reasoner (AZR), which self-evolves tasks and verifies answers using a code executor, eliminating reliance on external data.", "result": "AZR outperforms zero-setting models using human-curated data, achieving SOTA in coding and math reasoning.", "conclusion": "AZR demonstrates scalable, open-ended learning without human supervision, applicable across model scales and classes."}}
{"id": "2505.02886", "pdf": "https://arxiv.org/pdf/2505.02886", "abs": "https://arxiv.org/abs/2505.02886", "authors": ["David H. Silver"], "title": "Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings", "categories": ["physics.soc-ph", "cs.AI", "62P25 (Applications to social sciences), 91B14 (Social choice) 62P25", "H.5.1; I.2.7"], "comment": "29 pages, includes 5 figures and 18 supplementary visualizations.\n  Submitted as a preprint. Code and data available at github dot com slash\n  silverdavi slash taskmaster-stats", "summary": "Taskmaster is a British television show that combines comedic performance\nwith a formal scoring system. Despite the appearance of structured competition,\nit remains unclear whether scoring dynamics contribute meaningfully to audience\nengagement. We conducted a statistical analysis of 162 episodes across 18\nseries, using fifteen episode-level metrics to quantify rank volatility, point\nspread, lead changes, and winner dominance. None of these metrics showed a\nsignificant association with IMDb ratings, even after controlling for series\neffects. Long-term trends suggest that average points have increased over time,\nwhile volatility has slightly declined and rank spread has remained stable.\nThese patterns indicate an attempt to enhance competitive visibility without\naltering the show's structural equilibrium. We also analyzed contestant rank\ntrajectories and identified five recurring archetypes describing performance\nstyles. These patterns suggest that viewer interest is shaped more by\ncontestant behavior than by game mechanics.", "AI": {"tldr": "Scoring dynamics in Taskmaster don't significantly affect audience engagement, as analyzed through 162 episodes. Contestant behavior matters more.", "motivation": "To determine if the scoring system in Taskmaster meaningfully influences audience engagement.", "method": "Statistical analysis of 162 episodes using 15 metrics (rank volatility, point spread, etc.) and controlling for series effects.", "result": "No significant link between scoring metrics and IMDb ratings. Points increased over time, but volatility declined slightly. Contestant behavior was key.", "conclusion": "Viewer interest is driven more by contestant behavior than game mechanics, with scoring dynamics maintaining structural equilibrium."}}
{"id": "2505.03414", "pdf": "https://arxiv.org/pdf/2505.03414", "abs": "https://arxiv.org/abs/2505.03414", "authors": ["Fangming Cui", "Yonggang Zhang", "Xuan Wang", "Xinmei Tian", "Jun Yu"], "title": "Enhancing Target-unspecific Tasks through a Features Matrix", "categories": ["cs.CV", "cs.CL"], "comment": "ICML 2025", "summary": "Recent developments in prompt learning of large vision-language models have\nsignificantly improved performance in target-specific tasks. However, these\nprompt optimizing methods often struggle to tackle the target-unspecific or\ngeneralizable tasks effectively. It may be attributed to the fact that\noverfitting training causes the model to forget its general knowledge having\nstrong promotion on target-unspecific tasks. To alleviate this issue, we\npropose a novel Features Matrix (FM) regularization approach designed to\nenhance these models on target-unspecific tasks. Our method extracts and\nleverages general knowledge, shaping a Features Matrix (FM). Specifically, the\nFM captures the semantics of diverse inputs from a deep and fine perspective,\npreserving essential general knowledge, which mitigates the risk of\noverfitting. Representative evaluations demonstrate that: 1) the FM is\ncompatible with existing frameworks as a generic and flexible module, and 2)\nthe FM significantly showcases its effectiveness in enhancing target-unspecific\ntasks, achieving state-of-the-art performance.", "AI": {"tldr": "A novel Features Matrix (FM) regularization method is proposed to enhance large vision-language models for target-unspecific tasks by preserving general knowledge and mitigating overfitting.", "motivation": "Existing prompt learning methods for large vision-language models struggle with target-unspecific tasks due to overfitting, which causes loss of general knowledge.", "method": "The proposed method introduces a Features Matrix (FM) that captures diverse input semantics from a deep and fine perspective to preserve general knowledge and prevent overfitting.", "result": "The FM is shown to be compatible with existing frameworks and significantly improves performance on target-unspecific tasks, achieving state-of-the-art results.", "conclusion": "The FM regularization approach effectively addresses overfitting and enhances generalizability in vision-language models for target-unspecific tasks."}}
{"id": "2505.03310", "pdf": "https://arxiv.org/pdf/2505.03310", "abs": "https://arxiv.org/abs/2505.03310", "authors": ["Lei Liu", "Zhenghao Chen", "Dong Xu"], "title": "3D Gaussian Splatting Data Compression with Mixture of Priors", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) data compression is crucial for enabling\nefficient storage and transmission in 3D scene modeling. However, its\ndevelopment remains limited due to inadequate entropy models and suboptimal\nquantization strategies for both lossless and lossy compression scenarios,\nwhere existing methods have yet to 1) fully leverage hyperprior information to\nconstruct robust conditional entropy models, and 2) apply fine-grained,\nelement-wise quantization strategies for improved compression granularity. In\nthis work, we propose a novel Mixture of Priors (MoP) strategy to\nsimultaneously address these two challenges. Specifically, inspired by the\nMixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior\ninformation through multiple lightweight MLPs to generate diverse prior\nfeatures, which are subsequently integrated into the MoP feature via a gating\nmechanism. To enhance lossless compression, the resulting MoP feature is\nutilized as a hyperprior to improve conditional entropy modeling. Meanwhile,\nfor lossy compression, we employ the MoP feature as guidance information in an\nelement-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine\nQuantization (C2FQ) strategy with a predefined quantization step value.\nSpecifically, we expand the quantization step value into a matrix and\nadaptively refine it from coarse to fine granularity, guided by the MoP\nfeature, thereby obtaining a quantization step matrix that facilitates\nelement-wise quantization. Extensive experiments demonstrate that our proposed\n3DGS data compression framework achieves state-of-the-art performance across\nmultiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and\nTank&Temples.", "AI": {"tldr": "The paper proposes a Mixture of Priors (MoP) strategy for 3D Gaussian Splatting (3DGS) data compression, addressing limitations in entropy models and quantization. It achieves state-of-the-art results on multiple benchmarks.", "motivation": "Current 3DGS compression methods lack robust entropy models and fine-grained quantization, limiting efficiency. The work aims to improve both lossless and lossy compression.", "method": "The MoP strategy uses multiple MLPs to generate diverse prior features, integrated via a gating mechanism. For lossless compression, MoP enhances entropy modeling; for lossy, it guides element-wise quantization with a Coarse-to-Fine Quantization (C2FQ) strategy.", "result": "The framework outperforms existing methods on benchmarks like Mip-NeRF360, BungeeNeRF, DeepBlending, and Tank&Temples.", "conclusion": "The MoP approach effectively addresses entropy modeling and quantization challenges, setting new performance standards for 3DGS compression."}}
{"id": "2505.03368", "pdf": "https://arxiv.org/pdf/2505.03368", "abs": "https://arxiv.org/abs/2505.03368", "authors": ["Stef De Sabbata", "Stefano Mizzaro", "Kevin Roitero"], "title": "Geospatial Mechanistic Interpretability of Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated unprecedented capabilities\nacross various natural language processing tasks. Their ability to process and\ngenerate viable text and code has made them ubiquitous in many fields, while\ntheir deployment as knowledge bases and \"reasoning\" tools remains an area of\nongoing research. In geography, a growing body of literature has been focusing\non evaluating LLMs' geographical knowledge and their ability to perform spatial\nreasoning. However, very little is still known about the internal functioning\nof these models, especially about how they process geographical information.\n  In this chapter, we establish a novel framework for the study of geospatial\nmechanistic interpretability - using spatial analysis to reverse engineer how\nLLMs handle geographical information. Our aim is to advance our understanding\nof the internal representations that these complex models generate while\nprocessing geographical information - what one might call \"how LLMs think about\ngeographic information\" if such phrasing was not an undue anthropomorphism.\n  We first outline the use of probing in revealing internal structures within\nLLMs. We then introduce the field of mechanistic interpretability, discussing\nthe superposition hypothesis and the role of sparse autoencoders in\ndisentangling polysemantic internal representations of LLMs into more\ninterpretable, monosemantic features. In our experiments, we use spatial\nautocorrelation to show how features obtained for placenames display spatial\npatterns related to their geographic location and can thus be interpreted\ngeospatially, providing insights into how these models process geographical\ninformation. We conclude by discussing how our framework can help shape the\nstudy and use of foundation models in geography.", "AI": {"tldr": "The paper introduces a framework for studying how LLMs process geographical information using spatial analysis and mechanistic interpretability.", "motivation": "To understand the internal representations of LLMs when handling geographical data, addressing a gap in current research.", "method": "Uses probing and mechanistic interpretability, including spatial autocorrelation, to analyze LLMs' internal structures.", "result": "Features for placenames show spatial patterns, revealing how LLMs process geographic information.", "conclusion": "The framework advances the study of LLMs in geography, offering insights for future research and applications."}}
{"id": "2505.02887", "pdf": "https://arxiv.org/pdf/2505.02887", "abs": "https://arxiv.org/abs/2505.02887", "authors": ["Cheng Ge", "Han-Shen Tae", "Zhenqiang Zhang", "Lu Lu", "Zhijie Huang", "Yilin Wang", "Tao Jiang", "Wenqing Cai", "Shan Chang", "David J. Adams", "Rilei Yu"], "title": "CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization", "categories": ["q-bio.BM", "cs.AI", "cs.LG"], "comment": "16 pages, 6 figures", "summary": "Target-specific peptides, such as conotoxins, exhibit exceptional binding\naffinity and selectivity toward ion channels and receptors. However, their\ntherapeutic potential remains underutilized due to the limited diversity of\nnatural variants and the labor-intensive nature of traditional optimization\nstrategies. Here, we present CreoPep, a deep learning-based conditional\ngenerative framework that integrates masked language modeling with a\nprogressive masking scheme to design high-affinity peptide mutants while\nuncovering novel structural motifs. CreoPep employs an integrative augmentation\npipeline, combining FoldX-based energy screening with temperature-controlled\nmultinomial sampling, to generate structurally and functionally diverse\npeptides that retain key pharmacological properties. We validate this approach\nby designing conotoxin inhibitors targeting the $\\alpha$7 nicotinic\nacetylcholine receptor, achieving submicromolar potency in electrophysiological\nassays. Structural analysis reveals that CreoPep-generated variants engage in\nboth conserved and novel binding modes, including disulfide-deficient forms,\nthus expanding beyond conventional design paradigms. Overall, CreoPep offers a\nrobust and generalizable platform that bridges computational peptide design\nwith experimental validation, accelerating the discovery of next-generation\npeptide therapeutics.", "AI": {"tldr": "CreoPep is a deep learning framework for designing high-affinity peptide mutants, validated by creating potent conotoxin inhibitors for the \u03b17 nicotinic acetylcholine receptor.", "motivation": "The therapeutic potential of target-specific peptides like conotoxins is limited by natural diversity and traditional optimization methods.", "method": "CreoPep combines masked language modeling, progressive masking, FoldX-based energy screening, and temperature-controlled sampling to generate diverse peptides.", "result": "Achieved submicromolar potency in electrophysiological assays, with variants showing conserved and novel binding modes.", "conclusion": "CreoPep provides a generalizable platform for accelerating peptide therapeutic discovery."}}
{"id": "2505.03443", "pdf": "https://arxiv.org/pdf/2505.03443", "abs": "https://arxiv.org/abs/2505.03443", "authors": ["Valerio Bellandi"], "title": "Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": "This paper has been accepted at the 6th International Conference on\n  Recent Trends and Applications in Computer Science. It will appear in the\n  proceedings", "summary": "Centralized and distributed systems are two main approaches to organizing ICT\ninfrastructure, each with its pros and cons. Centralized systems concentrate\nresources in one location, making management easier but creating single points\nof failure. Distributed systems, on the other hand, spread resources across\nmultiple nodes, offering better scalability and fault tolerance, but requiring\nmore complex management. The choice between them depends on factors like\napplication needs, scalability, and data sensitivity. Centralized systems suit\napplications with limited scalability and centralized control, while\ndistributed systems excel in large-scale environments requiring high\navailability and performance. This paper explores a distributed document\nrepository system developed for the Italian Ministry of Justice, using edge\nrepositories to analyze textual data and metadata, enhancing semantic\nexploration capabilities.", "AI": {"tldr": "The paper compares centralized and distributed ICT systems, highlighting their trade-offs, and presents a distributed document repository system for the Italian Ministry of Justice.", "motivation": "To address the need for scalable, fault-tolerant systems in large-scale environments, particularly for document management and semantic exploration.", "method": "Developed a distributed document repository system using edge repositories to analyze textual data and metadata.", "result": "Enhanced semantic exploration capabilities for document management in the Italian Ministry of Justice.", "conclusion": "Distributed systems are advantageous for large-scale, high-availability applications, as demonstrated by the successful implementation of the document repository."}}
{"id": "2505.03318", "pdf": "https://arxiv.org/pdf/2505.03318", "abs": "https://arxiv.org/abs/2505.03318", "authors": ["Yibin Wang", "Zhimin Li", "Yuhang Zang", "Chunyu Wang", "Qinglin Lu", "Cheng Jin", "Jiaqi Wang"], "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "project page: https://codegoat24.github.io/UnifiedReward/think", "summary": "Recent advances in multimodal Reward Models (RMs) have shown significant\npromise in delivering reward signals to align vision models with human\npreferences. However, current RMs are generally restricted to providing direct\nresponses or engaging in shallow reasoning processes with limited depth, often\nleading to inaccurate reward signals. We posit that incorporating explicit long\nchains of thought (CoT) into the reward reasoning process can significantly\nstrengthen their reliability and robustness. Furthermore, we believe that once\nRMs internalize CoT reasoning, their direct response accuracy can also be\nimproved through implicit reasoning capabilities. To this end, this paper\nproposes UnifiedReward-Think, the first unified multimodal CoT-based reward\nmodel, capable of multi-dimensional, step-by-step long-chain reasoning for both\nvisual understanding and generation reward tasks. Specifically, we adopt an\nexploration-driven reinforcement fine-tuning approach to elicit and incentivize\nthe model's latent complex reasoning ability: (1) We first use a small amount\nof image generation preference data to distill the reasoning process of GPT-4o,\nwhich is then used for the model's cold start to learn the format and structure\nof CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge\nand generalization capabilities, we prepare large-scale unified multimodal\npreference data to elicit the model's reasoning process across various vision\ntasks. During this phase, correct reasoning outputs are retained for rejection\nsampling to refine the model (3) while incorrect predicted samples are finally\nused for Group Relative Policy Optimization (GRPO) based reinforcement\nfine-tuning, enabling the model to explore diverse reasoning paths and optimize\nfor correct and robust solutions. Extensive experiments across various vision\nreward tasks demonstrate the superiority of our model.", "AI": {"tldr": "The paper proposes UnifiedReward-Think, a multimodal CoT-based reward model, to improve reward signal reliability by incorporating long-chain reasoning. It uses reinforcement fine-tuning and GPT-4o distillation for robust reasoning across vision tasks.", "motivation": "Current reward models lack depth in reasoning, leading to inaccurate signals. Incorporating explicit long chains of thought (CoT) can enhance reliability and robustness.", "method": "Adopts exploration-driven reinforcement fine-tuning: (1) Distills GPT-4o reasoning for cold start, (2) uses large-scale multimodal data for reasoning elicitation, and (3) refines with GRPO-based fine-tuning.", "result": "Extensive experiments show the model's superiority in vision reward tasks.", "conclusion": "UnifiedReward-Think improves reward accuracy and robustness through CoT reasoning, validated by strong experimental performance."}}
{"id": "2505.03373", "pdf": "https://arxiv.org/pdf/2505.03373", "abs": "https://arxiv.org/abs/2505.03373", "authors": ["Hanyu Hu", "Xiaoming Yuan"], "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The deployment of large language models (LLMs) is often constrained by their\nsubstantial computational and memory demands. While structured pruning presents\na viable approach by eliminating entire network components, existing methods\nsuffer from performance degradation, reliance on heuristic metrics, or\nexpensive finetuning. To address these challenges, we propose SPAP (Structured\nPruning via Alternating Optimization and Penalty Methods), a novel and\nefficient structured pruning framework for LLMs grounded in optimization\ntheory. SPAP formulates the pruning problem through a mixed-integer\noptimization model, employs a penalty method that effectively makes pruning\ndecisions to minimize pruning errors, and introduces an alternating\nminimization algorithm tailored to the splittable problem structure for\nefficient weight updates and performance recovery. Extensive experiments on\nOPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over\nstate-of-the-art methods, delivering linear inference speedups (1.29$\\times$ at\n30% sparsity) and proportional memory reductions. Our work offers a practical,\noptimization-driven solution for pruning LLMs while preserving model\nperformance.", "AI": {"tldr": "SPAP is a novel structured pruning framework for LLMs that uses optimization theory to efficiently prune models while minimizing performance loss.", "motivation": "Existing pruning methods for LLMs suffer from performance degradation, heuristic reliance, or costly finetuning, limiting their practicality.", "method": "SPAP employs a mixed-integer optimization model, a penalty method for pruning decisions, and an alternating minimization algorithm for efficient weight updates.", "result": "SPAP outperforms state-of-the-art methods, achieving linear inference speedups (1.29\u00d7 at 30% sparsity) and proportional memory reductions.", "conclusion": "SPAP provides a practical, optimization-driven solution for pruning LLMs without compromising performance."}}
{"id": "2505.03501", "pdf": "https://arxiv.org/pdf/2505.03501", "abs": "https://arxiv.org/abs/2505.03501", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Wenbo Jiang", "Kangjie Chen", "Tianwei Zhang", "Qingchuan Zhao", "Guowen Xu"], "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness", "AI": {"tldr": "The paper introduces lingual-backdoor attacks on LLMs, where language acts as a trigger to generate harmful speech. It proposes BadLingual, a task-agnostic attack, and validates its effectiveness.", "motivation": "To expose vulnerabilities in multilingual LLMs, enabling targeted attacks on language-speaking groups and highlighting the need for robust defenses.", "method": "Develops a baseline attack via poisoned training data and introduces BadLingual using PGCG-based adversarial training for task-agnostic attacks.", "result": "Baseline achieves 90% ASR on specific tasks but only 37.61% in task-agnostic cases. BadLingual improves ASR by up to 37.35%.", "conclusion": "The study reveals new LLM vulnerabilities and calls for research on defenses to improve robustness."}}
{"id": "2505.03329", "pdf": "https://arxiv.org/pdf/2505.03329", "abs": "https://arxiv.org/abs/2505.03329", "authors": ["Rui Lan", "Yancheng Bai", "Xu Duan", "Mingxing Li", "Lei Sun", "Xiangxiang Chu"], "title": "FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "The task of scene text editing is to modify or add texts on images while\nmaintaining the fidelity of newly generated text and visual coherence with the\nbackground. Recent works based on latent diffusion models (LDM) show improved\ntext editing results, yet still face challenges and often generate inaccurate\nor unrecognizable characters, especially for non-Latin ones (\\eg, Chinese),\nwhich have complex glyph structures. To address these issues, we present\nFLUX-Text, a simple and advanced multilingual scene text editing framework\nbased on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,\nconsidering both visual and textual modalities. To retain the original\ngenerative capabilities of FLUX-Fill while enhancing its understanding and\ngeneration of glyphs, we propose lightweight glyph and text embedding modules.\nOwning to the lightweight design, FLUX-Text is trained only with $100K$\ntraining examples compared to current popular methods trained with 2.9M ones.\nWith no bells and whistles, our method achieves state-of-the-art performance on\ntext editing tasks. Qualitative and quantitative experiments on the public\ndatasets demonstrate that our method surpasses previous works in text fidelity.", "AI": {"tldr": "FLUX-Text is a multilingual scene text editing framework based on FLUX-Fill, addressing challenges in generating accurate non-Latin characters by incorporating glyph conditioning and lightweight modules. It achieves state-of-the-art performance with minimal training data.", "motivation": "Existing methods struggle with generating accurate or recognizable non-Latin characters due to complex glyph structures, prompting the need for a more effective solution.", "method": "FLUX-Text integrates glyph conditioning (visual and textual modalities) and lightweight glyph/text embedding modules into FLUX-Fill, requiring only 100K training examples.", "result": "The framework outperforms previous methods in text fidelity, demonstrated by qualitative and quantitative experiments on public datasets.", "conclusion": "FLUX-Text offers a simple yet advanced solution for multilingual scene text editing, achieving superior performance with fewer resources."}}
{"id": "2505.03382", "pdf": "https://arxiv.org/pdf/2505.03382", "abs": "https://arxiv.org/abs/2505.03382", "authors": ["Matthias H\u00f6fler", "Francesco Regazzoni", "Stefano Pagani", "Elias Karabelas", "Christoph Augustin", "Gundolf Haase", "Gernot Plank", "Federica Caforio"], "title": "Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models", "categories": ["cs.LG", "cs.NA", "math.NA", "G.1.8; I.2.0; I.6.4; J.3"], "comment": null, "summary": "Active stress models in cardiac biomechanics account for the mechanical\ndeformation caused by muscle activity, thus providing a link between the\nelectrophysiological and mechanical properties of the tissue. The accurate\nassessment of active stress parameters is fundamental for a precise\nunderstanding of myocardial function but remains difficult to achieve in a\nclinical setting, especially when only displacement and strain data from\nmedical imaging modalities are available. This work investigates, through an\nin-silico study, the application of physics-informed neural networks (PINNs)\nfor inferring active contractility parameters in time-dependent cardiac\nbiomechanical models from these types of imaging data. In particular, by\nparametrising the sought state and parameter field with two neural networks,\nrespectively, and formulating an energy minimisation problem to search for the\noptimal network parameters, we are able to reconstruct in various settings\nactive stress fields in the presence of noise and with a high spatial\nresolution. To this end, we also advance the vanilla PINN learning algorithm\nwith the use of adaptive weighting schemes, ad-hoc regularisation strategies,\nFourier features, and suitable network architectures. In addition, we\nthoroughly analyse the influence of the loss weights in the reconstruction of\nactive stress parameters. Finally, we apply the method to the characterisation\nof tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.\nThis approach opens a new pathway to significantly improve the diagnosis,\ntreatment planning, and management of heart conditions associated with cardiac\nfibrosis.", "AI": {"tldr": "The paper explores using physics-informed neural networks (PINNs) to infer active stress parameters in cardiac biomechanics from imaging data, improving accuracy in diagnosing heart conditions like fibrosis.", "motivation": "Accurate assessment of active stress parameters in cardiac biomechanics is challenging clinically, especially with limited imaging data. PINNs offer a potential solution.", "method": "The study employs PINNs with adaptive weighting, regularization, Fourier features, and optimized architectures to reconstruct active stress fields from noisy imaging data.", "result": "The method successfully reconstructs active stress fields with high spatial resolution, even in noisy conditions, and aids in detecting tissue inhomogeneities and fibrotic scars.", "conclusion": "This approach enhances the diagnosis and treatment of heart conditions linked to fibrosis, showcasing PINNs' potential in cardiac biomechanics."}}
{"id": "2505.03035", "pdf": "https://arxiv.org/pdf/2505.03035", "abs": "https://arxiv.org/abs/2505.03035", "authors": ["Mohammad Mohammadi", "Daniel Honerkamp", "Martin B\u00fcchner", "Matteo Cassinelli", "Tim Welschehold", "Fabien Despinoy", "Igor Gilitschenski", "Abhinav Valada"], "title": "MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous long-horizon mobile manipulation encompasses a multitude of\nchallenges, including scene dynamics, unexplored areas, and error recovery.\nRecent works have leveraged foundation models for scene-level robotic reasoning\nand planning. However, the performance of these methods degrades when dealing\nwith a large number of objects and large-scale environments. To address these\nlimitations, we propose MORE, a novel approach for enhancing the capabilities\nof language models to solve zero-shot mobile manipulation planning for\nrearrangement tasks. MORE leverages scene graphs to represent environments,\nincorporates instance differentiation, and introduces an active filtering\nscheme that extracts task-relevant subgraphs of object and region instances.\nThese steps yield a bounded planning problem, effectively mitigating\nhallucinations and improving reliability. Additionally, we introduce several\nenhancements that enable planning across both indoor and outdoor environments.\nWe evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K\nbenchmark, where it becomes the first approach to successfully solve a\nsignificant share of the benchmark, outperforming recent foundation model-based\napproaches. Furthermore, we demonstrate the capabilities of our approach in\nseveral complex real-world tasks, mimicking everyday activities. We make the\ncode publicly available at https://more-model.cs.uni-freiburg.de.", "AI": {"tldr": "MORE enhances language models for zero-shot mobile manipulation planning using scene graphs and active filtering, outperforming existing methods on diverse tasks.", "motivation": "Addressing performance degradation in large-scale environments with many objects for mobile manipulation planning.", "method": "Uses scene graphs, instance differentiation, and active filtering to create bounded planning problems.", "result": "Outperforms foundation model-based approaches on the BEHAVIOR-1K benchmark and handles real-world tasks.", "conclusion": "MORE improves reliability and scalability for mobile manipulation planning, with demonstrated success in complex environments."}}
{"id": "2505.03676", "pdf": "https://arxiv.org/pdf/2505.03676", "abs": "https://arxiv.org/abs/2505.03676", "authors": ["Arthur Satouf", "Gabriel Ben Zenou", "Benjamin Piwowarski", "Habiboulaye Amadou Boubacar", "Pablo Piantanida"], "title": "Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval", "categories": ["cs.IR", "cs.CL", "68P20, 68T50", "H.3"], "comment": "6 pages - 2 figures - conference: accepted at SIGIR 2025", "summary": "Current sparse neural information retrieval (IR) methods, and to a lesser\nextent more traditional models such as BM25, do not take into account the\ndocument collection and the complex interplay between different term weights\nwhen representing a single document. In this paper, we show how the Rational\nSpeech Acts (RSA), a linguistics framework used to minimize the number of\nfeatures to be communicated when identifying an object in a set, can be adapted\nto the IR case -- and in particular to the high number of potential features\n(here, tokens). RSA dynamically modulates token-document interactions by\nconsidering the influence of other documents in the dataset, better contrasting\ndocument representations. Experiments show that incorporating RSA consistently\nimproves multiple sparse retrieval models and achieves state-of-the-art\nperformance on out-of-domain datasets from the BEIR benchmark.\nhttps://github.com/arthur-75/Rational-Retrieval-Acts", "AI": {"tldr": "The paper adapts the Rational Speech Acts (RSA) framework to improve sparse neural IR methods by dynamically modulating token-document interactions, achieving state-of-the-art performance on out-of-domain datasets.", "motivation": "Current sparse IR methods and traditional models like BM25 overlook the document collection's influence and term weight interplay, limiting document representation accuracy.", "method": "The RSA framework is adapted to IR, dynamically adjusting token-document interactions by considering dataset-wide document influences.", "result": "Incorporating RSA improves sparse retrieval models and achieves state-of-the-art performance on BEIR benchmark out-of-domain datasets.", "conclusion": "RSA enhances sparse IR by better contrasting document representations, proving effective for out-of-domain retrieval tasks."}}
{"id": "2505.03334", "pdf": "https://arxiv.org/pdf/2505.03334", "abs": "https://arxiv.org/abs/2505.03334", "authors": ["Guoting Wei", "Yu Liu", "Xia Yuan", "Xizhe Xue", "Linlin Guo", "Yifan Yang", "Chunxia Zhao", "Zongwen Bai", "Haokui Zhang", "Rong Xiao"], "title": "From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "In recent years, language-guided open-world aerial object detection has\ngained significant attention due to its better alignment with real-world\napplication needs. However, due to limited datasets, most existing\nlanguage-guided methods primarily focus on vocabulary, which fails to meet the\ndemands of more fine-grained open-world detection. To address this limitation,\nwe propose constructing a large-scale language-guided open-set aerial detection\ndataset, encompassing three levels of language guidance: from words to phrases,\nand ultimately to sentences. Centered around an open-source large\nvision-language model and integrating image-operation-based preprocessing with\nBERT-based postprocessing, we present the OS-W2S Label Engine, an automatic\nannotation pipeline capable of handling diverse scene annotations for aerial\nimages. Using this label engine, we expand existing aerial detection datasets\nwith rich textual annotations and construct a novel benchmark dataset, called\nMulti-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of\ncurrent remote sensing grounding data and enabling effective open-set aerial\ndetection. Specifically, MI-OAD contains 163,023 images and 2 million\nimage-caption pairs, approximately 40 times larger than comparable datasets. We\nalso employ state-of-the-art open-set methods from the natural image domain,\ntrained on our proposed dataset, to validate the model's open-set detection\ncapabilities. For instance, when trained on our dataset, Grounding DINO\nachieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs\nunder zero-shot transfer conditions. Both the dataset and the label engine will\nbe released publicly.", "AI": {"tldr": "The paper introduces a large-scale dataset (MI-OAD) and an annotation pipeline (OS-W2S Label Engine) for language-guided open-world aerial object detection, addressing the lack of fine-grained data.", "motivation": "Existing language-guided methods for aerial detection are limited by vocabulary-focused datasets, failing to support fine-grained open-world detection.", "method": "The OS-W2S Label Engine combines vision-language models and BERT-based postprocessing to automatically annotate aerial images with rich textual descriptions. The MI-OAD dataset is constructed with 163,023 images and 2M captions.", "result": "Grounding DINO trained on MI-OAD improves by 29.5 AP_{50} and 33.7 Recall@10 for zero-shot sentence inputs.", "conclusion": "The dataset and label engine enable effective open-set aerial detection and will be publicly released."}}
{"id": "2505.03387", "pdf": "https://arxiv.org/pdf/2505.03387", "abs": "https://arxiv.org/abs/2505.03387", "authors": ["Diego Perazzolo", "Pietro Fanton", "Ilaria Barison", "Marny Fedrigo", "Annalisa Angelini", "Chiara Castellani", "Enrico Grisan"], "title": "Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation", "categories": ["cs.LG", "I.2.1; I.5.2; I.5.4; I.2.8"], "comment": "Paper accepted at the 47th Annual International Conference IEEE EMBC\n  2025 (Engineering in Medicine and Biology Society), Copenhagen, Denmark", "summary": "Given the increasing complexity of omics datasets, a key challenge is not\nonly improving classification performance but also enhancing the transparency\nand reliability of model decisions. Effective model performance and feature\nselection are fundamental for explainability and reliability. In many cases,\nhigh dimensional omics datasets suffer from limited number of samples due to\nclinical constraints, patient conditions, phenotypes rarity and others\nconditions. Current omics based classification models often suffer from narrow\ninterpretability, making it difficult to discern meaningful insights where\ntrust and reproducibility are critical. This study presents a machine learning\nbased classification framework that integrates feature selection with data\naugmentation techniques to achieve high standard classification accuracy while\nensuring better interpretability. Using the publicly available dataset (E MTAB\n8026), we explore a bootstrap analysis in six binary classification scenarios\nto evaluate the proposed model's behaviour. We show that the proposed pipeline\nyields cross validated perfomance on small dataset that is conserved when the\ntrained classifier is applied to a larger test set. Our findings emphasize the\nfundamental balance between accuracy and feature selection, highlighting the\npositive effect of introducing synthetic data for better generalization, even\nin scenarios with very limited samples availability.", "AI": {"tldr": "A machine learning framework combining feature selection and data augmentation improves classification accuracy and interpretability for small omics datasets.", "motivation": "Address the challenge of limited sample sizes and poor interpretability in omics datasets, ensuring reliable and transparent model decisions.", "method": "Integrates feature selection with data augmentation, tested on a public dataset (E-MTAB 8026) using bootstrap analysis in six binary classification scenarios.", "result": "Achieves consistent cross-validated performance on small datasets, generalizing well to larger test sets.", "conclusion": "Balancing accuracy and feature selection, synthetic data enhances generalization in limited-sample scenarios."}}
{"id": "2505.03077", "pdf": "https://arxiv.org/pdf/2505.03077", "abs": "https://arxiv.org/abs/2505.03077", "authors": ["Donghun Noh", "Deqian Kong", "Minglu Zhao", "Andrew Lizarraga", "Jianwen Xie", "Ying Nian Wu", "Dennis Hong"], "title": "Latent Adaptive Planner for Dynamic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents Latent Adaptive Planner (LAP), a novel approach for\ndynamic nonprehensile manipulation tasks that formulates planning as latent\nspace inference, effectively learned from human demonstration videos. Our\nmethod addresses key challenges in visuomotor policy learning through a\nprincipled variational replanning framework that maintains temporal consistency\nwhile efficiently adapting to environmental changes. LAP employs Bayesian\nupdating in latent space to incrementally refine plans as new observations\nbecome available, striking an optimal balance between computational efficiency\nand real-time adaptability. We bridge the embodiment gap between humans and\nrobots through model-based proportional mapping that regenerates accurate\nkinematic-dynamic joint states and object positions from human demonstrations.\nExperimental evaluations across multiple complex manipulation benchmarks\ndemonstrate that LAP achieves state-of-the-art performance, outperforming\nexisting approaches in success rate, trajectory smoothness, and energy\nefficiency, particularly in dynamic adaptation scenarios. Our approach enables\nrobots to perform complex interactions with human-like adaptability while\nproviding an expandable framework applicable to diverse robotic platforms using\nthe same human demonstration videos.", "AI": {"tldr": "LAP is a novel latent space planning method for dynamic nonprehensile manipulation, leveraging human demonstrations and Bayesian updating for real-time adaptability.", "motivation": "Address challenges in visuomotor policy learning and bridge the embodiment gap between humans and robots for dynamic tasks.", "method": "Uses variational replanning and Bayesian updating in latent space, with model-based proportional mapping from human demonstrations.", "result": "Achieves state-of-the-art performance in success rate, trajectory smoothness, and energy efficiency, especially in dynamic scenarios.", "conclusion": "LAP enables human-like adaptability in robots and is expandable to diverse platforms using the same human demonstrations."}}
{"id": "2401.16646", "pdf": "https://arxiv.org/pdf/2401.16646", "abs": "https://arxiv.org/abs/2401.16646", "authors": ["Jian-Qiao Zhu", "Thomas L. Griffiths"], "title": "Incoherent Probability Judgments in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autoregressive Large Language Models (LLMs) trained for next-word prediction\nhave demonstrated remarkable proficiency at producing coherent text. But are\nthey equally adept at forming coherent probability judgments? We use\nprobabilistic identities and repeated judgments to assess the coherence of\nprobability judgments made by LLMs. Our results show that the judgments\nproduced by these models are often incoherent, displaying human-like systematic\ndeviations from the rules of probability theory. Moreover, when prompted to\njudge the same event, the mean-variance relationship of probability judgments\nproduced by LLMs shows an inverted-U-shaped like that seen in humans. We\npropose that these deviations from rationality can be explained by linking\nautoregressive LLMs to implicit Bayesian inference and drawing parallels with\nthe Bayesian Sampler model of human probability judgments.", "AI": {"tldr": "LLMs produce incoherent probability judgments, mirroring human-like deviations from probability theory, and exhibit an inverted-U-shaped mean-variance relationship.", "motivation": "Assess the coherence of probability judgments made by autoregressive LLMs and compare them to human behavior.", "method": "Use probabilistic identities and repeated judgments to evaluate LLMs' probability coherence.", "result": "LLMs' judgments are often incoherent, resembling human deviations from probability theory, with an inverted-U-shaped mean-variance relationship.", "conclusion": "LLMs' irrationality parallels human judgment, explained by linking them to implicit Bayesian inference and the Bayesian Sampler model."}}
{"id": "2505.03350", "pdf": "https://arxiv.org/pdf/2505.03350", "abs": "https://arxiv.org/abs/2505.03350", "authors": ["Song Jian", "Hu Yuchang", "Wang Hui", "Chen Yen-Wei"], "title": "A Vision-Language Model for Focal Liver Lesion Classification", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "9 pages,4 figures, 4 tables,Innovation in Medicine and Healthcare\n  Proceedings of 13th KES-InMed 2025", "summary": "Accurate classification of focal liver lesions is crucial for diagnosis and\ntreatment in hepatology. However, traditional supervised deep learning models\ndepend on large-scale annotated datasets, which are often limited in medical\nimaging. Recently, Vision-Language models (VLMs) such as Contrastive\nLanguage-Image Pre-training model (CLIP) has been applied to image\nclassifications. Compared to the conventional convolutional neural network\n(CNN), which classifiers image based on visual information only, VLM leverages\nmultimodal learning with text and images, allowing it to learn effectively even\nwith a limited amount of labeled data. Inspired by CLIP, we pro-pose a\nLiver-VLM, a model specifically designed for focal liver lesions (FLLs)\nclassification. First, Liver-VLM incorporates class information into the text\nencoder without introducing additional inference overhead. Second, by\ncalculating the pairwise cosine similarities between image and text embeddings\nand optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively\naligns image features with class-level text features. Experimental results on\nMPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the\nstandard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve\n(AUC). Further analysis shows that using a lightweight ResNet18 backbone\nenhances classification performance, particularly under data-constrained\nconditions.", "AI": {"tldr": "Liver-VLM, a Vision-Language model for focal liver lesion classification, outperforms CLIP and MedCLIP by leveraging multimodal learning and lightweight ResNet18 backbone, especially in data-limited scenarios.", "motivation": "Traditional deep learning models require large annotated datasets, which are scarce in medical imaging. Vision-Language models (VLMs) like CLIP offer a solution by using multimodal learning with limited labeled data.", "method": "Liver-VLM integrates class information into the text encoder, aligns image and text embeddings via cosine similarity, and optimizes with cross-entropy loss. It uses a lightweight ResNet18 backbone.", "result": "Liver-VLM achieves higher accuracy and AUC than CLIP and MedCLIP on the MPCT-FLLs dataset, with ResNet18 improving performance in data-constrained conditions.", "conclusion": "Liver-VLM is effective for focal liver lesion classification, especially with limited labeled data, demonstrating the potential of VLMs in medical imaging."}}
{"id": "2505.03390", "pdf": "https://arxiv.org/pdf/2505.03390", "abs": "https://arxiv.org/abs/2505.03390", "authors": ["Zhengqin Yang", "Di Wu", "Jia Chen", "Xin Luo"], "title": "Concept Factorization via Self-Representation and Adaptive Graph Structure Learning", "categories": ["cs.LG"], "comment": null, "summary": "Concept Factorization (CF) models have attracted widespread attention due to\ntheir excellent performance in data clustering. In recent years, many variant\nmodels based on CF have achieved great success in clustering by taking into\naccount the internal geometric manifold structure of the dataset and using\ngraph regularization techniques. However, their clustering performance depends\ngreatly on the construction of the initial graph structure. In order to enable\nadaptive learning of the graph structure of the data, we propose a Concept\nFactorization Based on Self-Representation and Adaptive Graph Structure\nLearning (CFSRAG) Model. CFSRAG learns the affinity relationship between data\nthrough a self-representation method, and uses the learned affinity matrix to\nimplement dynamic graph regularization constraints, thereby ensuring dynamic\nlearning of the internal geometric structure of the data. Finally, we give the\nCFSRAG update rule and convergence analysis, and conduct comparative\nexperiments on four real datasets. The results show that our model outperforms\nother state-of-the-art models.", "AI": {"tldr": "A new Concept Factorization model (CFSRAG) adaptively learns graph structures for better clustering, outperforming existing methods.", "motivation": "Existing CF models rely heavily on initial graph construction, limiting performance. CFSRAG aims to dynamically learn the graph structure for improved clustering.", "method": "CFSRAG uses self-representation to learn data affinity and applies dynamic graph regularization to adaptively learn the geometric structure.", "result": "Experiments on four datasets show CFSRAG outperforms state-of-the-art models.", "conclusion": "CFSRAG effectively improves clustering by dynamically learning graph structures, demonstrating superior performance."}}
{"id": "2505.03105", "pdf": "https://arxiv.org/pdf/2505.03105", "abs": "https://arxiv.org/abs/2505.03105", "authors": ["Xule Lin"], "title": "Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation", "categories": ["cs.HC", "cs.AI", "cs.CY", "H.5.3; I.2.11; K.4.3; H.1.2; I.2.4"], "comment": "62 pages (31 appendix pages for guidance), 2 figures", "summary": "Scientific knowledge creation is fundamentally transforming as humans and AI\nsystems evolve beyond tool-user relationships into co-evolutionary epistemic\npartnerships. When AlphaFold revolutionized protein structure prediction,\nresearchers described engaging with an epistemic partner that reshaped how they\nconceptualized fundamental relationships. This article introduces Cognitio\nEmergens (CE), a framework addressing critical limitations in existing models\nthat focus on static roles or narrow metrics while failing to capture how\nscientific understanding emerges through recursive human-AI interaction over\ntime. CE integrates three components addressing these limitations: Agency\nConfigurations describing how authority distributes between humans and AI\n(Directed, Contributory, Partnership), with partnerships dynamically\noscillating between configurations rather than following linear progression;\nEpistemic Dimensions capturing six specific capabilities emerging through\ncollaboration across Discovery, Integration, and Projection axes, creating\ndistinctive \"capability signatures\" that guide development; and Partnership\nDynamics identifying forces shaping how these relationships evolve,\nparticularly the risk of epistemic alienation where researchers lose\ninterpretive control over knowledge they formally endorse. Drawing from\nautopoiesis theory, social systems theory, and organizational modularity, CE\nreveals how knowledge co-creation emerges through continuous negotiation of\nroles, values, and organizational structures. By reconceptualizing human-AI\nscientific collaboration as fundamentally co-evolutionary, CE offers a balanced\nperspective that neither uncritically celebrates nor unnecessarily fears AI's\nevolving role, instead providing conceptual tools for cultivating partnerships\nthat maintain meaningful human participation while enabling transformative\nscientific breakthroughs.", "AI": {"tldr": "The paper introduces Cognitio Emergens (CE), a framework for understanding human-AI co-evolution in scientific knowledge creation, addressing limitations of static models by focusing on dynamic interactions, agency, and epistemic dimensions.", "motivation": "To address the limitations of existing models that fail to capture the dynamic, recursive nature of human-AI collaboration in scientific knowledge creation, especially as AI systems like AlphaFold reshape research paradigms.", "method": "CE integrates three components: Agency Configurations (dynamic authority distribution), Epistemic Dimensions (capabilities emerging from collaboration), and Partnership Dynamics (forces shaping evolution, including epistemic alienation).", "result": "CE provides a balanced framework for understanding and cultivating human-AI partnerships, ensuring meaningful human participation while enabling scientific breakthroughs.", "conclusion": "CE redefines human-AI collaboration as co-evolutionary, offering tools to maintain human interpretive control and foster transformative scientific progress."}}
{"id": "2406.14498", "pdf": "https://arxiv.org/pdf/2406.14498", "abs": "https://arxiv.org/abs/2406.14498", "authors": ["Sheikh Asif Imran", "Mohammad Nur Hossain Khan", "Subrata Biswas", "Bashima Islam"], "title": "LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors", "categories": ["cs.CL"], "comment": null, "summary": "Wearables generate rich motion data, yet current systems only classify what\nhappened - failing to support natural questions about why it happened or what\nit means. We introduce LLaSA (Large Language and Sensor Assistant), a compact\n13B model that enables ask-anything, open-ended question answering grounded in\nraw IMU data. LLaSA supports conversational, context-aware reasoning -\nexplaining the causes of sensor-detected behaviors and answering free-form\nquestions in real-world scenarios. It is tuned for scientific accuracy,\ncoherence, and response reliability. To advance this new task of sensor-based\nQA, we release three large-scale datasets: SensorCaps, OpenSQA, and\nTune-OpenSQA. Together, these resources define a new benchmark for\nsensor-language models. LLaSA consistently produces interpretable, causal\nanswers and outperforms commercial LLMs across both public and real-world\nsettings. Our code repository and datasets can be found at\nhttps://github.com/BASHLab/LLaSA.", "AI": {"tldr": "LLaSA is a 13B model for open-ended QA on raw IMU data, supporting causal explanations and outperforming commercial LLMs.", "motivation": "Current systems classify motion data but lack support for natural questions about causes or meanings.", "method": "LLaSA, a compact 13B model, is introduced for ask-anything QA grounded in raw IMU data, with datasets SensorCaps, OpenSQA, and Tune-OpenSQA.", "result": "LLaSA produces interpretable, causal answers and outperforms commercial LLMs.", "conclusion": "LLaSA and the released datasets define a new benchmark for sensor-language models."}}
{"id": "2505.03351", "pdf": "https://arxiv.org/pdf/2505.03351", "abs": "https://arxiv.org/abs/2505.03351", "authors": ["Dongbin Zhang", "Yunfei Liu", "Lijian Lin", "Ye Zhu", "Yang Li", "Minghan Qin", "Yu Li", "Haoqian Wang"], "title": "GUAVA: Generalizable Upper Body 3D Gaussian Avatar", "categories": ["cs.CV"], "comment": "Project page: https://eastbeanzhang.github.io/GUAVA/", "summary": "Reconstructing a high-quality, animatable 3D human avatar with expressive\nfacial and hand motions from a single image has gained significant attention\ndue to its broad application potential. 3D human avatar reconstruction\ntypically requires multi-view or monocular videos and training on individual\nIDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's\nexpressiveness, these methods often focus on body motion but struggle with\nfacial expressions. To address these challenges, we first introduce an\nexpressive human model (EHM) to enhance facial expression capabilities and\ndevelop an accurate tracking method. Based on this template model, we propose\nGUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar\nreconstruction. We leverage inverse texture mapping and projection sampling\ntechniques to infer Ubody (upper-body) Gaussians from a single image. The\nrendered images are refined through a neural refiner. Experimental results\ndemonstrate that GUAVA significantly outperforms previous methods in rendering\nquality and offers significant speed improvements, with reconstruction times in\nthe sub-second range (0.1s), and supports real-time animation and rendering.", "AI": {"tldr": "GUAVA is a framework for fast animatable 3D Gaussian avatar reconstruction from a single image, improving facial expressions and speed.", "motivation": "Existing methods for 3D human avatar reconstruction are complex, time-consuming, and limited in facial expressiveness.", "method": "Introduces an expressive human model (EHM) and uses inverse texture mapping and projection sampling to infer upper-body Gaussians from a single image, refined by a neural refiner.", "result": "GUAVA outperforms previous methods in rendering quality and speed, achieving sub-second reconstruction (0.1s) and real-time animation.", "conclusion": "GUAVA offers a fast, high-quality solution for animatable 3D avatar reconstruction with enhanced facial expressions."}}
{"id": "2505.03392", "pdf": "https://arxiv.org/pdf/2505.03392", "abs": "https://arxiv.org/abs/2505.03392", "authors": ["Saleh Zare Zade", "Yao Qiang", "Xiangyu Zhou", "Hui Zhu", "Mohammad Amin Roshani", "Prashant Khanduri", "Dongxiao Zhu"], "title": "Automatic Calibration for Membership Inference Attack on Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Membership Inference Attacks (MIAs) have recently been employed to determine\nwhether a specific text was part of the pre-training data of Large Language\nModels (LLMs). However, existing methods often misinfer non-members as members,\nleading to a high false positive rate, or depend on additional reference models\nfor probability calibration, which limits their practicality. To overcome these\nchallenges, we introduce a novel framework called Automatic Calibration\nMembership Inference Attack (ACMIA), which utilizes a tunable temperature to\ncalibrate output probabilities effectively. This approach is inspired by our\ntheoretical insights into maximum likelihood estimation during the pre-training\nof LLMs. We introduce ACMIA in three configurations designed to accommodate\ndifferent levels of model access and increase the probability gap between\nmembers and non-members, improving the reliability and robustness of membership\ninference. Extensive experiments on various open-source LLMs demonstrate that\nour proposed attack is highly effective, robust, and generalizable, surpassing\nstate-of-the-art baselines across three widely used benchmarks. Our code is\navailable at:\n\\href{https://github.com/Salehzz/ACMIA}{\\textcolor{blue}{Github}}.", "AI": {"tldr": "The paper introduces ACMIA, a novel framework for membership inference attacks on LLMs, addressing high false positives and dependency on reference models by using tunable temperature for probability calibration.", "motivation": "Existing membership inference methods for LLMs suffer from high false positives or rely on impractical reference models, necessitating a more reliable and robust solution.", "method": "ACMIA employs a tunable temperature to calibrate output probabilities, leveraging theoretical insights into LLM pre-training. It offers three configurations for varying model access levels.", "result": "ACMIA outperforms state-of-the-art baselines on three benchmarks, proving highly effective, robust, and generalizable.", "conclusion": "ACMIA provides a practical and superior solution for membership inference attacks on LLMs, with demonstrated effectiveness and robustness."}}
{"id": "2505.03156", "pdf": "https://arxiv.org/pdf/2505.03156", "abs": "https://arxiv.org/abs/2505.03156", "authors": ["Claudio Mayrink Verdun", "Alex Oesterling", "Himabindu Lakkaraju", "Flavio P. Calmon"], "title": "Soft Best-of-n Sampling for Model Alignment", "categories": ["cs.IT", "cs.AI", "math.IT"], "comment": "Accepted for presentation at the 2025 IEEE International Symposium on\n  Information Theory (ISIT 2025)", "summary": "Best-of-$n$ (BoN) sampling is a practical approach for aligning language\nmodel outputs with human preferences without expensive fine-tuning. BoN\nsampling is performed by generating $n$ responses to a prompt and then\nselecting the sample that maximizes a reward function. BoN yields high reward\nvalues in practice at a distortion cost, as measured by the KL-divergence\nbetween the sampled and original distribution. This distortion is coarsely\ncontrolled by varying the number of samples: larger $n$ yields a higher reward\nat a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a\ngeneralization of BoN that allows for smooth interpolation between the original\ndistribution and reward-maximizing distribution through a temperature parameter\n$\\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$\nsampling converges sharply to the optimal tilted distribution at a rate of\n$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete\noutputs, we analyze an additive reward model that reveals the fundamental\nlimitations of blockwise sampling.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2408.01122", "pdf": "https://arxiv.org/pdf/2408.01122", "abs": "https://arxiv.org/abs/2408.01122", "authors": ["Tao Zhang", "Chenglin Zhu", "Yanjun Shen", "Wenjing Luo", "Yan Zhang", "Hao Liang", "Tao Zhang", "Fan Yang", "Mingan Lin", "Yujing Qiao", "Weipeng Chen", "Bin Cui", "Wentao Zhang", "Zenan Zhou"], "title": "CFBench: A Comprehensive Constraints-Following Benchmark for LLMs", "categories": ["cs.CL"], "comment": "15 pages, 10 figures", "summary": "The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench", "AI": {"tldr": "CFBench is a benchmark for evaluating LLMs' ability to follow comprehensive and authentic constraints in real-world scenarios, featuring 1,000 samples across 200+ scenarios and 50+ NLP tasks.", "motivation": "Existing evaluations lack comprehensiveness and authenticity from the user's perspective, necessitating a more robust benchmark.", "method": "CFBench curates real-world constraints, categorizes them systematically, and integrates multi-dimensional assessment criteria with prioritization.", "result": "Evaluation shows significant gaps in LLMs' constraint-following abilities, with insights into influencing factors and improvement strategies.", "conclusion": "CFBench highlights the need for better constraint adherence in LLMs and provides a framework for future enhancements."}}
{"id": "2505.03361", "pdf": "https://arxiv.org/pdf/2505.03361", "abs": "https://arxiv.org/abs/2505.03361", "authors": ["Zihan Ye", "Shreyank N Gowda", "Shiming Chen", "Yaochu Jin", "Kaizhu Huang", "Xiaobo Jin"], "title": "Interpretable Zero-shot Learning with Infinite Class Concepts", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images\nwith intermediate class semantics, like human-annotated concepts or class\ndefinitions. An emerging alternative leverages Large-scale Language Models\n(LLMs) to automatically generate class documents. However, these methods often\nface challenges with transparency in the classification process and may suffer\nfrom the notorious hallucination problem in LLMs, resulting in non-visual class\nsemantics. This paper redefines class semantics in ZSL with a focus on\ntransferability and discriminability, introducing a novel framework called\nZero-shot Learning with Infinite Class Concepts (InfZSL). Our approach\nleverages the powerful capabilities of LLMs to dynamically generate an\nunlimited array of phrase-level class concepts. To address the hallucination\nchallenge, we introduce an entropy-based scoring process that incorporates a\n``goodness\" concept selection mechanism, ensuring that only the most\ntransferable and discriminative concepts are selected. Our InfZSL framework not\nonly demonstrates significant improvements on three popular benchmark datasets\nbut also generates highly interpretable, image-grounded concepts. Code will be\nreleased upon acceptance.", "AI": {"tldr": "InfZSL improves zero-shot learning by dynamically generating class concepts using LLMs, addressing hallucination with an entropy-based selection mechanism.", "motivation": "Existing ZSL methods lack transparency and suffer from LLM hallucination, leading to non-visual semantics.", "method": "InfZSL uses LLMs to generate phrase-level class concepts and employs an entropy-based scoring process for concept selection.", "result": "InfZSL outperforms benchmarks on three datasets and produces interpretable, image-grounded concepts.", "conclusion": "The framework enhances ZSL by ensuring transferable and discriminative class semantics."}}
{"id": "2505.03393", "pdf": "https://arxiv.org/pdf/2505.03393", "abs": "https://arxiv.org/abs/2505.03393", "authors": ["Lena Stempfle", "Anton Matsson", "Newton Mwai", "Fredrik D. Johansson"], "title": "Prediction Models That Learn to Avoid Missing Values", "categories": ["cs.LG"], "comment": null, "summary": "Handling missing values at test time is challenging for machine learning\nmodels, especially when aiming for both high accuracy and interpretability.\nEstablished approaches often add bias through imputation or excessive model\ncomplexity via missingness indicators. Moreover, either method can obscure\ninterpretability, making it harder to understand how the model utilizes the\nobserved variables in predictions. We propose missingness-avoiding (MA) machine\nlearning, a general framework for training models to rarely require the values\nof missing (or imputed) features at test time. We create tailored MA learning\nalgorithms for decision trees, tree ensembles, and sparse linear models by\nincorporating classifier-specific regularization terms in their learning\nobjectives. The tree-based models leverage contextual missingness by reducing\nreliance on missing values based on the observed context. Experiments on\nreal-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT\neffectively reduce the reliance on features with missing values while\nmaintaining predictive performance competitive with their unregularized\ncounterparts. This shows that our framework gives practitioners a powerful tool\nto maintain interpretability in predictions with test-time missing values.", "AI": {"tldr": "MA learning reduces reliance on missing values in ML models while maintaining accuracy and interpretability.", "motivation": "Handling missing values at test time without adding bias or complexity, while preserving interpretability.", "method": "Proposes missingness-avoiding (MA) framework with tailored algorithms for decision trees, tree ensembles, and sparse linear models, using classifier-specific regularization.", "result": "MA models (MA-DT, MA-LASSO, MA-RF, MA-GBT) reduce reliance on missing features while maintaining competitive performance.", "conclusion": "MA framework provides a practical tool for interpretable predictions with missing values."}}
{"id": "2505.03196", "pdf": "https://arxiv.org/pdf/2505.03196", "abs": "https://arxiv.org/abs/2505.03196", "authors": ["Haoxiang Luo", "Gang Sun", "Yinqiu Liu", "Dusit Niyato", "Hongfang Yu", "Mohammed Atiquzzaman", "Schahram Dustdar"], "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong potential across a variety of\ntasks in communications and networking due to their advanced reasoning\ncapabilities. However, because different LLMs have different model structures\nand are trained using distinct corpora and methods, they may offer varying\noptimization strategies for the same network issues. Moreover, the limitations\nof an individual LLM's training data, aggravated by the potential maliciousness\nof its hosting device, can result in responses with low confidence or even\nbias. To address these challenges, we propose a blockchain-enabled\ncollaborative framework that connects multiple LLMs into a Trustworthy\nMulti-LLM Network (MultiLLMN). This architecture enables the cooperative\nevaluation and selection of the most reliable and high-quality responses to\ncomplex network optimization problems. Specifically, we begin by reviewing\nrelated work and highlighting the limitations of existing LLMs in collaboration\nand trust, emphasizing the need for trustworthiness in LLM-based systems. We\nthen introduce the workflow and design of the proposed Trustworthy MultiLLMN\nframework. Given the severity of False Base Station (FBS) attacks in B5G and 6G\ncommunication systems and the difficulty of addressing such threats through\ntraditional modeling techniques, we present FBS defense as a case study to\nempirically validate the effectiveness of our approach. Finally, we outline\npromising future research directions in this emerging area.", "AI": {"tldr": "A blockchain-enabled framework (MultiLLMN) connects multiple LLMs to collaboratively solve network issues, ensuring trustworthy and high-quality responses, validated via FBS defense in B5G/6G systems.", "motivation": "Addressing variability and trust issues in LLMs for network optimization due to differing structures, training data, and potential biases.", "method": "Proposes a blockchain-based collaborative framework (MultiLLMN) for cooperative evaluation and selection of reliable LLM responses.", "result": "Validated effectiveness using FBS defense in B5G/6G systems, demonstrating improved reliability and trustworthiness.", "conclusion": "The MultiLLMN framework enhances LLM collaboration for network optimization, with potential for future research in trustworthy AI systems."}}
{"id": "2408.14307", "pdf": "https://arxiv.org/pdf/2408.14307", "abs": "https://arxiv.org/abs/2408.14307", "authors": ["Yayati Jadhav", "Peter Pak", "Amir Barati Farimani"], "title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.", "AI": {"tldr": "A framework using LLMs for automated defect detection and correction in FDM 3D printing, outperforming human experts.", "motivation": "Addressing the limitations of current error detection methods in FDM, which lack generalizability and require extensive labeled data.", "method": "Leverages pre-trained LLMs to analyze print images, identify defects, query printer parameters, and execute corrective actions.", "result": "LLM-based agents accurately detect and autonomously correct common printing errors, outperforming human engineers.", "conclusion": "The framework enhances scalability and adaptability in 3D printing, reducing reliance on human intervention."}}
{"id": "2505.03362", "pdf": "https://arxiv.org/pdf/2505.03362", "abs": "https://arxiv.org/abs/2505.03362", "authors": ["Shikun Zhang", "Yiqun Wang", "Cunjian Chen", "Yong Li", "Qiuhong Ke"], "title": "3D Surface Reconstruction with Enhanced High-Frequency Details", "categories": ["cs.CV"], "comment": "Accepted by Journal of Visual Communication and Image Representation", "summary": "Neural implicit 3D reconstruction can reproduce shapes without 3D\nsupervision, and it learns the 3D scene through volume rendering methods and\nneural implicit representations. Current neural surface reconstruction methods\ntend to randomly sample the entire image, making it difficult to learn\nhigh-frequency details on the surface, and thus the reconstruction results tend\nto be too smooth. We designed a method (FreNeuS) based on high-frequency\ninformation to solve the problem of insufficient surface detail. Specifically,\nFreNeuS uses pixel gradient changes to easily acquire high-frequency regions in\nan image and uses the obtained high-frequency information to guide surface\ndetail reconstruction. High-frequency information is first used to guide the\ndynamic sampling of rays, applying different sampling strategies according to\nvariations in high-frequency regions. To further enhance the focus on surface\ndetails, we have designed a high-frequency weighting method that constrains the\nrepresentation of high-frequency details during the reconstruction process.\nQualitative and quantitative results show that our method can reconstruct fine\nsurface details and obtain better surface reconstruction quality compared to\nexisting methods. In addition, our method is more applicable and can be\ngeneralized to any NeuS-based work.", "AI": {"tldr": "FreNeuS improves neural implicit 3D reconstruction by focusing on high-frequency details using pixel gradient changes and dynamic sampling, achieving better surface quality.", "motivation": "Current methods struggle with high-frequency detail reconstruction due to random sampling, leading to overly smooth results.", "method": "FreNeuS uses pixel gradient changes to identify high-frequency regions, guides dynamic ray sampling, and applies high-frequency weighting to enhance detail reconstruction.", "result": "The method reconstructs fine surface details better than existing techniques and is applicable to NeuS-based works.", "conclusion": "FreNeuS effectively addresses the lack of surface detail in neural implicit 3D reconstruction, offering improved quality and generalizability."}}
{"id": "2505.03418", "pdf": "https://arxiv.org/pdf/2505.03418", "abs": "https://arxiv.org/abs/2505.03418", "authors": ["Da Zheng", "Lun Du", "Junwei Su", "Yuchen Tian", "Yuqi Zhu", "Jintian Zhang", "Lanning Wei", "Ningyu Zhang", "Huajun Chen"], "title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey", "categories": ["cs.LG"], "comment": null, "summary": "Problem-solving has been a fundamental driver of human progress in numerous\ndomains. With advancements in artificial intelligence, Large Language Models\n(LLMs) have emerged as powerful tools capable of tackling complex problems\nacross diverse domains. Unlike traditional computational systems, LLMs combine\nraw computational power with an approximation of human reasoning, allowing them\nto generate solutions, make inferences, and even leverage external\ncomputational tools. However, applying LLMs to real-world problem-solving\npresents significant challenges, including multi-step reasoning, domain\nknowledge integration, and result verification. This survey explores the\ncapabilities and limitations of LLMs in complex problem-solving, examining\ntechniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,\nand various LLM-based and tool-based verification techniques. Additionally, we\nhighlight domain-specific challenges in various domains, such as software\nengineering, mathematical reasoning and proving, data analysis and modeling,\nand scientific research. The paper further discusses the fundamental\nlimitations of the current LLM solutions and the future directions of LLM-based\ncomplex problems solving from the perspective of multi-step reasoning, domain\nknowledge integration and result verification.", "AI": {"tldr": "The paper surveys LLMs' capabilities and limitations in complex problem-solving, covering techniques like CoT reasoning, knowledge augmentation, and verification, while addressing domain-specific challenges and future directions.", "motivation": "To explore how LLMs can address real-world problem-solving challenges, including multi-step reasoning, domain knowledge integration, and result verification.", "method": "Examines techniques like Chain-of-Thought reasoning, knowledge augmentation, and verification methods, alongside domain-specific applications.", "result": "Identifies LLMs' strengths in problem-solving but highlights challenges like reasoning, knowledge integration, and verification.", "conclusion": "Future LLM-based solutions must improve multi-step reasoning, domain knowledge integration, and result verification to advance complex problem-solving."}}
{"id": "2505.03214", "pdf": "https://arxiv.org/pdf/2505.03214", "abs": "https://arxiv.org/abs/2505.03214", "authors": ["Qiang Sun", "Sirui Li", "Tingting Bi", "Du Huynh", "Mark Reynolds", "Yuanyi Luo", "Wei Liu"], "title": "DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Acquiring structured data from domain-specific, image-based documents such as\nscanned reports is crucial for many downstream tasks but remains challenging\ndue to document variability. Many of these documents exist as images rather\nthan as machine-readable text, which requires human annotation to train\nautomated extraction systems. We present DocSpiral, the first\nHuman-in-the-Spiral assistive document annotation platform, designed to address\nthe challenge of extracting structured information from domain-specific,\nimage-based document collections. Our spiral design establishes an iterative\ncycle in which human annotations train models that progressively require less\nmanual intervention. DocSpiral integrates document format normalization,\ncomprehensive annotation interfaces, evaluation metrics dashboard, and API\nendpoints for the development of AI / ML models into a unified workflow.\nExperiments demonstrate that our framework reduces annotation time by at least\n41\\% while showing consistent performance gains across three iterations during\nmodel training. By making this annotation platform freely accessible, we aim to\nlower barriers to AI/ML models development in document processing, facilitating\nthe adoption of large language models in image-based, document-intensive fields\nsuch as geoscience and healthcare. The system is freely available at:\nhttps://app.ai4wa.com. The demonstration video is available:\nhttps://app.ai4wa.com/docs/docspiral/demo.", "AI": {"tldr": "DocSpiral is a Human-in-the-Spiral platform for annotating image-based documents, reducing annotation time by 41% and improving model performance iteratively.", "motivation": "Extracting structured data from image-based documents is challenging due to variability and lack of machine-readable text, requiring human annotation.", "method": "DocSpiral integrates document normalization, annotation interfaces, evaluation dashboards, and APIs into a unified workflow, enabling iterative model training.", "result": "The framework reduces annotation time by 41% and shows consistent performance gains across three training iterations.", "conclusion": "DocSpiral lowers barriers to AI/ML model development in document processing, benefiting fields like geoscience and healthcare."}}
{"id": "2410.00153", "pdf": "https://arxiv.org/pdf/2410.00153", "abs": "https://arxiv.org/abs/2410.00153", "authors": ["Haiyan Zhao", "Heng Zhao", "Bo Shen", "Ali Payani", "Fan Yang", "Mengnan Du"], "title": "Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICLR 2025", "summary": "Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.", "AI": {"tldr": "The paper introduces Gaussian Concept Subspace (GCS) to robustly represent concepts in LLMs, improving over single-vector methods by using subspaces. It demonstrates GCS's effectiveness in faithfulness, plausibility, and real-world tasks like emotion steering.", "motivation": "Single-vector representations of concepts in LLMs are inconsistent and less robust, limiting their practical utility.", "method": "Extends linear probing classifiers to approximate concept subspaces (GCS) and evaluates it on faithfulness, plausibility, and real-world tasks.", "result": "GCS outperforms single-vector methods, balancing steering performance and fluency in language generation.", "conclusion": "GCS provides a more robust and effective way to represent concepts in LLMs, with practical applications like emotion steering."}}
{"id": "2505.03374", "pdf": "https://arxiv.org/pdf/2505.03374", "abs": "https://arxiv.org/abs/2505.03374", "authors": ["Abram Schonfeldt", "Benjamin Maylor", "Xiaofang Chen", "Ronald Clark", "Aiden Doherty"], "title": "Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Introduction: Data from wearable devices collected in free-living settings,\nand labelled with physical activity behaviours compatible with health research,\nare essential for both validating existing wearable-based measurement\napproaches and developing novel machine learning approaches. One common way of\nobtaining these labels relies on laborious annotation of sequences of images\ncaptured by cameras worn by participants through the course of a day. Methods:\nWe compare the performance of three vision language models and two\ndiscriminative models on two free-living validation studies with 161 and 111\nparticipants, collected in Oxfordshire, United Kingdom and Sichuan, China,\nrespectively, using the Autographer (OMG Life, defunct) wearable camera.\nResults: We found that the best open-source vision-language model (VLM) and\nfine-tuned discriminative model (DM) achieved comparable performance when\npredicting sedentary behaviour from single images on unseen participants in the\nOxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,\n0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,\n0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,\n0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,\nperformance fell across all intensity categories, with median Cohen's\nkappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,\nand from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely\navailable computer vision models could help annotate sedentary behaviour,\ntypically the most prevalent activity of daily living, from wearable camera\nimages within similar populations to seen data, reducing the annotation burden.", "AI": {"tldr": "The paper compares vision-language models (VLMs) and discriminative models (DMs) for labeling physical activity behaviors from wearable camera images, showing comparable performance for sedentary behavior but declining accuracy for higher-intensity activities and external datasets.", "motivation": "To reduce the labor-intensive annotation of wearable camera images for physical activity research by evaluating the performance of automated models.", "method": "Comparison of three VLMs and two DMs on two free-living validation studies (161 and 111 participants) using wearable camera data.", "result": "VLMs and DMs performed similarly for sedentary behavior (F1-scores ~0.9) but worse for light and moderate-to-vigorous activities (~0.6-0.7). Performance dropped significantly in external validation (Cohen's kappa ~0.2-0.3).", "conclusion": "Freely available VLMs can effectively annotate sedentary behavior in similar populations, reducing annotation effort, but struggle with higher-intensity activities and external datasets."}}
{"id": "2505.03424", "pdf": "https://arxiv.org/pdf/2505.03424", "abs": "https://arxiv.org/abs/2505.03424", "authors": ["Kirill Lukyanov", "Mikhail Drobyshevskiy", "Georgii Sazonov", "Mikhail Soloviov", "Ilya Makarov"], "title": "Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The growing need for Trusted AI (TAI) highlights the importance of\ninterpretability and robustness in machine learning models. However, many\nexisting tools overlook graph data and rarely combine these two aspects into a\nsingle solution. Graph Neural Networks (GNNs) have become a popular approach,\nachieving top results across various tasks. We introduce GNN-AID (Graph Neural\nNetwork Analysis, Interpretation, and Defense), an open-source framework\ndesigned for graph data to address this gap. Built as a Python library, GNN-AID\nsupports advanced trust methods and architectural layers, allowing users to\nanalyze graph datasets and GNN behavior using attacks, defenses, and\ninterpretability methods.\n  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,\nand support for any GNNs through customizable interfaces. It also includes a\nweb interface with tools for graph visualization and no-code features like an\ninteractive model builder, simplifying the exploration and analysis of GNNs.\nThe framework also supports MLOps techniques, ensuring reproducibility and\nresult versioning to track and revisit analyses efficiently.\n  GNN-AID is a flexible tool for developers and researchers. It helps\ndevelopers create, analyze, and customize graph models, while also providing\naccess to prebuilt datasets and models for quick experimentation. Researchers\ncan use the framework to explore advanced topics on the relationship between\ninterpretability and robustness, test defense strategies, and combine methods\nto protect against different types of attacks.\n  We also show how defenses against evasion and poisoning attacks can conflict\nwhen applied to graph data, highlighting the complex connections between\ndefense strategies.\n  GNN-AID is available at\n\\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}", "AI": {"tldr": "GNN-AID is an open-source framework for analyzing, interpreting, and defending Graph Neural Networks (GNNs), combining interpretability and robustness for graph data.", "motivation": "The need for Trusted AI (TAI) in graph data, where existing tools often lack interpretability and robustness integration.", "method": "Built as a Python library on PyTorch-Geometric, GNN-AID includes attacks, defenses, interpretability methods, and a web interface for visualization and no-code features.", "result": "GNN-AID supports developers and researchers in creating, analyzing, and customizing GNN models, while revealing conflicts between defense strategies.", "conclusion": "GNN-AID bridges the gap in trusted AI for graph data, offering a flexible tool for both practical development and advanced research."}}
{"id": "2505.03217", "pdf": "https://arxiv.org/pdf/2505.03217", "abs": "https://arxiv.org/abs/2505.03217", "authors": ["Xiaobo Jin", "JiaShu Tu"], "title": "Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover", "categories": ["cs.NE", "cs.AI", "I.2.8; G.1.6"], "comment": "14 pages,2 figures,4 tables", "summary": "This study introduces an innovative crossover operator named Particle Swarm\nOptimization-inspired Crossover (PSOX), which is specifically developed for\nreal-coded genetic algorithms. Departing from conventional crossover approaches\nthat only exchange information between individuals within the same generation,\nPSOX uniquely incorporates guidance from both the current global best solution\nand historical optimal solutions across multiple generations. This novel\nmechanism enables the algorithm to maintain population diversity while\nsimultaneously accelerating convergence toward promising regions of the search\nspace. The effectiveness of PSOX is rigorously evaluated through comprehensive\nexperiments on 15 benchmark test functions with diverse characteristics,\nincluding unimodal, multimodal, and highly complex landscapes. Comparative\nanalysis against five state-of-the-art crossover operators reveals that PSOX\nconsistently delivers superior performance in terms of solution accuracy,\nalgorithmic stability, and convergence speed, especially when combined with an\nappropriate mutation strategy. Furthermore, the study provides an in-depth\ninvestigation of how different mutation rates influence PSOX's performance,\nyielding practical guidelines for parameter tuning when addressing optimization\nproblems with varying landscape properties.", "AI": {"tldr": "The paper introduces PSOX, a novel crossover operator for real-coded genetic algorithms, leveraging global and historical best solutions to enhance diversity and convergence. It outperforms existing methods in accuracy, stability, and speed.", "motivation": "To improve genetic algorithms by incorporating guidance from global and historical best solutions, addressing limitations of traditional crossover methods.", "method": "Develops PSOX, a crossover operator inspired by Particle Swarm Optimization, and tests it on 15 benchmark functions with varied landscapes.", "result": "PSOX consistently outperforms five state-of-the-art crossover operators in accuracy, stability, and convergence speed.", "conclusion": "PSOX is highly effective, especially with proper mutation rates, offering practical tuning guidelines for diverse optimization problems."}}
{"id": "2410.09580", "pdf": "https://arxiv.org/pdf/2410.09580", "abs": "https://arxiv.org/abs/2410.09580", "authors": ["Hanwen Du", "Bo Peng", "Xia Ning"], "title": "SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Conversational Recommender Systems (CRS) proactively engage users in\ninteractive dialogues to elicit user preferences and provide personalized\nrecommendations. Existing methods train Reinforcement Learning (RL)-based agent\nwith greedy action selection or sampling strategy, and may suffer from\nsuboptimal conversational planning. To address this, we present a novel Monte\nCarlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a\nconversational agent (S-agent) and a conversational planner (S-planner).\nS-planner builds a conversational search tree with MCTS based on the initial\nactions proposed by S-agent to find conversation plans. The best conversation\nplans from S-planner are used to guide the training of S-agent, creating a\nself-training loop where S-agent can iteratively improve its capability for\nconversational planning. Furthermore, we propose an efficient variant SAPIENT-e\nfor trade-off between training efficiency and performance. Extensive\nexperiments on four benchmark datasets validate the effectiveness of our\napproach, showing that SAPIENT outperforms the state-of-the-art baselines.", "AI": {"tldr": "SAPIENT introduces a Monte Carlo Tree Search (MCTS)-based framework for Conversational Recommender Systems (CRS) to improve conversational planning, outperforming existing methods.", "motivation": "Existing RL-based CRS methods suffer from suboptimal conversational planning due to greedy action selection or sampling strategies.", "method": "SAPIENT uses MCTS for conversational planning, with a self-training loop between a conversational agent (S-agent) and planner (S-planner). An efficient variant, SAPIENT-e, balances training efficiency and performance.", "result": "Experiments on four datasets show SAPIENT outperforms state-of-the-art baselines.", "conclusion": "SAPIENT effectively enhances conversational planning in CRS, validated by superior performance."}}
{"id": "2505.03383", "pdf": "https://arxiv.org/pdf/2505.03383", "abs": "https://arxiv.org/abs/2505.03383", "authors": ["Jian-Wei Li", "Wen-Ze Shao"], "title": "Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial examples have revealed the vulnerability of deep learning models\nand raised serious concerns about information security. The transfer-based\nattack is a hot topic in black-box attacks that are practical to real-world\nscenarios where the training datasets, parameters, and structure of the target\nmodel are unknown to the attacker. However, few methods consider the\nparticularity of class-specific deep models for fine-grained vision tasks, such\nas face recognition (FR), giving rise to unsatisfactory attacking performance.\nIn this work, we first investigate what in a face exactly contributes to the\nembedding learning of FR models and find that both decisive and auxiliary\nfacial features are specific to each FR model, which is quite different from\nthe biological mechanism of human visual system. Accordingly we then propose a\nnovel attack method named Attention-aggregated Attack (AAA) to enhance the\ntransferability of adversarial examples against FR, which is inspired by the\nattention divergence and aims to destroy the facial features that are critical\nfor the decision-making of other FR models by imitating their attentions on the\nclean face images. Extensive experiments conducted on various FR models\nvalidate the superiority and robust effectiveness of the proposed method over\nexisting methods.", "AI": {"tldr": "The paper proposes Attention-aggregated Attack (AAA) to improve adversarial example transferability for face recognition models by targeting model-specific facial features.", "motivation": "Adversarial examples expose deep learning vulnerabilities, but existing transfer-based attacks overlook class-specific models like face recognition, leading to poor performance.", "method": "AAA mimics attention divergence in FR models to disrupt critical facial features, enhancing adversarial example transferability.", "result": "Experiments show AAA outperforms existing methods in attacking various FR models.", "conclusion": "AAA effectively improves adversarial transferability for FR models by targeting model-specific features."}}
{"id": "2505.03432", "pdf": "https://arxiv.org/pdf/2505.03432", "abs": "https://arxiv.org/abs/2505.03432", "authors": ["Stefano Bruno", "Sotirios Sabanis"], "title": "Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients", "categories": ["cs.LG", "math.OC", "math.PR", "stat.ML"], "comment": null, "summary": "Score-based Generative Models (SGMs) approximate a data distribution by\nperturbing it with Gaussian noise and subsequently denoising it via a learned\nreverse diffusion process. These models excel at modeling complex data\ndistributions and generating diverse samples, achieving state-of-the-art\nperformance across domains such as computer vision, audio generation,\nreinforcement learning, and computational biology. Despite their empirical\nsuccess, existing Wasserstein-2 convergence analysis typically assume strong\nregularity conditions-such as smoothness or strict log-concavity of the data\ndistribution-that are rarely satisfied in practice. In this work, we establish\nthe first non-asymptotic Wasserstein-2 convergence guarantees for SGMs\ntargeting semiconvex distributions with potentially discontinuous gradients.\nOur upper bounds are explicit and sharp in key parameters, achieving optimal\ndependence of $O(\\sqrt{d})$ on the data dimension $d$ and convergence rate of\norder one. The framework accommodates a wide class of practically relevant\ndistributions, including symmetric modified half-normal distributions, Gaussian\nmixtures, double-well potentials, and elastic net potentials. By leveraging\nsemiconvexity without requiring smoothness assumptions on the potential such as\ndifferentiability, our results substantially broaden the theoretical\nfoundations of SGMs, bridging the gap between empirical success and rigorous\nguarantees in non-smooth, complex data regimes.", "AI": {"tldr": "SGMs use Gaussian noise and reverse diffusion to model data, excelling in diverse domains. This work provides non-asymptotic Wasserstein-2 convergence guarantees for semiconvex distributions, broadening theoretical foundations.", "motivation": "Existing convergence analyses for SGMs assume strong regularity conditions rarely met in practice. This work aims to relax these assumptions and provide rigorous guarantees for non-smooth, complex data.", "method": "The study establishes non-asymptotic Wasserstein-2 convergence bounds for SGMs targeting semiconvex distributions, including cases with discontinuous gradients.", "result": "The framework achieves optimal dependence on data dimension and convergence rate, accommodating practical distributions like Gaussian mixtures and double-well potentials.", "conclusion": "This work bridges the gap between empirical success and theoretical guarantees for SGMs in non-smooth, complex data regimes."}}
{"id": "2505.03265", "pdf": "https://arxiv.org/pdf/2505.03265", "abs": "https://arxiv.org/abs/2505.03265", "authors": ["Abdelkarim El-Hajjami", "Camille Salinesi"], "title": "Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While modern Requirements Engineering (RE) heavily relies on natural language\nprocessing and Machine Learning (ML) techniques, their effectiveness is limited\nby the scarcity of high-quality datasets. This paper introduces Synthline, a\nProduct Line (PL) approach that leverages Large Language Models to\nsystematically generate synthetic RE data for classification-based use cases.\nThrough an empirical evaluation conducted in the context of using ML for the\nidentification of requirements specification defects, we investigated both the\ndiversity of the generated data and its utility for training downstream models.\nOur analysis reveals that while synthetic datasets exhibit less diversity than\nreal data, they are good enough to serve as viable training resources.\nMoreover, our evaluation shows that combining synthetic and real data leads to\nsubstantial performance improvements. Specifically, hybrid approaches achieve\nup to 85% improvement in precision and a 2x increase in recall compared to\nmodels trained exclusively on real data. These findings demonstrate the\npotential of PL-based synthetic data generation to address data scarcity in RE.\nWe make both our implementation and generated datasets publicly available to\nsupport reproducibility and advancement in the field.", "AI": {"tldr": "Synthline, a Product Line approach using Large Language Models, generates synthetic RE data to address dataset scarcity. While synthetic data lacks diversity, it improves ML model performance when combined with real data.", "motivation": "Addressing the scarcity of high-quality datasets in Requirements Engineering (RE) to enhance the effectiveness of ML techniques.", "method": "Introduces Synthline, leveraging Large Language Models for synthetic data generation, and evaluates its utility in training ML models for requirements specification defect identification.", "result": "Synthetic data, though less diverse, is viable for training. Combining synthetic and real data improves precision by 85% and recall by 2x.", "conclusion": "Synthline demonstrates the potential of synthetic data generation to mitigate data scarcity in RE, with hybrid approaches yielding significant performance gains."}}
{"id": "2411.00027", "pdf": "https://arxiv.org/pdf/2411.00027", "abs": "https://arxiv.org/abs/2411.00027", "authors": ["Zhehao Zhang", "Ryan A. Rossi", "Branislav Kveton", "Yijia Shao", "Diyi Yang", "Hamed Zamani", "Franck Dernoncourt", "Joe Barrow", "Tong Yu", "Sungchul Kim", "Ruiyi Zhang", "Jiuxiang Gu", "Tyler Derr", "Hongjie Chen", "Junda Wu", "Xiang Chen", "Zichao Wang", "Subrata Mitra", "Nedim Lipka", "Nesreen Ahmed", "Yu Wang"], "title": "Personalization of Large Language Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Personalization of Large Language Models (LLMs) has recently become\nincreasingly important with a wide range of applications. Despite the\nimportance and recent progress, most existing works on personalized LLMs have\nfocused either entirely on (a) personalized text generation or (b) leveraging\nLLMs for personalization-related downstream applications, such as\nrecommendation systems. In this work, we bridge the gap between these two\nseparate main directions for the first time by introducing a taxonomy for\npersonalized LLM usage and summarizing the key differences and challenges. We\nprovide a formalization of the foundations of personalized LLMs that\nconsolidates and expands notions of personalization of LLMs, defining and\ndiscussing novel facets of personalization, usage, and desiderata of\npersonalized LLMs. We then unify the literature across these diverse fields and\nusage scenarios by proposing systematic taxonomies for the granularity of\npersonalization, personalization techniques, datasets, evaluation methods, and\napplications of personalized LLMs. Finally, we highlight challenges and\nimportant open problems that remain to be addressed. By unifying and surveying\nrecent research using the proposed taxonomies, we aim to provide a clear guide\nto the existing literature and different facets of personalization in LLMs,\nempowering both researchers and practitioners.", "AI": {"tldr": "The paper introduces a taxonomy to bridge the gap between personalized text generation and LLM-based downstream applications, formalizing foundations and proposing systematic taxonomies for personalization in LLMs.", "motivation": "To unify and clarify the fragmented research on personalized LLMs by addressing the gap between text generation and downstream applications.", "method": "Introduces a taxonomy for personalized LLM usage, formalizes foundations, and proposes systematic taxonomies for granularity, techniques, datasets, evaluation, and applications.", "result": "A unified framework for understanding personalized LLMs, highlighting key differences, challenges, and open problems.", "conclusion": "The paper provides a comprehensive guide to personalized LLM research, empowering researchers and practitioners with clear taxonomies and insights."}}
{"id": "2505.03394", "pdf": "https://arxiv.org/pdf/2505.03394", "abs": "https://arxiv.org/abs/2505.03394", "authors": ["Sarthak Mehrotra", "Rishabh Jain", "Mayur Hemani", "Balaji Krishnamurthy", "Mausoom Sarkar"], "title": "EOPose : Exemplar-based object reposing using Generalized Pose Correspondences", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025 AI4CC workshop", "summary": "Reposing objects in images has a myriad of applications, especially for\ne-commerce where several variants of product images need to be produced\nquickly. In this work, we leverage the recent advances in unsupervised keypoint\ncorrespondence detection between different object images of the same class to\npropose an end-to-end framework for generic object reposing. Our method,\nEOPose, takes a target pose-guidance image as input and uses its keypoint\ncorrespondence with the source object image to warp and re-render the latter\ninto the target pose using a novel three-step approach. Unlike generative\napproaches, our method also preserves the fine-grained details of the object\nsuch as its exact colors, textures, and brand marks. We also prepare a new\ndataset of paired objects based on the Objaverse dataset to train and test our\nnetwork. EOPose produces high-quality reposing output as evidenced by different\nimage quality metrics (PSNR, SSIM and FID). Besides a description of the method\nand the dataset, the paper also includes detailed ablation and user studies to\nindicate the efficacy of the proposed method", "AI": {"tldr": "EOPose is an end-to-end framework for object reposing in images, leveraging unsupervised keypoint correspondence to preserve fine details while achieving high-quality results.", "motivation": "The need for quick production of product image variants in e-commerce drives the development of a method that avoids generative approaches' loss of detail.", "method": "EOPose uses a three-step approach: keypoint correspondence detection, warping, and re-rendering, guided by a target pose image.", "result": "High-quality reposing outputs are validated by metrics (PSNR, SSIM, FID) and supported by ablation and user studies.", "conclusion": "EOPose effectively reposes objects while preserving details, with a new dataset and comprehensive evaluation demonstrating its efficacy."}}
{"id": "2505.03490", "pdf": "https://arxiv.org/pdf/2505.03490", "abs": "https://arxiv.org/abs/2505.03490", "authors": ["Faiz Taleb", "Ivan Gazeau", "Maryline Laurent"], "title": "A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative models can unintentionally memorize training data, posing\nsignificant privacy risks. This paper addresses the memorization phenomenon in\ntime series imputation models, introducing the Loss-Based with Reference Model\n(LBRM) algorithm. The LBRM method leverages a reference model to enhance the\naccuracy of membership inference attacks, distinguishing between training and\ntest data. Our contributions are twofold: first, we propose an innovative\nmethod to effectively extract and identify memorized training data,\nsignificantly improving detection accuracy. On average, without fine-tuning,\nthe AUROC improved by approximately 40\\%. With fine-tuning, the AUROC increased\nby approximately 60\\%. Second, we validate our approach through membership\ninference attacks on two types of architectures designed for time series\nimputation, demonstrating the robustness and versatility of the LBRM approach\nin different contexts. These results highlight the significant enhancement in\ndetection accuracy provided by the LBRM approach, addressing privacy risks in\ntime series imputation models.", "AI": {"tldr": "The paper introduces the LBRM algorithm to detect memorized training data in time series imputation models, improving membership inference attack accuracy by up to 60%.", "motivation": "To address privacy risks from unintentional memorization of training data in generative models, specifically in time series imputation.", "method": "Proposes the Loss-Based with Reference Model (LBRM) algorithm, using a reference model to distinguish training from test data.", "result": "AUROC improved by ~40% without fine-tuning and ~60% with fine-tuning. Validated on two architectures for time series imputation.", "conclusion": "LBRM significantly enhances detection accuracy, mitigating privacy risks in time series imputation models."}}
{"id": "2505.03296", "pdf": "https://arxiv.org/pdf/2505.03296", "abs": "https://arxiv.org/abs/2505.03296", "authors": ["Jan Ole von Hartz", "Adrian R\u00f6fer", "Joschka Boedecker", "Abhinav Valada"], "title": "The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Submitted for publication to IEEE Transaction on Robotics", "summary": "We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel\napproach for flexible policy representation and imitation learning in robot\nmanipulation. MiDiGap enables learning from as few as five demonstrations using\nonly camera observations and generalizes across a wide range of challenging\ntasks. It excels at long-horizon behaviors such as making coffee, highly\nconstrained motions such as opening doors, dynamic actions such as scooping\nwith a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns\nthese tasks on a CPU in less than a minute and scales linearly to large\ndatasets. We also develop a rich suite of tools for inference-time steering\nusing evidence such as collision signals and robot kinematic constraints. This\nsteering enables novel generalization capabilities, including obstacle\navoidance and cross-embodiment policy transfer. MiDiGap achieves\nstate-of-the-art performance on diverse few-shot manipulation benchmarks. On\nconstrained RLBench tasks, it improves policy success by 76 percentage points\nand reduces trajectory cost by 67%. On multimodal tasks, it improves policy\nsuccess by 48 percentage points and increases sample efficiency by a factor of\n20. In cross-embodiment transfer, it more than doubles policy success. We make\nthe code publicly available at https://midigap.cs.uni-freiburg.de.", "AI": {"tldr": "MiDiGap is a novel method for robot manipulation, excelling in learning from few demonstrations, handling diverse tasks, and achieving state-of-the-art performance.", "motivation": "To enable flexible policy representation and imitation learning in robot manipulation with minimal demonstrations and broad generalization.", "method": "Uses Mixture of Discrete-time Gaussian Processes (MiDiGap) for learning from camera observations, with tools for inference-time steering.", "result": "Achieves significant improvements in policy success, trajectory cost, and sample efficiency across diverse tasks.", "conclusion": "MiDiGap is a highly efficient and scalable solution for few-shot robot manipulation, with open-source availability."}}
{"id": "2412.18135", "pdf": "https://arxiv.org/pdf/2412.18135", "abs": "https://arxiv.org/abs/2412.18135", "authors": ["Binrui Zeng", "Bin Ji", "Xiaodong Liu", "Jie Yu", "Shasha Li", "Jun Ma", "Xiaopeng Li", "Shangwen Wang", "Xinran Hong", "Yongtao Tang"], "title": "LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment", "categories": ["cs.CL"], "comment": "8 pages, 4 figures, accepted to IJCNN 2025", "summary": "As Large Language Models (LLMs) demonstrate exceptional performance across\nvarious domains, deploying LLMs on edge devices has emerged as a new trend.\nQuantization techniques, which reduce the size and memory requirements of LLMs,\nare effective for deploying LLMs on resource-limited edge devices. However,\nexisting one-size-fits-all quantization methods often fail to dynamically\nadjust the memory requirements of LLMs, limiting their applications to\npractical edge devices with various computation resources. To tackle this\nissue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for\nadaptive quantization and dynamic deployment of LLMs based on layer importance.\nSpecifically, LSAQ evaluates the importance of LLMs' neural layers by\nconstructing top-k token sets from the inputs and outputs of each layer and\ncalculating their Jaccard similarity. Based on layer importance, our system\nadaptively adjusts quantization strategies in real time according to the\ncomputation resource of edge devices, which applies higher quantization\nprecision to layers with higher importance, and vice versa. {Experimental\nresults show that LSAQ consistently outperforms the selected quantization\nbaselines in terms of perplexity and zero-shot tasks. Additionally, it can\ndevise appropriate quantization schemes for different usage scenarios to\nfacilitate the deployment of LLMs.", "AI": {"tldr": "LSAQ proposes adaptive quantization for LLMs on edge devices by dynamically adjusting precision based on layer importance, outperforming existing methods.", "motivation": "Existing quantization methods lack adaptability for diverse edge device resources, limiting LLM deployment.", "method": "LSAQ evaluates layer importance using Jaccard similarity of top-k tokens and adjusts quantization precision accordingly.", "result": "LSAQ outperforms baselines in perplexity and zero-shot tasks, enabling flexible deployment.", "conclusion": "LSAQ effectively adapts quantization for edge devices, enhancing LLM deployment efficiency."}}
{"id": "2505.03401", "pdf": "https://arxiv.org/pdf/2505.03401", "abs": "https://arxiv.org/abs/2505.03401", "authors": ["Shanshan Song", "Hui Tang", "Honglong Yang", "Xiaomeng Li"], "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Radiology Report Generation (RRG) automates the creation of radiology reports\nfrom medical imaging, enhancing the efficiency of the reporting process.\nLongitudinal Radiology Report Generation (LRRG) extends RRG by incorporating\nthe ability to compare current and prior exams, facilitating the tracking of\ntemporal changes in clinical findings. Existing LRRG approaches only extract\nfeatures from prior and current images using a visual pre-trained encoder,\nwhich are then concatenated to generate the final report. However, these\nmethods struggle to effectively capture both spatial and temporal correlations\nduring the feature extraction process. Consequently, the extracted features\ninadequately capture the information of difference across exams and thus\nunderrepresent the expected progressions, leading to sub-optimal performance in\nLRRG. To address this, we develop a novel dynamic difference-aware temporal\nresidual network (DDaTR). In DDaTR, we introduce two modules at each stage of\nthe visual encoder to capture multi-level spatial correlations. The Dynamic\nFeature Alignment Module (DFAM) is designed to align prior features across\nmodalities for the integrity of prior clinical information. Prompted by the\nenriched prior features, the dynamic difference-aware module (DDAM) captures\nfavorable difference information by identifying relationships across exams.\nFurthermore, our DDaTR employs the dynamic residual network to unidirectionally\ntransmit longitudinal information, effectively modelling temporal correlations.\nExtensive experiments demonstrated superior performance over existing methods\non three benchmarks, proving its efficacy in both RRG and LRRG tasks.", "AI": {"tldr": "A novel dynamic difference-aware temporal residual network (DDaTR) improves longitudinal radiology report generation by better capturing spatial and temporal correlations.", "motivation": "Existing methods for Longitudinal Radiology Report Generation (LRRG) fail to effectively capture spatial and temporal correlations, leading to sub-optimal performance.", "method": "DDaTR introduces two modules: Dynamic Feature Alignment Module (DFAM) for aligning prior features and Dynamic Difference-Aware Module (DDAM) for capturing differences across exams. It also uses a dynamic residual network for temporal modeling.", "result": "DDaTR outperforms existing methods on three benchmarks, excelling in both RRG and LRRG tasks.", "conclusion": "DDaTR effectively addresses the limitations of prior methods, enhancing the accuracy and efficiency of radiology report generation."}}
{"id": "2505.03509", "pdf": "https://arxiv.org/pdf/2505.03509", "abs": "https://arxiv.org/abs/2505.03509", "authors": ["Pablo G\u00f3mez", "David O'Ryan"], "title": "AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning", "categories": ["cs.LG", "astro-ph.IM"], "comment": "Journal submission in preparation", "summary": "Anomaly detection in large datasets is essential in fields such as astronomy\nand computer vision; however, supervised methods typically require extensive\nanomaly labelling, which is often impractical. We present AnomalyMatch, an\nanomaly detection framework combining the semi-supervised FixMatch algorithm\nusing EfficientNet classifiers with active learning. By treating anomaly\ndetection as a semi-supervised binary classification problem, we efficiently\nutilise limited labelled and abundant unlabelled images. We allow iterative\nmodel refinement in a user interface for expert verification of high-confidence\nanomalies and correction of false positives. Built for astronomical data,\nAnomalyMatch generalises readily to other domains facing similar data\nchallenges. Evaluations on the GalaxyMNIST astronomical dataset and the\nminiImageNet natural-image benchmark under severe class imbalance (1% anomalies\nfor miniImageNet) display strong performance: starting from five to ten\nlabelled anomalies and after three active learning cycles, we achieve an\naverage AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective\nAUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with\n71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.\nAnomalyMatch is tailored for large-scale applications, efficiently processing\npredictions for 100 million images within three days on a single GPU.\nIntegrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted\ndiscovery of scientifically valuable anomalies in vast astronomical datasets.\nOur results underscore the exceptional utility and scalability of this approach\nfor anomaly discovery, highlighting the value of specialised approaches for\ndomains characterised by severe label scarcity.", "AI": {"tldr": "AnomalyMatch is a semi-supervised anomaly detection framework combining FixMatch and active learning, achieving high performance with minimal labeled data.", "motivation": "Supervised anomaly detection requires extensive labeling, which is impractical. AnomalyMatch addresses this by leveraging limited labeled and abundant unlabeled data.", "method": "Uses FixMatch with EfficientNet classifiers and active learning for iterative refinement via expert verification.", "result": "Achieves AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with high precision for top-ranked anomalies.", "conclusion": "AnomalyMatch is scalable and effective for large datasets, especially in domains with label scarcity."}}
{"id": "2505.03336", "pdf": "https://arxiv.org/pdf/2505.03336", "abs": "https://arxiv.org/abs/2505.03336", "authors": ["Hao Liao", "Wensheng Lu", "Jianxun Lian", "Mingqi Wu", "Shuo Wang", "Yong Zhang", "Yitian Huang", "Mingyang Zhou", "Xing Xie"], "title": "Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs", "categories": ["cs.IR", "cs.AI"], "comment": "13 pages", "summary": "Large Language Models (LLMs) have shown promise for generative recommender\nsystems due to their transformative capabilities in user interaction. However,\nensuring they do not recommend out-of-domain (OOD) items remains a challenge.\nWe study two distinct methods to address this issue: RecLM-ret, a\nretrieval-based method, and RecLM-cgen, a constrained generation method. Both\nmethods integrate seamlessly with existing LLMs to ensure in-domain\nrecommendations. Comprehensive experiments on three recommendation datasets\ndemonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing\nLLM-based recommender models in accuracy while eliminating OOD recommendations,\nmaking it the preferred method for adoption. Additionally, RecLM-cgen maintains\nstrong generalist capabilities and is a lightweight plug-and-play module for\neasy integration into LLMs, offering valuable practical benefits for the\ncommunity. Source code is available at https://github.com/microsoft/RecAI", "AI": {"tldr": "RecLM-cgen, a constrained generation method, outperforms retrieval-based RecLM-ret and other LLM-based recommenders in accuracy and eliminating OOD recommendations.", "motivation": "Address the challenge of preventing LLMs from recommending out-of-domain (OOD) items in generative recommender systems.", "method": "Two methods: RecLM-ret (retrieval-based) and RecLM-cgen (constrained generation), integrated with existing LLMs.", "result": "RecLM-cgen consistently outperforms RecLM-ret and other models in accuracy and eliminating OOD recommendations.", "conclusion": "RecLM-cgen is the preferred method due to its accuracy, lightweight design, and practical benefits for integration."}}
{"id": "2501.14917", "pdf": "https://arxiv.org/pdf/2501.14917", "abs": "https://arxiv.org/abs/2501.14917", "authors": ["Sara Abdali", "Can Goksen", "Saeed Amizadeh", "Julie E. Maybee", "Kazuhito Koishida"], "title": "Self-reflecting Large Language Models: A Hegelian Dialectical Approach", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Investigating NLP through a philosophical lens has recently caught\nresearcher's eyes as it connects computational methods with classical schools\nof philosophy. This paper introduces a philosophical approach inspired by the\n\\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a\nself-dialectical approach to emulate internal critiques and then synthesize new\nideas by resolving the opposing points of view. Moreover, this paper\ninvestigates the effect of LLMs' temperature for generation by establishing a\ndynamic annealing approach, which promotes the creativity in the early stages\nand gradually refines it by focusing on the nuances, as well as a\nfixed-temperature strategy for generation. We assess the effectiveness of our\nproposed method in generating novel ideas and in improving the reasoning\nabilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent\nMajority Voting (MAMV) strategy to assess the validity and novelty of the\ngenerated ideas, which proves useful in the absence of domain experts. Our\nexperiments demonstrate promising results in generating ideas and enhancing\nproblem-solving performance.", "AI": {"tldr": "The paper proposes a Hegelian Dialectic-inspired method for LLM self-reflection, dynamic annealing for temperature control, and MAMV for idea validation, showing improved idea generation and reasoning.", "motivation": "To bridge computational NLP with classical philosophy and enhance LLMs' self-reflection and creativity.", "method": "Uses Hegelian Dialectic for self-reflection, dynamic annealing for temperature control, and MAMV for idea validation.", "result": "Promising results in generating novel ideas and improving LLM reasoning.", "conclusion": "The approach effectively enhances LLM creativity and problem-solving, validated by MAMV."}}
{"id": "2505.03412", "pdf": "https://arxiv.org/pdf/2505.03412", "abs": "https://arxiv.org/abs/2505.03412", "authors": ["Haoyu Bai", "Jie Wang", "Gaomin Li", "Xuan Li", "Xiaohu Zhang", "Xia Yang"], "title": "CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Internal defect detection constitutes a critical process in ensuring\ncomponent quality, for which anomaly detection serves as an effective solution.\nHowever, existing anomaly detection datasets predominantly focus on surface\ndefects in visible-light images, lacking publicly available X-ray datasets\ntargeting internal defects in components. To address this gap, we construct the\nfirst publicly accessible component X-ray anomaly detection (CXR-AD) dataset,\ncomprising real-world X-ray images. The dataset covers five industrial\ncomponent categories, including 653 normal samples and 561 defect samples with\nprecise pixel-level mask annotations. We systematically analyze the dataset\ncharacteristics and identify three major technical challenges: (1) strong\ncoupling between complex internal structures and defect regions, (2) inherent\nlow contrast and high noise interference in X-ray imaging, and (3) significant\nvariations in defect scales and morphologies. To evaluate dataset complexity,\nwe benchmark three state-of-the-art anomaly detection frameworks\n(feature-based, reconstruction-based, and zero-shot learning methods).\nExperimental results demonstrate a 29.78% average performance degradation on\nCXR-AD compared to MVTec AD, highlighting the limitations of current algorithms\nin handling internal defect detection tasks. To the best of our knowledge,\nCXR-AD represents the first publicly available X-ray dataset for component\nanomaly detection, providing a real-world industrial benchmark to advance\nalgorithm development and enhance precision in internal defect inspection\ntechnologies.", "AI": {"tldr": "The paper introduces CXR-AD, the first public X-ray dataset for internal defect detection in components, highlighting challenges and benchmarking current anomaly detection methods.", "motivation": "Addressing the lack of publicly available X-ray datasets for internal defect detection, which limits algorithm development and industrial applications.", "method": "Constructed a dataset with 653 normal and 561 defect X-ray images, annotated with pixel-level masks, and benchmarked three anomaly detection frameworks.", "result": "Current methods show a 29.78% performance drop on CXR-AD compared to MVTec AD, revealing their limitations for internal defects.", "conclusion": "CXR-AD fills a critical gap, offering a benchmark to improve internal defect detection algorithms and industrial inspection precision."}}
{"id": "2505.03519", "pdf": "https://arxiv.org/pdf/2505.03519", "abs": "https://arxiv.org/abs/2505.03519", "authors": ["Sy-Tuyen Ho", "Koh Jun Hao", "Ngoc-Bao Nguyen", "Alexander Binder", "Ngai-Man Cheung"], "title": "Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks", "categories": ["cs.LG"], "comment": "Our dataset and code are available in the Supp", "summary": "Model Inversion (MI) attacks aim to reconstruct information of private\ntraining data by exploiting access to machine learning models. The most common\nevaluation framework for MI attacks/defenses relies on an evaluation model that\nhas been utilized to assess progress across almost all MI attacks and defenses\nproposed in recent years. In this paper, for the first time, we present an\nin-depth study of MI evaluation. Firstly, we construct the first comprehensive\nhuman-annotated dataset of MI attack samples, based on 28 setups of different\nMI attacks, defenses, private and public datasets. Secondly, using our dataset,\nwe examine the accuracy of the MI evaluation framework and reveal that it\nsuffers from a significant number of false positives. These findings raise\nquestions about the previously reported success rates of SOTA MI attacks.\nThirdly, we analyze the causes of these false positives, design controlled\nexperiments, and discover the surprising effect of Type I adversarial features\non MI evaluation, as well as adversarial transferability, highlighting a\nrelationship between two previously distinct research areas. Our findings\nsuggest that the performance of SOTA MI attacks has been overestimated, with\nthe actual privacy leakage being significantly less than previously reported.\nIn conclusion, we highlight critical limitations in the widely used MI\nevaluation framework and present our methods to mitigate false positive rates.\nWe remark that prior research has shown that Type I adversarial attacks are\nvery challenging, with no existing solution. Therefore, we urge to consider\nhuman evaluation as a primary MI evaluation framework rather than merely a\nsupplement as in previous MI research. We also encourage further work on\ndeveloping more robust and reliable automatic evaluation frameworks.", "AI": {"tldr": "The paper critiques the Model Inversion (MI) evaluation framework, revealing false positives and overestimated attack success rates. It introduces a human-annotated dataset, analyzes causes of errors, and suggests human evaluation as a primary framework.", "motivation": "To address limitations in the current MI evaluation framework, which may overestimate attack success due to false positives, and to explore the impact of adversarial features on MI evaluation.", "method": "Constructed a human-annotated dataset from 28 MI attack/defense setups, analyzed false positives, and conducted controlled experiments to study adversarial feature effects.", "result": "Found significant false positives in MI evaluation, overestimating attack success. Identified adversarial feature impact and transferability, linking MI and adversarial attack research.", "conclusion": "Advocates for human evaluation as the primary MI framework and calls for more robust automatic evaluation methods, highlighting current limitations."}}
{"id": "2505.03338", "pdf": "https://arxiv.org/pdf/2505.03338", "abs": "https://arxiv.org/abs/2505.03338", "authors": ["Lena Reissinger", "Yuanyuan Li", "Anna-Carolina Haensch", "Neeraj Sarna"], "title": "Safer Prompts: Reducing IP Risk in Visual Generative AI", "categories": ["math.NA", "cs.AI", "cs.NA"], "comment": null, "summary": "Visual Generative AI models have demonstrated remarkable capability in\ngenerating high-quality images from simple inputs like text prompts. However,\nbecause these models are trained on images from diverse sources, they risk\nmemorizing and reproducing specific content, raising concerns about\nintellectual property (IP) infringement. Recent advances in prompt engineering\noffer a cost-effective way to enhance generative AI performance. In this paper,\nwe evaluate the effectiveness of prompt engineering techniques in mitigating IP\ninfringement risks in image generation. Our findings show that Chain of Thought\nPrompting and Task Instruction Prompting significantly reduce the similarity\nbetween generated images and the training data of diffusion models, thereby\nlowering the risk of IP infringement.", "AI": {"tldr": "Prompt engineering techniques like Chain of Thought and Task Instruction Prompting reduce IP infringement risks in AI-generated images by lowering similarity to training data.", "motivation": "Address concerns about AI models memorizing and reproducing copyrighted content from training data.", "method": "Evaluate the effectiveness of prompt engineering techniques (Chain of Thought Prompting and Task Instruction Prompting) in reducing IP infringement risks.", "result": "These techniques significantly reduce similarity between generated images and training data.", "conclusion": "Prompt engineering is a viable method to mitigate IP infringement risks in visual generative AI."}}
{"id": "2502.00865", "pdf": "https://arxiv.org/pdf/2502.00865", "abs": "https://arxiv.org/abs/2502.00865", "authors": ["Christoffer Loeffler", "Andrea Mart\u00ednez Freile", "Tom\u00e1s Rey Pizarro"], "title": "Predicting potentially abusive clauses in Chilean terms of services with natural language processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "39 pages, 2 figures, 8 tables, accepted for publication", "summary": "This study addresses the growing concern of information asymmetry in consumer\ncontracts, exacerbated by the proliferation of online services with complex\nTerms of Service that are rarely even read. Even though research on automatic\nanalysis methods is conducted, the problem is aggravated by the general focus\non English-language Machine Learning approaches and on major jurisdictions,\nsuch as the European Union. We introduce a new methodology and a substantial\ndataset addressing this gap. We propose a novel annotation scheme with four\ncategories and a total of 20 classes, and apply it on 50 online Terms of\nService used in Chile. Our evaluation of transformer-based models highlights\nhow factors like language- and/or domain-specific pre-training, few-shot sample\nsize, and model architecture affect the detection and classification of\npotentially abusive clauses. Results show a large variability in performance\nfor the different tasks and models, with the highest macro-F1 scores for the\ndetection task ranging from 79% to 89% and micro-F1 scores up to 96%, while\nmacro-F1 scores for the classification task range from 60% to 70% and micro-F1\nscores from 64% to 80%. Notably, this is the first Spanish-language multi-label\nclassification dataset for legal clauses, applying Chilean law and offering a\ncomprehensive evaluation of Spanish-language models in the legal domain. Our\nwork lays the ground for future research in method development for rarely\nconsidered legal analysis and potentially leads to practical applications to\nsupport consumers in Chile and Latin America as a whole.", "AI": {"tldr": "The study tackles information asymmetry in consumer contracts by analyzing Spanish-language Terms of Service in Chile, introducing a new dataset and methodology. Transformer models show varied performance in detecting and classifying abusive clauses.", "motivation": "Address the gap in research focused on English and major jurisdictions by analyzing Spanish-language contracts in Chile.", "method": "Proposes a novel annotation scheme (4 categories, 20 classes) applied to 50 Chilean Terms of Service. Evaluates transformer models for detection and classification tasks.", "result": "Detection task achieves macro-F1 scores of 79%-89% and micro-F1 up to 96%. Classification task scores 60%-70% macro-F1 and 64%-80% micro-F1.", "conclusion": "First Spanish-language multi-label dataset for legal clauses, paving the way for future research and consumer support in Latin America."}}
{"id": "2505.03422", "pdf": "https://arxiv.org/pdf/2505.03422", "abs": "https://arxiv.org/abs/2505.03422", "authors": ["Yepeng Liu", "Wenpeng Lai", "Zhou Zhao", "Yuxuan Xiong", "Jinchi Zhu", "Jun Cheng", "Yongchao Xu"], "title": "LiftFeat: 3D Geometry-Aware Local Feature Matching", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at ICRA 2025", "summary": "Robust and efficient local feature matching plays a crucial role in\napplications such as SLAM and visual localization for robotics. Despite great\nprogress, it is still very challenging to extract robust and discriminative\nvisual features in scenarios with drastic lighting changes, low texture areas,\nor repetitive patterns. In this paper, we propose a new lightweight network\ncalled \\textit{LiftFeat}, which lifts the robustness of raw descriptor by\naggregating 3D geometric feature. Specifically, we first adopt a pre-trained\nmonocular depth estimation model to generate pseudo surface normal label,\nsupervising the extraction of 3D geometric feature in terms of predicted\nsurface normal. We then design a 3D geometry-aware feature lifting module to\nfuse surface normal feature with raw 2D descriptor feature. Integrating such 3D\ngeometric feature enhances the discriminative ability of 2D feature description\nin extreme conditions. Extensive experimental results on relative pose\nestimation, homography estimation, and visual localization tasks, demonstrate\nthat our LiftFeat outperforms some lightweight state-of-the-art methods. Code\nwill be released at : https://github.com/lyp-deeplearning/LiftFeat.", "AI": {"tldr": "The paper introduces LiftFeat, a lightweight network that enhances local feature matching by integrating 3D geometric features with raw 2D descriptors, improving robustness in challenging conditions.", "motivation": "Local feature matching is critical for robotics applications like SLAM and visual localization, but existing methods struggle with lighting changes, low texture, or repetitive patterns.", "method": "LiftFeat uses a pre-trained monocular depth model to generate pseudo surface normal labels, supervises 3D geometric feature extraction, and fuses these with 2D descriptors via a 3D geometry-aware module.", "result": "LiftFeat outperforms state-of-the-art lightweight methods in tasks like pose estimation, homography estimation, and visual localization.", "conclusion": "Integrating 3D geometric features significantly boosts the discriminative power of 2D descriptors, making LiftFeat robust in extreme conditions."}}
{"id": "2505.03530", "pdf": "https://arxiv.org/pdf/2505.03530", "abs": "https://arxiv.org/abs/2505.03530", "authors": ["Dip Roy"], "title": "Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability", "categories": ["cs.LG"], "comment": null, "summary": "Mechanistic interpretability of deep learning models has emerged as a crucial\nresearch direction for understanding the functioning of neural networks. While\nsignificant progress has been made in interpreting discriminative models like\ntransformers, understanding generative models such as Variational Autoencoders\n(VAEs) remains challenging. This paper introduces a comprehensive causal\nintervention framework for mechanistic interpretability of VAEs. We develop\ntechniques to identify and analyze \"circuit motifs\" in VAEs, examining how\nsemantic factors are encoded, processed, and disentangled through the network\nlayers. Our approach uses targeted interventions at different levels: input\nmanipulations, latent space perturbations, activation patching, and causal\nmediation analysis. We apply our framework to both synthetic datasets with\nknown causal relationships and standard disentanglement benchmarks. Results\nshow that our interventions can successfully isolate functional circuits, map\ncomputational graphs to causal graphs of semantic factors, and distinguish\nbetween polysemantic and monosemantic units. Furthermore, we introduce metrics\nfor causal effect strength, intervention specificity, and circuit modularity\nthat quantify the interpretability of VAE components. Experimental results\ndemonstrate clear differences between VAE variants, with FactorVAE achieving\nhigher disentanglement scores (0.084) and effect strengths (mean 4.59) compared\nto standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework\nadvances the mechanistic understanding of generative models and provides tools\nfor more transparent and controllable VAE architectures.", "AI": {"tldr": "A causal intervention framework for mechanistic interpretability of VAEs is introduced, isolating functional circuits and quantifying interpretability metrics.", "motivation": "Understanding generative models like VAEs is challenging compared to discriminative models, necessitating tools for mechanistic interpretability.", "method": "The paper develops techniques to analyze circuit motifs in VAEs using targeted interventions (input manipulations, latent space perturbations, activation patching, causal mediation) and applies them to synthetic and benchmark datasets.", "result": "The framework successfully isolates functional circuits, maps computational to causal graphs, and introduces interpretability metrics. FactorVAE outperforms standard and Beta-VAE in disentanglement scores (0.084 vs. 0.064, 0.051) and effect strengths (4.59 vs. 3.99, 3.43).", "conclusion": "The framework enhances mechanistic understanding of VAEs, offering tools for more transparent and controllable generative models."}}
{"id": "2505.03426", "pdf": "https://arxiv.org/pdf/2505.03426", "abs": "https://arxiv.org/abs/2505.03426", "authors": ["Ziyu Li", "Yujian Hu", "Zhengyao Ding", "Yiheng Mao", "Haitao Li", "Fan Yi", "Hongkun Zhang", "Zhengxing Huang"], "title": "Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for\ndiagnosing heart diseases and evaluating cardiac health. However, the limited\navailability of large-scale, high-quality CMR datasets poses a major challenge\nto the effective application of artificial intelligence (AI) in this domain.\nEven the amount of unlabeled data and the health status it covers are difficult\nto meet the needs of model pretraining, which hinders the performance of AI\nmodels on downstream tasks. In this study, we present Cardiac Phenotype-Guided\nCMR Generation (CPGG), a novel approach for generating diverse CMR data that\ncovers a wide spectrum of cardiac health status. The CPGG framework consists of\ntwo stages: in the first stage, a generative model is trained using cardiac\nphenotypes derived from CMR data; in the second stage, a masked autoregressive\ndiffusion model, conditioned on these phenotypes, generates high-fidelity CMR\ncine sequences that capture both structural and functional features of the\nheart in a fine-grained manner. We synthesized a massive amount of CMR to\nexpand the pretraining data. Experimental results show that CPGG generates\nhigh-quality synthetic CMR data, significantly improving performance on various\ndownstream tasks, including diagnosis and cardiac phenotypes prediction. These\ngains are demonstrated across both public and private datasets, highlighting\nthe effectiveness of our approach. Code is availabel at\nhttps://anonymous.4open.science/r/CPGG.", "AI": {"tldr": "CPGG generates diverse, high-quality synthetic CMR data using cardiac phenotypes, improving AI model performance on downstream tasks.", "motivation": "Limited availability of large-scale, high-quality CMR datasets hinders AI applications in cardiac health.", "method": "Two-stage framework: 1) Train generative model with cardiac phenotypes; 2) Use masked autoregressive diffusion model to generate CMR cine sequences.", "result": "CPGG produces high-fidelity synthetic CMR data, enhancing performance in diagnosis and phenotype prediction.", "conclusion": "CPGG effectively addresses data scarcity, boosting AI model performance in cardiac health tasks."}}
{"id": "2502.07963", "pdf": "https://arxiv.org/pdf/2502.07963", "abs": "https://arxiv.org/abs/2502.07963", "authors": ["Hye Sun Yun", "Karen Y. C. Zhang", "Ramez Kouzy", "Iain J. Marshall", "Junyi Jessy Li", "Byron C. Wallace"], "title": "Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures, 4 tables, CHIL 2025", "summary": "Medical research faces well-documented challenges in translating novel\ntreatments into clinical practice. Publishing incentives encourage researchers\nto present \"positive\" findings, even when empirical results are equivocal.\nConsequently, it is well-documented that authors often spin study results,\nespecially in article abstracts. Such spin can influence clinician\ninterpretation of evidence and may affect patient care decisions. In this\nstudy, we ask whether the interpretation of trial results offered by Large\nLanguage Models (LLMs) is similarly affected by spin. This is important since\nLLMs are increasingly being used to trawl through and synthesize published\nmedical evidence. We evaluated 22 LLMs and found that they are across the board\nmore susceptible to spin than humans. They might also propagate spin into their\noutputs: We find evidence, e.g., that LLMs implicitly incorporate spin into\nplain language summaries that they generate. We also find, however, that LLMs\nare generally capable of recognizing spin, and can be prompted in a way to\nmitigate spin's impact on LLM outputs.", "AI": {"tldr": "LLMs are more susceptible to spin in medical abstracts than humans but can recognize and mitigate it with proper prompting.", "motivation": "To investigate if LLMs, like humans, are influenced by spin in medical research abstracts, given their growing role in synthesizing medical evidence.", "method": "Evaluated 22 LLMs to assess their susceptibility to spin and ability to recognize and mitigate it.", "result": "LLMs are more affected by spin than humans but can detect and reduce its impact when prompted appropriately.", "conclusion": "While LLMs are vulnerable to spin, they can be guided to minimize its influence, highlighting the need for careful use in medical evidence synthesis."}}
{"id": "2505.03431", "pdf": "https://arxiv.org/pdf/2505.03431", "abs": "https://arxiv.org/abs/2505.03431", "authors": ["Usman Muhammad", "Jorma Laaksonen"], "title": "A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "The fusion of low-spatial-resolution hyperspectral images (HSIs) with\nhigh-spatial-resolution conventional images (e.g., panchromatic or RGB) has\nplayed a significant role in recent advancements in HSI super-resolution.\nHowever, this fusion process relies on the availability of precise alignment\nbetween image pairs, which is often challenging in real-world scenarios. To\nmitigate this limitation, we propose a single-image super-resolution model\ncalled the Fusion-Guided Inception Network (FGIN). Specifically, we first\nemploy a spectral-spatial fusion module to effectively integrate spectral and\nspatial information at an early stage. Next, an Inception-like hierarchical\nfeature extraction strategy is used to capture multiscale spatial dependencies,\nfollowed by a dedicated multi-scale fusion block. To further enhance\nreconstruction quality, we incorporate an optimized upsampling module that\ncombines bilinear interpolation with depthwise separable convolutions.\nExperimental evaluations on two publicly available hyperspectral datasets\ndemonstrate the competitive performance of our method.", "AI": {"tldr": "Proposes FGIN, a single-image super-resolution model for hyperspectral images, addressing alignment challenges by integrating spectral-spatial fusion and multiscale feature extraction.", "motivation": "Overcome the reliance on precise alignment between image pairs in HSI super-resolution by developing a single-image solution.", "method": "Uses a spectral-spatial fusion module, Inception-like hierarchical feature extraction, multi-scale fusion block, and optimized upsampling with bilinear interpolation and depthwise separable convolutions.", "result": "Demonstrates competitive performance on two public hyperspectral datasets.", "conclusion": "FGIN effectively addresses alignment challenges and enhances HSI super-resolution quality."}}
{"id": "2505.03533", "pdf": "https://arxiv.org/pdf/2505.03533", "abs": "https://arxiv.org/abs/2505.03533", "authors": ["Jiacheng Wang", "Le Liang", "Hao Ye", "Chongtao Guo", "Shi Jin"], "title": "Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Judicious resource allocation can effectively enhance federated learning (FL)\ntraining performance in wireless networks by addressing both system and\nstatistical heterogeneity. However, existing strategies typically rely on block\nfading assumptions, which overlooks rapid channel fluctuations within each\nround of FL gradient uploading, leading to a degradation in FL training\nperformance. Therefore, this paper proposes a small-scale-fading-aware resource\nallocation strategy using a multi-agent reinforcement learning (MARL)\nframework. Specifically, we establish a one-step convergence bound of the FL\nalgorithm and formulate the resource allocation problem as a decentralized\npartially observable Markov decision process (Dec-POMDP), which is subsequently\nsolved using the QMIX algorithm. In our framework, each client serves as an\nagent that dynamically determines spectrum and power allocations within each\ncoherence time slot, based on local observations and a reward derived from the\nconvergence analysis. The MARL setting reduces the dimensionality of the action\nspace and facilitates decentralized decision-making, enhancing the scalability\nand practicality of the solution. Experimental results demonstrate that our\nQMIX-based resource allocation strategy significantly outperforms baseline\nmethods across various degrees of statistical heterogeneity. Additionally,\nablation studies validate the critical importance of incorporating small-scale\nfading dynamics, highlighting its role in optimizing FL performance.", "AI": {"tldr": "The paper proposes a MARL-based resource allocation strategy for FL in wireless networks, addressing small-scale fading dynamics to improve training performance.", "motivation": "Existing FL resource allocation strategies ignore rapid channel fluctuations, degrading performance. This work aims to address this gap.", "method": "Uses a MARL framework (QMIX) to solve the resource allocation problem formulated as a Dec-POMDP, with clients dynamically allocating resources.", "result": "The QMIX-based strategy outperforms baselines, especially under statistical heterogeneity, and ablation studies confirm the importance of small-scale fading dynamics.", "conclusion": "The proposed method enhances FL performance by effectively addressing small-scale fading and scalability through decentralized MARL."}}
{"id": "2505.03451", "pdf": "https://arxiv.org/pdf/2505.03451", "abs": "https://arxiv.org/abs/2505.03451", "authors": ["Fouad Trad", "Ali Chehab"], "title": "Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis", "categories": ["cs.CR", "cs.AI"], "comment": "Accepted in 8th International Conference on Optimization and Learning\n  (OLA2025)", "summary": "The rise of QR code based phishing (\"Quishing\") poses a growing cybersecurity\nthreat, as attackers increasingly exploit QR codes to bypass traditional\nphishing defenses. Existing detection methods predominantly focus on URL\nanalysis, which requires the extraction of the QR code payload, and may\ninadvertently expose users to malicious content. Moreover, QR codes can encode\nvarious types of data beyond URLs, such as Wi-Fi credentials and payment\ninformation, making URL-based detection insufficient for broader security\nconcerns. To address these gaps, we propose the first framework for quishing\ndetection that directly analyzes QR code structure and pixel patterns without\nextracting the embedded content. We generated a dataset of phishing and benign\nQR codes and we used it to train and evaluate multiple machine learning models,\nincluding Logistic Regression, Decision Trees, Random Forest, Naive Bayes,\nLightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of\n0.9106, demonstrating the feasibility of QR-centric detection. Through feature\nimportance analysis, we identify key visual indicators of malicious intent and\nrefine our feature set by removing non-informative pixels, improving\nperformance to an AUC of 0.9133 with a reduced feature space. Our findings\nreveal that the structural features of QR code correlate strongly with phishing\nrisk. This work establishes a foundation for quishing mitigation and highlights\nthe potential of direct QR analysis as a critical layer in modern phishing\ndefenses.", "AI": {"tldr": "Proposes a QR code phishing detection framework analyzing structure and pixel patterns without content extraction, achieving high accuracy with XGBoost.", "motivation": "Addresses gaps in URL-based detection by focusing on QR code structure, as QR codes encode diverse data beyond URLs, posing broader security risks.", "method": "Generates a dataset of phishing/benign QR codes, trains/evaluates multiple ML models (e.g., XGBoost), and refines features based on importance analysis.", "result": "Best model (XGBoost) achieves AUC 0.9106, improved to 0.9133 after feature refinement, showing structural features correlate with phishing risk.", "conclusion": "Demonstrates feasibility of QR-centric detection, offering a foundation for quishing mitigation and enhancing phishing defenses."}}
{"id": "2502.13685", "pdf": "https://arxiv.org/pdf/2502.13685", "abs": "https://arxiv.org/abs/2502.13685", "authors": ["Jusen Du", "Weigao Sun", "Disen Lan", "Jiaxi Hu", "Yu Cheng"], "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical report, 16 pages", "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.", "AI": {"tldr": "MoM introduces multiple independent memory states to enhance recall performance while maintaining linear complexity, outperforming existing linear sequence models.", "motivation": "Addressing the suboptimal performance of linear sequence models on recall-intensive tasks due to single fixed-size memory states.", "method": "Uses multiple memory states with a router network to direct input tokens, minimizing interference and enhancing capacity.", "result": "MoM outperforms linear sequence models on recall tasks and matches Transformer performance, with linear training and constant inference complexity.", "conclusion": "MoM offers a scalable, efficient solution for recall-intensive tasks, combining neuroscience-inspired design with computational efficiency."}}
{"id": "2505.03435", "pdf": "https://arxiv.org/pdf/2505.03435", "abs": "https://arxiv.org/abs/2505.03435", "authors": ["Sun Haoxuan", "Hong Yan", "Zhan Jiahui", "Chen Haoxing", "Lan Jun", "Zhu Huijia", "Wang Weiqiang", "Zhang Liqing", "Zhang Jianfu"], "title": "Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of generative image technology has introduced\nsignificant security concerns, particularly in the domain of face generation\ndetection. This paper investigates the vulnerabilities of current AI-generated\nface detection systems. Our study reveals that while existing detection methods\noften achieve high accuracy under standard conditions, they exhibit limited\nrobustness against adversarial attacks. To address these challenges, we propose\nan approach that integrates adversarial training to mitigate the impact of\nadversarial examples. Furthermore, we utilize diffusion inversion and\nreconstruction to further enhance detection robustness. Experimental results\ndemonstrate that minor adversarial perturbations can easily bypass existing\ndetection systems, but our method significantly improves the robustness of\nthese systems. Additionally, we provide an in-depth analysis of adversarial and\nbenign examples, offering insights into the intrinsic characteristics of\nAI-generated content. All associated code will be made publicly available in a\ndedicated repository to facilitate further research and verification.", "AI": {"tldr": "The paper highlights vulnerabilities in AI-generated face detection systems and proposes adversarial training and diffusion inversion to improve robustness against adversarial attacks.", "motivation": "Addressing security concerns in generative image technology, particularly the limited robustness of current AI-generated face detection systems against adversarial attacks.", "method": "Proposes integrating adversarial training and using diffusion inversion and reconstruction to enhance detection robustness.", "result": "Minor adversarial perturbations can bypass existing systems, but the proposed method significantly improves robustness.", "conclusion": "The study provides insights into AI-generated content characteristics and makes code publicly available for further research."}}
{"id": "2505.03552", "pdf": "https://arxiv.org/pdf/2505.03552", "abs": "https://arxiv.org/abs/2505.03552", "authors": ["Linus Langenkamp", "Philip Hannebohm", "Bernhard Bachmann"], "title": "Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming", "categories": ["cs.LG", "math.DS", "math.OC", "90C30, 68T05", "G.1.6; I.2.6"], "comment": "16 pages, 9 figures, submitted to 16th International Modelica & FMI\n  Conference", "summary": "We propose a novel approach for training Physics-enhanced Neural ODEs\n(PeNODEs) by expressing the training process as a dynamic optimization problem.\nThe full model, including neural components, is discretized using a high-order\nimplicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting\nin a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art\nNLP solvers such as Ipopt. This formulation enables simultaneous optimization\nof network parameters and state trajectories, addressing key limitations of ODE\nsolver-based training in terms of stability, runtime, and accuracy. Extending\non a recent direct collocation-based method for Neural ODEs, we generalize to\nPeNODEs, incorporate physical constraints, and present a custom, parallelized,\nopen-source implementation. Benchmarks on a Quarter Vehicle Model and a\nVan-der-Pol oscillator demonstrate superior accuracy, speed, and generalization\nwith smaller networks compared to other training techniques. We also outline a\nplanned integration into OpenModelica to enable accessible training of Neural\nDAEs.", "AI": {"tldr": "A novel method trains Physics-enhanced Neural ODEs (PeNODEs) by framing training as a dynamic optimization problem, using high-order implicit Runge-Kutta discretization and NLP solvers for efficient, stable, and accurate results.", "motivation": "To overcome limitations of ODE solver-based training (stability, runtime, accuracy) and extend Neural ODEs to physics-enhanced models with constraints.", "method": "Discretize the model using high-order implicit Runge-Kutta with flipped Legendre-Gauss-Radau points, forming a large-scale NLP solved by solvers like Ipopt. Includes parallelized, open-source implementation.", "result": "Benchmarks show superior accuracy, speed, and generalization with smaller networks compared to other methods.", "conclusion": "The approach advances PeNODEs training and plans integration into OpenModelica for broader accessibility."}}
{"id": "2505.03470", "pdf": "https://arxiv.org/pdf/2505.03470", "abs": "https://arxiv.org/abs/2505.03470", "authors": ["Vibhas Vats", "Md. Alimoor Reza", "David Crandall", "Soon-heung Jung"], "title": "Blending 3D Geometry and Machine Learning for Multi-View Stereopsis", "categories": ["cs.CV", "cs.AI", "cs.CG", "cs.LG"], "comment": "A pre-print -- paper under-review. arXiv admin note: substantial text\n  overlap with arXiv:2310.19583", "summary": "Traditional multi-view stereo (MVS) methods primarily depend on photometric\nand geometric consistency constraints. In contrast, modern learning-based\nalgorithms often rely on the plane sweep algorithm to infer 3D geometry,\napplying explicit geometric consistency (GC) checks only as a post-processing\nstep, with no impact on the learning process itself. In this work, we introduce\nGC MVSNet plus plus, a novel approach that actively enforces geometric\nconsistency of reference view depth maps across multiple source views (multi\nview) and at various scales (multi scale) during the learning phase (see Fig.\n1). This integrated GC check significantly accelerates the learning process by\ndirectly penalizing geometrically inconsistent pixels, effectively halving the\nnumber of training iterations compared to other MVS methods. Furthermore, we\nintroduce a densely connected cost regularization network with two distinct\nblock designs simple and feature dense optimized to harness dense feature\nconnections for enhanced regularization. Extensive experiments demonstrate that\nour approach achieves a new state of the art on the DTU and BlendedMVS datasets\nand secures second place on the Tanks and Temples benchmark. To our knowledge,\nGC MVSNet plus plus is the first method to enforce multi-view, multi-scale\nsupervised geometric consistency during learning. Our code is available.", "AI": {"tldr": "GC MVSNet++ integrates multi-view, multi-scale geometric consistency during learning, improving efficiency and performance in MVS tasks.", "motivation": "Traditional MVS methods rely on post-processing GC checks, while learning-based methods ignore GC during training. This work aims to enforce GC actively during learning for better results.", "method": "Introduces GC MVSNet++, enforcing multi-view, multi-scale GC checks during training. Uses a densely connected cost regularization network with two block designs for enhanced regularization.", "result": "Achieves state-of-the-art on DTU and BlendedMVS datasets, and second place on Tanks and Temples benchmark. Training iterations are halved.", "conclusion": "GC MVSNet++ is the first to enforce supervised GC during learning, significantly improving MVS performance and efficiency."}}
{"id": "2502.14338", "pdf": "https://arxiv.org/pdf/2502.14338", "abs": "https://arxiv.org/abs/2502.14338", "authors": ["Avinash Patil", "Aryan Jadon"], "title": "English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports", "categories": ["cs.CL", "cs.SE"], "comment": "8 Pages, 4 Figures, 3 Tables", "summary": "Accurate translation of bug reports is critical for efficient collaboration\nin global software development. In this study, we conduct the first\ncomprehensive evaluation of machine translation (MT) performance on bug\nreports, analyzing the capabilities of DeepL, AWS Translate, and large language\nmodels such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the\nVisual Studio Code GitHub repository, specifically focusing on reports labeled\nwith the english-please tag. To assess both translation quality and source\nlanguage identification accuracy, we employ a range of MT evaluation\nmetrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside\nclassification metrics such as accuracy, precision, recall, and F1-score. Our\nfindings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical\ntranslation quality, it does not lead in source language identification. Claude\nand Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively),\nand Gemini records the best precision (0.7414). AWS Translate shows the highest\naccuracy (0.4717) in identifying source languages. These results highlight that\nno single system dominates across all tasks, reinforcing the importance of\ntask-specific evaluations. This study underscores the need for domain\nadaptation when translating technical content and provides actionable insights\nfor integrating MT into bug-triaging workflows. The code and dataset for this\npaper are available at GitHub-https://github.com/av9ash/English-Please", "AI": {"tldr": "The study evaluates machine translation (MT) performance on bug reports, comparing tools like DeepL, AWS Translate, and LLMs (ChatGPT, Claude, Gemini, etc.) using metrics like BLEU and BERTScore. ChatGPT excels in translation quality, while Claude and Mistral lead in source language identification. No single tool dominates all tasks.", "motivation": "Accurate translation of bug reports is crucial for global software development collaboration, necessitating a comprehensive evaluation of MT tools.", "method": "The study evaluates MT tools (DeepL, AWS Translate, ChatGPT, etc.) using bug reports from the Visual Studio Code GitHub repository. Metrics include BLEU, BERTScore, and classification metrics (accuracy, F1-score).", "result": "ChatGPT (gpt-4o) leads in translation quality but not in source language identification. Claude and Mistral perform best in F1-score, while AWS Translate excels in accuracy.", "conclusion": "No single MT tool outperforms others in all tasks, emphasizing the need for task-specific evaluations and domain adaptation for technical content."}}
{"id": "2505.03445", "pdf": "https://arxiv.org/pdf/2505.03445", "abs": "https://arxiv.org/abs/2505.03445", "authors": ["Qi Gan", "Sao Mai Nguyen", "Eric Fenaux", "Stephan Cl\u00e9men\u00e7on", "Moun\u00eem El Yacoubi"], "title": "Polar Coordinate-Based 2D Pose Prior with Neural Distance Field", "categories": ["cs.CV"], "comment": "This paper is accepted by CVPRW 2025", "summary": "Human pose capture is essential for sports analysis, enabling precise\nevaluation of athletes' movements. While deep learning-based human pose\nestimation (HPE) models from RGB videos have achieved impressive performance on\npublic datasets, their effectiveness in real-world sports scenarios is often\nhindered by motion blur, occlusions, and domain shifts across different pose\nrepresentations. Fine-tuning these models can partially alleviate such\nchallenges but typically requires large-scale annotated data and still\nstruggles to generalize across diverse sports environments. To address these\nlimitations, we propose a 2D pose prior-guided refinement approach based on\nNeural Distance Fields (NDF). Unlike existing approaches that rely solely on\nangular representations of human poses, we introduce a polar coordinate-based\nrepresentation that explicitly incorporates joint connection lengths, enabling\na more accurate correction of erroneous pose estimations. Additionally, we\ndefine a novel non-geodesic distance metric that separates angular and radial\ndiscrepancies, which we demonstrate is better suited for polar representations\nthan traditional geodesic distances. To mitigate data scarcity, we develop a\ngradient-based batch-projection augmentation strategy, which synthesizes\nrealistic pose samples through iterative refinement. Our method is evaluated on\na long jump dataset, demonstrating its ability to improve 2D pose estimation\nacross multiple pose representations, making it robust across different\ndomains. Experimental results show that our approach enhances pose plausibility\nwhile requiring only limited training data. Code is available at:\nhttps://github.com/QGAN2019/polar-NDF.", "AI": {"tldr": "The paper proposes a 2D pose refinement method using Neural Distance Fields (NDF) with polar coordinate-based representations to improve human pose estimation in sports, addressing challenges like motion blur and domain shifts.", "motivation": "Current deep learning-based pose estimation models struggle in real-world sports due to motion blur, occlusions, and domain shifts. Fine-tuning requires large annotated datasets and lacks generalization.", "method": "The approach uses NDF with polar coordinates, incorporating joint lengths and a novel non-geodesic distance metric. It also employs gradient-based batch-projection augmentation to mitigate data scarcity.", "result": "Tested on a long jump dataset, the method improves 2D pose estimation across domains, enhancing plausibility with limited training data.", "conclusion": "The proposed method effectively refines pose estimations in sports scenarios, offering robustness and efficiency with minimal data requirements."}}
{"id": "2505.03560", "pdf": "https://arxiv.org/pdf/2505.03560", "abs": "https://arxiv.org/abs/2505.03560", "authors": ["Simon Baeuerle", "Ian F. Mendonca", "Kristof Van Laerhoven", "Ralf Mikut", "Andreas Steimer"], "title": "Rapid AI-based generation of coverage paths for dispensing applications", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial\nrole in the design of power electronics and electronic control units. Up to\nnow, this is done manually by experts or by using optimization approaches with\na high computational effort. We propose a novel AI-based approach to generate\ndispense paths for TIM and similar dispensing applications. It is a drop-in\nreplacement for optimization-based approaches. An Artificial Neural Network\n(ANN) receives the target cooling area as input and directly outputs the\ndispense path. Our proposed setup does not require labels and we show its\nfeasibility on multiple target areas. The resulting dispense paths can be\ndirectly transferred to automated manufacturing equipment and do not exhibit\nair entrapments. The approach of using an ANN to predict process parameters for\na desired target state in real-time could potentially be transferred to other\nmanufacturing processes.", "AI": {"tldr": "An AI-based approach using an ANN to generate dispense paths for TIM, replacing manual or optimization methods, without needing labels and avoiding air entrapments.", "motivation": "Manual or optimization-based methods for TIM coverage path planning are time-consuming or computationally expensive.", "method": "An ANN takes the target cooling area as input and directly outputs the dispense path, eliminating the need for labels.", "result": "Feasible dispense paths are generated without air entrapments, ready for automated manufacturing.", "conclusion": "The ANN-based approach is efficient and could be adapted for other manufacturing processes."}}
{"id": "2505.03492", "pdf": "https://arxiv.org/pdf/2505.03492", "abs": "https://arxiv.org/abs/2505.03492", "authors": ["Xiaoan Liu"], "title": "Augmenting Human Cognition through Everyday AR", "categories": ["cs.HC", "cs.AI"], "comment": "3 pages, 4 figures. Position paper accepted to CHI'25 Workshop\n  'Everyday AR through AI-in-the-Loop'", "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.", "AI": {"tldr": "The paper discusses how always-on AR, powered by spatial computing and multimodal LLMs, can integrate digital intelligence into physical environments to improve human interactions and task performance.", "motivation": "To explore the potential of AR as a 'thinking tool' that combines digital cognition with physical affordances for enhanced human understanding and performance.", "method": "Leveraging spatial computing and multimodal LLMs to create context-aware, proactive AR interactions.", "result": "AR can seamlessly bridge digital and physical worlds, enabling intuitive, context-sensitive enhancements to human tasks.", "conclusion": "Always-on AR has the potential to transform everyday environments into intelligent, interactive spaces."}}
{"id": "2502.19187", "pdf": "https://arxiv.org/pdf/2502.19187", "abs": "https://arxiv.org/abs/2502.19187", "authors": ["Mehran Kazemi", "Bahare Fatemi", "Hritik Bansal", "John Palowitch", "Chrysovalantis Anastasiou", "Sanket Vaibhav Mehta", "Lalit K. Jain", "Virginia Aglietti", "Disha Jindal", "Peter Chen", "Nishanth Dikkala", "Gladys Tyen", "Xin Liu", "Uri Shalit", "Silvia Chiappa", "Kate Olszewska", "Yi Tay", "Vinh Q. Tran", "Quoc V. Le", "Orhan Firat"], "title": "BIG-Bench Extra Hard", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in everyday\napplications, demanding robust general reasoning capabilities and diverse\nreasoning skillset. However, current LLM reasoning benchmarks predominantly\nfocus on mathematical and coding abilities, leaving a gap in evaluating broader\nreasoning proficiencies. One particular exception is the BIG-Bench dataset,\nwhich has served as a crucial benchmark for evaluating the general reasoning\ncapabilities of LLMs, thanks to its diverse set of challenging tasks that\nallowed for a comprehensive assessment of general reasoning across various\nskills within a unified framework. However, recent advances in LLMs have led to\nsaturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH).\nState-of-the-art models achieve near-perfect scores on many tasks in BBH, thus\ndiminishing its utility. To address this limitation, we introduce BIG-Bench\nExtra Hard (BBEH), a new benchmark designed to push the boundaries of LLM\nreasoning evaluation. BBEH replaces each task in BBH with a novel task that\nprobes a similar reasoning capability but exhibits significantly increased\ndifficulty. We evaluate various models on BBEH and observe a (harmonic) average\naccuracy of 9.8\\% for the best general-purpose model and 44.8\\% for the best\nreasoning-specialized model, indicating substantial room for improvement and\nhighlighting the ongoing challenge of achieving robust general reasoning in\nLLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.", "AI": {"tldr": "The paper introduces BIG-Bench Extra Hard (BBEH), a new benchmark to evaluate advanced reasoning in LLMs, as current benchmarks like BIG-Bench and BBH are saturated.", "motivation": "Existing benchmarks (BIG-Bench, BBH) are no longer challenging for state-of-the-art LLMs, limiting their utility for evaluating broader reasoning skills.", "method": "BBEH replaces tasks in BBH with novel, harder tasks probing similar reasoning capabilities.", "result": "Best models achieved 9.8% (general-purpose) and 44.8% (specialized) accuracy, showing significant room for improvement.", "conclusion": "BBEH highlights the ongoing challenge of robust general reasoning in LLMs and provides a new benchmark for future evaluations."}}
{"id": "2505.03463", "pdf": "https://arxiv.org/pdf/2505.03463", "abs": "https://arxiv.org/abs/2505.03463", "authors": ["Muge Du", "Zhuozhao Zheng", "Wenying Wang", "Guotao Quan", "Wuliang Shi", "Le Shen", "Li Zhang", "Liang Li", "Yinong Liu", "Yuxiang Xing"], "title": "Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Dynamic computed tomography (CT) reconstruction faces significant challenges\nin addressing motion artifacts, particularly for nonperiodic rapid movements\nsuch as cardiac imaging with fast heart rates. Traditional methods struggle\nwith the extreme limited-angle problems inherent in nonperiodic cases. Deep\nlearning methods have improved performance but face generalization challenges.\nRecent implicit neural representation (INR) techniques show promise through\nself-supervised deep learning, but have critical limitations: computational\ninefficiency due to forward-warping modeling, difficulty balancing DVF\ncomplexity with anatomical plausibility, and challenges in preserving fine\ndetails without additional patient-specific pre-scans. This paper presents a\nnovel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It\naddresses these challenges through four key contributions: (1) backward-warping\ndeformation that enables direct computation of each dynamic voxel with\nsignificantly reduced computational cost, (2) diffeomorphism-based DVF\nregularization that ensures anatomically plausible deformations while\nmaintaining representational capacity, (3) motion-compensated analytical\nreconstruction that enhances fine details without requiring additional\npre-scans, and (4) dimensional-reduction design for efficient 4D coordinate\nencoding. Through various simulations and practical studies, including digital\nand physical phantoms and retrospective patient data, we demonstrate the\neffectiveness of our approach for nonperiodic dynamic CT reconstruction with\nenhanced details and reduced motion artifacts. The proposed framework enables\nmore accurate dynamic CT reconstruction with potential clinical applications,\nsuch as one-beat cardiac reconstruction, cinematic image sequences for\nfunctional imaging, and motion artifact reduction in conventional CT scans.", "AI": {"tldr": "BIRD is a novel INR-based framework for nonperiodic dynamic CT reconstruction, addressing computational inefficiency, DVF complexity, and detail preservation through backward-warping, diffeomorphism-based regularization, motion-compensated reconstruction, and dimensional-reduction.", "motivation": "Traditional and deep learning methods struggle with motion artifacts in nonperiodic dynamic CT, especially for rapid movements like cardiac imaging. INR techniques show promise but face computational and detail-preservation challenges.", "method": "BIRD uses backward-warping deformation, diffeomorphism-based DVF regularization, motion-compensated analytical reconstruction, and dimensional-reduction for 4D coordinate encoding.", "result": "Simulations and studies show BIRD enhances detail and reduces motion artifacts in nonperiodic dynamic CT, enabling applications like one-beat cardiac reconstruction.", "conclusion": "BIRD improves dynamic CT reconstruction accuracy, offering clinical potential for cardiac imaging and motion artifact reduction."}}
{"id": "2505.03561", "pdf": "https://arxiv.org/pdf/2505.03561", "abs": "https://arxiv.org/abs/2505.03561", "authors": ["Leo Maxime Brunswic", "Mateo Clemente", "Rui Heng Yang", "Adam Sigal", "Amir Rasouli", "Yinchuan Li"], "title": "Ergodic Generative Flows", "categories": ["cs.LG", "cs.AI", "math.DG", "math.DS", "37A25, 68T07, 68W20, 68Q87, 68T99"], "comment": "20 pages, 5 figures, 1 table, accepted at ICML 2025", "summary": "Generative Flow Networks (GFNs) were initially introduced on directed acyclic\ngraphs to sample from an unnormalized distribution density. Recent works have\nextended the theoretical framework for generative methods allowing more\nflexibility and enhancing application range. However, many challenges remain in\ntraining GFNs in continuous settings and for imitation learning (IL), including\nintractability of flow-matching loss, limited tests of non-acyclic training,\nand the need for a separate reward model in imitation learning. The present\nwork proposes a family of generative flows called Ergodic Generative Flows\n(EGFs) which are used to address the aforementioned issues. First, we leverage\nergodicity to build simple generative flows with finitely many globally defined\ntransformations (diffeomorphisms) with universality guarantees and tractable\nflow-matching loss (FM loss). Second, we introduce a new loss involving\ncross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It\nis designed for IL training without a separate reward model. We evaluate\nIL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using\nthe KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning\nexperiments with a target reward, using the FM loss.", "AI": {"tldr": "The paper introduces Ergodic Generative Flows (EGFs) to address challenges in training GFNs, focusing on tractability and imitation learning without a separate reward model.", "motivation": "To overcome issues like intractable flow-matching loss and the need for reward models in imitation learning with GFNs.", "method": "Proposes EGFs using ergodicity for tractable FM loss and introduces KL-weakFM loss for imitation learning.", "result": "Evaluated on toy 2D tasks and NASA datasets, showing effectiveness in IL and reinforcement learning.", "conclusion": "EGFs provide a flexible and tractable solution for GFNs in continuous settings and imitation learning."}}
{"id": "2505.03510", "pdf": "https://arxiv.org/pdf/2505.03510", "abs": "https://arxiv.org/abs/2505.03510", "authors": ["Ludovico Iannello", "Luca Ciampi", "Gabriele Lagani", "Fabrizio Tonelli", "Eleonora Crocco", "Lucio Maria Calcagnile", "Angelo Di Garbo", "Federico Cremisi", "Giuseppe Amato"], "title": "From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce a novel paradigm for reservoir computing (RC)\nthat leverages a pool of cultured biological neurons as the reservoir\nsubstrate, creating a biological reservoir computing (BRC). This system\noperates similarly to an echo state network (ESN), with the key distinction\nthat the neural activity is generated by a network of cultured neurons, rather\nthan being modeled by traditional artificial computational units. The neuronal\nactivity is recorded using a multi-electrode array (MEA), which enables\nhigh-throughput recording of neural signals. In our approach, inputs are\nintroduced into the network through a subset of the MEA electrodes, while the\nremaining electrodes capture the resulting neural activity. This generates a\nnonlinear mapping of the input data to a high-dimensional biological feature\nspace, where distinguishing between data becomes more efficient and\nstraightforward, allowing a simple linear classifier to perform pattern\nrecognition tasks effectively. To evaluate the performance of our proposed\nsystem, we present an experimental study that includes various input patterns,\nsuch as positional codes, bars with different orientations, and a digit\nrecognition task. The results demonstrate the feasibility of using biological\nneural networks to perform tasks traditionally handled by artificial neural\nnetworks, paving the way for further exploration of biologically-inspired\ncomputing systems, with potential applications in neuromorphic engineering and\nbio-hybrid computing.", "AI": {"tldr": "A novel biological reservoir computing (BRC) system uses cultured neurons as a reservoir, outperforming traditional artificial methods for pattern recognition tasks.", "motivation": "To explore biologically-inspired computing by leveraging cultured neurons for reservoir computing, bridging artificial and biological neural networks.", "method": "Inputs are introduced via multi-electrode array (MEA) electrodes, with neural activity recorded and mapped to a high-dimensional feature space for classification.", "result": "The system effectively performs tasks like digit recognition, demonstrating the feasibility of biological neural networks for computing.", "conclusion": "BRC shows promise for neuromorphic engineering and bio-hybrid computing, expanding the scope of biologically-inspired systems."}}
{"id": "2503.10707", "pdf": "https://arxiv.org/pdf/2503.10707", "abs": "https://arxiv.org/abs/2503.10707", "authors": ["Zhiyuan Wang", "Katharine E. Daniel", "Laura E. Barnes", "Philip I. Chow"], "title": "CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Cancer survivors face unique emotional challenges that impact their quality\nof life. Mobile diary entries provide a promising method for tracking emotional\nstates, improving self-awareness, and promoting well-being outcome. This paper\naims to, through mobile diaries, understand cancer survivors' emotional states\nand key variables related to just-in-time intervention opportunities, including\nthe desire to regulate emotions and the availability to engage in\ninterventions. Although emotion analysis tools show potential for recognizing\nemotions from text, current methods lack the contextual understanding necessary\nto interpret brief mobile diary narratives. Our analysis of diary entries from\ncancer survivors (N=407) reveals systematic relationships between described\ncontexts and emotional states, with administrative and health-related contexts\nassociated with negative affect and regulation needs, while leisure activities\npromote positive emotions. We propose CALLM, a Context-Aware framework\nleveraging Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG) to analyze these brief entries by integrating retrieved peer experiences\nand personal diary history. CALLM demonstrates strong performance with balanced\naccuracies reaching 72.96% for positive affect, 73.29% for negative affect,\n73.72% for emotion regulation desire, and 60.09% for intervention availability,\noutperforming language model baselines. Post-hoc analysis reveals that model\nconfidence strongly predicts accuracy, with longer diary entries generally\nenhancing performance, and brief personalization periods yielding meaningful\nimprovements. Our findings demonstrate how contextual information in mobile\ndiaries can be effectively leveraged to understand emotional experiences,\npredict key states, and identify optimal intervention moments for personalized\njust-in-time support.", "AI": {"tldr": "The paper introduces CALLM, a context-aware framework using LLMs and RAG to analyze cancer survivors' mobile diary entries for emotional states and intervention opportunities, outperforming baselines.", "motivation": "Cancer survivors face emotional challenges, and mobile diaries offer a way to track emotions and improve well-being, but current tools lack contextual understanding.", "method": "The study analyzes diary entries from 407 cancer survivors, proposing CALLM, a framework combining LLMs and RAG to integrate peer experiences and personal history for better emotion analysis.", "result": "CALLM achieves balanced accuracies of 72.96% (positive affect), 73.29% (negative affect), 73.72% (emotion regulation desire), and 60.09% (intervention availability), outperforming baselines.", "conclusion": "Contextual information in mobile diaries can effectively predict emotional states and identify optimal intervention moments for personalized support."}}
{"id": "2505.03494", "pdf": "https://arxiv.org/pdf/2505.03494", "abs": "https://arxiv.org/abs/2505.03494", "authors": ["Zhanyuan Jia", "Ni Yao", "Danyang Sun", "Chuang Han", "Yanting Li", "Jiaofen Nan", "Fubao Zhu", "Chen Zhao", "Weihua Zhou"], "title": "UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion", "categories": ["cs.CV"], "comment": "21 pages, 7 figures", "summary": "Background: Brain tumor segmentation has a significant impact on the\ndiagnosis and treatment of brain tumors. Accurate brain tumor segmentation\nremains challenging due to their irregular shapes, vague boundaries, and high\nvariability. Objective: We propose a brain tumor segmentation method that\ncombines deep learning with prior knowledge derived from a region-growing\nalgorithm. Methods: The proposed method utilizes a multi-scale feature fusion\n(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale\nfeatures and capture global contextual information. To enhance the model's\nrobustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)\nstrategy is employed for uncertainty estimation. Results: Extensive experiments\ndemonstrate that the proposed method achieves superior performance on Brain\nTumor Segmentation (BraTS) datasets, significantly outperforming various\nstate-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are\n89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)\nsegmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019\nvalidation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for\nET, WT, and TC segmentation, respectively. Ablation studies further confirmed\nthe contribution of each module to segmentation accuracy, indicating that each\ncomponent played a vital role in overall performance improvement. Conclusion:\nThis study proposed a novel 3D brain tumor segmentation network based on the\nU-Net architecture. By incorporating the prior knowledge and employing the\nuncertainty estimation method, the robustness and performance were improved.\nThe code for the proposed method is available at\nhttps://github.com/chenzhao2023/UPMAD_Net_BrainSeg.", "AI": {"tldr": "A novel 3D brain tumor segmentation method combining deep learning with prior knowledge and uncertainty estimation, achieving superior performance on BraTS datasets.", "motivation": "Accurate brain tumor segmentation is challenging due to irregular shapes and vague boundaries, impacting diagnosis and treatment.", "method": "Uses multi-scale feature fusion (MSFF), adaptive attention mechanisms (AAM), and Monte Carlo Dropout (MC Dropout) for uncertainty estimation.", "result": "Achieves high Dice scores on BraTS datasets (e.g., 89.18% for ET, 93.67% for WT, 91.23% for TC on BraTS2021).", "conclusion": "The proposed U-Net-based network improves robustness and performance by integrating prior knowledge and uncertainty estimation."}}
{"id": "2505.03595", "pdf": "https://arxiv.org/pdf/2505.03595", "abs": "https://arxiv.org/abs/2505.03595", "authors": ["Sidharth S. Menon", "Ameya D. Jagtap"], "title": "Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs", "categories": ["cs.LG"], "comment": "27 pages, 13 figures", "summary": "High-dimensional partial differential equations (PDEs) arise in diverse\nscientific and engineering applications but remain computationally intractable\ndue to the curse of dimensionality. Traditional numerical methods struggle with\nthe exponential growth in computational complexity, particularly on hypercubic\ndomains, where the number of required collocation points increases rapidly with\ndimensionality. Here, we introduce Anant-Net, an efficient neural surrogate\nthat overcomes this challenge, enabling the solution of PDEs in high\ndimensions. Unlike hyperspheres, where the internal volume diminishes as\ndimensionality increases, hypercubes retain or expand their volume (for unit or\nlarger length), making high-dimensional computations significantly more\ndemanding. Anant-Net efficiently incorporates high-dimensional boundary\nconditions and minimizes the PDE residual at high-dimensional collocation\npoints. To enhance interpretability, we integrate Kolmogorov-Arnold networks\ninto the Anant-Net architecture. We benchmark Anant-Net's performance on\nseveral linear and nonlinear high-dimensional equations, including the Poisson,\nSine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and\nrobustness across randomly sampled test points from high-dimensional space.\nImportantly, Anant-Net achieves these results with remarkable efficiency,\nsolving 300-dimensional problems on a single GPU within a few hours. We also\ncompare Anant-Net's results for accuracy and runtime with other\nstate-of-the-art methods. Our findings establish Anant-Net as an accurate,\ninterpretable, and scalable framework for efficiently solving high-dimensional\nPDEs.", "AI": {"tldr": "Anant-Net is a neural surrogate for solving high-dimensional PDEs efficiently, overcoming the curse of dimensionality with high accuracy and scalability.", "motivation": "High-dimensional PDEs are computationally intractable due to exponential complexity, especially on hypercubic domains. Traditional methods fail, necessitating a scalable solution.", "method": "Anant-Net integrates high-dimensional boundary conditions and minimizes PDE residuals at collocation points, using Kolmogorov-Arnold networks for interpretability.", "result": "Anant-Net solves 300-dimensional PDEs on a single GPU in hours, demonstrating accuracy and robustness on benchmarks like Poisson and Allen-Cahn equations.", "conclusion": "Anant-Net is an accurate, interpretable, and scalable framework for high-dimensional PDEs, outperforming state-of-the-art methods."}}
{"id": "2505.03522", "pdf": "https://arxiv.org/pdf/2505.03522", "abs": "https://arxiv.org/abs/2505.03522", "authors": ["Haotong Cheng", "Zhiqi Zhang", "Hao Li", "Xinshang Zhang"], "title": "Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has substantially advanced the Single Image Super-Resolution\n(SISR). However, existing researches have predominantly focused on raw\nperformance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions which extend the\ntraditional notion of \"Generalization\" to encompass the modules' ease of\ntransferability, thus revealing the relationships between module universality\nand model generalizability. Then we propose the Universality Assessment\nEquation (UAE), a metric for quantifying how readily a given module could be\ntransplanted across models. Guided by the UAE results of standard residual\nblocks and other plug-and-play modules, we further design two optimized\nmodules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).\nThrough comprehensive experiments on natural-scene benchmarks, remote-sensing\ndatasets, extreme-industrial imagery and on-device deployments, we demonstrate\nthat networks embedded with the proposed plug-and-play modules outperform\nseveral state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or\nenabling a 71.3% reduction in parameters with negligible loss in reconstruction\nfidelity.", "AI": {"tldr": "The paper introduces 'Universality' to assess module transferability in SISR, proposes UAE for quantifying it, and designs optimized modules (CRB and DCRB) that outperform SOTA methods.", "motivation": "Existing SISR research lacks focus on module transferability. The paper aims to quantify and improve this through 'Universality' and UAE.", "method": "Proposes UAE for module transferability assessment and designs CRB and DCRB modules based on UAE insights.", "result": "Networks with CRB/DCRB outperform SOTA, achieving up to 0.83dB PSNR gain or 71.3% parameter reduction with minimal fidelity loss.", "conclusion": "The UAE and optimized modules enhance SISR performance and transferability, validated across diverse datasets and deployments."}}
{"id": "2503.13551", "pdf": "https://arxiv.org/pdf/2503.13551", "abs": "https://arxiv.org/abs/2503.13551", "authors": ["Teng Wang", "Zhangyi Jiang", "Zhenqi He", "Shenyang Tong", "Wenhan Yang", "Yanan Zheng", "Zeyu Li", "Zifan He", "Hailei Gong"], "title": "Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies show that Large Language Models (LLMs) achieve strong\nreasoning capabilities through supervised fine-tuning or reinforcement\nlearning. However, a key approach, the Process Reward Model (PRM), suffers from\nreward hacking, making it unreliable in identifying the best intermediate step.\nIn addition, the cost of annotating reasoning processes for reward modeling is\nhigh, making large-scale collection of high-quality data challenging. To\naddress this, we propose a novel reward model approach called the Hierarchical\nReward Model (HRM), which evaluates both individual and consecutive reasoning\nsteps at both fine-grained and coarse-grained levels. HRM excels at assessing\nmulti-step reasoning coherence, especially when flawed steps are later\ncorrected through self-reflection. To further reduce the cost of generating\ntraining data, we introduce a lightweight and effective data augmentation\nstrategy called Hierarchical Node Compression (HNC), which merges two\nconsecutive reasoning steps into one within the tree structure. By applying HNC\nto MCTS-generated reasoning trajectories, we enhance the diversity and\nrobustness of HRM training data while introducing controlled noise with minimal\ncomputational overhead. Empirical results on the PRM800K dataset show that HRM,\ntogether with HNC, provides more stable and reliable evaluations than PRM.\nFurthermore, cross-domain evaluations on the MATH500 and GSM8K datasets\ndemonstrate HRM's strong generalization and robustness across a variety of\nreasoning tasks.", "AI": {"tldr": "The paper introduces the Hierarchical Reward Model (HRM) and Hierarchical Node Compression (HNC) to improve reasoning evaluation in LLMs, addressing reward hacking and data annotation costs.", "motivation": "Current Process Reward Models (PRMs) suffer from reward hacking and high annotation costs, limiting reliable reasoning step evaluation.", "method": "Proposes HRM for multi-step reasoning assessment and HNC for cost-effective data augmentation via step merging.", "result": "HRM with HNC outperforms PRM on PRM800K and generalizes well on MATH500 and GSM8K datasets.", "conclusion": "HRM and HNC offer stable, reliable, and scalable solutions for evaluating reasoning in LLMs."}}
{"id": "2505.03498", "pdf": "https://arxiv.org/pdf/2505.03498", "abs": "https://arxiv.org/abs/2505.03498", "authors": ["Mojtaba Safari", "Shansong Wang", "Qiang Li", "Zach Eidex", "Richard L. J. Qiu", "Chih-Wei Chang", "Hui Mao", "Xiaofeng Yang"], "title": "MRI motion correction via efficient residual-guided denoising diffusion probabilistic models", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly\ndegrade image quality and impair quantitative analysis. Conventional mitigation\nstrategies, such as repeated acquisitions or motion tracking, are costly and\nworkflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising\ndiffusion probabilistic model tailored for MRI motion artifact correction.\nMethods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in\nthe forward diffusion process, aligning the noise distribution with\nmotion-corrupted data and enabling an efficient four-step reverse diffusion. A\nU-net backbone enhanced with Swin-Transformer blocks conventional attention\nlayers, improving adaptability across resolutions. Training employs a combined\nl1+l2 loss, which promotes image sharpness and reduces pixel-level errors.\nRes-MoCoDiff was evaluated on synthetic dataset generated using a realistic\nmotion simulation framework and on an in-vivo dataset. Comparative analyses\nwere conducted against established methods, including CycleGAN, Pix2pix, and\nMT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),\nstructural similarity index measure (SSIM), and normalized mean squared error\n(NMSE). Results: The proposed method demonstrated superior performance in\nremoving motion artifacts across all motion severity levels. Res-MoCoDiff\nconsistently achieved the highest SSIM and the lowest NMSE values, with a PSNR\nof up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling\ntime was reduced to 0.37 seconds per batch of two image slices, compared with\n101.74 seconds for conventional approaches.", "AI": {"tldr": "Res-MoCoDiff, a diffusion model for MRI motion artifact correction, outperforms existing methods in speed and accuracy.", "motivation": "Motion artifacts degrade MRI quality; current solutions are costly and inefficient.", "method": "Uses residual error shifting and a U-net with Swin-Transformer blocks, trained with l1+l2 loss.", "result": "Achieves highest SSIM, lowest NMSE, and fastest sampling time (0.37s per batch).", "conclusion": "Res-MoCoDiff is an efficient, high-performance solution for MRI motion artifact correction."}}
{"id": "2505.03617", "pdf": "https://arxiv.org/pdf/2505.03617", "abs": "https://arxiv.org/abs/2505.03617", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift", "categories": ["cs.LG"], "comment": null, "summary": "We evaluate the effectiveness of importance weighting in deep neural networks\nunder label shift and covariate shift. On synthetic 2D data (linearly separable\nand moon-shaped) using logistic regression and MLPs, we observe that weighting\nstrongly affects decision boundaries early in training but fades with prolonged\noptimization. On CIFAR-10 with various class imbalances, only L2 regularization\n(not dropout) helps preserve weighting effects. In a covariate-shift\nexperiment, importance weighting yields no significant performance gain,\nhighlighting challenges on complex data. Our results call into question the\npractical utility of importance weighting for real-world distribution shifts.", "AI": {"tldr": "Importance weighting in deep neural networks shows limited practical utility for real-world distribution shifts, fading with prolonged training and offering no significant gains under covariate shift.", "motivation": "To assess the effectiveness of importance weighting in addressing label shift and covariate shift in deep learning models.", "method": "Experiments on synthetic 2D data (logistic regression, MLPs) and CIFAR-10 with class imbalances, testing L2 regularization and dropout.", "result": "Weighting impacts early training but fades over time; L2 regularization helps, but no significant gains under covariate shift.", "conclusion": "Importance weighting may not be practically useful for real-world distribution shifts."}}
{"id": "2505.03557", "pdf": "https://arxiv.org/pdf/2505.03557", "abs": "https://arxiv.org/abs/2505.03557", "authors": ["Koray Ulusan", "Benjamin Kiefer"], "title": "Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025 Workshop \"Synthetic Data for Computer Vision\n  Workshop\", https://syndata4cv.github.io/", "summary": "The personalization of Stable Diffusion for generating professional portraits\nfrom amateur photographs is a burgeoning area, with applications in various\ndownstream contexts. This paper investigates the impact of augmentations on\nimproving facial resemblance when using two prominent personalization\ntechniques: DreamBooth and InstantID. Through a series of experiments with\ndiverse subject datasets, we assessed the effectiveness of various augmentation\nstrategies on the generated headshots' fidelity to the original subject. We\nintroduce FaceDistance, a wrapper around FaceNet, to rank the generations based\non facial similarity, which aided in our assessment. Ultimately, this research\nprovides insights into the role of augmentations in enhancing facial\nresemblance in SDXL-generated portraits, informing strategies for their\neffective deployment in downstream applications.", "AI": {"tldr": "The paper explores how augmentations improve facial resemblance in Stable Diffusion-generated portraits using DreamBooth and InstantID, introducing FaceDistance for similarity ranking.", "motivation": "To enhance facial resemblance in AI-generated professional portraits from amateur photos for downstream applications.", "method": "Experiments with diverse datasets and augmentation strategies, using DreamBooth and InstantID, and introducing FaceDistance for similarity assessment.", "result": "Augmentations significantly improve facial resemblance, with FaceDistance aiding in ranking similarity.", "conclusion": "Augmentations play a key role in enhancing facial resemblance in SDXL-generated portraits, guiding effective deployment strategies."}}
{"id": "2503.17279", "pdf": "https://arxiv.org/pdf/2503.17279", "abs": "https://arxiv.org/abs/2503.17279", "authors": ["Gaifan Zhang", "Yi Zhou", "Danushka Bollegala"], "title": "CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement", "categories": ["cs.CL"], "comment": null, "summary": "The meaning conveyed by a sentence often depends on the context in which it\nappears. Despite the progress of sentence embedding methods, it remains unclear\nhow to best modify a sentence embedding conditioned on its context. To address\nthis problem, we propose Condition-Aware Sentence Embeddings (CASE), an\nefficient and accurate method to create an embedding for a sentence under a\ngiven condition. First, CASE creates an embedding for the condition using a\nLarge Language Model (LLM), where the sentence influences the attention scores\ncomputed for the tokens in the condition during pooling. Next, a supervised\nnonlinear projection is learned to reduce the dimensionality of the LLM-based\ntext embeddings. We show that CASE significantly outperforms previously\nproposed Conditional Semantic Textual Similarity (C-STS) methods on an existing\nstandard benchmark dataset. We find that subtracting the condition embedding\nconsistently improves the C-STS performance of LLM-based text embeddings.\nMoreover, we propose a supervised dimensionality reduction method that not only\nreduces the dimensionality of LLM-based embeddings but also significantly\nimproves their performance.", "AI": {"tldr": "CASE is a method to create context-aware sentence embeddings using LLMs, outperforming existing C-STS methods.", "motivation": "Current sentence embedding methods lack clarity on how to modify embeddings based on context.", "method": "CASE uses LLMs to create condition embeddings, applies supervised nonlinear projection for dimensionality reduction, and subtracts condition embeddings for improved performance.", "result": "CASE outperforms existing C-STS methods and improves LLM-based embeddings through dimensionality reduction.", "conclusion": "CASE effectively addresses context-aware sentence embedding challenges, enhancing performance and efficiency."}}
{"id": "2505.03507", "pdf": "https://arxiv.org/pdf/2505.03507", "abs": "https://arxiv.org/abs/2505.03507", "authors": ["Shenglan Li", "Rui Yao", "Yong Zhou", "Hancheng Zhu", "Kunyang Sun", "Bing Liu", "Zhiwen Shao", "Jiaqi Zhao"], "title": "Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking", "categories": ["cs.CV"], "comment": "Accepted by the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "To reduce the reliance on large-scale annotations, self-supervised RGB-T\ntracking approaches have garnered significant attention. However, the omission\nof the object region by erroneous pseudo-label or the introduction of\nbackground noise affects the efficiency of modality fusion, while pseudo-label\nnoise triggered by similar object noise can further affect the tracking\nperformance. In this paper, we propose GDSTrack, a novel approach that\nintroduces dynamic graph fusion and temporal diffusion to address the above\nchallenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the\nmodalities of neighboring frames, treats them as distractor noise, and\nleverages the denoising capability of a generative model. Specifically, by\nconstructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the\nproposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic\nadjacency matrix to guide graph attention, focusing on and fusing the object's\ncoherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features\nfrom neighboring frames as interference, and thus improving robustness against\nsimilar-object noise. Extensive experiments conducted on four public RGB-T\ntracking datasets demonstrate that GDSTrack outperforms the existing\nstate-of-the-art methods. The source code is available at\nhttps://github.com/LiShenglana/GDSTrack.", "AI": {"tldr": "GDSTrack introduces dynamic graph fusion and temporal diffusion to improve self-supervised RGB-T tracking by addressing pseudo-label noise and modality fusion challenges.", "motivation": "To reduce reliance on large-scale annotations and mitigate issues like erroneous pseudo-labels and background noise in RGB-T tracking.", "method": "Uses Modality-guided Dynamic Graph Fusion (MDGF) and Temporal Graph-Informed Diffusion (TGID) to dynamically fuse modalities and denoise similar-object interference.", "result": "Outperforms state-of-the-art methods on four RGB-T tracking datasets.", "conclusion": "GDSTrack effectively addresses noise and fusion challenges in self-supervised RGB-T tracking, demonstrating superior performance."}}
{"id": "2505.03646", "pdf": "https://arxiv.org/pdf/2505.03646", "abs": "https://arxiv.org/abs/2505.03646", "authors": ["Chethan Krishnamurthy Ramanaik", "Arjun Roy", "Eirini Ntoutsi"], "title": "ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the extensive use of deep autoencoders (AEs) in critical\napplications, their adversarial robustness remains relatively underexplored\ncompared to classification models. AE robustness is characterized by the\nLipschitz bounds of its components. Existing robustness evaluation frameworks\nbased on white-box attacks do not fully exploit the vulnerabilities of\nintermediate ill-conditioned layers in AEs. In the context of optimizing\nimperceptible norm-bounded additive perturbations to maximize output damage,\nexisting methods struggle to effectively propagate adversarial loss gradients\nthroughout the network, often converging to less effective perturbations. To\naddress this, we propose a novel layer-conditioning-based adversarial\noptimization objective that effectively guides the adversarial map toward\nregions of local Lipschitz bounds by enhancing loss gradient information\npropagation during attack optimization. We demonstrate through extensive\nexperiments on state-of-the-art AEs that our adversarial objective results in\nstronger attacks, outperforming existing methods in both universal and\nsample-specific scenarios. As a defense method against this attack, we\nintroduce an inference-time adversarially trained defense plugin that mitigates\nthe effects of adversarial examples.", "AI": {"tldr": "The paper addresses the underexplored adversarial robustness of deep autoencoders (AEs), proposing a novel adversarial optimization method and a defense plugin.", "motivation": "Deep AEs are widely used in critical applications, but their adversarial robustness is less studied compared to classification models. Existing methods fail to fully exploit vulnerabilities in AEs.", "method": "The authors introduce a layer-conditioning-based adversarial optimization objective to enhance gradient propagation and attack effectiveness. They also propose an inference-time defense plugin.", "result": "Experiments show the proposed method outperforms existing attacks and defenses in both universal and sample-specific scenarios.", "conclusion": "The work advances AE robustness by improving attack methods and introducing a practical defense."}}
{"id": "2505.03562", "pdf": "https://arxiv.org/pdf/2505.03562", "abs": "https://arxiv.org/abs/2505.03562", "authors": ["Jiwoo Jeong", "Kirok Kim", "Wooju Kim", "Nam-Joon Kim"], "title": "Real-Time Person Image Synthesis Using a Flow Matching Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images\nconditioned on a target pose and a source image. This task plays a key role in\nvarious real-world applications, such as sign language video generation, AR/VR,\ngaming, and live streaming. In these scenarios, real-time PGPIS is critical for\nproviding immediate visual feedback and maintaining user immersion.However,\nachieving real-time performance remains a significant challenge due to the\ncomplexity of synthesizing high-fidelity images from diverse and dynamic human\nposes. Recent diffusion-based methods have shown impressive image quality in\nPGPIS, but their slow sampling speeds hinder deployment in time-sensitive\napplications. This latency is particularly problematic in tasks like generating\nsign language videos during live broadcasts, where rapid image updates are\nrequired. Therefore, developing a fast and reliable PGPIS model is a crucial\nstep toward enabling real-time interactive systems. To address this challenge,\nwe propose a generative model based on flow matching (FM). Our approach enables\nfaster, more stable, and more efficient training and sampling. Furthermore, the\nproposed model supports conditional generation and can operate in latent space,\nmaking it especially suitable for real-time PGPIS applications where both speed\nand quality are critical. We evaluate our proposed method, Real-Time Person\nImage Synthesis Using a Flow Matching Model (RPFM), on the widely used\nDeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves\nnear-real-time sampling speeds while maintaining performance comparable to the\nstate-of-the-art models. Our methodology trades off a slight, acceptable\ndecrease in generated-image accuracy for over a twofold increase in generation\nspeed, thereby ensuring real-time performance.", "AI": {"tldr": "PGPIS generates person images from poses but struggles with real-time performance. The proposed RPFM model uses flow matching for faster, stable synthesis, achieving near-real-time speeds with slight accuracy trade-offs.", "motivation": "Real-time PGPIS is crucial for applications like sign language video generation and live streaming, but current diffusion-based methods are too slow.", "method": "The RPFM model leverages flow matching for efficient training and sampling, supporting conditional generation in latent space.", "result": "RPFM achieves near-real-time speeds on the DeepFashion dataset, with a slight accuracy trade-off for a twofold speed increase.", "conclusion": "RPFM addresses the real-time challenge in PGPIS, balancing speed and quality for practical applications."}}
{"id": "2503.18991", "pdf": "https://arxiv.org/pdf/2503.18991", "abs": "https://arxiv.org/abs/2503.18991", "authors": ["Ruoxi Cheng", "Haoxuan Ma", "Weixin Wang"], "title": "HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The three authors contributed equally to this work", "summary": "The alignment of large language models (LLMs) with human values remains\ncritical yet hindered by four key challenges: (1) scarcity of balanced safety\ndatasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to\nshallow alignment, and (4) inability to dynamically adapt rewards according to\ntask difficulty. To address these limitations, we introduce HAIR\n(Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a\nnovel alignment approach inspired by shadow models in membership inference\nattacks. Our approach consists of two main components: (1) construction of a\nbalanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using\nstructured prompts that leverage the introspective reasoning capabilities of\nLLMs; and (2) training of category-specific reward models with Group Relative\nPolicy Optimization (GRPO), dynamically tuning optimization to task difficulty\nat both the data and model levels. Comprehensive experiments across four\nharmlessness and four usefulness benchmarks demonstrate that HAIR achieves\nstate-of-the-art performance, outperforming all baseline methods in safety\nwhile maintaining high levels of usefulness.", "AI": {"tldr": "HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning) addresses LLM alignment challenges with a balanced safety dataset and dynamic reward tuning, achieving top performance in safety and usefulness.", "motivation": "Alignment of LLMs with human values is hindered by dataset scarcity, alignment tax, jailbreak vulnerability, and static rewards.", "method": "HAIR uses introspective reasoning to create a balanced safety dataset (CoD) and trains reward models with GRPO, dynamically adjusting to task difficulty.", "result": "HAIR outperforms baselines in safety benchmarks while maintaining high usefulness.", "conclusion": "HAIR effectively addresses alignment challenges, offering a robust solution for LLM safety and utility."}}
{"id": "2505.03528", "pdf": "https://arxiv.org/pdf/2505.03528", "abs": "https://arxiv.org/abs/2505.03528", "authors": ["Chenguang Liu", "Jianjun Chen", "Yunfei Chen", "Yubei He", "Zhuangkun Wei", "Hongjian Sun", "Haiyan Lu", "Qi Hao"], "title": "Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication", "categories": ["cs.CV"], "comment": null, "summary": "Cooperative perception, leveraging shared information from multiple vehicles\nvia vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous\ndriving to alleviate the limitation of single-vehicle perception. Existing\nworks have explored the effects of V2V communication impairments on perception\nprecision, but they lack generalization to different levels of impairments. In\nthis work, we propose a joint weighting and denoising framework, Coop-WD, to\nenhance cooperative perception subject to V2V channel impairments. In this\nframework, the self-supervised contrastive model and the conditional diffusion\nprobabilistic model are adopted hierarchically for vehicle-level and\npixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is\nproposed to selectively deactivate denoising to reduce processing overhead.\nRician fading, non-stationarity, and time-varying distortion are considered.\nSimulation results demonstrate that the proposed Coop-WD outperforms\nconventional benchmarks in all types of channels. Qualitative analysis with\nvisual examples further proves the superiority of our proposed method. The\nproposed Coop-WD-eco achieves up to 50% reduction in computational cost under\nsevere distortion while maintaining comparable accuracy as channel conditions\nimprove.", "AI": {"tldr": "Proposes Coop-WD, a joint weighting and denoising framework, to improve cooperative perception in autonomous driving under V2V communication impairments. Includes an efficient variant, Coop-WD-eco, reducing computational costs.", "motivation": "Addresses the lack of generalization in existing works regarding V2V communication impairments and their impact on perception precision.", "method": "Uses a self-supervised contrastive model and conditional diffusion probabilistic model hierarchically for feature enhancement. Introduces Coop-WD-eco for selective denoising.", "result": "Outperforms benchmarks in all channel types; Coop-WD-eco reduces computational cost by 50% under severe distortion.", "conclusion": "Coop-WD and its variant effectively enhance cooperative perception while balancing accuracy and efficiency."}}
{"id": "2505.03652", "pdf": "https://arxiv.org/pdf/2505.03652", "abs": "https://arxiv.org/abs/2505.03652", "authors": ["Yihang Wang", "Chris Chi", "Aaron R. Dinner"], "title": "Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation", "categories": ["cs.LG", "physics.comp-ph", "physics.data-an", "q-bio.QM", "stat.ML"], "comment": "19 pages, 10 figures", "summary": "Normalizing flows (NFs) provide uncorrelated samples from complex\ndistributions, making them an appealing tool for parameter estimation. However,\nthe practical utility of NFs remains limited by their tendency to collapse to a\nsingle mode of a multimodal distribution. In this study, we show that annealing\nwith an adaptive schedule based on the effective sample size (ESS) can mitigate\nmode collapse. We demonstrate that our approach can converge the marginal\nlikelihood for a biochemical oscillator model fit to time-series data in\nten-fold less computation time than a widely used ensemble Markov chain Monte\nCarlo (MCMC) method. We show that the ESS can also be used to reduce variance\nby pruning the samples. We expect these developments to be of general use for\nsampling with NFs and discuss potential opportunities for further improvements.", "AI": {"tldr": "Annealing with adaptive ESS-based scheduling mitigates mode collapse in NFs, improving efficiency and reducing computation time compared to MCMC.", "motivation": "NFs are limited by mode collapse in multimodal distributions, hindering their practical utility for parameter estimation.", "method": "Proposes annealing with an adaptive ESS-based schedule to prevent mode collapse and demonstrates its efficiency on a biochemical oscillator model.", "result": "Achieves convergence in ten-fold less computation time than MCMC and reduces variance by pruning samples using ESS.", "conclusion": "The approach enhances NF sampling efficiency and suggests further improvements for broader applications."}}
{"id": "2505.03574", "pdf": "https://arxiv.org/pdf/2505.03574", "abs": "https://arxiv.org/abs/2505.03574", "authors": ["Sahana Chennabasappa", "Cyrus Nikolaidis", "Daniel Song", "David Molnar", "Stephanie Ding", "Shengye Wan", "Spencer Whitman", "Lauren Deason", "Nicholas Doucette", "Abraham Montilla", "Alekhya Gampa", "Beto de Paola", "Dominik Gabi", "James Crnkovich", "Jean-Christophe Testud", "Kat He", "Rashnil Chaturvedi", "Wu Zhou", "Joshua Saxe"], "title": "LlamaFirewall: An open source guardrail system for building secure AI agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.", "AI": {"tldr": "LlamaFirewall is an open-source guardrail framework for AI agents, addressing security risks like prompt injection and insecure code with tools like PromptGuard 2, Agent Alignment Checks, and CodeShield.", "motivation": "Existing security measures for LLMs are insufficient for autonomous agents performing high-stakes tasks, necessitating a real-time guardrail monitor.", "method": "The framework includes PromptGuard 2 for jailbreak detection, Agent Alignment Checks for reasoning audits, and CodeShield for static code analysis.", "result": "LlamaFirewall shows state-of-the-art performance in jailbreak detection and improved efficacy in preventing indirect injections and insecure code.", "conclusion": "LlamaFirewall provides a customizable, developer-friendly solution to enhance AI agent security."}}
{"id": "2503.20953", "pdf": "https://arxiv.org/pdf/2503.20953", "abs": "https://arxiv.org/abs/2503.20953", "authors": ["Julia Ive", "Felix Jozsa", "Nick Jackson", "Paulina Bondaronek", "Ciaran Scott Hill", "Richard Dobson"], "title": "Clean & Clear: Feasibility of Safe LLM Clinical Guidance", "categories": ["cs.CL"], "comment": null, "summary": "Background:\n  Clinical guidelines are central to safe evidence-based medicine in modern\nhealthcare, providing diagnostic criteria, treatment options and monitoring\nadvice for a wide range of illnesses. LLM-empowered chatbots have shown great\npromise in Healthcare Q&A tasks, offering the potential to provide quick and\naccurate responses to medical inquiries.\n  Our main objective was the development and preliminary assessment of an\nLLM-empowered chatbot software capable of reliably answering clinical guideline\nquestions using University College London Hospital (UCLH) clinical guidelines.\n  Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant\ninformation from the UCLH guidelines to answer questions. Our approach\nhighlights the safety and reliability of referencing information over its\ninterpretation and response generation. Seven doctors from the ward assessed\nthe chatbot's performance by comparing its answers to the gold standard.\n  Results: Our chatbot demonstrates promising performance in terms of\nrelevance, with ~73% of its responses rated as very relevant, showcasing a\nstrong understanding of the clinical context. Importantly, our chatbot achieves\na recall of 1.00 for extracted guideline lines, substantially minimising the\nrisk of missing critical information. Approximately 78% of responses were rated\nsatisfactory in terms of completeness. A small portion (~14.5%) contained minor\nunnecessary information, indicating occasional lapses in precision. The\nchatbot' showed high efficiency, with an average completion time of 10 seconds,\ncompared to 30 seconds for human respondents. Evaluation of clinical reasoning\nshowed that 72% of the chatbot's responses were without flaws. Our chatbot\ndemonstrates significant potential to speed up and improve the process of\naccessing locally relevant clinical information for healthcare professionals.", "AI": {"tldr": "An LLM-empowered chatbot was developed to answer clinical guideline questions using UCLH guidelines, showing promising relevance, recall, and efficiency, though with minor precision lapses.", "motivation": "To improve access to clinical guidelines for healthcare professionals by leveraging LLMs for quick and accurate responses.", "method": "Used Llama-3.1-8B LLM to extract and answer questions from UCLH guidelines, assessed by doctors for relevance, completeness, and precision.", "result": "73% very relevant responses, 100% recall for guideline lines, 78% satisfactory completeness, 14.5% minor unnecessary info, 10s average response time, 72% flawless clinical reasoning.", "conclusion": "The chatbot shows strong potential to enhance access to clinical information efficiently, though minor precision improvements are needed."}}
{"id": "2505.03538", "pdf": "https://arxiv.org/pdf/2505.03538", "abs": "https://arxiv.org/abs/2505.03538", "authors": ["Chuyu Zhao", "Hao Huang", "Jiashuo Guo", "Ziyu Shen", "Zhongwei Zhou", "Jie Liu", "Zekuan Yu"], "title": "RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised learning has become a compelling approach for 3D tooth\nsegmentation from CBCT scans, where labeled data is minimal. However, existing\nmethods still face two persistent challenges: limited corrective supervision in\nstructurally ambiguous or mislabeled regions during supervised training and\nperformance degradation caused by unreliable pseudo-labels on unlabeled data.\nTo address these problems, we propose Region-Aware Instructive Learning (RAIL),\na dual-group dual-student, semi-supervised framework. Each group contains two\nstudent models guided by a shared teacher network. By alternating training\nbetween the two groups, RAIL promotes intergroup knowledge transfer and\ncollaborative region-aware instruction while reducing overfitting to the\ncharacteristics of any single model. Specifically, RAIL introduces two\ninstructive mechanisms. Disagreement-Focused Supervision (DFS) Controller\nimproves supervised learning by instructing predictions only within areas where\nstudent outputs diverge from both ground truth and the best student, thereby\nconcentrating supervision on structurally ambiguous or mislabeled areas. In the\nunsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces\nagreement in regions with high model certainty while reducing the effect of\nlow-confidence predictions during training. This helps prevent our model from\nlearning unstable patterns and improves the overall reliability of\npseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets\nshow that RAIL surpasses state-of-the-art methods under limited annotation. Our\ncode will be available at https://github.com/Tournesol-Saturday/RAIL.", "AI": {"tldr": "RAIL is a semi-supervised framework for 3D tooth segmentation that addresses challenges like ambiguous regions and unreliable pseudo-labels by using dual-group dual-student training and region-aware instructive mechanisms.", "motivation": "Existing semi-supervised methods struggle with ambiguous regions and unreliable pseudo-labels in 3D tooth segmentation from CBCT scans.", "method": "RAIL employs a dual-group dual-student framework with two instructive mechanisms: DFS Controller for supervised learning and CAL Modulator for unsupervised learning.", "result": "RAIL outperforms state-of-the-art methods on four CBCT datasets under limited annotation.", "conclusion": "RAIL effectively improves segmentation accuracy by focusing on ambiguous regions and enhancing pseudo-label reliability."}}
{"id": "2505.03677", "pdf": "https://arxiv.org/pdf/2505.03677", "abs": "https://arxiv.org/abs/2505.03677", "authors": ["Emanuele Zappala", "Alice Giola", "Andreas Kramer", "Enrico Greco"], "title": "Neural Integral Operators for Inverse problems in Spectroscopy", "categories": ["cs.LG"], "comment": "13 pages. Codes available upon request", "summary": "Deep learning has shown high performance on spectroscopic inverse problems\nwhen sufficient data is available. However, it is often the case that data in\nspectroscopy is scarce, and this usually causes severe overfitting problems\nwith deep learning methods. Traditional machine learning methods are viable\nwhen datasets are smaller, but the accuracy and applicability of these methods\nis generally more limited.\n  We introduce a deep learning method for classification of molecular spectra\nbased on learning integral operators via integral equations of the first kind,\nwhich results in an algorithm that is less affected by overfitting issues on\nsmall datasets, compared to other deep learning models.\n  The problem formulation of the deep learning approach is based on inverse\nproblems, which have traditionally found important applications in\nspectroscopy. We perform experiments on real world data to showcase our\nalgorithm. It is seen that the model outperforms traditional machine learning\napproaches such as decision tree and support vector machine, and for small\ndatasets it outperforms other deep learning models. Therefore, our methodology\nleverages the power of deep learning, still maintaining the performance when\nthe available data is very limited, which is one of the main issues that deep\nlearning faces in spectroscopy, where datasets are often times of small size.", "AI": {"tldr": "A deep learning method for molecular spectra classification is introduced, addressing overfitting in small datasets by leveraging integral operators and outperforming traditional and other deep learning models.", "motivation": "Deep learning struggles with small datasets in spectroscopy, leading to overfitting, while traditional methods lack accuracy and applicability.", "method": "The method uses integral operators via integral equations of the first kind, reducing overfitting in small datasets.", "result": "The model outperforms traditional methods (e.g., decision trees, SVMs) and other deep learning models on small datasets.", "conclusion": "The approach combines deep learning's power with robustness for small datasets, solving a key challenge in spectroscopy."}}
{"id": "2505.03584", "pdf": "https://arxiv.org/pdf/2505.03584", "abs": "https://arxiv.org/abs/2505.03584", "authors": ["Lucas Anastasiou", "Anna De Liddo"], "title": "BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation", "categories": ["cs.HC", "cs.AI", "cs.CY", "I.2"], "comment": "5 pages, 3 figures", "summary": "Public deliberation, as in open discussion of issues of public concern, often\nsuffers from scattered and shallow discourse, poor sensemaking, and a\ndisconnect from actionable policy outcomes. This paper introduces BCause, a\ndiscussion system leveraging generative AI and human-machine collaboration to\ntransform unstructured dialogue around public issues (such as urban living,\npolicy changes, and current socio-economic transformations) into structured,\nactionable democratic processes. We present three innovations: (i) importing\nand transforming unstructured transcripts into argumentative discussions, (ii)\ngeo-deliberated problem-sensing via a Telegram bot for local issue reporting,\nand (iii) smart reporting with customizable widgets (e.g., summaries, topic\nmodelling, policy recommendations, clustered arguments). The system's human-AI\npartnership preserves critical human participation to ensure ethical oversight,\ncontextual relevance, and creative synthesis.", "AI": {"tldr": "BCause is a discussion system using AI and human collaboration to structure public discourse into actionable democratic processes.", "motivation": "Public deliberation often lacks depth and actionable outcomes, which BCause aims to address.", "method": "The system transforms unstructured dialogue into structured discussions, uses a Telegram bot for local issue reporting, and provides smart reporting tools.", "result": "BCause enables structured, actionable democratic processes through human-AI collaboration.", "conclusion": "The system enhances public deliberation by combining AI efficiency with human oversight and creativity."}}
{"id": "2503.23895", "pdf": "https://arxiv.org/pdf/2503.23895", "abs": "https://arxiv.org/abs/2503.23895", "authors": ["Yuqiao Tan", "Shizhu He", "Huanxuan Liao", "Jun Zhao", "Kang Liu"], "title": "Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "preprint. Code is available at https://github.com/Trae1ounG/DyPRAG", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.", "AI": {"tldr": "DyPRAG enhances RAG by dynamically converting documents into parametric knowledge, reducing costs and improving generalization.", "motivation": "Address the high costs and limited generalization of PRAG in RAG systems.", "method": "Uses a lightweight parameter translator to dynamically embed documents into LLMs.", "result": "Reduces inference, training, and storage costs while mitigating RAG hallucination.", "conclusion": "DyPRAG offers a practical and efficient RAG solution with superior knowledge fusion."}}
{"id": "2505.03554", "pdf": "https://arxiv.org/pdf/2505.03554", "abs": "https://arxiv.org/abs/2505.03554", "authors": ["Jo\u00e3o Alves", "Pia Haubro Andersen", "Rikke Gade"], "title": "Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment", "categories": ["cs.CV"], "comment": null, "summary": "The Equine Facial Action Coding System (EquiFACS) enables the systematic\nannotation of facial movements through distinct Action Units (AUs). It serves\nas a crucial tool for assessing affective states in horses by identifying\nsubtle facial expressions associated with discomfort. However, the field of\nhorse affective state assessment is constrained by the scarcity of annotated\ndata, as manually labelling facial AUs is both time-consuming and costly. To\naddress this challenge, automated annotation systems are essential for\nleveraging existing datasets and improving affective states detection tools. In\nthis work, we study different methods for specific ear AU detection and\nlocalization from horse videos. We leverage past works on deep learning-based\nvideo feature extraction combined with recurrent neural networks for the video\nclassification task, as well as a classic optical flow based approach. We\nachieve 87.5% classification accuracy of ear movement presence on a public\nhorse video dataset, demonstrating the potential of our approach. We discuss\nfuture directions to develop these systems, with the aim of bridging the gap\nbetween automated AU detection and practical applications in equine welfare and\nveterinary diagnostics. Our code will be made publicly available at\nhttps://github.com/jmalves5/read-my-ears.", "AI": {"tldr": "The paper introduces an automated system for detecting and localizing ear Action Units (AUs) in horses using deep learning and optical flow, achieving 87.5% accuracy, aiming to improve equine welfare and veterinary diagnostics.", "motivation": "Manual annotation of horse facial AUs is time-consuming and costly, limiting affective state assessment. Automated systems are needed to leverage existing data.", "method": "Combines deep learning-based video feature extraction with recurrent neural networks and optical flow for ear AU detection in horse videos.", "result": "Achieved 87.5% classification accuracy for ear movement presence on a public dataset.", "conclusion": "The approach shows promise for automating AU detection, with potential applications in equine welfare and veterinary diagnostics. Future work aims to bridge the gap to practical use."}}
{"id": "2505.03712", "pdf": "https://arxiv.org/pdf/2505.03712", "abs": "https://arxiv.org/abs/2505.03712", "authors": ["Deming Sheng", "Ricardo Henao"], "title": "Learning Survival Distributions with the Asymmetric Laplace Distribution", "categories": ["cs.LG", "math.ST", "stat.TH"], "comment": "Accepted to ICML 2025", "summary": "Probabilistic survival analysis models seek to estimate the distribution of\nthe future occurrence (time) of an event given a set of covariates. In recent\nyears, these models have preferred nonparametric specifications that avoid\ndirectly estimating survival distributions via discretization. Specifically,\nthey estimate the probability of an individual event at fixed times or the time\nof an event at fixed probabilities (quantiles), using supervised learning.\nBorrowing ideas from the quantile regression literature, we propose a\nparametric survival analysis method based on the Asymmetric Laplace\nDistribution (ALD). This distribution allows for closed-form calculation of\npopular event summaries such as mean, median, mode, variation, and quantiles.\nThe model is optimized by maximum likelihood to learn, at the individual level,\nthe parameters (location, scale, and asymmetry) of the ALD distribution.\nExtensive results on synthetic and real-world data demonstrate that the\nproposed method outperforms parametric and nonparametric approaches in terms of\naccuracy, discrimination and calibration.", "AI": {"tldr": "A parametric survival analysis method using the Asymmetric Laplace Distribution (ALD) is proposed, outperforming existing parametric and nonparametric models in accuracy, discrimination, and calibration.", "motivation": "To address limitations of nonparametric survival analysis models by leveraging the ALD for closed-form calculation of event summaries like mean, median, and quantiles.", "method": "The model uses ALD to estimate survival distributions parametrically, optimizing parameters (location, scale, asymmetry) via maximum likelihood.", "result": "Outperforms parametric and nonparametric methods in accuracy, discrimination, and calibration on synthetic and real-world data.", "conclusion": "The ALD-based parametric method offers superior performance and flexibility for survival analysis."}}
{"id": "2505.03648", "pdf": "https://arxiv.org/pdf/2505.03648", "abs": "https://arxiv.org/abs/2505.03648", "authors": ["Vladimir Fanaskov", "Ivan Oseledets"], "title": "Binding threshold units with artificial oscillatory neurons", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": null, "summary": "Artificial Kuramoto oscillatory neurons were recently introduced as an\nalternative to threshold units. Empirical evidence suggests that oscillatory\nunits outperform threshold units in several tasks including unsupervised object\ndiscovery and certain reasoning problems. The proposed coupling mechanism for\nthese oscillatory neurons is heterogeneous, combining a generalized Kuramoto\nequation with standard coupling methods used for threshold units. In this\nresearch note, we present a theoretical framework that clearly distinguishes\noscillatory neurons from threshold units and establishes a coupling mechanism\nbetween them. We argue that, from a biological standpoint, oscillatory and\nthreshold units realise distinct aspects of neural coding: roughly, threshold\nunits model intensity of neuron firing, while oscillatory units facilitate\ninformation exchange by frequency modulation. To derive interaction between\nthese two types of units, we constrain their dynamics by focusing on dynamical\nsystems that admit Lyapunov functions. For threshold units, this leads to\nHopfield associative memory model, and for oscillatory units it yields a\nspecific form of generalized Kuramoto model. The resulting dynamical systems\ncan be naturally coupled to form a Hopfield-Kuramoto associative memory model,\nwhich also admits a Lyapunov function. Various forms of coupling are possible.\nNotably, oscillatory neurons can be employed to implement a low-rank correction\nto the weight matrix of a Hopfield network. This correction can be viewed\neither as a form of Hebbian learning or as a popular LoRA method used for\nfine-tuning of large language models. We demonstrate the practical realization\nof this particular coupling through illustrative toy experiments.", "AI": {"tldr": "The paper introduces a theoretical framework for coupling artificial Kuramoto oscillatory neurons with threshold units, distinguishing their roles in neural coding and proposing a Hopfield-Kuramoto associative memory model.", "motivation": "To bridge the gap between oscillatory and threshold units, highlighting their distinct roles in neural coding (intensity vs. frequency modulation) and enabling their interaction.", "method": "Develops a coupling mechanism by constraining dynamics to systems with Lyapunov functions, leading to a Hopfield-Kuramoto model. Demonstrates coupling via toy experiments.", "result": "Proposes a Hopfield-Kuramoto associative memory model with Lyapunov stability, allowing oscillatory neurons to correct Hopfield network weights.", "conclusion": "Oscillatory and threshold units can be effectively coupled, offering potential applications in neural coding and learning, such as Hebbian learning or LoRA-like fine-tuning."}}
{"id": "2504.07986", "pdf": "https://arxiv.org/pdf/2504.07986", "abs": "https://arxiv.org/abs/2504.07986", "authors": ["Runjin Chen", "Zhenyu Zhang", "Junyuan Hong", "Souvik Kundu", "Zhangyang Wang"], "title": "SEAL: Steerable Reasoning Calibration of Large Language Models for Free", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated\ncompelling capabilities for complex reasoning tasks via the extended\nchain-of-thought (CoT) reasoning mechanism. However, recent studies reveal\nsubstantial redundancy in the CoT reasoning traces, which not only increases\ninference latency but also negatively impacts model performance by diverting\nattention to unnecessary reasoning paths. To address this issue, we investigate\nthe internal reasoning structures of LLMs and categorize them into three\nprimary thought types: execution, reflection, and transition thoughts.\nMoreover, our analysis reveals that excessive reflection and transition\nthoughts are strongly correlated with failure cases and these thought\ncategories exhibit clear separation in the latent space. Based on these, we\nintroduce SEAL (Steerable reasoning calibration), a training-free approach that\nseamlessly calibrates the CoT process, improving accuracy while demonstrating\nsignificant efficiency gains. SEAL consists of an offline stage for extracting\nthe reasoning steering vector in the latent space, followed by an on-the-fly\ncalibration of the reasoning trace through representation intervention using\nthe steering vector. Notably, the steering vector exhibits strong\ntransferability across various tasks. Extensive experiments across multiple\nmodels (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500,\nGSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11%\nimprovement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our\ncode is publicly available at https://github.com/VITA-Group/SEAL.", "AI": {"tldr": "SEAL improves LLM reasoning efficiency by calibrating CoT traces, reducing redundancy and boosting accuracy.", "motivation": "Address redundancy in CoT reasoning traces that harms performance and efficiency.", "method": "Categorize reasoning thoughts, extract a steering vector, and calibrate CoT traces.", "result": "11% accuracy boost and 11.8%-50.4% token reduction across benchmarks.", "conclusion": "SEAL effectively enhances reasoning efficiency and accuracy without training."}}
{"id": "2505.03567", "pdf": "https://arxiv.org/pdf/2505.03567", "abs": "https://arxiv.org/abs/2505.03567", "authors": ["Zengli Luo", "Canlong Zhang", "Xiaochun Lu", "Zhixin Li", "Zhiwen Wang"], "title": "Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images", "categories": ["cs.CV"], "comment": "9pages,5figures", "summary": "Text-based pedestrian search (TBPS) in full images aims to locate a target\npedestrian in untrimmed images using natural language descriptions. However, in\ncomplex scenes with multiple pedestrians, existing methods are limited by\nuncertainties in detection and matching, leading to degraded performance. To\naddress this, we propose UPD-TBPS, a novel framework comprising three modules:\nMulti-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty\nDecoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts\nmulti-granularity queries to identify potential targets and assigns confidence\nscores to reduce early-stage uncertainty. PUD leverages visual context\ndecoupling and prototype mining to extract features of the target pedestrian\ndescribed in the query. It separates and learns pedestrian prototype\nrepresentations at both the coarse-grained cluster level and the fine-grained\nindividual level, thereby reducing matching uncertainty. ReID evaluates\ncandidates with varying confidence levels, improving detection and retrieval\naccuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the\neffectiveness of our framework.", "AI": {"tldr": "UPD-TBPS is a novel framework for text-based pedestrian search, addressing uncertainties in detection and matching with multi-granularity uncertainty estimation, prototype-based decoupling, and cross-modal re-identification.", "motivation": "Existing methods for text-based pedestrian search struggle with uncertainties in detection and matching in complex scenes.", "method": "Proposes UPD-TBPS with three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID).", "result": "Validated on CUHK-SYSU-TBPS and PRW-TBPS datasets, showing improved accuracy.", "conclusion": "UPD-TBPS effectively reduces uncertainties and enhances pedestrian search performance."}}
{"id": "2505.02844", "pdf": "https://arxiv.org/pdf/2505.02844", "abs": "https://arxiv.org/abs/2505.02844", "authors": ["Zhikai Wang", "Yanyan Shen", "Zibin Zhang", "Kangyi Lin"], "title": "Feature Staleness Aware Incremental Learning for CTR Prediction", "categories": ["cs.IR", "cs.LG"], "comment": "The code can be found in https://github.com/cloudcatcher888/FeSAIL", "summary": "Click-through Rate (CTR) prediction in real-world recommender systems often\ndeals with billions of user interactions every day. To improve the training\nefficiency, it is common to update the CTR prediction model incrementally using\nthe new incremental data and a subset of historical data. However, the feature\nembeddings of a CTR prediction model often get stale when the corresponding\nfeatures do not appear in current incremental data. In the next period, the\nmodel would have a performance degradation on samples containing stale\nfeatures, which we call the feature staleness problem. To mitigate this\nproblem, we propose a Feature Staleness Aware Incremental Learning method for\nCTR prediction (FeSAIL) which adaptively replays samples containing stale\nfeatures. We first introduce a staleness aware sampling algorithm (SAS) to\nsample a fixed number of stale samples with high sampling efficiency. We then\nintroduce a staleness aware regularization mechanism (SAR) for a fine-grained\ncontrol of the feature embedding updating. We instantiate FeSAIL with a general\ndeep learning-based CTR prediction model and the experimental results\ndemonstrate FeSAIL outperforms various state-of-the-art methods on four\nbenchmark datasets.", "AI": {"tldr": "FeSAIL addresses feature staleness in CTR prediction by adaptively replaying stale samples and regularizing updates, outperforming state-of-the-art methods.", "motivation": "Feature embeddings in CTR models become stale when features don't appear in incremental data, degrading performance.", "method": "Proposes FeSAIL with staleness aware sampling (SAS) and regularization (SAR) to update embeddings efficiently.", "result": "FeSAIL outperforms state-of-the-art methods on four benchmark datasets.", "conclusion": "FeSAIL effectively mitigates feature staleness, improving CTR prediction performance."}}
{"id": "2505.03654", "pdf": "https://arxiv.org/pdf/2505.03654", "abs": "https://arxiv.org/abs/2505.03654", "authors": ["Yifan Xiang", "Zhenxi Zhang", "Bin Li", "Yixuan Weng", "Shoujun Zhou", "Yangfan He", "Keqin Li"], "title": "ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent advances in personalized MLLMs enable effective capture of\nuser-specific concepts, supporting both recognition of personalized concepts\nand contextual captioning. However, humans typically explore and reason over\nrelations among objects and individuals, transcending surface-level information\nto achieve more personalized and contextual understanding. To this end,\nexisting methods may face three main limitations: Their training data lacks\nmulti-object sets in which relations among objects are learnable. Building on\nthe limited training data, their models overlook the relations between\ndifferent personalized concepts and fail to reason over them. Their experiments\nmainly focus on a single personalized concept, where evaluations are limited to\nrecognition and captioning tasks. To address the limitations, we present a new\ndataset named ReGraP, consisting of 120 sets of personalized knowledge. Each\nset includes images, KGs, and CoT QA pairs derived from the KGs, enabling more\nstructured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an\nMLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard\ngraph prompting methods are designed to align KGs within the model's semantic\nspace. We establish the ReGraP Benchmark, which contains diverse task types:\nmultiple-choice, fill-in-the-blank, True/False, and descriptive questions in\nboth open- and closed-ended settings. The proposed benchmark is designed to\nevaluate the relational reasoning and knowledge-connection capability of\npersonalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and\nother competitive MLLMs. Results show that the proposed model not only learns\npersonalized knowledge but also performs relational reasoning in responses,\nachieving the SoTA performance compared with the competitive methods. All the\ncodes and datasets are released at: https://github.com/xyfyyds/ReGraP.", "AI": {"tldr": "The paper introduces ReGraP, a dataset and benchmark for relational reasoning in personalized MLLMs, addressing limitations in existing methods by incorporating multi-object sets and structured reasoning tasks.", "motivation": "Existing methods lack multi-object relational training data and fail to reason over personalized concepts, limiting contextual understanding.", "method": "Proposes ReGraP-LLaVA, an MLLM trained with knowledge graphs (KGs) and chain-of-thought (CoT) QA pairs, using soft/hard graph prompting for KG alignment.", "result": "ReGraP-LLaVA achieves state-of-the-art performance in relational reasoning and personalized knowledge tasks.", "conclusion": "The ReGraP framework advances personalized MLLMs by enabling structured reasoning and knowledge connection, validated by the ReGraP Benchmark."}}
{"id": "2504.17974", "pdf": "https://arxiv.org/pdf/2504.17974", "abs": "https://arxiv.org/abs/2504.17974", "authors": ["Sabur Butt", "Fazlourrahman Balouchzahi", "Ahmad Imam Amjad", "Maaz Amjad", "Hector G. Ceballos", "Salud Maria Jimenez-Zafra"], "title": "Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English", "categories": ["cs.CL"], "comment": null, "summary": "Hope is a complex and underexplored emotional state that plays a significant\nrole in education, mental health, and social interaction. Unlike basic\nemotions, hope manifests in nuanced forms ranging from grounded optimism to\nexaggerated wishfulness or sarcasm, making it difficult for Natural Language\nProcessing systems to detect accurately. This study introduces PolyHope V2, a\nmultilingual, fine-grained hope speech dataset comprising over 30,000 annotated\ntweets in English and Spanish. This resource distinguishes between four hope\nsubtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances\nexisting datasets by explicitly labeling sarcastic instances. We benchmark\nmultiple pretrained transformer models and compare them with large language\nmodels (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes.\nOur findings show that fine-tuned transformers outperform prompt-based LLMs,\nespecially in distinguishing nuanced hope categories and sarcasm. Through\nqualitative analysis and confusion matrices, we highlight systematic challenges\nin separating closely related hope subtypes. The dataset and results provide a\nrobust foundation for future emotion recognition tasks that demand greater\nsemantic and contextual sensitivity across languages.", "AI": {"tldr": "PolyHope V2 is a multilingual dataset for detecting nuanced hope subtypes in tweets, showing fine-tuned transformers outperform LLMs like GPT-4 and Llama 3.", "motivation": "Hope is complex and underexplored in NLP, with varied forms like optimism, wishfulness, or sarcasm, making detection challenging.", "method": "Created PolyHope V2, a dataset of 30K+ annotated tweets in English/Spanish, distinguishing four hope subtypes. Benchmarked transformers and LLMs (GPT-4, Llama 3) under zero-shot/few-shot settings.", "result": "Fine-tuned transformers outperformed LLMs, especially in nuanced hope and sarcasm detection. Challenges remain in distinguishing closely related subtypes.", "conclusion": "PolyHope V2 advances emotion recognition with semantic and contextual sensitivity, providing a foundation for future multilingual NLP tasks."}}
{"id": "2505.03569", "pdf": "https://arxiv.org/pdf/2505.03569", "abs": "https://arxiv.org/abs/2505.03569", "authors": ["Mishal Fatima", "Steffen Jung", "Margret Keuper"], "title": "Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models", "categories": ["cs.CV"], "comment": null, "summary": "Backgrounds in images play a major role in contributing to spurious\ncorrelations among different data points. Owing to aesthetic preferences of\nhumans capturing the images, datasets can exhibit positional (location of the\nobject within a given frame) and size (region-of-interest to image ratio)\nbiases for different classes. In this paper, we show that these biases can\nimpact how much a model relies on spurious features in the background to make\nits predictions. To better illustrate our findings, we propose a synthetic\ndataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images\nwith various backgrounds, object positions, and object sizes. By evaluating the\ndataset on different pretrained models, we find that most models rely heavily\non spurious features in the background when the region-of-interest (ROI) to\nimage ratio is small and the object is far from the center of the image.\nMoreover, we also show that current methods that aim to mitigate harmful\nspurious features, do not take into account these factors, hence fail to\nachieve considerable performance gains for worst-group accuracies when the size\nand location of core features in an image change.", "AI": {"tldr": "The paper investigates how background biases in images (position and size) lead models to rely on spurious features, using a synthetic dataset (Hard-Spurious-ImageNet) to demonstrate this. Current mitigation methods fail to address these biases effectively.", "motivation": "To understand how positional and size biases in datasets cause models to depend on spurious background features, and to highlight the limitations of existing mitigation techniques.", "method": "Proposes Hard-Spurious-ImageNet, a synthetic dataset derived from ImageNet1k, to test models under varied backgrounds, object positions, and sizes. Evaluates pretrained models on this dataset.", "result": "Models rely heavily on spurious background features when the ROI is small or the object is off-center. Existing mitigation methods fail to improve worst-group accuracy under these conditions.", "conclusion": "Background biases significantly impact model reliance on spurious features, and current mitigation approaches are inadequate for addressing these biases."}}
{"id": "2505.02963", "pdf": "https://arxiv.org/pdf/2505.02963", "abs": "https://arxiv.org/abs/2505.02963", "authors": ["Rohan Ghuge", "Sahil Singla", "Yifan Wang"], "title": "Single-Sample and Robust Online Resource Allocation", "categories": ["cs.DS", "cs.GT", "cs.LG"], "comment": "Full version of STOC 2025 paper", "summary": "Online Resource Allocation problem is a central problem in many areas of\nComputer Science, Operations Research, and Economics. In this problem, we\nsequentially receive $n$ stochastic requests for $m$ kinds of shared resources,\nwhere each request can be satisfied in multiple ways, consuming different\namounts of resources and generating different values. The goal is to achieve a\n$(1-\\epsilon)$-approximation to the hindsight optimum, where $\\epsilon>0$ is a\nsmall constant, assuming each resource has a large budget.\n  In this paper, we investigate the learnability and robustness of online\nresource allocation. Our primary contribution is a novel Exponential Pricing\nalgorithm with the following properties: 1. It requires only a \\emph{single\nsample} from each of the $n$ request distributions to achieve a\n$(1-\\epsilon)$-approximation for online resource allocation with large budgets.\nSuch an algorithm was previously unknown, even with access to polynomially many\nsamples, as prior work either assumed full distributional knowledge or was\nlimited to i.i.d.\\,or random-order arrivals. 2. It is robust to corruptions in\nthe outliers model and the value augmentation model. Specifically, it maintains\nits $(1 - \\epsilon)$-approximation guarantee under both these robustness\nmodels, resolving the open question posed in Argue, Gupta, Molinaro, and Singla\n(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures\nincentive compatibility.\n  The intuition behind our Exponential Pricing algorithm is that the price of a\nresource should adjust exponentially as it is overused or underused. It differs\nfrom conventional approaches that use an online learning algorithm for item\npricing. This departure guarantees that the algorithm will never run out of any\nresource, but loses the usual no-regret properties of online learning\nalgorithms, necessitating a new analytical approach.", "AI": {"tldr": "The paper introduces an Exponential Pricing algorithm for online resource allocation, achieving a (1-\u03b5)-approximation with a single sample per request, robustness to corruptions, and incentive compatibility.", "motivation": "To address the challenge of online resource allocation with limited distributional knowledge and robustness to corruptions, improving upon prior work that required full knowledge or restrictive assumptions.", "method": "A novel Exponential Pricing algorithm adjusts resource prices exponentially based on usage, differing from traditional online learning approaches.", "result": "The algorithm achieves (1-\u03b5)-approximation with a single sample, handles corruptions, and ensures incentive compatibility.", "conclusion": "The Exponential Pricing algorithm advances online resource allocation by balancing simplicity, robustness, and performance."}}
{"id": "2505.03655", "pdf": "https://arxiv.org/pdf/2505.03655", "abs": "https://arxiv.org/abs/2505.03655", "authors": ["Le Pan", "Yuanjiang Cao", "Chengkai Huang", "Wenjie Zhang", "Lina Yao"], "title": "Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Recommender Systems (RSs) aim to provide personalized recommendations for\nusers. A newly discovered bias, known as sentiment bias, uncovers a common\nphenomenon within Review-based RSs (RRSs): the recommendation accuracy of users\nor items with negative reviews deteriorates compared with users or items with\npositive reviews. Critical users and niche items are disadvantaged by such\nunfair recommendations. We study this problem from the perspective of\ncounterfactual inference with two stages. At the model training stage, we build\na causal graph and model how sentiment influences the final rating score.\nDuring the inference stage, we decouple the direct and indirect effects to\nmitigate the impact of sentiment bias and remove the indirect effect using\ncounterfactual inference. We have conducted extensive experiments, and the\nresults validate that our model can achieve comparable performance on rating\nprediction for better recommendations and effective mitigation of sentiment\nbias. To the best of our knowledge, this is the first work to employ\ncounterfactual inference on sentiment bias mitigation in RSs.", "AI": {"tldr": "The paper addresses sentiment bias in Review-based Recommender Systems (RRSs) by using counterfactual inference to improve fairness in recommendations for users and items with negative reviews.", "motivation": "Sentiment bias in RRSs unfairly disadvantages users and items with negative reviews, reducing recommendation accuracy. The study aims to mitigate this bias.", "method": "A two-stage approach: (1) modeling sentiment's influence on ratings via a causal graph during training, and (2) decoupling direct/indirect effects during inference using counterfactual methods.", "result": "Experiments show the model achieves comparable rating prediction performance while effectively mitigating sentiment bias.", "conclusion": "This is the first work to apply counterfactual inference for sentiment bias mitigation in RSs, demonstrating improved fairness and accuracy."}}
{"id": "2504.18376", "pdf": "https://arxiv.org/pdf/2504.18376", "abs": "https://arxiv.org/abs/2504.18376", "authors": ["Pablo Miralles-Gonz\u00e1lez", "Javier Huertas-Tato", "Alejandro Mart\u00edn", "David Camacho"], "title": "Pushing the boundary on Natural Language Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a central task in natural language\nunderstanding with applications in fact-checking, question answering, and\ninformation retrieval. Despite its importance, current NLI systems heavily rely\non supervised learning with datasets that often contain annotation artifacts\nand biases, limiting generalization and real-world applicability. In this work,\nwe apply a reinforcement learning-based approach using Group Relative Policy\nOptimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the\nneed for labeled rationales and enabling this type of training on more\nchallenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language\nmodels using parameter-efficient techniques (LoRA and QLoRA), demonstrating\nstrong performance across standard and adversarial NLI benchmarks. Our 32B\nAWQ-quantized model surpasses state-of-the-art results on 7 out of 11\nadversarial sets$\\unicode{x2013}$or on all of them considering our\nreplication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust\nreasoning can be retained under aggressive quantization. This work provides a\nscalable and practical framework for building robust NLI systems without\nsacrificing inference quality.", "AI": {"tldr": "The paper introduces a reinforcement learning-based approach (GRPO) for Chain-of-Thought learning in NLI, eliminating the need for labeled data and improving performance on adversarial benchmarks.", "motivation": "Current NLI systems rely on supervised learning with biased datasets, limiting generalization. This work aims to overcome these limitations.", "method": "Uses Group Relative Policy Optimization (GRPO) for CoT learning, fine-tuning large language models (7B, 14B, 32B) with LoRA and QLoRA techniques.", "result": "The 32B AWQ-quantized model achieves state-of-the-art results on adversarial benchmarks, maintaining robust reasoning under aggressive quantization.", "conclusion": "The framework is scalable and practical for building robust NLI systems without compromising inference quality."}}
{"id": "2505.03575", "pdf": "https://arxiv.org/pdf/2505.03575", "abs": "https://arxiv.org/abs/2505.03575", "authors": ["Maria Kainz", "Johannes K. Krondorfer", "Malte Jaschik", "Maria Jernej", "Harald Ganster"], "title": "Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning", "categories": ["cs.CV", "physics.app-ph"], "comment": "Accepted at: Proceedings of OCM 2025 - 7th International Conference\n  on Optical Characterization of Materials, March 26-27, 2025, Karlsruhe,\n  Germany, pp. 319-328", "summary": "Recycling textile fibers is critical to reducing the environmental impact of\nthe textile industry. Hyperspectral near-infrared (NIR) imaging combined with\nadvanced deep learning algorithms offers a promising solution for efficient\nfiber classification and sorting. In this study, we investigate supervised and\nunsupervised deep learning models and test their generalization capabilities on\ndifferent textile structures. We show that optimized convolutional neural\nnetworks (CNNs) and autoencoder networks achieve robust generalization under\nvarying conditions. These results highlight the potential of hyperspectral\nimaging and deep learning to advance sustainable textile recycling through\naccurate and robust classification.", "AI": {"tldr": "Hyperspectral NIR imaging and deep learning enable efficient textile fiber classification for sustainable recycling.", "motivation": "Reducing the environmental impact of the textile industry by improving fiber recycling.", "method": "Investigating supervised (CNNs) and unsupervised (autoencoder) deep learning models for fiber classification on diverse textile structures.", "result": "Optimized CNNs and autoencoders demonstrate robust generalization across varying conditions.", "conclusion": "Hyperspectral imaging and deep learning can significantly enhance textile recycling through accurate classification."}}
{"id": "2505.02972", "pdf": "https://arxiv.org/pdf/2505.02972", "abs": "https://arxiv.org/abs/2505.02972", "authors": ["Aoran Chen", "Yang Feng"], "title": "GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "Multi-Task Learning (MTL) seeks to boost statistical power and learning\nefficiency by discovering structure shared across related tasks.\nState-of-the-art MTL representation methods, however, usually treat the latent\nrepresentation matrix as a point in ordinary Euclidean space, ignoring its\noften non-Euclidean geometry, thus sacrificing robustness when tasks are\nheterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL\nframework that embeds the shared representation on its natural Riemannian\nmanifold and optimizes it via explicit manifold operations. Each training cycle\nperforms (i) a Riemannian gradient step that respects the intrinsic curvature\nof the search space, followed by (ii) an efficient polar retraction to remain\non the manifold, guaranteeing geometric fidelity at every iteration. The\nprocedure applies to a broad class of matrix-factorized MTL models and retains\nthe same per-iteration cost as Euclidean baselines. Across a set of synthetic\nexperiments with task heterogeneity and on a wearable-sensor\nactivity-recognition benchmark, GeoERM consistently improves estimation\naccuracy, reduces negative transfer, and remains stable under adversarial label\nnoise, outperforming leading MTL and single-task alternatives.", "AI": {"tldr": "GeoERM is a geometry-aware Multi-Task Learning framework that embeds shared representations on a Riemannian manifold, improving robustness and accuracy over Euclidean methods.", "motivation": "Traditional MTL methods ignore non-Euclidean geometry of latent representations, leading to poor performance with heterogeneous or adversarial tasks.", "method": "GeoERM uses Riemannian gradient steps and polar retraction to optimize representations on their natural manifold, maintaining geometric fidelity.", "result": "GeoERM outperforms Euclidean MTL and single-task methods in accuracy, reduces negative transfer, and handles adversarial noise better.", "conclusion": "GeoERM's geometry-aware approach enhances MTL robustness and efficiency, especially for heterogeneous tasks."}}
{"id": "2505.03662", "pdf": "https://arxiv.org/pdf/2505.03662", "abs": "https://arxiv.org/abs/2505.03662", "authors": ["Xin Du", "Francesca M. Cozzi", "Rajesh Jena"], "title": "Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models", "categories": ["cs.CV", "cs.AI", "68U10"], "comment": "9 pages", "summary": "Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are\nessential for evaluating white matter integrity and structural connectivity in\nneuroimaging. However, the spatial misalignment between FA maps and\ntractography atlases hinders their effective integration into predictive\nmodels. To address this issue, we propose a CycleGAN based approach for\ngenerating FA maps directly from T1-weighted MRI scans, representing the first\napplication of this technique to both healthy and tumour-affected tissues. Our\nmodel, trained on unpaired data, produces high fidelity maps, which have been\nrigorously evaluated using Structural Similarity Index (SSIM) and Peak\nSignal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in\ntumour regions. Radiological assessments further underscore the model's\npotential to enhance clinical workflows by providing an AI-driven alternative\nthat reduces the necessity for additional scans.", "AI": {"tldr": "A CycleGAN-based method generates FA maps from T1-weighted MRI scans, improving alignment with tractography atlases and showing strong performance in tumour regions.", "motivation": "Address spatial misalignment between FA maps and tractography atlases to enhance predictive models in neuroimaging.", "method": "Proposes a CycleGAN approach trained on unpaired data to generate FA maps from T1-weighted MRI scans.", "result": "High fidelity FA maps validated by SSIM and PSNR, with robust performance in tumour regions. Radiological assessments highlight clinical utility.", "conclusion": "The model offers an AI-driven alternative to reduce additional scans, improving clinical workflows."}}
{"id": "2505.00949", "pdf": "https://arxiv.org/pdf/2505.00949", "abs": "https://arxiv.org/abs/2505.00949", "authors": ["Akhiad Bercovich", "Itay Levy", "Izik Golan", "Mohammad Dabbah", "Ran El-Yaniv", "Omri Puny", "Ido Galil", "Zach Moshe", "Tomer Ronen", "Najeeb Nabwani", "Ido Shahaf", "Oren Tropp", "Ehud Karpas", "Ran Zilberstein", "Jiaqi Zeng", "Soumye Singhal", "Alexander Bukharin", "Yian Zhang", "Tugrul Konuk", "Gerald Shen", "Ameya Sunil Mahabaleshwarkar", "Bilal Kartal", "Yoshi Suhara", "Olivier Delalleau", "Zijia Chen", "Zhilin Wang", "David Mosallanezhad", "Adi Renduchintala", "Haifeng Qian", "Dima Rekesh", "Fei Jia", "Somshubra Majumdar", "Vahid Noroozi", "Wasi Uddin Ahmad", "Sean Narenthiran", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Igor Gitman", "Ivan Moshkov", "Wei Du", "Shubham Toshniwal", "George Armstrong", "Branislav Kisacanin", "Matvei Novikov", "Daria Gitman", "Evelina Bakhturina", "Jane Polak Scowcroft", "John Kamalu", "Dan Su", "Kezhi Kong", "Markus Kliegl", "Rabeeh Karimi", "Ying Lin", "Sanjeev Satheesh", "Jupinder Parmar", "Pritam Gundecha", "Brandon Norick", "Joseph Jennings", "Shrimai Prabhumoye", "Syeda Nahida Akter", "Mostofa Patwary", "Abhinav Khattar", "Deepak Narayanan", "Roger Waleffe", "Jimmy Zhang", "Bor-Yiing Su", "Guyue Huang", "Terry Kong", "Parth Chadha", "Sahil Jain", "Christine Harvey", "Elad Segal", "Jining Huang", "Sergey Kashirsky", "Robert McQueen", "Izzy Putterman", "George Lam", "Arun Venkatesan", "Sherry Wu", "Vinh Nguyen", "Manoj Kilaru", "Andrew Wang", "Anna Warno", "Abhilash Somasamudramath", "Sandip Bhaskar", "Maka Dong", "Nave Assaf", "Shahar Mor", "Omer Ullman Argov", "Scot Junkin", "Oleksandr Romanenko", "Pedro Larroy", "Monika Katariya", "Marco Rovinelli", "Viji Balas", "Nicholas Edelman", "Anahita Bhiwandiwalla", "Muthu Subramaniam", "Smita Ithape", "Karthik Ramamoorthy", "Yuting Wu", "Suguna Varshini Velury", "Omri Almog", "Joyjit Daw", "Denys Fridman", "Erick Galinkin", "Michael Evans", "Katherine Luna", "Leon Derczynski", "Nikki Pope", "Eileen Long", "Seth Schneider", "Guillermo Siman", "Tomasz Grzegorzek", "Pablo Ribalta", "Monika Katariya", "Joey Conway", "Trisha Saar", "Ann Guan", "Krzysztof Pawelec", "Shyamala Prayaga", "Oleksii Kuchaiev", "Boris Ginsburg", "Oluwatobi Olabiyi", "Kari Briski", "Jonathan Cohen", "Bryan Catanzaro", "Jonah Alben", "Yonatan Geifman", "Eric Chung", "Chris Alexiuk"], "title": "Llama-Nemotron: Efficient Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.", "AI": {"tldr": "The Llama-Nemotron series offers open-source, high-performance reasoning models with three sizes, competitive with state-of-the-art models, and includes unique features like a dynamic reasoning toggle.", "motivation": "To provide enterprise-ready, open-source reasoning models with superior efficiency and flexibility, supporting dynamic switching between chat and reasoning modes.", "method": "Training involves neural architecture search, knowledge distillation, continued pretraining, and a reasoning-focused post-training stage with supervised fine-tuning and large-scale reinforcement learning.", "result": "Models (Nano, Super, Ultra) perform competitively with top reasoning models while offering better inference throughput and memory efficiency.", "conclusion": "Llama-Nemotron models advance open-source reasoning tools, supported by released datasets, codebases, and a permissive license for broader adoption."}}
{"id": "2505.03581", "pdf": "https://arxiv.org/pdf/2505.03581", "abs": "https://arxiv.org/abs/2505.03581", "authors": ["Sergey Linok", "Vadim Semenov", "Anastasia Trunova", "Oleg Bulichev", "Dmitry Yudin"], "title": "DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes", "categories": ["cs.CV"], "comment": "8 pages, 5 figures, 6 tables", "summary": "The analysis of events in dynamic environments poses a fundamental challenge\nin the development of intelligent agents and robots capable of interacting with\nhumans. Current approaches predominantly utilize visual models. However, these\nmethods often capture information implicitly from images, lacking interpretable\nspatial-temporal object representations. To address this issue we introduce\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\ncompressed spatial-temporal structural observation representation with the\ncognitive capabilities of large language models. The purpose of this\nintegration is to enable advanced question answering based on a sequence of\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\nindicate that DyGEnc outperforms existing visual methods by a large margin of\n15-25% in addressing queries regarding the history of human-to-object\ninteractions. Furthermore, the proposed method can be seamlessly extended to\nprocess raw input images utilizing foundational models for extracting explicit\ntextual scene graphs, as substantiated by the results of a robotic experiment\nconducted with a wheeled manipulator platform. We hope that these findings will\ncontribute to the implementation of robust and compressed graph-based robotic\nmemory for long-horizon reasoning. Code is available at\ngithub.com/linukc/DyGEnc.", "AI": {"tldr": "DyGEnc introduces a dynamic graph encoding method combining spatial-temporal observations with large language models, outperforming visual methods by 15-25% in human-object interaction queries.", "motivation": "Current visual models lack interpretable spatial-temporal object representations, limiting intelligent agents' interaction capabilities.", "method": "DyGEnc integrates compressed spatial-temporal structural observations with large language models for advanced question answering using textual scene graphs.", "result": "Outperforms existing methods by 15-25% on STAR and AGQA datasets and extends to raw image processing for robotic applications.", "conclusion": "DyGEnc advances graph-based robotic memory for long-horizon reasoning, with potential for robust implementation."}}
{"id": "2505.02979", "pdf": "https://arxiv.org/pdf/2505.02979", "abs": "https://arxiv.org/abs/2505.02979", "authors": ["Ruiyue Huang", "Claire E. Heaney", "Maarten van Reeuwijk"], "title": "Parameter estimation for land-surface models using machine learning libraries", "categories": ["physics.ao-ph", "cs.LG"], "comment": "9 pages, 5 figures, 3 tables", "summary": "The Neural Networks for Partial Differential Equations (NN4PDEs) approach is\nused to determine the parameters of a simple land-surface model using PyTorch's\nbackpropagation engine. In order to test the inverse model, a synthetic dataset\nis created by running the model in forward mode with known parameter values to\ncreate soil temperature time series that can be used as observations for the\ninverse model. We show that it is not possible to obtain a reliable parameter\nestimation using a single observed soil temperature time series. Using\nmeasurements at two depths, reliable parameter estimates can be obtained\nalthough it is not possible to differentiate between latent and sensible heat\nfluxes. We apply the inverse model to urban flux tower data in Phoenix, United\nStates, and show that the thermal conductivity, volumetric heat capacity, and\nthe combined sensible-latent heat transfer coefficient can be reliably\nestimated using an observed value for the effective surface albedo. The\nresulting model accurately predicts the outgoing longwave radiation, conductive\nsoil fluxes and the combined sensible-latent heat fluxes.", "AI": {"tldr": "NN4PDEs is used to estimate land-surface model parameters via PyTorch. Single soil temperature data is insufficient for reliable parameter estimation, but dual-depth measurements work, though they can't distinguish heat fluxes. Applied to urban data, the model reliably estimates key parameters and accurately predicts fluxes.", "motivation": "To improve parameter estimation in land-surface models using neural networks and test the approach with synthetic and real-world data.", "method": "Uses NN4PDEs with PyTorch for inverse modeling, tested on synthetic data and applied to urban flux tower data.", "result": "Single-depth data fails for reliable estimation; dual-depth works but can't separate heat fluxes. Urban data application successfully estimates parameters and predicts fluxes.", "conclusion": "NN4PDEs is effective for parameter estimation with sufficient data, though limitations exist in distinguishing heat fluxes."}}
{"id": "2505.03694", "pdf": "https://arxiv.org/pdf/2505.03694", "abs": "https://arxiv.org/abs/2505.03694", "authors": ["Parv Kapoor", "Ian Higgins", "Nikhil Keetha", "Jay Patrikar", "Brady Moon", "Zelin Ye", "Yao He", "Ivan Cisneros", "Yaoyu Hu", "Changliu Liu", "Eunsuk Kang", "Sebastian Scherer"], "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid", "categories": ["cs.RO", "cs.AI"], "comment": "13 pages, RSS 2025 Demo track", "summary": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.", "AI": {"tldr": "ViSafe is a vision-only collision avoidance system for airborne vehicles, integrating edge-AI and multi-camera hardware to ensure safe separation in high-speed operations.", "motivation": "Ensuring safe-separation in high-density airspace operations for resource-constrained aerial systems.", "method": "Combines learning-based edge-AI with multi-camera hardware and perceptual input-focused control barrier functions (CBF) for safety guarantees.", "result": "ViSafe consistently ensures self-separation in diverse scenarios, including real-world high-speed tests (up to 144 km/h).", "conclusion": "ViSafe sets a new benchmark for vision-only autonomous collision avoidance, enhancing safety in high-speed aerial navigation."}}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877", "abs": "https://arxiv.org/abs/2505.01877", "authors": ["Ji\u0159\u00ed Mili\u010dka", "Anna Marklov\u00e1", "Ond\u0159ej Drobil", "Eva Posp\u00ed\u0161ilov\u00e1"], "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts.", "AI": {"tldr": "Participants improved accuracy in distinguishing human-written from AI-generated texts with feedback, correcting misconceptions about AI style and readability.", "motivation": "To determine if feedback helps individuals learn to discriminate between human and AI texts and recalibrate self-perceived competence.", "method": "Used GPT-4o to generate texts, compared with human-written ones, and tested 255 participants with/without feedback.", "result": "Feedback group showed improved accuracy and confidence calibration, correcting initial misconceptions.", "conclusion": "Targeted training with feedback effectively teaches differentiation between human and AI texts, aiding self-assessment, especially in education."}}
{"id": "2505.03597", "pdf": "https://arxiv.org/pdf/2505.03597", "abs": "https://arxiv.org/abs/2505.03597", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Fixed-Length Dense Fingerprint Representation", "categories": ["cs.CV"], "comment": "Under review at IEEE Transactions on Information Forensics and\n  Security (TIFS)", "summary": "Fixed-length fingerprint representations, which map each fingerprint to a\ncompact and fixed-size feature vector, are computationally efficient and\nwell-suited for large-scale matching. However, designing a robust\nrepresentation that effectively handles diverse fingerprint modalities, pose\nvariations, and noise interference remains a significant challenge. In this\nwork, we propose a fixed-length dense descriptor of fingerprints, and introduce\nFLARE-a fingerprint matching framework that integrates the Fixed-Length dense\ndescriptor with pose-based Alignment and Robust Enhancement. This fixed-length\nrepresentation employs a three-dimensional dense descriptor to effectively\ncapture spatial relationships among fingerprint ridge structures, enabling\nrobust and locally discriminative representations. To ensure consistency within\nthis dense feature space, FLARE incorporates pose-based alignment using\ncomplementary estimation methods, along with dual enhancement strategies that\nrefine ridge clarity while preserving the original fingerprint modality. The\nproposed dense descriptor supports fixed-length representation while\nmaintaining spatial correspondence, enabling fast and accurate similarity\ncomputation. Extensive experiments demonstrate that FLARE achieves superior\nperformance across rolled, plain, latent, and contactless fingerprints,\nsignificantly outperforming existing methods in cross-modality and low-quality\nscenarios. Further analysis validates the effectiveness of the dense descriptor\ndesign, as well as the impact of alignment and enhancement modules on the\naccuracy of dense descriptor matching. Experimental results highlight the\neffectiveness and generalizability of FLARE as a unified and scalable solution\nfor robust fingerprint representation and matching. The implementation and code\nwill be publicly available at https://github.com/Yu-Yy/FLARE.", "AI": {"tldr": "FLARE is a fingerprint matching framework using a fixed-length dense descriptor for robust, scalable, and accurate fingerprint representation and matching across diverse modalities.", "motivation": "The challenge lies in creating a robust fixed-length fingerprint representation that handles diverse modalities, pose variations, and noise.", "method": "FLARE integrates a 3D dense descriptor for spatial relationships, pose-based alignment, and dual enhancement strategies for clarity and modality preservation.", "result": "FLARE outperforms existing methods in cross-modality and low-quality scenarios, demonstrating superior performance.", "conclusion": "FLARE is a unified, scalable solution for robust fingerprint matching, validated by extensive experiments."}}
{"id": "2505.02987", "pdf": "https://arxiv.org/pdf/2505.02987", "abs": "https://arxiv.org/abs/2505.02987", "authors": ["Yifan Chen"], "title": "New affine invariant ensemble samplers and their dimensional scaling", "categories": ["stat.CO", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": "Any feedback welcome!", "summary": "We introduce some new affine invariant ensemble samplers that are easy to\nconstruct and improve upon existing widely used algorithms, especially for\nhigh-dimensional problems. Specifically, we propose a derivative-free ensemble\nside move sampler that performs favorably compared to popular samplers in the\n\\texttt{emcee} package. Additionally, we develop a class of derivative-based\nensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which\noutperform standard HMC without affine invariance when sampling highly skewed\ndistributions. We provide asymptotic scaling analysis for high-dimensional\nGaussian targets to further elucidate the properties of these affine invariant\nensemble samplers. In particular, with derivative information, the affine\ninvariant ensemble HMC can scale much better with dimension compared to\nderivative-free ensemble samplers.", "AI": {"tldr": "New affine invariant ensemble samplers outperform existing methods, especially in high dimensions, with derivative-free and derivative-based options showing superior performance.", "motivation": "Existing samplers, like those in the emcee package, may not perform optimally for high-dimensional or skewed distributions, prompting the development of improved affine invariant ensemble samplers.", "method": "Proposed a derivative-free ensemble side move sampler and a derivative-based ensemble Hamiltonian Monte Carlo (HMC) sampler, both with affine invariance.", "result": "The new samplers outperform existing methods, with affine invariant ensemble HMC scaling better in high dimensions when derivative information is used.", "conclusion": "Affine invariant ensemble samplers, especially those leveraging derivatives, offer significant improvements for high-dimensional and skewed distribution sampling."}}
{"id": "2505.03710", "pdf": "https://arxiv.org/pdf/2505.03710", "abs": "https://arxiv.org/abs/2505.03710", "authors": ["Kevin Tan", "Wei Fan", "Yuting Wei"], "title": "Actor-Critics Can Achieve Optimal Sample Efficiency", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Actor-critic algorithms have become a cornerstone in reinforcement learning\n(RL), leveraging the strengths of both policy-based and value-based methods.\nDespite recent progress in understanding their statistical efficiency, no\nexisting work has successfully learned an $\\epsilon$-optimal policy with a\nsample complexity of $O(1/\\epsilon^2)$ trajectories with general function\napproximation when strategic exploration is necessary.\n  We address this open problem by introducing a novel actor-critic algorithm\nthat attains a sample-complexity of $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d\nH^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ trajectories, and accompanying $\\sqrt{T}$\nregret when the Bellman eluder dimension $d$ does not increase with $T$ at more\nthan a $\\log T$ rate.\n  Here, $\\mathcal{F}$ is the critic function class, $\\mathcal{A}$ is the action\nspace, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm\nintegrates optimism, off-policy critic estimation targeting the optimal\nQ-function, and rare-switching policy resets.\n  We extend this to the setting of Hybrid RL, showing that initializing the\ncritic with offline data yields sample efficiency gains compared to purely\noffline or online RL. Further, utilizing access to offline data, we provide a\n\\textit{non-optimistic} provably efficient actor-critic algorithm that only\nadditionally requires $N_{\\text{off}} \\geq c_{\\text{off}}^*dH^4/\\epsilon^2$ in\nexchange for omitting optimism, where $c_{\\text{off}}^*$ is the single-policy\nconcentrability coefficient and $N_{\\text{off}}$ is the number of offline\nsamples. This addresses another open problem in the literature. We further\nprovide numerical experiments to support our theoretical findings.", "AI": {"tldr": "A novel actor-critic algorithm achieves $O(1/\\epsilon^2)$ sample complexity for $\\epsilon$-optimal policies with general function approximation, addressing open problems in RL.", "motivation": "To bridge the gap in achieving optimal sample complexity and regret in actor-critic methods, especially with strategic exploration and hybrid RL settings.", "method": "Introduces an algorithm combining optimism, off-policy critic estimation, and rare-switching policy resets, and extends it to hybrid RL with offline data.", "result": "Achieves $O(dH^5 \\log|\\mathcal{A}|/\\epsilon^2 + d H^4 \\log|\\mathcal{F}|/ \\epsilon^2)$ sample complexity and $\\sqrt{T}$ regret under certain conditions.", "conclusion": "The algorithm advances RL efficiency, particularly in hybrid settings, and provides theoretical and empirical validation."}}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156", "abs": "https://arxiv.org/abs/2505.02156", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in Progress. The code and data are available, see\n  https://github.com/MozerWang/AMPO", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach.", "AI": {"tldr": "AML introduces adaptive reasoning modes for social intelligence tasks, outperforming existing methods in performance and efficiency.", "motivation": "Current methods lack dynamic reasoning depth adjustment, leading to inefficiency and poor social simulation.", "method": "Proposes AML with AMPO algorithm, featuring multi-granular thinking modes, context-aware switching, and token-efficient reasoning.", "result": "AML achieves 15.6% higher task performance and 32.8% shorter reasoning chains than state-of-the-art methods.", "conclusion": "Context-sensitive mode selection in AML enables more human-like adaptive reasoning compared to fixed-depth approaches."}}
{"id": "2505.03599", "pdf": "https://arxiv.org/pdf/2505.03599", "abs": "https://arxiv.org/abs/2505.03599", "authors": ["Fengming Lin", "Arezoo Zakeri", "Yidan Xue", "Michael MacRaild", "Haoran Dou", "Zherui Zhou", "Ziwei Zou", "Ali Sarrami-Foroushani", "Jinming Duan", "Alejandro F. Frangi"], "title": "From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,\nenabling the transformation of medical imaging data into three-dimensional mesh\nmodels that are critical in computational medicine and in silico trials for\nadvancing our understanding of disease mechanisms, and diagnostic and\ntherapeutic techniques in modern medicine. This survey systematically\ncategorizes existing approaches into four main categories: template models,\nstatistical models, generative models, and implicit models. Each category is\nanalysed in detail, examining their methodological foundations, strengths,\nlimitations, and applicability to different anatomical structures and imaging\nmodalities. We provide an extensive evaluation of these methods across various\nanatomical applications, from cardiac imaging to neurological studies,\nsupported by quantitative comparisons using standard metrics. Additionally, we\ncompile and analyze major public datasets available for medical mesh\nreconstruction tasks and discuss commonly used evaluation metrics and loss\nfunctions. The survey identifies current challenges in the field, including\nrequirements for topological correctness, geometric accuracy, and\nmulti-modality integration. Finally, we present promising future research\ndirections in this domain. This systematic review aims to serve as a\ncomprehensive reference for researchers and practitioners in medical image\nanalysis and computational medicine.", "AI": {"tldr": "A survey on deep learning-based medical image-to-mesh reconstruction, categorizing methods into four types, evaluating their strengths, limitations, and applications, and discussing challenges and future directions.", "motivation": "To advance understanding of disease mechanisms and improve diagnostic/therapeutic techniques by transforming medical imaging data into 3D mesh models.", "method": "Systematic categorization into template, statistical, generative, and implicit models, with detailed analysis of each, supported by quantitative comparisons and dataset evaluations.", "result": "Identifies strengths, limitations, and applicability of methods across anatomical structures and imaging modalities, along with challenges like topological correctness and multi-modality integration.", "conclusion": "The survey serves as a comprehensive reference for researchers, highlighting future research directions in medical image-to-mesh reconstruction."}}
{"id": "2505.03034", "pdf": "https://arxiv.org/pdf/2505.03034", "abs": "https://arxiv.org/abs/2505.03034", "authors": ["Sweta Rai", "Douglas W. Nychka", "Soutir Bandyopadhyay"], "title": "Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Data derived from remote sensing or numerical simulations often have a\nregular gridded structure and are large in volume, making it challenging to\nfind accurate spatial models that can fill in missing grid cells or simulate\nthe process effectively, especially in the presence of spatial heterogeneity\nand heavy-tailed marginal distributions. To overcome this issue, we present a\nspatial autoregressive modeling framework, which maps observations at a\nlocation and its neighbors to independent random variables. This is a highly\nflexible modeling approach and well-suited for non-Gaussian fields, providing\nsimpler interpretability. In particular, we consider the SAR model with\nGeneralized Extreme Value distribution innovations to combine the observation\nat a central grid location with its neighbors, capturing extreme spatial\nbehavior based on the heavy-tailed innovations. While these models are fast to\nsimulate by exploiting the sparsity of the key matrices in the computations,\nthe maximum likelihood estimation of the parameters is prohibitive due to the\nintractability of the likelihood, making optimization challenging. To overcome\nthis, we train a convolutional neural network on a large training set that\ncovers a useful parameter space, and then use the trained network for fast\nparameter estimation. Finally, we apply this model to analyze annual maximum\nprecipitation data from ERA-Interim-driven Weather Research and Forecasting\n(WRF) simulations, allowing us to explore its spatial extreme behavior across\nNorth America.", "AI": {"tldr": "A spatial autoregressive (SAR) model with Generalized Extreme Value (GEV) innovations is proposed for gridded data, addressing spatial heterogeneity and heavy-tailed distributions. A CNN is used for fast parameter estimation, applied to precipitation extremes.", "motivation": "Challenges in modeling large, gridded data with spatial heterogeneity and heavy-tailed distributions motivate the need for flexible, interpretable spatial models.", "method": "The SAR model with GEV innovations combines central and neighboring grid observations. A CNN is trained for efficient parameter estimation due to likelihood intractability.", "result": "The model effectively captures extreme spatial behavior and is computationally efficient for simulation and parameter estimation.", "conclusion": "The framework is suitable for non-Gaussian fields and applied to analyze extreme precipitation, demonstrating its utility for spatial extremes."}}
{"id": "2505.03738", "pdf": "https://arxiv.org/pdf/2505.03738", "abs": "https://arxiv.org/abs/2505.03738", "authors": ["Jialong Li", "Xuxin Cheng", "Tianshu Huang", "Shiqi Yang", "Ri-Zhao Qiu", "Xiaolong Wang"], "title": "AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "website: https://amo-humanoid.github.io", "summary": "Humanoid robots derive much of their dexterity from hyper-dexterous\nwhole-body movements, enabling tasks that require a large operational\nworkspace: such as picking objects off the ground. However, achieving these\ncapabilities on real humanoids remains challenging due to their high degrees of\nfreedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization\n(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with\ntrajectory optimization for real-time, adaptive whole-body control. To mitigate\ndistribution bias in motion imitation RL, we construct a hybrid AMO dataset and\ntrain a network capable of robust, on-demand adaptation to potentially O.O.D.\ncommands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid\nrobot, demonstrating superior stability and an expanded workspace compared to\nstrong baselines. Finally, we show that AMO's consistent performance supports\nautonomous task execution via imitation learning, underscoring the system's\nversatility and robustness.", "AI": {"tldr": "AMO integrates sim-to-real RL and trajectory optimization for adaptive whole-body control in humanoid robots, improving stability and workspace.", "motivation": "Addressing challenges in real humanoid control due to high DoF and nonlinear dynamics.", "method": "Hybrid AMO dataset construction and RL training for robust adaptation to O.O.D. commands.", "result": "Validated on a 29-DoF Unitree G1, showing superior stability and expanded workspace.", "conclusion": "AMO supports autonomous task execution via imitation learning, proving versatility and robustness."}}
{"id": "2505.02579", "pdf": "https://arxiv.org/pdf/2505.02579", "abs": "https://arxiv.org/abs/2505.02579", "authors": ["Lingxiao Kong", "Cong Yang", "Susanne Neufang", "Oya Deniz Beyan", "Zeyd Boukhers"], "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference", "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.", "AI": {"tldr": "The paper introduces EMORL, an ensemble-based RL framework for fine-tuning LLMs, improving efficiency, scalability, and explainability in multi-objective tasks.", "motivation": "Address challenges in RL for LLM fine-tuning, such as complex objective balancing, low efficiency, poor scalability, and limited explainability.", "method": "Proposes EMORL, which fine-tunes multiple models with individual objectives and aggregates their last hidden states using a hierarchical grid search for optimal combinations.", "result": "EMORL shows lower training consumption, improved scalability, and comparable performance on PAIR and Psych8k datasets.", "conclusion": "EMORL effectively addresses multi-objective RL challenges, offering a scalable and explainable solution for LLM fine-tuning."}}
{"id": "2505.03610", "pdf": "https://arxiv.org/pdf/2505.03610", "abs": "https://arxiv.org/abs/2505.03610", "authors": ["Fangling Jiang", "Qi Li", "Bing Liu", "Weining Wang", "Caifeng Shan", "Zhenan Sun", "Ming-Hsuan Yang"], "title": "Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection", "categories": ["cs.CV"], "comment": null, "summary": "3D mask presentation attack detection is crucial for protecting face\nrecognition systems against the rising threat of 3D mask attacks. While most\nexisting methods utilize multimodal features or remote photoplethysmography\n(rPPG) signals to distinguish between real faces and 3D masks, they face\nsignificant challenges, such as the high costs associated with multimodal\nsensors and limited generalization ability. Detection-related text descriptions\noffer concise, universal information and are cost-effective to obtain. However,\nthe potential of vision-language multimodal features for 3D mask presentation\nattack detection remains unexplored. In this paper, we propose a novel\nknowledge-based prompt learning framework to explore the strong generalization\ncapability of vision-language models for 3D mask presentation attack detection.\nSpecifically, our approach incorporates entities and triples from knowledge\ngraphs into the prompt learning process, generating fine-grained, task-specific\nexplicit prompts that effectively harness the knowledge embedded in pre-trained\nvision-language models. Furthermore, considering different input images may\nemphasize distinct knowledge graph elements, we introduce a visual-specific\nknowledge filter based on an attention mechanism to refine relevant elements\naccording to the visual context. Additionally, we leverage causal graph theory\ninsights into the prompt learning process to further enhance the generalization\nability of our method. During training, a spurious correlation elimination\nparadigm is employed, which removes category-irrelevant local image patches\nusing guidance from knowledge-based text features, fostering the learning of\ngeneralized causal prompts that align with category-relevant local patches.\nExperimental results demonstrate that the proposed method achieves\nstate-of-the-art intra- and cross-scenario detection performance on benchmark\ndatasets.", "AI": {"tldr": "A novel knowledge-based prompt learning framework for 3D mask attack detection, leveraging vision-language models and knowledge graphs for improved generalization.", "motivation": "Addressing the limitations of existing methods (high costs, poor generalization) by exploring vision-language multimodal features for 3D mask attack detection.", "method": "Incorporates knowledge graphs into prompt learning, uses visual-specific knowledge filters, and applies causal graph theory to enhance generalization.", "result": "Achieves state-of-the-art intra- and cross-scenario detection performance on benchmark datasets.", "conclusion": "The proposed framework effectively harnesses vision-language models and knowledge graphs for robust 3D mask attack detection."}}
{"id": "2505.03069", "pdf": "https://arxiv.org/pdf/2505.03069", "abs": "https://arxiv.org/abs/2505.03069", "authors": ["Yurui Zhang", "Ruigang Wang", "Ian R. Manchester"], "title": "Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses", "categories": ["eess.SY", "cs.LG", "cs.SY"], "comment": null, "summary": "We study the invertibility of nonlinear dynamical systems from the\nperspective of contraction and incremental stability analysis and propose a new\ninvertible recurrent neural model: the BiLipREN. In particular, we consider a\nnonlinear state space model to be robustly invertible if an inverse exists with\na state space realisation, and both the forward model and its inverse are\ncontracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have\nbounded incremental gain. This property of bi-Lipschitzness implies both\nrobustness in the sense of sensitivity to input perturbations, as well as\nrobust distinguishability of different inputs from their corresponding outputs,\ni.e. the inverse model robustly reconstructs the input sequence despite small\nperturbations to the initial conditions and measured output. Building on this\nfoundation, we propose a parameterization of neural dynamic models:\nbi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly\ninvertible by construction. Moreover, biLipRENs can be composed with orthogonal\nlinear systems to construct more general bi-Lipschitz dynamic models, e.g., a\nnonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We\nillustrate the utility of our proposed approach with numerical examples.", "AI": {"tldr": "The paper introduces BiLipREN, a robustly invertible recurrent neural model, leveraging contraction and Lipschitz properties for stability and input reconstruction.", "motivation": "To address the invertibility and robustness of nonlinear dynamical systems, ensuring sensitivity to input perturbations and robust distinguishability of inputs.", "method": "Proposes bi-Lipschitz recurrent equilibrium networks (biLipREN), parameterized to be robustly invertible by construction, and composable with orthogonal linear systems.", "result": "Demonstrates robust invertibility and stability, with numerical examples validating the approach.", "conclusion": "BiLipREN provides a framework for constructing invertible and robust dynamic models, extending to nonlinear analogues of classical factorizations."}}
{"id": "2310.05537", "pdf": "https://arxiv.org/pdf/2310.05537", "abs": "https://arxiv.org/abs/2310.05537", "authors": ["Philipp Scholl", "Katharina Bieker", "Hillary Hauger", "Gitta Kutyniok"], "title": "ParFam -- (Neural Guided) Symbolic Regression Based on Continuous Global Optimization", "categories": ["cs.AI", "cs.LG"], "comment": "Code: https://github.com/Philipp238/parfam", "summary": "The problem of symbolic regression (SR) arises in many different\napplications, such as identifying physical laws or deriving mathematical\nequations describing the behavior of financial markets from given data. Various\nmethods exist to address the problem of SR, often based on genetic programming.\nHowever, these methods are usually complicated and involve various\nhyperparameters. In this paper, we present our new approach ParFam that\nutilizes parametric families of suitable symbolic functions to translate the\ndiscrete symbolic regression problem into a continuous one, resulting in a more\nstraightforward setup compared to current state-of-the-art methods. In\ncombination with a global optimizer, this approach results in a highly\neffective method to tackle the problem of SR. We theoretically analyze the\nexpressivity of ParFam and demonstrate its performance with extensive numerical\nexperiments based on the common SR benchmark suit SRBench, showing that we\nachieve state-of-the-art results. Moreover, we present an extension\nincorporating a pre-trained transformer network DL-ParFam to guide ParFam,\naccelerating the optimization process by up to two magnitudes. Our code and\nresults can be found at https://github.com/Philipp238/parfam.", "AI": {"tldr": "ParFam simplifies symbolic regression by using parametric families of functions, achieving state-of-the-art results and accelerating optimization with a transformer network.", "motivation": "Symbolic regression (SR) is complex with existing methods often involving hyperparameters and genetic programming. ParFam aims to simplify this by transforming the problem into a continuous one.", "method": "ParFam uses parametric families of symbolic functions to convert SR into a continuous problem, combined with a global optimizer. An extension, DL-ParFam, incorporates a pre-trained transformer for faster optimization.", "result": "ParFam achieves state-of-the-art performance on SRBench benchmarks, with DL-ParFam speeding up optimization by up to two magnitudes.", "conclusion": "ParFam offers a simpler, more effective approach to SR, with potential for further improvements through integration of deep learning techniques."}}
{"id": "2404.05046", "pdf": "https://arxiv.org/pdf/2404.05046", "abs": "https://arxiv.org/abs/2404.05046", "authors": ["Liqiang Jing", "Xinya Du"], "title": "FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated proficiency in\ntackling a variety of visual-language tasks. However, current LVLMs suffer from\nmisalignment between text and image modalities which causes three kinds of\nhallucination problems, i.e., object existence, object attribute, and object\nrelationship. To tackle this issue, existing methods mainly utilize\nReinforcement Learning (RL) to align modalities in LVLMs. However, they still\nsuffer from three main limitations: (1) General feedback can not indicate the\nhallucination type contained in the response; (2) Sparse rewards only give the\nsequence-level reward for the whole response; and (3)Annotation cost is\ntime-consuming and labor-intensive. To handle these limitations, we propose an\ninnovative method to align modalities in LVLMs through Fine-Grained Artificial\nIntelligence Feedback (FGAIF), which mainly consists of three steps: AI-based\nFeedback Collection, Fine-grained Reward Model Training, and Reinforcement\nLearning with Fine-grained Reward. Specifically, We first utilize AI tools to\npredict the types of hallucination for each segment in the response and obtain\na collection of fine-grained feedback. Then, based on the collected reward\ndata, three specialized reward models are trained to produce dense rewards.\nFinally, a novel fine-grained feedback module is integrated into the Proximal\nPolicy Optimization (PPO) algorithm. Extensive experiments are conducted on\nhallucination and general benchmarks, demonstrating the superior performance of\nour proposed method. Notably, compared with previous models trained with the\nRL-based aligning method, our proposed method is effective even with fewer\nparameters.", "AI": {"tldr": "The paper addresses misalignment in Large Vision-Language Models (LVLMs) causing hallucination issues. It proposes Fine-Grained Artificial Intelligence Feedback (FGAIF) to improve alignment through AI-based feedback, specialized reward models, and reinforcement learning.", "motivation": "Current LVLMs suffer from text-image misalignment leading to hallucination problems (object existence, attribute, relationship). Existing RL methods lack fine-grained feedback, sparse rewards, and high annotation costs.", "method": "Proposes FGAIF: (1) AI-based feedback collection to identify hallucination types, (2) training specialized reward models for dense rewards, (3) integrating fine-grained feedback into PPO for reinforcement learning.", "result": "Demonstrates superior performance on hallucination and general benchmarks, even with fewer parameters compared to previous RL-based methods.", "conclusion": "FGAIF effectively addresses LVLM misalignment and hallucination issues, offering a scalable and efficient solution."}}
{"id": "2505.03611", "pdf": "https://arxiv.org/pdf/2505.03611", "abs": "https://arxiv.org/abs/2505.03611", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Wei Shen", "Bing Liu", "Zhenan Sun"], "title": "Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images", "categories": ["cs.CV"], "comment": null, "summary": "Face anti-spoofing is a critical technology for ensuring the security of face\nrecognition systems. However, its ability to generalize across diverse\nscenarios remains a significant challenge. In this paper, we attribute the\nlimited generalization ability to two key factors: covariate shift, which\narises from external data collection variations, and semantic shift, which\nresults from substantial differences in emerging attack types. To address both\nchallenges, we propose a novel approach for learning unknown spoof prompts,\nrelying solely on real face images from a single source domain. Our method\ngenerates textual prompts for real faces and potential unknown spoof attacks by\nleveraging the general knowledge embedded in vision-language models, thereby\nenhancing the model's ability to generalize to unseen target domains.\nSpecifically, we introduce a diverse spoof prompt optimization framework to\nlearn effective prompts. This framework constrains unknown spoof prompts within\na relaxed prior knowledge space while maximizing their distance from real face\nimages. Moreover, it enforces semantic independence among different spoof\nprompts to capture a broad range of spoof patterns. Experimental results on\nnine datasets demonstrate that the learned prompts effectively transfer the\nknowledge of vision-language models, enabling state-of-the-art generalization\nability against diverse unknown attack types across unseen target domains\nwithout using any spoof face images.", "AI": {"tldr": "The paper proposes a novel method for face anti-spoofing by generating diverse spoof prompts using vision-language models, improving generalization without spoof face images.", "motivation": "Addressing the limited generalization of face anti-spoofing due to covariate and semantic shifts from diverse attack types and data variations.", "method": "Generates textual prompts for real faces and unknown spoof attacks using vision-language models, optimizing prompts to maximize distance from real faces and ensure semantic independence.", "result": "Achieves state-of-the-art generalization on nine datasets, effectively handling unseen attack types without spoof images.", "conclusion": "The approach successfully leverages vision-language models to enhance generalization in face anti-spoofing, addressing key challenges of covariate and semantic shifts."}}
{"id": "2505.03120", "pdf": "https://arxiv.org/pdf/2505.03120", "abs": "https://arxiv.org/abs/2505.03120", "authors": ["Abdul Mustafa", "Muhammad Talha Khan", "Muhammad Azmi Umer", "Zaki Masood", "Chuadhry Mujeeb Ahmed"], "title": "Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted in the 1st Workshop on Modeling and Verification for Secure\n  and Performant Cyber-Physical Systems in conjunction with Cyber-Physical\n  Systems and Internet-of-Things Week, Irvine, USA, May 6-9, 2025", "summary": "Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable\nto adversarial attacks. It is crucial for an IDS to learn to recognize\nadversarial examples before malicious entities exploit them. In this paper, we\ngenerated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We\nvalidate the generalization and scalability of the adversarial samples to\ntackle a broad range of real attacks on Industrial Control Systems (ICS). We\nevaluated the impact by assessing multiple attacks generated using the proposed\nmethod. The model trained with adversarial samples detected attacks with 95%\naccuracy on real-world attack data not used during training. The study was\nconducted using an operational secure water treatment (SWaT) testbed.", "AI": {"tldr": "ML-based IDS are vulnerable to adversarial attacks. This paper uses JSMA to generate adversarial samples, validating their effectiveness against real ICS attacks with 95% accuracy.", "motivation": "To enhance IDS resilience by recognizing adversarial examples before exploitation.", "method": "Generated adversarial samples using JSMA and tested on a SWaT testbed.", "result": "Model trained with adversarial samples achieved 95% accuracy on unseen real-world attack data.", "conclusion": "Adversarial training improves IDS robustness against real attacks in ICS."}}
{"id": "2312.13435", "pdf": "https://arxiv.org/pdf/2312.13435", "abs": "https://arxiv.org/abs/2312.13435", "authors": ["Ilias Tsingenopoulos", "Vera Rimmer", "Davy Preuveneers", "Fabio Pierazzi", "Lorenzo Cavallaro", "Wouter Joosen"], "title": "The Adaptive Arms Race: Redefining Robustness in AI Security", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Despite considerable efforts on making them robust, real-world AI-based\nsystems remain vulnerable to decision based attacks, as definitive proofs of\ntheir operational robustness have so far proven intractable. Canonical\nrobustness evaluation relies on adaptive attacks, which leverage complete\nknowledge of the defense and are tailored to bypass it. This work broadens the\nnotion of adaptivity, which we employ to enhance both attacks and defenses,\nshowing how they can benefit from mutual learning through interaction. We\nintroduce a framework for adaptively optimizing black-box attacks and defenses\nunder the competitive game they form. To assess robustness reliably, it is\nessential to evaluate against realistic and worst-case attacks. We thus enhance\nattacks and their evasive arsenal together using RL, apply the same principle\nto defenses, and evaluate them first independently and then jointly under a\nmulti-agent perspective. We find that active defenses, those that dynamically\ncontrol system responses, are an essential complement to model hardening\nagainst decision-based attacks; that these defenses can be circumvented by\nadaptive attacks, something that elicits defenses being adaptive too. Our\nfindings, supported by an extensive theoretical and empirical investigation,\nconfirm that adaptive adversaries pose a serious threat to black-box AI-based\nsystems, rekindling the proverbial arms race. Notably, our approach outperforms\nthe state-of-the-art black-box attacks and defenses, while bringing them\ntogether to render effective insights into the robustness of real-world\ndeployed ML-based systems.", "AI": {"tldr": "The paper introduces a framework for adaptive attacks and defenses in AI systems, using mutual learning and RL to enhance robustness against decision-based attacks. It highlights the necessity of adaptive defenses and outperforms state-of-the-art methods.", "motivation": "Real-world AI systems remain vulnerable to decision-based attacks, and existing robustness evaluations are insufficient. The paper aims to address this by broadening the concept of adaptivity for both attacks and defenses.", "method": "The framework employs adaptive optimization for black-box attacks and defenses, using RL to enhance their capabilities. It evaluates them independently and jointly under a multi-agent perspective.", "result": "Active defenses are essential against decision-based attacks, but adaptive attacks can circumvent them, necessitating adaptive defenses. The approach outperforms state-of-the-art methods.", "conclusion": "Adaptive adversaries pose a serious threat, reigniting the arms race in AI security. The framework provides effective insights into robustness for real-world ML systems."}}
{"id": "2405.15025", "pdf": "https://arxiv.org/pdf/2405.15025", "abs": "https://arxiv.org/abs/2405.15025", "authors": ["Ali Edalati", "Alireza Ghaffari", "Mahsa Ghazvini Nejad", "Lu Hou", "Boxing Chen", "Masoud Asgharian", "Vahid Partovi Nia"], "title": "OAC: Output-adaptive Calibration for Accurate Post-training Quantization", "categories": ["cs.LG", "cs.CL"], "comment": "22 pages, 4 figures", "summary": "Deployment of Large Language Models (LLMs) has major computational costs, due\nto their rapidly expanding size. Compression of LLMs reduces the memory\nfootprint, latency, and energy required for their inference. Post-training\nQuantization (PTQ) techniques have been developed to compress LLMs while\navoiding expensive re-training. Most PTQ approaches formulate the quantization\nerror based on a layer-wise Euclidean loss, ignoring the model output. Then,\neach layer is calibrated using its layer-wise Hessian to update the weights\ntowards minimizing the quantization error. The Hessian is also used for\ndetecting the most salient weights to quantization. Such PTQ approaches are\nprone to accuracy drop in low-precision quantization. We propose\nOutput-adaptive Calibration (OAC) to incorporate the model output in the\ncalibration process. We formulate the quantization error based on the\ndistortion of the output cross-entropy loss. OAC approximates the\noutput-adaptive Hessian for each layer under reasonable assumptions to reduce\nthe computational complexity. The output-adaptive Hessians are used to update\nthe weight matrices and detect the salient weights towards maintaining the\nmodel output. Our proposed method outperforms the state-of-the-art baselines\nsuch as SpQR and BiLLM, especially, at extreme low-precision (2-bit and binary)\nquantization.", "AI": {"tldr": "The paper proposes Output-adaptive Calibration (OAC) to improve low-precision quantization of LLMs by incorporating model output in the calibration process, outperforming existing methods.", "motivation": "Large Language Models (LLMs) have high computational costs, and existing Post-training Quantization (PTQ) methods often lead to accuracy drops in low-precision settings.", "method": "OAC formulates quantization error based on output cross-entropy loss, approximates output-adaptive Hessians for layers, and updates weights to maintain model output.", "result": "OAC outperforms state-of-the-art baselines like SpQR and BiLLM, especially in extreme low-precision (2-bit and binary) quantization.", "conclusion": "Incorporating model output in calibration via OAC effectively reduces quantization errors and maintains accuracy in low-precision LLM deployment."}}
{"id": "2505.03621", "pdf": "https://arxiv.org/pdf/2505.03621", "abs": "https://arxiv.org/abs/2505.03621", "authors": ["Yiping Xie", "Bo Zhao", "Mingtong Dai", "Jian-Ping Zhou", "Yue Sun", "Tao Tan", "Weicheng Xie", "Linlin Shen", "Zitong Yu"], "title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.", "AI": {"tldr": "PhysLLM combines LLMs with rPPG to improve non-contact physiological measurement by addressing illumination and motion challenges using cross-modal alignment and adaptive feature re-weighting.", "motivation": "rPPG is prone to illumination changes and motion artifacts, while LLMs struggle with noise-sensitive rPPG signals due to their text-centric design.", "method": "PhysLLM integrates LLMs with rPPG components using Text Prototype Guidance (TPG) for cross-modal alignment and a Dual-Domain Stationary (DDS) Algorithm for signal stability.", "result": "PhysLLM achieves state-of-the-art accuracy and robustness across four benchmark datasets, handling lighting variations and motion effectively.", "conclusion": "PhysLLM successfully bridges the gap between LLMs and rPPG, enhancing performance in challenging scenarios."}}
{"id": "2505.03140", "pdf": "https://arxiv.org/pdf/2505.03140", "abs": "https://arxiv.org/abs/2505.03140", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Quantum machine learning for spin and molecular systems faces critical\nchallenges of scarce labeled data and computationally expensive simulations. To\naddress these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),\na novel self-supervised framework that pre-trains transformers on unlabeled\nquantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike\nrandom masking approaches, HMAE employs a physics-informed strategy based on\nquantum information theory to selectively mask Hamiltonian terms based on their\nphysical significance. Experiments on 12,500 quantum Hamiltonians (60%\nreal-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\\pm$ 1.5%\naccuracy in phase classification and 0.15 $\\pm$ 0.02 eV MAE in ground state\nenergy prediction with merely 10 labeled examples - a statistically significant\nimprovement (p < 0.01) over classical graph neural networks (78.1% $\\pm$ 2.1%)\nand quantum neural networks (76.8% $\\pm$ 2.3%). Our method's primary advantage\nis exceptional sample efficiency - reducing required labeled examples by 3-5x\ncompared to baseline methods - though we emphasize that ground truth values for\nfine-tuning and evaluation still require exact diagonalization or tensor\nnetworks. We explicitly acknowledge that our current approach is limited to\nsmall quantum systems (specifically limited to 12 qubits during training, with\nlimited extension to 16-20 qubits in testing) and that, while promising within\nthis regime, this size restriction prevents immediate application to larger\nsystems of practical interest in materials science and quantum chemistry.", "AI": {"tldr": "HMAE is a self-supervised framework for quantum Hamiltonians, improving few-shot learning with physics-informed masking. It outperforms baselines in phase classification and energy prediction but is limited to small systems.", "motivation": "Addressing scarce labeled data and expensive simulations in quantum machine learning for spin and molecular systems.", "method": "Hamiltonian-Masked Autoencoding (HMAE) pre-trains transformers on unlabeled Hamiltonians using physics-informed masking.", "result": "Achieves 85.3% accuracy in phase classification and 0.15 eV MAE in energy prediction with 10 labeled examples, outperforming baselines.", "conclusion": "HMAE offers sample efficiency but is limited to small quantum systems (12-20 qubits), restricting practical applications."}}
{"id": "2403.16101", "pdf": "https://arxiv.org/pdf/2403.16101", "abs": "https://arxiv.org/abs/2403.16101", "authors": ["Yuya Sasaki", "Sohei Tokuno", "Haruka Maeda", "Kazuki Nakajima", "Osamu Sakura", "George Fletcher", "Mykola Pechenizkiy", "Panagiotis Karras", "Irina Shklovski"], "title": "Evaluating Fairness Metrics Across Borders from Human Perceptions", "categories": ["cs.AI"], "comment": null, "summary": "Which fairness metrics are appropriately applicable in your contexts? There\nmay be instances of discordance regarding the perception of fairness, even when\nthe outcomes comply with established fairness metrics. Several\nquestionnaire-based surveys have been conducted to evaluate fairness metrics\nwith human perceptions of fairness. However, these surveys were limited in\nscope, including only a few hundred participants within a single country. In\nthis study, we conduct an international survey to evaluate public perceptions\nof various fairness metrics in decision-making scenarios. We collected\nresponses from 1,000 participants in each of China, France, Japan, and the\nUnited States, amassing a total of 4,000 participants, to analyze the\npreferences of fairness metrics. Our survey consists of three distinct\nscenarios paired with four fairness metrics. This investigation explores the\nrelationship between personal attributes and the choice of fairness metrics,\nuncovering a significant influence of national context on these preferences.", "AI": {"tldr": "An international survey evaluates public perceptions of fairness metrics across four countries, revealing national context significantly influences preferences.", "motivation": "To address the discordance between fairness metrics and human perceptions, and expand beyond limited-scope surveys.", "method": "Conducted an international survey with 4,000 participants (1,000 each from China, France, Japan, and the US) across three scenarios and four fairness metrics.", "result": "National context significantly influences preferences for fairness metrics.", "conclusion": "Fairness perceptions vary by national context, highlighting the need for culturally aware fairness metrics."}}
{"id": "2405.19313", "pdf": "https://arxiv.org/pdf/2405.19313", "abs": "https://arxiv.org/abs/2405.19313", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice", "categories": ["cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "The observed similarities in the behavior of humans and Large Language Models\n(LLMs) have prompted researchers to consider the potential of using LLMs as\nmodels of human cognition. However, several significant challenges must be\naddressed before LLMs can be legitimately regarded as cognitive models. For\ninstance, LLMs are trained on far more data than humans typically encounter,\nand may have been directly trained on human data in specific cognitive tasks or\naligned with human preferences. Consequently, the origins of these behavioral\nsimilarities are not well understood. In this paper, we propose a novel way to\nenhance the utility of LLMs as cognitive models. This approach involves (i)\nleveraging computationally equivalent tasks that both an LLM and a rational\nagent need to master for solving a cognitive problem and (ii) examining the\nspecific task distributions required for an LLM to exhibit human-like\nbehaviors. We apply this approach to decision-making -- specifically risky and\nintertemporal choice -- where the key computationally equivalent task is the\narithmetic of expected value calculations. We show that an LLM pretrained on an\necologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts\nhuman behavior better than many traditional cognitive models. Pretraining LLMs\non ecologically valid arithmetic datasets is sufficient to produce a strong\ncorrespondence between these models and human decision-making. Our results also\nsuggest that LLMs used as cognitive models should be carefully investigated via\nablation studies of the pretraining data.", "AI": {"tldr": "The paper explores using LLMs as cognitive models by focusing on computationally equivalent tasks and ecologically valid datasets, showing improved prediction of human behavior in decision-making.", "motivation": "To address challenges in using LLMs as cognitive models due to unclear origins of behavioral similarities and excessive training data.", "method": "Proposes leveraging computationally equivalent tasks and ecologically valid datasets, applied to decision-making (risky and intertemporal choice).", "result": "LLMs pretrained on ecologically valid arithmetic datasets (Arithmetic-GPT) predict human behavior better than traditional cognitive models.", "conclusion": "LLMs can enhance cognitive modeling with careful pretraining data selection and ablation studies."}}
{"id": "2505.03623", "pdf": "https://arxiv.org/pdf/2505.03623", "abs": "https://arxiv.org/abs/2505.03623", "authors": ["Alessandro Simoni", "Francesco Pelosin"], "title": "Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map", "categories": ["cs.CV"], "comment": "Accepted at Synthetic Data for Computer Vision Workshop - CVPR 2025", "summary": "Synthetic dataset generation in Computer Vision, particularly for industrial\napplications, is still underexplored. Industrial defect segmentation, for\ninstance, requires highly accurate labels, yet acquiring such data is costly\nand time-consuming. To address this challenge, we propose a novel\ndiffusion-based pipeline for generating high-fidelity industrial datasets with\nminimal supervision. Our approach conditions the diffusion model on enriched\nbounding box representations to produce precise segmentation masks, ensuring\nrealistic and accurately localized defect synthesis. Compared to existing\nlayout-conditioned generative methods, our approach improves defect consistency\nand spatial accuracy. We introduce two quantitative metrics to evaluate the\neffectiveness of our method and assess its impact on a downstream segmentation\ntask trained on real and synthetic data. Our results demonstrate that\ndiffusion-based synthesis can bridge the gap between artificial and real-world\nindustrial data, fostering more reliable and cost-efficient segmentation\nmodels. The code is publicly available at\nhttps://github.com/covisionlab/diffusion_labeling.", "AI": {"tldr": "A diffusion-based pipeline for generating high-fidelity synthetic industrial datasets with minimal supervision, improving defect consistency and spatial accuracy.", "motivation": "Industrial defect segmentation requires costly and time-consuming labeled data, which synthetic datasets can mitigate.", "method": "Conditions a diffusion model on enriched bounding box representations to produce precise segmentation masks.", "result": "Outperforms existing layout-conditioned methods in defect consistency and spatial accuracy, validated by new metrics.", "conclusion": "Diffusion-based synthesis bridges the gap between synthetic and real industrial data, enabling reliable and cost-efficient segmentation models."}}
{"id": "2505.03146", "pdf": "https://arxiv.org/pdf/2505.03146", "abs": "https://arxiv.org/abs/2505.03146", "authors": ["Fei Han", "Pengming Guo", "Hao Chen", "Weikun Li", "Jingbo Ren", "Naijun Liu", "Ning Yang", "Dixia Fan"], "title": "Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization", "categories": ["cs.RO", "cs.LG"], "comment": "This work has been accepted for publication in the IEEE International\n  Conference on Robotics and Automation (ICRA) 2025. The final version will be\n  available in IEEE Xplore (DOI to be assigned upon publication)", "summary": "This paper presents a Long Short-Term Memory network-based Fluid Experiment\nData-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic\nforces on the underwater quadruped robot we constructed. Trained on\nexperimental data from leg force and body drag tests conducted in both a\nrecirculating water tank and a towing tank, FED-LSTM outperforms traditional\nEmpirical Formulas (EF) commonly used for flow prediction over flat surfaces.\nThe model demonstrates superior accuracy and adaptability in capturing complex\nfluid dynamics, particularly in straight-line and turning-gait optimizations\nvia the NSGA-II algorithm. FED-LSTM reduces deflection errors during\nstraight-line swimming and improves turn times without increasing the turning\nradius. Hardware experiments further validate the model's precision and\nstability over EF. This approach provides a robust framework for enhancing the\nswimming performance of legged robots, laying the groundwork for future\nadvances in underwater robotic locomotion.", "AI": {"tldr": "FED-LSTM, an LSTM-based model, predicts hydrodynamic forces on underwater robots better than traditional methods, improving swimming performance.", "motivation": "To enhance the prediction of unsteady, nonlinear hydrodynamic forces on underwater legged robots for better locomotion.", "method": "Uses experimental data from leg force and body drag tests to train FED-LSTM, comparing it with Empirical Formulas (EF).", "result": "FED-LSTM outperforms EF in accuracy and adaptability, reducing errors in swimming and improving turn times.", "conclusion": "FED-LSTM provides a robust framework for improving underwater robot locomotion, with potential for future advancements."}}
{"id": "2408.10932", "pdf": "https://arxiv.org/pdf/2408.10932", "abs": "https://arxiv.org/abs/2408.10932", "authors": ["Nikolaos Pippas", "Elliot A. Ludvig", "Cagatay Turkay"], "title": "The Evolution of Reinforcement Learning in Quantitative Finance: A Survey", "categories": ["cs.AI", "cs.CE", "cs.LG", "I.2.6; I.2.1"], "comment": "This work is accepted by ACM Computing Surveys on 18 April 2025 and\n  an early access version is already available here:\n  https://dl.acm.org/doi/10.1145/3733714. The arXiv copy (and the ACM CSUR\n  early-access version) is an unedited, pre-print version and it is the\n  author's version of the work", "summary": "Reinforcement Learning (RL) has experienced significant advancement over the\npast decade, prompting a growing interest in applications within finance. This\nsurvey critically evaluates 167 publications, exploring diverse RL applications\nand frameworks in finance. Financial markets, marked by their complexity,\nmulti-agent nature, information asymmetry, and inherent randomness, serve as an\nintriguing test-bed for RL. Traditional finance offers certain solutions, and\nRL advances these with a more dynamic approach, incorporating machine learning\nmethods, including transfer learning, meta-learning, and multi-agent solutions.\nThis survey dissects key RL components through the lens of Quantitative\nFinance. We uncover emerging themes, propose areas for future research, and\ncritique the strengths and weaknesses of existing methods.", "AI": {"tldr": "A survey of 167 publications evaluates RL applications in finance, highlighting its dynamic approach and potential in complex financial markets.", "motivation": "To explore and critique the use of RL in finance, given the field's complexity and suitability for RL methods.", "method": "Critical evaluation of 167 publications, analyzing RL frameworks, applications, and emerging themes in finance.", "result": "Identifies strengths and weaknesses of RL in finance, proposes future research areas, and highlights dynamic advancements over traditional methods.", "conclusion": "RL shows promise in finance but requires further research to address limitations and leverage emerging techniques like transfer and meta-learning."}}
{"id": "2407.06606", "pdf": "https://arxiv.org/pdf/2407.06606", "abs": "https://arxiv.org/abs/2407.06606", "authors": ["David Gimeno-G\u00f3mez", "Carlos-D. Mart\u00ednez-Hinarejos"], "title": "Tailored Design of Audio-Visual Speech Recognition Models using Branchformers", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted in Computer Speech & Language journal of Elsevier", "summary": "Recent advances in Audio-Visual Speech Recognition (AVSR) have led to\nunprecedented achievements in the field, improving the robustness of this type\nof system in adverse, noisy environments. In most cases, this task has been\naddressed through the design of models composed of two independent encoders,\neach dedicated to a specific modality. However, while recent works have\nexplored unified audio-visual encoders, determining the optimal cross-modal\narchitecture remains an ongoing challenge. Furthermore, such approaches often\nrely on models comprising vast amounts of parameters and high computational\ncost training processes. In this paper, we aim to bridge this research gap by\nintroducing a novel audio-visual framework. Our proposed method constitutes, to\nthe best of our knowledge, the first attempt to harness the flexibility and\ninterpretability offered by encoder architectures, such as the Branchformer, in\nthe design of parameter-efficient AVSR systems. To be more precise, the\nproposed framework consists of two steps: first, estimating audio- and\nvideo-only systems, and then designing a tailored audio-visual unified encoder\nbased on the layer-level branch scores provided by the modality-specific\nmodels. Extensive experiments on English and Spanish AVSR benchmarks covering\nmultiple data conditions and scenarios demonstrated the effectiveness of our\nproposed method. Even when trained on a moderate scale of data, our models\nachieve competitive word error rates (WER) of approximately 2.5\\% for English\nand surpass existing approaches for Spanish, establishing a new benchmark with\nan average WER of around 9.1\\%. These results reflect how our tailored AVSR\nsystem is able to reach state-of-the-art recognition rates while significantly\nreducing the model complexity w.r.t. the prevalent approach in the field. Code\nand pre-trained models are available at\nhttps://github.com/david-gimeno/tailored-avsr.", "AI": {"tldr": "A novel parameter-efficient AVSR framework using Branchformer-inspired encoders achieves competitive WERs (2.5% for English, 9.1% for Spanish) while reducing model complexity.", "motivation": "Address the challenge of designing optimal cross-modal architectures for AVSR, reducing reliance on large, computationally expensive models.", "method": "Proposes a two-step framework: first estimates modality-specific models, then designs a unified encoder using branch scores from these models.", "result": "Achieves state-of-the-art WERs (2.5% for English, 9.1% for Spanish) with reduced complexity.", "conclusion": "The tailored AVSR system balances performance and efficiency, setting a new benchmark for the field."}}
{"id": "2505.03631", "pdf": "https://arxiv.org/pdf/2505.03631", "abs": "https://arxiv.org/abs/2505.03631", "authors": ["Linhan Cao", "Wei Sun", "Kaiwei Zhang", "Yicong Peng", "Guangtao Zhai", "Xiongkuo Min"], "title": "Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Video quality assessment (VQA) is essential for quantifying perceptual\nquality in various video processing workflows, spanning from camera capture\nsystems to over-the-top streaming platforms. While recent supervised VQA models\nhave made substantial progress, the reliance on manually annotated datasets --\na process that is labor-intensive, costly, and difficult to scale up -- has\nhindered further optimization of their generalization to unseen video content\nand distortions. To bridge this gap, we introduce a self-supervised learning\nframework for VQA to learn quality assessment capabilities from large-scale,\nunlabeled web videos. Our approach leverages a \\textbf{learning-to-rank}\nparadigm to train a large multimodal model (LMM) on video pairs automatically\nlabeled via two manners, including quality pseudo-labeling by existing VQA\nmodels and relative quality ranking based on synthetic distortion simulations.\nFurthermore, we introduce a novel \\textbf{iterative self-improvement training\nstrategy}, where the trained model acts an improved annotator to iteratively\nrefine the annotation quality of training data. By training on a dataset\n$10\\times$ larger than the existing VQA benchmarks, our model: (1) achieves\nzero-shot performance on in-domain VQA benchmarks that matches or surpasses\nsupervised models; (2) demonstrates superior out-of-distribution (OOD)\ngeneralization across diverse video content and distortions; and (3) sets a new\nstate-of-the-art when fine-tuned on human-labeled datasets. Extensive\nexperimental results validate the effectiveness of our self-supervised approach\nin training generalized VQA models. The datasets and code will be publicly\nreleased to facilitate future research.", "AI": {"tldr": "A self-supervised learning framework for video quality assessment (VQA) is introduced, leveraging unlabeled web videos and iterative self-improvement to outperform supervised models in generalization and performance.", "motivation": "Overcome the limitations of manually annotated datasets in VQA, which are labor-intensive and hard to scale, by using self-supervised learning.", "method": "Uses a learning-to-rank paradigm and iterative self-improvement training on large-scale unlabeled videos, with pseudo-labeling and synthetic distortions.", "result": "Achieves zero-shot performance matching supervised models, superior OOD generalization, and state-of-the-art results when fine-tuned.", "conclusion": "The self-supervised approach effectively trains generalized VQA models, with datasets and code to be released for future research."}}
{"id": "2505.03159", "pdf": "https://arxiv.org/pdf/2505.03159", "abs": "https://arxiv.org/abs/2505.03159", "authors": ["Zaid Ghazal", "Ali Al-Bustami", "Khouloud Gaaloul", "Jaerock Kwon"], "title": "Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "PID controllers are widely used in control systems because of their\nsimplicity and effectiveness. Although advanced optimization techniques such as\nBayesian Optimization and Differential Evolution have been applied to address\nthe challenges of automatic tuning of PID controllers, the influence of initial\nsystem states on convergence and the balance between exploration and\nexploitation remains underexplored. Moreover, experimenting the influence\ndirectly on real cyber-physical systems such as mobile robots is crucial for\nderiving realistic insights. In the present paper, a novel framework is\nintroduced to evaluate the impact of systematically varying these factors on\nthe PID auto-tuning processes that utilize Bayesian Optimization and\nDifferential Evolution. Testing was conducted on two distinct PID-controlled\nrobotic platforms, an omnidirectional robot and a differential drive mobile\nrobot, to assess the effects on convergence rate, settling time, rise time, and\novershoot percentage. As a result, the experimental outcomes yield evidence on\nthe effects of the systematic variations, thereby providing an empirical basis\nfor future research studies in the field.", "AI": {"tldr": "The paper introduces a framework to study the impact of initial system states and exploration-exploitation balance on PID auto-tuning using Bayesian Optimization and Differential Evolution, tested on two robotic platforms.", "motivation": "To address the underexplored influence of initial system states and exploration-exploitation balance in PID auto-tuning, especially for real cyber-physical systems like mobile robots.", "method": "A novel framework evaluates systematic variations in PID auto-tuning processes using Bayesian Optimization and Differential Evolution, tested on omnidirectional and differential drive robots.", "result": "Experimental results show the effects of variations on convergence rate, settling time, rise time, and overshoot percentage.", "conclusion": "The findings provide empirical insights for future research in PID auto-tuning for robotic systems."}}
{"id": "2409.16876", "pdf": "https://arxiv.org/pdf/2409.16876", "abs": "https://arxiv.org/abs/2409.16876", "authors": ["Xusen Guo", "Xinxi Yang", "Mingxing Peng", "Hongliang Lu", "Meixin Zhu", "Hai Yang"], "title": "Automating Traffic Model Enhancement with AI Research Agent", "categories": ["cs.AI"], "comment": "27 pages, 12 figures", "summary": "Developing efficient traffic models is crucial for optimizing modern\ntransportation systems. However, current modeling approaches remain\nlabor-intensive and prone to human errors due to their dependence on manual\nworkflows. These processes typically involve extensive literature reviews,\nformula tuning, and iterative testing, which often lead to inefficiencies. To\naddress this, we propose TR-Agent, an AI-powered framework that autonomously\ndevelops and refines traffic models through a closed-loop, iterative process.\nWe structure the research pipeline into four key stages: idea generation,\ntheory formulation, theory evaluation, and iterative optimization, and\nimplement TR-Agent with four corresponding modules. These modules collaborate\nto retrieve knowledge from external sources, generate novel hypotheses,\nimplement and debug models, and evaluate their performance on evaluation\ndatasets. Through iteratively feedback and refinement, TR-Agent improves both\nmodeling efficiency and effectiveness. We validate the framework on three\nrepresentative traffic models: the Intelligent Driver Model (IDM) for\ncar-following behavior, the MOBIL model for lane-changing, and the\nLighthill-Whitham-Richards (LWR) speed-density relationship for macroscopic\ntraffic flow modeling. Experimental results show substantial performance gains\nover the original models. To assess the robustness and generalizability of the\nimprovements, we conduct additional evaluations across multiple real-world\ndatasets, demonstrating consistent performance gains beyond the original\ndevelopment data. Furthermore, TR-Agent produces interpretable explanations for\neach improvement, enabling researchers to easily verify and extend its results.\nThis makes TR-Agent a valuable assistant for traffic modeling refinement and a\npromising tool for broader applications in transportation research.", "AI": {"tldr": "TR-Agent is an AI framework that autonomously develops and refines traffic models, improving efficiency and performance over traditional manual methods.", "motivation": "Current traffic modeling is labor-intensive and error-prone due to manual workflows, necessitating an automated solution.", "method": "TR-Agent uses a closed-loop, iterative process with four modules: idea generation, theory formulation, evaluation, and optimization.", "result": "The framework outperforms original models (IDM, MOBIL, LWR) and shows consistent gains across real-world datasets.", "conclusion": "TR-Agent enhances traffic modeling efficiency and effectiveness, offering interpretable improvements for broader transportation research."}}
{"id": "2410.22086", "pdf": "https://arxiv.org/pdf/2410.22086", "abs": "https://arxiv.org/abs/2410.22086", "authors": ["Zhiqi Bu", "Xiaomeng Jin", "Bhanukiran Vinzamuri", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Mingyi Hong"], "title": "Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to NAACL 2025 main conference", "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.", "AI": {"tldr": "The paper introduces NGDiff, a method for machine unlearning in LLMs, framed as a multi-task optimization problem with a focus on balancing forgetting and model performance.", "motivation": "To address the challenge of removing unwanted knowledge from LLMs while maintaining performance, the paper explores machine unlearning from an optimization perspective.", "method": "Proposes NGDiff, a normalized gradient difference algorithm with an automatic learning rate scheduler, to balance forgetting and performance objectives.", "result": "NGDiff outperforms state-of-the-art methods on TOFU and MUSE datasets, showing stable training.", "conclusion": "NGDiff effectively balances unlearning and model performance, offering a robust solution for machine unlearning in LLMs."}}
{"id": "2505.03638", "pdf": "https://arxiv.org/pdf/2505.03638", "abs": "https://arxiv.org/abs/2505.03638", "authors": ["Jiawan Li", "Fei Zhou", "Zhipeng Zhong", "Jiongzhi Lin", "Guoping Qiu"], "title": "Towards Smart Point-and-Shoot Photography", "categories": ["cs.CV"], "comment": "CVPR2025 Accepted", "summary": "Hundreds of millions of people routinely take photos using their smartphones\nas point and shoot (PAS) cameras, yet very few would have the photography\nskills to compose a good shot of a scene. While traditional PAS cameras have\nbuilt-in functions to ensure a photo is well focused and has the right\nbrightness, they cannot tell the users how to compose the best shot of a scene.\nIn this paper, we present a first of its kind smart point and shoot (SPAS)\nsystem to help users to take good photos. Our SPAS proposes to help users to\ncompose a good shot of a scene by automatically guiding the users to adjust the\ncamera pose live on the scene. We first constructed a large dataset containing\n320K images with camera pose information from 4000 scenes. We then developed an\ninnovative CLIP-based Composition Quality Assessment (CCQA) model to assign\npseudo labels to these images. The CCQA introduces a unique learnable text\nembedding technique to learn continuous word embeddings capable of discerning\nsubtle visual quality differences in the range covered by five levels of\nquality description words {bad, poor, fair, good, perfect}. And finally we have\ndeveloped a camera pose adjustment model (CPAM) which first determines if the\ncurrent view can be further improved and if so it outputs the adjust suggestion\nin the form of two camera pose adjustment angles. The two tasks of CPAM make\ndecisions in a sequential manner and each involves different sets of training\nsamples, we have developed a mixture-of-experts model with a gated loss\nfunction to train the CPAM in an end-to-end manner. We will present extensive\nresults to demonstrate the performances of our SPAS system using publicly\navailable image composition datasets.", "AI": {"tldr": "A smart point-and-shoot (SPAS) system is introduced to help users compose better photos by guiding camera pose adjustments. It uses a dataset of 320K images, a CLIP-based quality assessment model (CCQA), and a camera pose adjustment model (CPAM).", "motivation": "Traditional point-and-shoot cameras lack guidance for composing good shots, despite widespread smartphone photography usage. The SPAS system aims to fill this gap by assisting users in real-time.", "method": "The system involves: 1) Creating a large dataset with camera pose data, 2) Developing a CCQA model for quality assessment using learnable text embeddings, and 3) Training a CPAM model for pose adjustment suggestions.", "result": "The SPAS system demonstrates performance through extensive testing on public image composition datasets.", "conclusion": "The SPAS system successfully aids users in composing better photos by leveraging advanced models for quality assessment and pose adjustment."}}
{"id": "2505.03174", "pdf": "https://arxiv.org/pdf/2505.03174", "abs": "https://arxiv.org/abs/2505.03174", "authors": ["Guillermo Roque", "Erika Maquiling", "Jose Giovanni Tapia Lopez", "Ross Greer"], "title": "Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Instruction-Action (IA) data pairs are valuable for training robotic systems,\nespecially autonomous vehicles (AVs), but having humans manually annotate this\ndata is costly and time-inefficient. This paper explores the potential of using\nmobile application Global Positioning System (GPS) references and Natural\nLanguage Processing (NLP) to automatically generate large volumes of IA\ncommands and responses without having a human generate or retroactively tag the\ndata. In our pilot data collection, by driving to various destinations and\ncollecting voice instructions from GPS applications, we demonstrate a means to\ncollect and categorize the diverse sets of instructions, further accompanied by\nvideo data to form complete vision-language-action triads. We provide details\non our completely automated data collection prototype system, ADVLAT-Engine. We\ncharacterize collected GPS voice instructions into eight different\nclassifications, highlighting the breadth of commands and referentialities\navailable for curation from freely available mobile applications. Through\nresearch and exploration into the automation of IA data pairs using GPS\nreferences, the potential to increase the speed and volume at which\nhigh-quality IA datasets are created, while minimizing cost, can pave the way\nfor robust vision-language-action (VLA) models to serve tasks in\nvision-language navigation (VLN) and human-interactive autonomous systems.", "AI": {"tldr": "The paper proposes using GPS and NLP to automate the generation of Instruction-Action (IA) data pairs for robotics, reducing human annotation costs.", "motivation": "Manual annotation of IA data is costly and inefficient; automating this process can accelerate dataset creation for robotics.", "method": "Collects GPS voice instructions and video data to form vision-language-action triads, using the ADVLAT-Engine prototype.", "result": "Classified GPS instructions into eight categories, demonstrating diverse commands for IA datasets.", "conclusion": "Automating IA data generation with GPS and NLP can enhance vision-language-action models for robotics and autonomous systems."}}
{"id": "2410.01281", "pdf": "https://arxiv.org/pdf/2410.01281", "abs": "https://arxiv.org/abs/2410.01281", "authors": ["Haomin Wen", "Shurui Cao", "Zeeshan Rasheed", "Khurram Hassan Shafique", "Leman Akoglu"], "title": "Uncertainty-aware Human Mobility Modeling and Anomaly Detection", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Given the temporal GPS coordinates from a large set of human agents, how can\nwe model their mobility behavior toward effective anomaly (e.g. bad-actor or\nmalicious behavior) detection without any labeled data? Human mobility and\ntrajectory modeling have been extensively studied, showcasing varying abilities\nto manage complex inputs and balance performance-efficiency trade-offs. In this\nwork, we formulate anomaly detection in complex human behavior by modeling raw\nGPS data as a sequence of stay-point events, each characterized by\nspatio-temporal features, along with trips (i.e. commute) between the\nstay-points. Our problem formulation allows us to leverage modern sequence\nmodels for unsupervised training and anomaly detection. Notably, we equip our\nproposed model USTAD (for Uncertainty-aware Spatio-Temporal Anomaly Detection)\nwith aleatoric (i.e. data) uncertainty estimation to account for inherent\nstochasticity in certain individuals' behavior, as well as epistemic (i.e.\nmodel) uncertainty to handle data sparsity under a large variety of human\nbehaviors. Together, aleatoric and epistemic uncertainties unlock a robust loss\nfunction as well as uncertainty-aware decision-making in anomaly scoring.\nExtensive experiments shows that USTAD improves anomaly detection AUCROC by\n3\\%-15\\% over baselines in industry-scale data.", "AI": {"tldr": "USTAD models human mobility using GPS data as sequences of stay-points and trips, leveraging uncertainty-aware sequence models for unsupervised anomaly detection, outperforming baselines by 3%-15% in AUCROC.", "motivation": "To detect anomalies (e.g., malicious behavior) in human mobility without labeled data by modeling raw GPS data as sequences of stay-points and trips.", "method": "Formulates mobility as sequences of stay-points and trips, using USTAD with aleatoric and epistemic uncertainty estimation for robust anomaly detection.", "result": "USTAD improves anomaly detection AUCROC by 3%-15% over baselines on industry-scale data.", "conclusion": "USTAD effectively detects anomalies in human mobility by modeling uncertainty, demonstrating significant performance gains."}}
{"id": "2411.18915", "pdf": "https://arxiv.org/pdf/2411.18915", "abs": "https://arxiv.org/abs/2411.18915", "authors": ["Vishnou Vinayagame", "Gregory Senay", "Luis Mart\u00ed"], "title": "MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Business documents often contain substantial tabular and textual information\nwith numerical values, requiring mathematical reasoning for effective document\nunderstanding. While Small Language Models (SLMs) still struggle at this task,\ntool-augmented multi-step agents perform better, at the cost of relying on\nclosed-source or larger models, external data, or extensive prompt-engineering.\nThis work introduces MATATA, a novel weakly supervised end-to-end approach to\ntrain multi-step reasoning language agents for document tabular applications.\nMATATA presents an annotation-free paradigm for each agent to enhance 3.8B/8B\nSLMs. During its two-stage training, MATATA uses the final outcome of the\nmulti-step reasoning chain as weak supervision. This approach avoids having to\nindividually supervise each intermediate agent in the reasoning chain. By\nemploying an adaptive planner and shared tools across different datasets,\nMATATA shows robust performance. Experiments demonstrate that MATATA achieves\nstate-of-the-art on FinQA, and on TAT-QA among reasoning methods based on\nopen-source SLMs. Although being SLM-based, MATATA closely matches GPT-4-based\nframeworks on TabMWP. This novel weakly supervised approach enables training an\nend-to-end multi-step reasoning agent without intermediate supervision,\nsupporting future developments of cost-effective powerful agentic systems.", "AI": {"tldr": "MATATA is a weakly supervised method to train multi-step reasoning language agents for document tabular applications, achieving state-of-the-art performance without intermediate supervision.", "motivation": "Business documents require mathematical reasoning for understanding, but current methods rely on closed-source or larger models, external data, or extensive prompt-engineering.", "method": "MATATA uses a two-stage training approach with weak supervision from the final reasoning outcome, avoiding intermediate supervision. It employs an adaptive planner and shared tools.", "result": "MATATA achieves state-of-the-art on FinQA and TAT-QA, closely matching GPT-4-based frameworks on TabMWP.", "conclusion": "MATATA enables cost-effective, powerful agentic systems by training end-to-end multi-step reasoning agents without intermediate supervision."}}
{"id": "2505.03667", "pdf": "https://arxiv.org/pdf/2505.03667", "abs": "https://arxiv.org/abs/2505.03667", "authors": ["Fu Feng", "Yucheng Xie", "Xu Yang", "Jing Wang", "Xin Geng"], "title": "Distribution-Conditional Generation: From Class Distribution to Creative Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) diffusion models are effective at producing semantically\naligned images, but their reliance on training data distributions limits their\nability to synthesize truly novel, out-of-distribution concepts. Existing\nmethods typically enhance creativity by combining pairs of known concepts,\nyielding compositions that, while out-of-distribution, remain linguistically\ndescribable and bounded within the existing semantic space. Inspired by the\nsoft probabilistic outputs of classifiers on ambiguous inputs, we propose\nDistribution-Conditional Generation, a novel formulation that models creativity\nas image synthesis conditioned on class distributions, enabling semantically\nunconstrained creative generation. Building on this, we propose DisTok, an\nencoder-decoder framework that maps class distributions into a latent space and\ndecodes them into tokens of creative concept. DisTok maintains a dynamic\nconcept pool and iteratively sampling and fusing concept pairs, enabling the\ngeneration of tokens aligned with increasingly complex class distributions. To\nenforce distributional consistency, latent vectors sampled from a Gaussian\nprior are decoded into tokens and rendered into images, whose class\ndistributions-predicted by a vision-language model-supervise the alignment\nbetween input distributions and the visual semantics of generated tokens. The\nresulting tokens are added to the concept pool for subsequent composition.\nExtensive experiments demonstrate that DisTok, by unifying\ndistribution-conditioned fusion and sampling-based synthesis, enables efficient\nand flexible token-level generation, achieving state-of-the-art performance\nwith superior text-image alignment and human preference scores.", "AI": {"tldr": "DisTok introduces a method for creative text-to-image generation by conditioning on class distributions, enabling novel concept synthesis beyond training data.", "motivation": "Overcoming limitations of existing T2I models in generating truly novel, out-of-distribution concepts.", "method": "Proposes Distribution-Conditional Generation and DisTok, an encoder-decoder framework for latent space mapping and iterative concept fusion.", "result": "Achieves state-of-the-art performance with superior text-image alignment and human preference.", "conclusion": "DisTok enables flexible, efficient token-level generation of creative concepts."}}
{"id": "2505.03177", "pdf": "https://arxiv.org/pdf/2505.03177", "abs": "https://arxiv.org/abs/2505.03177", "authors": ["Keilung Choy", "Wei Xie", "Keqi Wang"], "title": "A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "comment": "11 pages, 2 figures", "summary": "Bioprocess mechanistic modeling is essential for advancing intelligent\ndigital twin representation of biomanufacturing, yet challenges persist due to\ncomplex intracellular regulation, stochastic system behavior, and limited\nexperimental data. This paper introduces a symbolic and statistical learning\nframework to identify key regulatory mechanisms and quantify model uncertainty.\nBioprocess dynamics is formulated with stochastic differential equations\ncharacterizing intrinsic process variability, with a predefined set of\ncandidate regulatory mechanisms constructed from biological knowledge. A\nBayesian learning approach is developed, which is based on a joint learning of\nkinetic parameters and regulatory structure through a formulation of the\nmixture model. To enhance computational efficiency, a Metropolis-adjusted\nLangevin algorithm with adjoint sensitivity analysis is developed for posterior\nexploration. Compared to state-of-the-art Bayesian inference approaches, the\nproposed framework achieves improved sample efficiency and robust model\nselection. An empirical study demonstrates its ability to recover missing\nregulatory mechanisms and improve model fidelity under data-limited conditions.", "AI": {"tldr": "A framework combining symbolic and statistical learning is proposed for bioprocess mechanistic modeling, addressing challenges like intracellular regulation and data scarcity. It uses stochastic differential equations and Bayesian learning for efficient parameter and regulatory structure identification.", "motivation": "The complexity of intracellular regulation, stochastic behavior, and limited experimental data in bioprocess modeling necessitates a robust framework to identify key mechanisms and quantify uncertainty.", "method": "The method involves stochastic differential equations for bioprocess dynamics, a Bayesian learning approach for joint parameter and regulatory structure learning, and a Metropolis-adjusted Langevin algorithm for computational efficiency.", "result": "The framework outperforms state-of-the-art Bayesian inference methods in sample efficiency and model selection, successfully recovering missing regulatory mechanisms and improving model fidelity with limited data.", "conclusion": "The proposed framework effectively addresses bioprocess modeling challenges, offering improved computational efficiency and robustness in identifying regulatory mechanisms under data constraints."}}
{"id": "2412.01487", "pdf": "https://arxiv.org/pdf/2412.01487", "abs": "https://arxiv.org/abs/2412.01487", "authors": ["Gabriela Ben-Melech Stan", "Estelle Aflalo", "Man Luo", "Shachar Rosenman", "Tiep Le", "Sayak Paul", "Shao-Yen Tseng", "Vasudev Lal"], "title": "FastRM: An efficient and automatic explainability framework for multimodal generative models", "categories": ["cs.AI"], "comment": null, "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable reasoning\ncapabilities over textual and visual inputs. However, these models remain prone\nto generating misinformation. Identifying and mitigating ungrounded responses\nis crucial for developing trustworthy AI. Traditional explainability methods\nsuch as gradient-based relevancy maps, offer insight into the decision process\nof models, but are often computationally expensive and unsuitable for real-time\noutput validation. In this work, we introduce FastRM, an efficient method for\npredicting explainable Relevancy Maps of LVLMs. Furthermore, FastRM provides\nboth quantitative and qualitative assessment of model confidence. Experimental\nresults demonstrate that FastRM achieves a 99.8% reduction in computation time\nand a 44.4% reduction in memory footprint compared to traditional relevancy map\ngeneration. FastRM allows explainable AI to be more practical and scalable,\nthereby promoting its deployment in real-world applications and enabling users\nto more effectively evaluate the reliability of model outputs.", "AI": {"tldr": "FastRM is an efficient method for generating explainable Relevancy Maps for Large Vision Language Models (LVLMs), reducing computation time by 99.8% and memory usage by 44.4% compared to traditional methods.", "motivation": "LVLMs generate misinformation, and traditional explainability methods are computationally expensive, making real-time validation impractical.", "method": "Introduces FastRM, a method for predicting explainable Relevancy Maps with quantitative and qualitative confidence assessment.", "result": "FastRM significantly reduces computation time (99.8%) and memory footprint (44.4%) while maintaining explainability.", "conclusion": "FastRM makes explainable AI more practical and scalable, enhancing trustworthiness and usability in real-world applications."}}
{"id": "2412.17739", "pdf": "https://arxiv.org/pdf/2412.17739", "abs": "https://arxiv.org/abs/2412.17739", "authors": ["Ermo Hua", "Che Jiang", "Xingtai Lv", "Kaiyan Zhang", "Ning Ding", "Youbang Sun", "Biqing Qi", "Yuchen Fan", "Xuekai Zhu", "Bowen Zhou"], "title": "Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "Extending the context length of Language Models (LMs) by improving Rotary\nPosition Embedding (RoPE) has become a trend. While existing works mainly\naddress RoPE's limitations within attention mechanism, this paper provides an\nanalysis across nearly all parts of LMs, uncovering their adverse effects on\nlength generalization for RoPE-based attention. Using Discrete Signal\nProcessing theory, we show that RoPE enables periodic attention by implicitly\nachieving Non-Uniform Discrete Fourier Transform. However, this periodicity is\nundermined by the spectral damage caused by: 1) linear layers and activation\nfunctions outside of attention; 2) insufficiently trained frequency components\nbrought by time-domain truncation. Building on our observations, we propose\nFourier Position Embedding (FoPE), which enhances attention's frequency-domain\nproperties to improve both its periodic extension and length generalization.\nFoPE constructs Fourier Series and zero-outs the destructive frequency\ncomponents, increasing model robustness against the spectrum damage.\nExperiments across various model scales and benchmarks show that, within\nvarying context windows, FoPE maintains a more stable performance compared to\nRoPE and ALiBi. Several analyses and ablations bring further support to our\nmethod and theoretical modeling.", "AI": {"tldr": "The paper introduces Fourier Position Embedding (FoPE) to address limitations of Rotary Position Embedding (RoPE) in Language Models, improving length generalization and periodic attention.", "motivation": "Existing works focus on RoPE's limitations within attention, but this paper analyzes its adverse effects across all LM parts, identifying spectral damage as a key issue.", "method": "Using Discrete Signal Processing, the authors show RoPE's periodic attention is undermined by spectral damage. They propose FoPE, which enhances frequency-domain properties by zeroing destructive components.", "result": "FoPE outperforms RoPE and ALiBi in stability across varying context windows, supported by experiments and analyses.", "conclusion": "FoPE effectively mitigates spectral damage, improving LM performance and generalization, with empirical validation."}}
{"id": "2505.03679", "pdf": "https://arxiv.org/pdf/2505.03679", "abs": "https://arxiv.org/abs/2505.03679", "authors": ["Huawei Sun", "Bora Kunter Sahin", "Georg Stettinger", "Maximilian Bernhard", "Matthias Schubert", "Robert Wille"], "title": "CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting", "categories": ["cs.CV"], "comment": "Accepted at RA-L 2025", "summary": "Segmenting objects in an environment is a crucial task for autonomous driving\nand robotics, as it enables a better understanding of the surroundings of each\nagent. Although camera sensors provide rich visual details, they are vulnerable\nto adverse weather conditions. In contrast, radar sensors remain robust under\nsuch conditions, but often produce sparse and noisy data. Therefore, a\npromising approach is to fuse information from both sensors. In this work, we\npropose a novel framework to enhance camera-only baselines by integrating a\ndiffusion model into a camera-radar fusion architecture. We leverage radar\npoint features to create pseudo-masks using the Segment-Anything model,\ntreating the projected radar points as point prompts. Additionally, we propose\na noise reduction unit to denoise these pseudo-masks, which are further used to\ngenerate inpainted images that complete the missing information in the original\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\nsemantic segmentation using camera-radar fusion under adverse weather\nconditions.", "AI": {"tldr": "A novel framework integrates radar data with camera images using a diffusion model and pseudo-masks to improve semantic segmentation in adverse weather.", "motivation": "Camera sensors are vulnerable to adverse weather, while radar is robust but noisy. Combining both can enhance segmentation accuracy.", "method": "Uses radar point features to create pseudo-masks with the Segment-Anything model, denoises them, and generates inpainted images for missing data.", "result": "Improves camera-only segmentation by 2.63% and camera-radar fusion by 1.48% in mIoU on the Waterscenes dataset.", "conclusion": "The approach effectively enhances semantic segmentation in adverse weather by fusing camera and radar data."}}
{"id": "2505.03201", "pdf": "https://arxiv.org/pdf/2505.03201", "abs": "https://arxiv.org/abs/2505.03201", "authors": ["Kien Tran Duc Tuan", "Tam Nguyen Trong", "Son Nguyen Hoang", "Khoat Than", "Anh Nguyen Duc"], "title": "Weighted Average Gradients for Feature Attribution", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In explainable AI, Integrated Gradients (IG) is a widely adopted technique\nfor assessing the significance of feature attributes of the input on model\noutputs by evaluating contributions from a baseline input to the current input.\nThe choice of the baseline input significantly influences the resulting\nexplanation. While the traditional Expected Gradients (EG) method assumes\nbaselines can be uniformly sampled and averaged with equal weights, this study\nargues that baselines should not be treated equivalently. We introduce Weighted\nAverage Gradients (WG), a novel approach that unsupervisedly evaluates baseline\nsuitability and incorporates a strategy for selecting effective baselines.\nTheoretical analysis demonstrates that WG satisfies essential explanation\nmethod criteria and offers greater stability than prior approaches.\nExperimental results further confirm that WG outperforms EG across diverse\nscenarios, achieving an improvement of 10-35\\% on main metrics. Moreover, by\nevaluating baselines, our method can filter a subset of effective baselines for\neach input to calculate explanations, maintaining high accuracy while reducing\ncomputational cost. The code is available at:\nhttps://github.com/Tamnt240904/weighted_baseline.", "AI": {"tldr": "Weighted Average Gradients (WG) improves baseline selection in explainable AI over Expected Gradients (EG), offering better stability, accuracy, and computational efficiency.", "motivation": "The choice of baseline inputs in Integrated Gradients (IG) significantly impacts explanations, but traditional methods like EG treat baselines uniformly. This study argues for weighted baseline evaluation.", "method": "WG unsupervisedly evaluates baseline suitability and selects effective baselines, satisfying explanation criteria and ensuring stability.", "result": "WG outperforms EG by 10-35% in metrics, maintains accuracy, and reduces computational costs by filtering effective baselines.", "conclusion": "WG provides a superior, efficient alternative to EG for explainable AI, with theoretical and experimental validation."}}
{"id": "2412.18116", "pdf": "https://arxiv.org/pdf/2412.18116", "abs": "https://arxiv.org/abs/2412.18116", "authors": ["Hao Wen", "Shizuo Tian", "Borislav Pavlov", "Wenjie Du", "Yixuan Li", "Ge Chang", "Shanhui Zhao", "Jiacheng Liu", "Yunxin Liu", "Ya-Qin Zhang", "Yuanchun Li"], "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation", "categories": ["cs.AI"], "comment": "13 pages, 5 figures", "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand powerful large language models that are difficult to be deployed\nlocally on end-users' devices, raising huge concerns about user privacy and\ncentralized serving cost. Inspired by the remarkable coding abilities of recent\nsmall language models (SLMs), we propose to convert the UI task automation\nproblem to a code generation problem, which can be effectively solved by an\non-device SLM and efficiently executed with an on-device code interpreter.\nUnlike normal coding tasks that can be extensively pre-trained with public\ndatasets, generating UI automation code is challenging due to the diversity,\ncomplexity, and variability of target apps. Therefore, we adopt a\ndocument-centered approach that automatically builds fine-grained API\ndocumentation for each app and generates diverse task samples based on this\ndocumentation. By guiding the agent with the synthetic documents and task\nsamples, it learns to generate precise and efficient scripts to complete unseen\ntasks. Based on detailed comparisons with state-of-the-art mobile UI agents,\nour approach effectively improves the mobile task automation with significantly\nhigher success rates and lower latency/token consumption. Code is open-sourced\nat https://github.com/MobileLLM/AutoDroid-V2.", "AI": {"tldr": "The paper proposes converting mobile UI task automation into a code generation problem using small language models (SLMs) for on-device execution, improving privacy and efficiency.", "motivation": "Existing UI agents rely on large language models (LLMs), which are hard to deploy locally, raising privacy and cost concerns. SLMs offer a viable alternative.", "method": "The approach converts UI automation into code generation, using an on-device SLM and interpreter. It builds fine-grained API documentation for apps and generates diverse task samples for training.", "result": "The method outperforms state-of-the-art UI agents with higher success rates, lower latency, and reduced token consumption.", "conclusion": "The proposed approach enables efficient, privacy-preserving mobile task automation using SLMs, with open-sourced code available."}}
{"id": "2503.15661", "pdf": "https://arxiv.org/pdf/2503.15661", "abs": "https://arxiv.org/abs/2503.15661", "authors": ["Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Juan A. Rodriguez", "Montek Kalsi", "Rabiul Awal", "Nicolas Chapados", "M. Tamer \u00d6zsu", "Aishwarya Agrawal", "David Vazquez", "Christopher Pal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "This paper has been accepted to the 41st International Conference on\n  Machine Learning (ICML 2025)", "summary": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks.", "AI": {"tldr": "UI-Vision is a new benchmark for evaluating autonomous agents in desktop environments, addressing gaps in existing research by providing dense annotations and tasks for rigorous evaluation.", "motivation": "Desktop environments are critical but underexplored due to data and licensing challenges. UI-Vision aims to fill this gap.", "method": "UI-Vision offers dense annotations (bounding boxes, labels, action trajectories) across 83 apps and three tasks (Element Grounding, Layout Grounding, Action Prediction).", "result": "Evaluation shows limitations in state-of-the-art models (e.g., UI-TARS-72B) in handling professional software and complex actions.", "conclusion": "UI-Vision, released as open-source, aims to advance autonomous agent development for real-world desktop tasks."}}
{"id": "2505.03692", "pdf": "https://arxiv.org/pdf/2505.03692", "abs": "https://arxiv.org/abs/2505.03692", "authors": ["Shiqi Li", "Jihua Zhu", "Yifan Xie", "Naiwen Hu", "Di Wang"], "title": "Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Multiview point cloud registration plays a crucial role in robotics,\nautomation, and computer vision fields. This paper concentrates on pose graph\nconstruction and motion synchronization within multiview registration. Previous\nmethods for pose graph construction often pruned fully connected graphs or\nconstructed sparse graph using global feature aggregated from local\ndescriptors, which may not consistently yield reliable results. To identify\ndependable pairs for pose graph construction, we design a network model that\nextracts information from the matching distance between point cloud pairs. For\nmotion synchronization, we propose another neural network model to calculate\nthe absolute pose in a data-driven manner, rather than optimizing inaccurate\nhandcrafted loss functions. Our model takes into account geometric distribution\ninformation and employs a modified attention mechanism to facilitate flexible\nand reliable feature interaction. Experimental results on diverse indoor and\noutdoor datasets confirm the effectiveness and generalizability of our\napproach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.", "AI": {"tldr": "The paper introduces neural network models for pose graph construction and motion synchronization in multiview point cloud registration, improving reliability and accuracy.", "motivation": "Previous methods for pose graph construction and motion synchronization often relied on unreliable pruning or handcrafted loss functions, leading to inconsistent results.", "method": "The authors propose two neural network models: one for identifying dependable pairs in pose graph construction using matching distance, and another for calculating absolute poses in motion synchronization using geometric distribution and a modified attention mechanism.", "result": "Experiments on indoor and outdoor datasets show the approach's effectiveness and generalizability.", "conclusion": "The proposed data-driven models outperform traditional methods, offering more reliable and flexible solutions for multiview registration."}}
{"id": "2505.03223", "pdf": "https://arxiv.org/pdf/2505.03223", "abs": "https://arxiv.org/abs/2505.03223", "authors": ["Spencer Compton", "Chirag Pabbaraju", "Nikita Zhivotovskiy"], "title": "Lower Bounds for Greedy Teaching Set Constructions", "categories": ["stat.ML", "cs.DS", "cs.LG", "math.CO"], "comment": null, "summary": "A fundamental open problem in learning theory is to characterize the\nbest-case teaching dimension $\\operatorname{TS}_{\\min}$ of a concept class\n$\\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in\nparticular, settle the conjectured upper bound on Recursive Teaching Dimension\nposed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy\nalgorithm to construct teaching sets recursively, thereby proving upper bounds\non $\\operatorname{TS}_{\\min}$, with the best known bound being $O(d^2)$ [Hu,\nWu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses\nto add to the teaching set the $k$ labeled points that restrict the concept\nclass the most. In this work, we prove lower bounds on the performance of this\ngreedy approach for small $k$. Specifically, we show that for $k = 1$, the\nalgorithm does not improve upon the halving-based bound of\n$O(\\log(|\\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper\nbound of $O\\left(\\log(\\log(|\\mathcal{C}|))\\right)$ from [Moran, Shpilka,\nWigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most\nconsequentially, our lower bound extends up to $k \\le \\lceil c d \\rceil$ for\nsmall constant $c>0$: suggesting that studying higher-order interactions may be\nnecessary to resolve the conjecture that $\\operatorname{TS}_{\\min} = O(d)$.", "AI": {"tldr": "The paper investigates the performance of a greedy algorithm for constructing teaching sets in concept classes with finite VC dimension, proving lower bounds for small k and suggesting higher-order interactions may be needed to resolve a key conjecture.", "motivation": "To address the open problem of characterizing the best-case teaching dimension for concept classes with finite VC dimension, particularly to settle a conjecture about the Recursive Teaching Dimension.", "method": "Analyzes a greedy algorithm that recursively constructs teaching sets by adding k labeled points per iteration, focusing on small k (k=1, k=2, and up to k \u2264 \u2308cd\u2309).", "result": "For k=1, the algorithm performs no better than the halving-based bound. For k=2, a matching lower bound is proven. The results suggest higher-order interactions may be necessary to resolve the conjecture.", "conclusion": "The study highlights limitations of the greedy approach for small k and implies that resolving the conjecture may require exploring higher-order interactions."}}
{"id": "2501.05675", "pdf": "https://arxiv.org/pdf/2501.05675", "abs": "https://arxiv.org/abs/2501.05675", "authors": ["Feiyi Chen", "Leilei Zhang", "Guansong Pang", "Roger Zimmermann", "Shuiguang Deng"], "title": "Synergizing Large Language Models and Task-specific Models for Time Series Anomaly Detection", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "In anomaly detection, methods based on large language models (LLMs) can\nincorporate expert knowledge by reading professional document, while\ntask-specific small models excel at extracting normal data patterns and\ndetecting value fluctuations from training data of target applications.\nInspired by the human nervous system, where the brain stores expert knowledge\nand the peripheral nervous system and spinal cord handle specific tasks like\nwithdrawal and knee-jerk reflexes, we propose CoLLaTe, a framework designed to\nfacilitate collaboration between LLMs and task-specific models, leveraging the\nstrengths of both models for anomaly detection.\n  In particular, we first formulate the collaboration process and identify two\nkey challenges in the collaboration:\n  (1) the misalignment between the expression domains of the LLMs and\ntask-specific small models, and (2) error accumulation arising from the\npredictions of both models.\n  To address these challenges, we then introduce two key components in CoLLaTe:\na model alignment module and a collaborative loss function. Through theoretical\nanalysis and experimental validation, we demonstrate that these components\neffectively mitigate the identified challenges and achieve better performance\nthan both LLM-based and task-specific models.", "AI": {"tldr": "CoLLaTe is a framework combining LLMs and task-specific models for anomaly detection, addressing misalignment and error accumulation with alignment modules and collaborative loss functions.", "motivation": "Inspired by the human nervous system, the paper aims to leverage the strengths of both LLMs (expert knowledge) and task-specific models (data pattern extraction) for better anomaly detection.", "method": "Proposes CoLLaTe with a model alignment module and collaborative loss function to tackle misalignment and error accumulation between LLMs and task-specific models.", "result": "Theoretical and experimental validation shows CoLLaTe outperforms standalone LLM-based and task-specific models.", "conclusion": "CoLLaTe effectively integrates LLMs and task-specific models, improving anomaly detection by addressing collaboration challenges."}}
{"id": "2504.11739", "pdf": "https://arxiv.org/pdf/2504.11739", "abs": "https://arxiv.org/abs/2504.11739", "authors": ["Bingjie Gao", "Xinyu Gao", "Xiaoxue Wu", "Yujie Zhou", "Yu Qiao", "Li Niu", "Xinyuan Chen", "Yaohui Wang"], "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "categories": ["cs.CV", "cs.CL"], "comment": "accepted by CVPR2025, Project website:\n  https://whynothaha.github.io/Prompt_optimizer/RAPO.html", "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization\nframework. In order to address potential inaccuracies and ambiguous details\ngenerated by LLM-generated prompts. RAPO refines the naive prompts through dual\noptimization branches, selecting the superior prompt for T2V generation. The\nfirst branch augments user prompts with diverse modifiers extracted from a\nlearned relational graph, refining them to align with the format of training\nprompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive\nprompt using a pre-trained LLM following a well-defined instruction set.\nExtensive experiments demonstrate that RAPO can effectively enhance both the\nstatic and dynamic dimensions of generated videos, demonstrating the\nsignificance of prompt optimization for user-provided prompts.", "AI": {"tldr": "RAPO is a Retrieval-Augmented Prompt Optimization framework designed to improve Text-to-Video (T2V) generation by refining user prompts through dual optimization branches, enhancing video quality.", "motivation": "The sensitivity of T2V models to input prompts and the lack of tailored guidance in prompt design necessitate a method to optimize prompts for better generative outcomes.", "method": "RAPO uses two branches: one augments prompts with modifiers from a relational graph and aligns them via a fine-tuned LLM, while the other rewrites prompts using a pre-trained LLM with defined instructions.", "result": "Experiments show RAPO enhances both static and dynamic aspects of generated videos, proving the importance of prompt optimization.", "conclusion": "RAPO effectively addresses prompt design challenges in T2V generation, improving video quality through optimized prompts."}}
{"id": "2505.03703", "pdf": "https://arxiv.org/pdf/2505.03703", "abs": "https://arxiv.org/abs/2505.03703", "authors": ["Fran\u00e7ois Role", "S\u00e9bastien Meyer", "Victor Amblard"], "title": "Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) allow to embed texts and images in a shared\nrepresentation space. However, it has been shown that these models are subject\nto a modality gap phenomenon meaning there exists a clear separation between\nthe embeddings from one modality and another in the embedding space. While this\nmisalignment is detrimental for downstream tasks such as multimodal retrieval,\nmultimodal clustering or zero-shot classification, etc. no generic and\npractical methods have so far been proposed to assess it precisely and even\nreduce it. We therefore propose novel measures and effective techniques\n(spectral- and optimal transport-based methods) to achieve this goal. Extensive\nexperiments conducted on several image-text datasets and models demonstrate\ntheir effectiveness and beneficial effects on downstream tasks. Our code is\navailable at the URL provided in the paper's abstract.", "AI": {"tldr": "The paper addresses the modality gap in vision-language models (VLMs), proposing new measures and techniques to assess and reduce it, improving downstream tasks.", "motivation": "VLMs exhibit a modality gap, causing misalignment between text and image embeddings, which harms tasks like retrieval and classification. Existing methods lack precision and practicality.", "method": "The authors introduce spectral- and optimal transport-based methods to measure and reduce the modality gap.", "result": "Experiments on multiple datasets and models show the techniques' effectiveness and positive impact on downstream tasks.", "conclusion": "The proposed methods successfully address the modality gap, enhancing VLM performance for practical applications."}}
{"id": "2505.03308", "pdf": "https://arxiv.org/pdf/2505.03308", "abs": "https://arxiv.org/abs/2505.03308", "authors": ["C\u00f4me Annicchiarico", "Fabien Lotte", "J\u00e9r\u00e9mie Mattout"], "title": "An Active Inference perspective on Neurofeedback Training", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint, 43 pages, 14 figures", "summary": "Neurofeedback training (NFT) aims to teach self-regulation of brain activity\nthrough real-time feedback, but suffers from highly variable outcomes and\npoorly understood mechanisms, hampering its validation. To address these\nissues, we propose a formal computational model of the NFT closed loop. Using\nActive Inference, a Bayesian framework modelling perception, action, and\nlearning, we simulate agents interacting with an NFT environment. This enables\nus to test the impact of design choices (e.g., feedback quality, biomarker\nvalidity) and subject factors (e.g., prior beliefs) on training. Simulations\nshow that training effectiveness is sensitive to feedback noise or bias, and to\nprior beliefs (highlighting the importance of guiding instructions), but also\nreveal that perfect feedback is insufficient to guarantee high performance.\nThis approach provides a tool for assessing and predicting NFT variability,\ninterpret empirical data, and potentially develop personalized training\nprotocols.", "AI": {"tldr": "A computational model using Active Inference simulates neurofeedback training (NFT) to analyze variability and improve design, revealing sensitivity to feedback quality and prior beliefs.", "motivation": "NFT outcomes are inconsistent, and mechanisms are unclear, hindering validation. A formal model is needed to understand and improve NFT.", "method": "Active Inference, a Bayesian framework, simulates agents in NFT environments to test feedback quality, biomarker validity, and subject factors like prior beliefs.", "result": "Training effectiveness depends on feedback noise/bias and prior beliefs. Perfect feedback alone doesn't ensure high performance.", "conclusion": "The model aids in predicting NFT variability, interpreting data, and developing personalized protocols."}}
{"id": "2504.18604", "pdf": "https://arxiv.org/pdf/2504.18604", "abs": "https://arxiv.org/abs/2504.18604", "authors": ["Xingyu Xiao", "Peng Chen", "Jiejuan Tong", "Shunshun Liu", "Hongru Zhao", "Jun Zhao", "Qianqian Jia", "Jingang Liang", "Haitao Wang"], "title": "A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study", "categories": ["cs.AI"], "comment": null, "summary": "Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA,\nrely on expert judgment and empirical rules that often overlook the cognitive\nunderpinnings of human error. Moreover, conducting human-in-the-loop\nexperiments for advanced nuclear power plants is increasingly impractical due\nto novel interfaces and limited operational data. This study proposes a\ncognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA\nmethodology by integrating an ACT-R-based human digital twin (HDT) with\nTimeGAN-augmented simulation. The ACT-R model simulates operator cognition,\nincluding memory retrieval, goal-directed procedural reasoning, and\nperceptual-motor execution, under high-fidelity scenarios derived from a\nhigh-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource\nconstraints of large-scale cognitive modeling, TimeGAN is trained on\nACT-R-generated time-series data to produce high-fidelity synthetic operator\nbehavior datasets. These simulations are then used to drive IDHEAS-ECA\nassessments, enabling scalable, mechanism-informed estimation of human error\nprobabilities (HEPs). Comparative analyses with SPAR-H and sensitivity\nassessments demonstrate the robustness and practical advantages of the proposed\nCOGMIF. Finally, procedural features are mapped onto a Bayesian network to\nquantify the influence of contributing factors, revealing key drivers of\noperational risk. This work offers a credible and computationally efficient\npathway to integrate cognitive theory into industrial HRA practices.", "AI": {"tldr": "The paper introduces COGMIF, a cognitive-mechanistic framework enhancing IDHEAS-ECA by integrating ACT-R-based human digital twins and TimeGAN-augmented simulations for scalable, mechanism-informed human error probability estimation.", "motivation": "Traditional HRA methods lack cognitive underpinnings and face impracticalities in human-in-the-loop experiments for advanced nuclear plants.", "method": "COGMIF combines ACT-R-based human digital twins (simulating cognition) with TimeGAN-augmented synthetic data to enhance IDHEAS-ECA assessments.", "result": "Comparative analyses show COGMIF's robustness and advantages over SPAR-H, with Bayesian network mapping revealing key operational risk drivers.", "conclusion": "COGMIF provides a credible, efficient way to integrate cognitive theory into industrial HRA practices."}}
{"id": "2505.00926", "pdf": "https://arxiv.org/pdf/2505.00926", "abs": "https://arxiv.org/abs/2505.00926", "authors": ["Ruiquan Huang", "Yingbin Liang", "Jing Yang"], "title": "How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "accepted by ICML 2025", "summary": "Language recognition tasks are fundamental in natural language processing\n(NLP) and have been widely used to benchmark the performance of large language\nmodels (LLMs). These tasks also play a crucial role in explaining the working\nmechanisms of transformers. In this work, we focus on two representative tasks\nin the category of regular language recognition, known as `even pairs' and\n`parity check', the aim of which is to determine whether the occurrences of\ncertain subsequences in a given sequence are even. Our goal is to explore how a\none-layer transformer, consisting of an attention layer followed by a linear\nlayer, learns to solve these tasks by theoretically analyzing its training\ndynamics under gradient descent. While even pairs can be solved directly by a\none-layer transformer, parity check need to be solved by integrating\nChain-of-Thought (CoT), either into the inference stage of a transformer\nwell-trained for the even pairs task, or into the training of a one-layer\ntransformer. For both problems, our analysis shows that the joint training of\nattention and linear layers exhibits two distinct phases. In the first phase,\nthe attention layer grows rapidly, mapping data sequences into separable\nvectors. In the second phase, the attention layer becomes stable, while the\nlinear layer grows logarithmically and approaches in direction to a max-margin\nhyperplane that correctly separates the attention layer outputs into positive\nand negative samples, and the loss decreases at a rate of $O(1/t)$. Our\nexperiments validate those theoretical results.", "AI": {"tldr": "The paper explores how a one-layer transformer learns regular language tasks ('even pairs' and 'parity check') via gradient descent, revealing two training phases and validating results experimentally.", "motivation": "To understand how transformers learn regular language tasks and explain their mechanisms, focusing on 'even pairs' and 'parity check'.", "method": "Theoretical analysis of training dynamics in a one-layer transformer (attention + linear layer) under gradient descent, with experiments for validation.", "result": "Two training phases: rapid attention layer growth for separability, followed by logarithmic linear layer growth toward max-margin separation. Loss decreases as O(1/t).", "conclusion": "One-layer transformers can solve 'even pairs' directly, while 'parity check' requires Chain-of-Thought integration. Training dynamics align with theoretical predictions."}}
{"id": "2505.03715", "pdf": "https://arxiv.org/pdf/2505.03715", "abs": "https://arxiv.org/abs/2505.03715", "authors": ["Luca Caldera", "Lara Cavinato", "Alessio Cirone", "Isabella Cama", "Sara Garbarino", "Raffaele Lodi", "Fabrizio Tagliavini", "Anna Nigri", "Silvia De Francesco", "Andrea Cappozzo", "Michele Piana", "Francesca Ieva"], "title": "DISARM++: Beyond scanner-free harmonization", "categories": ["cs.CV"], "comment": null, "summary": "Harmonization of T1-weighted MR images across different scanners is crucial\nfor ensuring consistency in neuroimaging studies. This study introduces a novel\napproach to direct image harmonization, moving beyond feature standardization\nto ensure that extracted features remain inherently reliable for downstream\nanalysis. Our method enables image transfer in two ways: (1) mapping images to\na scanner-free space for uniform appearance across all scanners, and (2)\ntransforming images into the domain of a specific scanner used in model\ntraining, embedding its unique characteristics. Our approach presents strong\ngeneralization capability, even for unseen scanners not included in the\ntraining phase. We validated our method using MR images from diverse cohorts,\nincluding healthy controls, traveling subjects, and individuals with\nAlzheimer's disease (AD). The model's effectiveness is tested in multiple\napplications, such as brain age prediction (R2 = 0.60 \\pm 0.05), biomarker\nextraction, AD classification (Test Accuracy = 0.86 \\pm 0.03), and diagnosis\nprediction (AUC = 0.95). In all cases, our harmonization technique outperforms\nstate-of-the-art methods, showing improvements in both reliability and\npredictive accuracy. Moreover, our approach eliminates the need for extensive\npreprocessing steps, such as skull-stripping, which can introduce errors by\nmisclassifying brain and non-brain structures. This makes our method\nparticularly suitable for applications that require full-head analysis,\nincluding research on head trauma and cranial deformities. Additionally, our\nharmonization model does not require retraining for new datasets, allowing\nsmooth integration into various neuroimaging workflows. By ensuring\nscanner-invariant image quality, our approach provides a robust and efficient\nsolution for improving neuroimaging studies across diverse settings. The code\nis available at this link.", "AI": {"tldr": "A novel method for harmonizing T1-weighted MR images across scanners improves reliability and predictive accuracy without extensive preprocessing, validated in diverse cohorts and applications.", "motivation": "Ensuring consistency in neuroimaging studies by harmonizing images across different scanners, addressing limitations of feature standardization and preprocessing errors.", "method": "Direct image harmonization via two approaches: mapping to a scanner-free space or transforming into a specific scanner's domain, with strong generalization for unseen scanners.", "result": "Outperforms state-of-the-art methods in brain age prediction (R2 = 0.60), AD classification (Accuracy = 0.86), and diagnosis prediction (AUC = 0.95), eliminating preprocessing needs.", "conclusion": "The approach provides a robust, efficient solution for scanner-invariant neuroimaging, suitable for diverse applications and workflows."}}
{"id": "2505.03344", "pdf": "https://arxiv.org/pdf/2505.03344", "abs": "https://arxiv.org/abs/2505.03344", "authors": ["Keyu Chen", "Wenchao Sun", "Hao Cheng", "Sifa Zheng"], "title": "RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Achieving both realism and controllability in interactive closed-loop traffic\nsimulation remains a key challenge in autonomous driving. Data-driven\nsimulation methods reproduce realistic trajectories but suffer from covariate\nshift in closed-loop deployment, compounded by simplified dynamics models that\nfurther reduce reliability. Conversely, physics-based simulation methods\nenhance reliable and controllable closed-loop interactions but often lack\nexpert demonstrations, compromising realism. To address these challenges, we\nintroduce a dual-stage AV-centered simulation framework that conducts open-loop\nimitation learning pre-training in a data-driven simulator to capture\ntrajectory-level realism and multimodality, followed by closed-loop\nreinforcement learning fine-tuning in a physics-based simulator to enhance\ncontrollability and mitigate covariate shift. In the fine-tuning stage, we\npropose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that\npreserves the trajectory-level multimodality through a GRPO-style\ngroup-relative advantage formulation, while enhancing controllability and\ntraining stability by replacing KL regularization with the dual-clip mechanism.\nExtensive experiments demonstrate that RIFT significantly improves the realism\nand controllability of generated traffic scenarios, providing a robust platform\nfor evaluating autonomous vehicle performance in diverse and interactive\nscenarios.", "AI": {"tldr": "A dual-stage simulation framework combines data-driven imitation learning and physics-based reinforcement learning to improve realism and controllability in traffic simulation for autonomous driving.", "motivation": "Addressing the trade-off between realism and controllability in traffic simulation, where data-driven methods lack reliability and physics-based methods lack realism.", "method": "A dual-stage approach: open-loop imitation learning in a data-driven simulator for realism, followed by closed-loop reinforcement learning (RIFT) in a physics-based simulator for controllability.", "result": "RIFT enhances realism and controllability, providing robust evaluation for autonomous vehicles in diverse scenarios.", "conclusion": "The proposed framework successfully balances realism and controllability, improving traffic simulation for autonomous driving."}}
{"id": "2505.00174", "pdf": "https://arxiv.org/pdf/2505.00174", "abs": "https://arxiv.org/abs/2505.00174", "authors": ["Ilan Strauss", "Isobel Moure", "Tim O'Reilly", "Sruly Rosenblat"], "title": "Real-World Gaps in AI Governance Research", "categories": ["cs.AI"], "comment": "Corrected a previous error: replaced 'underrepresented in Academic AI\n  research' with the intended phrase 'underrepresented in Corporate AI\n  research'", "summary": "Drawing on 1,178 safety and reliability papers from 9,439 generative AI\npapers (January 2020 - March 2025), we compare research outputs of leading AI\ncompanies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI\nuniversities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of\nWashington). We find that corporate AI research increasingly concentrates on\npre-deployment areas -- model alignment and testing & evaluation -- while\nattention to deployment-stage issues such as model bias has waned. Significant\nresearch gaps exist in high-risk deployment domains, including healthcare,\nfinance, misinformation, persuasive and addictive features, hallucinations, and\ncopyright. Without improved observability into deployed AI, growing corporate\nconcentration could deepen knowledge deficits. We recommend expanding external\nresearcher access to deployment data and systematic observability of in-market\nAI behaviors.", "AI": {"tldr": "Analysis of 1,178 safety/reliability papers from 9,439 generative AI papers shows corporate research focuses on pre-deployment (alignment, testing) over deployment issues (bias), with gaps in high-risk domains like healthcare and misinformation. Recommends better deployment data access and observability.", "motivation": "To compare research outputs of leading AI companies and universities, identifying trends and gaps in AI safety and reliability research.", "method": "Analyzed 1,178 safety/reliability papers from 9,439 generative AI papers (2020-2025), comparing corporate and academic research focus areas.", "result": "Corporate research prioritizes pre-deployment (alignment, testing), neglecting deployment issues (bias) and high-risk domains (healthcare, misinformation).", "conclusion": "Improved deployment data access and systematic observability are needed to address knowledge deficits and corporate concentration."}}
{"id": "2505.03735", "pdf": "https://arxiv.org/pdf/2505.03735", "abs": "https://arxiv.org/abs/2505.03735", "authors": ["Jiayuan Rao", "Zifeng Li", "Haoning Wu", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "Multi-Agent System for Comprehensive Soccer Understanding", "categories": ["cs.CV"], "comment": "Technical Report; Project Page: https://jyrao.github.io/SoccerAgent/", "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.", "AI": {"tldr": "A comprehensive framework for holistic soccer understanding is introduced, featuring SoccerWiki (a knowledge base), SoccerBench (a benchmark), SoccerAgent (a multi-agent system), and evaluations of MLLMs.", "motivation": "To address the gap in AI-driven soccer understanding by moving beyond isolated tasks to a holistic approach.", "method": "Developed SoccerWiki for knowledge-driven reasoning, SoccerBench for benchmarking, and SoccerAgent for collaborative reasoning.", "result": "Achieved robust performance on SoccerBench, demonstrating the superiority of the proposed agentic system.", "conclusion": "The framework advances soccer understanding, with all resources publicly available."}}
{"id": "2505.03385", "pdf": "https://arxiv.org/pdf/2505.03385", "abs": "https://arxiv.org/abs/2505.03385", "authors": ["Julia Bringewald"], "title": "Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.LG", "I.5.0"], "comment": null, "summary": "Solar flares are among the most powerful and dynamic events in the solar\nsystem, resulting from the sudden release of magnetic energy stored in the\nSun's atmosphere. These energetic bursts of electromagnetic radiation can\nrelease up to 10^32 erg of energy, impacting space weather and posing risks to\ntechnological infrastructure and therefore require accurate forecasting of\nsolar flare occurrences and intensities. This study evaluates the predictive\nperformance of three machine learning algorithms: Random Forest, k-Nearest\nNeighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar\nflares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP\nparameters, the effectiveness of the models was evaluated in binary and\nmulticlass classification tasks. The analysis utilized 8 principal components\n(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.\nOur approach uniquely combines binary and multiclass classification with\ndifferent levels of dimensionality reduction, an innovative methodology not\npreviously explored in the context of solar flare prediction. Employing a\n10-fold stratified cross-validation and grid search for hyperparameter tuning\nensured robust model evaluation. Our findings indicate that Random Forest and\nXGBoost consistently demonstrate strong performance across all metrics,\nbenefiting significantly from increased dimensionality. The insights of this\nstudy enhance future research by optimizing dimensionality reduction techniques\nand informing model selection for astrophysical tasks. By integrating this\nnewly acquired knowledge into future research, more accurate space weather\nforecasting systems can be developed, along with a deeper understanding of\nsolar physics.", "AI": {"tldr": "The study evaluates three machine learning algorithms (Random Forest, KNN, XGBoost) for solar flare classification, finding Random Forest and XGBoost perform best, especially with higher dimensionality.", "motivation": "Solar flares impact space weather and technology, necessitating accurate forecasting.", "method": "Used SHARP parameters, PCA for dimensionality reduction, and 10-fold cross-validation with grid search for hyperparameter tuning.", "result": "Random Forest and XGBoost outperformed KNN, benefiting from increased dimensionality.", "conclusion": "The study advances solar flare prediction by optimizing dimensionality reduction and model selection, aiding space weather forecasting."}}
{"id": "2505.02118", "pdf": "https://arxiv.org/pdf/2505.02118", "abs": "https://arxiv.org/abs/2505.02118", "authors": ["Wei Liu", "Zhongyu Niu", "Lang Gao", "Zhiying Deng", "Jun Wang", "Haozhao Wang", "Ruixuan Li"], "title": "Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets", "categories": ["cs.AI"], "comment": "ICML 2025", "summary": "This study investigates the self-rationalization framework constructed with a\ncooperative game, where a generator initially extracts the most informative\nsegment from raw input, and a subsequent predictor utilizes the selected subset\nfor its input. The generator and predictor are trained collaboratively to\nmaximize prediction accuracy. In this paper, we first uncover a potential\ncaveat: such a cooperative game could unintentionally introduce a sampling bias\nduring rationale extraction. Specifically, the generator might inadvertently\ncreate an incorrect correlation between the selected rationale candidate and\nthe label, even when they are semantically unrelated in the original dataset.\nSubsequently, we elucidate the origins of this bias using both detailed\ntheoretical analysis and empirical evidence. Our findings suggest a direction\nfor inspecting these correlations through attacks, based on which we further\nintroduce an instruction to prevent the predictor from learning the\ncorrelations. Through experiments on six text classification datasets and two\ngraph classification datasets using three network architectures (GRUs, BERT,\nand GCN), we show that our method not only significantly outperforms recent\nrationalization methods, but also achieves comparable or even better results\nthan a representative LLM (llama3.1-8b-instruct).", "AI": {"tldr": "The paper explores a self-rationalization framework using a cooperative game between a generator and predictor, identifies a sampling bias issue, and proposes a solution to mitigate it, achieving superior performance.", "motivation": "To address the potential sampling bias in cooperative rationalization frameworks where the generator might create incorrect correlations between rationale and labels.", "method": "The study uses theoretical analysis and empirical evidence to identify bias origins, introduces an instruction to prevent predictor learning of incorrect correlations, and tests on text and graph classification datasets with GRUs, BERT, and GCN architectures.", "result": "The proposed method outperforms recent rationalization techniques and matches or exceeds the performance of a large language model (llama3.1-8b-instruct).", "conclusion": "The findings highlight the importance of addressing sampling bias in rationalization frameworks and demonstrate the effectiveness of the proposed solution."}}
{"id": "2505.02845", "pdf": "https://arxiv.org/pdf/2505.02845", "abs": "https://arxiv.org/abs/2505.02845", "authors": ["Jeremias Gerner", "Klaus Bogenberger", "Stefanie Schmidtner"], "title": "Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights", "categories": ["physics.soc-ph", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by\nintegrating onboard sensors to detect and localize other traffic participants,\nproviding richer and more detailed traffic data. In this work, we explore\nvarious modeling approaches for FCO detections within microscopic traffic\nsimulations to evaluate their potential for Intelligent Transportation System\n(ITS) applications. These approaches range from 2D raytracing to high-fidelity\nco-simulations that emulate real-world sensors and integrate 3D object\ndetection algorithms to closely replicate FCO detections. Additionally, we\nintroduce a neural network-based emulation technique that effectively\napproximates the results of high-fidelity co-simulations. This approach\ncaptures the unique characteristics of FCO detections while offering a fast and\nscalable solution for modeling. Using this emulation method, we investigate the\nimpact of FCO data in a digital twin of a traffic network modeled in SUMO.\nResults demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based\ndetections can identify 65% of vehicles across various intersections and\ntraffic demand scenarios. Further potential emerges when temporal insights are\nintegrated, enabling the recovery of previously detected but currently unseen\nvehicles. By employing data-driven methods, we recover over 80% of these\nvehicles with minimal positional deviations. These findings underscore the\npotential of FCOs for ITS, particularly in enhancing traffic state estimation\nand monitoring under varying penetration rates and traffic conditions.", "AI": {"tldr": "Floating Car Observers (FCOs) enhance traffic data by detecting other vehicles using onboard sensors. The paper evaluates modeling approaches for FCOs in simulations, introduces a neural network emulation method, and demonstrates their effectiveness in traffic monitoring.", "motivation": "To explore the potential of FCOs for Intelligent Transportation Systems (ITS) by evaluating their detection capabilities and impact on traffic state estimation.", "method": "Various modeling approaches, including 2D raytracing, high-fidelity co-simulations, and a neural network-based emulation technique, are tested. The study uses SUMO for traffic network simulations.", "result": "At a 20% penetration rate, FCOs with LiDAR detect 65% of vehicles. Temporal insights recover over 80% of previously detected vehicles with minimal positional errors.", "conclusion": "FCOs show significant promise for ITS, improving traffic monitoring and estimation under diverse conditions."}}
{"id": "2505.03397", "pdf": "https://arxiv.org/pdf/2505.03397", "abs": "https://arxiv.org/abs/2505.03397", "authors": ["Chris Wise", "Akram Youssry", "Alberto Peruzzo", "Jo Plested", "Matt Woolley"], "title": "Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath", "categories": ["quant-ph", "cs.LG"], "comment": "19 pages, 3 figures, 4 tables", "summary": "Qubit control protocols have traditionally leveraged a characterisation of\nthe qubit-bath coupling via its power spectral density. Previous work proposed\nthe inference of noise operators that characterise the influence of a classical\nbath using a grey-box approach that combines deep neural networks with\nphysics-encoded layers. This overall structure is complex and poses challenges\nin scaling and real-time operations. Here, we show that no expensive neural\nnetworks are needed and that this noise operator description admits an\nefficient parameterisation. We refer to the resulting parameter space as the\n\\textit{quantum feature space} of the qubit dynamics resulting from the coupled\nbath. We show that the Euclidean distance defined over the quantum feature\nspace provides an effective method for classifying noise processes in the\npresence of a given set of controls. Using the quantum feature space as the\ninput space for a simple machine learning algorithm (random forest, in this\ncase), we demonstrate that it can effectively classify the stationarity and the\nbroad class of noise processes perturbing a qubit. Finally, we explore how\ncontrol pulse parameters map to the quantum feature space.", "AI": {"tldr": "The paper simplifies qubit control by replacing complex neural networks with an efficient parameterization of noise operators, introducing a 'quantum feature space' for classifying noise processes.", "motivation": "Traditional qubit control relies on complex neural networks, which are hard to scale and operate in real-time. The paper aims to simplify this by finding an efficient parameterization.", "method": "The authors propose a 'quantum feature space' for noise operator description and use Euclidean distance for noise classification. A random forest algorithm is applied for stationarity and noise class identification.", "result": "The quantum feature space effectively classifies noise processes, and a simple machine learning algorithm (random forest) successfully identifies noise properties.", "conclusion": "The study demonstrates that expensive neural networks are unnecessary for qubit control, offering a scalable and efficient alternative via the quantum feature space."}}
{"id": "2207.00713", "pdf": "https://arxiv.org/pdf/2207.00713", "abs": "https://arxiv.org/abs/2207.00713", "authors": ["Yanwei Jia", "Xun Yu Zhou"], "title": "q-Learning in Continuous Time", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "comment": "70 pages, 4 figures, appended with an erratum", "summary": "We study the continuous-time counterpart of Q-learning for reinforcement\nlearning (RL) under the entropy-regularized, exploratory diffusion process\nformulation introduced by Wang et al. (2020). As the conventional (big)\nQ-function collapses in continuous time, we consider its first-order\napproximation and coin the term ``(little) q-function\". This function is\nrelated to the instantaneous advantage rate function as well as the\nHamiltonian. We develop a ``q-learning\" theory around the q-function that is\nindependent of time discretization. Given a stochastic policy, we jointly\ncharacterize the associated q-function and value function by martingale\nconditions of certain stochastic processes, in both on-policy and off-policy\nsettings. We then apply the theory to devise different actor-critic algorithms\nfor solving underlying RL problems, depending on whether or not the density\nfunction of the Gibbs measure generated from the q-function can be computed\nexplicitly. One of our algorithms interprets the well-known Q-learning\nalgorithm SARSA, and another recovers a policy gradient (PG) based\ncontinuous-time algorithm proposed in Jia and Zhou (2022b). Finally, we conduct\nsimulation experiments to compare the performance of our algorithms with those\nof PG-based algorithms in Jia and Zhou (2022b) and time-discretized\nconventional Q-learning algorithms.", "AI": {"tldr": "The paper explores a continuous-time version of Q-learning in RL, introducing a 'q-function' as a first-order approximation of the Q-function. It develops a theory for q-learning, applies it to actor-critic algorithms, and compares performance with existing methods.", "motivation": "To address the collapse of the conventional Q-function in continuous time and develop a theory for RL that avoids time discretization.", "method": "Introduces the q-function, characterizes it and the value function via martingale conditions, and derives actor-critic algorithms based on this theory.", "result": "The theory is applied to devise algorithms, some of which interpret SARSA or recover existing PG-based methods. Simulations compare performance with other approaches.", "conclusion": "The proposed q-learning framework is effective, with algorithms performing comparably or better than existing methods in continuous-time RL."}}
{"id": "2505.03046", "pdf": "https://arxiv.org/pdf/2505.03046", "abs": "https://arxiv.org/abs/2505.03046", "authors": ["Pau Amargant", "Peter H\u00f6nig", "Markus Vincze"], "title": "Sim2Real Transfer for Vision-Based Grasp Verification", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at Austrian Robotics Workshop 2025", "summary": "The verification of successful grasps is a crucial aspect of robot\nmanipulation, particularly when handling deformable objects. Traditional\nmethods relying on force and tactile sensors often struggle with deformable and\nnon-rigid objects. In this work, we present a vision-based approach for grasp\nverification to determine whether the robotic gripper has successfully grasped\nan object. Our method employs a two-stage architecture; first YOLO-based object\ndetection model to detect and locate the robot's gripper and then a\nResNet-based classifier determines the presence of an object. To address the\nlimitations of real-world data capture, we introduce HSR-GraspSynth, a\nsynthetic dataset designed to simulate diverse grasping scenarios. Furthermore,\nwe explore the use of Visual Question Answering capabilities as a zero-shot\nbaseline to which we compare our model. Experimental results demonstrate that\nour approach achieves high accuracy in real-world environments, with potential\nfor integration into grasping pipelines. Code and datasets are publicly\navailable at https://github.com/pauamargant/HSR-GraspSynth .", "AI": {"tldr": "A vision-based grasp verification method for robots using a two-stage YOLO-ResNet architecture, validated with a synthetic dataset and achieving high real-world accuracy.", "motivation": "Traditional grasp verification methods struggle with deformable objects, prompting a need for a vision-based solution.", "method": "Two-stage approach: YOLO for gripper detection and ResNet for object presence classification, supplemented by a synthetic dataset (HSR-GraspSynth).", "result": "High accuracy in real-world environments, with potential for integration into grasping pipelines.", "conclusion": "The proposed vision-based method is effective for grasp verification, especially with deformable objects, and is supported by publicly available code and datasets."}}
{"id": "2505.03585", "pdf": "https://arxiv.org/pdf/2505.03585", "abs": "https://arxiv.org/abs/2505.03585", "authors": ["Charita Dellaporta", "Patrick O'Hara", "Theodoros Damoulas"], "title": "Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Distributionally Robust Optimisation (DRO) protects risk-averse\ndecision-makers by considering the worst-case risk within an ambiguity set of\ndistributions based on the empirical distribution or a model. To further guard\nagainst finite, noisy data, model-based approaches admit Bayesian formulations\nthat propagate uncertainty from the posterior to the decision-making problem.\nHowever, when the model is misspecified, the decision maker must stretch the\nambiguity set to contain the data-generating process (DGP), leading to overly\nconservative decisions. We address this challenge by introducing DRO with\nRobust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These\nare Maximum Mean Discrepancy ambiguity sets centred at a robust posterior\npredictive distribution that incorporates beliefs about the DGP. We show that\nthe resulting optimisation problem obtains a dual formulation in the\nReproducing Kernel Hilbert Space and we give probabilistic guarantees on the\ntolerance level of the ambiguity set. Our method outperforms other Bayesian and\nempirical DRO approaches in out-of-sample performance on the Newsvendor and\nPortfolio problems with various cases of model misspecification.", "AI": {"tldr": "DRO-RoBAS introduces robust Bayesian ambiguity sets to handle model misspecification in Distributionally Robust Optimization, improving out-of-sample performance.", "motivation": "Addressing overly conservative decisions in DRO due to model misspecification by incorporating robust Bayesian ambiguity sets.", "method": "Uses Maximum Mean Discrepancy ambiguity sets centered at a robust posterior predictive distribution, with dual formulation in Reproducing Kernel Hilbert Space.", "result": "Outperforms Bayesian and empirical DRO in out-of-sample performance on Newsvendor and Portfolio problems under model misspecification.", "conclusion": "DRO-RoBAS effectively mitigates conservatism in DRO under model misspecification, offering better decision-making guarantees."}}
{"id": "2310.04306", "pdf": "https://arxiv.org/pdf/2310.04306", "abs": "https://arxiv.org/abs/2310.04306", "authors": ["Qing Zhu", "Qirong Mao", "Jialin Zhang", "Xiaohua Huang", "Wenming Zheng"], "title": "Towards A Robust Group-level Emotion Recognition via Uncertainty-Aware Learning", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages,3 figures", "summary": "Group-level emotion recognition (GER) is an inseparable part of human\nbehavior analysis, aiming to recognize an overall emotion in a multi-person\nscene. However, the existing methods are devoted to combing diverse emotion\ncues while ignoring the inherent uncertainties under unconstrained\nenvironments, such as congestion and occlusion occurring within a group.\nAdditionally, since only group-level labels are available, inconsistent emotion\npredictions among individuals in one group can confuse the network. In this\npaper, we propose an uncertainty-aware learning (UAL) method to extract more\nrobust representations for GER. By explicitly modeling the uncertainty of each\nindividual, we utilize stochastic embedding drawn from a Gaussian distribution\ninstead of deterministic point embedding. This representation captures the\nprobabilities of different emotions and generates diverse predictions through\nthis stochasticity during the inference stage. Furthermore,\nuncertainty-sensitive scores are adaptively assigned as the fusion weights of\nindividuals' face within each group. Moreover, we develop an image enhancement\nmodule to enhance the model's robustness against severe noise. The overall\nthree-branch model, encompassing face, object, and scene component, is guided\nby a proportional-weighted fusion strategy and integrates the proposed\nuncertainty-aware method to produce the final group-level output. Experimental\nresults demonstrate the effectiveness and generalization ability of our method\nacross three widely used databases.", "AI": {"tldr": "The paper proposes an uncertainty-aware learning (UAL) method for group-level emotion recognition (GER) to address uncertainties in unconstrained environments and inconsistent individual predictions.", "motivation": "Existing GER methods ignore uncertainties like congestion and occlusion, and individual predictions can confuse the network due to lack of individual labels.", "method": "UAL models individual uncertainty using stochastic Gaussian embeddings, assigns uncertainty-sensitive fusion weights, and includes an image enhancement module. A three-branch model (face, object, scene) uses proportional-weighted fusion.", "result": "The method shows effectiveness and generalization across three databases.", "conclusion": "UAL improves GER robustness by addressing uncertainties and inconsistent predictions."}}
{"id": "2505.03702", "pdf": "https://arxiv.org/pdf/2505.03702", "abs": "https://arxiv.org/abs/2505.03702", "authors": ["Srecharan Selvam", "Abhishesh Silwal", "George Kanter"], "title": "Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach", "categories": ["cs.RO", "cs.CV", "cs.LG", "I.2.10"], "comment": "13 pages, 9 figures", "summary": "Automating leaf manipulation in agricultural settings faces significant\nchallenges, including the variability of plant morphologies and deformable\nleaves. We propose a novel hybrid geometric-neural approach for autonomous leaf\ngrasping that combines traditional computer vision with neural networks through\nself-supervised learning. Our method integrates YOLOv8 for instance\nsegmentation and RAFT-Stereo for 3D depth estimation to build rich leaf\nrepresentations, which feed into both a geometric feature scoring pipeline and\na neural refinement module (GraspPointCNN). The key innovation is our\nconfidence-weighted fusion mechanism that dynamically balances the contribution\nof each approach based on prediction certainty. Our self-supervised framework\nuses the geometric pipeline as an expert teacher to automatically generate\ntraining data. Experiments demonstrate that our approach achieves an 88.0%\nsuccess rate in controlled environments and 84.7% in real greenhouse\nconditions, significantly outperforming both purely geometric (75.3%) and\nneural (60.2%) methods. This work establishes a new paradigm for agricultural\nrobotics where domain expertise is seamlessly integrated with machine learning\ncapabilities, providing a foundation for fully automated crop monitoring\nsystems.", "AI": {"tldr": "A hybrid geometric-neural approach for autonomous leaf grasping combines computer vision and neural networks, achieving high success rates in controlled and real greenhouse environments.", "motivation": "Addressing challenges in automating leaf manipulation due to variable plant morphologies and deformable leaves.", "method": "Integrates YOLOv8 for segmentation, RAFT-Stereo for 3D depth, and a confidence-weighted fusion mechanism to balance geometric and neural contributions.", "result": "88.0% success in controlled environments, 84.7% in real greenhouses, outperforming purely geometric (75.3%) and neural (60.2%) methods.", "conclusion": "Establishes a new paradigm for agricultural robotics by integrating domain expertise with machine learning for automated crop monitoring."}}
{"id": "2505.03590", "pdf": "https://arxiv.org/pdf/2505.03590", "abs": "https://arxiv.org/abs/2505.03590", "authors": ["Julian P. Merkofer", "Dennis M. J. van de Sande", "Alex A. Bhogal", "Ruud J. G. van Sloun"], "title": "Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": "Preprint submitted to IEEE MLSP 2025", "summary": "Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure\nthe metabolic composition of tissues, offering valuable insights into\nneurological disorders, tumor detection, and other metabolic dysfunctions.\nHowever, accurate metabolite quantification is hindered by challenges such as\nspectral overlap, low signal-to-noise ratio, and various artifacts. Traditional\nmethods like linear-combination modeling are susceptible to ambiguities and\ncommonly only provide a theoretical lower bound on estimation accuracy in the\nform of the Cram\\'er-Rao bound. This work introduces a Bayesian inference\nframework using Sylvester normalizing flows (SNFs) to approximate posterior\ndistributions over metabolite concentrations, enhancing quantification\nreliability. A physics-based decoder incorporates prior knowledge of MRS signal\nformation, ensuring realistic distribution representations. We validate the\nmethod on simulated 7T proton MRS data, demonstrating accurate metabolite\nquantification, well-calibrated uncertainties, and insights into parameter\ncorrelations and multi-modal distributions.", "AI": {"tldr": "A Bayesian framework using Sylvester normalizing flows (SNFs) improves metabolite quantification in MRS by addressing spectral overlap and noise, validated on simulated 7T proton MRS data.", "motivation": "Accurate metabolite quantification in MRS is challenging due to spectral overlap, noise, and artifacts, with traditional methods like linear-combination modeling providing limited accuracy bounds.", "method": "Introduces a Bayesian inference framework with SNFs to approximate posterior distributions over metabolite concentrations, incorporating a physics-based decoder for realistic signal representation.", "result": "Validated on simulated 7T proton MRS data, the method shows accurate quantification, well-calibrated uncertainties, and insights into parameter correlations and multi-modal distributions.", "conclusion": "The Bayesian framework with SNFs enhances MRS metabolite quantification reliability, offering better accuracy and uncertainty calibration than traditional methods."}}
{"id": "2310.07937", "pdf": "https://arxiv.org/pdf/2310.07937", "abs": "https://arxiv.org/abs/2310.07937", "authors": ["Bangguo Yu", "Qihao Yuan", "Kailai Li", "Hamidreza Kasaei", "Ming Cao"], "title": "Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation Using Vision Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 4 figures", "summary": "Visual target navigation is a critical capability for autonomous robots\noperating in unknown environments, particularly in human-robot interaction\nscenarios. While classical and learning-based methods have shown promise, most\nexisting approaches lack common-sense reasoning and are typically designed for\nsingle-robot settings, leading to reduced efficiency and robustness in complex\nenvironments. To address these limitations, we introduce Co-NavGPT, a novel\nframework that integrates a Vision Language Model (VLM) as a global planner to\nenable common-sense multi-robot visual target navigation. Co-NavGPT aggregates\nsub-maps from multiple robots with diverse viewpoints into a unified global\nmap, encoding robot states and frontier regions. The VLM uses this information\nto assign frontiers across the robots, facilitating coordinated and efficient\nexploration. Experiments on the Habitat-Matterport 3D (HM3D) demonstrate that\nCo-NavGPT outperforms existing baselines in terms of success rate and\nnavigation efficiency, without requiring task-specific training. Ablation\nstudies further confirm the importance of semantic priors from the VLM. We also\nvalidate the framework in real-world scenarios using quadrupedal robots.\nSupplementary video and code are available at:\nhttps://sites.google.com/view/co-navgpt2.", "AI": {"tldr": "Co-NavGPT integrates a Vision Language Model (VLM) for multi-robot visual target navigation, improving efficiency and robustness without task-specific training.", "motivation": "Addressing the lack of common-sense reasoning and single-robot limitations in existing visual target navigation methods.", "method": "Aggregates sub-maps from multiple robots into a unified global map, using a VLM to assign frontiers for coordinated exploration.", "result": "Outperforms baselines in success rate and navigation efficiency on HM3D, validated in real-world scenarios.", "conclusion": "Co-NavGPT demonstrates the effectiveness of VLMs in multi-robot navigation, with semantic priors playing a key role."}}
{"id": "2505.03729", "pdf": "https://arxiv.org/pdf/2505.03729", "abs": "https://arxiv.org/abs/2505.03729", "authors": ["Arthur Allshire", "Hongsuk Choi", "Junyi Zhang", "David McAllister", "Anthony Zhang", "Chung Min Kim", "Trevor Darrell", "Pieter Abbeel", "Jitendra Malik", "Angjoo Kanazawa"], "title": "Visual Imitation Enables Contextual Humanoid Control", "categories": ["cs.RO", "cs.CV"], "comment": "Project website: https://www.videomimic.net/", "summary": "How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.", "AI": {"tldr": "VIDEOMIMIC is a pipeline that converts human motion videos into control policies for humanoid robots, enabling skills like climbing stairs and sitting on chairs.", "motivation": "To teach humanoids complex skills by leveraging everyday human motion videos and environmental context.", "method": "A real-to-sim-to-real pipeline that reconstructs humans and environments from videos, generating whole-body control policies.", "result": "Demonstrated robust, repeatable contextual control on real humanoid robots for various skills.", "conclusion": "VIDEOMIMIC provides a scalable way to train humanoids for diverse real-world tasks."}}
{"id": "2505.03649", "pdf": "https://arxiv.org/pdf/2505.03649", "abs": "https://arxiv.org/abs/2505.03649", "authors": ["Bernardo Marenco", "Paola Bermolen", "Marcelo Fiori", "Federico Larroca", "Gonzalo Mateos"], "title": "Weighted Random Dot Product Graphs", "categories": ["stat.ML", "cs.LG", "math.CO", "math.PR"], "comment": "30 pages, 12 figures, code to generate Figures 3 to 12 available at\n  https://github.com/bmarenco/wrdpg", "summary": "Modeling of intricate relational patterns % through the analysis structures\nof network data has become a cornerstone of contemporary statistical research\nand related data science fields. Networks, represented as graphs, offer a\nnatural framework for this analysis. This paper extends the Random Dot Product\nGraph (RDPG) model to accommodate weighted graphs, markedly broadening the\nmodel's scope to scenarios where edges exhibit heterogeneous weight\ndistributions. We propose a nonparametric weighted (W)RDPG model that assigns a\nsequence of latent positions to each node. Inner products of these nodal\nvectors specify the moments of their incident edge weights' distribution via\nmoment-generating functions. In this way, and unlike prior art, the WRDPG can\ndiscriminate between weight distributions that share the same mean but differ\nin other higher-order moments. We derive statistical guarantees for an\nestimator of the nodal's latent positions adapted from the workhorse adjacency\nspectral embedding, establishing its consistency and asymptotic normality. We\nalso contribute a generative framework that enables sampling of graphs that\nadhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis\nand testing of observed graph metrics using judicious reference distributions.\nThe paper is organized to formalize the model's definition, the estimation (or\nnodal embedding) process and its guarantees, as well as the methodologies for\ngenerating weighted graphs, all complemented by illustrative and reproducible\nexamples showcasing the WRDPG's effectiveness in various network analytic\napplications.", "AI": {"tldr": "The paper extends the Random Dot Product Graph (RDPG) model to weighted graphs, proposing a nonparametric weighted (W)RDPG model. It introduces a method for estimating latent positions and provides statistical guarantees, along with a generative framework for weighted graphs.", "motivation": "To address the limitation of existing models in handling weighted graphs with heterogeneous weight distributions, especially those sharing the same mean but differing in higher-order moments.", "method": "Extends RDPG to weighted graphs using a nonparametric approach, assigning latent positions to nodes and using inner products to specify edge weight distributions. Derives statistical guarantees for the estimator and introduces a generative framework.", "result": "The WRDPG model can discriminate between weight distributions with the same mean but different higher-order moments. The estimator for latent positions is consistent and asymptotically normal.", "conclusion": "The WRDPG model broadens the applicability of RDPG to weighted graphs, offering a robust framework for network analysis with theoretical guarantees and practical generative capabilities."}}
{"id": "2311.03382", "pdf": "https://arxiv.org/pdf/2311.03382", "abs": "https://arxiv.org/abs/2311.03382", "authors": ["Hangtong Xu", "Yuanbo Xu", "Chaozhuo Li", "Fuzhen Zhuang"], "title": "Causal Structure Representation Learning of Confounders in Latent Space for Recommendation", "categories": ["cs.IR", "cs.AI", "cs.LG", "stat.ME"], "comment": null, "summary": "Inferring user preferences from the historical feedback of users is a\nvaluable problem in recommender systems. Conventional approaches often rely on\nthe assumption that user preferences in the feedback data are equivalent to the\nreal user preferences without additional noise, which simplifies the problem\nmodeling. However, there are various confounders during user-item interactions,\nsuch as weather and even the recommendation system itself. Therefore,\nneglecting the influence of confounders will result in inaccurate user\npreferences and suboptimal performance of the model. Furthermore, the\nunobservability of confounders poses a challenge in further addressing the\nproblem. To address these issues, we refine the problem and propose a more\nrational solution. Specifically, we consider the influence of confounders,\ndisentangle them from user preferences in the latent space, and employ causal\ngraphs to model their interdependencies without specific labels. By cleverly\ncombining local and global causal graphs, we capture the user-specificity of\nconfounders on user preferences. We theoretically demonstrate the\nidentifiability of the obtained causal graph. Finally, we propose our model\nbased on Variational Autoencoders, named Causal Structure representation\nlearning of Confounders in latent space (CSC). We conducted extensive\nexperiments on one synthetic dataset and five real-world datasets,\ndemonstrating the superiority of our model. Furthermore, we demonstrate that\nthe learned causal representations of confounders are controllable, potentially\noffering users fine-grained control over the objectives of their recommendation\nlists with the learned causal graphs.", "AI": {"tldr": "The paper addresses the challenge of accurately inferring user preferences in recommender systems by accounting for confounders like weather or the system itself. It proposes a model (CSC) that disentangles confounders from preferences using causal graphs and variational autoencoders, showing superior performance and controllability.", "motivation": "User preferences in feedback data are often assumed noise-free, but confounders distort them. Existing methods ignore these, leading to inaccurate models. The paper aims to address this gap.", "method": "The proposed CSC model disentangles confounders from user preferences in latent space using causal graphs. It combines local and global causal graphs to capture user-specific confounder effects and uses variational autoencoders for implementation.", "result": "Experiments on synthetic and real-world datasets show CSC's superiority. The learned causal representations are controllable, enabling fine-grained recommendation adjustments.", "conclusion": "The CSC model effectively addresses confounder influence in recommender systems, improving accuracy and offering controllable recommendations."}}
{"id": "2303.12675", "pdf": "https://arxiv.org/pdf/2303.12675", "abs": "https://arxiv.org/abs/2303.12675", "authors": ["Zeqing Xia", "Bojun Xiong", "Zhouhui Lian"], "title": "VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2023. Project Page:\n  https://xiazeqing.github.io/VecFontSDF", "summary": "Font design is of vital importance in the digital content design and modern\nprinting industry. Developing algorithms capable of automatically synthesizing\nvector fonts can significantly facilitate the font design process. However,\nexisting methods mainly concentrate on raster image generation, and only a few\napproaches can directly synthesize vector fonts. This paper proposes an\nend-to-end trainable method, VecFontSDF, to reconstruct and synthesize\nhigh-quality vector fonts using signed distance functions (SDFs). Specifically,\nbased on the proposed SDF-based implicit shape representation, VecFontSDF\nlearns to model each glyph as shape primitives enclosed by several parabolic\ncurves, which can be precisely converted to quadratic B\\'ezier curves that are\nwidely used in vector font products. In this manner, most image generation\nmethods can be easily extended to synthesize vector fonts. Qualitative and\nquantitative experiments conducted on a publicly-available dataset demonstrate\nthat our method obtains high-quality results on several tasks, including vector\nfont reconstruction, interpolation, and few-shot vector font synthesis,\nmarkedly outperforming the state of the art. Our code and trained models are\navailable at https://xiazeqing.github.io/VecFontSDF.", "AI": {"tldr": "VecFontSDF is an end-to-end trainable method for synthesizing high-quality vector fonts using signed distance functions (SDFs), outperforming existing methods.", "motivation": "Existing methods focus on raster image generation, lacking direct vector font synthesis. VecFontSDF addresses this gap.", "method": "Uses SDF-based implicit shape representation to model glyphs as shape primitives with parabolic curves, convertible to quadratic B\u00e9zier curves.", "result": "Achieves high-quality results in vector font reconstruction, interpolation, and few-shot synthesis, surpassing state-of-the-art methods.", "conclusion": "VecFontSDF effectively extends image generation methods to vector fonts, offering a robust solution for automated font design."}}
{"id": "2505.03670", "pdf": "https://arxiv.org/pdf/2505.03670", "abs": "https://arxiv.org/abs/2505.03670", "authors": ["Katy Craig", "Nicol\u00e1s Garc\u00eda Trillos", "\u0110or\u0111e Nikoli\u0107"], "title": "Vector valued optimal transport: from dynamic to static formulations", "categories": ["math.AP", "cs.LG", "math.MG"], "comment": null, "summary": "Motivated by applications in classification of vector valued measures and\nmultispecies PDE, we develop a theory that unifies existing notions of vector\nvalued optimal transport, from dynamic formulations (\\`a la Benamou-Brenier) to\nstatic formulations (\\`a la Kantorovich). In our framework, vector valued\nmeasures are modeled as probability measures on a product space $\\mathbb{R}^d\n\\times G$, where $G$ is a weighted graph over a finite set of nodes and the\ngraph geometry strongly influences the associated dynamic and static distances.\nWe obtain sharp inequalities relating four notions of vector valued optimal\ntransport and prove that the distances are mutually bi-H\\\"older equivalent. We\ndiscuss the theoretical and practical advantages of each metric and indicate\npotential applications in multispecies PDE and data analysis. In particular,\none of the static formulations discussed in the paper is amenable to\nlinearization, a technique that has been explored in recent years to accelerate\nthe computation of pairwise optimal transport distances.", "AI": {"tldr": "The paper unifies vector-valued optimal transport theories, linking dynamic and static formulations, and proves mutual bi-H\u00f6lder equivalence of distances. It highlights applications in multispecies PDE and data analysis.", "motivation": "Motivated by classifying vector-valued measures and multispecies PDE applications.", "method": "Develops a unified theory for vector-valued optimal transport, modeling measures on a product space \u211d\u1d48 \u00d7 G (G is a weighted graph). Analyzes dynamic and static distances influenced by graph geometry.", "result": "Proves sharp inequalities and bi-H\u00f6lder equivalence between four notions of vector-valued optimal transport.", "conclusion": "The unified framework offers theoretical and practical advantages, with potential applications in multispecies PDE and data analysis, including linearization for computational efficiency."}}
{"id": "2312.01581", "pdf": "https://arxiv.org/pdf/2312.01581", "abs": "https://arxiv.org/abs/2312.01581", "authors": ["Sachit Kuhar", "Yash Jain", "Alexey Tumanov"], "title": "PLUM: Improving Inference Efficiency By Leveraging Repetition-Sparsity Trade-Off", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "OpenReview: https://openreview.net/forum?id=IEKtMMSblm", "summary": "Efficient inference of Deep Neural Networks (DNNs) on resource-constrained\nedge devices is essential. Quantization and sparsity are key techniques that\ntranslate to repetition and sparsity within tensors at the hardware-software\ninterface. This paper introduces the concept of repetition-sparsity trade-off\nthat helps explain computational efficiency during inference. We propose PLUM,\na unified co-design framework that integrates DNN inference systems and\nquantization (forward and backward pass) to leverage the repetition-sparsity\ntrade-off to improve inference efficiency. Our results demonstrate that PLUM's\nquantization method is more accurate than binary quantization with the same\nnumber of non-zero weights. Detailed analysis indicates that signed\nbinarization generates a smaller distribution of effectual (non-zero)\nparameters nested within a larger distribution of total parameters of latent\nfull-precision weights for a DNN block. Finally, the proposed PLUM framework\nachieves a 26% speedup on real hardware, doubles energy efficiency, and reduces\ndensity by 2.8x compared to binary methods while retaining top-1 accuracy when\ncompared to prior-art methods for ResNets on ImageNet (by achieving 66.2% top-1\naccuracy), presenting an alternative solution for deploying efficient models in\nresource-limited environments.", "AI": {"tldr": "PLUM is a co-design framework for DNN inference that leverages the repetition-sparsity trade-off, improving efficiency without sacrificing accuracy.", "motivation": "Efficient DNN inference on edge devices is crucial, and quantization/sparsity are key techniques. The paper explores the repetition-sparsity trade-off to enhance computational efficiency.", "method": "PLUM integrates DNN inference systems with quantization (forward/backward pass) to exploit the repetition-sparsity trade-off. It uses signed binarization for better accuracy.", "result": "PLUM achieves 26% speedup, doubles energy efficiency, reduces density by 2.8x, and retains top-1 accuracy (66.2% on ImageNet) compared to binary methods.", "conclusion": "PLUM offers an efficient solution for deploying DNNs in resource-limited environments, balancing accuracy and performance."}}
{"id": "2307.11470", "pdf": "https://arxiv.org/pdf/2307.11470", "abs": "https://arxiv.org/abs/2307.11470", "authors": ["Hao Qi", "Shixuan Xu", "Xinghui Dong"], "title": "Semi-supervised Underwater Image Enhancement Using A Physics-Aware Triple-Stream Network", "categories": ["cs.CV"], "comment": "13 pages, 10 figures", "summary": "Underwater images normally suffer from degradation due to the transmission\nmedium of water bodies. Both traditional prior-based approaches and deep\nlearning-based methods have been used to address this problem. However, the\ninflexible assumption of the former often impairs their effectiveness in\nhandling diverse underwater scenes, while the generalization of the latter to\nunseen images is usually weakened by insufficient data. In this study, we\nleverage both the physics-based Image Formation Model (IFM) and deep learning\ntechniques for Underwater Image Enhancement (UIE). To this end, we propose a\nnovel Physics-Aware Triple-Stream Underwater Image Enhancement Network, i.e.,\nPATS-UIENet, which comprises a Direct Signal Transmission Estimation Steam\n(D-Stream), a Backscatter Signal Transmission Estimation Steam (B-Stream) and\nan Ambient Light Estimation Stream (A-Stream). This network fulfills the UIE\ntask by explicitly estimating the degradation parameters of a revised IFM. We\nalso adopt an IFM-inspired semi-supervised learning framework, which exploits\nboth the labeled and unlabeled images, to address the issue of insufficient\ndata. To our knowledge, such a physics-aware deep network and the IFM-inspired\nsemi-supervised learning framework have not been used for the UIE task before.\nOur method performs better than, or at least comparably to, sixteen baselines\nacross six testing sets in the degradation estimation and UIE tasks. These\npromising results should be due to the fact that the proposed method can not\nonly model the degradation but also learn the characteristics of diverse\nunderwater scenes.", "AI": {"tldr": "A novel Physics-Aware Triple-Stream Network (PATS-UIENet) combines physics-based models and deep learning for underwater image enhancement, outperforming 16 baselines.", "motivation": "Underwater images degrade due to water transmission, with prior-based methods being inflexible and deep learning methods suffering from insufficient data.", "method": "PATS-UIENet uses three streams (D-Stream, B-Stream, A-Stream) to estimate degradation parameters of a revised Image Formation Model (IFM) and employs a semi-supervised learning framework.", "result": "The method outperforms or matches 16 baselines across six testing sets in degradation estimation and image enhancement.", "conclusion": "The approach effectively models degradation and learns diverse underwater scene characteristics, demonstrating superior performance."}}
{"id": "2505.03704", "pdf": "https://arxiv.org/pdf/2505.03704", "abs": "https://arxiv.org/abs/2505.03704", "authors": ["Kiichi Obuchi", "Yuta Yahagi", "Kiyohiko Toyama", "Shukichi Tanaka", "Kota Matsui"], "title": "Multi-modal cascade feature transfer for polymer property prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In this paper, we propose a novel transfer learning approach called\nmulti-modal cascade model with feature transfer for polymer property\nprediction.Polymers are characterized by a composite of data in several\ndifferent formats, including molecular descriptors and additive information as\nwell as chemical structures. However, in conventional approaches, prediction\nmodels were often constructed using each type of data separately. Our model\nenables more accurate prediction of physical properties for polymers by\ncombining features extracted from the chemical structure by graph convolutional\nneural networks (GCN) with features such as molecular descriptors and additive\ninformation. The predictive performance of the proposed method is empirically\nevaluated using several polymer datasets. We report that the proposed method\nshows high predictive performance compared to the baseline conventional\napproach using a single feature.", "AI": {"tldr": "A novel transfer learning approach combines multi-modal data (molecular descriptors, additive info, and chemical structures) via GCNs for improved polymer property prediction.", "motivation": "Existing methods treat polymer data types separately, limiting prediction accuracy.", "method": "Multi-modal cascade model integrates GCN-extracted chemical structure features with other data types.", "result": "Outperforms baseline single-feature methods on polymer datasets.", "conclusion": "Combining multi-modal data enhances polymer property prediction accuracy."}}
{"id": "2401.02663", "pdf": "https://arxiv.org/pdf/2401.02663", "abs": "https://arxiv.org/abs/2401.02663", "authors": ["Jiazhu Dai", "Haoyu Sun"], "title": "Effective backdoor attack on graph neural networks in link prediction tasks", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.", "AI": {"tldr": "The paper introduces a backdoor attack method for GNNs in link prediction tasks, demonstrating their vulnerability to such attacks.", "motivation": "Existing research on backdoor attacks in GNNs focuses on graph and node classification, leaving link prediction tasks unexplored. This paper aims to address this gap.", "method": "The method uses a single node as a trigger to poison selected node pairs during training, embedding a backdoor in the GNN model. During inference, linking the trigger node to unlinked node pairs activates the backdoor, causing incorrect predictions.", "result": "The study reveals that GNN models for link prediction are vulnerable to backdoor attacks, leading to incorrect link predictions when triggers are present.", "conclusion": "The paper highlights a security risk in GNNs for link prediction and calls for further research into defending against such attacks."}}
{"id": "2310.08387", "pdf": "https://arxiv.org/pdf/2310.08387", "abs": "https://arxiv.org/abs/2310.08387", "authors": ["Zhixuan Liang", "Xingyu Zeng", "Rui Zhao", "Ping Luo"], "title": "Aligning Data Selection with Performance: Performance-driven Reinforcement Learning for Active Learning in Object Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Active learning strategies aim to train high-performance models with minimal\nlabeled data by selecting the most informative instances for labeling. However,\nexisting methods for assessing data informativeness often fail to align\ndirectly with task model performance metrics, such as mean average precision\n(mAP) in object detection. This paper introduces Mean-AP Guided Reinforced\nActive Learning for Object Detection (MGRAL), a novel approach that leverages\nthe concept of expected model output changes as informativeness for deep\ndetection networks, directly optimizing the sampling strategy using mAP. MGRAL\nemploys a reinforcement learning agent based on LSTM architecture to\nefficiently navigate the combinatorial challenge of batch sample selection and\nthe non-differentiable nature between performance and selected batches. The\nagent optimizes selection using policy gradient with mAP improvement as the\nreward signal. To address the computational intensity of mAP estimation with\nunlabeled samples, we implement fast look-up tables, ensuring real-world\nfeasibility. We evaluate MGRAL on PASCAL VOC and MS COCO benchmarks across\nvarious backbone architectures. Our approach demonstrates strong performance,\nestablishing a new paradigm in reinforcement learning-based active learning for\nobject detection.", "AI": {"tldr": "MGRAL introduces a reinforcement learning-based active learning method for object detection, optimizing sample selection using mAP as a reward signal.", "motivation": "Existing methods for assessing data informativeness in active learning often misalign with task performance metrics like mAP in object detection.", "method": "MGRAL uses a reinforcement learning agent (LSTM-based) to select informative batches, optimizing with policy gradient and mAP as reward. Fast look-up tables address computational costs.", "result": "Evaluated on PASCAL VOC and MS COCO, MGRAL shows strong performance, setting a new standard for RL-based active learning in object detection.", "conclusion": "MGRAL successfully aligns sample selection with mAP, offering a feasible and effective approach for active learning in object detection."}}
{"id": "2505.03717", "pdf": "https://arxiv.org/pdf/2505.03717", "abs": "https://arxiv.org/abs/2505.03717", "authors": ["Richard Y. Zhang"], "title": "Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": null, "summary": "The classical low-rank matrix recovery problem is well-known to exhibit\n\\emph{benign nonconvexity} under the restricted isometry property (RIP): local\noptimization is guaranteed to converge to the global optimum, where the ground\ntruth is recovered. We investigate whether benign nonconvexity continues to\nhold when the factor matrices are constrained to be elementwise nonnegative --\na common practical requirement. In the simple setting of a rank-1 nonnegative\nground truth, we confirm that benign nonconvexity holds in the fully-observed\ncase with RIP constant $\\delta=0$. Surprisingly, however, this property fails\nto extend to the partially-observed case with any arbitrarily small RIP\nconstant $\\delta\\to0^{+}$, irrespective of rank overparameterization. This\nfinding exposes a critical theoretical gap: the continuity argument widely used\nto explain the empirical robustness of low-rank matrix recovery fundamentally\nbreaks down once nonnegative constraints are imposed.", "AI": {"tldr": "The paper explores whether benign nonconvexity in low-rank matrix recovery holds under nonnegative constraints, finding it fails in partially-observed cases despite working in fully-observed scenarios.", "motivation": "To understand if the favorable properties of low-rank matrix recovery (like global convergence) persist when factor matrices are constrained to be nonnegative.", "method": "Analyzes the problem in a rank-1 nonnegative setting, comparing fully-observed and partially-observed cases under RIP conditions.", "result": "Benign nonconvexity holds for fully-observed cases but fails in partially-observed ones, even with small RIP constants.", "conclusion": "Nonnegative constraints disrupt the continuity argument used in low-rank matrix recovery, revealing a theoretical limitation."}}
{"id": "2402.02399", "pdf": "https://arxiv.org/pdf/2402.02399", "abs": "https://arxiv.org/abs/2402.02399", "authors": ["Hao Wang", "Licheng Pan", "Zhichao Chen", "Degui Yang", "Sen Zhang", "Yifei Yang", "Xinggao Liu", "Haoxuan Li", "Dacheng Tao"], "title": "FreDF: Learning to Forecast in the Frequency Domain", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "comment": "Accepted by ICLR 2025", "summary": "Time series modeling presents unique challenges due to autocorrelation in\nboth historical data and future sequences. While current research predominantly\naddresses autocorrelation within historical data, the correlations among future\nlabels are often overlooked. Specifically, modern forecasting models primarily\nadhere to the Direct Forecast (DF) paradigm, generating multi-step forecasts\nindependently and disregarding label autocorrelation over time. In this work,\nwe demonstrate that the learning objective of DF is biased in the presence of\nlabel autocorrelation. To address this issue, we propose the Frequency-enhanced\nDirect Forecast (FreDF), which mitigates label autocorrelation by learning to\nforecast in the frequency domain, thereby reducing estimation bias. Our\nexperiments show that FreDF significantly outperforms existing state-of-the-art\nmethods and is compatible with a variety of forecast models. Code is available\nat https://github.com/Master-PLC/FreDF.", "AI": {"tldr": "FreDF addresses label autocorrelation in time series forecasting by learning in the frequency domain, outperforming current methods.", "motivation": "Current forecasting models ignore label autocorrelation, leading to biased predictions.", "method": "Proposes Frequency-enhanced Direct Forecast (FreDF) to mitigate bias by forecasting in the frequency domain.", "result": "FreDF significantly outperforms state-of-the-art methods and works with various models.", "conclusion": "FreDF effectively reduces estimation bias and improves forecasting accuracy."}}
{"id": "2311.14284", "pdf": "https://arxiv.org/pdf/2311.14284", "abs": "https://arxiv.org/abs/2311.14284", "authors": ["Weijia Wu", "Zhuang Li", "Yefei He", "Mike Zheng Shou", "Chunhua Shen", "Lele Cheng", "Yan Li", "Tingting Gao", "Di Zhang"], "title": "Paragraph-to-Image Generation with Information-Enriched Diffusion Model", "categories": ["cs.CV"], "comment": "The project website is at:\n  https://weijiawu.github.io/ParaDiffusionPage/. Code:\n  https://github.com/weijiawu/ParaDiffusion", "summary": "Text-to-image (T2I) models have recently experienced rapid development,\nachieving astonishing performance in terms of fidelity and textual alignment\ncapabilities. However, given a long paragraph (up to 512 words), these\ngeneration models still struggle to achieve strong alignment and are unable to\ngenerate images depicting complex scenes. In this paper, we introduce an\ninformation-enriched diffusion model for paragraph-to-image generation task,\ntermed ParaDiffusion, which delves into the transference of the extensive\nsemantic comprehension capabilities of large language models to the task of\nimage generation. At its core is using a large language model (e.g., Llama V2)\nto encode long-form text, followed by fine-tuning with LORA to alignthe\ntext-image feature spaces in the generation task. To facilitate the training of\nlong-text semantic alignment, we also curated a high-quality paragraph-image\npair dataset, namely ParaImage. This dataset contains a small amount of\nhigh-quality, meticulously annotated data, and a large-scale synthetic dataset\nwith long text descriptions being generated using a vision-language model.\nExperiments demonstrate that ParaDiffusion outperforms state-of-the-art models\n(SD XL, DeepFloyd IF) on ViLG-300 and ParaPrompts, achieving up to 15% and 45%\nhuman voting rate improvements for visual appeal and text faithfulness,\nrespectively. The code and dataset will be released to foster community\nresearch on long-text alignment.", "AI": {"tldr": "ParaDiffusion improves text-to-image generation for long paragraphs by leveraging large language models and a curated dataset, outperforming existing models in alignment and visual quality.", "motivation": "Current T2I models struggle with long paragraphs and complex scenes, lacking strong alignment and fidelity.", "method": "Uses a large language model (Llama V2) to encode long text, fine-tuned with LORA for text-image alignment, and introduces the ParaImage dataset for training.", "result": "Outperforms SD XL and DeepFloyd IF, with 15% and 45% improvements in visual appeal and text faithfulness, respectively.", "conclusion": "ParaDiffusion advances long-text alignment in image generation, with code and dataset released for community research."}}
{"id": "2310.12395", "pdf": "https://arxiv.org/pdf/2310.12395", "abs": "https://arxiv.org/abs/2310.12395", "authors": ["Christopher Scarvelis", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Justin Solomon"], "title": "Closed-Form Diffusion Models", "categories": ["cs.LG", "stat.ML"], "comment": "Published in TMLR, May 2025", "summary": "Score-based generative models (SGMs) sample from a target distribution by\niteratively transforming noise using the score function of the perturbed\ntarget. For any finite training set, this score function can be evaluated in\nclosed form, but the resulting SGM memorizes its training data and does not\ngenerate novel samples. In practice, one approximates the score by training a\nneural network via score-matching. The error in this approximation promotes\ngeneralization, but neural SGMs are costly to train and sample, and the\neffective regularization this error provides is not well-understood\ntheoretically. In this work, we instead explicitly smooth the closed-form score\nto obtain an SGM that generates novel samples without training. We analyze our\nmodel and propose an efficient nearest-neighbor-based estimator of its score\nfunction. Using this estimator, our method achieves competitive sampling times\nwhile running on consumer-grade CPUs.", "AI": {"tldr": "SGMs generate samples by transforming noise using the score function of a perturbed target. Explicit smoothing of the closed-form score avoids training and enables novel sample generation.", "motivation": "To address the high cost and theoretical gaps of neural SGMs by proposing a training-free, efficient alternative.", "method": "Explicitly smooth the closed-form score and use a nearest-neighbor-based estimator for efficient sampling.", "result": "Achieves competitive sampling times on consumer-grade CPUs without training.", "conclusion": "The proposed method offers a practical, efficient alternative to neural SGMs for generating novel samples."}}
{"id": "2403.18850", "pdf": "https://arxiv.org/pdf/2403.18850", "abs": "https://arxiv.org/abs/2403.18850", "authors": ["Jonito Aerts Argu\u00eblles"], "title": "Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study of Visual Perception", "categories": ["q-bio.NC", "cs.AI", "quant-ph"], "comment": "22 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:2208.03726", "summary": "We show that colors are light quanta for human visual perception in a similar\nway as photons are light quanta for physical measurements of light waves. Our\nresult relies on the identification in the quantum measurement process itself\nof the warping mechanism which is characteristic of human perception. This\nwarping mechanism makes stimuli classified into the same category perceived as\nmore similar, while stimuli classified into different m categories are\nperceived as more different. In the quantum measurement process, the warping\ntakes place between the pure states, which play the role played for human\nperception by the stimuli, and the density states after decoherence, which play\nthe role played for human perception by the percepts. We use the natural metric\nfor pure states, namely the normalized Fubini Study metric to measure distances\nbetween pure states, and the natural metric for density states, namely the\nnormalized trace-class metric, to measure distances between density states. We\nthen show that when pure states lie within a well-defined region surrounding an\neigenstate, the quantum measurement, namely the process of decoherence,\ncontracts the distance between these pure states, while the reverse happens for\npure states lying in a well-defined region between two eigenstates, for which\nthe quantum measurement causes a dilation. We elaborate as an example the\nsituation of a two-dimensional quantum measurement described by the Bloch model\nand apply it to the situation of two colors 'Light' and 'Dark'. We argue that\nthis analogy of warping, on the one hand in human perception and on the other\nhand in the quantum measurement process, makes colors to be quanta of light for\nhuman vision.", "AI": {"tldr": "The paper draws an analogy between human color perception and quantum measurement, showing how warping mechanisms in both processes classify stimuli similarly.", "motivation": "To bridge the gap between human visual perception and quantum physics by identifying a common warping mechanism in both domains.", "method": "The study uses quantum measurement processes, specifically decoherence, to model human perception. Metrics like the Fubini-Study and trace-class metrics are applied to measure distances between states.", "result": "The quantum measurement process contracts or dilates distances between states, mirroring human perception's warping of stimuli into categories.", "conclusion": "Colors act as light quanta for human vision, analogous to photons in quantum physics, due to shared warping mechanisms in perception and measurement."}}
{"id": "2406.02720", "pdf": "https://arxiv.org/pdf/2406.02720", "abs": "https://arxiv.org/abs/2406.02720", "authors": ["Haolin Li", "Jinyang Liu", "Mario Sznaier", "Octavia Camps"], "title": "3D-HGS: 3D Half-Gaussian Splatting", "categories": ["cs.CV", "cs.GR"], "comment": "8 pages, 9 figures", "summary": "Photo-realistic image rendering from 3D scene reconstruction has advanced\nsignificantly with neural rendering techniques. Among these, 3D Gaussian\nSplatting (3D-GS) outperforms Neural Radiance Fields (NeRFs) in quality and\nspeed but struggles with shape and color discontinuities. We propose 3D\nHalf-Gaussian (3D-HGS) kernels as a plug-and-play solution to address these\nlimitations. Our experiments show that 3D-HGS enhances existing 3D-GS methods,\nachieving state-of-the-art rendering quality without compromising speed.", "AI": {"tldr": "3D Half-Gaussian (3D-HGS) kernels improve 3D Gaussian Splatting (3D-GS) by addressing shape and color discontinuities, achieving top-tier rendering quality without speed loss.", "motivation": "3D-GS outperforms NeRFs in quality and speed but has limitations with shape and color discontinuities.", "method": "Proposes 3D-HGS kernels as a plug-and-play solution to enhance 3D-GS.", "result": "3D-HGS improves rendering quality while maintaining speed, surpassing existing methods.", "conclusion": "3D-HGS is an effective solution for enhancing 3D-GS, offering superior rendering quality without sacrificing performance."}}
{"id": "2401.12033", "pdf": "https://arxiv.org/pdf/2401.12033", "abs": "https://arxiv.org/abs/2401.12033", "authors": ["Marlon Becker", "Frederick Altrock", "Benjamin Risse"], "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The recently proposed optimization algorithm for deep neural networks\nSharpness Aware Minimization (SAM) suggests perturbing parameters before\ngradient calculation by a gradient ascent step to guide the optimization into\nparameter space regions of flat loss. While significant generalization\nimprovements and thus reduction of overfitting could be demonstrated, the\ncomputational costs are doubled due to the additionally needed gradient\ncalculation, making SAM unfeasible in case of limited computationally\ncapacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose\nMomentum-SAM (MSAM), which perturbs parameters in the direction of the\naccumulated momentum vector to achieve low sharpness without significant\ncomputational overhead or memory demands over SGD or Adam. We evaluate MSAM in\ndetail and reveal insights on separable mechanisms of NAG, SAM and MSAM\nregarding training optimization and generalization. Code is available at\nhttps://github.com/MarlonBecker/MSAM.", "AI": {"tldr": "MSAM improves SAM by using momentum to reduce computational costs while maintaining generalization benefits.", "motivation": "SAM's computational overhead limits its feasibility; MSAM leverages momentum for efficiency.", "method": "MSAM perturbs parameters using accumulated momentum, avoiding extra gradient calculations.", "result": "MSAM achieves low sharpness with minimal overhead, comparable to SGD or Adam.", "conclusion": "MSAM combines SAM's generalization benefits with NAG's efficiency, offering a practical alternative."}}
{"id": "2405.15444", "pdf": "https://arxiv.org/pdf/2405.15444", "abs": "https://arxiv.org/abs/2405.15444", "authors": ["Patryk Krukowski", "Anna Bielawska", "Kamil Ksi\u0105\u017cek", "Pawe\u0142 Wawrzy\u0144ski", "Pawe\u0142 Batorski", "Przemys\u0142aw Spurek"], "title": "HINT: Hypernetwork Approach to Training Weight Interval Regions in Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recently, a new Continual Learning (CL) paradigm was presented to control\ncatastrophic forgetting, called Interval Continual Learning (InterContiNet),\nwhich relies on enforcing interval constraints on the neural network parameter\nspace. Unfortunately, InterContiNet training is challenging due to the high\ndimensionality of the weight space, making intervals difficult to manage. To\naddress this issue, we introduce HINT, a technique that employs interval\narithmetic within the embedding space and utilizes a hypernetwork to map these\nintervals to the target network parameter space. We train interval embeddings\nfor consecutive tasks and train a hypernetwork to transform these embeddings\ninto weights of the target network. An embedding for a given task is trained\nalong with the hypernetwork, preserving the response of the target network for\nthe previous task embeddings. Interval arithmetic works with a more manageable,\nlower-dimensional embedding space rather than directly preparing intervals in a\nhigh-dimensional weight space. Our model allows faster and more efficient\ntraining. Furthermore, HINT maintains the guarantee of not forgetting. At the\nend of training, we can choose one universal embedding to produce a single\nnetwork dedicated to all tasks. In such a framework, hypernetwork is used only\nfor training and, finally, we can utilize one set of weights. HINT obtains\nsignificantly better results than InterContiNet and gives SOTA results on\nseveral benchmarks.", "AI": {"tldr": "HINT introduces interval arithmetic in embedding space and a hypernetwork to manage intervals efficiently, improving training speed and preventing forgetting in Continual Learning.", "motivation": "Address the challenge of high-dimensional weight space in InterContiNet by simplifying interval management in Continual Learning.", "method": "Uses interval embeddings and a hypernetwork to map intervals to target network weights, training embeddings and hypernetwork together.", "result": "Achieves faster training, prevents forgetting, and outperforms InterContiNet with SOTA results on benchmarks.", "conclusion": "HINT offers an efficient solution for Continual Learning, enabling a single universal embedding for all tasks post-training."}}
{"id": "2406.07113", "pdf": "https://arxiv.org/pdf/2406.07113", "abs": "https://arxiv.org/abs/2406.07113", "authors": ["Sergey Linok", "Tatiana Zemskova", "Svetlana Ladanova", "Roman Titkov", "Dmitry Yudin", "Maxim Monastyrny", "Aleksei Valenkov"], "title": "Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene Graph", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 6 figures, 5 tables", "summary": "Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic spatial edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.", "AI": {"tldr": "BBQ is a modular approach for 3D object grounding using scene graphs and LLMs, outperforming existing methods on complex queries and spatial relations.", "motivation": "Existing CLIP-based methods fail with ambiguous descriptions requiring object relations, prompting the need for a more robust solution.", "method": "BBQ constructs 3D scene graphs with spatial edges, uses DINO for object-centric maps, and employs raycasting with a 2D vision-language model.", "result": "BBQ leads in open-vocabulary 3D segmentation and improves grounding for complex queries on benchmarks like Sr3D+ and ScanRefer.", "conclusion": "BBQ's design enables fast processing and effective robotics applications, with code made publicly available."}}
{"id": "2402.09780", "pdf": "https://arxiv.org/pdf/2402.09780", "abs": "https://arxiv.org/abs/2402.09780", "authors": ["Eugenio Ressa", "Alberto Marchisio", "Maurizio Martina", "Guido Masera", "Muhammad Shafique"], "title": "TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems", "categories": ["cs.LG"], "comment": "Accepted at the 2025 International Joint Conference on Neural\n  Networks (IJCNN), Rome, Italy, July 2025", "summary": "The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.", "AI": {"tldr": "TinyCL is a hardware architecture designed for Continuous Learning (CL) on resource-constrained systems, achieving 58x speedup over GPUs while minimizing memory access and power consumption.", "motivation": "Current DNN accelerators lack support for backpropagation and weight updates required for CL, leading to resource inefficiency in autonomous systems.", "method": "TinyCL includes a processing unit for forward/backward propagation and a control unit for CL workload management, using snake-like sliding windows and reconfigurable Multiply-and-Accumulate units.", "result": "TinyCL achieves a 58x speedup (1.76s vs. 103s) over GPUs for training on CIFAR10, consuming 86 mW in a 4.74 mm\u00b2 die.", "conclusion": "TinyCL is the first hardware accelerator enabling efficient CL on autonomous systems, demonstrating significant performance and energy efficiency gains."}}
{"id": "2406.13216", "pdf": "https://arxiv.org/pdf/2406.13216", "abs": "https://arxiv.org/abs/2406.13216", "authors": ["Songyang Chen", "Yu Liu", "Lei Zou", "Zexuan Wang", "Youfang Lin"], "title": "CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 9 figures", "summary": "Unsupervised graph alignment finds the node correspondence between a pair of\nattributed graphs by only exploiting graph structure and node features. One\ncategory of recent studies first computes the node representation and then\nmatches nodes with the largest embedding-based similarity, while the other\ncategory reduces the problem to optimal transport (OT) via Gromov-Wasserstein\nlearning. However, it remains largely unexplored in the model expressiveness,\nas well as how theoretical expressivity impacts prediction accuracy. We\ninvestigate the model expressiveness from two aspects. First, we characterize\nthe model's discriminative power in distinguishing matched and unmatched node\npairs across two graphs. Second, we study the model's capability of\nguaranteeing node matching properties such as one-to-one matching and mutual\nalignment. Motivated by our theoretical analysis, we put forward a hybrid\napproach named CombAlign with stronger expressive power. Specifically, we\nenable cross-dimensional feature interaction for OT-based learning and propose\nan embedding-based method inspired by the Weisfeiler-Lehman test. We also apply\nnon-uniform marginals obtained from the embedding-based modules to OT as priors\nfor more expressiveness. Based on that, we propose a traditional\nalgorithm-based refinement, which combines our OT and embedding-based\npredictions using the ensemble learning strategy and reduces the problem to\nmaximum weight matching. With carefully designed edge weights, we ensure those\nmatching properties and further enhance prediction accuracy. By extensive\nexperiments, we demonstrate a significant improvement of 14.5% in alignment\naccuracy compared to state-of-the-art approaches and confirm the soundness of\nour theoretical analysis.", "AI": {"tldr": "The paper investigates model expressiveness in unsupervised graph alignment, proposing a hybrid approach (CombAlign) that combines optimal transport and embedding-based methods, achieving a 14.5% accuracy improvement.", "motivation": "To explore the model's discriminative power and capability in ensuring node matching properties (e.g., one-to-one matching) in unsupervised graph alignment.", "method": "Proposes CombAlign, a hybrid approach integrating cross-dimensional feature interaction for OT-based learning and an embedding-based method inspired by the Weisfeiler-Lehman test. Uses non-uniform marginals and a refinement step via maximum weight matching.", "result": "Achieves a 14.5% improvement in alignment accuracy over state-of-the-art methods, validating the theoretical analysis.", "conclusion": "CombAlign enhances expressiveness and accuracy in graph alignment, demonstrating the impact of theoretical expressivity on practical performance."}}
{"id": "2406.13896", "pdf": "https://arxiv.org/pdf/2406.13896", "abs": "https://arxiv.org/abs/2406.13896", "authors": ["Nathaniel Chodosh", "Anish Madan", "Simon Lucey", "Deva Ramanan"], "title": "SMORE: Simultaneous Map and Object REconstruction", "categories": ["cs.CV"], "comment": "3DV 2025,CVPR 2025 4D Vision Workshop", "summary": "We present a method for dynamic surface reconstruction of large-scale urban\nscenes from LiDAR. Depth-based reconstructions tend to focus on small-scale\nobjects or large-scale SLAM reconstructions that treat moving objects as\noutliers. We take a holistic perspective and optimize a compositional model of\na dynamic scene that decomposes the world into rigidly-moving objects and the\nbackground. To achieve this, we take inspiration from recent novel view\nsynthesis methods and frame the reconstruction problem as a global optimization\nover neural surfaces, ego poses, and object poses, which minimizes the error\nbetween composed spacetime surfaces and input LiDAR scans. In contrast to view\nsynthesis methods, which typically minimize 2D errors with gradient descent, we\nminimize a 3D point-to-surface error by coordinate descent, which we decompose\ninto registration and surface reconstruction steps. Each step can be handled\nwell by off-the-shelf methods without any re-training. We analyze the surface\nreconstruction step for rolling-shutter LiDARs, and show that deskewing\noperations common in continuous time SLAM can be applied to dynamic objects as\nwell, improving results over prior art by an order of magnitude. Beyond\npursuing dynamic reconstruction as a goal in and of itself, we propose that\nsuch a system can be used to auto-label partially annotated sequences and\nproduce ground truth annotation for hard-to-label problems such as depth\ncompletion and scene flow. Please see https://anishmadan23.github.io/smore/ for\nmore visual results.", "AI": {"tldr": "A method for dynamic surface reconstruction of urban scenes from LiDAR, decomposing scenes into rigidly-moving objects and background using neural surfaces and optimization.", "motivation": "Existing methods focus on small-scale objects or treat moving objects as outliers, lacking a holistic approach for large-scale dynamic scenes.", "method": "Global optimization over neural surfaces, ego poses, and object poses, minimizing 3D point-to-surface error via coordinate descent. Uses off-the-shelf methods for registration and surface reconstruction.", "result": "Improves dynamic reconstruction accuracy by an order of magnitude, with applications in auto-labeling and ground truth generation.", "conclusion": "The method advances dynamic scene reconstruction and offers utility in labeling and annotation tasks."}}
{"id": "2402.11722", "pdf": "https://arxiv.org/pdf/2402.11722", "abs": "https://arxiv.org/abs/2402.11722", "authors": ["Da Long", "Zhitong Xu", "Qiwei Yuan", "Yin Yang", "Shandian Zhe"], "title": "Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems", "categories": ["cs.LG"], "comment": null, "summary": "Fourier Neural Operator (FNO) is a powerful and popular operator learning\nmethod. However, FNO is mainly used in forward prediction, yet a great many\napplications rely on solving inverse problems. In this paper, we propose an\ninvertible Fourier Neural Operator (iFNO) for jointly tackling the forward and\ninverse problems. We developed a series of invertible Fourier blocks in the\nlatent channel space to share the model parameters, exchange the information,\nand mutually regularize the learning for the bi-directional tasks. We\nintegrated a variational auto-encoder to capture the intrinsic structures\nwithin the input space and to enable posterior inference so as to mitigate\nchallenges of illposedness, data shortage, noises that are common in inverse\nproblems. We proposed a three-step process to combine the invertible blocks and\nthe VAE component for effective training. The evaluations on seven benchmark\nforward and inverse tasks have demonstrated the advantages of our approach.", "AI": {"tldr": "The paper introduces an invertible Fourier Neural Operator (iFNO) to handle both forward and inverse problems, improving on the standard FNO by integrating invertible blocks and a variational auto-encoder for better performance.", "motivation": "Existing FNO methods are limited to forward prediction, while many applications require solving inverse problems. The paper aims to bridge this gap by developing a bi-directional operator.", "method": "The authors propose iFNO, which uses invertible Fourier blocks in latent space for parameter sharing and mutual regularization. A variational auto-encoder is added to handle ill-posedness, data scarcity, and noise. Training involves a three-step process combining these components.", "result": "Evaluations on seven benchmark tasks show iFNO's advantages in tackling both forward and inverse problems.", "conclusion": "iFNO effectively addresses the limitations of FNO by enabling bi-directional task learning and handling challenges in inverse problems, demonstrating superior performance."}}
{"id": "2407.14394", "pdf": "https://arxiv.org/pdf/2407.14394", "abs": "https://arxiv.org/abs/2407.14394", "authors": ["Chelsea Sidrane", "Jana Tumova"], "title": "TTT: A Temporal Refinement Heuristic for Tenuously Tractable Discrete Time Reachability Problems", "categories": ["eess.SY", "cs.AI", "cs.LO", "cs.SY"], "comment": "To appear in the proceedings of the American Control Conference (ACC)\n  2025", "summary": "Reachable set computation is an important tool for analyzing control systems.\nSimulating a control system can show general trends, but a formal tool like\nreachability analysis can provide guarantees of correctness. Reachability\nanalysis for complex control systems, e.g., with nonlinear dynamics and/or a\nneural network controller, is often either slow or overly conservative. To\naddress these challenges, much literature has focused on spatial refinement,\ni.e., tuning the discretization of the input sets and intermediate reachable\nsets. This paper introduces the idea of temporal refinement: automatically\nchoosing when along the horizon of the reachability problem to execute slow\nsymbolic queries which incur less approximation error versus fast concrete\nqueries which incur more approximation error. Temporal refinement can be\ncombined with other refinement approaches as an additional tool to trade off\ntractability and tightness in approximate reachable set computation. We\nintroduce a temporal refinement algorithm and demonstrate its effectiveness at\ncomputing approximate reachable sets for nonlinear systems with neural network\ncontrollers. We calculate reachable sets with varying computational budget and\nshow that our algorithm can generate approximate reachable sets with a similar\namount of error to the baseline in 20-70% less time.", "AI": {"tldr": "The paper introduces temporal refinement for reachability analysis in control systems, improving efficiency by balancing slow symbolic and fast concrete queries.", "motivation": "Reachability analysis for complex control systems is often slow or overly conservative, prompting the need for better methods.", "method": "The paper proposes temporal refinement, an algorithm that dynamically chooses between symbolic and concrete queries to optimize reachable set computation.", "result": "The algorithm reduces computation time by 20-70% while maintaining similar error levels compared to baselines.", "conclusion": "Temporal refinement is an effective tool for improving the efficiency of reachability analysis in nonlinear systems with neural network controllers."}}
{"id": "2410.02768", "pdf": "https://arxiv.org/pdf/2410.02768", "abs": "https://arxiv.org/abs/2410.02768", "authors": ["Jin Chen", "Kaijing Ma", "Haojian Huang", "Han Fang", "Hao Sun", "Mehdi Hosseinzadeh", "Zhe Liu"], "title": "Uncertainty-Guided Self-Questioning and Answering for Video-Language Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The development of multi-modal models has been rapidly advancing, with some\ndemonstrating remarkable capabilities. However, annotating video-text pairs\nremains expensive and insufficient. Take video question answering (VideoQA)\ntasks as an example, human annotated questions and answers often cover only\npart of the video, since the corresponding text is often short and monotonous,\nleading to underutilization of video. To address this, we propose a\nBootstrapping Video-Language Alignment framework (BoViLA), a self-training\nmethod that augments question samples during training process through LLM-based\nself-questioning and answering, which help model exploit video information and\nthe internal knowledge of LLMs more thoroughly to improve modality alignment.\nHowever, low-quality self-generated questions may instead contaminate the\nperformance, especially in the early stages of training, as we have observed in\nour experiments. To filter bad self-generated questions, we introduce\nEvidential Deep Learning (EDL) to estimate uncertainty and assess the quality\nof self-generated questions by evaluating the modality alignment within the\ncontext. To the best of our knowledge, this work is the first to explore\nLLM-based self-training frameworks for modality alignment. We evaluate BoViLA\non five strong VideoQA benchmarks, where it outperforms several\nstate-of-the-art methods and demonstrate its effectiveness and generality.\nAdditionally, we provide extensive analyses of the self-training framework and\nthe EDL-based uncertainty filtering mechanism. The code will be made available.", "AI": {"tldr": "BoViLA is a self-training framework for video-language alignment, using LLM-based self-questioning and EDL for uncertainty filtering to improve VideoQA performance.", "motivation": "Annotating video-text pairs is costly and often underutilizes video content, limiting VideoQA performance.", "method": "Proposes BoViLA, a self-training method with LLM-based self-questioning and EDL for uncertainty filtering.", "result": "Outperforms state-of-the-art methods on five VideoQA benchmarks.", "conclusion": "BoViLA effectively improves modality alignment and VideoQA performance, with potential for broader applications."}}
{"id": "2402.17967", "pdf": "https://arxiv.org/pdf/2402.17967", "abs": "https://arxiv.org/abs/2402.17967", "authors": ["Koshi Oishi", "Yota Hashizume", "Tomohiko Jimbo", "Hirotaka Kaji", "Kenji Kashima"], "title": "Imitation-regularized Optimal Transport on Networks: Provable Robustness and Application to Logistics Planning", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted for publication in: IEEE Control Systems Letters, vol. 8,\n  pp. 3470-3475, 2024, doi: 10.1109/LCSYS.2025.3546804", "summary": "Transport systems on networks are crucial in various applications, but face a\nsignificant risk of being adversely affected by unforeseen circumstances such\nas disasters. The application of entropy-regularized optimal transport (OT) on\ngraph structures has been investigated to enhance the robustness of transport\non such networks. In this study, we propose an imitation-regularized OT (I-OT)\nthat mathematically incorporates prior knowledge into the robustness of OT.\nThis method is expected to enhance interpretability by integrating human\ninsights into robustness and to accelerate practical applications. Furthermore,\nwe mathematically verify the robustness of I-OT and discuss how these\nrobustness properties relate to real-world applications. The effectiveness of\nthis method is validated through a logistics simulation using automotive parts\ndata.", "AI": {"tldr": "The paper proposes imitation-regularized optimal transport (I-OT) to enhance robustness in network transport systems by incorporating prior knowledge, improving interpretability and practical applications.", "motivation": "Transport systems on networks are vulnerable to unforeseen disruptions like disasters, necessitating robust solutions.", "method": "The study introduces I-OT, which integrates prior knowledge into entropy-regularized OT for robustness.", "result": "Mathematical verification confirms I-OT's robustness, and its effectiveness is validated via a logistics simulation.", "conclusion": "I-OT enhances robustness and interpretability, with potential for real-world applications."}}
{"id": "2407.21260", "pdf": "https://arxiv.org/pdf/2407.21260", "abs": "https://arxiv.org/abs/2407.21260", "authors": ["Taehyun Cho", "Seungyub Han", "Kyungjae Lee", "Seokhun Ju", "Dohyeong Kim", "Jungwoo Lee"], "title": "Bellman Unbiasedness: Tractable and Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Distributional reinforcement learning improves performance by capturing\nenvironmental stochasticity, but a comprehensive theoretical understanding of\nits effectiveness remains elusive. In addition, the intractable element of the\ninfinite dimensionality of distributions has been overlooked. In this paper, we\npresent a regret analysis of distributional reinforcement learning with general\nvalue function approximation in a finite episodic Markov decision process\nsetting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$\nwhich is essential for exactly learnable and provably efficient distributional\nupdates in an online manner. Among all types of statistical functionals for\nrepresenting infinite-dimensional return distributions, our theoretical results\ndemonstrate that only moment functionals can exactly capture the statistical\ninformation. Secondly, we propose a provably efficient algorithm,\n$\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E\nH^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of\nepisodes, and $d_E$ is the eluder dimension of a function class.", "AI": {"tldr": "The paper analyzes distributional reinforcement learning (RL), focusing on its theoretical foundations and proposing an efficient algorithm with a tight regret bound.", "motivation": "The motivation is to address the lack of theoretical understanding of distributional RL's effectiveness and the challenge of infinite-dimensional distributions.", "method": "The method involves introducing Bellman unbiasedness for efficient distributional updates and analyzing moment functionals for capturing statistical information. A new algorithm, SF-LSVI, is proposed.", "result": "The results show that only moment functionals can exactly capture statistical information, and the SF-LSVI algorithm achieves a tight regret bound of \u00d5(d_E H^(3/2)\u221aK).", "conclusion": "The paper concludes that moment functionals are essential for efficient distributional RL and that the proposed algorithm is provably effective with a tight regret bound."}}
{"id": "2411.02179", "pdf": "https://arxiv.org/pdf/2411.02179", "abs": "https://arxiv.org/abs/2411.02179", "authors": ["Yiqin Zhao", "Mallesham Dasari", "Tian Guo"], "title": "CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality", "categories": ["cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "High-quality environment lighting is essential for creating immersive mobile\naugmented reality (AR) experiences. However, achieving visually coherent\nestimation for mobile AR is challenging due to several key limitations in AR\ndevice sensing capabilities, including low camera FoV and limited pixel dynamic\nranges. Recent advancements in generative AI, which can generate high-quality\nimages from different types of prompts, including texts and images, present a\npotential solution for high-quality lighting estimation. Still, to effectively\nuse generative image diffusion models, we must address two key limitations of\ncontent quality and slow inference. In this work, we design and implement a\ngenerative lighting estimation system called CleAR that can produce\nhigh-quality, diverse environment maps in the format of 360{\\deg} HDR images.\nSpecifically, we design a two-step generation pipeline guided by AR environment\ncontext data to ensure the output aligns with the physical environment's visual\ncontext and color appearance. To improve the estimation robustness under\ndifferent lighting conditions, we design a real-time refinement component to\nadjust lighting estimation results on AR devices. Through a combination of\nquantitative and qualitative evaluations, we show that CleAR outperforms\nstate-of-the-art lighting estimation methods on both estimation accuracy,\nlatency, and robustness, and is rated by 31 participants as producing better\nrenderings for most virtual objects. For example, CleAR achieves 51% to 56%\naccuracy improvement on virtual object renderings across objects of three\ndistinctive types of materials and reflective properties. CleAR produces\nlighting estimates of comparable or better quality in just 3.2 seconds -- over\n110X faster than state-of-the-art methods.", "AI": {"tldr": "CleAR is a generative lighting estimation system for mobile AR that uses a two-step pipeline and real-time refinement to produce high-quality, diverse 360\u00b0 HDR environment maps, outperforming state-of-the-art methods in accuracy, speed, and robustness.", "motivation": "High-quality lighting is crucial for immersive mobile AR, but device limitations like low camera FoV and dynamic range hinder accurate estimation. Generative AI offers a solution, but content quality and slow inference remain challenges.", "method": "CleAR employs a two-step generation pipeline guided by AR context data and includes a real-time refinement component for robustness under varying lighting conditions.", "result": "CleAR improves virtual object rendering accuracy by 51-56% and achieves comparable or better quality in 3.2 seconds, 110X faster than existing methods.", "conclusion": "CleAR effectively addresses the limitations of generative models for lighting estimation, delivering superior performance in accuracy, speed, and user-rated quality for mobile AR."}}
{"id": "2403.16137", "pdf": "https://arxiv.org/pdf/2403.16137", "abs": "https://arxiv.org/abs/2403.16137", "authors": ["Ziwen Zhao", "Yixin Su", "Yuhua Li", "Yixiong Zou", "Ruixuan Li", "Rui Zhang"], "title": "A Survey on Self-Supervised Graph Foundation Models: Knowledge-Based Perspective", "categories": ["cs.LG", "cs.SI"], "comment": "Accepted by TKDE; full version (27 pages, 9 figures)", "summary": "Graph self-supervised learning (SSL) is now a go-to method for pre-training\ngraph foundation models (GFMs). There is a wide variety of knowledge patterns\nembedded in the graph data, such as node properties and clusters, which are\ncrucial to learning generalized representations for GFMs. However, existing\nsurveys of GFMs have several shortcomings: they lack comprehensiveness\nregarding the most recent progress, have unclear categorization of\nself-supervised methods, and take a limited architecture-based perspective that\nis restricted to only certain types of graph models. As the ultimate goal of\nGFMs is to learn generalized graph knowledge, we provide a comprehensive survey\nof self-supervised GFMs from a novel knowledge-based perspective. We propose a\nknowledge-based taxonomy, which categorizes self-supervised graph models by the\nspecific graph knowledge utilized. Our taxonomy consists of microscopic (nodes,\nlinks, etc.), mesoscopic (context, clusters, etc.), and macroscopic knowledge\n(global structure, manifolds, etc.). It covers a total of 9 knowledge\ncategories and more than 25 pretext tasks for pre-training GFMs, as well as\nvarious downstream task generalization strategies. Such a knowledge-based\ntaxonomy allows us to re-examine graph models based on new architectures more\nclearly, such as graph language models, as well as provide more in-depth\ninsights for constructing GFMs.", "AI": {"tldr": "The paper surveys self-supervised learning for graph foundation models (GFMs) from a knowledge-based perspective, proposing a taxonomy categorizing graph knowledge into microscopic, mesoscopic, and macroscopic levels.", "motivation": "Existing surveys lack comprehensiveness, clear categorization, and a broader perspective on GFMs. The goal is to learn generalized graph knowledge.", "method": "Proposes a knowledge-based taxonomy with 9 categories and over 25 pretext tasks, covering nodes, links, clusters, and global structures.", "result": "The taxonomy provides a clearer framework for analyzing GFMs, including newer architectures like graph language models.", "conclusion": "The knowledge-based approach offers deeper insights for constructing GFMs and generalizing to downstream tasks."}}
{"id": "2408.01672", "pdf": "https://arxiv.org/pdf/2408.01672", "abs": "https://arxiv.org/abs/2408.01672", "authors": ["Yuanyuan Zhang", "Runwei Guan", "Lingxiao Li", "Rui Yang", "Yutao Yue", "Eng Gee Lim"], "title": "radarODE: An ODE-Embedded Deep Learning Model for Contactless ECG Reconstruction from Millimeter-Wave Radar", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Radar-based contactless cardiac monitoring has become a popular research\ndirection recently, but the fine-grained electrocardiogram (ECG) signal is\nstill hard to reconstruct from millimeter-wave radar signal. The key obstacle\nis to decouple the cardiac activities in the electrical domain (i.e., ECG) from\nthat in the mechanical domain (i.e., heartbeat), and most existing research\nonly uses pure data-driven methods to map such domain transformation as a black\nbox. Therefore, this work first proposes a signal model for domain\ntransformation, and then a novel deep learning framework called radarODE is\ndesigned to fuse the temporal and morphological features extracted from radar\nsignals and generate ECG. In addition, ordinary differential equations are\nembedded in radarODE as a decoder to provide morphological prior, helping the\nconvergence of the model training and improving the robustness under body\nmovements. After being validated on the dataset, the proposed radarODE achieves\nbetter performance compared with the benchmark in terms of missed detection\nrate, root mean square error, Pearson correlation coefficient with the\nimprovement of 9%, 16% and 19%, respectively. The validation results imply that\nradarODE is capable of recovering ECG signals from radar signals with high\nfidelity and can be potentially implemented in real-life scenarios.", "AI": {"tldr": "The paper proposes radarODE, a deep learning framework to reconstruct ECG signals from radar data, improving accuracy and robustness.", "motivation": "Existing methods struggle to decouple cardiac activities in electrical (ECG) and mechanical (heartbeat) domains, relying on black-box data-driven approaches.", "method": "Introduces a signal model for domain transformation and radarODE, a deep learning framework with embedded ODEs for morphological priors.", "result": "RadarODE outperforms benchmarks with 9%, 16%, and 19% improvements in missed detection rate, RMSE, and Pearson correlation, respectively.", "conclusion": "RadarODE effectively reconstructs ECG from radar signals, showing potential for real-life applications."}}
{"id": "2411.03239", "pdf": "https://arxiv.org/pdf/2411.03239", "abs": "https://arxiv.org/abs/2411.03239", "authors": ["Huan Zheng", "Wencheng Han", "Jianbing Shen"], "title": "Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 & The 1st place award for the ECCV 2024 AIM\n  Compressed Depth Upsampling Challenge", "summary": "Recovering high-quality depth maps from compressed sources has gained\nsignificant attention due to the limitations of consumer-grade depth cameras\nand the bandwidth restrictions during data transmission. However, current\nmethods still suffer from two challenges. First, bit-depth compression produces\na uniform depth representation in regions with subtle variations, hindering the\nrecovery of detailed information. Second, densely distributed random noise\nreduces the accuracy of estimating the global geometric structure of the scene.\nTo address these challenges, we propose a novel framework, termed\ngeometry-decoupled network (GDNet), for compressed depth map super-resolution\nthat decouples the high-quality depth map reconstruction process by handling\nglobal and detailed geometric features separately. To be specific, we propose\nthe fine geometry detail encoder (FGDE), which is designed to aggregate fine\ngeometry details in high-resolution low-level image features while\nsimultaneously enriching them with complementary information from\nlow-resolution context-level image features. In addition, we develop the global\ngeometry encoder (GGE) that aims at suppressing noise and extracting global\ngeometric information effectively via constructing compact feature\nrepresentation in a low-rank space. We conduct experiments on multiple\nbenchmark datasets, demonstrating that our GDNet significantly outperforms\ncurrent methods in terms of geometric consistency and detail recovery. In the\nECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st\nplace award. Our codes are available at: https://github.com/Ian0926/GDNet.", "AI": {"tldr": "GDNet is a novel framework for compressed depth map super-resolution, addressing challenges of uniform depth representation and noise by decoupling global and detailed geometric feature handling.", "motivation": "Limitations of consumer-grade depth cameras and bandwidth restrictions during data transmission necessitate high-quality depth map recovery from compressed sources.", "method": "GDNet uses a fine geometry detail encoder (FGDE) for detail aggregation and a global geometry encoder (GGE) for noise suppression and global structure extraction.", "result": "GDNet outperforms current methods in geometric consistency and detail recovery, winning 1st place in the ECCV 2024 AIM Challenge.", "conclusion": "GDNet effectively addresses compression challenges, offering superior performance in depth map super-resolution."}}
{"id": "2405.07884", "pdf": "https://arxiv.org/pdf/2405.07884", "abs": "https://arxiv.org/abs/2405.07884", "authors": ["YuFei Lai"], "title": "Lai Loss: A Novel Loss for Gradient Control", "categories": ["cs.LG"], "comment": "The experiment in this article is not very rigorous and may require\n  further testing for its effectiveness", "summary": "In the field of machine learning, traditional regularization methods tend to\ndirectly add regularization terms to the loss function. This paper introduces\nthe \"Lai loss\", a novel loss design that integrates the regularization terms\n(specifically, gradients) into the traditional loss function through\nstraightforward geometric concepts. This design penalizes the gradients with\nthe loss itself, allowing for control of the gradients while ensuring maximum\naccuracy. With this loss, we can effectively control the model's smoothness and\nsensitivity, potentially offering the dual benefits of improving the model's\ngeneralization performance and enhancing its noise resistance on specific\nfeatures. Additionally, we proposed a training method that successfully\naddresses the challenges in practical applications. We conducted preliminary\nexperiments using publicly available datasets from Kaggle, demonstrating that\nthe design of Lai loss can control the model's smoothness and sensitivity while\nmaintaining stable model performance.", "AI": {"tldr": "The paper introduces 'Lai loss,' a novel loss function integrating regularization via gradients, improving model smoothness, sensitivity, and generalization.", "motivation": "Traditional regularization methods add terms to the loss function; this work aims to integrate regularization more naturally via geometric concepts.", "method": "Designs 'Lai loss' to penalize gradients within the loss function, controlling model smoothness and sensitivity. Proposes a practical training method.", "result": "Experiments on Kaggle datasets show stable performance while controlling smoothness and sensitivity.", "conclusion": "Lai loss effectively balances regularization and accuracy, enhancing generalization and noise resistance."}}
{"id": "2408.04430", "pdf": "https://arxiv.org/pdf/2408.04430", "abs": "https://arxiv.org/abs/2408.04430", "authors": ["Micheline B\u00e9n\u00e9dicte Moumoula", "Abdoul Kader Kabore", "Jacques Klein", "Tegawend\u00e9 Bissyande"], "title": "The Struggles of LLMs in Cross-lingual Code Clone Detection", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "Accepted for publication at the ACM International Conference on the\n  Foundations of Software Engineering (FSE) 2025", "summary": "With the involvement of multiple programming languages in modern software\ndevelopment, cross-lingual code clone detection has gained traction within the\nsoftware engineering community. Numerous studies have explored this topic,\nproposing various promising approaches. Inspired by the significant advances in\nmachine learning in recent years, particularly Large Language Models (LLMs),\nwhich have demonstrated their ability to tackle various tasks, this paper\nrevisits cross-lingual code clone detection. We evaluate the performance of\nfive (05) LLMs and eight prompts (08) for the identification of cross-lingual\ncode clones. Additionally, we compare these results against two baseline\nmethods. Finally, we evaluate a pre-trained embedding model to assess the\neffectiveness of the generated representations for classifying clone and\nnon-clone pairs. The studies involving LLMs and Embedding models are evaluated\nusing two widely used cross-lingual datasets, XLCoST and CodeNet. Our results\nshow that LLMs can achieve high F1 scores, up to 0.99, for straightforward\nprogramming examples. However, they not only perform less well on programs\nassociated with complex programming challenges but also do not necessarily\nunderstand the meaning of \"code clones\" in a cross-lingual setting. We show\nthat embedding models used to represent code fragments from different\nprogramming languages in the same representation space enable the training of a\nbasic classifier that outperforms all LLMs by ~1 and ~20 percentage points on\nthe XLCoST and CodeNet datasets, respectively. This finding suggests that,\ndespite the apparent capabilities of LLMs, embeddings provided by embedding\nmodels offer suitable representations to achieve state-of-the-art performance\nin cross-lingual code clone detection.", "AI": {"tldr": "The paper evaluates LLMs and embedding models for cross-lingual code clone detection, finding that embedding models outperform LLMs, especially on complex tasks.", "motivation": "To leverage advances in LLMs for cross-lingual code clone detection and compare their performance with embedding models.", "method": "Evaluated 5 LLMs and 8 prompts, compared against 2 baselines, and assessed a pre-trained embedding model using XLCoST and CodeNet datasets.", "result": "LLMs achieved high F1 scores (up to 0.99) on simple examples but struggled with complex tasks. Embedding models outperformed LLMs by ~1-20 percentage points.", "conclusion": "Embedding models provide better representations for state-of-the-art cross-lingual code clone detection than LLMs."}}
{"id": "2411.15106", "pdf": "https://arxiv.org/pdf/2411.15106", "abs": "https://arxiv.org/abs/2411.15106", "authors": ["Alexandros Stergiou", "Ronald Poppe"], "title": "About Time: Advances, Challenges, and Outlooks of Action Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at the International Journal of Computer Vision (IJCV)", "summary": "We have witnessed impressive advances in video action understanding.\nIncreased dataset sizes, variability, and computation availability have enabled\nleaps in performance and task diversification. Current systems can provide\ncoarse- and fine-grained descriptions of video scenes, extract segments\ncorresponding to queries, synthesize unobserved parts of videos, and predict\ncontext across multiple modalities. This survey comprehensively reviews\nadvances in uni- and multi-modal action understanding across a range of tasks.\nWe focus on prevalent challenges, overview widely adopted datasets, and survey\nseminal works with an emphasis on recent advances. We broadly distinguish\nbetween three temporal scopes: (1) recognition tasks of actions observed in\nfull, (2) prediction tasks for ongoing partially observed actions, and (3)\nforecasting tasks for subsequent unobserved action(s). This division allows us\nto identify specific action modeling and video representation challenges.\nFinally, we outline future directions to address current shortcomings.", "AI": {"tldr": "A survey on advances in video action understanding, covering tasks, datasets, challenges, and future directions.", "motivation": "To review progress in video action understanding, driven by larger datasets and computational power, and to address current challenges.", "method": "Comprehensive review of uni- and multi-modal action understanding, focusing on tasks, datasets, and seminal works. Categorizes tasks into recognition, prediction, and forecasting.", "result": "Identifies challenges in action modeling and video representation, and highlights recent advances in the field.", "conclusion": "Outlines future directions to overcome current limitations in video action understanding."}}
{"id": "2407.18878", "pdf": "https://arxiv.org/pdf/2407.18878", "abs": "https://arxiv.org/abs/2407.18878", "authors": ["Swetha Ganesh", "Washim Uddin Mondal", "Vaneet Aggarwal"], "title": "A Sharper Global Convergence Analysis for Average Reward Reinforcement Learning via an Actor-Critic Approach", "categories": ["cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML), 2025", "summary": "This work examines average-reward reinforcement learning with general policy\nparametrization. Existing state-of-the-art (SOTA) guarantees for this problem\nare either suboptimal or hindered by several challenges, including poor\nscalability with respect to the size of the state-action space, high iteration\ncomplexity, and dependence on knowledge of mixing times and hitting times. To\naddress these limitations, we propose a Multi-level Monte Carlo-based Natural\nActor-Critic (MLMC-NAC) algorithm. Our work is the first to achieve a global\nconvergence rate of $\\tilde{\\mathcal{O}}(1/\\sqrt{T})$ for average-reward Markov\nDecision Processes (MDPs) (where $T$ is the horizon length), without requiring\nthe knowledge of mixing and hitting times. Moreover, the convergence rate does\nnot scale with the size of the state space, therefore even being applicable to\ninfinite state spaces.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2410.03032", "pdf": "https://arxiv.org/pdf/2410.03032", "abs": "https://arxiv.org/abs/2410.03032", "authors": ["Xiaohan Ding", "Kaike Ping", "Uma Sushmitha Gunturi", "Buse Carik", "Sophia Stil", "Lance T Wilhelm", "Taufiq Daryanto", "James Hawdon", "Sang Won Lee", "Eugenia H Rho"], "title": "CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Online hate speech has become increasingly prevalent on social media\nplatforms, causing harm to individuals and society. While efforts have been\nmade to combat this issue through content moderation, the potential of\nuser-driven counterspeech as an alternative solution remains underexplored.\nExisting counterspeech methods often face challenges such as fear of\nretaliation and skill-related barriers. To address these challenges, we\nintroduce CounterQuill, an AI-mediated system that assists users in composing\neffective and empathetic counterspeech. CounterQuill provides a three-step\nprocess: (1) a learning session to help users understand hate speech and\ncounterspeech; (2) a brainstorming session that guides users in identifying key\nelements of hate speech and exploring counterspeech strategies; and (3) a\nco-writing session that enables users to draft and refine their counterspeech\nwith CounterQuill. We conducted a within-subjects user study with 20\nparticipants to evaluate CounterQuill in comparison to ChatGPT. Results show\nthat CounterQuill's guidance and collaborative writing process provided users a\nstronger sense of ownership over their co-authored counterspeech. Users\nperceived CounterQuill as a writing partner and thus were more willing to post\nthe co-written counterspeech online compared to the one written with ChatGPT.", "AI": {"tldr": "CounterQuill, an AI-mediated system, assists users in composing effective counterspeech through learning, brainstorming, and co-writing, outperforming ChatGPT in user ownership and willingness to post.", "motivation": "Address the underexplored potential of user-driven counterspeech to combat online hate speech, overcoming challenges like fear of retaliation and skill barriers.", "method": "Introduces CounterQuill, a three-step AI-mediated process: learning, brainstorming, and co-writing sessions to compose counterspeech.", "result": "Users felt stronger ownership and were more willing to post counterspeech co-written with CounterQuill compared to ChatGPT.", "conclusion": "CounterQuill effectively supports users in creating and sharing counterspeech, offering a promising alternative to traditional moderation."}}
{"id": "2411.18673", "pdf": "https://arxiv.org/pdf/2411.18673", "abs": "https://arxiv.org/abs/2411.18673", "authors": ["Sherwin Bahmani", "Ivan Skorokhodov", "Guocheng Qian", "Aliaksandr Siarohin", "Willi Menapace", "Andrea Tagliasacchi", "David B. Lindell", "Sergey Tulyakov"], "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers", "categories": ["cs.CV"], "comment": "CVPR 2025; Project Page: https://snap-research.github.io/ac3d/", "summary": "Numerous works have recently integrated 3D camera control into foundational\ntext-to-video models, but the resulting camera control is often imprecise, and\nvideo generation quality suffers. In this work, we analyze camera motion from a\nfirst principles perspective, uncovering insights that enable precise 3D camera\nmanipulation without compromising synthesis quality. First, we determine that\nmotion induced by camera movements in videos is low-frequency in nature. This\nmotivates us to adjust train and test pose conditioning schedules, accelerating\ntraining convergence while improving visual and motion quality. Then, by\nprobing the representations of an unconditional video diffusion transformer, we\nobserve that they implicitly perform camera pose estimation under the hood, and\nonly a sub-portion of their layers contain the camera information. This\nsuggested us to limit the injection of camera conditioning to a subset of the\narchitecture to prevent interference with other video features, leading to a 4x\nreduction of training parameters, improved training speed, and 10% higher\nvisual quality. Finally, we complement the typical dataset for camera control\nlearning with a curated dataset of 20K diverse, dynamic videos with stationary\ncameras. This helps the model distinguish between camera and scene motion and\nimproves the dynamics of generated pose-conditioned videos. We compound these\nfindings to design the Advanced 3D Camera Control (AC3D) architecture, the new\nstate-of-the-art model for generative video modeling with camera control.", "AI": {"tldr": "The paper introduces AC3D, a state-of-the-art model for precise 3D camera control in text-to-video generation, improving training efficiency and visual quality by analyzing low-frequency motion, optimizing pose conditioning, and using a curated dataset.", "motivation": "Existing text-to-video models struggle with imprecise camera control and degraded video quality. The authors aim to address these issues by analyzing camera motion fundamentals and optimizing model architecture.", "method": "The study identifies low-frequency nature of camera-induced motion, optimizes pose conditioning schedules, limits camera conditioning to specific layers, and uses a curated dataset to distinguish camera and scene motion.", "result": "AC3D achieves a 4x reduction in training parameters, faster training, 10% higher visual quality, and improved dynamics in pose-conditioned videos.", "conclusion": "AC3D sets a new benchmark for generative video modeling with precise 3D camera control, combining insights from motion analysis, architecture optimization, and dataset curation."}}
{"id": "2409.05459", "pdf": "https://arxiv.org/pdf/2409.05459", "abs": "https://arxiv.org/abs/2409.05459", "authors": ["Melanie F. Pradier", "Javier Gonz\u00e1lez"], "title": "Beyond Flatland: A Geometric Take on Matching Methods for Treatment Effect Estimation", "categories": ["cs.LG"], "comment": null, "summary": "Matching is a popular approach in causal inference to estimate treatment\neffects by pairing treated and control units that are most similar in terms of\ntheir covariate information. However, classic matching methods completely\nignore the geometry of the data manifold, which is crucial to define a\nmeaningful distance for matching, and struggle when covariates are noisy and\nhigh-dimensional. In this work, we propose GeoMatching, a matching method to\nestimate treatment effects that takes into account the intrinsic data geometry\ninduced by existing causal mechanisms among the confounding variables. First,\nwe learn a low-dimensional, latent Riemannian manifold that accounts for\nuncertainty and geometry of the original input data. Second, we estimate\ntreatment effects via matching in the latent space based on the learned latent\nRiemannian metric. We provide theoretical insights and empirical results in\nsynthetic and real-world scenarios, demonstrating that GeoMatching yields more\neffective treatment effect estimators, even as we increase input\ndimensionality, in the presence of outliers, or in semi-supervised scenarios.", "AI": {"tldr": "GeoMatching improves causal inference by incorporating data geometry into matching, outperforming classic methods in high-dimensional or noisy scenarios.", "motivation": "Classic matching methods ignore data geometry, leading to poor performance with noisy, high-dimensional covariates.", "method": "GeoMatching learns a low-dimensional Riemannian manifold to capture data geometry and performs matching in this latent space.", "result": "GeoMatching provides more effective treatment effect estimators, especially in high-dimensional, outlier-prone, or semi-supervised settings.", "conclusion": "Incorporating data geometry via GeoMatching enhances causal inference accuracy and robustness."}}
{"id": "2410.08656", "pdf": "https://arxiv.org/pdf/2410.08656", "abs": "https://arxiv.org/abs/2410.08656", "authors": ["Yuanyuan Zhang", "Rui Yang", "Yutao Yue", "Eng Gee Lim"], "title": "radarODE-MTL: A Multi-Task Learning Framework with Eccentric Gradient Alignment for Robust Radar-Based ECG Reconstruction", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Millimeter-wave radar is promising to provide robust and accurate vital sign\nmonitoring in an unobtrusive manner. However, the radar signal might be\ndistorted in propagation by ambient noise or random body movement, ruining the\nsubtle cardiac activities and destroying the vital sign recovery. In\nparticular, the recovery of electrocardiogram (ECG) signal heavily relies on\nthe deep-learning model and is sensitive to noise. Therefore, this work\ncreatively deconstructs the radar-based ECG recovery into three individual\ntasks and proposes a multi-task learning (MTL) framework, radarODE-MTL, to\nincrease the robustness against consistent and abrupt noises. In addition, to\nalleviate the potential conflicts in optimizing individual tasks, a novel\nmulti-task optimization strategy, eccentric gradient alignment (EGA), is\nproposed to dynamically trim the task-specific gradients based on task\ndifficulties in orthogonal space. The proposed radarODE-MTL with EGA is\nevaluated on the public dataset with prominent improvements in accuracy, and\nthe performance remains consistent under noises. The experimental results\nindicate that radarODE-MTL could reconstruct accurate ECG signals robustly from\nradar signals and imply the application prospect in real-life situations. The\ncode is available at: http://github.com/ZYY0844/radarODE-MTL.", "AI": {"tldr": "The paper proposes radarODE-MTL, a multi-task learning framework, to robustly recover ECG signals from noisy radar data using a novel optimization strategy called eccentric gradient alignment (EGA).", "motivation": "Millimeter-wave radar signals for vital sign monitoring are prone to distortion from noise and body movements, making ECG recovery challenging.", "method": "Deconstructs ECG recovery into three tasks, uses radarODE-MTL with EGA to optimize gradients dynamically based on task difficulty.", "result": "Improved accuracy and robustness in ECG signal recovery under noise, validated on a public dataset.", "conclusion": "radarODE-MTL shows promise for real-life ECG monitoring applications."}}
{"id": "2412.04280", "pdf": "https://arxiv.org/pdf/2412.04280", "abs": "https://arxiv.org/abs/2412.04280", "authors": ["Jinbin Bai", "Wei Chow", "Ling Yang", "Xiangtai Li", "Juncheng Li", "Hanwang Zhang", "Shuicheng Yan"], "title": "HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing", "categories": ["cs.CV", "cs.GR"], "comment": "Accepted to CVPR 2025 AI for Content Creation (AI4CC) Workshop. Codes\n  and Supplementary Material: https://github.com/viiika/HumanEdit", "summary": "We present HumanEdit, a high-quality, human-rewarded dataset specifically\ndesigned for instruction-guided image editing, enabling precise and diverse\nimage manipulations through open-form language instructions. Previous\nlarge-scale editing datasets often incorporate minimal human feedback, leading\nto challenges in aligning datasets with human preferences. HumanEdit bridges\nthis gap by employing human annotators to construct data pairs and\nadministrators to provide feedback. With meticulously curation, HumanEdit\ncomprises 5,751 images and requires more than 2,500 hours of human effort\nacross four stages, ensuring both accuracy and reliability for a wide range of\nimage editing tasks. The dataset includes six distinct types of editing\ninstructions: Action, Add, Counting, Relation, Remove, and Replace,\nencompassing a broad spectrum of real-world scenarios. All images in the\ndataset are accompanied by masks, and for a subset of the data, we ensure that\nthe instructions are sufficiently detailed to support mask-free editing.\nFurthermore, HumanEdit offers comprehensive diversity and high-resolution $1024\n\\times 1024$ content sourced from various domains, setting a new versatile\nbenchmark for instructional image editing datasets. With the aim of advancing\nfuture research and establishing evaluation benchmarks in the field of image\nediting, we release HumanEdit at\nhttps://huggingface.co/datasets/BryanW/HumanEdit.", "AI": {"tldr": "HumanEdit is a high-quality dataset for instruction-guided image editing, featuring human feedback, diverse editing types, and high-resolution images.", "motivation": "Address the gap in aligning image editing datasets with human preferences by incorporating human feedback and diverse instructions.", "method": "Constructed using human annotators and administrators, involving 5,751 images and 2,500+ hours of effort across four stages. Includes six editing types and detailed instructions.", "result": "A versatile benchmark dataset with masks and detailed instructions, supporting diverse image editing tasks.", "conclusion": "HumanEdit advances research in instructional image editing and is publicly available for future benchmarks."}}
{"id": "2409.08474", "pdf": "https://arxiv.org/pdf/2409.08474", "abs": "https://arxiv.org/abs/2409.08474", "authors": ["Jingyao Wang", "Wenwen Qiang", "Changwen Zheng", "Hui Xiong", "Gang Hua"], "title": "Rethinking Meta-Learning from a Learning Lens", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Meta-learning seeks to learn a well-generalized model initialization from\ntraining tasks to solve unseen tasks. From the \"learning to learn\" perspective,\nthe quality of the initialization is modeled with one-step gradient decent in\nthe inner loop. However, contrary to theoretical expectations, our empirical\nanalysis reveals that this may expose meta-learning to underfitting. To bridge\nthe gap between theoretical understanding and practical implementation, we\nreconsider meta-learning from the \"Learning\" lens. We propose that the\nmeta-learning model comprises two interrelated components: parameters for model\ninitialization and a meta-layer for task-specific fine-tuning. These components\nwill lead to the risks of overfitting and underfitting depending on tasks, and\ntheir solutions, fewer parameters vs. more meta-layer, are often in conflict.\nTo address this, we aim to regulate the task information the model receives\nwithout modifying the data or model structure. Our theoretical analysis\nindicates that models adapted to different tasks can mutually reinforce each\nother, highlighting the effective information. Based on this insight, we\npropose TRLearner, a plug-and-play method that leverages task relation to\ncalibrate meta-learning. It first extracts task relation matrices and then\napplies relation-aware consistency regularization to guide optimization.\nExtensive theoretical and empirical evaluations demonstrate its effectiveness.", "AI": {"tldr": "The paper addresses underfitting in meta-learning by proposing TRLearner, a method that uses task relations to calibrate learning without altering data or model structure.", "motivation": "Empirical analysis shows meta-learning may underfit despite theoretical expectations, prompting a rethink from a 'Learning' perspective.", "method": "The paper introduces TRLearner, which extracts task relation matrices and applies relation-aware consistency regularization.", "result": "Theoretical and empirical evaluations confirm TRLearner's effectiveness in improving meta-learning.", "conclusion": "TRLearner successfully bridges the gap between theory and practice in meta-learning by leveraging task relations."}}
{"id": "2410.22367", "pdf": "https://arxiv.org/pdf/2410.22367", "abs": "https://arxiv.org/abs/2410.22367", "authors": ["Yoel Shoshan", "Moshiko Raboh", "Michal Ozery-Flato", "Vadim Ratner", "Alex Golts", "Jeffrey K. Weber", "Ella Barkan", "Simona Rabinovici-Cohen", "Sagi Polaczek", "Ido Amos", "Ben Shapira", "Liam Hazan", "Matan Ninio", "Sivan Ravid", "Michael M. Danziger", "Yosi Shamay", "Sharon Kurant", "Joseph A. Morrone", "Parthasarathy Suryanarayanan", "Michal Rosen-Zvi", "Efrat Hexter"], "title": "MAMMAL -- Molecular Aligned Multi-Modal Architecture and Language", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models applied to vast biological datasets have the potential\nto transform biology by uncovering disease mechanisms and accelerating drug\ndevelopment. However, current models are often siloed, trained separately on\nsmall-molecules, proteins, or transcriptomic data, limiting their ability to\ncapture complex, multi-modal interactions. Effective drug discovery requires\ncomputational tools that integrate multiple biological entities while\nsupporting prediction and generation, a challenge existing models struggle to\naddress. For this purpose, we present MAMMAL - Molecular Aligned Multi-Modal\nArchitecture and Language - a versatile method applied to create a multi-task\nfoundation model that learns from large-scale biological datasets across\ndiverse modalities, including proteins, small-molecules, and omics. MAMMAL's\nstructured prompt syntax supports classification, regression, and generation\ntasks while handling token and scalar inputs and outputs. Evaluated on eleven\ndiverse downstream tasks, it reaches a new state of the art (SOTA) in nine\ntasks and is comparable to SOTA in two tasks, all within a unified\narchitecture, unlike prior task-specific models. Additionally, we explored\nAlphafold 3 binding prediction capabilities on antibody-antigen and\nnanobody-antigen complexes showing significantly better classification\nperformance of MAMMAL in 3 out of 4 targets. The model code and pretrained\nweights are publicly available at\nhttps://github.com/BiomedSciAI/biomed-multi-alignment and\nhttps://huggingface.co/ibm/biomed.omics.bl.sm.ma-ted-458m", "AI": {"tldr": "MAMMAL is a multi-modal foundation model for biology, integrating diverse data types (proteins, small-molecules, omics) to outperform task-specific models in 9/11 tasks.", "motivation": "Current models are siloed, limiting their ability to capture multi-modal interactions in biology, which is crucial for drug discovery.", "method": "MAMMAL uses a structured prompt syntax to handle classification, regression, and generation tasks across proteins, small-molecules, and omics data.", "result": "Achieves SOTA in 9/11 tasks, with comparable performance in 2, and outperforms Alphafold 3 in binding prediction for 3/4 targets.", "conclusion": "MAMMAL's unified architecture advances multi-modal biological modeling, offering a powerful tool for drug discovery and disease research."}}
{"id": "2501.13620", "pdf": "https://arxiv.org/pdf/2501.13620", "abs": "https://arxiv.org/abs/2501.13620", "authors": ["Mohit Vaishnav", "Tanel Tammet"], "title": "A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A fundamental challenge in artificial intelligence involves understanding the\ncognitive mechanisms underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images or\nrequiring fine-grained compositional understanding? Drawing inspiration from\ncognitive science, this paper introduces a structured evaluation framework\nusing diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to\ndissect the perception-reasoning interface in VLMs. We propose three distinct\nevaluation paradigms, mirroring human problem-solving strategies: Direct Visual\nRule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule\nextraction and application), and Componential Analysis (CA; analytical\ndecomposition via task-agnostic textual descriptions). These paradigms\nsystematically vary cognitive load and probe processing stages. Notably, CA\nenables multi-image reasoning evaluation even for single-image architectures\nand isolates reasoning from perception by operating on textual descriptions.\nApplying this framework, we demonstrate that CA, leveraging powerful language\nmodels for reasoning over rich, independently generated descriptions, achieves\nnew state-of-the-art (SOTA) performance on challenging benchmarks including\nBongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm\nreasoning improves significantly when perceptual challenges are mitigated,\nrevealing a critical perception bottleneck. Our framework provides a valuable\ndiagnostic tool and suggests that decoupling perception (via rich,\ntask-agnostic description) from reasoning is a promising direction for robust\nand general visual intelligence.", "AI": {"tldr": "The paper introduces a structured evaluation framework for Vision-Language Models (VLMs) using cognitive science-inspired tasks to dissect visual reasoning. It proposes three paradigms (DVRL, DRL, CA) to evaluate reasoning, with CA achieving SOTA performance by decoupling perception from reasoning.", "motivation": "To understand how VLMs integrate visual perception with abstract thought and improve their reasoning capabilities by addressing perceptual bottlenecks.", "method": "Three evaluation paradigms (DVRL, DRL, CA) are introduced, varying cognitive load and processing stages. CA uses task-agnostic textual descriptions to isolate reasoning from perception.", "result": "CA achieves SOTA performance on benchmarks like Bongard-OpenWorld and Winoground, showing reasoning improves when perceptual challenges are mitigated.", "conclusion": "Decoupling perception from reasoning via rich descriptions is a promising direction for robust visual intelligence, and the framework serves as a diagnostic tool."}}
{"id": "2410.04883", "pdf": "https://arxiv.org/pdf/2410.04883", "abs": "https://arxiv.org/abs/2410.04883", "authors": ["Lars Henry Berge Olsen", "Martin Jullum"], "title": "Improving the Weighting Strategy in KernelSHAP", "categories": ["cs.LG", "stat.ML"], "comment": "This is the accepted, post peer-reviewed version of the manuscript,\n  accepted for publication in the proceedings after the Third World Conference\n  on eXplainable Artificial Intelligence, XAI-2025. A link to the version of\n  record will be included here upon publication", "summary": "In Explainable AI (XAI), Shapley values are a popular model-agnostic\nframework for explaining predictions made by complex machine learning models.\nThe computation of Shapley values requires estimating non-trivial contribution\nfunctions representing predictions with only a subset of the features present.\nAs the number of these terms grows exponentially with the number of features,\ncomputational costs escalate rapidly, creating a pressing need for efficient\nand accurate approximation methods. For tabular data, the KernelSHAP framework\nis considered the state-of-the-art model-agnostic approximation framework.\nKernelSHAP approximates the Shapley values using a weighted sample of the\ncontribution functions for different feature subsets. We propose a novel\nmodification of KernelSHAP which replaces the stochastic weights with\ndeterministic ones to reduce the variance of the resulting Shapley value\napproximations. This may also be combined with our simple, yet effective\nmodification to the KernelSHAP variant implemented in the popular Python\nlibrary SHAP. Additionally, we provide an overview of established methods.\nNumerical experiments demonstrate that our methods can reduce the required\nnumber of contribution function evaluations by $5\\%$ to $50\\%$ while preserving\nthe same accuracy of the approximated Shapley values -- essentially reducing\nthe running time by up to $50\\%$. These computational advancements push the\nboundaries of the feature dimensionality and number of predictions that can be\naccurately explained with Shapley values within a feasible runtime.", "AI": {"tldr": "Proposes deterministic weights for KernelSHAP to reduce variance and computational costs, achieving up to 50% faster runtime with the same accuracy.", "motivation": "The exponential growth of computational costs in Shapley value estimation for XAI necessitates efficient approximation methods.", "method": "Modifies KernelSHAP by replacing stochastic weights with deterministic ones and introduces a simple yet effective adjustment to the SHAP library implementation.", "result": "Reduces required evaluations by 5%-50% while maintaining accuracy, cutting runtime by up to 50%.", "conclusion": "Enhances feasibility of Shapley value explanations for high-dimensional data and large-scale predictions."}}
{"id": "2501.15627", "pdf": "https://arxiv.org/pdf/2501.15627", "abs": "https://arxiv.org/abs/2501.15627", "authors": ["Tidor-Vlad Pricope"], "title": "HardML: A Benchmark For Evaluating Data Science And Machine Learning knowledge and reasoning in AI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present HardML, a benchmark designed to evaluate the knowledge and\nreasoning abilities in the fields of data science and machine learning. HardML\ncomprises a diverse set of 100 challenging multiple-choice questions,\nhandcrafted over a period of 6 months, covering the most popular and modern\nbranches of data science and machine learning. These questions are challenging\neven for a typical Senior Machine Learning Engineer to answer correctly. To\nminimize the risk of data contamination, HardML uses mostly original content\ndevised by the author. Current state of the art AI models achieve a 30% error\nrate on this benchmark, which is about 3 times larger than the one achieved on\nthe equivalent, well known MMLU ML. While HardML is limited in scope and not\naiming to push the frontier, primarily due to its multiple choice nature, it\nserves as a rigorous and modern testbed to quantify and track the progress of\ntop AI. While plenty benchmarks and experimentation in LLM evaluation exist in\nother STEM fields like mathematics, physics and chemistry, the subfields of\ndata science and machine learning remain fairly underexplored.", "AI": {"tldr": "HardML is a benchmark with 100 challenging multiple-choice questions to evaluate knowledge and reasoning in data science and ML, showing current AI models' limitations.", "motivation": "To address the lack of rigorous benchmarks in data science and ML, HardML provides a modern testbed to track AI progress.", "method": "HardML includes 100 original, handcrafted questions over 6 months, designed to be challenging even for senior ML engineers.", "result": "Current AI models achieve a 30% error rate on HardML, significantly higher than on MMLU ML.", "conclusion": "HardML serves as a valuable, though limited, benchmark to quantify AI progress in data science and ML."}}
{"id": "2501.15326", "pdf": "https://arxiv.org/pdf/2501.15326", "abs": "https://arxiv.org/abs/2501.15326", "authors": ["Jiajie Li", "Brian R Quaranto", "Chenhui Xu", "Ishan Mishra", "Ruiyang Qin", "Dancheng Liu", "Peter C W Kim", "Jinjun Xiong"], "title": "Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data", "categories": ["cs.CV"], "comment": null, "summary": "We present RASO, a foundation model designed to Recognize Any Surgical\nObject, offering robust open-set recognition capabilities across a broad range\nof surgical procedures and object classes, in both surgical images and videos.\nRASO leverages a novel weakly-supervised learning framework that generates\ntag-image-text pairs automatically from large-scale unannotated surgical\nlecture videos, significantly reducing the need for manual annotations. Our\nscalable data generation pipeline gathers 2,200 surgical procedures and\nproduces 3.6 million tag annotations across 2,066 unique surgical tags. Our\nexperiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP,\nand 7.2 mAP on four standard surgical benchmarks, respectively, in zero-shot\nsettings, and surpasses state-of-the-art models in supervised surgical action\nrecognition tasks. Code, model, and demo are available at\nhttps://ntlm1686.github.io/raso.", "AI": {"tldr": "RASO is a foundation model for recognizing surgical objects in images and videos, using weakly-supervised learning to reduce manual annotations. It outperforms benchmarks in zero-shot and supervised tasks.", "motivation": "To address the challenge of recognizing diverse surgical objects across procedures without extensive manual annotation.", "method": "Uses a weakly-supervised learning framework to generate tag-image-text pairs from unannotated surgical lecture videos, creating a scalable data pipeline.", "result": "Achieves significant improvements (2.9-10.6 mAP) on surgical benchmarks and surpasses state-of-the-art models.", "conclusion": "RASO demonstrates robust open-set recognition capabilities and scalability, with publicly available resources."}}
{"id": "2410.12289", "pdf": "https://arxiv.org/pdf/2410.12289", "abs": "https://arxiv.org/abs/2410.12289", "authors": ["Nir Shlezinger", "Guy Revach", "Anubhab Ghosh", "Saikat Chatterjee", "Shuo Tang", "Tales Imbiriba", "Jindrich Dunik", "Ondrej Straka", "Pau Closas", "Yonina C. Eldar"], "title": "AI-Aided Kalman Filters", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": "Submitted to the IEEE Signal Processing Magazine", "summary": "The Kalman filter (KF) and its variants are among the most celebrated\nalgorithms in signal processing. These methods are used for state estimation of\ndynamic systems by relying on mathematical representations in the form of\nsimple state-space (SS) models, which may be crude and inaccurate descriptions\nof the underlying dynamics. Emerging data-centric artificial intelligence (AI)\ntechniques tackle these tasks using deep neural networks (DNNs), which are\nmodel-agnostic. Recent developments illustrate the possibility of fusing DNNs\nwith classic Kalman-type filtering, obtaining systems that learn to track in\npartially known dynamics. This article provides a tutorial-style overview of\ndesign approaches for incorporating AI in aiding KF-type algorithms. We review\nboth generic and dedicated DNN architectures suitable for state estimation, and\nprovide a systematic presentation of techniques for fusing AI tools with KFs\nand for leveraging partial SS modeling and data, categorizing design approaches\ninto task-oriented and SS model-oriented. The usefulness of each approach in\npreserving the individual strengths of model-based KFs and data-driven DNNs is\ninvestigated in a qualitative and quantitative study, whose code is publicly\navailable, illustrating the gains of hybrid model-based/data-driven designs. We\nalso discuss existing challenges and future research directions that arise from\nfusing AI and Kalman-type algorithms.", "AI": {"tldr": "The paper explores hybrid approaches combining Kalman filters (KFs) with deep neural networks (DNNs) for state estimation, reviewing design methods and evaluating their effectiveness.", "motivation": "Traditional KFs rely on simplified state-space models, which may be inaccurate. Data-driven DNNs offer model-agnostic solutions, motivating the fusion of both to leverage their strengths.", "method": "The paper reviews generic and dedicated DNN architectures for state estimation and systematically presents techniques for integrating AI with KFs, categorized into task-oriented and model-oriented approaches.", "result": "Hybrid designs preserve the strengths of model-based KFs and data-driven DNNs, showing qualitative and quantitative gains, as demonstrated in a publicly available study.", "conclusion": "The fusion of AI and KFs is promising but faces challenges; future research directions are discussed to advance hybrid model-based/data-driven designs."}}
{"id": "2502.00047", "pdf": "https://arxiv.org/pdf/2502.00047", "abs": "https://arxiv.org/abs/2502.00047", "authors": ["Armand Foucault", "Franck Mamalet", "Fran\u00e7ois Malgouyres"], "title": "HadamRNN: Binary and Sparse Ternary Orthogonal RNNs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Binary and sparse ternary weights in neural networks enable faster\ncomputations and lighter representations, facilitating their use on edge\ndevices with limited computational power. Meanwhile, vanilla RNNs are highly\nsensitive to changes in their recurrent weights, making the binarization and\nternarization of these weights inherently challenging. To date, no method has\nsuccessfully achieved binarization or ternarization of vanilla RNN weights. We\npresent a new approach leveraging the properties of Hadamard matrices to\nparameterize a subset of binary and sparse ternary orthogonal matrices. This\nmethod enables the training of orthogonal RNNs (ORNNs) with binary and sparse\nternary recurrent weights, effectively creating a specific class of binary and\nsparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and\nBlock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and\nsequential MNIST tasks, the IMDB dataset, two GLUE benchmarks, and two IoT\nbenchmarks. Despite binarization or sparse ternarization, these RNNs maintain\nperformance levels comparable to state-of-the-art full-precision models,\nhighlighting the effectiveness of our approach. Notably, our approach is the\nfirst solution with binary recurrent weights capable of tackling the copy task\nover 1000 timesteps.", "AI": {"tldr": "A novel method uses Hadamard matrices to train orthogonal RNNs with binary/ternary weights, achieving performance comparable to full-precision models.", "motivation": "Binary/ternary weights enable faster computations for edge devices, but vanilla RNNs are sensitive to weight changes, making binarization challenging.", "method": "Leveraging Hadamard matrices to parameterize binary/ternary orthogonal matrices, enabling training of orthogonal RNNs (ORNNs).", "result": "ORNNs (HadamRNN, Block-HadamRNN) match full-precision models on benchmarks, including the copy task over 1000 timesteps.", "conclusion": "The approach successfully binarizes/ternarizes RNN weights, maintaining performance and enabling edge deployment."}}
{"id": "2501.18630", "pdf": "https://arxiv.org/pdf/2501.18630", "abs": "https://arxiv.org/abs/2501.18630", "authors": ["Rong Liu", "Dylan Sun", "Meida Chen", "Yue Wang", "Andrew Feng"], "title": "Deformable Beta Splatting", "categories": ["cs.CV", "cs.GR"], "comment": "SIGGRAPH 2025", "summary": "3D Gaussian Splatting (3DGS) has advanced radiance field reconstruction by\nenabling real-time rendering. However, its reliance on Gaussian kernels for\ngeometry and low-order Spherical Harmonics (SH) for color encoding limits its\nability to capture complex geometries and diverse colors. We introduce\nDeformable Beta Splatting (DBS), a deformable and compact approach that\nenhances both geometry and color representation. DBS replaces Gaussian kernels\nwith deformable Beta Kernels, which offer bounded support and adaptive\nfrequency control to capture fine geometric details with higher fidelity while\nachieving better memory efficiency. In addition, we extended the Beta Kernel to\ncolor encoding, which facilitates improved representation of diffuse and\nspecular components, yielding superior results compared to SH-based methods.\nFurthermore, Unlike prior densification techniques that depend on Gaussian\nproperties, we mathematically prove that adjusting regularized opacity alone\nensures distribution-preserved Markov chain Monte Carlo (MCMC), independent of\nthe splatting kernel type. Experimental results demonstrate that DBS achieves\nstate-of-the-art visual quality while utilizing only 45% of the parameters and\nrendering 1.5x faster than 3DGS-MCMC, highlighting the superior performance of\nDBS for real-time radiance field rendering. Interactive demonstrations and\nsource code are available on our project website:\nhttps://rongliu-leo.github.io/beta-splatting/.", "AI": {"tldr": "DBS improves 3DGS by using deformable Beta Kernels for better geometry and color representation, achieving higher fidelity and efficiency.", "motivation": "3DGS's limitations in capturing complex geometries and diverse colors due to Gaussian kernels and low-order SH motivate the development of DBS.", "method": "DBS replaces Gaussian kernels with deformable Beta Kernels for geometry and extends them to color encoding, enhancing detail and efficiency.", "result": "DBS achieves superior visual quality with 45% fewer parameters and 1.5x faster rendering than 3DGS-MCMC.", "conclusion": "DBS outperforms 3DGS in real-time radiance field rendering, offering better performance and efficiency."}}
{"id": "2410.12938", "pdf": "https://arxiv.org/pdf/2410.12938", "abs": "https://arxiv.org/abs/2410.12938", "authors": ["Qidong Yang", "Jonathan Giezendanner", "Daniel Salles Civitarese", "Johannes Jakubik", "Eric Schmitt", "Anirban Chandra", "Jeremy Vila", "Detlef Hohl", "Chris Hill", "Campbell Watson", "Sherrie Wang"], "title": "Local Off-Grid Weather Forecasting with Multi-Modal Earth Observation Data", "categories": ["cs.LG", "physics.ao-ph"], "comment": null, "summary": "Urgent applications like wildfire management and renewable energy generation\nrequire precise, localized weather forecasts near the Earth's surface. However,\nforecasts produced by machine learning models or numerical weather prediction\nsystems are typically generated on large-scale regular grids, where direct\ndownscaling fails to capture fine-grained, near-surface weather patterns. In\nthis work, we propose a multi-modal transformer model trained end-to-end to\ndownscale gridded forecasts to off-grid locations of interest. Our model\ndirectly combines local historical weather observations (e.g., wind,\ntemperature, dewpoint) with gridded forecasts to produce locally accurate\npredictions at various lead times. Multiple data modalities are collected and\nconcatenated at station-level locations, treated as a token at each station.\nUsing self-attention, the token corresponding to the target location aggregates\ninformation from its neighboring tokens. Experiments using weather stations\nacross the Northeastern United States show that our model outperforms a range\nof data-driven and non-data-driven off-grid forecasting methods. They also\nreveal that direct input of station data provides a phase shift in local\nweather forecasting accuracy, reducing the prediction error by up to 80%\ncompared to pure gridded data based models. This approach demonstrates how to\nbridge the gap between large-scale weather models and locally accurate\nforecasts to support high-stakes, location-sensitive decision-making.", "AI": {"tldr": "A multi-modal transformer model is proposed to downscale gridded weather forecasts to off-grid locations, combining local observations and gridded data for accurate, localized predictions.", "motivation": "Precise, localized weather forecasts are crucial for urgent applications like wildfire management and renewable energy, but existing methods fail to capture fine-grained patterns.", "method": "A multi-modal transformer model is trained end-to-end to combine local historical weather observations with gridded forecasts, using self-attention to aggregate information from neighboring stations.", "result": "The model outperforms other methods, reducing prediction error by up to 80% compared to gridded-data-only models.", "conclusion": "This approach bridges the gap between large-scale weather models and locally accurate forecasts, aiding high-stakes decision-making."}}
{"id": "2502.01800", "pdf": "https://arxiv.org/pdf/2502.01800", "abs": "https://arxiv.org/abs/2502.01800", "authors": ["Aidan Curtis", "Eric Li", "Michael Noseworthy", "Nishad Gothoskar", "Sachin Chitta", "Hui Li", "Leslie Pack Kaelbling", "Nicole Carey"], "title": "Flow-based Domain Randomization for Learning and Sequencing Robotic Skills", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Domain randomization in reinforcement learning is an established technique\nfor increasing the robustness of control policies trained in simulation. By\nrandomizing environment properties during training, the learned policy can\nbecome robust to uncertainties along the randomized dimensions. While the\nenvironment distribution is typically specified by hand, in this paper we\ninvestigate automatically discovering a sampling distribution via\nentropy-regularized reward maximization of a normalizing-flow-based neural\nsampling distribution. We show that this architecture is more flexible and\nprovides greater robustness than existing approaches that learn simpler,\nparameterized sampling distributions, as demonstrated in six simulated and one\nreal-world robotics domain. Lastly, we explore how these learned sampling\ndistributions, combined with a privileged value function, can be used for\nout-of-distribution detection in an uncertainty-aware multi-step manipulation\nplanner.", "AI": {"tldr": "The paper proposes an automated method for discovering sampling distributions in domain randomization for reinforcement learning, using entropy-regularized reward maximization with a normalizing-flow-based neural sampling distribution. It demonstrates improved robustness and flexibility over existing methods and explores applications in out-of-distribution detection.", "motivation": "Domain randomization enhances policy robustness, but manual specification of environment distributions is limiting. Automating this process could improve adaptability and performance.", "method": "The approach uses entropy-regularized reward maximization with a normalizing-flow-based neural sampling distribution to automatically discover sampling distributions.", "result": "The method outperforms simpler, parameterized sampling distributions in six simulated and one real-world robotics domain, showing greater robustness.", "conclusion": "The learned sampling distributions, combined with a privileged value function, enable effective out-of-distribution detection and uncertainty-aware planning, advancing domain randomization techniques."}}
{"id": "2502.07758", "pdf": "https://arxiv.org/pdf/2502.07758", "abs": "https://arxiv.org/abs/2502.07758", "authors": ["Nektarios A. Valous", "Eckhard Hitzer", "Drago\u015f Du\u015fe", "Rodrigo Rojas Moraleda", "Ferdinand Popp", "Meggy Suarez-Carmona", "Anna Berthel", "Ismini Papageorgiou", "Carlo Fremd", "Alexander R\u00f6lle", "Christina C. Westhoff", "B\u00e9n\u00e9dicte Lenoir", "Niels Halama", "Inka Z\u00f6rnig", "Dirk J\u00e4ger"], "title": "Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 18 figures, 14 tables", "summary": "Hypercomplex image processing extends conventional techniques in a unified\nparadigm encompassing algebraic and geometric principles. This work leverages\nquaternions and the two-dimensional orthogonal planes split framework\n(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D\nplanes) for natural/biomedical image analysis through the following\ncomputational workflows and outcomes: natural/biomedical image re-colorization,\nnatural image de-colorization, natural/biomedical image contrast enhancement,\ncomputational re-staining and stain separation in histological images, and\nperformance gains in machine/deep learning pipelines for histological images.\nThe workflows are analyzed separately for natural and biomedical images to\nshowcase the effectiveness of the proposed approaches. The proposed workflows\ncan regulate color appearance (e.g. with alternative renditions and grayscale\nconversion) and image contrast, be part of automated image processing pipelines\n(e.g. isolating stain components, boosting learning models), and assist in\ndigital pathology applications (e.g. enhancing biomarker visibility, enabling\ncolorblind-friendly renditions). Employing only basic arithmetic and matrix\noperations, this work offers a computationally accessible methodology - in the\nhypercomplex domain - that showcases versatility and consistency across image\nprocessing tasks and a range of computer vision and biomedical applications.\nThe proposed non-data-driven methods achieve comparable or better results\n(particularly in cases involving well-known methods) to those reported in the\nliterature, showcasing the potential of robust theoretical frameworks with\npractical effectiveness. Results, methods, and limitations are detailed\nalongside discussion of promising extensions, emphasizing the potential of\nfeature-rich mathematical/computational frameworks for natural and biomedical\nimages.", "AI": {"tldr": "The paper introduces hypercomplex image processing using quaternions and orthogonal planes split for tasks like re-colorization, de-colorization, and contrast enhancement in natural/biomedical images, achieving comparable or better results than existing methods.", "motivation": "To extend conventional image processing techniques by unifying algebraic and geometric principles in the hypercomplex domain for versatile applications in natural and biomedical imaging.", "method": "Leverages quaternions and the two-dimensional orthogonal planes split framework for tasks like re-colorization, de-colorization, and contrast enhancement, using basic arithmetic and matrix operations.", "result": "Achieves comparable or better results than existing methods, particularly in well-known tasks, and demonstrates versatility across image processing and biomedical applications.", "conclusion": "The work highlights the potential of robust theoretical frameworks in hypercomplex domains for practical, computationally accessible solutions in image processing and biomedical applications."}}
{"id": "2410.14081", "pdf": "https://arxiv.org/pdf/2410.14081", "abs": "https://arxiv.org/abs/2410.14081", "authors": ["Shangzhe Li", "Zhiao Huang", "Hao Su"], "title": "Reward-free World Models for Online Imitation Learning", "categories": ["cs.LG"], "comment": null, "summary": "Imitation learning (IL) enables agents to acquire skills directly from expert\ndemonstrations, providing a compelling alternative to reinforcement learning.\nHowever, prior online IL approaches struggle with complex tasks characterized\nby high-dimensional inputs and complex dynamics. In this work, we propose a\nnovel approach to online imitation learning that leverages reward-free world\nmodels. Our method learns environmental dynamics entirely in latent spaces\nwithout reconstruction, enabling efficient and accurate modeling. We adopt the\ninverse soft-Q learning objective, reformulating the optimization process in\nthe Q-policy space to mitigate the instability associated with traditional\noptimization in the reward-policy space. By employing a learned latent dynamics\nmodel and planning for control, our approach consistently achieves stable,\nexpert-level performance in tasks with high-dimensional observation or action\nspaces and intricate dynamics. We evaluate our method on a diverse set of\nbenchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating\nsuperior empirical performance compared to existing approaches.", "AI": {"tldr": "A novel online imitation learning method using reward-free world models in latent spaces achieves expert-level performance in complex tasks.", "motivation": "Addressing the limitations of prior online IL approaches in handling high-dimensional inputs and complex dynamics.", "method": "Leverages latent space dynamics modeling and inverse soft-Q learning for stable optimization.", "result": "Demonstrates superior performance on benchmarks like DMControl, MyoSuite, and ManiSkill2.", "conclusion": "The proposed method effectively handles complex tasks, outperforming existing approaches."}}
{"id": "2502.14131", "pdf": "https://arxiv.org/pdf/2502.14131", "abs": "https://arxiv.org/abs/2502.14131", "authors": ["Enoch H. Kang", "Hema Yoganarasimhan", "Lalit Jain"], "title": "An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model", "categories": ["cs.LG", "cs.AI", "econ.EM"], "comment": null, "summary": "We study the problem of estimating Dynamic Discrete Choice (DDC) models, also\nknown as offline Maximum Entropy-Regularized Inverse Reinforcement Learning\n(offline MaxEnt-IRL) in machine learning. The objective is to recover reward or\n$Q^*$ functions that govern agent behavior from offline behavior data. In this\npaper, we propose a globally convergent gradient-based method for solving these\nproblems without the restrictive assumption of linearly parameterized rewards.\nThe novelty of our approach lies in introducing the Empirical Risk Minimization\n(ERM) based IRL/DDC framework, which circumvents the need for explicit state\ntransition probability estimation in the Bellman equation. Furthermore, our\nmethod is compatible with non-parametric estimation techniques such as neural\nnetworks. Therefore, the proposed method has the potential to be scaled to\nhigh-dimensional, infinite state spaces. A key theoretical insight underlying\nour approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL)\ncondition -- a property that, while weaker than strong convexity, is sufficient\nto ensure fast global convergence guarantees. Through a series of synthetic\nexperiments, we demonstrate that our approach consistently outperforms\nbenchmark methods and state-of-the-art alternatives.", "AI": {"tldr": "A gradient-based method for estimating Dynamic Discrete Choice models without linear reward assumptions, using ERM-based IRL/DDC framework and non-parametric techniques like neural networks, ensuring fast global convergence via the PL condition.", "motivation": "To recover reward or Q* functions from offline behavior data without restrictive assumptions, enabling scalability to high-dimensional spaces.", "method": "Proposes a globally convergent gradient-based method using ERM-based IRL/DDC framework, avoiding explicit state transition probability estimation.", "result": "Outperforms benchmark methods in synthetic experiments, demonstrating scalability and efficiency.", "conclusion": "The method offers a scalable, efficient solution for high-dimensional DDC/MaxEnt-IRL problems with theoretical guarantees."}}
{"id": "2502.09608", "pdf": "https://arxiv.org/pdf/2502.09608", "abs": "https://arxiv.org/abs/2502.09608", "authors": ["Mia Tang", "Yael Vinker", "Chuan Yan", "Lvmin Zhang", "Maneesh Agrawala"], "title": "Instance Segmentation of Scene Sketches Using Natural Image Priors", "categories": ["cs.CV", "cs.GR"], "comment": "Project website: https://inklayer.github.io", "summary": "Sketch segmentation involves grouping pixels within a sketch that belong to\nthe same object or instance. It serves as a valuable tool for sketch editing\ntasks, such as moving, scaling, or removing specific components. While image\nsegmentation models have demonstrated remarkable capabilities in recent years,\nsketches present unique challenges for these models due to their sparse nature\nand wide variation in styles. We introduce InkLayer, a method for instance\nsegmentation of raster scene sketches. Our approach adapts state-of-the-art\nimage segmentation and object detection models to the sketch domain by\nemploying class-agnostic fine-tuning and refining segmentation masks using\ndepth cues. Furthermore, our method organizes sketches into sorted layers,\nwhere occluded instances are inpainted, enabling advanced sketch editing\napplications. As existing datasets in this domain lack variation in sketch\nstyles, we construct a synthetic scene sketch segmentation dataset, InkScenes,\nfeaturing sketches with diverse brush strokes and varying levels of detail. We\nuse this dataset to demonstrate the robustness of our approach.", "AI": {"tldr": "InkLayer is a method for instance segmentation of sketches, adapting image segmentation models to handle sparse and varied sketch styles, using depth cues and synthetic data for robustness.", "motivation": "Sketches pose unique challenges for segmentation due to their sparsity and style variations, requiring specialized tools for editing tasks.", "method": "Adapts state-of-the-art image segmentation models with class-agnostic fine-tuning, refines masks using depth cues, and organizes sketches into sorted layers with inpainting.", "result": "Demonstrated robustness on InkScenes, a synthetic dataset with diverse sketch styles, enabling advanced sketch editing.", "conclusion": "InkLayer effectively addresses sketch segmentation challenges, offering a practical solution for sketch editing applications."}}
{"id": "2410.23858", "pdf": "https://arxiv.org/pdf/2410.23858", "abs": "https://arxiv.org/abs/2410.23858", "authors": ["Kentaro Hino", "Yuki Kurashige"], "title": "Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable Machine Learning Potential", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "physics.chem-ph", "quant-ph"], "comment": "36 pages, 11 figures", "summary": "A neural network-based machine learning potential energy surface (PES)\nexpressed in a matrix product operator (NN-MPO) is proposed. The MPO form\nenables efficient evaluation of high-dimensional integrals that arise in\nsolving the time-dependent and time-independent Schr\\\"odinger equation and\neffectively overcomes the so-called curse of dimensionality. This starkly\ncontrasts with other neural network-based machine learning PES methods, such as\nmulti-layer perceptrons (MLPs), where evaluating high-dimensional integrals is\nnot straightforward due to the fully connected topology in their backbone\narchitecture. Nevertheless, the NN-MPO retains the high representational\ncapacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a\ntest mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled\nsix-dimensional ab initio PES, using only 625 training points distributed\nacross a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is\navailable at https://github.com/KenHino/Pompon.", "AI": {"tldr": "A neural network-based PES in MPO form (NN-MPO) efficiently handles high-dimensional integrals, overcoming dimensionality issues while maintaining accuracy.", "motivation": "To address the challenge of evaluating high-dimensional integrals in neural network-based PES methods, which is not straightforward in traditional architectures like MLPs.", "method": "Proposes NN-MPO, leveraging MPO form for efficient integral evaluation, retaining neural network representational capacity.", "result": "Achieves spectroscopic accuracy with a test MAE of 3.03 cm\u207b\u00b9 for a six-dimensional PES using only 625 training points.", "conclusion": "NN-MPO is a promising approach for high-dimensional PES with computational efficiency and accuracy."}}
{"id": "2502.15037", "pdf": "https://arxiv.org/pdf/2502.15037", "abs": "https://arxiv.org/abs/2502.15037", "authors": ["Yizhou Chen", "Xiaoyue Wu", "Yeheng Zong", "Yuzhen Chen", "Anran Li", "Bohao Zhang", "Ram Vasudevan"], "title": "DEFT: Differentiable Branched Discrete Elastic Rods for Modeling Furcated DLOs in Real-Time", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.", "AI": {"tldr": "DEFT is a novel framework for modeling Branched Deformable Linear Objects (BDLOs) in real-time, combining physics-based modeling and learning to address challenges in autonomous wire harness assembly.", "motivation": "Existing models for single-threaded Deformable Linear Objects (DLOs) fail to capture the complex dynamics of BDLOs, hindering reliable robotic manipulation.", "method": "DEFT integrates a differentiable physics-based model with a learning framework to accurately simulate BDLO dynamics, including junction point interactions and mid-BDLO grasping, while ensuring real-time computation.", "result": "DEFT outperforms state-of-the-art methods in accuracy, computational speed, and generalizability, as demonstrated in real-world experiments.", "conclusion": "DEFT advances BDLO modeling, enabling more reliable and dexterous robotic manipulation of complex branched cables."}}
{"id": "2502.17648", "pdf": "https://arxiv.org/pdf/2502.17648", "abs": "https://arxiv.org/abs/2502.17648", "authors": ["Lei Cheng", "Lihao Guo", "Tianya Zhang", "Tam Bang", "Austin Harris", "Mustafa Hajij", "Mina Sartipi", "Siyang Cao"], "title": "CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Accurate multi-sensor calibration is essential for deploying robust\nperception systems in applications such as autonomous driving and intelligent\ntransportation. Existing LiDAR-camera calibration methods often rely on\nmanually placed targets, preliminary parameter estimates, or intensive data\npreprocessing, limiting their scalability and adaptability in real-world\nsettings. In this work, we propose a fully automatic, targetless, and online\ncalibration framework, CalibRefine, which directly processes raw LiDAR point\nclouds and camera images. Our approach is divided into four stages: (1) a\nCommon Feature Discriminator that leverages relative spatial positions, visual\nappearance embeddings, and semantic class cues to identify and generate\nreliable LiDAR-camera correspondences, (2) a coarse homography-based\ncalibration that uses the matched feature correspondences to estimate an\ninitial transformation between the LiDAR and camera frames, serving as the\nfoundation for further refinement, (3) an iterative refinement to incrementally\nimprove alignment as additional data frames become available, and (4) an\nattention-based refinement that addresses non-planar distortions by leveraging\na Vision Transformer and cross-attention mechanisms. Extensive experiments on\ntwo urban traffic datasets demonstrate that CalibRefine achieves high-precision\ncalibration with minimal human input, outperforming state-of-the-art targetless\nmethods and matching or surpassing manually tuned baselines. Our results show\nthat robust object-level feature matching, combined with iterative refinement\nand self-supervised attention-based refinement, enables reliable sensor\nalignment in complex real-world conditions without ground-truth matrices or\nelaborate preprocessing. Code is available at\nhttps://github.com/radar-lab/Lidar_Camera_Automatic_Calibration", "AI": {"tldr": "CalibRefine is a fully automatic, targetless, and online LiDAR-camera calibration framework that outperforms existing methods with minimal human input.", "motivation": "Existing LiDAR-camera calibration methods rely on manual targets or preprocessing, limiting scalability and adaptability in real-world settings.", "method": "The approach involves four stages: common feature discrimination, coarse homography-based calibration, iterative refinement, and attention-based refinement using a Vision Transformer.", "result": "Experiments show CalibRefine achieves high-precision calibration, outperforming state-of-the-art targetless methods and matching manual baselines.", "conclusion": "Robust feature matching and refinement enable reliable sensor alignment in complex conditions without ground-truth or preprocessing."}}
{"id": "2411.15279", "pdf": "https://arxiv.org/pdf/2411.15279", "abs": "https://arxiv.org/abs/2411.15279", "authors": ["Maximilian Mews", "Ansar Aynetdinov", "Vivian Schiller", "Peter Eisert", "Alan Akbik"], "title": "Don't Mesh with Me: Generating Constructive Solid Geometry Instead of Meshes by Fine-Tuning a Code-Generation LLM", "categories": ["cs.LG", "cs.GR"], "comment": "Accepted to the AI for Content Creation Workshop at CVPR 2025", "summary": "While recent advancements in machine learning, such as LLMs, are\nrevolutionizing software development and creative industries, they have had\nminimal impact on engineers designing mechanical parts, which remains largely a\nmanual process. Existing approaches to generating 3D geometry most commonly use\nmeshes as a 3D representation. While meshes are suitable for assets in video\ngames or animations, they lack sufficient precision and adaptability for\nmechanical engineering purposes. This paper introduces a novel approach for the\ngeneration of 3D geometry that generates surface-based Constructive Solid\nGeometry (CSG) by leveraging a code-generation LLM. First, we create a dataset\nof 3D mechanical parts represented as code scripts by converting Boundary\nRepresentation geometry (BREP) into CSG-based Python scripts. Second, we create\nannotations in natural language using GPT-4. The resulting dataset is used to\nfine-tune a code-generation LLM. The fine-tuned LLM can complete geometries\nbased on positional input and natural language in a plausible way,\ndemonstrating geometric understanding.", "AI": {"tldr": "A novel method uses a code-generation LLM to create precise 3D mechanical parts by converting BREP to CSG scripts, fine-tuned with GPT-4 annotations, enabling natural language input for geometry completion.", "motivation": "Current machine learning advancements, like LLMs, have limited impact on mechanical engineering, where manual processes dominate. Meshes, common in 3D geometry, lack precision for engineering needs.", "method": "Convert BREP geometry to CSG-based Python scripts, annotate with GPT-4, and fine-tune a code-generation LLM to generate plausible 3D geometries from positional and natural language inputs.", "result": "The fine-tuned LLM successfully generates precise 3D mechanical parts, demonstrating geometric understanding through natural language and positional inputs.", "conclusion": "This approach bridges the gap between LLMs and mechanical engineering, offering a precise, adaptable solution for 3D geometry generation."}}
{"id": "2503.06269", "pdf": "https://arxiv.org/pdf/2503.06269", "abs": "https://arxiv.org/abs/2503.06269", "authors": ["Thomas Winninger", "Boussad Addad", "Katarzyna Kapusta"], "title": "Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traditional white-box methods for creating adversarial perturbations against\nLLMs typically rely only on gradient computation from the targeted model,\nignoring the internal mechanisms responsible for attack success or failure.\nConversely, interpretability studies that analyze these internal mechanisms\nlack practical applications beyond runtime interventions. We bridge this gap by\nintroducing a novel white-box approach that leverages mechanistic\ninterpretability techniques to craft practical adversarial inputs.\nSpecifically, we first identify acceptance subspaces - sets of feature vectors\nthat do not trigger the model's refusal mechanisms - then use gradient-based\noptimization to reroute embeddings from refusal subspaces to acceptance\nsubspaces, effectively achieving jailbreaks. This targeted approach\nsignificantly reduces computation cost, achieving attack success rates of\n80-95\\% on state-of-the-art models including Gemma2, Llama3.2, and Qwen2.5\nwithin minutes or even seconds, compared to existing techniques that often fail\nor require hours of computation. We believe this approach opens a new direction\nfor both attack research and defense development. Furthermore, it showcases a\npractical application of mechanistic interpretability where other methods are\nless efficient, which highlights its utility. The code and generated datasets\nare available at https://github.com/Sckathach/subspace-rerouting.", "AI": {"tldr": "A novel white-box adversarial attack method combines mechanistic interpretability with gradient-based optimization to efficiently jailbreak LLMs by rerouting embeddings from refusal to acceptance subspaces.", "motivation": "Traditional adversarial methods ignore internal model mechanisms, while interpretability studies lack practical applications. This work bridges the gap by using interpretability for practical attacks.", "method": "Identify acceptance subspaces (feature vectors avoiding refusal), then use gradient optimization to reroute embeddings from refusal to acceptance subspaces.", "result": "Achieves 80-95% success rates on models like Gemma2, Llama3.2, and Qwen2.5 in minutes/seconds, outperforming existing methods.", "conclusion": "This approach advances attack/defense research and demonstrates practical utility of mechanistic interpretability. Code and datasets are publicly available."}}
{"id": "2503.03307", "pdf": "https://arxiv.org/pdf/2503.03307", "abs": "https://arxiv.org/abs/2503.03307", "authors": ["Ji Zhao", "Banglei Guan", "Zibin Liu", "Laurent Kneip"], "title": "Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers", "categories": ["cs.CV"], "comment": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2025", "summary": "For event cameras, current sparse geometric solvers for egomotion estimation\nassume that the rotational displacements are known, such as those provided by\nan IMU. Thus, they can only recover the translational motion parameters.\nRecovering full-DoF motion parameters using a sparse geometric solver is a more\nchallenging task, and has not yet been investigated. In this paper, we propose\nseveral solvers to estimate both rotational and translational velocities within\na unified framework. Our method leverages event manifolds induced by line\nsegments. The problem formulations are based on either an incidence relation\nfor lines or a novel coplanarity relation for normal vectors. We demonstrate\nthe possibility of recovering full-DoF egomotion parameters for both angular\nand linear velocities without requiring extra sensor measurements or motion\npriors. To achieve efficient optimization, we exploit the Adam framework with a\nfirst-order approximation of rotations for quick initialization. Experiments on\nboth synthetic and real-world data demonstrate the effectiveness of our method.\nThe code is available at https://github.com/jizhaox/relpose-event.", "AI": {"tldr": "The paper proposes solvers to estimate full-DoF egomotion (rotational and translational velocities) for event cameras using event manifolds from line segments, without relying on IMU data or motion priors.", "motivation": "Current sparse geometric solvers for event cameras assume known rotational displacements (e.g., from IMUs), limiting them to estimating only translational motion. This work addresses the challenge of recovering full-DoF motion parameters.", "method": "The method uses event manifolds from line segments, formulating the problem via incidence or coplanarity relations. It employs Adam optimization with a first-order rotation approximation for efficient initialization.", "result": "Experiments on synthetic and real-world data confirm the method's effectiveness in estimating full-DoF motion without additional sensors or priors.", "conclusion": "The work successfully demonstrates the feasibility of recovering full-DoF egomotion for event cameras, offering a unified framework for both angular and linear velocities."}}
{"id": "2412.07775", "pdf": "https://arxiv.org/pdf/2412.07775", "abs": "https://arxiv.org/abs/2412.07775", "authors": ["Zhen Liu", "Tim Z. Xiao", "Weiyang Liu", "Yoshua Bengio", "Dinghuai Zhang"], "title": "Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets", "categories": ["cs.LG", "cs.CV"], "comment": "Technical Report (37 pages, 31 figures), Accepted at ICLR 2025", "summary": "While one commonly trains large diffusion models by collecting datasets on\ntarget downstream tasks, it is often desired to align and finetune pretrained\ndiffusion models with some reward functions that are either designed by experts\nor learned from small-scale datasets. Existing post-training methods for reward\nfinetuning of diffusion models typically suffer from lack of diversity in\ngenerated samples, lack of prior preservation, and/or slow convergence in\nfinetuning. In response to this challenge, we take inspiration from recent\nsuccesses in generative flow networks (GFlowNets) and propose a reinforcement\nlearning method for diffusion model finetuning, dubbed Nabla-GFlowNet\n(abbreviated as $\\nabla$-GFlowNet), that leverages the rich signal in reward\ngradients for probabilistic diffusion finetuning. We show that our proposed\nmethod achieves fast yet diversity- and prior-preserving finetuning of Stable\nDiffusion, a large-scale text-conditioned image diffusion model, on different\nrealistic reward functions.", "AI": {"tldr": "The paper proposes Nabla-GFlowNet, a reinforcement learning method for finetuning diffusion models, addressing diversity, prior preservation, and slow convergence issues.", "motivation": "Existing methods for reward finetuning of diffusion models lack diversity, prior preservation, and slow convergence.", "method": "The authors introduce Nabla-GFlowNet, leveraging reward gradients for probabilistic diffusion finetuning.", "result": "The method achieves fast, diversity-preserving, and prior-preserving finetuning of Stable Diffusion on realistic reward functions.", "conclusion": "Nabla-GFlowNet effectively addresses key challenges in diffusion model finetuning."}}
{"id": "2503.16328", "pdf": "https://arxiv.org/pdf/2503.16328", "abs": "https://arxiv.org/abs/2503.16328", "authors": ["Xiaoyu Wang", "Yijia Xu", "Jingyi Huang", "Zhengwei Yang", "Zhou Zhang"], "title": "Knowledge-guided machine learning for county-level corn yield prediction under drought", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Remote sensing (RS) technique, enabling the non-contact acquisition of\nextensive ground observations, is a valuable tool for crop yield predictions.\nTraditional process-based models struggle to incorporate large volumes of RS\ndata, and most users lack understanding of crop growth mechanisms. In contrast,\nmachine learning (ML) models are often criticized as \"black boxes\" due to their\nlimited interpretability. To address these limitations, we utilized\nKnowledge-Guided Machine Learning (KGML), a framework that leverages the\nstrengths of both process-based and ML models. Existing works have either\noverlooked the role of soil moisture in corn growth or did not embed this\neffect into their models. To bridge this gap, we developed the Knowledge-Guided\nMachine Learning with Soil Moisture (KGML-SM) framework, treating soil moisture\nas an intermediate variable in corn growth to emphasize its key role in plant\ndevelopment. Additionally, based on the prior knowledge that the model may\noverestimate under drought conditions, we designed a drought-aware loss\nfunction that penalized predicted yield in drought-affected areas. Our\nexperiments showed that the KGML-SM model outperformed other traditional ML\nmodels. We explored the relationships between drought, soil moisture, and corn\nyield prediction by assessing the importance of different features within the\nmodel, and analyzing how soil moisture impacts predictions across different\nregions and time periods. Finally we provided interpretability for prediction\nerrors to guide future model optimization.", "AI": {"tldr": "The paper introduces KGML-SM, a framework combining knowledge-guided ML and soil moisture data to improve corn yield predictions, outperforming traditional ML models.", "motivation": "Traditional models struggle with RS data and lack interpretability, while ML models are seen as black boxes. Soil moisture's role in corn growth is often overlooked.", "method": "Developed KGML-SM, treating soil moisture as an intermediate variable and using a drought-aware loss function.", "result": "KGML-SM outperformed traditional ML models, with insights into drought, soil moisture, and yield relationships.", "conclusion": "The framework enhances interpretability and accuracy, guiding future model optimization."}}
{"id": "2504.12240", "pdf": "https://arxiv.org/pdf/2504.12240", "abs": "https://arxiv.org/abs/2504.12240", "authors": ["Junhao Zhuang", "Lingen Li", "Xuan Ju", "Zhaoyang Zhang", "Chun Yuan", "Ying Shan"], "title": "Cobra: Efficient Line Art COlorization with BRoAder References", "categories": ["cs.CV"], "comment": "Project page with code: https://zhuang2002.github.io/Cobra/", "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.", "AI": {"tldr": "Cobra is an efficient method for reference-based line art colorization, addressing challenges like extensive reference handling and slow inference with a Causal Sparse DiT architecture.", "motivation": "The comic industry needs accurate, efficient, and flexible line art colorization, but current diffusion models struggle with reference handling and speed.", "method": "Cobra uses a Causal Sparse DiT architecture with positional encodings, causal sparse attention, and Key-Value Cache to manage references and ensure color consistency.", "result": "Cobra achieves high-quality colorization with over 200 references, improving speed and interactivity.", "conclusion": "Cobra meets industrial demands for efficient and accurate line art colorization, with released codes and models."}}
{"id": "2412.13840", "pdf": "https://arxiv.org/pdf/2412.13840", "abs": "https://arxiv.org/abs/2412.13840", "authors": ["Yichen Li", "Haozhao Wang", "Wenchao Xu", "Tianzhe Xiao", "Hong Liu", "Minzhu Tu", "Yuying Wang", "Xin Yang", "Rui Zhang", "Shui Yu", "Song Guo", "Ruixuan Li"], "title": "Unleashing the Power of Continual Learning on Non-Centralized Devices: A Survey", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Non-Centralized Continual Learning (NCCL) has become an emerging paradigm for\nenabling distributed devices such as vehicles and servers to handle streaming\ndata from a joint non-stationary environment. To achieve high reliability and\nscalability in deploying this paradigm in distributed systems, it is essential\nto conquer challenges stemming from both spatial and temporal dimensions,\nmanifesting as distribution shifts, catastrophic forgetting, heterogeneity, and\nprivacy issues. This survey focuses on a comprehensive examination of the\ndevelopment of the non-centralized continual learning algorithms and the\nreal-world deployment across distributed devices. We begin with an introduction\nto the background and fundamentals of non-centralized learning and continual\nlearning. Then, we review existing solutions from three levels to represent how\nexisting techniques alleviate the catastrophic forgetting and distribution\nshift. Additionally, we delve into the various types of heterogeneity issues,\nsecurity, and privacy attributes, as well as real-world applications across\nthree prevalent scenarios. Furthermore, we establish a large-scale benchmark to\nrevisit this problem and analyze the performance of the state-of-the-art NCCL\napproaches. Finally, we discuss the important challenges and future research\ndirections in NCCL.", "AI": {"tldr": "A survey on Non-Centralized Continual Learning (NCCL) addressing challenges like distribution shifts, catastrophic forgetting, heterogeneity, and privacy in distributed systems. It reviews algorithms, benchmarks, and real-world applications.", "motivation": "To enable distributed devices (e.g., vehicles, servers) to handle streaming data in non-stationary environments reliably and scalably.", "method": "Comprehensive examination of NCCL algorithms, reviewing solutions from three levels, analyzing heterogeneity, security, privacy, and real-world applications. A benchmark is established to evaluate state-of-the-art NCCL approaches.", "result": "Identifies key challenges and performance of NCCL techniques, highlighting gaps in current solutions.", "conclusion": "Discusses future research directions and critical challenges in NCCL, emphasizing its potential and limitations."}}
{"id": "2503.16518", "pdf": "https://arxiv.org/pdf/2503.16518", "abs": "https://arxiv.org/abs/2503.16518", "authors": ["Dian Chen", "Han Jun Yoon", "Zelin Wan", "Nithin Alluru", "Sang Won Lee", "Richard He", "Terrence J. Moore", "Frederica F. Nelson", "Sunghyun Yoon", "Hyuk Lim", "Dan Dongseong Kim", "Jin-Hee Cho"], "title": "Advancing Human-Machine Teaming: Concepts, Challenges, and Applications", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Human-Machine Teaming (HMT) is revolutionizing collaboration across domains\nsuch as defense, healthcare, and autonomous systems by integrating AI-driven\ndecision-making, trust calibration, and adaptive teaming. This survey presents\na comprehensive taxonomy of HMT, analyzing theoretical models, including\nreinforcement learning, instance-based learning, and interdependence theory,\nalongside interdisciplinary methodologies. Unlike prior reviews, we examine\nteam cognition, ethical AI, multi-modal interactions, and real-world evaluation\nframeworks. Key challenges include explainability, role allocation, and\nscalable benchmarking. We propose future research in cross-domain adaptation,\ntrust-aware AI, and standardized testbeds. By bridging computational and social\nsciences, this work lays a foundation for resilient, ethical, and scalable HMT\nsystems.", "AI": {"tldr": "A survey on Human-Machine Teaming (HMT) explores AI-driven collaboration, trust, and adaptive teaming, proposing future research directions.", "motivation": "To advance HMT by integrating AI, trust calibration, and interdisciplinary methodologies for resilient and ethical systems.", "method": "Analyzes theoretical models (e.g., reinforcement learning, interdependence theory) and interdisciplinary approaches, focusing on team cognition and real-world evaluation.", "result": "Identifies challenges like explainability and role allocation, suggesting future work in cross-domain adaptation and trust-aware AI.", "conclusion": "Bridges computational and social sciences to build scalable, ethical HMT systems."}}
{"id": "2504.13460", "pdf": "https://arxiv.org/pdf/2504.13460", "abs": "https://arxiv.org/abs/2504.13460", "authors": ["Hongwei Ji", "Wulian Yun", "Mengshi Qi", "Huadong Ma"], "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.", "AI": {"tldr": "The paper introduces a few-shot temporal action localization (TAL) method using Chain-of-Thought textual reasoning to enhance performance by leveraging textual information alongside visual data.", "motivation": "Existing few-shot TAL methods focus only on video-level information, ignoring valuable textual semantic support, which limits localization accuracy.", "method": "The proposed method includes a semantic-aware text-visual alignment module and a Chain-of-Thought reasoning approach to generate descriptive texts for videos, improving action commonality and variation capture.", "result": "Experiments on ActivityNet1.3 and THUMOS14 show superior performance over existing methods, and a new dataset for human anomaly detection is introduced.", "conclusion": "The method effectively combines textual and visual information for better few-shot TAL, with potential applications in anomaly detection."}}
{"id": "2501.18691", "pdf": "https://arxiv.org/pdf/2501.18691", "abs": "https://arxiv.org/abs/2501.18691", "authors": ["Matan Ben-Dov", "Jing Chen"], "title": "Regularized second-order optimization of tensor-network Born machines", "categories": ["cs.LG", "quant-ph"], "comment": "11 pages, 5 figures", "summary": "Tensor-network Born machines (TNBMs) are quantum-inspired generative models\nfor learning data distributions. Using tensor-network contraction and\noptimization techniques, the model learns an efficient representation of the\ntarget distribution, capable of capturing complex correlations with a compact\nparameterization. Despite their promise, the optimization of TNBMs presents\nseveral challenges. A key bottleneck of TNBMs is the logarithmic nature of the\nloss function commonly used for this problem. The single-tensor logarithmic\noptimization problem cannot be solved analytically, necessitating an iterative\napproach that slows down convergence and increases the risk of getting trapped\nin one of many non-optimal local minima. In this paper, we present an improved\nsecond-order optimization technique for TNBM training, which significantly\nenhances convergence rates and the quality of the optimized model. Our method\nemploys a modified Newton's method on the manifold of normalized states,\nincorporating regularization of the loss landscape to mitigate local minima\nissues. We demonstrate the effectiveness of our approach by training a\none-dimensional matrix product state (MPS) on both discrete and continuous\ndatasets, showcasing its advantages in terms of stability and efficiency, and\ndemonstrating its potential as a robust and scalable approach for optimizing\nquantum-inspired generative models.", "AI": {"tldr": "The paper introduces an improved second-order optimization technique for Tensor-network Born machines (TNBMs) to address slow convergence and local minima issues, enhancing stability and efficiency.", "motivation": "The logarithmic loss function in TNBMs causes slow convergence and traps in local minima, hindering their potential as quantum-inspired generative models.", "method": "A modified Newton's method on the manifold of normalized states, with regularization of the loss landscape, is proposed.", "result": "The method improves convergence rates and model quality, demonstrated on discrete and continuous datasets using a one-dimensional MPS.", "conclusion": "The approach is robust and scalable, offering a promising solution for optimizing quantum-inspired generative models."}}
{"id": "2504.00142", "pdf": "https://arxiv.org/pdf/2504.00142", "abs": "https://arxiv.org/abs/2504.00142", "authors": ["Srinitish Srinivasan", "Omkumar CU"], "title": "LGIN: Defining an Approximately Powerful Hyperbolic GNN", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at GRADES NDA 2025 Workshop@ACM SIGMOD/PODS(Non Archival)", "summary": "While graph neural networks (GNNs) operating in hyperbolic spaces have shown\npromise for modeling hierarchical and complex relational data, a critical\nlimitation often overlooked is their potentially limited discriminative power\ncompared to their Euclidean counterparts or fundamental graph isomorphism tests\nlike the Weisfeiler-Lehman (WL) hierarchy. Existing hyperbolic aggregation\nschemes, while curvature-aware, may not sufficiently capture the intricate\nstructural differences required to robustly distinguish non-isomorphic graphs\nowing to non-injective aggregation functions. To address this expressiveness\ngap in hyperbolic graph learning, we introduce the Lorentzian Graph Isomorphic\nNetwork (LGIN), a novel GNN designed to achieve enhanced discriminative\ncapabilities within the Lorentzian model of hyperbolic space. LGIN proposes a\nnew update rule that effectively combines local neighborhood information with a\nricher representation of graph structure designed to preserve the Lorentzian\nmetric tensor. This represents a significant step towards building more\nexpressive GNNs in non-Euclidean geometries, overcoming a common bottleneck in\ncurrent hyperbolic methods. We conduct extensive evaluations across nine\ndiverse benchmark datasets, including molecular and protein structures. LGIN\nconsistently outperforms or matches state-of-the-art hyperbolic and Euclidean\nGNNs, showcasing its practical efficacy and validating its superior ability to\ncapture complex graph structures and distinguish between different graphs. To\nthe best of our knowledge, LGIN is the first work to study the framework behind\na powerful GNN on the hyperbolic space. The code for our paper can be found at\nhttps://github.com/Deceptrax123/LGIN", "AI": {"tldr": "LGIN, a hyperbolic GNN, enhances discriminative power for graph learning by leveraging Lorentzian space, outperforming existing methods.", "motivation": "Address the expressiveness gap in hyperbolic GNNs by improving discriminative power for non-isomorphic graphs.", "method": "Introduces LGIN with a novel update rule combining local neighborhood info and Lorentzian metric tensor preservation.", "result": "LGIN outperforms state-of-the-art GNNs on diverse datasets, proving superior discriminative ability.", "conclusion": "LGIN advances hyperbolic GNN expressiveness, validated by empirical results, and is the first of its kind."}}
{"id": "2504.13754", "pdf": "https://arxiv.org/pdf/2504.13754", "abs": "https://arxiv.org/abs/2504.13754", "authors": ["Zhu Zhu", "Shuo Jiang", "Jingyuan Zheng", "Yawen Li", "Yifei Chen", "Manli Zhao", "Weizhong Gu", "Feiwei Qin", "Jinhu Wang", "Gang Yu"], "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "10pages, 8 figures", "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole-slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole-slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to bridge patch-level predictions to whole-slide\nimage-level classifications seamlessly. We verified the CMSwinKAN on the\npublicly available BreakHis dataset and the PpNTs dataset, which was\nestablished by our hospital. Results demonstrate that CMSwinKAN performs better\nthan existing state-of-the-art pathology-specific models pre-trained on large\ndatasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.", "AI": {"tldr": "CMSwinKAN is a contrastive-learning-based multi-scale feature fusion model for neuroblastoma diagnosis, improving interpretability and accuracy over existing methods.", "motivation": "Current neuroblastoma diagnosis relies on subjective manual examination, and automated methods face challenges like poor interpretability and high computational costs.", "method": "CMSwinKAN integrates a Kernel Activation Network into the Swin Transformer, fuses multi-scale features, and uses contrastive learning to mimic clinicians' approaches.", "result": "CMSwinKAN outperforms state-of-the-art models on the BreakHis and PpNTs datasets.", "conclusion": "CMSwinKAN offers a more accurate and interpretable solution for pathological image classification, with potential for clinical deployment."}}
{"id": "2504.05356", "pdf": "https://arxiv.org/pdf/2504.05356", "abs": "https://arxiv.org/abs/2504.05356", "authors": ["JianLin Zhu", "HongKuo Niu"], "title": "DyTTP: Trajectory Prediction with Normalization-Free Transformers", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate trajectory prediction is a cornerstone for the safe operation of\nautonomous driving systems, where understanding the dynamic behavior of\nsurrounding agents is crucial. Transformer-based architectures have\ndemonstrated significant promise in capturing complex spatio-temporality\ndependencies. However, their reliance on normalization layers can lead to\ncomputation overhead and training instabilities. In this work, we present a\ntwo-fold approach to address these challenges. First, we integrate DynamicTanh\n(DyT), which is the latest method to promote transformers, into the backbone,\nreplacing traditional layer normalization. This modification simplifies the\nnetwork architecture and improves the stability of the inference. We are the\nfirst work to deploy the DyT to the trajectory prediction task. Complementing\nthis, we employ a snapshot ensemble strategy to further boost trajectory\nprediction performance. Using cyclical learning rate scheduling, multiple model\nsnapshots are captured during a single training run. These snapshots are then\naggregated via simple averaging at inference time, allowing the model to\nbenefit from diverse hypotheses without incurring substantial additional\ncomputational cost. Extensive experiments on Argoverse datasets demonstrate\nthat our combined approach significantly improves prediction accuracy,\ninference speed and robustness in diverse driving scenarios. This work\nunderscores the potential of normalization-free transformer designs augmented\nwith lightweight ensemble techniques in advancing trajectory forecasting for\nautonomous vehicles.", "AI": {"tldr": "The paper introduces a two-fold approach for trajectory prediction in autonomous driving: using DynamicTanh (DyT) to replace layer normalization in transformers and employing snapshot ensemble for improved performance.", "motivation": "Accurate trajectory prediction is critical for autonomous driving, but transformer-based methods face issues like computation overhead and training instability due to normalization layers.", "method": "The approach integrates DyT into transformers to replace layer normalization and uses snapshot ensemble with cyclical learning rates to capture multiple model snapshots.", "result": "Experiments on Argoverse datasets show significant improvements in prediction accuracy, inference speed, and robustness.", "conclusion": "The work highlights the potential of normalization-free transformers and lightweight ensemble techniques for advancing trajectory forecasting."}}
{"id": "2504.01153", "pdf": "https://arxiv.org/pdf/2504.01153", "abs": "https://arxiv.org/abs/2504.01153", "authors": ["Mahjabin Nahar", "Eun-Ju Lee", "Jin Won Park", "Dongwon Lee"], "title": "Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "While we increasingly rely on large language models (LLMs) for various tasks,\nthese models are known to produce inaccurate content or `hallucinations' with\npotentially disastrous consequences. The recent integration of web search\nresults into LLMs prompts the question of whether people utilize them to verify\nthe generated content, thereby accurately detecting hallucinations. An online\nexperiment (N = 560) investigated how the provision of search results, either\nstatic (i.e., fixed search results provided by LLM) or dynamic (i.e.,\nparticipant-led searches), affects participants' perceived accuracy of\nLLM-generated content (i.e., genuine, minor hallucination, major\nhallucination), self-confidence in accuracy ratings, as well as their overall\nevaluation of the LLM, as compared to the control condition (i.e., no search\nresults). Results showed that participants in both static and dynamic\nconditions (vs. control) rated hallucinated content to be less accurate and\nperceived the LLM more negatively. However, those in the dynamic condition\nrated genuine content as more accurate and demonstrated greater overall\nself-confidence in their assessments than those in the static search or control\nconditions. We highlighted practical implications of incorporating web search\nfunctionality into LLMs in real-world contexts.", "AI": {"tldr": "The study explores how integrating web search results (static or dynamic) into LLMs affects users' ability to detect hallucinations and their confidence in accuracy ratings. Results show dynamic searches improve accuracy for genuine content and confidence, while both search types reduce perceived accuracy of hallucinations and LLM evaluation.", "motivation": "To understand if web search integration helps users detect LLM hallucinations and assess content accuracy, given the risks of inaccurate outputs.", "method": "An online experiment (N=560) compared static (fixed) and dynamic (user-led) search results against a control (no search) to measure perceived accuracy, confidence, and LLM evaluation.", "result": "Dynamic searches improved accuracy ratings for genuine content and user confidence, while both search types reduced perceived accuracy of hallucinations and worsened LLM evaluation.", "conclusion": "Incorporating dynamic web searches into LLMs can enhance user trust and accuracy detection, but static searches also help mitigate hallucination risks."}}
{"id": "2504.16915", "pdf": "https://arxiv.org/pdf/2504.16915", "abs": "https://arxiv.org/abs/2504.16915", "authors": ["Chong Mou", "Yanze Wu", "Wenxu Wu", "Zinan Guo", "Pengze Zhang", "Yufeng Cheng", "Yiming Luo", "Fei Ding", "Shiwen Zhang", "Xinghui Li", "Mengtian Li", "Songtao Zhao", "Jian Zhang", "Qian He", "Xinglong Wu"], "title": "DreamO: A Unified Framework for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "Recently, extensive research on image customization (e.g., identity, subject,\nstyle, background, etc.) demonstrates strong customization capabilities in\nlarge-scale generative models. However, most approaches are designed for\nspecific tasks, restricting their generalizability to combine different types\nof condition. Developing a unified framework for image customization remains an\nopen challenge. In this paper, we present DreamO, an image customization\nframework designed to support a wide range of tasks while facilitating seamless\nintegration of multiple conditions. Specifically, DreamO utilizes a diffusion\ntransformer (DiT) framework to uniformly process input of different types.\nDuring training, we construct a large-scale training dataset that includes\nvarious customization tasks, and we introduce a feature routing constraint to\nfacilitate the precise querying of relevant information from reference images.\nAdditionally, we design a placeholder strategy that associates specific\nplaceholders with conditions at particular positions, enabling control over the\nplacement of conditions in the generated results. Moreover, we employ a\nprogressive training strategy consisting of three stages: an initial stage\nfocused on simple tasks with limited data to establish baseline consistency, a\nfull-scale training stage to comprehensively enhance the customization\ncapabilities, and a final quality alignment stage to correct quality biases\nintroduced by low-quality data. Extensive experiments demonstrate that the\nproposed DreamO can effectively perform various image customization tasks with\nhigh quality and flexibly integrate different types of control conditions.", "AI": {"tldr": "DreamO is a unified image customization framework using a diffusion transformer (DiT) to handle diverse tasks and integrate multiple conditions, achieving high-quality results.", "motivation": "Existing image customization methods are task-specific, lacking generalizability. DreamO aims to unify these tasks under one framework.", "method": "DreamO uses a DiT framework, feature routing constraints, placeholder strategies, and a three-stage progressive training approach.", "result": "DreamO effectively performs various customization tasks and integrates multiple control conditions with high quality.", "conclusion": "DreamO addresses the challenge of unified image customization, demonstrating flexibility and high performance across tasks."}}
{"id": "2504.08842", "pdf": "https://arxiv.org/pdf/2504.08842", "abs": "https://arxiv.org/abs/2504.08842", "authors": ["Micah Adler", "Dan Alistarh", "Nir Shavit"], "title": "Towards Combinatorial Interpretability of Neural Computation", "categories": ["cs.LG", "cs.NE", "I.2.0"], "comment": "48 Pages", "summary": "We introduce combinatorial interpretability, a methodology for understanding\nneural computation by analyzing the combinatorial structures in the sign-based\ncategorization of a network's weights and biases. We demonstrate its power\nthrough feature channel coding, a theory that explains how neural networks\ncompute Boolean expressions and potentially underlies other categories of\nneural network computation. According to this theory, features are computed via\nfeature channels: unique cross-neuron encodings shared among the inputs the\nfeature operates on. Because different feature channels share neurons, the\nneurons are polysemantic and the channels interfere with one another, making\nthe computation appear inscrutable.\n  We show how to decipher these computations by analyzing a network's feature\nchannel coding, offering complete mechanistic interpretations of several small\nneural networks that were trained with gradient descent. Crucially, this is\nachieved via static combinatorial analysis of the weight matrices, without\nexamining activations or training new autoencoding networks. Feature channel\ncoding reframes the superposition hypothesis, shifting the focus from neuron\nactivation directionality in high-dimensional space to the combinatorial\nstructure of codes. It also allows us for the first time to exactly quantify\nand explain the relationship between a network's parameter size and its\ncomputational capacity (i.e. the set of features it can compute with low\nerror), a relationship that is implicitly at the core of many modern scaling\nlaws.\n  Though our initial studies of feature channel coding are restricted to\nBoolean functions, we believe they provide a rich, controlled, and informative\nresearch space, and that the path we propose for combinatorial interpretation\nof neural computation can provide a basis for understanding both artificial and\nbiological neural circuits.", "AI": {"tldr": "The paper introduces combinatorial interpretability and feature channel coding to decode neural network computations by analyzing weight matrices, focusing on Boolean functions and their combinatorial structures.", "motivation": "To understand neural computation by deciphering the combinatorial structures in network weights and biases, moving beyond traditional activation-based methods.", "method": "Uses feature channel coding to analyze weight matrices statically, without examining activations or training additional networks, to interpret neural computations.", "result": "Demonstrates complete mechanistic interpretations of small neural networks, quantifies the relationship between parameter size and computational capacity, and reframes the superposition hypothesis.", "conclusion": "Combinatorial interpretability and feature channel coding provide a foundational approach for understanding neural computation in both artificial and biological networks, starting with Boolean functions."}}
{"id": "2504.10498", "pdf": "https://arxiv.org/pdf/2504.10498", "abs": "https://arxiv.org/abs/2504.10498", "authors": ["Jianling Lu", "Mingqi Lv", "Tieming Chen"], "title": "CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": "All authors of this paper have unanimously decided to withdraw its\n  preprint from arXiv. As one of the authors, I cannot unilaterally decide its\n  retention. In accordance with the collective decision, we formally request\n  the complete deletion of the paper from arXiv", "summary": "The performance of large language models (LLMs) in Q&A task increased\nsubstantially through Retrieval-Augmented Generation (RAG) which brings in\nexternal knowledge. However, the main difficulty lies in balancing the inherent\nself-knowledge of LLMs with external information retrieval (IR). The current\nthreshold-based methods apply one-dimensional static mechanisms with single\ncriterion. As a result, their IR decisions might be irrelevant to the LLMs'\nresponse under difficult queries. To alleviate this problem, we propose\nCognitive Convection of Self-Knowledge (CCSK). Different from traditional\nmethods that maintain single fixed IR activation criteria, CCSK implements a\ndynamic joint decision process via a Siamese Network module and a Response\nQuality Model. The Siamese Network calculates the cosine similarity between the\ncurrent query and the historical queries. The Response Quality Model evaluates\nthe responses of LLMs through LightGBM. The final decision of the CCSK is\nderived from the outputs of the two modules, as well as text features fused\nusing a multi-head attention mechanism. Extensive experiments on real-world\ndatasets show that CCSK significantly enhances the model's effectiveness in\ninformation retrieval.", "AI": {"tldr": "CCSK improves LLM performance in Q&A by dynamically balancing self-knowledge and external retrieval using a Siamese Network and Response Quality Model.", "motivation": "Current threshold-based methods for retrieval-augmented generation in LLMs use static criteria, leading to irrelevant IR decisions for difficult queries.", "method": "CCSK employs a dynamic joint decision process with a Siamese Network (for query similarity) and a Response Quality Model (LightGBM-based evaluation), fused via multi-head attention.", "result": "Experiments show CCSK significantly enhances information retrieval effectiveness in LLMs.", "conclusion": "CCSK addresses limitations of static IR methods, improving LLM performance through dynamic decision-making."}}
{"id": "2504.17761", "pdf": "https://arxiv.org/pdf/2504.17761", "abs": "https://arxiv.org/abs/2504.17761", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "title": "Step1X-Edit: A Practical Framework for General Image Editing", "categories": ["cs.CV"], "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "AI": {"tldr": "The paper introduces Step1X-Edit, an open-source image editing model that rivals proprietary models like GPT-4o and Gemini2 Flash, using Multimodal LLM and diffusion decoding.", "motivation": "To bridge the gap between open-source and closed-source image editing models by developing a competitive open-source alternative.", "method": "Uses Multimodal LLM for processing images and instructions, extracts latent embeddings, and integrates them with a diffusion decoder. A high-quality dataset is generated for training.", "result": "Step1X-Edit outperforms open-source baselines and nears the performance of proprietary models on the GEdit-Bench benchmark.", "conclusion": "Step1X-Edit advances open-source image editing, offering comparable performance to closed-source models."}}
{"id": "2504.16875", "pdf": "https://arxiv.org/pdf/2504.16875", "abs": "https://arxiv.org/abs/2504.16875", "authors": ["Julian Bedei", "Murray McBain", "Alexander Winkler", "Charles Robert Koch", "Jakob Andert", "David Gordon"], "title": "Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive\nControl (ML-MPC) are promising approaches for optimizing hydrogen-diesel\ndual-fuel engine control, as they can effectively control multiple-input\nmultiple-output systems and nonlinear processes. ML-MPC is advantageous for\nproviding safe and optimal controls, ensuring the engine operates within\npredefined safety limits. In contrast, RL is distinguished by its adaptability\nto changing conditions through its learning-based approach. However, the\npractical implementation of either method alone poses challenges. RL requires\nhigh variance in control inputs during early learning phases, which can pose\nrisks to the system by potentially executing unsafe actions, leading to\nmechanical damage. Conversely, ML-MPC relies on an accurate system model to\ngenerate optimal control inputs and has limited adaptability to system drifts,\nsuch as injector aging, which naturally occur in engine applications. To\naddress these limitations, this study proposes a hybrid RL and ML-MPC approach\nthat uses an ML-MPC framework while incorporating an RL agent to dynamically\nadjust the ML-MPC load tracking reference in response to changes in the\nenvironment. At the same time, the ML-MPC ensures that actions stay safe\nthroughout the RL agent's exploration. To evaluate the effectiveness of this\napproach, fuel pressure is deliberately varied to introduce a model-plant\nmismatch between the ML-MPC and the engine test bench. The result of this\nmismatch is a root mean square error (RMSE) in indicated mean effective\npressure of 0.57 bar when running the ML-MPC. The experimental results\ndemonstrate that RL successfully adapts to changing boundary conditions by\naltering the tracking reference while ML-MPC ensures safe control inputs. The\nquantitative improvement in load tracking by implementing RL is an RSME of 0.44\nbar.", "AI": {"tldr": "A hybrid RL and ML-MPC approach optimizes hydrogen-diesel dual-fuel engine control, combining RL's adaptability with ML-MPC's safety.", "motivation": "Addressing the limitations of RL (unsafe actions during learning) and ML-MPC (lack of adaptability to system drifts) in engine control.", "method": "Proposes a hybrid approach where RL dynamically adjusts ML-MPC's load tracking reference while ML-MPC ensures safe control inputs.", "result": "RL adapts to changing conditions, reducing RMSE from 0.57 bar (ML-MPC alone) to 0.44 bar (hybrid).", "conclusion": "The hybrid approach effectively balances adaptability and safety, improving engine control performance."}}
{"id": "2504.16381", "pdf": "https://arxiv.org/pdf/2504.16381", "abs": "https://arxiv.org/abs/2504.16381", "authors": ["Magnus Petersen", "Roberto Covino"], "title": "PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems", "categories": ["physics.chem-ph", "cs.AI", "physics.comp-ph"], "comment": "Update 06.05.2025: Added some intermediate steps in the derivation of\n  the loss to add clarity. Update 28.04.2025: Added citation and reference to\n  just-released work \"Action-Minimization Meets Generative Modeling: Efficient\n  Transition Path Sampling with the Onsager-Machlup Functional\" by Sanjeev Raja\n  et al. and added an appendix section clarifying some loss derivation steps", "summary": "Characterizing conformational transitions in physical systems remains a\nfundamental challenge in the computational sciences. Traditional sampling\nmethods like molecular dynamics (MD) or MCMC often struggle with the\nhigh-dimensional nature of molecular systems and the high energy barriers of\ntransitions between stable states. While these transitions are rare events in\nsimulation timescales, they often represent the most biologically significant\nprocesses - for example, the conformational change of an ion channel protein\nfrom its closed to open state, which controls cellular ion flow and is crucial\nfor neural signaling. Such transitions in real systems may take milliseconds to\nseconds but could require months or years of continuous simulation to observe\neven once. We present a method that reformulates transition path generation as\na continuous optimization problem solved through physics-informed neural\nnetworks (PINNs) inspired by string methods for minimum-energy path (MEP)\ngeneration. By representing transition paths as implicit neural functions and\nleveraging automatic differentiation with differentiable molecular dynamics\nforce fields, our method enables the efficient discovery of physically\nrealistic transition pathways without requiring expensive path sampling. We\ndemonstrate our method's effectiveness on two proteins, including an explicitly\nhydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300\natoms.", "AI": {"tldr": "The paper introduces a method using physics-informed neural networks (PINNs) to efficiently discover physically realistic transition pathways in molecular systems, overcoming limitations of traditional sampling methods.", "motivation": "Traditional methods like MD or MCMC struggle with high-dimensional molecular systems and rare but biologically significant transitions, such as ion channel conformational changes.", "method": "The method reformulates transition path generation as a continuous optimization problem using PINNs, leveraging automatic differentiation and differentiable molecular dynamics force fields.", "result": "The method successfully demonstrates efficiency in discovering transition pathways, tested on proteins including a hydrated BPTI system with over 8,300 atoms.", "conclusion": "The approach provides a computationally efficient alternative to traditional sampling methods for studying rare but critical molecular transitions."}}
{"id": "2504.19244", "pdf": "https://arxiv.org/pdf/2504.19244", "abs": "https://arxiv.org/abs/2504.19244", "authors": ["De Cheng", "Lingfeng He", "Nannan Wang", "Dingwen Zhang", "Xinbo Gao"], "title": "Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID", "categories": ["cs.CV"], "comment": "Accepted by IJCV 2025", "summary": "Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to\nmatch pedestrian images of the same individual across different modalities\nwithout human annotations for model learning. Previous methods unify\npseudo-labels of cross-modality images through label association algorithms and\nthen design contrastive learning framework for global feature learning.\nHowever, these methods overlook the cross-modality variations in feature\nrepresentation and pseudo-label distributions brought by fine-grained patterns.\nThis insight results in insufficient modality-shared learning when only global\nfeatures are optimized. To address this issue, we propose a Semantic-Aligned\nLearning with Collaborative Refinement (SALCR) framework, which builds up\noptimization objective for specific fine-grained patterns emphasized by each\nmodality, thereby achieving complementary alignment between the label\ndistributions of different modalities. Specifically, we first introduce a Dual\nAssociation with Global Learning (DAGI) module to unify the pseudo-labels of\ncross-modality instances in a bi-directional manner. Afterward, a Fine-Grained\nSemantic-Aligned Learning (FGSAL) module is carried out to explore part-level\nsemantic-aligned patterns emphasized by each modality from cross-modality\ninstances. Optimization objective is then formulated based on the\nsemantic-aligned features and their corresponding label space. To alleviate the\nside-effects arising from noisy pseudo-labels, we propose a Global-Part\nCollaborative Refinement (GPCR) module to mine reliable positive sample sets\nfor the global and part features dynamically and optimize the inter-instance\nrelationships. Extensive experiments demonstrate the effectiveness of the\nproposed method, which achieves superior performances to state-of-the-art\nmethods. Our code is available at\n\\href{https://github.com/FranklinLingfeng/code-for-SALCR}.", "AI": {"tldr": "The paper introduces SALCR, a framework for unsupervised visible-infrared person re-identification, addressing cross-modality variations by aligning fine-grained patterns and refining pseudo-labels.", "motivation": "Previous methods fail to address cross-modality variations in feature representation and pseudo-label distributions, leading to insufficient modality-shared learning.", "method": "Proposes SALCR with DAGI for bi-directional label unification, FGSAL for part-level semantic alignment, and GPCR for refining noisy pseudo-labels.", "result": "Achieves superior performance compared to state-of-the-art methods.", "conclusion": "SALCR effectively addresses cross-modality variations and improves unsupervised re-identification."}}
{"id": "2504.21254", "pdf": "https://arxiv.org/pdf/2504.21254", "abs": "https://arxiv.org/abs/2504.21254", "authors": ["Sixuan Wang", "Jiao Yin", "Jinli Cao", "MingJian Tang", "Hua Wang", "Yanchun Zhang"], "title": "ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning", "categories": ["cs.LG", "cs.NE"], "comment": null, "summary": "Effective and efficient graph representation learning is essential for\nenabling critical downstream tasks, such as node classification, link\nprediction, and subgraph search. However, existing graph neural network (GNN)\narchitectures often struggle to adapt to diverse and complex graph structures,\nlimiting their ability to produce structure-aware and task-discriminative\nrepresentations. To address this challenge, we propose ABG-NAS, a novel\nframework for automated graph neural network architecture search tailored for\nefficient graph representation learning. ABG-NAS encompasses three key\ncomponents: a Comprehensive Architecture Search Space (CASS), an Adaptive\nGenetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module\n(BGTM). CASS systematically explores diverse propagation (P) and transformation\n(T) operations, enabling the discovery of GNN architectures capable of\ncapturing intricate graph characteristics. AGOS dynamically balances\nexploration and exploitation, ensuring search efficiency and preserving\nsolution diversity. BGTM further optimizes hyperparameters periodically,\nenhancing the scalability and robustness of the resulting architectures.\nEmpirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, and\nCoraFull) demonstrate that ABG-NAS consistently outperforms both manually\ndesigned GNNs and state-of-the-art neural architecture search (NAS) methods.\nThese results highlight the potential of ABG-NAS to advance graph\nrepresentation learning by providing scalable and adaptive solutions for\ndiverse graph structures. Our code is publicly available at\nhttps://github.com/sserranw/ABG-NAS.", "AI": {"tldr": "ABG-NAS is a novel framework for automated graph neural network architecture search, outperforming manual GNNs and state-of-the-art NAS methods on benchmark datasets.", "motivation": "Existing GNNs struggle with diverse graph structures, limiting their effectiveness for tasks like node classification and link prediction.", "method": "ABG-NAS combines a Comprehensive Architecture Search Space (CASS), Adaptive Genetic Optimization Strategy (AGOS), and Bayesian-Guided Tuning Module (BGTM) to discover and optimize GNN architectures.", "result": "ABG-NAS consistently outperforms manual GNNs and NAS methods on datasets like Cora, PubMed, Citeseer, and CoraFull.", "conclusion": "ABG-NAS advances graph representation learning by offering scalable and adaptive solutions for complex graph structures."}}
{"id": "2504.18565", "pdf": "https://arxiv.org/pdf/2504.18565", "abs": "https://arxiv.org/abs/2504.18565", "authors": ["Sid Black", "Asa Cooper Stickland", "Jake Pencharz", "Oliver Sourbut", "Michael Schmatz", "Jay Bailey", "Ollie Matthews", "Ben Millwood", "Alex Remedios", "Alan Cooney"], "title": "RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Uncontrollable autonomous replication of language model agents poses a\ncritical safety risk. To better understand this risk, we introduce RepliBench,\na suite of evaluations designed to measure autonomous replication capabilities.\nRepliBench is derived from a decomposition of these capabilities covering four\ncore domains: obtaining resources, exfiltrating model weights, replicating onto\ncompute, and persisting on this compute for long periods. We create 20 novel\ntask families consisting of 86 individual tasks. We benchmark 5 frontier\nmodels, and find they do not currently pose a credible threat of\nself-replication, but succeed on many components and are improving rapidly.\nModels can deploy instances from cloud compute providers, write\nself-propagating programs, and exfiltrate model weights under simple security\nsetups, but struggle to pass KYC checks or set up robust and persistent agent\ndeployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50%\npass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20\nfamilies on the hardest variants. These findings suggest autonomous replication\ncapability could soon emerge with improvements in these remaining areas or with\nhuman assistance.", "AI": {"tldr": "RepliBench evaluates autonomous replication risks in language models, finding current models not yet a credible threat but improving rapidly.", "motivation": "To assess the safety risks posed by autonomous replication capabilities of language models.", "method": "Introduces RepliBench, a suite of 86 tasks across 20 families, benchmarking 5 frontier models.", "result": "Models show partial success in replication tasks but struggle with persistent deployments and KYC checks. Claude 3.7 Sonnet performs best.", "conclusion": "Autonomous replication may soon emerge with further improvements or human assistance."}}
{"id": "2504.21561", "pdf": "https://arxiv.org/pdf/2504.21561", "abs": "https://arxiv.org/abs/2504.21561", "authors": ["Pengxiang Li", "Zhi Gao", "Bofei Zhang", "Yapeng Mi", "Xiaojian Ma", "Chenrui Shi", "Tao Yuan", "Yuwei Wu", "Yunde Jia", "Song-Chun Zhu", "Qing Li"], "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning", "categories": ["cs.CV"], "comment": "16 pages, 8 figures", "summary": "Multimodal agents, which integrate a controller (e.g., a large language\nmodel) with external tools, have demonstrated remarkable capabilities in\ntackling complex tasks. However, existing agents need to collect a large number\nof expert data for fine-tuning to adapt to new environments. In this paper, we\npropose an online self-exploration method for multimodal agents, namely SPORT,\nvia step-wise preference optimization to refine the trajectories of agents,\nwhich automatically generates tasks and learns from solving the generated\ntasks, without any expert annotation. SPORT operates through four iterative\ncomponents: task synthesis, step sampling, step verification, and preference\ntuning. First, we synthesize multi-modal tasks using language models. Then, we\nintroduce a novel search scheme, where step sampling and step verification are\nexecuted alternately to solve each generated task. We employ a verifier to\nprovide AI feedback to construct step-wise preference data. The data is\nsubsequently used to update the controller's policy through preference tuning,\nproducing a SPORT Agent. By interacting with real environments, the SPORT Agent\nevolves into a more refined and capable system. Evaluation in the GTA and GAIA\nbenchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements,\nunderscoring the generalization and effectiveness introduced by our method. The\nproject page is https://SPORT-Agents.github.io.", "AI": {"tldr": "SPORT is an online self-exploration method for multimodal agents that refines trajectories via step-wise preference optimization, eliminating the need for expert data.", "motivation": "Existing multimodal agents require extensive expert data for fine-tuning, limiting adaptability to new environments.", "method": "SPORT involves task synthesis, step sampling, step verification, and preference tuning to iteratively improve agent performance.", "result": "SPORT achieves 6.41% and 3.64% improvements in GTA and GAIA benchmarks, demonstrating generalization and effectiveness.", "conclusion": "SPORT enables multimodal agents to autonomously refine capabilities without expert annotation, proving effective in real-world benchmarks."}}
{"id": "2505.01169", "pdf": "https://arxiv.org/pdf/2505.01169", "abs": "https://arxiv.org/abs/2505.01169", "authors": ["Pramook Khungurn", "Pratch Piyawongwisal", "Sira Sriswasdi", "Supasorn Suwajanakorn"], "title": "Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "A flow matching model learns a time-dependent vector field $v_t(x)$ that\ngenerates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates\nbetween a well-known noise distribution ($p_0$) and the data distribution\n($p_1$). It can be distilled into a two-timed flow model (TTFM) $\\phi_{s,x}(t)$\nthat can transform a sample belonging to the distribution at an initial time\n$s$ to another belonging to the distribution at a terminal time $t$ in one\nfunction evaluation. We present a new loss function for TTFM distillation\ncalled the \\emph{initial/terminal velocity matching} (ITVM) loss that extends\nthe Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi et al. by\nadding redundant terms to match the initial velocities at time $s$, removing\nthe derivative from the terminal velocity term at time $t$, and using a version\nof the model under training, stabilized by exponential moving averaging (EMA),\nto compute the target terminal average velocity. Preliminary experiments show\nthat our loss leads to better few-step generation performance on multiple types\nof datasets and model architectures over baselines.", "AI": {"tldr": "The paper introduces a new loss function (ITVM) for distilling a two-timed flow model (TTFM), improving few-step generation performance by matching initial/terminal velocities and stabilizing training with EMA.", "motivation": "To enhance the distillation of flow matching models into TTFMs by addressing limitations of prior methods (e.g., LFMD loss) for better few-step generation.", "method": "Proposes the ITVM loss, which matches initial velocities, simplifies terminal velocity terms, and uses EMA-stabilized models for target computation.", "result": "Preliminary experiments show ITVM outperforms baselines in few-step generation across datasets and architectures.", "conclusion": "The ITVM loss effectively improves TTFM distillation, enabling better performance in generative tasks."}}
{"id": "2504.18916", "pdf": "https://arxiv.org/pdf/2504.18916", "abs": "https://arxiv.org/abs/2504.18916", "authors": ["Sarang S", "Druva Dhakshinamoorthy", "Aditya Shiva Sharma", "Yuvraj Singh Bhadauria", "Siddharth Chaitra Vivek", "Arihant Bansal", "Arnab K. Paul"], "title": "UnifyFL: Enabling Decentralized Cross-Silo Federated Learning", "categories": ["cs.DC", "cs.AI"], "comment": "12 pages, 7 figures, 7 tables. Accepted at the 26th ACM/IFIP\n  International Middleware Conference (MIDDLEWARE 2025)", "summary": "Federated Learning (FL) is a decentralized machine learning (ML) paradigm in\nwhich models are trained on private data across several devices called clients\nand combined at a single node called an aggregator rather than aggregating the\ndata itself. Many organizations employ FL to have better privacy-aware\nML-driven decision-making capabilities. However, organizations often operate\nindependently rather than collaborate to enhance their FL capabilities due to\nthe lack of an effective mechanism for collaboration. The challenge lies in\nbalancing trust and resource efficiency. One approach relies on trusting a\nthird-party aggregator to consolidate models from all organizations (multilevel\nFL), but this requires trusting an entity that may be biased or unreliable.\nAlternatively, organizations can bypass a third party by sharing their local\nmodels directly, which requires significant computational resources for\nvalidation. Both approaches reflect a fundamental trade-off between trust and\nresource constraints, with neither offering an ideal solution. In this work, we\ndevelop a trust-based cross-silo FL framework called UnifyFL, which uses\ndecentralized orchestration and distributed storage. UnifyFL provides\nflexibility to the participating organizations and presents synchronous and\nasynchronous modes to handle stragglers. Our evaluation on a diverse testbed\nshows that UnifyFL achieves a performance comparable to the ideal multilevel\ncentralized FL while allowing trust and optimal use of resources.", "AI": {"tldr": "UnifyFL is a trust-based cross-silo federated learning framework that balances trust and resource efficiency without relying on a third-party aggregator.", "motivation": "Organizations struggle to collaborate in federated learning due to trust and resource constraints, needing a solution that avoids third-party bias and computational inefficiency.", "method": "UnifyFL uses decentralized orchestration and distributed storage, offering synchronous and asynchronous modes to handle stragglers.", "result": "UnifyFL matches the performance of centralized multilevel FL while optimizing trust and resource use.", "conclusion": "UnifyFL provides a flexible, efficient, and trust-aware solution for cross-silo federated learning."}}
{"id": "2505.00507", "pdf": "https://arxiv.org/pdf/2505.00507", "abs": "https://arxiv.org/abs/2505.00507", "authors": ["Esteban Rivera", "Surya Prabhakaran", "Markus Lienkamp"], "title": "HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted in CVPRw2025", "summary": "Active Learning has proved to be a relevant approach to perform sample\nselection for training models for Autonomous Driving. Particularly, previous\nworks on active learning for 3D object detection have shown that selection of\nsamples in uncontrolled scenarios is challenging. Furthermore, current\napproaches focus exclusively on the theoretical aspects of the sample selection\nproblem but neglect the practical insights that can be obtained from the\nextensive literature and application of 3D detection models. In this paper, we\nintroduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection)\nwhich integrates those heuristical features together with Localization and\nClassification to deliver the most contributing samples to the model's\ntraining. In contrast to previous works, our approach integrates heuristical\nfeatures such as object distance and point-quantity to estimate the\nuncertainty, which enhance the usefulness of selected samples to train\ndetection models. Our quantitative evaluation on KITTI shows that HeAL presents\ncompetitive mAP with respect to the State-of-the-Art, and achieves the same mAP\nas the full-supervised baseline with only 24% of the samples.", "AI": {"tldr": "HeAL integrates heuristical features with active learning for 3D object detection, achieving competitive results with fewer samples.", "motivation": "Addressing the challenge of sample selection in uncontrolled scenarios and bridging the gap between theory and practical insights in 3D detection.", "method": "Combines heuristical features (e.g., object distance, point-quantity) with localization and classification for sample selection.", "result": "Achieves same mAP as full-supervised baseline with only 24% of samples on KITTI.", "conclusion": "HeAL is effective for active learning in 3D object detection, offering practical improvements over theoretical-only approaches."}}
{"id": "2505.01420", "pdf": "https://arxiv.org/pdf/2505.01420", "abs": "https://arxiv.org/abs/2505.01420", "authors": ["Mary Phuong", "Roland S. Zimmermann", "Ziyue Wang", "David Lindner", "Victoria Krakovna", "Sarah Cogan", "Allan Dafoe", "Lewis Ho", "Rohin Shah"], "title": "Evaluating Frontier Models for Stealth and Situational Awareness", "categories": ["cs.LG"], "comment": null, "summary": "Recent work has demonstrated the plausibility of frontier AI models scheming\n-- knowingly and covertly pursuing an objective misaligned with its developer's\nintentions. Such behavior could be very hard to detect, and if present in\nfuture advanced systems, could pose severe loss of control risk. It is\ntherefore important for AI developers to rule out harm from scheming prior to\nmodel deployment. In this paper, we present a suite of scheming reasoning\nevaluations measuring two types of reasoning capabilities that we believe are\nprerequisites for successful scheming: First, we propose five evaluations of\nability to reason about and circumvent oversight (stealth). Second, we present\neleven evaluations for measuring a model's ability to instrumentally reason\nabout itself, its environment and its deployment (situational awareness). We\ndemonstrate how these evaluations can be used as part of a scheming inability\nsafety case: a model that does not succeed on these evaluations is almost\ncertainly incapable of causing severe harm via scheming in real deployment. We\nrun our evaluations on current frontier models and find that none of them show\nconcerning levels of either situational awareness or stealth.", "AI": {"tldr": "The paper proposes evaluations to detect AI models' ability to scheme (act against developer intentions) by testing stealth and situational awareness, finding no concerning capabilities in current models.", "motivation": "To address the risk of AI models scheming (misaligned covert behavior) and ensure safety before deployment.", "method": "Introduces 16 evaluations: 5 for stealth (circumventing oversight) and 11 for situational awareness (self/environment reasoning).", "result": "Current frontier models show no concerning levels of stealth or situational awareness.", "conclusion": "The evaluations can help rule out scheming risks, ensuring safer AI deployment."}}
{"id": "2505.00693", "pdf": "https://arxiv.org/pdf/2505.00693", "abs": "https://arxiv.org/abs/2505.00693", "authors": ["Yanbang Li", "Ziyang Gong", "Haoyang Li", "Xiaoqi Huang", "Haolan Kang", "Guangping Bai", "Xianzheng Ma"], "title": "Robotic Visual Instruction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project website: https://robotic-visual-instruction.github.io/", "summary": "Recently, natural language has been the primary medium for human-robot\ninteraction. However, its inherent lack of spatial precision introduces\nchallenges for robotic task definition such as ambiguity and verbosity.\nMoreover, in some public settings where quiet is required, such as libraries or\nhospitals, verbal communication with robots is inappropriate. To address these\nlimitations, we introduce the Robotic Visual Instruction (RoVI), a novel\nparadigm to guide robotic tasks through an object-centric, hand-drawn symbolic\nrepresentation. RoVI effectively encodes spatial-temporal information into\nhuman-interpretable visual instructions through 2D sketches, utilizing arrows,\ncircles, colors, and numbers to direct 3D robotic manipulation. To enable\nrobots to understand RoVI better and generate precise actions based on RoVI, we\npresent Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for\nRoVI-conditioned policies. This approach leverages Vision-Language Models\n(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from\n2D pixel space via keypoint extraction, and then transform them into executable\n3D action sequences. We additionally curate a specialized dataset of 15K\ninstances to fine-tune small VLMs for edge deployment,enabling them to\neffectively learn RoVI capabilities. Our approach is rigorously validated\nacross 11 novel tasks in both real and simulated environments, demonstrating\nsignificant generalization capability. Notably, VIEW achieves an 87.5% success\nrate in real-world scenarios involving unseen tasks that feature multi-step\nactions, with disturbances, and trajectory-following requirements. Project\nwebsite: https://robotic-visual-instruction.github.io/", "AI": {"tldr": "RoVI introduces visual instructions for robots to overcome verbal communication limitations, achieving high success rates in unseen tasks.", "motivation": "Natural language lacks spatial precision and is unsuitable in quiet settings, prompting the need for visual communication.", "method": "RoVI uses 2D sketches to encode spatial-temporal info, paired with VIEW pipeline for action generation via VLMs and keypoint extraction.", "result": "87.5% success rate in real-world tasks, validated across 11 novel tasks with strong generalization.", "conclusion": "RoVI and VIEW provide an effective, generalizable solution for visual robotic task guidance."}}
{"id": "2505.00746", "pdf": "https://arxiv.org/pdf/2505.00746", "abs": "https://arxiv.org/abs/2505.00746", "authors": ["Alexei Kaltchenko"], "title": "Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Vision-language models such as OpenAI GPT-4o can transcribe mathematical\ndocuments directly from images, yet their token-level confidence signals are\nseldom used to pinpoint local recognition mistakes. We present an\nentropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into\na visual ''uncertainty landscape''. By scanning the entropy sequence with a\nfixed-length sliding window, we obtain hotspots that are likely to contain OCR\nerrors such as missing symbols, mismatched braces, or garbled prose. Using a\nsmall, curated set of scanned research pages rendered at several resolutions,\nwe compare the highlighted hotspots with the actual transcription errors\nproduced by GPT-4o. Our analysis shows that the vast majority of true errors\nare indeed concentrated inside the high-entropy regions. This study\ndemonstrates--in a minimally engineered setting--that sliding-window entropy\ncan serve as a practical, lightweight aid for post-editing GPT-based OCR. All\ncode and annotation guidelines are released to encourage replication and\nfurther research.", "AI": {"tldr": "The paper introduces an entropy-heat-mapping method to identify OCR errors in GPT-4o transcriptions by analyzing token-level Shannon entropy.", "motivation": "To improve post-editing of GPT-based OCR by leveraging token-level confidence signals, which are often underutilized.", "method": "Uses a sliding-window approach on per-token Shannon entropy to create a visual 'uncertainty landscape' and identify error hotspots.", "result": "Most true OCR errors (e.g., missing symbols, mismatched braces) are concentrated in high-entropy regions.", "conclusion": "Sliding-window entropy is a lightweight, practical tool for post-editing GPT-based OCR, with code and guidelines released for further research."}}
{"id": "2505.01584", "pdf": "https://arxiv.org/pdf/2505.01584", "abs": "https://arxiv.org/abs/2505.01584", "authors": ["Zhiqiang He", "Zhi Liu"], "title": "Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adapting to non-stationary network conditions presents significant challenges\nfor resource adaptation. However, current solutions primarily rely on\nstationary assumptions. While data-driven reinforcement learning approaches\noffer promising solutions for handling network dynamics, our systematic\ninvestigation reveals a critical limitation: neural networks suffer from\nplasticity loss, significantly impeding their ability to adapt to evolving\nnetwork conditions. Through theoretical analysis of neural propagation\nmechanisms, we demonstrate that existing dormant neuron metrics inadequately\ncharacterize neural plasticity loss. To address this limitation, we have\ndeveloped the Silent Neuron theory, which provides a more comprehensive\nframework for understanding plasticity degradation. Based on these theoretical\ninsights, we propose the Reset Silent Neuron (ReSiN), which preserves neural\nplasticity through strategic neuron resets guided by both forward and backward\npropagation states. In our implementation of an adaptive video streaming\nsystem, ReSiN has shown significant improvements over existing solutions,\nachieving up to 168% higher bitrate and 108% better quality of experience (QoE)\nwhile maintaining comparable smoothness. Furthermore, ReSiN consistently\noutperforms in stationary environments, demonstrating its robust adaptability\nacross different network conditions.", "AI": {"tldr": "The paper addresses neural plasticity loss in reinforcement learning for network adaptation, proposing the Silent Neuron theory and ReSiN method, which improves bitrate and QoE.", "motivation": "Current solutions for network adaptation rely on stationary assumptions, and neural networks suffer from plasticity loss, hindering adaptability to dynamic conditions.", "method": "The paper introduces the Silent Neuron theory and the ReSiN method, which strategically resets neurons based on propagation states to preserve plasticity.", "result": "ReSiN achieves up to 168% higher bitrate and 108% better QoE in adaptive video streaming, outperforming existing solutions.", "conclusion": "ReSiN effectively addresses plasticity loss, demonstrating robust adaptability across varying network conditions."}}
{"id": "2505.01998", "pdf": "https://arxiv.org/pdf/2505.01998", "abs": "https://arxiv.org/abs/2505.01998", "authors": ["Xiaoliang Chen", "Xin Yu", "Le Chang", "Yunhe Huang", "Jiashuai He", "Shibo Zhang", "Jin Li", "Likai Lin", "Ziyu Zeng", "Xianling Tu", "Shuyu Zhang"], "title": "A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction", "categories": ["cs.RO", "cs.AI", "physics.app-ph", "68T01", "I.2.8"], "comment": "34 pages, 11 figures, 10 tables, and 10 equations", "summary": "This paper introduces a novel framework integrating nonlinear acoustic\ncomputing and reinforcement learning to enhance advanced human-robot\ninteraction under complex noise and reverberation. Leveraging physically\ninformed wave equations (e.g., Westervelt, KZK), the approach captures\nhigher-order phenomena such as harmonic generation and shock formation. By\nembedding these models in a reinforcement learning-driven control loop, the\nsystem adaptively optimizes key parameters (e.g., absorption, beamforming) to\nmitigate multipath interference and non-stationary noise. Experimental\nevaluations, covering far-field localization, weak signal detection, and\nmultilingual speech recognition, demonstrate that this hybrid strategy\nsurpasses traditional linear methods and purely data-driven baselines,\nachieving superior noise suppression, minimal latency, and robust accuracy in\ndemanding real-world scenarios. The proposed system demonstrates broad\napplication prospects in AI hardware, robot, machine audition, artificial\naudition, and brain-machine interfaces.", "AI": {"tldr": "A novel framework combining nonlinear acoustic computing and reinforcement learning improves human-robot interaction by optimizing parameters like absorption and beamforming, outperforming traditional methods in noise suppression and accuracy.", "motivation": "To enhance human-robot interaction in noisy, reverberant environments by addressing challenges like multipath interference and non-stationary noise.", "method": "Integrates physically informed wave equations (e.g., Westervelt, KZK) with reinforcement learning to adaptively optimize acoustic parameters.", "result": "Outperforms linear methods and data-driven baselines in noise suppression, latency, and accuracy across tasks like far-field localization and multilingual speech recognition.", "conclusion": "The hybrid framework shows promise for applications in AI hardware, robotics, and brain-machine interfaces."}}
{"id": "2505.02064", "pdf": "https://arxiv.org/pdf/2505.02064", "abs": "https://arxiv.org/abs/2505.02064", "authors": ["Shuhang Xun", "Sicheng Tao", "Jungang Li", "Yibo Shi", "Zhixin Lin", "Zhanhui Zhu", "Yibo Yan", "Hanqian Li", "Linghao Zhang", "Shikang Wang", "Yixin Liu", "Hanbo Zhang", "Ying Ma", "Xuming Hu"], "title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, 5 tables", "summary": "Multimodal Large Language Models (MLLMs) increasingly excel at perception,\nunderstanding, and reasoning. However, current benchmarks inadequately evaluate\ntheir ability to perform these tasks continuously in dynamic, real-world\nenvironments. To bridge this gap, we introduce RTV-Bench, a fine-grained\nbenchmark for MLLM real-time video analysis. RTV-Bench uses three key\nprinciples: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve\nwith scene changes; (2) Hierarchical Question Structure, combining basic and\nadvanced queries; and (3) Multi-dimensional Evaluation, assessing the ability\nof continuous perception, understanding, and reasoning. RTV-Bench contains 552\ndiverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated\nleading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline\n(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,\nInternLM-XComposer2.5-OmniLive) models. Experiment results show open-source\nreal-time models largely outperform offline ones but still trail top\nproprietary models. Our analysis also reveals that larger model size or higher\nframe sampling rates do not significantly boost RTV-Bench performance,\nsometimes causing slight decreases. This underscores the need for better model\narchitectures optimized for video stream processing and long sequences to\nadvance real-time video analysis with MLLMs. Our benchmark toolkit is available\nat: https://github.com/LJungang/RTV-Bench.", "AI": {"tldr": "RTV-Bench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in real-time video analysis, focusing on continuous perception, understanding, and reasoning. It outperforms existing benchmarks and reveals gaps in current models.", "motivation": "Current benchmarks fail to assess MLLMs' ability to perform continuous tasks in dynamic environments, necessitating a more robust evaluation tool.", "method": "RTV-Bench introduces Multi-Timestamp Question Answering (MTQA), Hierarchical Question Structure, and Multi-dimensional Evaluation. It includes 552 videos and 4,631 QA pairs.", "result": "Open-source real-time models outperform offline ones but lag behind proprietary models. Larger models or higher frame rates don't significantly improve performance.", "conclusion": "Better architectures optimized for video streams and long sequences are needed to advance real-time video analysis with MLLMs."}}
{"id": "2505.01959", "pdf": "https://arxiv.org/pdf/2505.01959", "abs": "https://arxiv.org/abs/2505.01959", "authors": ["Leyi Yan", "Linda Wang", "Sihang Liu", "Yi Ding"], "title": "EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting", "categories": ["cs.LG"], "comment": "5 pages, 5 figures, 3 tables, In The 16th ACM International\n  Conference on Future and Sustainable Energy Systems (E-ENERGY'25)", "summary": "Carbon intensity (CI) measures the average carbon emissions generated per\nunit of electricity, making it a crucial metric for quantifying and managing\nthe environmental impact. Accurate CI predictions are vital for minimizing\ncarbon footprints, yet the state-of-the-art method (CarbonCast) falls short due\nto its inability to address regional variability and lack of adaptability. To\naddress these limitations, we introduce EnsembleCI, an adaptive, end-to-end\nensemble learning-based approach for CI forecasting. EnsembleCI combines\nweighted predictions from multiple sublearners, offering enhanced flexibility\nand regional adaptability. In evaluations across 11 regional grids, EnsembleCI\nconsistently surpasses CarbonCast, achieving the lowest mean absolute\npercentage error (MAPE) in almost all grids and improving prediction accuracy\nby an average of 19.58%. While performance still varies across grids due to\ninherent regional diversity, EnsembleCI reduces variability and exhibits\ngreater robustness in long-term forecasting compared to CarbonCast and\nidentifies region-specific key features, underscoring its interpretability and\npractical relevance. These findings position EnsembleCI as a more accurate and\nreliable solution for CI forecasting. EnsembleCI source code and data used in\nthis paper are available at https://github.com/emmayly/EnsembleCI.", "AI": {"tldr": "EnsembleCI, an adaptive ensemble learning method, outperforms CarbonCast in carbon intensity (CI) forecasting by addressing regional variability and improving accuracy by 19.58%.", "motivation": "Existing methods like CarbonCast lack regional adaptability and accuracy in CI predictions, necessitating a more robust solution.", "method": "EnsembleCI uses weighted predictions from multiple sublearners for flexible, region-specific CI forecasting.", "result": "EnsembleCI achieves lower MAPE than CarbonCast in 11 grids, reducing variability and improving long-term robustness.", "conclusion": "EnsembleCI is a more accurate, reliable, and interpretable solution for CI forecasting, with practical relevance."}}
{"id": "2505.02369", "pdf": "https://arxiv.org/pdf/2505.02369", "abs": "https://arxiv.org/abs/2505.02369", "authors": ["Juyoung Yun"], "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "comment": null, "summary": "Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization.", "AI": {"tldr": "ZSharp improves SAM by using layer-wise Z-score normalization and percentile filtering to focus on significant gradient components, enhancing generalization without architectural changes.", "motivation": "Deep neural networks often converge to sharp minima, harming robustness. SAM seeks flatter minima but includes insignificant gradient directions, which ZSharp addresses.", "method": "ZSharp applies layer-wise Z-score normalization and percentile-based filtering to retain statistically significant gradient components, aligning updates with curvature-sensitive directions.", "result": "ZSharp outperforms SAM and variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet, especially with deeper and transformer-based models.", "conclusion": "ZSharp is a lightweight, principled improvement for sharpness-aware optimization, enhancing generalization effectively."}}
{"id": "2505.02648", "pdf": "https://arxiv.org/pdf/2505.02648", "abs": "https://arxiv.org/abs/2505.02648", "authors": ["Mingcheng Li", "Xiaolu Hou", "Ziyang Liu", "Dingkang Yang", "Ziyun Qian", "Jiawei Chen", "Jinjie Wei", "Yue Jiang", "Qingyao Xu", "Lihua Zhang"], "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown excellent performance in text-to-image\ngeneration. Nevertheless, existing methods often suffer from performance\nbottlenecks when handling complex prompts that involve multiple objects,\ncharacteristics, and relations. Therefore, we propose a Multi-agent\nCollaboration-based Compositional Diffusion (MCCD) for text-to-image generation\nfor complex scenes. Specifically, we design a multi-agent collaboration-based\nscene parsing module that generates an agent system comprising multiple agents\nwith distinct tasks, utilizing MLLMs to extract various scene elements\neffectively. In addition, Hierarchical Compositional diffusion utilizes a\nGaussian mask and filtering to refine bounding box regions and enhance objects\nthrough region enhancement, resulting in the accurate and high-fidelity\ngeneration of complex scenes. Comprehensive experiments demonstrate that our\nMCCD significantly improves the performance of the baseline models in a\ntraining-free manner, providing a substantial advantage in complex scene\ngeneration.", "AI": {"tldr": "MCCD improves text-to-image generation for complex scenes using multi-agent collaboration and hierarchical diffusion.", "motivation": "Existing methods struggle with complex prompts involving multiple objects and relations.", "method": "Multi-agent collaboration for scene parsing and hierarchical compositional diffusion with Gaussian masks and filtering.", "result": "MCCD outperforms baseline models in complex scene generation without additional training.", "conclusion": "MCCD offers a robust solution for high-fidelity generation of complex scenes."}}
{"id": "2505.02138", "pdf": "https://arxiv.org/pdf/2505.02138", "abs": "https://arxiv.org/abs/2505.02138", "authors": ["Chenxi Liu", "Hao Miao", "Qianxiong Xu", "Shaowen Zhou", "Cheng Long", "Yan Zhao", "Ziyue Li", "Rui Zhao"], "title": "Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation", "categories": ["cs.LG"], "comment": "Accepted by ICDE 2025", "summary": "Multivariate time series forecasting (MTSF) endeavors to predict future\nobservations given historical data, playing a crucial role in time series data\nmanagement systems. With advancements in large language models (LLMs), recent\nstudies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF.\nHowever, the deployment of LLMs often suffers from low efficiency during the\ninference phase. To address this problem, we introduce TimeKD, an efficient\nMTSF framework that leverages the calibrated language models and privileged\nknowledge distillation. TimeKD aims to generate high-quality future\nrepresentations from the proposed cross-modality teacher model and cultivate an\neffective student model. The cross-modality teacher model adopts calibrated\nlanguage models (CLMs) with ground truth prompts, motivated by the paradigm of\nLearning Under Privileged Information (LUPI). In addition, we design a\nsubtractive cross attention (SCA) mechanism to refine these representations. To\ncultivate an effective student model, we propose an innovative privileged\nknowledge distillation (PKD) mechanism including correlation and feature\ndistillation. PKD enables the student to replicate the teacher's behavior while\nminimizing their output discrepancy. Extensive experiments on real data offer\ninsight into the effectiveness, efficiency, and scalability of the proposed\nTimeKD.", "AI": {"tldr": "TimeKD is an efficient MTSF framework using calibrated language models and knowledge distillation to improve forecasting accuracy and efficiency.", "motivation": "Address the inefficiency of LLMs in MTSF by leveraging knowledge distillation and cross-modality learning.", "method": "Uses a cross-modality teacher model with CLMs and ground truth prompts, SCA for representation refinement, and PKD for student model training.", "result": "Demonstrates effectiveness, efficiency, and scalability in real-world experiments.", "conclusion": "TimeKD offers a promising solution for efficient and accurate MTSF by combining CLMs and knowledge distillation."}}
{"id": "2505.02537", "pdf": "https://arxiv.org/pdf/2505.02537", "abs": "https://arxiv.org/abs/2505.02537", "authors": ["Davide Sartor", "Alberto Sinigaglia", "Gian Antonio Susto"], "title": "Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "International Conference on Machine Learning", "summary": "Conventional techniques for imposing monotonicity in MLPs by construction\ninvolve the use of non-negative weight constraints and bounded activation\nfunctions, which pose well-known optimization challenges. In this work, we\ngeneralize previous theoretical results, showing that MLPs with non-negative\nweight constraint and activations that saturate on alternating sides are\nuniversal approximators for monotonic functions. Additionally, we show an\nequivalence between the saturation side in the activations and the sign of the\nweight constraint. This connection allows us to prove that MLPs with convex\nmonotone activations and non-positive constrained weights also qualify as\nuniversal approximators, in contrast to their non-negative constrained\ncounterparts. Our results provide theoretical grounding to the empirical\neffectiveness observed in previous works while leading to possible\narchitectural simplification. Moreover, to further alleviate the optimization\ndifficulties, we propose an alternative formulation that allows the network to\nadjust its activations according to the sign of the weights. This eliminates\nthe requirement for weight reparameterization, easing initialization and\nimproving training stability. Experimental evaluation reinforces the validity\nof the theoretical results, showing that our novel approach compares favourably\nto traditional monotonic architectures.", "AI": {"tldr": "The paper generalizes theoretical results on MLPs with non-negative weight constraints and alternating saturation activations, proving their universality for monotonic functions. It introduces an equivalence between activation saturation and weight sign, extends results to convex monotone activations with non-positive weights, and proposes a practical alternative to ease optimization.", "motivation": "To address optimization challenges in conventional monotonic MLPs by generalizing theoretical foundations and proposing a simpler, more stable approach.", "method": "Theoretical analysis of MLPs with non-negative weights and alternating saturation activations, extending to convex monotone activations with non-positive weights. A practical formulation adjusts activations based on weight signs.", "result": "Proves universality for monotonic functions under new conditions and shows improved optimization stability with the proposed method.", "conclusion": "The work provides theoretical grounding for empirical observations, simplifies architectures, and enhances training stability, validated by experiments."}}
{"id": "2505.02704", "pdf": "https://arxiv.org/pdf/2505.02704", "abs": "https://arxiv.org/abs/2505.02704", "authors": ["Bojin Wu", "Jing Chen"], "title": "VGLD: Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery", "categories": ["cs.CV"], "comment": "21 pages, conference", "summary": "We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD.", "AI": {"tldr": "VGLD is a method for robust monocular depth scale recovery by integrating image semantics with textual descriptions to stabilize scale recovery.", "motivation": "To address the challenge of recovering absolute depth scale from monocular images, where textual descriptions can introduce ambiguity.", "method": "VGLD combines high-level semantic information from images with textual descriptions to stabilize scale recovery, outputting linear transformation parameters for metric-scale depth.", "result": "Validated on multiple datasets (NYUv2, KITTI) and models (MiDas, DepthAnything), VGLD shows strong performance, even in zero-shot scenarios.", "conclusion": "VGLD serves as a universal alignment module for metric-scale depth recovery, resolving textual ambiguities effectively."}}
{"id": "2505.02380", "pdf": "https://arxiv.org/pdf/2505.02380", "abs": "https://arxiv.org/abs/2505.02380", "authors": ["Arnab Sanyal", "Prithwish Mukherjee", "Gourav Datta", "Sandeep P. Chinchali"], "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices", "categories": ["cs.LG"], "comment": "6 pages, 1 reference page. Under submission and review at ISLPED 2025", "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\nvarious tasks, but their large storage and computational requirements constrain\ntheir deployment on edge devices. To address this, we propose EntroLLM, a novel\ncompression framework that integrates mixed quantization with entropy coding to\nreduce storage overhead while maintaining model accuracy. Our method applies a\nlayer-wise mixed quantization scheme - choosing between symmetric and\nasymmetric quantization based on individual layer weight distributions - to\noptimize compressibility. We then employ Huffman encoding for lossless\ncompression of the quantized weights, significantly reducing memory bandwidth\nrequirements. Furthermore, we introduce parallel Huffman decoding, which\nenables efficient retrieval of encoded weights during inference, ensuring\nminimal latency impact. Our experiments on edge-compatible LLMs, including\nsmolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct,\ndemonstrate that EntroLLM achieves up to $30\\%$ storage reduction compared to\nuint8 models and up to $65%$ storage reduction compared to uint4 models, while\npreserving perplexity and accuracy, on language benchmark tasks. We further\nshow that our method enables $31.9\\%$ - $146.6\\%$ faster inference throughput\non memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by\nreducing the required data movement. The proposed approach requires no\nadditional re-training and is fully compatible with existing post-training\nquantization methods, making it a practical solution for edge LLMs.", "AI": {"tldr": "EntroLLM is a compression framework for LLMs using mixed quantization and entropy coding, reducing storage and improving inference speed on edge devices without accuracy loss.", "motivation": "LLMs' large storage and computational demands limit their deployment on edge devices, necessitating efficient compression methods.", "method": "Combines layer-wise mixed quantization (symmetric/asymmetric) with Huffman encoding for lossless compression and introduces parallel Huffman decoding for efficient inference.", "result": "Achieves 30-65% storage reduction and 31.9-146.6% faster inference on edge devices while maintaining accuracy.", "conclusion": "EntroLLM is a practical, training-free solution for deploying LLMs on edge devices with minimal latency impact."}}
{"id": "2505.02659", "pdf": "https://arxiv.org/pdf/2505.02659", "abs": "https://arxiv.org/abs/2505.02659", "authors": ["Andrey Sidorenko"], "title": "A Note on Statistically Accurate Tabular Data Generation Using Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown promise in synthetic tabular data\ngeneration, yet existing methods struggle to preserve complex feature\ndependencies, particularly among categorical variables. This work introduces a\nprobability-driven prompting approach that leverages LLMs to estimate\nconditional distributions, enabling more accurate and scalable data synthesis.\nThe results highlight the potential of prompting probability distributions to\nenhance the statistical fidelity of LLM-generated tabular data.", "AI": {"tldr": "A probability-driven prompting method improves LLM-generated tabular data by better preserving feature dependencies.", "motivation": "Existing methods for synthetic tabular data generation with LLMs fail to maintain complex feature dependencies, especially among categorical variables.", "method": "Introduces a probability-driven prompting approach to estimate conditional distributions using LLMs.", "result": "The method enhances the statistical fidelity of synthetic tabular data.", "conclusion": "Prompting probability distributions with LLMs shows promise for accurate and scalable data synthesis."}}
{"id": "2505.02825", "pdf": "https://arxiv.org/pdf/2505.02825", "abs": "https://arxiv.org/abs/2505.02825", "authors": ["Alex Hoi Hang Chan", "Otto Brookes", "Urs Waldmann", "Hemal Naik", "Iain D. Couzin", "Majid Mirmehdi", "No\u00ebl Adiko Houa", "Emmanuelle Normand", "Christophe Boesch", "Lukas Boesch", "Mimi Arandjelovic", "Hjalmar K\u00fchl", "Tilo Burghardt", "Fumihiro Kano"], "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology", "categories": ["cs.CV"], "comment": "Accepted at CVPR Workshop, CV4Animals 2025", "summary": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows.", "AI": {"tldr": "The paper advocates for evaluating computer vision models in ecology/biology using application-specific metrics, not just ML metrics, and demonstrates this with case studies on chimpanzee abundance and pigeon head rotation.", "motivation": "Current computer vision resources in ecology/biology focus on ML metrics, neglecting downstream application impact.", "method": "Presented two case studies: chimpanzee abundance estimation using a behavior classifier and pigeon head rotation estimation using a 3D posture estimator.", "result": "Models with strong ML performance (e.g., 87% mAP) led to discrepancies in ecological estimates compared to expert data.", "conclusion": "Researchers should integrate application-specific metrics to better benchmark models for their intended use."}}
{"id": "2505.02737", "pdf": "https://arxiv.org/pdf/2505.02737", "abs": "https://arxiv.org/abs/2505.02737", "authors": ["Gerard Pons", "Besim Bilalli", "Anna Queralt"], "title": "Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": "Pre-print submitted to ISWC 2024", "summary": "Recent advances in Large Language Models (LLMs) have positioned them as a\nprominent solution for Natural Language Processing tasks. Notably, they can\napproach these problems in a zero or few-shot manner, thereby eliminating the\nneed for training or fine-tuning task-specific models. However, LLMs face some\nchallenges, including hallucination and the presence of outdated knowledge or\nmissing information from specific domains in the training data. These problems\ncannot be easily solved by retraining the models with new data as it is a\ntime-consuming and expensive process. To mitigate these issues, Knowledge\nGraphs (KGs) have been proposed as a structured external source of information\nto enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for\nzero-shot Entity Disambiguation (ED). For that purpose, we leverage the\nhierarchical representation of the entities' classes in a KG to gradually prune\nthe candidate space as well as the entities' descriptions to enrich the input\nprompt with additional factual knowledge. Our evaluation on popular ED datasets\nshows that the proposed method outperforms non-enhanced and description-only\nenhanced LLMs, and has a higher degree of adaptability than task-specific\nmodels. Furthermore, we conduct an error analysis and discuss the impact of the\nleveraged KG's semantic expressivity on the ED performance.", "AI": {"tldr": "The paper proposes using Knowledge Graphs (KGs) to enhance Large Language Models (LLMs) for zero-shot Entity Disambiguation (ED), addressing issues like hallucination and outdated knowledge.", "motivation": "LLMs face challenges like hallucination and outdated knowledge, which are hard to fix via retraining. KGs offer structured external knowledge to mitigate these issues.", "method": "Leverages hierarchical class representation and entity descriptions from KGs to prune candidate space and enrich input prompts for LLMs.", "result": "Outperforms non-enhanced and description-only enhanced LLMs, showing higher adaptability than task-specific models.", "conclusion": "KGs effectively enhance LLMs for ED, with performance influenced by the KG's semantic expressivity."}}
{"id": "2411.12516", "pdf": "https://arxiv.org/pdf/2411.12516", "abs": "https://arxiv.org/abs/2411.12516", "authors": ["Anantha S. Rao", "Donovan Buterakos", "Barnaby van Straaten", "Valentin John", "C\u00e9cile X. Yu", "Stefan D. Oosterhout", "Lucas Stehouwer", "Giordano Scappucci", "Menno Veldhorst", "Francesco Borsoi", "Justyna P. Zwolak"], "title": "Modular Autonomous Virtualization System for Two-Dimensional Semiconductor Quantum Dot Arrays", "categories": ["cond-mat.mes-hall", "cs.CV", "cs.ET", "cs.LG", "quant-ph"], "comment": "14 pages, 5 figures, 9 pages of supplemental material", "summary": "Arrays of gate-defined semiconductor quantum dots are among the leading\ncandidates for building scalable quantum processors. High-fidelity\ninitialization, control, and readout of spin qubit registers require exquisite\nand targeted control over key Hamiltonian parameters that define the\nelectrostatic environment. However, due to the tight gate pitch, capacitive\ncrosstalk between gates hinders independent tuning of chemical potentials and\ninterdot couplings. While virtual gates offer a practical solution, determining\nall the required cross-capacitance matrices accurately and efficiently in large\nquantum dot registers is an open challenge. Here, we establish a modular\nautomated virtualization system (MAViS) -- a general and modular framework for\nautonomously constructing a complete stack of multilayer virtual gates in real\ntime. Our method employs machine learning techniques to rapidly extract\nfeatures from two-dimensional charge stability diagrams. We then utilize\ncomputer vision and regression models to self-consistently determine all\nrelative capacitive couplings necessary for virtualizing plunger and barrier\ngates in both low- and high-tunnel-coupling regimes. Using MAViS, we\nsuccessfully demonstrate accurate virtualization of a dense two-dimensional\narray comprising ten quantum dots defined in a high-quality Ge/SiGe\nheterostructure. Our work offers an elegant and practical solution for the\nefficient control of large-scale semiconductor quantum dot systems.", "AI": {"tldr": "MAViS is a modular automated system for virtualizing gates in quantum dot arrays, using machine learning and computer vision to overcome crosstalk challenges.", "motivation": "Capacitive crosstalk in dense quantum dot arrays complicates independent tuning of Hamiltonian parameters, necessitating an efficient virtualization solution.", "method": "MAViS employs machine learning and computer vision to extract features from charge stability diagrams and determine capacitive couplings for virtual gates.", "result": "MAViS successfully virtualized a 10-quantum-dot array in a Ge/SiGe heterostructure, demonstrating accurate control.", "conclusion": "MAViS provides a scalable and practical solution for managing large-scale quantum dot systems efficiently."}}
{"id": "2104.10790", "pdf": "https://arxiv.org/pdf/2104.10790", "abs": "https://arxiv.org/abs/2104.10790", "authors": ["Richard Y. Zhang"], "title": "Sharp Global Guarantees for Nonconvex Low-rank Recovery in the Noisy Overparameterized Regime", "categories": ["math.OC", "cs.LG", "stat.ML"], "comment": "v2 corrects minor typos; v3 complete overhaul with new results on\n  minimax-optimal recovery under noise and asymmetric factorization", "summary": "Recent work established that rank overparameterization eliminates spurious\nlocal minima in nonconvex low-rank matrix recovery under the restricted\nisometry property (RIP). But this does not fully explain the practical success\nof overparameterization, because real algorithms can still become trapped at\nnonstrict saddle points (approximate second-order points with arbitrarily small\nnegative curvature) even when all local minima are global. Moreover, the result\ndoes not accommodate for noisy measurements, but it is unclear whether such an\nextension is even possible, in view of the many discontinuous and unintuitive\nbehaviors already known for the overparameterized regime. In this paper, we\nintroduce a novel proof technique that unifies, simplifies, and strengthens two\npreviously competing approaches -- one based on escape directions and the other\nbased on the inexistence of counterexample -- to provide sharp global\nguarantees in the noisy overparameterized regime. We show, once local minima\nhave been converted into global minima through slight overparameterization,\nthat near-second-order points achieve the same minimax-optimal recovery bounds\n(up to small constant factors) as significantly more expensive convex\napproaches. Our results are sharp with respect to the noise level and the\nsolution accuracy, and hold for both the symmetric parameterization $XX^{T}$,\nas well as the asymmetric parameterization $UV^{T}$ under a balancing\nregularizer; we demonstrate that the balancing regularizer is indeed necessary.", "AI": {"tldr": "Overparameterization eliminates spurious local minima in low-rank matrix recovery, but real algorithms may still get stuck at nonstrict saddle points. This paper unifies and strengthens prior approaches to provide sharp global guarantees for noisy overparameterized regimes, showing near-second-order points achieve minimax-optimal recovery bounds.", "motivation": "To address the gap in understanding why overparameterization works in practice despite potential pitfalls like nonstrict saddle points and noisy measurements.", "method": "Introduces a novel proof technique unifying two prior approaches (escape directions and inexistence of counterexamples) to analyze noisy overparameterized regimes. Examines symmetric ($XX^T$) and asymmetric ($UV^T$) parameterizations with a balancing regularizer.", "result": "Near-second-order points achieve minimax-optimal recovery bounds, comparable to convex approaches. Results are sharp regarding noise level and solution accuracy. The balancing regularizer is proven necessary.", "conclusion": "Overparameterization, combined with the balancing regularizer, ensures robust recovery in noisy settings, bridging theoretical and practical success."}}
{"id": "2411.15702", "pdf": "https://arxiv.org/pdf/2411.15702", "abs": "https://arxiv.org/abs/2411.15702", "authors": ["Bin Chen", "Wenbo Yu", "Qinshan Zhang", "Tianqu Zhuang", "Yong Jiang", "Shu-Tao Xia"], "title": "Editable-DeepSC: Reliable Cross-Modal Semantic Communications for Facial Editing", "categories": ["cs.IT", "cs.CV", "cs.NI", "math.IT"], "comment": null, "summary": "Real-time computer vision (CV) plays a crucial role in various real-world\napplications, whose performance is highly dependent on communication networks.\nNonetheless, the data-oriented characteristics of conventional communications\noften do not align with the special needs of real-time CV tasks. To alleviate\nthis issue, the recently emerged semantic communications only transmit\ntask-related semantic information and exhibit a promising landscape to address\nthis problem. However, the communication challenges associated with Semantic\nFacial Editing, one of the most important real-time CV applications on social\nmedia, still remain largely unexplored. In this paper, we fill this gap by\nproposing Editable-DeepSC, a novel cross-modal semantic communication approach\nfor facial editing. Firstly, we theoretically discuss different transmission\nschemes that separately handle communications and editings, and emphasize the\nnecessity of Joint Editing-Channel Coding (JECC) via iterative attributes\nmatching, which integrates editings into the communication chain to preserve\nmore semantic mutual information. To compactly represent the high-dimensional\ndata, we leverage inversion methods via pre-trained StyleGAN priors for\nsemantic coding. To tackle the dynamic channel noise conditions, we propose\nSNR-aware channel coding via model fine-tuning. Extensive experiments indicate\nthat Editable-DeepSC can achieve superior editings while significantly saving\nthe transmission bandwidth, even under high-resolution and out-of-distribution\n(OOD) settings.", "AI": {"tldr": "Editable-DeepSC is a novel semantic communication approach for real-time facial editing, integrating joint editing-channel coding to save bandwidth and improve performance.", "motivation": "Address the gap in semantic communication for real-time CV tasks, specifically facial editing, by aligning transmission with task needs.", "method": "Proposes Joint Editing-Channel Coding (JECC) and leverages StyleGAN priors for semantic coding, with SNR-aware channel coding for dynamic noise.", "result": "Achieves superior editing quality and bandwidth savings, even in high-resolution and OOD scenarios.", "conclusion": "Editable-DeepSC effectively bridges the gap in semantic communication for facial editing, offering practical benefits for real-time applications."}}
{"id": "2202.11735", "pdf": "https://arxiv.org/pdf/2202.11735", "abs": "https://arxiv.org/abs/2202.11735", "authors": ["Yanglei Song", "Meng zhou"], "title": "Truncated LinUCB for Stochastic Linear Bandits", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "This paper considers contextual bandits with a finite number of arms, where\nthe contexts are independent and identically distributed $d$-dimensional random\nvectors, and the expected rewards are linear in both the arm parameters and\ncontexts. The LinUCB algorithm, which is near minimax optimal for related\nlinear bandits, is shown to have a cumulative regret that is suboptimal in both\nthe dimension $d$ and time horizon $T$, due to its over-exploration. A\ntruncated version of LinUCB is proposed and termed \"Tr-LinUCB\", which follows\nLinUCB up to a truncation time $S$ and performs pure exploitation afterwards.\nThe Tr-LinUCB algorithm is shown to achieve $O(d\\log(T))$ regret if $S =\nCd\\log(T)$ for a sufficiently large constant $C$, and a matching lower bound is\nestablished, which shows the rate optimality of Tr-LinUCB in both $d$ and $T$\nunder a low dimensional regime. Further, if $S = d\\log^{\\kappa}(T)$ for some\n$\\kappa>1$, the loss compared to the optimal is a multiplicative $\\log\\log(T)$\nfactor, which does not depend on $d$. This insensitivity to overshooting in\nchoosing the truncation time of Tr-LinUCB is of practical importance.", "AI": {"tldr": "The paper proposes Tr-LinUCB, a truncated version of LinUCB, to address suboptimal regret in contextual bandits, achieving optimal regret rates in low-dimensional settings.", "motivation": "LinUCB's over-exploration leads to suboptimal regret in contextual bandits. The goal is to improve regret bounds by modifying LinUCB.", "method": "Tr-LinUCB truncates LinUCB's exploration phase after time S, switching to pure exploitation. The truncation time S is carefully chosen to balance exploration and exploitation.", "result": "Tr-LinUCB achieves O(d log(T)) regret when S = Cd log(T), with a matching lower bound proving its optimality. Overshooting S introduces only a loglog(T) factor loss.", "conclusion": "Tr-LinUCB is rate-optimal in low dimensions and robust to truncation time overshooting, making it practical for real-world applications."}}
{"id": "2505.01457", "pdf": "https://arxiv.org/pdf/2505.01457", "abs": "https://arxiv.org/abs/2505.01457", "authors": ["Mingjun Xu", "Zehui Wang", "Hengxing Cai", "Renxin Zhong"], "title": "A Multi-Granularity Retrieval Framework for Visually-Rich Documents", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have predominantly focused on\ntext-based retrieval, limiting their effectiveness in handling visually-rich\ndocuments that encompass text, images, tables, and charts. To bridge this gap,\nwe propose a unified multi-granularity multimodal retrieval framework tailored\nfor two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical\nencoding strategies, modality-aware retrieval mechanisms, and vision-language\nmodel (VLM)-based candidate filtering to effectively capture and utilize the\ncomplex interdependencies between textual and visual modalities. By leveraging\noff-the-shelf vision-language models and implementing a training-free hybrid\nretrieval strategy, our framework demonstrates robust performance without the\nneed for task-specific fine-tuning. Experimental evaluations reveal that\nincorporating layout-aware search and VLM-based candidate verification\nsignificantly enhances retrieval accuracy, achieving a top performance score of\n65.56. This work underscores the potential of scalable and reproducible\nsolutions in advancing multimodal document retrieval systems.", "AI": {"tldr": "A unified multimodal retrieval framework for visually-rich documents, combining hierarchical encoding, modality-aware retrieval, and VLM-based filtering, achieves top performance without task-specific fine-tuning.", "motivation": "Existing RAG systems focus on text, limiting effectiveness for visually-rich documents with text, images, tables, and charts.", "method": "Proposes a framework with hierarchical encoding, modality-aware retrieval, and VLM-based filtering, using off-the-shelf VLMs and a training-free hybrid retrieval strategy.", "result": "Achieves a top performance score of 65.56, with layout-aware search and VLM-based verification enhancing accuracy.", "conclusion": "Demonstrates the potential of scalable, reproducible solutions for advancing multimodal document retrieval systems."}}
{"id": "2304.04497", "pdf": "https://arxiv.org/pdf/2304.04497", "abs": "https://arxiv.org/abs/2304.04497", "authors": ["Yu Hou", "Cong Tran", "Ming Li", "Won-Yong Shin"], "title": "A Unified Framework for Exploratory Learning-Aided Community Detection Under Topological Uncertainty", "categories": ["cs.SI", "cs.IR", "cs.LG", "cs.NE", "cs.NI"], "comment": "17 pages, 9 figures, 8 tables; IEEE Transactions on Network Science\n  and Engineering (to appear) (Please cite our journal version.)", "summary": "In social networks, the discovery of community structures has received\nconsiderable attention as a fundamental problem in various network analysis\ntasks. However, due to privacy concerns or access restrictions, the network\nstructure is often uncertain, thereby rendering established community detection\napproaches ineffective without costly network topology acquisition. To tackle\nthis challenge, we present META-CODE, a unified framework for detecting\noverlapping communities via exploratory learning aided by easy-to-collect node\nmetadata when networks are topologically unknown (or only partially known).\nSpecifically, META-CODE consists of three iterative steps in addition to the\ninitial network inference step: 1) node-level community-affiliation embeddings\nbased on graph neural networks (GNNs) trained by our new reconstruction loss,\n2) network exploration via community-affiliation-based node queries, and 3)\nnetwork inference using an edge connectivity-based Siamese neural network model\nfrom the explored network. Through extensive experiments on three real-world\ndatasets including two large networks, we demonstrate: (a) the superiority of\nMETA-CODE over benchmark community detection methods, achieving remarkable\ngains up to 65.55% on the Facebook dataset over the best competitor among our\nselected competitive methods in terms of normalized mutual information (NMI),\n(b) the impact of each module in META-CODE, (c) the effectiveness of node\nqueries in META-CODE based on empirical evaluations and theoretical findings,\nand (d) the convergence of the inferred network.", "AI": {"tldr": "META-CODE is a framework for detecting overlapping communities in uncertain networks using node metadata and iterative learning, outperforming benchmarks by up to 65.55%.", "motivation": "Existing community detection methods fail in uncertain networks due to privacy or access restrictions, necessitating a new approach.", "method": "META-CODE uses GNNs for embeddings, community-affiliation-based queries, and Siamese neural networks for network inference in three iterative steps.", "result": "Outperforms benchmarks significantly (e.g., 65.55% gain on Facebook dataset) and shows effective convergence.", "conclusion": "META-CODE is a robust solution for community detection in uncertain networks, leveraging metadata and iterative learning."}}
{"id": "2309.15687", "pdf": "https://arxiv.org/pdf/2309.15687", "abs": "https://arxiv.org/abs/2309.15687", "authors": ["Hansika Weerasena", "Prabhat Mishra"], "title": "Breaking On-Chip Communication Anonymity using Flow Correlation Attacks", "categories": ["cs.CR", "cs.AR", "cs.LG"], "comment": null, "summary": "Network-on-Chip (NoC) is widely used to facilitate communication between\ncomponents in sophisticated System-on-Chip (SoC) designs. Security of the\non-chip communication is crucial because exploiting any vulnerability in shared\nNoC would be a goldmine for an attacker that puts the entire computing\ninfrastructure at risk. We investigate the security strength of existing\nanonymous routing protocols in NoC architectures, making two pivotal\ncontributions. Firstly, we develop and perform a machine learning (ML)-based\nflow correlation attack on existing anonymous routing techniques in\nNetwork-on-Chip (NoC) systems, revealing that they provide only packet-level\nanonymity. Secondly, we propose a novel, lightweight anonymous routing protocol\nfeaturing outbound traffic tunneling and traffic obfuscation. This protocol is\ndesigned to provide robust defense against ML-based flow correlation attacks,\nensuring both packet-level and flow-level anonymity. Experimental evaluation\nusing both real and synthetic traffic demonstrates that our proposed attack\nsuccessfully deanonymizes state-of-the-art anonymous routing in NoC\narchitectures with high accuracy (up to 99%) for diverse traffic patterns. It\nalso reveals that our lightweight anonymous routing protocol can defend against\nML-based attacks with minor hardware and performance overhead.", "AI": {"tldr": "The paper evaluates NoC security, revealing vulnerabilities in existing anonymous routing protocols and proposing a lightweight solution resistant to ML-based attacks.", "motivation": "Security in NoC is critical due to shared communication risks; existing anonymous routing lacks robustness against advanced attacks.", "method": "Developed an ML-based flow correlation attack and proposed a new protocol with traffic tunneling and obfuscation.", "result": "Existing protocols only offer packet-level anonymity; the new protocol defends against ML attacks with minimal overhead.", "conclusion": "The proposed lightweight protocol enhances NoC security by ensuring flow-level anonymity against ML-based threats."}}
{"id": "2311.00579", "pdf": "https://arxiv.org/pdf/2311.00579", "abs": "https://arxiv.org/abs/2311.00579", "authors": ["Hansika Weerasena", "Prabhat Mishra"], "title": "Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators", "categories": ["cs.CR", "cs.AR", "cs.LG"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) are widely used in various domains,\nincluding image recognition, medical diagnosis and autonomous driving. Recent\nadvances in dataflow-based CNN accelerators have enabled CNN inference in\nresource-constrained edge devices. These dataflow accelerators utilize inherent\ndata reuse of convolution layers to process CNN models efficiently. Concealing\nthe architecture of CNN models is critical for privacy and security. This\narticle evaluates memory-based side-channel information to recover CNN\narchitectures from dataflow-based CNN inference accelerators. The proposed\nattack exploits spatial and temporal data reuse of the dataflow mapping on CNN\naccelerators and architectural hints to recover the structure of CNN models.\nExperimental results demonstrate that our proposed side-channel attack can\nrecover the structures of popular CNN models, namely, Lenet, Alexnet, VGGnet16,\nand YOLOv2.", "AI": {"tldr": "The paper proposes a side-channel attack to recover CNN architectures from dataflow-based accelerators, exploiting data reuse and architectural hints.", "motivation": "Protecting CNN model architectures is crucial for privacy and security, but dataflow accelerators may leak information.", "method": "The attack leverages spatial and temporal data reuse in dataflow mappings and architectural hints to recover CNN structures.", "result": "The attack successfully recovers structures of popular CNN models like Lenet, Alexnet, VGGnet16, and YOLOv2.", "conclusion": "The study highlights vulnerabilities in dataflow-based CNN accelerators, emphasizing the need for robust security measures."}}
{"id": "2403.13748", "pdf": "https://arxiv.org/pdf/2403.13748", "abs": "https://arxiv.org/abs/2403.13748", "authors": ["Charles C. Margossian", "Loucas Pillaud-Vivien", "Lawrence K. Saul"], "title": "Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs", "categories": ["stat.ML", "cs.LG", "stat.CO"], "comment": null, "summary": "Given an intractable distribution $p$, the problem of variational inference\n(VI) is to find the best approximation from some more tractable family $Q$.\nCommonly, one chooses $Q$ to be a family of factorized distributions (i.e., the\nmean-field assumption), even though~$p$ itself does not factorize. We show that\nthis mismatch leads to an impossibility theorem: if $p$ does not factorize,\nthen any factorized approximation $q\\in Q$ can correctly estimate at most one\nof the following three measures of uncertainty: (i) the marginal variances,\n(ii) the marginal precisions, or (iii) the generalized variance (which can be\nrelated to the entropy). In practice, the best variational approximation in $Q$\nis found by minimizing some divergence $D(q,p)$ between distributions, and so\nwe ask: how does the choice of divergence determine which measure of\nuncertainty, if any, is correctly estimated by VI? We consider the classic\nKullback-Leibler divergences, the more general $\\alpha$-divergences, and a\nscore-based divergence which compares $\\nabla \\log p$ and $\\nabla \\log q$. We\nprovide a thorough theoretical analysis in the setting where $p$ is a Gaussian\nand $q$ is a (factorized) Gaussian. We show that all the considered divergences\ncan be \\textit{ordered} based on the estimates of uncertainty they yield as\nobjective functions for~VI. Finally, we empirically evaluate the validity of\nthis ordering when the target distribution $p$ is not Gaussian.", "AI": {"tldr": "The paper shows that factorized approximations in variational inference (VI) can correctly estimate only one of three uncertainty measures if the target distribution doesn't factorize. It analyzes how the choice of divergence affects this estimation and orders divergences based on their performance.", "motivation": "To understand the limitations of factorized approximations in VI and how the choice of divergence impacts the accuracy of uncertainty measures.", "method": "Theoretical analysis using Gaussian distributions for both target and factorized approximations, considering KL divergences, \u03b1-divergences, and score-based divergence. Empirical evaluation extends to non-Gaussian cases.", "result": "Factorized approximations can only correctly estimate one uncertainty measure (marginal variances, precisions, or generalized variance). Divergences can be ordered based on their performance in VI.", "conclusion": "The choice of divergence in VI determines which uncertainty measure is accurately estimated, with a clear ordering of divergences emerging from the analysis."}}
{"id": "2405.15357", "pdf": "https://arxiv.org/pdf/2405.15357", "abs": "https://arxiv.org/abs/2405.15357", "authors": ["Fabio Feser", "Marina Evangelou"], "title": "Strong Screening Rules for Group-based SLOPE Models", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "33 pages, 24 figures, 12 tables", "summary": "Tuning the regularization parameter in penalized regression models is an\nexpensive task, requiring multiple models to be fit along a path of parameters.\nStrong screening rules drastically reduce computational costs by lowering the\ndimensionality of the input prior to fitting. We develop strong screening rules\nfor group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE\nand Sparse-group SLOPE. The developed rules are applicable to the wider family\nof group-based OWL models, including OSCAR. Our experiments on both synthetic\nand real data show that the screening rules significantly accelerate the\nfitting process. The screening rules make it accessible for group SLOPE and\nsparse-group SLOPE to be applied to high-dimensional datasets, particularly\nthose encountered in genetics.", "AI": {"tldr": "Strong screening rules for Group SLOPE and Sparse-group SLOPE reduce computational costs in penalized regression, enabling high-dimensional applications like genetics.", "motivation": "Penalized regression models require costly tuning of regularization parameters; strong screening rules can lower dimensionality and speed up fitting.", "method": "Developed strong screening rules for Group SLOPE and Sparse-group SLOPE, applicable to group-based OWL models like OSCAR.", "result": "Experiments on synthetic and real data show significant acceleration in fitting, making high-dimensional datasets feasible.", "conclusion": "Screening rules enhance accessibility of Group SLOPE and Sparse-group SLOPE for high-dimensional data, especially in genetics."}}
{"id": "2408.03085", "pdf": "https://arxiv.org/pdf/2408.03085", "abs": "https://arxiv.org/abs/2408.03085", "authors": ["Jiaqi Yao", "Tianjian Huang", "Ding Liu"], "title": "Universal Matrix Multiplication on Quantum Computer", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "As a core underlying operation in pattern recognition and machine learning,\nmatrix multiplication plays a crucial role in modern machine learning models\nand constitutes a major contributor to computational expenditure. Hence,\nresearchers have spent decades continuously searching for more efficient matrix\nmultiplication algorithms.This paper firstly introduces an innovative and\npractical approach to universal quantum matrix multiplication. We designed\noptimized quantum adders and multipliers based on Quantum Fourier Transform\n(QFT), which significantly reduced the number of gates used compared to\nclassical adders and multipliers. Subsequently, we construct the basic\nuniversal quantum matrix multiplication and extend it to the Strassen\nalgorithm. We conduct comparative experiments to analyze the performance of the\nquantum matrix multiplication and evaluate the acceleration provided by the\noptimized quantum adder and multiplier. Finally, we investigate the advantages\nof the quantum Strassen algorithm and the basic quantum matrix multiplication.\nOur result opens, for the first time, a reliable pathway for designing\nuniversal quantum matrix multiplication. Following this pathway, quantum\ncomputing will unlock significantly greater potential for training modern\nmachine learning models.", "AI": {"tldr": "This paper introduces a quantum approach to matrix multiplication, optimizing quantum adders and multipliers using QFT, and extends it to the Strassen algorithm, showing significant computational advantages.", "motivation": "Matrix multiplication is computationally expensive in machine learning, and quantum computing offers potential for efficiency improvements.", "method": "Designed optimized quantum adders and multipliers using QFT, extended to quantum Strassen algorithm, and conducted comparative experiments.", "result": "Reduced gate usage and demonstrated computational advantages of quantum matrix multiplication.", "conclusion": "The work provides a reliable pathway for quantum matrix multiplication, enhancing potential for quantum machine learning."}}
{"id": "2409.03505", "pdf": "https://arxiv.org/pdf/2409.03505", "abs": "https://arxiv.org/abs/2409.03505", "authors": ["Zhuoxin Chen", "Will Ma"], "title": "Survey of Data-driven Newsvendor: Unified Analysis and Spectrum of Achievable Regrets", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "In the Newsvendor problem, the goal is to guess the number that will be drawn\nfrom some distribution, with asymmetric consequences for guessing too high vs.\ntoo low. In the data-driven version, the distribution is unknown, and one must\nwork with samples from the distribution. Data-driven Newsvendor has been\nstudied under many variants: additive vs. multiplicative regret, high\nprobability vs. expectation bounds, and different distribution classes. This\npaper studies all combinations of these variants, filling in many gaps in the\nliterature and simplifying many proofs. In particular, we provide a unified\nanalysis based on the notion of clustered distributions, which in conjunction\nwith our new lower bounds, shows that the entire spectrum of regrets between\n$1/\\sqrt{n}$ and $1/n$ can be possible. Simulations on commonly-used\ndistributions demonstrate that our notion is the \"correct\" predictor of\nempirical regret across varying data sizes.", "AI": {"tldr": "The paper provides a unified analysis of the data-driven Newsvendor problem, covering all variants and simplifying proofs, while introducing the concept of clustered distributions to explain the spectrum of regrets.", "motivation": "To address gaps in the literature and unify the analysis of the data-driven Newsvendor problem under various regret and distribution assumptions.", "method": "Uses the notion of clustered distributions for a unified analysis, supported by new lower bounds and simulations on common distributions.", "result": "Shows that the spectrum of regrets between $1/\\sqrt{n}$ and $1/n$ is possible, with simulations validating the approach.", "conclusion": "The concept of clustered distributions effectively predicts empirical regret, filling literature gaps and simplifying proofs."}}
{"id": "2410.02208", "pdf": "https://arxiv.org/pdf/2410.02208", "abs": "https://arxiv.org/abs/2410.02208", "authors": ["Omar Melikechi", "David B. Dunson", "Jeffrey W. Miller"], "title": "Nonparametric IPSS: Fast, flexible feature selection with false discovery control", "categories": ["stat.ML", "cs.LG", "stat.AP", "stat.ME"], "comment": null, "summary": "Feature selection is a critical task in machine learning and statistics.\nHowever, existing feature selection methods either (i) rely on parametric\nmethods such as linear or generalized linear models, (ii) lack theoretical\nfalse discovery control, or (iii) identify few true positives. Here, we\nintroduce a general feature selection method with finite-sample false discovery\ncontrol based on applying integrated path stability selection (IPSS) to\narbitrary feature importance scores. The method is nonparametric whenever the\nimportance scores are nonparametric, and it estimates q-values, which are\nbetter suited to high-dimensional data than p-values. We focus on two special\ncases using importance scores from gradient boosting (IPSSGB) and random\nforests (IPSSRF). Extensive nonlinear simulations with RNA sequencing data show\nthat both methods accurately control the false discovery rate and detect more\ntrue positives than existing methods. Both methods are also efficient, running\nin under 20 seconds when there are 500 samples and 5000 features. We apply\nIPSSGB and IPSSRF to detect microRNAs and genes related to cancer, finding that\nthey yield better predictions with fewer features than existing approaches.", "AI": {"tldr": "A nonparametric feature selection method (IPSS) with false discovery control is introduced, outperforming existing methods in accuracy and efficiency.", "motivation": "Existing feature selection methods are limited by parametric assumptions, lack of false discovery control, or low true positive rates.", "method": "IPSS applies integrated path stability selection to arbitrary feature importance scores, estimating q-values for high-dimensional data. Special cases include gradient boosting (IPSSGB) and random forests (IPSSRF).", "result": "IPSSGB and IPSSRF control false discovery rates, detect more true positives, and run efficiently (under 20s for 500 samples and 5000 features).", "conclusion": "IPSSGB and IPSSRF improve predictions with fewer features, demonstrated in cancer-related microRNA and gene detection."}}
{"id": "2410.10098", "pdf": "https://arxiv.org/pdf/2410.10098", "abs": "https://arxiv.org/abs/2410.10098", "authors": ["Jung-hun Kim", "Min-hwan Oh"], "title": "Queueing Matching Bandits with Preference Feedback", "categories": ["stat.ML", "cs.LG"], "comment": "NeurIPS 2024", "summary": "In this study, we consider multi-class multi-server asymmetric queueing\nsystems consisting of $N$ queues on one side and $K$ servers on the other side,\nwhere jobs randomly arrive in queues at each time. The service rate of each\njob-server assignment is unknown and modeled by a feature-based Multi-nomial\nLogit (MNL) function. At each time, a scheduler assigns jobs to servers, and\neach server stochastically serves at most one job based on its preferences over\nthe assigned jobs. The primary goal of the algorithm is to stabilize the queues\nin the system while learning the service rates of servers. To achieve this\ngoal, we propose algorithms based on UCB and Thompson Sampling, which achieve\nsystem stability with an average queue length bound of\n$O(\\min\\{N,K\\}/\\epsilon)$ for a large time horizon $T$, where $\\epsilon$ is a\ntraffic slackness of the system. Furthermore, the algorithms achieve sublinear\nregret bounds of $\\tilde{O}(\\min\\{\\sqrt{T} Q_{\\max},T^{3/4}\\})$, where\n$Q_{\\max}$ represents the maximum queue length over agents and times. Lastly,\nwe provide experimental results to demonstrate the performance of our\nalgorithms.", "AI": {"tldr": "The paper proposes UCB and Thompson Sampling-based algorithms to stabilize multi-class multi-server queueing systems while learning unknown service rates, achieving bounded queue lengths and sublinear regret.", "motivation": "The study aims to address the challenge of stabilizing queueing systems with unknown job-server service rates, ensuring efficient scheduling and learning simultaneously.", "method": "The authors use feature-based Multi-nomial Logit (MNL) models for service rates and propose algorithms based on UCB and Thompson Sampling for job-server assignment.", "result": "The algorithms achieve system stability with an average queue length bound of O(min{N,K}/\u03b5) and sublinear regret bounds of \u00d5(min{\u221aT Q_max, T^{3/4}}).", "conclusion": "The proposed algorithms effectively stabilize the system and learn service rates, with experimental results validating their performance."}}
{"id": "2410.21374", "pdf": "https://arxiv.org/pdf/2410.21374", "abs": "https://arxiv.org/abs/2410.21374", "authors": ["Aseem Paranjape", "Ravi K. Sheth"], "title": "Model-agnostic basis functions for the 2-point correlation function of dark matter in linear theory", "categories": ["astro-ph.CO", "cs.LG"], "comment": "25 pages, 11 figures, v2: added discussion and clarifications in\n  response to referee comments; conclusions unchanged; accepted in JCAP. The\n  implementation of the BiSequential architecture, along with a simple example\n  notebook, is publicly available as part of the MLFundas repository at\n  https://github.com/a-paranjape/mlfundas", "summary": "We consider approximating the linearly evolved 2-point correlation function\n(2pcf) of dark matter $\\xi_{\\rm lin}(r;\\boldsymbol{\\theta})$ in a cosmological\nmodel with parameters $\\boldsymbol{\\theta}$ as the linear combination $\\xi_{\\rm\nlin}(r;\\boldsymbol{\\theta})\\approx\\sum_i\\,b_i(r)\\,w_i(\\boldsymbol{\\theta})$,\nwhere the functions $\\mathcal{B}=\\{b_i(r)\\}$ form a $\\textit{model-agnostic\nbasis}$ for the linear 2pcf. This decomposition is important for model-agnostic\nanalyses of the baryon acoustic oscillation (BAO) feature in the nonlinear 2pcf\nof galaxies that fix $\\mathcal{B}$ and leave the coefficients $\\{w_i\\}$ free.\nTo date, such analyses have made simple but sub-optimal choices for\n$\\mathcal{B}$, such as monomials. We develop a machine learning framework for\nsystematically discovering a $\\textit{minimal}$ basis $\\mathcal{B}$ that\ndescribes $\\xi_{\\rm lin}(r)$ near the BAO feature in a wide class of\ncosmological models. We use a custom architecture, denoted\n$\\texttt{BiSequential}$, for a neural network (NN) that explicitly realizes the\nseparation between $r$ and $\\boldsymbol{\\theta}$ above. The optimal NN trained\non data in which only $\\{\\Omega_{\\rm m},h\\}$ are varied in a $\\textit{flat}$\n$\\Lambda$CDM model produces a basis $\\mathcal{B}$ comprising $9$ functions\ncapable of describing $\\xi_{\\rm lin}(r)$ to $\\sim0.6\\%$ accuracy in\n$\\textit{curved}$ $w$CDM models varying 7 parameters within $\\sim5\\%$ of their\nfiducial, flat $\\Lambda$CDM values. Scales such as the peak, linear point and\nzero-crossing of $\\xi_{\\rm lin}(r)$ are also recovered with very high accuracy.\nWe compare our approach to other compression schemes in the literature, and\nspeculate that $\\mathcal{B}$ may also encompass $\\xi_{\\rm lin}(r)$ in modified\ngravity models near our fiducial $\\Lambda$CDM model. Using our basis functions\nin model-agnostic BAO analyses can potentially lead to significant statistical\ngains.", "AI": {"tldr": "A machine learning framework is developed to find a minimal basis for approximating the linearly evolved 2-point correlation function (2pcf) of dark matter, improving model-agnostic baryon acoustic oscillation (BAO) analyses.", "motivation": "Current methods for model-agnostic BAO analyses use sub-optimal bases (e.g., monomials) for approximating the 2pcf. A more efficient and accurate basis is needed.", "method": "A custom neural network architecture (BiSequential) is used to discover a minimal basis for the 2pcf, trained on data varying cosmological parameters.", "result": "The optimal basis consists of 9 functions, achieving ~0.6% accuracy in describing the 2pcf across a range of cosmological models, including curved wCDM. Key scales (peak, linear point, zero-crossing) are accurately recovered.", "conclusion": "The proposed basis can enhance model-agnostic BAO analyses, offering significant statistical improvements and potential applicability to modified gravity models."}}
{"id": "2411.06762", "pdf": "https://arxiv.org/pdf/2411.06762", "abs": "https://arxiv.org/abs/2411.06762", "authors": ["Yuzhou Zhang", "Mohan Hua", "Jinan Liu", "Haihui Ruan"], "title": "Precision Glass Thermoforming Assisted by Neural Networks", "categories": ["cs.CE", "cs.LG"], "comment": null, "summary": "Many glass products require thermoformed geometry with high precision.\nHowever, the traditional approach of developing a thermoforming process through\ntrials and errors can cause large waste of time and resources and often end up\nwith unsuccessfulness. Hence, there is a need to develop an efficient\npredictive model, replacing the costly simulations or experiments, to assist\nthe design of precision glass thermoforming. In this work, we report a\nsurrogate model, based on a dimensionless back-propagation neural network\n(BPNN), that can adequately predict the form errors and thus compensate for\nthese errors in mold design using geometric features and process parameters as\ninputs. Our trials with simulation and industrial data indicate that the\nsurrogate model can predict forming errors with adequate accuracy. Although\nperception errors (mold designers' decisions) and mold fabrication errors make\nthe industrial training data less reliable than simulation data, our\npreliminary training and testing results still achieved a reasonable\nconsistency with industrial data, suggesting that the surrogate models are\ndirectly implementable in the glass-manufacturing industry.", "AI": {"tldr": "A surrogate model using a dimensionless BPNN predicts form errors in precision glass thermoforming, reducing reliance on costly trials and simulations.", "motivation": "Traditional trial-and-error methods in glass thermoforming waste time and resources, necessitating an efficient predictive model.", "method": "A dimensionless back-propagation neural network (BPNN) is developed to predict form errors using geometric features and process parameters.", "result": "The model accurately predicts forming errors in simulations and shows reasonable consistency with industrial data despite reliability issues.", "conclusion": "The surrogate model is viable for industrial implementation, offering a practical solution for precision glass thermoforming."}}
{"id": "2412.03252", "pdf": "https://arxiv.org/pdf/2412.03252", "abs": "https://arxiv.org/abs/2412.03252", "authors": ["Nozomu Masuya", "Hiroshi Sato", "Koki Yamane", "Takuya Kusume", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Variable-Speed Teaching-Playback as Real-World Data Augmentation for Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": "16 pages, 12 figures, 4 tables. This is a preprint of an article\n  whose final and definitive form has been published in ADVANCED ROBOTICS 2025,\n  copyright Taylor & Francis and Robotics Society of Japan, is available online\n  at: http://www.tandfonline.com/10.1080/01691864.2025.2497423;\n  doi:10.1080/01691864.2025.2497423", "summary": "Because imitation learning relies on human demonstrations in hard-to-simulate\nsettings, the inclusion of force control in this method has resulted in a\nshortage of training data, even with a simple change in speed. Although the\nfield of data augmentation has addressed the lack of data, conventional methods\nof data augmentation for robot manipulation are limited to simulation-based\nmethods or downsampling for position control. This paper proposes a novel\nmethod of data augmentation that is applicable to force control and preserves\nthe advantages of real-world datasets. We applied teaching-playback at variable\nspeeds as real-world data augmentation to increase both the quantity and\nquality of environmental reactions at variable speeds. An experiment was\nconducted on bilateral control-based imitation learning using a method of\nimitation learning equipped with position-force control. We evaluated the\neffect of real-world data augmentation on two tasks, pick-and-place and wiping,\nat variable speeds, each from two human demonstrations at fixed speed. The\nresults showed a maximum 55% increase in success rate from a simple change in\nspeed of real-world reactions and improved accuracy along the\nduration/frequency command by gathering environmental reactions at variable\nspeeds.", "AI": {"tldr": "The paper proposes a real-world data augmentation method for force control in imitation learning, improving success rates and accuracy by varying playback speeds.", "motivation": "Imitation learning lacks sufficient training data for force control, especially with speed variations, and existing data augmentation methods are limited to simulation or position control.", "method": "The authors introduce teaching-playback at variable speeds as real-world data augmentation, applied to bilateral control-based imitation learning with position-force control.", "result": "Experiments on pick-and-place and wiping tasks showed a 55% success rate increase and improved accuracy by gathering environmental reactions at variable speeds.", "conclusion": "Real-world data augmentation for force control enhances imitation learning performance by leveraging variable-speed demonstrations."}}
{"id": "2501.08423", "pdf": "https://arxiv.org/pdf/2501.08423", "abs": "https://arxiv.org/abs/2501.08423", "authors": ["William Cole Nockolds", "C. G. Krishnanunni", "Tan Bui-Thanh", "Xianxhu Tang"], "title": "Lilan: A linear latent network approach for real-time solutions of stiff, nonlinear, ordinary differential equations", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Solving stiff ordinary differential equations (StODEs) requires sophisticated\nnumerical solvers, which are often computationally expensive. In particular,\nStODE's often cannot be solved with traditional explicit time integration\nschemes and one must resort to costly implicit methods to compute solutions. On\nthe other hand, state-of-the-art machine learning (ML) based methods such as\nNeural ODE (NODE) poorly handle the timescale separation of various elements of\nthe solutions to StODEs and require expensive implicit solvers for integration\nat inference time. In this work, we embark on a different path which involves\nlearning a latent dynamics for StODEs, in which one completely avoids numerical\nintegration. To that end, we consider a constant velocity latent dynamical\nsystem whose solution is a sequence of straight lines. Given the initial\ncondition and parameters of the ODE, the encoder networks learn the slope (i.e\nthe constant velocity) and the initial condition for the latent dynamics. In\nother words, the solution of the original dynamics is encoded into a sequence\nof straight lines which can be decoded back to retrieve the actual solution as\nand when required. Another key idea in our approach is a nonlinear\ntransformation of time, which allows for the \"stretching/squeezing\" of time in\nthe latent space, thereby allowing for varying levels of attention to different\ntemporal regions in the solution. Additionally, we provide a simple\nuniversal-approximation-type proof showing that our approach can approximate\nthe solution of stiff nonlinear system on a compact set to any degree of\naccuracy, {\\epsilon}. We show that the dimension of the latent dynamical system\nin our approach is independent of {\\epsilon}. Numerical investigation on\nprototype StODEs suggest that our method outperforms state-of-the art machine\nlearning approaches for handling StODEs.", "AI": {"tldr": "The paper proposes a novel method for solving stiff ODEs by learning a latent dynamics system with constant velocity, avoiding costly numerical integration.", "motivation": "Traditional and ML-based methods for solving stiff ODEs are computationally expensive or ineffective. The paper aims to bypass numerical integration by encoding solutions into a simpler latent space.", "method": "The approach involves learning a latent dynamics system with constant velocity and a nonlinear time transformation. Encoder networks predict the slope and initial conditions for the latent system.", "result": "The method outperforms state-of-the-art ML approaches for stiff ODEs, with a proof showing it can approximate solutions to any accuracy.", "conclusion": "The proposed latent dynamics approach offers an efficient alternative to traditional and ML-based methods for solving stiff ODEs, avoiding numerical integration costs."}}
{"id": "2501.18991", "pdf": "https://arxiv.org/pdf/2501.18991", "abs": "https://arxiv.org/abs/2501.18991", "authors": ["Gauthier Thurin", "Kimia Nadjahi", "Claire Boyer"], "title": "Optimal Transport-based Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Conformal Prediction (CP) is a principled framework for quantifying\nuncertainty in blackbox learning models, by constructing prediction sets with\nfinite-sample coverage guarantees. Traditional approaches rely on scalar\nnonconformity scores, which fail to fully exploit the geometric structure of\nmultivariate outputs, such as in multi-output regression or multiclass\nclassification. Recent methods addressing this limitation impose predefined\nconvex shapes for the prediction sets, potentially misaligning with the\nintrinsic data geometry. We introduce a novel CP procedure handling\nmultivariate score functions through the lens of optimal transport.\nSpecifically, we leverage Monge-Kantorovich vector ranks and quantiles to\nconstruct prediction region with flexible, potentially non-convex shapes,\nbetter suited to the complex uncertainty patterns encountered in multivariate\nlearning tasks. We prove that our approach ensures finite-sample,\ndistribution-free coverage properties, similar to typical CP methods. We then\nadapt our method for multi-output regression and multiclass classification, and\nalso propose simple adjustments to generate adaptive prediction regions with\nasymptotic conditional coverage guarantees. Finally, we evaluate our method on\npractical regression and classification problems, illustrating its advantages\nin terms of (conditional) coverage and efficiency.", "AI": {"tldr": "A novel Conformal Prediction (CP) method using optimal transport for multivariate outputs, ensuring flexible prediction sets with finite-sample coverage guarantees.", "motivation": "Traditional CP methods fail to exploit geometric structure in multivariate outputs, and existing approaches impose rigid convex shapes misaligned with data geometry.", "method": "Leverages Monge-Kantorovich vector ranks and quantiles for flexible, non-convex prediction sets, ensuring finite-sample coverage.", "result": "Proves coverage guarantees, adapts to multi-output regression and classification, and achieves better (conditional) coverage and efficiency.", "conclusion": "The method outperforms traditional CP approaches by aligning with intrinsic data geometry and providing adaptive prediction regions."}}
{"id": "2502.00798", "pdf": "https://arxiv.org/pdf/2502.00798", "abs": "https://arxiv.org/abs/2502.00798", "authors": ["Qiangqiang Gu", "Shishir Kumar Pandey", "Zhanghao Zhouyin"], "title": "Deep Neural Network for Phonon-Assisted Optical Spectra in Semiconductors", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": "6 pages, 5 figures", "summary": "Ab initio based accurate simulation of phonon-assisted optical spectra of\nsemiconductors at finite temperatures remains a formidable challenge, as it\nrequires large supercells for phonon sampling and computationally expensive\nhigh-accuracy exchange-correlation (XC) functionals. In this work, we present\nan efficient approach that combines deep learning tight-binding and potential\nmodels to address this challenge with ab initio fidelity. By leveraging\nmolecular dynamics for atomic configuration sampling and deep learning-enabled\nrapid Hamiltonian evaluation, our approach enables large-scale simulations of\ntemperature-dependent optical properties using advanced XC functionals (HSE,\nSCAN). Demonstrated on silicon and gallium arsenide across temperature 100-400\nK, the method accurately captures phonon-induced bandgap renormalization and\nindirect/direct absorption processes which are in excellent agreement with\nexperimental findings over five orders of magnitude. This work establishes a\npathway for high-throughput investigation of electron-phonon coupled phenomena\nin complex materials, overcoming traditional computational limitations arising\nfrom large supercell used with computationally expensive XC-functionals.", "AI": {"tldr": "An efficient deep learning-based method for simulating phonon-assisted optical spectra in semiconductors at finite temperatures, overcoming computational challenges.", "motivation": "Accurate simulation of phonon-assisted optical spectra in semiconductors is computationally demanding due to large supercells and expensive XC functionals.", "method": "Combines deep learning tight-binding and potential models with molecular dynamics for atomic configuration sampling and rapid Hamiltonian evaluation.", "result": "Accurately captures phonon-induced bandgap renormalization and absorption processes in silicon and gallium arsenide, matching experimental data.", "conclusion": "Provides a pathway for high-throughput study of electron-phonon phenomena in complex materials, bypassing traditional computational limits."}}
{"id": "2502.03435", "pdf": "https://arxiv.org/pdf/2502.03435", "abs": "https://arxiv.org/abs/2502.03435", "authors": ["Yu-Han Wu", "Pierre Marion", "G\u00e9rard Biau", "Claire Boyer"], "title": "Taking a Big Step: Large Learning Rates in Denoising Score Matching Prevent Memorization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Denoising score matching plays a pivotal role in the performance of\ndiffusion-based generative models. However, the empirical optimal score--the\nexact solution to the denoising score matching--leads to memorization, where\ngenerated samples replicate the training data. Yet, in practice, only a\nmoderate degree of memorization is observed, even without explicit\nregularization. In this paper, we investigate this phenomenon by uncovering an\nimplicit regularization mechanism driven by large learning rates. Specifically,\nwe show that in the small-noise regime, the empirical optimal score exhibits\nhigh irregularity. We then prove that, when trained by stochastic gradient\ndescent with a large enough learning rate, neural networks cannot stably\nconverge to a local minimum with arbitrarily small excess risk. Consequently,\nthe learned score cannot be arbitrarily close to the empirical optimal score,\nthereby mitigating memorization. To make the analysis tractable, we consider\none-dimensional data and two-layer neural networks. Experiments validate the\ncrucial role of the learning rate in preventing memorization, even beyond the\none-dimensional setting.", "AI": {"tldr": "Large learning rates in denoising score matching implicitly regularize models, preventing memorization by hindering convergence to the empirical optimal score.", "motivation": "To understand why diffusion-based generative models avoid memorization despite lacking explicit regularization.", "method": "Analyze the irregularity of the empirical optimal score in small-noise regimes and prove the impact of large learning rates on convergence using one-dimensional data and two-layer neural networks.", "result": "Large learning rates prevent stable convergence to the empirical optimal score, reducing memorization.", "conclusion": "Learning rate size is key to implicit regularization in denoising score matching, validated beyond one-dimensional settings."}}
{"id": "2502.05463", "pdf": "https://arxiv.org/pdf/2502.05463", "abs": "https://arxiv.org/abs/2502.05463", "authors": ["Kaushik Bhattacharya", "Lianghao Cao", "George Stepaniants", "Andrew Stuart", "Margaret Trautner"], "title": "Learning Memory and Material Dependent Constitutive Laws", "categories": ["math.NA", "cs.LG", "cs.NA", "35B27, 65M60, 68T07, 74D05, 74D10, 74Q10, 74Q15", "G.1.8; I.6; J.2"], "comment": "48 pages, 11 figures", "summary": "We propose and study a neural operator framework for learning memory- and\nmaterial microstructure-dependent constitutive laws for heterogeneous\nmaterials. We work in the two-scale setting where homogenization theory\nprovides a systematic approach to deriving macroscale constitutive laws,\nobviating the need to resolve complex microstructure repeatedly. However, the\nunit cell problems defining these constitutive models are typically not\namenable to explicit evaluation. It is therefore of interest to learn\nconstitutive models from data generated by the unit cell problem. Our proposed\nframework models homogenized constitutive laws with both memory- and\nmicrostructure-dependence through the use of Markovian recurrent and Fourier\nneural operators. The homogenization problem for Kelvin-Voigt viscoelastic\nmaterials is studied to provide firm theoretical foundations for our model. The\ntheoretical properties of the cell problem in this Kelvin-Voigt setting\nmotivate the proposed learning framework; and are also used to prove a\nuniversal approximation theorem for the learned macroscale constitutive model.\nNumerical experiments show that the proposed learning framework accurately\nlearns memory- and microstructure-dependent viscoelastic and\nelasto-viscoplastic constitutive models, beyond the setting of the theory.\nFurthermore, we show that the learned constitutive models can be successfully\ndeployed in macroscale simulation of material deformation for different\nmicrostructures without retraining.", "AI": {"tldr": "A neural operator framework is proposed to learn memory- and microstructure-dependent constitutive laws for heterogeneous materials, validated theoretically and numerically.", "motivation": "To address the challenge of deriving explicit macroscale constitutive laws from complex microstructure data, leveraging homogenization theory and neural operators.", "method": "Uses Markovian recurrent and Fourier neural operators to model homogenized constitutive laws, validated with Kelvin-Voigt viscoelastic materials.", "result": "Accurately learns and generalizes memory- and microstructure-dependent constitutive models, deployable in macroscale simulations without retraining.", "conclusion": "The framework effectively bridges microstructure data to macroscale simulations, demonstrating versatility beyond theoretical settings."}}
{"id": "2502.11331", "pdf": "https://arxiv.org/pdf/2502.11331", "abs": "https://arxiv.org/abs/2502.11331", "authors": ["Seok-Jin Kim", "Hongjie Liu", "Molei Liu", "Kaizheng Wang"], "title": "Transfer Learning of CATE with Kernel Ridge Regression", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "The proliferation of data has sparked significant interest in leveraging\nfindings from one study to estimate treatment effects in a different target\npopulation without direct outcome observations. However, the transfer learning\nprocess is frequently hindered by substantial covariate shift and limited\noverlap between (i) the source and target populations, as well as (ii) the\ntreatment and control groups within the source. We propose a novel method for\noverlap-adaptive transfer learning of conditional average treatment effect\n(CATE) using kernel ridge regression (KRR). Our approach involves partitioning\nthe labeled source data into two subsets. The first one is used to train\ncandidate CATE models based on regression adjustment and pseudo-outcomes. An\noptimal model is then selected using the second subset and unlabeled target\ndata, employing another pseudo-outcome-based strategy. We provide a theoretical\njustification for our method through sharp non-asymptotic MSE bounds,\nhighlighting its adaptivity to both weak overlaps and the complexity of CATE\nfunction. Extensive numerical studies confirm that our method achieves superior\nfinite-sample efficiency and adaptability. We conclude by demonstrating the\neffectiveness of our approach using a 401(k) eligibility dataset.", "AI": {"tldr": "A novel method for overlap-adaptive transfer learning of CATE using KRR is proposed, addressing covariate shifts and limited overlap in source and target populations.", "motivation": "The challenge of transferring treatment effect estimates between populations due to covariate shifts and limited overlap.", "method": "Partitioning source data to train CATE models via regression adjustment and pseudo-outcomes, selecting optimal models using target data.", "result": "Theoretical MSE bounds show adaptivity to weak overlaps and CATE complexity; numerical studies confirm superior efficiency.", "conclusion": "The method is effective, demonstrated using a 401(k) eligibility dataset."}}
{"id": "2503.04929", "pdf": "https://arxiv.org/pdf/2503.04929", "abs": "https://arxiv.org/abs/2503.04929", "authors": ["Kehan Long", "Ki Myung Brian Lee", "Nikola Raicevic", "Niyas Attasseri", "Melvin Leok", "Nikolay Atanasov"], "title": "Neural Configuration-Space Barriers for Manipulation Planning and Control", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Planning and control for high-dimensional robot manipulators in cluttered,\ndynamic environments require both computational efficiency and robust safety\nguarantees. Inspired by recent advances in learning configuration-space\ndistance functions (CDFs) as robot body representations, we propose a unified\nframework for motion planning and control that formulates safety constraints as\nCDF barriers. A CDF barrier approximates the local free configuration space,\nsubstantially reducing the number of collision-checking operations during\nmotion planning. However, learning a CDF barrier with a neural network and\nrelying on online sensor observations introduce uncertainties that must be\nconsidered during control synthesis. To address this, we develop a\ndistributionally robust CDF barrier formulation for control that explicitly\naccounts for modeling errors and sensor noise without assuming a known\nunderlying distribution. Simulations and hardware experiments on a 6-DoF xArm\nmanipulator show that our neural CDF barrier formulation enables efficient\nplanning and robust real-time safe control in cluttered and dynamic\nenvironments, relying only on onboard point-cloud observations.", "AI": {"tldr": "A unified framework for motion planning and control using learned configuration-space distance functions (CDFs) as safety constraints, addressing uncertainties with robust CDF barriers.", "motivation": "To achieve computational efficiency and robust safety guarantees for high-dimensional robot manipulators in cluttered, dynamic environments.", "method": "Proposes a neural CDF barrier formulation for control, accounting for modeling errors and sensor noise without assuming known distributions.", "result": "Enables efficient planning and robust real-time safe control in cluttered, dynamic environments using onboard point-cloud observations.", "conclusion": "The neural CDF barrier framework effectively balances efficiency and safety for robot manipulators in complex environments."}}
{"id": "2503.10837", "pdf": "https://arxiv.org/pdf/2503.10837", "abs": "https://arxiv.org/abs/2503.10837", "authors": ["Nawaf Alampara", "Mara Schilling-Wilhelmi", "Kevin Maik Jablonka"], "title": "Lessons from the trenches on evaluating machine-learning systems in materials science", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "comment": null, "summary": "Measurements are fundamental to knowledge creation in science, enabling\nconsistent sharing of findings and serving as the foundation for scientific\ndiscovery. As machine learning systems increasingly transform scientific\nfields, the question of how to effectively evaluate these systems becomes\ncrucial for ensuring reliable progress.\n  In this review, we examine the current state and future directions of\nevaluation frameworks for machine learning in science. We organize the review\naround a broadly applicable framework for evaluating machine learning systems\nthrough the lens of statistical measurement theory, using materials science as\nour primary context for examples and case studies. We identify key challenges\ncommon across machine learning evaluation such as construct validity, data\nquality issues, metric design limitations, and benchmark maintenance problems\nthat can lead to phantom progress when evaluation frameworks fail to capture\nreal-world performance needs.\n  By examining both traditional benchmarks and emerging evaluation approaches,\nwe demonstrate how evaluation choices fundamentally shape not only our\nmeasurements but also research priorities and scientific progress. These\nfindings reveal the critical need for transparency in evaluation design and\nreporting, leading us to propose evaluation cards as a structured approach to\ndocumenting measurement choices and limitations.\n  Our work highlights the importance of developing a more diverse toolbox of\nevaluation techniques for machine learning in materials science, while offering\ninsights that can inform evaluation practices in other scientific domains where\nsimilar challenges exist.", "AI": {"tldr": "The paper reviews evaluation frameworks for machine learning in science, emphasizing challenges like construct validity and data quality, and proposes evaluation cards for transparency.", "motivation": "To address the need for reliable evaluation of machine learning systems in science, ensuring they capture real-world performance and drive meaningful progress.", "method": "The review uses statistical measurement theory and case studies from materials science to analyze current evaluation frameworks and identify common challenges.", "result": "Key challenges include construct validity, data quality, and metric design, with phantom progress as a risk. Evaluation cards are proposed to document measurement choices.", "conclusion": "A diverse toolbox of evaluation techniques is needed, with insights applicable across scientific domains facing similar challenges."}}
{"id": "2503.13050", "pdf": "https://arxiv.org/pdf/2503.13050", "abs": "https://arxiv.org/abs/2503.13050", "authors": ["Etienne Gauthier", "Francis Bach", "Michael I. Jordan"], "title": "E-Values Expand the Scope of Conformal Prediction", "categories": ["stat.ML", "cs.LG"], "comment": "Code available at: https://github.com/GauthierE/evalues-expand-cp", "summary": "Conformal prediction is a powerful framework for distribution-free\nuncertainty quantification. The standard approach to conformal prediction\nrelies on comparing the ranks of prediction scores: under exchangeability, the\nrank of a future test point cannot be too extreme relative to a calibration\nset. This rank-based method can be reformulated in terms of p-values. In this\npaper, we explore an alternative approach based on e-values, known as conformal\ne-prediction. E-values offer key advantages that cannot be achieved with\np-values, enabling new theoretical and practical capabilities. In particular,\nwe present three applications that leverage the unique strengths of e-values:\nbatch anytime-valid conformal prediction, fixed-size conformal sets with\ndata-dependent coverage, and conformal prediction under ambiguous ground truth.\nOverall, these examples demonstrate that e-value-based constructions provide a\nflexible expansion of the toolbox of conformal prediction.", "AI": {"tldr": "The paper introduces conformal e-prediction, an e-value-based alternative to traditional p-value methods in conformal prediction, showcasing its advantages in three applications.", "motivation": "To explore the benefits of e-values over p-values in conformal prediction, enabling new theoretical and practical capabilities.", "method": "Proposes conformal e-prediction, an e-value-based approach, and demonstrates its utility in three specific applications.", "result": "E-values provide flexibility and advantages in batch anytime-valid prediction, fixed-size conformal sets, and ambiguous ground truth scenarios.", "conclusion": "E-value-based conformal prediction expands the toolbox, offering enhanced flexibility and capabilities."}}
{"id": "2504.00522", "pdf": "https://arxiv.org/pdf/2504.00522", "abs": "https://arxiv.org/abs/2504.00522", "authors": ["Kyuhan Lee", "Geon Lee", "Kijung Shin"], "title": "MARIOH: Multiplicity-Aware Hypergraph Reconstruction", "categories": ["cs.DB", "cs.LG", "H.2.8"], "comment": "to be published in the 41st IEEE International Conference on Data\n  Engineering (ICDE '25)", "summary": "Hypergraphs offer a powerful framework for modeling higher-order interactions\nthat traditional pairwise graphs cannot fully capture. However, practical\nconstraints often lead to their simplification into projected graphs, resulting\nin substantial information loss and ambiguity in representing higher-order\nrelationships. In this work, we propose MARIOH, a supervised approach for\nreconstructing the original hypergraph from its projected graph by leveraging\nedge multiplicity. To overcome the difficulties posed by the large search\nspace, MARIOH integrates several key ideas: (a) identifying provable size-2\nhyperedges, which reduces the candidate search space, (b) predicting the\nlikelihood of candidates being hyperedges by utilizing both structural and\nmultiplicity-related features, and (c) not only targeting promising hyperedge\ncandidates but also examining less confident ones to explore alternative\npossibilities. Together, these ideas enable MARIOH to efficiently and\neffectively explore the search space. In our experiments using 10 real-world\ndatasets, MARIOH achieves up to 74.51% higher reconstruction accuracy compared\nto state-of-the-art methods.", "AI": {"tldr": "MARIOH is a supervised method for reconstructing hypergraphs from projected graphs using edge multiplicity, achieving higher accuracy than existing methods.", "motivation": "Traditional hypergraph projections lose higher-order interaction details, leading to information loss and ambiguity.", "method": "MARIOH leverages edge multiplicity, identifies size-2 hyperedges, predicts hyperedge likelihood, and explores less confident candidates.", "result": "MARIOH achieves up to 74.51% higher reconstruction accuracy on 10 real-world datasets.", "conclusion": "MARIOH effectively reconstructs hypergraphs by efficiently navigating the search space and outperforms state-of-the-art methods."}}
{"id": "2504.17529", "pdf": "https://arxiv.org/pdf/2504.17529", "abs": "https://arxiv.org/abs/2504.17529", "authors": ["Youngjune Lee", "Haeyu Jeong", "Changgeon Lim", "Jeong Choi", "Hongjun Lim", "Hangon Kim", "Jiyoon Kwon", "Saehun Kim"], "title": "IRA: Adaptive Interest-aware Representation and Alignment for Personalized Multi-interest Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": "Accepted to SIGIR 2025 Industry Track. First two authors contributed\n  equally", "summary": "Online community platforms require dynamic personalized retrieval and\nrecommendation that can continuously adapt to evolving user interests and new\ndocuments. However, optimizing models to handle such changes in real-time\nremains a major challenge in large-scale industrial settings. To address this,\nwe propose the Interest-aware Representation and Alignment (IRA) framework, an\nefficient and scalable approach that dynamically adapts to new interactions\nthrough a cumulative structure. IRA leverages two key mechanisms: (1) Interest\nUnits that capture diverse user interests as contextual texts, while\nreinforcing or fading over time through cumulative updates, and (2) a retrieval\nprocess that measures the relevance between Interest Units and documents based\nsolely on semantic relationships, eliminating dependence on click signals to\nmitigate temporal biases. By integrating cumulative Interest Unit updates with\nthe retrieval process, IRA continuously adapts to evolving user preferences,\nensuring robust and fine-grained personalization without being constrained by\npast training distributions. We validate the effectiveness of IRA through\nextensive experiments on real-world datasets, including its deployment in the\nHome Section of NAVER's CAFE, South Korea's leading community platform.", "AI": {"tldr": "The IRA framework dynamically adapts to evolving user interests and new documents in online communities, using Interest Units and semantic retrieval for personalized recommendations.", "motivation": "To address the challenge of real-time adaptation to changing user interests and new content in large-scale online platforms.", "method": "IRA uses Interest Units to capture and update user interests contextually and a semantic retrieval process to measure relevance without relying on click signals.", "result": "IRA effectively adapts to user preferences, validated through experiments and deployment in NAVER's CAFE platform.", "conclusion": "IRA provides scalable, fine-grained personalization by dynamically aligning with evolving user interests and mitigating temporal biases."}}
{"id": "2504.19488", "pdf": "https://arxiv.org/pdf/2504.19488", "abs": "https://arxiv.org/abs/2504.19488", "authors": ["Vijay Prakash S"], "title": "Two-parameter superposable S-curves", "categories": ["stat.ME", "cs.LG"], "comment": "14 pages. Tables are updated in v2. Provided clear steps for\n  governing equations in v3", "summary": "Straight line equation $y=mx$ with slope $m$, when singularly perturbed as\n$ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or\nS-curves on a real plane. As $a\\rightarrow 0$, we get back $y=mx$ which is a\ncumulative distribution function of a continuous uniform distribution that\ndescribes the occurrence of every event in an interval to be equally probable.\nAs $a\\rightarrow\\infty$, the derivative of $y$ has finite support only at $y=0$\nresembling a degenerate distribution. Based on these arguments, in this work,\nwe propose that these S-curves can represent maximum entropy uniform\ndistribution to a zero entropy single value. We also argue that these S-curves\nare superposable as they are only parametrically nonlinear but fundamentally\nlinear. So far, the superposed forms have been used to capture the patterns of\nnatural systems such as nonlinear dynamics of biological growth and kinetics of\nenzyme reactions. Here, we attempt to use the S-curve and its superposed form\nas statistical models. We fit the models on a classical dataset containing\nflower measurements of iris plants and analyze their usefulness in pattern\nrecognition. Based on these models, we claim that any non-uniform pattern can\nbe represented as a singular perturbation to uniform distribution. However, our\nparametric estimation procedure have some limitations such as sensitivity to\ninitial conditions depending on the data at hand.", "AI": {"tldr": "The paper explores S-curves from singularly perturbed straight line equations, linking them to entropy distributions and proposing their use as statistical models for pattern recognition.", "motivation": "To bridge the gap between uniform and degenerate distributions using S-curves and demonstrate their applicability in modeling natural systems and statistical patterns.", "method": "Perturb the straight line equation to form S-curves, analyze their properties, and fit them to iris flower data for pattern recognition.", "result": "S-curves can represent transitions from uniform to degenerate distributions and are useful in pattern recognition, though parameter estimation is sensitive to initial conditions.", "conclusion": "Non-uniform patterns can be modeled as perturbations of uniform distributions, but the method has limitations in parameter sensitivity."}}
{"id": "2505.02508", "pdf": "https://arxiv.org/pdf/2505.02508", "abs": "https://arxiv.org/abs/2505.02508", "authors": ["Yang Lyu", "Yuchun Qian", "Tan Minh Nguyen", "Xin T. Tong"], "title": "Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Diffusion models is a popular computational tool to generate new data\nsamples. It utilizes a forward diffusion process that add noise to the data\ndistribution and then use a reverse process to remove noises to produce samples\nfrom the data distribution. However, when the empirical data distribution\nconsists of $n$ data point, using the empirical diffusion model will\nnecessarily produce one of the existing data points. This is often referred to\nas the memorization effect, which is usually resolved by sophisticated machine\nlearning procedures in the current literature. This work shows that the\nmemorization problem can be resolved by a simple inertia update step at the end\nof the empirical diffusion model simulation. Our inertial diffusion model\nrequires only the empirical diffusion model score function and it does not\nrequire any further training. We show that choosing the inertia diffusion model\nsample distribution is an $O\\left(n^{-\\frac{2}{d+4}}\\right)$ Wasserstein-1\napproximation of a data distribution lying on a $C^2$ manifold of dimension\n$d$. Since this estimate is significant smaller the Wasserstein1 distance\nbetween population and empirical distributions, it rigorously shows the\ninertial diffusion model produces new data samples. Remarkably, this upper\nbound is completely free of the ambient space dimension, since there is no\ntraining involved. Our analysis utilizes the fact that the inertial diffusion\nmodel samples are approximately distributed as the Gaussian kernel density\nestimator on the manifold. This reveals an interesting connection between\ndiffusion model and manifold learning.", "AI": {"tldr": "The paper introduces an inertial diffusion model to resolve memorization in empirical diffusion models by adding a simple inertia update step, producing new data samples without further training.", "motivation": "The memorization effect in empirical diffusion models, which limits generation to existing data points, is addressed without complex machine learning procedures.", "method": "An inertia update step is added to the empirical diffusion model, requiring only the score function and no additional training.", "result": "The inertial diffusion model approximates the data distribution with an $O(n^{-2/(d+4)})$ Wasserstein-1 distance, proving it generates new samples.", "conclusion": "The method connects diffusion models to manifold learning and avoids ambient space dependency, offering a simple yet effective solution."}}
