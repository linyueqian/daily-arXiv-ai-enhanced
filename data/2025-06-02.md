<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 253]
- [cs.CV](#cs.CV) [Total: 195]
- [cs.AI](#cs.AI) [Total: 61]
- [cs.SD](#cs.SD) [Total: 21]
- [cs.LG](#cs.LG) [Total: 241]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 4]
- [eess.AS](#eess.AS) [Total: 20]
- [eess.IV](#eess.IV) [Total: 15]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/pdf/2505.23785)
*Cody Kommers, Drew Hemment, Maria Antoniak, Joel Z. Leibo, Hoyt Long, Emily Robinson, Adam Sobey*

Main category: cs.CL

TL;DR: LLMs can represent human meaning at scale by using thick descriptions, overcoming limitations of thin numerical representations in AI systems.


<details>
  <summary>Details</summary>
Motivation: Current AI systems lack cultural context due to reliance on thin descriptions, while humanities use thick descriptions to capture meaning. LLMs can bridge this gap.

Method: Proposes using LLMs to automate thick descriptions, retaining contextual and heterogeneous information for human meaning.

Result: Identifies five challenges for LLMs in representing meaning: context preservation, pluralism, lived experience integration, qualitative vs. quantitative distinction, and dynamic meaning.

Conclusion: Thick descriptions via LLMs could unify efforts to represent culture in AI, addressing key challenges in generative AI applications.

Abstract: This position paper argues that large language models (LLMs) can make
cultural context, and therefore human meaning, legible at an unprecedented
scale in AI-based sociotechnical systems. We argue that such systems have
previously been unable to represent human meaning because they rely on thin
descriptions: numerical representations that enforce standardization and
therefore strip human activity of the cultural context that gives it meaning.
By contrast, scholars in the humanities and qualitative social sciences have
developed frameworks for representing meaning through thick description: verbal
representations that accommodate heterogeneity and retain contextual
information needed to represent human meaning. While these methods can
effectively codify meaning, they are difficult to deploy at scale. However, the
verbal capabilities of LLMs now provide a means of (at least partially)
automating the generation and processing of thick descriptions, potentially
overcoming this bottleneck. We argue that the problem of rendering human
meaning legible is not just about selecting better metrics, but about
developing new representational formats (based on thick description). We frame
this as a crucial direction for the application of generative AI and identify
five key challenges: preserving context, maintaining interpretive pluralism,
integrating perspectives based on lived experience and critical distance,
distinguishing qualitative content from quantitative magnitude, and
acknowledging meaning as dynamic rather than static. Furthermore, we suggest
that thick description has the potential to serve as a unifying framework to
address a number of emerging concerns about the difficulties of representing
culture in (or using) LLMs.

</details>


### [2] [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/pdf/2505.23846)
*Atanu Barai, Stephan Eidenbenz, Nandakishore Santhi*

Main category: cs.CL

TL;DR: A PDES-based methodology integrates AI and non-AI agents for trustworthy, scalable, and accurate problem-solving, outperforming standalone AI models.


<details>
  <summary>Details</summary>
Motivation: To ensure trustworthy and correct AI outputs by combining AI and non-AI systems in a causal, rule-based manner.

Method: Uses parallel discrete event simulation (PDES) to integrate AI and non-AI agents, enabling cooperative, scalable problem-solving with dynamic constraints.

Result: Achieves 68% accuracy across four domains, significantly higher than standalone AI models (23%).

Conclusion: The approach enhances accuracy and scalability by coupling AI with non-AI agents, proving effective in diverse problem domains.

Abstract: To fully leverage the potential of artificial intelligence (AI) systems in a
trustworthy manner, it is desirable to couple multiple AI and non-AI systems
together seamlessly for constraining and ensuring correctness of the output.
This paper introduces a novel parallel discrete event simulation (PDES) based
methodology to combine multiple AI and non-AI agents in a causal, rule-based
way. Our approach tightly integrates the concept of passage of time, with each
agent considered as an entity in the PDES framework and responding to prior
requests from other agents. Such coupling mechanism enables the agents to work
in a co-operative environment towards a common goal while many tasks run in
parallel throughout the simulation. It further enables setting up boundaries to
the outputs of the AI agents by applying necessary dynamic constraints using
non-AI agents while allowing for scalability through deployment of hundreds of
such agents in a larger compute cluster. Distributing smaller AI agents can
enable extremely scalable simulations in the future, addressing local memory
bottlenecks for model parameter storage. Within a PDES involving both AI and
non-AI agents, we break down the problem at hand into structured steps, when
necessary, providing a set of multiple choices to the AI agents, and then
progressively solve these steps towards a final goal. At each step, the non-AI
agents act as unbiased auditors, verifying each action by the AI agents so that
certain rules of engagement are followed. We evaluate our approach by solving
four problems from four different domains and comparing the results with those
from AI models alone. Our results show greater accuracy in solving problems
from various domains where the AI models struggle to solve the problems solely
by themselves. Results show that overall accuracy of our approach is 68% where
as the accuracy of vanilla models is less than 23%.

</details>


### [3] [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/pdf/2505.23788)
*Aakash Sen Sharma, Debdeep Sanyal, Priyansh Srivastava, Sundar Atreya H., Shirish Karande, Mohan Kankanhalli, Murari Mandal*

Main category: cs.CL

TL;DR: FUA-LLM is a framework to align LLM outputs with fair-use doctrine, reducing copyright infringement risks while maintaining utility.


<details>
  <summary>Details</summary>
Motivation: Addressing ethical, legal, and practical concerns of LLMs reproducing copyrighted content verbatim.

Method: Developed FairUseDB (18k expert-validated examples) and used DPO to fine-tune LLMs for compliant outputs.

Result: Reduced problematic outputs by up to 20% while preserving usability, validated by expert evaluations.

Conclusion: FUA-LLM offers a legally-grounded, practical solution to copyright risks in LLMs.

Abstract: Large language models (LLMs) commonly risk copyright infringement by
reproducing protected content verbatim or with insufficient transformative
modifications, posing significant ethical, legal, and practical concerns.
Current inference-time safeguards predominantly rely on restrictive
refusal-based filters, often compromising the practical utility of these
models. To address this, we collaborated closely with intellectual property
experts to develop FUA-LLM (Fair Use Aligned Language Models), a
legally-grounded framework explicitly designed to align LLM outputs with
fair-use doctrine. Central to our method is FairUseDB, a carefully constructed
dataset containing 18,000 expert-validated examples covering nine realistic
infringement scenarios. Leveraging this dataset, we apply Direct Preference
Optimization (DPO) to fine-tune open-source LLMs, encouraging them to produce
legally compliant and practically useful alternatives rather than resorting to
blunt refusal. Recognizing the shortcomings of traditional evaluation metrics,
we propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic
Mean (CAH) to balance infringement risk against response utility. Extensive
quantitative experiments coupled with expert evaluations confirm that FUA-LLM
substantially reduces problematic outputs (up to 20\%) compared to
state-of-the-art approaches, while preserving real-world usability.

</details>


### [4] [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/pdf/2505.23852)
*Nic Dobbins, Christelle Xiong, Kristine Lan, Meliha Yetisgen*

Main category: cs.CL

TL;DR: LLMs were tested as autonomous agents to reproduce findings from Alzheimer's research studies using the NACC dataset, achieving partial success but revealing limitations.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in automating the reproducibility of published research findings.

Method: A simulated research team of LLM-based agents was tasked with reproducing five Alzheimer's studies using only abstracts, methods sections, and data dictionaries.

Result: LLM agents reproduced 53.2% of findings on average, with discrepancies in numeric values and methods.

Conclusion: LLMs show promise for automating reproducibility but currently have limitations in fully replicating research.

Abstract: Objective: To demonstrate the capabilities of Large Language Models (LLMs) as
autonomous agents to reproduce findings of published research studies using the
same or similar dataset.
  Materials and Methods: We used the "Quick Access" dataset of the National
Alzheimer's Coordinating Center (NACC). We identified highly cited published
research manuscripts using NACC data and selected five studies that appeared
reproducible using this dataset alone. Using GPT-4o, we created a simulated
research team of LLM-based autonomous agents tasked with writing and executing
code to dynamically reproduce the findings of each study, given only study
Abstracts, Methods sections, and data dictionary descriptions of the dataset.
  Results: We extracted 35 key findings described in the Abstracts across 5
Alzheimer's studies. On average, LLM agents approximately reproduced 53.2% of
findings per study. Numeric values and range-based findings often differed
between studies and agents. The agents also applied statistical methods or
parameters that varied from the originals, though overall trends and
significance were sometimes similar.
  Discussion: In some cases, LLM-based agents replicated research techniques
and findings. In others, they failed due to implementation flaws or missing
methodological detail. These discrepancies show the current limits of LLMs in
fully automating reproducibility assessments. Still, this early investigation
highlights the potential of structured agent-based systems to provide scalable
evaluation of scientific rigor.
  Conclusion: This exploratory work illustrates both the promise and
limitations of LLMs as autonomous agents for automating reproducibility in
biomedical research.

</details>


### [5] [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/pdf/2505.23789)
*Mingyu Huang, Shasha Zhou, Yuxuan Chen, Ke Li*

Main category: cs.CL

TL;DR: LitChat is an interactive literature agent combining LLMs and data-driven tools to provide comprehensive, evidence-based insights for navigating large-scale scientific literature.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of digital publications makes manual literature review impractical, and existing LLMs lack the transparency and comprehensiveness needed for systematic reviews.

Method: LitChat integrates LLMs with data-driven discovery tools to interpret queries, retrieve sources, build knowledge graphs, and apply data-mining for evidence-based insights.

Result: A case study on AI4Health demonstrates LitChat's ability to efficiently explore large literature landscapes with data-backed evidence.

Conclusion: LitChat effectively addresses the limitations of traditional and LLM-based methods, offering a scalable solution for literature exploration.

Abstract: We are living in an era of "big literature", where the volume of digital
scientific publications is growing exponentially. While offering new
opportunities, this also poses challenges for understanding literature
landscapes, as traditional manual reviewing is no longer feasible. Recent large
language models (LLMs) have shown strong capabilities for literature
comprehension, yet they are incapable of offering "comprehensive, objective,
open and transparent" views desired by systematic reviews due to their limited
context windows and trust issues like hallucinations. Here we present LitChat,
an end-to-end, interactive and conversational literature agent that augments
LLM agents with data-driven discovery tools to facilitate literature
exploration. LitChat automatically interprets user queries, retrieves relevant
sources, constructs knowledge graphs, and employs diverse data-mining
techniques to generate evidence-based insights addressing user needs. We
illustrate the effectiveness of LitChat via a case study on AI4Health,
highlighting its capacity to quickly navigate the users through large-scale
literature landscape with data-based evidence that is otherwise infeasible with
traditional means.

</details>


### [6] [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/pdf/2505.23822)
*Mai Ali, Christopher Lucasius, Tanmay P. Patel, Madison Aitken, Jacob Vorstman, Peter Szatmari, Marco Battaglia, Deepa Kundur*

Main category: cs.CL

TL;DR: The paper proposes a trimodal approach for depression detection using speech data, integrating text, acoustic landmarks, and vocal biomarkers. It also employs multi-task learning for comorbid conditions and longitudinal analysis, achieving better accuracy than unimodal methods.


<details>
  <summary>Details</summary>
Motivation: Speech is a valuable but underutilized digital phenotype for mental health. The study aims to improve depression detection by treating speech as a multimodal source and addressing comorbid conditions like suicidal ideation and sleep disturbances.

Method: The study uses large language model-based architectures to analyze speech-derived text, acoustic landmarks, and vocal biomarkers. It integrates multi-task learning for comorbid conditions and longitudinal analysis for temporal progression.

Result: The proposed trimodal, longitudinal multi-task learning approach achieves a balanced accuracy of 70.8%, outperforming unimodal, single-task, and non-longitudinal methods.

Conclusion: The study demonstrates the effectiveness of multimodal and longitudinal approaches in improving depression detection and understanding its progression, with potential applications in mental health monitoring.

Abstract: Speech is a noninvasive digital phenotype that can offer valuable insights
into mental health conditions, but it is often treated as a single modality. In
contrast, we propose the treatment of patient speech data as a trimodal
multimedia data source for depression detection. This study explores the
potential of large language model-based architectures for speech-based
depression prediction in a multimodal regime that integrates speech-derived
text, acoustic landmarks, and vocal biomarkers. Adolescent depression presents
a significant challenge and is often comorbid with multiple disorders, such as
suicidal ideation and sleep disturbances. This presents an additional
opportunity to integrate multi-task learning (MTL) into our study by
simultaneously predicting depression, suicidal ideation, and sleep disturbances
using the multimodal formulation. We also propose a longitudinal analysis
strategy that models temporal changes across multiple clinical interactions,
allowing for a comprehensive understanding of the conditions' progression. Our
proposed approach, featuring trimodal, longitudinal MTL is evaluated on the
Depression Early Warning dataset. It achieves a balanced accuracy of 70.8%,
which is higher than each of the unimodal, single-task, and non-longitudinal
methods.

</details>


### [7] [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/pdf/2505.23790)
*Shaojie Wang, Sirui Ding, Na Zou*

Main category: cs.CL

TL;DR: The paper proposes an information-theoretic framework using mutual information (MI) to evaluate linguistic understanding in LLMs, revealing differences in information fidelity between encoder-only and decoder-only models and showing that optimizing recoverability improves model performance.


<details>
  <summary>Details</summary>
Motivation: Evaluating intrinsic linguistic understanding in LLMs is challenging; the paper aims to move beyond specialized tasks by using MI to measure how well input information is preserved in latent representations.

Method: The framework decomposes sentence-level MI into token-level MI, derives a computable lower bound using Fano's inequality, and implements a recoverability task to compare MI across LLMs.

Result: Encoder-only models maintain higher information fidelity than decoder-only models, which show a late-layer 'forgetting' pattern. Fine-tuning for recoverability improves understanding without task-specific supervision.

Conclusion: Mutual information serves as a foundation for understanding and enhancing LLM capabilities, with practical implications for model improvement.

Abstract: Recent advances in large language models (LLMs) have revolutionized natural
language processing, yet evaluating their intrinsic linguistic understanding
remains challenging. Moving beyond specialized evaluation tasks, we propose an
information-theoretic framework grounded in mutual information (MI) to achieve
this. We formalize the understanding as MI between an input sentence and its
latent representation (sentence-level MI), measuring how effectively input
information is preserved in latent representation. Given that LLMs learn
embeddings for individual tokens, we decompose sentence-level MI into
token-level MI between tokens and sentence embeddings, establishing theoretical
bounds connecting these measures. Based on this foundation, we theoretically
derive a computable lower bound for token-level MI using Fano's inequality,
which directly relates to token-level recoverability-the ability to predict
original tokens from sentence embedding. We implement this recoverability task
to comparatively measure MI across different LLMs, revealing that encoder-only
models consistently maintain higher information fidelity than their
decoder-only counterparts, with the latter exhibiting a distinctive late-layer
"forgetting" pattern where mutual information is first enhanced and then
discarded. Moreover, fine-tuning to maximize token-level recoverability
consistently improves understanding ability of LLMs on tasks without
task-specific supervision, demonstrating that mutual information can serve as a
foundation for understanding and improving language model capabilities.

</details>


### [8] [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.23794)
*Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu*

Main category: cs.CL

TL;DR: R3-RAG uses reinforcement learning to improve retrieval-augmented generation by enabling step-by-step reasoning and retrieval, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address limitations of dense retrievers in RAG systems, such as limited parameters and lack of step-by-step reasoning, by automating the process.

Method: Two-stage approach: cold start for iterative reasoning and retrieval, followed by reinforcement learning with two rewards (answer correctness and document relevance).

Result: R3-RAG significantly outperforms baselines and adapts well to different retrievers.

Conclusion: R3-RAG effectively enhances retrieval-augmented generation by automating reasoning and retrieval, improving factual correctness.

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge with Large
Language Models (LLMs) to enhance factual correctness and mitigate
hallucination. However, dense retrievers often become the bottleneck of RAG
systems due to their limited parameters compared to LLMs and their inability to
perform step-by-step reasoning. While prompt-based iterative RAG attempts to
address these limitations, it is constrained by human-designed workflows. To
address these limitations, we propose $\textbf{R3-RAG}$, which uses
$\textbf{R}$einforcement learning to make the LLM learn how to
$\textbf{R}$eason and $\textbf{R}$etrieve step by step, thus retrieving
comprehensive external knowledge and leading to correct answers. R3-RAG is
divided into two stages. We first use cold start to make the model learn the
manner of iteratively interleaving reasoning and retrieval. Then we use
reinforcement learning to further harness its ability to better explore the
external retrieval environment. Specifically, we propose two rewards for
R3-RAG: 1) answer correctness for outcome reward, which judges whether the
trajectory leads to a correct answer; 2) relevance-based document verification
for process reward, encouraging the model to retrieve documents that are
relevant to the user question, through which we can let the model learn how to
iteratively reason and retrieve relevant documents to get the correct answer.
Experimental results show that R3-RAG significantly outperforms baselines and
can transfer well to different retrievers. We release R3-RAG at
https://github.com/Yuan-Li-FNLP/R3-RAG.

</details>


### [9] [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/pdf/2505.23796)
*Christopher Barrie, Petter Törnberg*

Main category: cs.CL

TL;DR: The paper critiques Ashery et al.'s claim that LLMs develop human-like linguistic conventions, arguing the results are due to data leakage and memorization, not emergent behavior.


<details>
  <summary>Details</summary>
Motivation: To challenge the interpretation that LLMs exhibit emergent social norms and demonstrate their behaviors stem from pre-training data.

Method: Multiple analyses showing LLMs recognize and recall game structures from training data, not forming new conventions.

Result: LLMs' behaviors align with memorization, not emergent linguistic conventions.

Conclusion: Suggests reevaluating LLMs' role in social science models and exploring alternative strategies.

Abstract: Ashery et al. recently argue that large language models (LLMs), when paired
to play a classic "naming game," spontaneously develop linguistic conventions
reminiscent of human social norms. Here, we show that their results are better
explained by data leakage: the models simply reproduce conventions they already
encountered during pre-training. Despite the authors' mitigation measures, we
provide multiple analyses demonstrating that the LLMs recognize the structure
of the coordination game and recall its outcomes, rather than exhibit
"emergent" conventions. Consequently, the observed behaviors are
indistinguishable from memorization of the training corpus. We conclude by
pointing to potential alternative strategies and reflecting more generally on
the place of LLMs for social science models.

</details>


### [10] [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/pdf/2505.23797)
*Zaihan Yang, Ryan Leonard, Hien Tran, Rory Driscoll, Chadbourne Davis*

Main category: cs.CL

TL;DR: A hybrid model combining RoBERTa, TF-IDF, and PCA is developed for classifying Reddit posts into four suicide risk levels, outperforming other models with a weighted F1 score of 0.7512.


<details>
  <summary>Details</summary>
Motivation: Address the urgent need for early detection of suicidal risk using machine learning on social media data.

Method: Multi-class classification using a RoBERTa-TF-IDF-PCA hybrid model, with data resampling and augmentation to handle imbalance and overfitting.

Result: The hybrid model achieves a best weighted F1 score of 0.7512, outperforming RoBERTa, BERT, and traditional classifiers.

Conclusion: The proposed hybrid model is effective for suicide risk assessment, offering improved accuracy and reliability.

Abstract: Suicidal thoughts and behaviors are increasingly recognized as a critical
societal concern, highlighting the urgent need for effective tools to enable
early detection of suicidal risk. In this work, we develop robust machine
learning models that leverage Reddit posts to automatically classify them into
four distinct levels of suicide risk severity. We frame this as a multi-class
classification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating
the deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),
a state-of-the-art deep learning transformer model, with the statistical
term-weighting of TF-IDF, further compressed with PCA, to boost the accuracy
and reliability of suicide risk assessment. To address data imbalance and
overfitting, we explore various data resampling techniques and data
augmentation strategies to enhance model generalization. Additionally, we
compare our model's performance against that of using RoBERTa only, the BERT
model and other traditional machine learning classifiers. Experimental results
demonstrate that the hybrid model can achieve improved performance, giving a
best weighted $F_{1}$ score of 0.7512.

</details>


### [11] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/pdf/2505.23798)
*Jian Lan, Yifei Fu, Udo Schlegel, Gengyuan Zhang, Tanveer Hannan, Haokun Chen, Thomas Seidl*

Main category: cs.CL

TL;DR: The paper evaluates and mitigates social bias in large vision-language models (VLMs), revealing biases in responses and confidence levels, and proposes a post-hoc method to reduce bias during inference.


<details>
  <summary>Details</summary>
Motivation: Social bias in VLMs harms fairness and ethics, but its extent and mitigation in generative responses are unclear.

Method: Evaluated four VLMs on PAIRS and SocialCounterfactuals datasets, analyzed hidden layers, and proposed a training-free, model-agnostic post-hoc method to mitigate bias.

Result: VLMs exhibit gender and race biases, mis-calibrated confidence, and fluctuating fairness in hidden layers. The proposed method outperforms training strategies.

Conclusion: The post-hoc method effectively reduces bias in VLMs, improving fairness and confidence reliability.

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [12] [LLM Agents Making Agent Tools](https://arxiv.org/pdf/2502.11705)
*Georg Wölflein, Dyke Ferber, Daniel Truhn, Ognjen Arandjelović, Jakob Nikolas Kather*

Main category: cs.CL

TL;DR: ToolMaker autonomously converts papers with code into LLM-compatible tools, outperforming current methods by correctly implementing 80% of tasks.


<details>
  <summary>Details</summary>
Motivation: Addresses the limitation of pre-implemented tools in LLMs for specialized domains like life sciences and medicine by leveraging public code repositories.

Method: Uses a closed-loop self-correction mechanism to autonomously install dependencies and generate task-specific code from GitHub URLs.

Result: Achieves 80% task correctness on a benchmark of 15 complex computational tasks, surpassing state-of-the-art software engineering agents.

Conclusion: ToolMaker advances autonomous agent-based scientific workflows, with publicly available code and benchmarks.

Abstract: Tool use has turned large language models (LLMs) into powerful agents that
can perform complex multi-step tasks by dynamically utilising external software
components. However, these tools must be implemented in advance by human
developers, hindering the applicability of LLM agents in domains demanding
large numbers of highly specialised tools, like in life sciences and medicine.
Motivated by the growing trend of scientific studies accompanied by public code
repositories, we propose ToolMaker, an agentic framework that autonomously
transforms papers with code into LLM-compatible tools. Given a GitHub URL and
short task description, ToolMaker autonomously installs dependencies and
generates code to perform the task, using a closed-loop self-correction
mechanism for debugging. To evaluate our approach, we introduce a benchmark
comprising 15 complex computational tasks spanning various domains with over
100 unit tests to assess correctness and robustness. Our method correctly
implements 80% of the tasks, substantially outperforming current
state-of-the-art software engineering agents. ToolMaker therefore is a step
towards fully autonomous agent-based scientific workflows. Our code and
benchmark are publicly available at https://github.com/KatherLab/ToolMaker.

</details>


### [13] [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/pdf/2505.23799)
*Xiaoyuan Wu, Weiran Lin, Omer Akgul, Lujo Bauer*

Main category: cs.CL

TL;DR: The paper evaluates methods for measuring LLM response consistency, finding current approaches poorly match human perceptions. It proposes a logit-based ensemble method that performs comparably to existing metrics and advocates for more human evaluation in assessing LLM consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce inconsistent or unreliable text due to hallucinations and prompt sensitivity. Existing methods for measuring consistency lack alignment with human perceptions.

Method: The authors conducted a user study (n=2,976) to compare human perceptions of LLM consistency with existing metrics. They proposed a logit-based ensemble method for consistency estimation.

Result: Current methods poorly approximate human perceptions of LLM consistency. The proposed logit-based ensemble method matches the best existing metric in estimating human ratings.

Conclusion: Human evaluation should be more widely used for assessing LLM consistency, as automated methods remain imperfect.

Abstract: Large language models (LLMs) are prone to hallucinations and sensitive to
prompt perturbations, often resulting in inconsistent or unreliable generated
text. Different methods have been proposed to mitigate such hallucinations and
fragility -- one of them being measuring the consistency (the model's
confidence in the response, or likelihood of generating a similar response when
resampled) of LLM responses. In previous work, measuring consistency often
relied on the probability of a response appearing within a pool of resampled
responses, or internal states or logits of responses. However, it is not yet
clear how well these approaches approximate how humans perceive the consistency
of LLM responses. We performed a user study (n=2,976) and found current methods
typically do not approximate users' perceptions of LLM consistency very well.
We propose a logit-based ensemble method for estimating LLM consistency, and we
show that this method matches the performance of the best-performing existing
metric in estimating human ratings of LLM consistency. Our results suggest that
methods of estimating LLM consistency without human evaluation are sufficiently
imperfect that we suggest evaluation with human input be more broadly used.

</details>


### [14] [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/pdf/2505.23801)
*Sajid Hussain, Muhammad Sohail, Nauman Ali Khan*

Main category: cs.CL

TL;DR: SEMFED is a semantic-aware, resource-efficient federated learning framework for NLP tasks, reducing communication costs by 80.5% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in FL for NLP, such as semantic heterogeneity, vocabulary mismatches, and resource constraints on edge devices.

Method: Incorporates semantic-aware client selection, adaptive NLP-specific model architectures, and communication-efficient semantic feature compression.

Result: Achieves 80.5% reduction in communication costs with model accuracy above 98%, outperforming existing FL approaches.

Conclusion: SEMFED is effective for real-world federated NLP deployments, handling heterogeneous client environments efficiently.

Abstract: Background: Federated Learning (FL) has emerged as a promising paradigm for
training machine learning models while preserving data privacy. However,
applying FL to Natural Language Processing (NLP) tasks presents unique
challenges due to semantic heterogeneity across clients, vocabulary mismatches,
and varying resource constraints on edge devices. Objectives: This paper
introduces SEMFED, a novel semantic-aware resource-efficient federated learning
framework specifically designed for heterogeneous NLP tasks. Methods: SEMFED
incorporates three key innovations: (1) a semantic-aware client selection
mechanism that balances semantic diversity with resource constraints, (2)
adaptive NLP-specific model architectures tailored to device capabilities while
preserving semantic information, and (3) a communication-efficient semantic
feature compression technique that significantly reduces bandwidth
requirements. Results: Experimental results on various NLP classification tasks
demonstrate that SEMFED achieves an 80.5% reduction in communication costs
while maintaining model accuracy above 98%, outperforming state-of-the-art FL
approaches. Conclusion: SEMFED effectively manages heterogeneous client
environments with varying computational resources, network reliability, and
semantic data distributions, making it particularly suitable for real-world
federated NLP deployments.

</details>


### [15] [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/pdf/2505.23802)
*Suhana Bedi, Hejie Cui, Miguel Fuentes, Alyssa Unell, Michael Wornow, Juan M. Banda, Nikesh Kotecha, Timothy Keyes, Yifan Mai, Mert Oez, Hao Qiu, Shrey Jain, Leonardo Schettini, Mehr Kashyap, Jason Alan Fries, Akshay Swaminathan, Philip Chung, Fateme Nateghi, Asad Aali, Ashwin Nayak, Shivam Vedak, Sneha S. Jain, Birju Patel, Oluseyi Fayanju, Shreya Shah, Ethan Goh, Dong-han Yao, Brian Soetikno, Eduardo Reis, Sergios Gatidis, Vasu Divi, Robson Capasso, Rachna Saralkar, Chia-Chun Chiang, Jenelle Jindal, Tho Pham, Faraz Ghoddusi, Steven Lin, Albert S. Chiou, Christy Hong, Mohana Roy, Michael F. Gensheimer, Hinesh Patel, Kevin Schulman, Dev Dash, Danton Char, Lance Downing, Francois Grolleau, Kameron Black, Bethel Mieso, Aydin Zahedivash, Wen-wai Yim, Harshita Sharma, Tony Lee, Hannah Kirsch, Jennifer Lee, Nerissa Ambers, Carlene Lugtu, Aditya Sharma, Bilal Mawji, Alex Alekseyev, Vicky Zhou, Vikas Kakkar, Jarrod Helzer, Anurang Revri, Yair Bannett, Roxana Daneshjou, Jonathan Chen, Emily Alsentzer, Keith Morse, Nirmal Ravi, Nima Aghaeepour, Vanessa Kennedy, Akshay Chaudhari, Thomas Wang, Sanmi Koyejo, Matthew P. Lungren, Eric Horvitz, Percy Liang, Mike Pfeffer, Nigam H. Shah*

Main category: cs.CL

TL;DR: MedHELM introduces a clinician-validated evaluation framework for LLMs in medical tasks, revealing performance variations and cost efficiencies among models.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs in medicine lack real-world clinical complexity, necessitating a more comprehensive framework.

Method: Developed a taxonomy with clinician input, created a benchmark suite, and used an LLM-jury for systematic evaluation of 9 LLMs.

Result: Performance varied by task category; advanced reasoning models excelled, with Claude 3.5 Sonnet offering cost efficiency. LLM-jury agreed well with clinicians.

Conclusion: MedHELM provides a robust, open-source framework for real-world medical LLM evaluation, emphasizing task-specific assessments.

Abstract: While large language models (LLMs) achieve near-perfect scores on medical
licensing exams, these evaluations inadequately reflect the complexity and
diversity of real-world clinical practice. We introduce MedHELM, an extensible
evaluation framework for assessing LLM performance for medical tasks with three
key contributions. First, a clinician-validated taxonomy spanning 5 categories,
22 subcategories, and 121 tasks developed with 29 clinicians. Second, a
comprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly
formulated) providing complete coverage of all categories and subcategories in
the taxonomy. Third, a systematic comparison of LLMs with improved evaluation
methods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9
frontier LLMs, using the 35 benchmarks, revealed significant performance
variation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%
win-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved
comparable results at 40% lower estimated computational cost. On a normalized
accuracy scale (0-1), most models performed strongly in Clinical Note
Generation (0.73-0.85) and Patient Communication & Education (0.78-0.83),
moderately in Medical Research Assistance (0.65-0.75), and generally lower in
Clinical Decision Support (0.56-0.72) and Administration & Workflow
(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with
clinician ratings (ICC = 0.47), surpassing both average clinician-clinician
agreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and
BERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top
models at lower estimated cost. These findings highlight the importance of
real-world, task-specific evaluation for medical use of LLMs and provides an
open source framework to enable this.

</details>


### [16] [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/pdf/2505.23804)
*Terrance Liu, Shuyi Wang, Daniel Preotiuc-Pietro, Yash Chandarana, Chirag Gupta*

Main category: cs.CL

TL;DR: The paper introduces a method to improve confidence calibration in LLM-based text-to-SQL parsing, combining sub-clause frequency scores with multivariate Platt scaling for better accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail confidently in text-to-SQL parsing, necessitating reliable uncertainty measures for trustworthy systems.

Method: Proposes sub-clause frequency (SCF) scores and multivariate Platt scaling (MPS) to enhance calibration over raw probabilities or traditional Platt scaling.

Result: Empirical evaluation shows MPS and SCF improve calibration and error detection on text-to-SQL datasets.

Conclusion: The approach advances confidence calibration in LLM-based text-to-SQL systems, offering more reliable uncertainty measures.

Abstract: While large language models (LLMs) achieve strong performance on text-to-SQL
parsing, they sometimes exhibit unexpected failures in which they are
confidently incorrect. Building trustworthy text-to-SQL systems thus requires
eliciting reliable uncertainty measures from the LLM. In this paper, we study
the problem of providing a calibrated confidence score that conveys the
likelihood of an output query being correct. Our work is the first to establish
a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In
particular, we show that Platt scaling, a canonical method for calibration,
provides substantial improvements over directly using raw model output
probabilities as confidence scores. Furthermore, we propose a method for
text-to-SQL calibration that leverages the structured nature of SQL queries to
provide more granular signals of correctness, named "sub-clause frequency"
(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the
canonical Platt scaling technique, we combine individual SCF scores into an
overall accurate and calibrated score. Empirical evaluation on two popular
text-to-SQL datasets shows that our approach of combining MPS and SCF yields
further improvements in calibration and the related task of error detection
over traditional Platt scaling.

</details>


### [17] [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/pdf/2505.24229)
*Luong Ho, Khanh Le, Vinh Pham, Bao Nguyen, Tan Tran, Duc Chau*

Main category: cs.CL

TL;DR: A streaming pretrained language model for Inverse Text Normalization (ITN) is introduced, addressing challenges in accuracy and efficiency for streaming ASR. It uses Dynamic Context-Aware adjustments and achieves comparable accuracy to non-streaming ITN with low latency.


<details>
  <summary>Details</summary>
Motivation: The integration of streaming ITN within streaming ASR is underexplored due to challenges in accuracy, efficiency, and adaptability, especially in low-resource settings.

Method: A streaming pretrained language model is proposed, incorporating Dynamic Context-Aware adjustments for adaptive chunk sizes and right-context integration during training and inference.

Result: The method matches non-streaming ITN accuracy and outperforms existing streaming ITN models on a Vietnamese dataset, maintaining low latency.

Conclusion: The proposed model effectively integrates into ASR systems, offering robust performance in streaming scenarios.

Abstract: Inverse Text Normalization (ITN) is crucial for converting spoken Automatic
Speech Recognition (ASR) outputs into well-formatted written text, enhancing
both readability and usability. Despite its importance, the integration of
streaming ITN within streaming ASR remains largely unexplored due to challenges
in accuracy, efficiency, and adaptability, particularly in low-resource and
limited-context scenarios. In this paper, we introduce a streaming pretrained
language model for ITN, leveraging pretrained linguistic representations for
improved robustness. To address streaming constraints, we propose Dynamic
Context-Aware during training and inference, enabling adaptive chunk size
adjustments and the integration of right-context information. Experimental
results demonstrate that our method achieves accuracy comparable to
non-streaming ITN and surpasses existing streaming ITN models on a Vietnamese
dataset, all while maintaining low latency, ensuring seamless integration into
ASR systems.

</details>


### [18] [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/pdf/2505.23806)
*Sihyeon Lee, Hyunjoo Song, Jong-chan Lee, Yoon Jin Lee, Boram Lee, Hee-Eon Lim, Dongyeong Kim, Jinwook Seo, Bohyoung Kim*

Main category: cs.CL

TL;DR: MedOrchestra is a hybrid framework combining cloud and local LLMs to balance privacy and performance in clinical tasks, achieving higher accuracy than baselines and clinicians.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between privacy and performance when deploying LLMs in clinical settings, where cloud LLMs risk data privacy and local LLMs lack complex task capability.

Method: MedOrchestra uses a cloud LLM to decompose tasks and generate prompts, while a local LLM executes subtasks privately, validated via clinical guidelines and synthetic cases.

Result: Achieves 70.21% accuracy on free-text reports and 85.42% on structured reports, outperforming local models and clinicians.

Conclusion: MedOrchestra effectively balances privacy and performance, demonstrating superior accuracy in clinical tasks.

Abstract: Deploying large language models (LLMs) in clinical settings faces critical
trade-offs: cloud LLMs, with their extensive parameters and superior
performance, pose risks to sensitive clinical data privacy, while local LLMs
preserve privacy but often fail at complex clinical interpretation tasks. We
propose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex
clinical tasks into manageable subtasks and prompt generation, while a local
LLM executes these subtasks in a privacy-preserving manner. Without accessing
clinical data, the cloud LLM generates and validates subtask prompts using
clinical guidelines and synthetic test cases. The local LLM executes subtasks
locally and synthesizes outputs generated by the cloud LLM. We evaluate
MedOrchestra on pancreatic cancer staging using 100 radiology reports under
NCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,
outperforming local model baselines (without guideline: 48.94%, with guideline:
56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:
65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches
85.42% accuracy, showing clear superiority across all settings.

</details>


### [19] [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/pdf/2505.23807)
*Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang*

Main category: cs.CL

TL;DR: Dynamic Layerwise Pruning (DLP) adaptively prunes LLM layers by integrating weights and input activations, outperforming uniform and pre-defined non-uniform methods at high sparsity.


<details>
  <summary>Details</summary>
Motivation: Uniform pruning degrades performance at high sparsity, and pre-defined non-uniform methods are suboptimal. DLP aims to dynamically optimize pruning rates per layer.

Method: DLP integrates model weights and input activation information to adaptively assign layerwise pruning rates.

Result: At 70% sparsity, DLP reduces perplexity by 7.79 and improves accuracy by 2.7% for LLaMA2-7B vs. state-of-the-art.

Conclusion: DLP preserves performance at high sparsity, is compatible with other compression techniques, and integrates with PEFT. Code is released for research.

Abstract: Pruning has recently been widely adopted to reduce the parameter scale and
improve the inference efficiency of Large Language Models (LLMs). Mainstream
pruning techniques often rely on uniform layerwise pruning strategies, which
can lead to severe performance degradation at high sparsity levels. Recognizing
the varying contributions of different layers in LLMs, recent studies have
shifted their focus toward non-uniform layerwise pruning. However, these
approaches often rely on pre-defined values, which can result in suboptimal
performance. To overcome these limitations, we propose a novel method called
Dynamic Layerwise Pruning (DLP). This approach adaptively determines the
relative importance of each layer by integrating model weights with input
activation information, assigning pruning rates accordingly. Experimental
results show that DLP effectively preserves model performance at high sparsity
levels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the
perplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%
compared to state-of-the-art methods. Moreover, DLP is compatible with various
existing LLM compression techniques and can be seamlessly integrated into
Parameter-Efficient Fine-Tuning (PEFT). We release the code at
https://github.com/ironartisan/DLP to facilitate future research.

</details>


### [20] [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/pdf/2505.24347)
*Yangui Fang, Baixu Cheng, Jing Peng, Xu Li, Yu Xi, Chengwei Zhang, Guohui Zhong*

Main category: cs.CL

TL;DR: The paper introduces RLLM-CF, a framework to correct ASR errors using LLMs without training or labeled data, addressing hallucinations through a three-stage process.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR error correction methods are limited, and direct LLM use risks modifying correct text due to hallucinations.

Method: RLLM-CF involves error pre-detection, chain-of-thought sub-tasks iterative correction, and reasoning process verification.

Result: Experiments show 21%, 11%, 9%, and 11.4% relative CER/WER reductions on AISHELL-1, AISHELL-2, and Librispeech datasets.

Conclusion: RLLM-CF effectively corrects ASR errors without additional data or model fine-tuning, ensuring reliability.

Abstract: Automatic Speech Recognition (ASR) error correction aims to correct
recognition errors while preserving accurate text. Although traditional
approaches demonstrate moderate effectiveness, LLMs offer a paradigm that
eliminates the need for training and labeled data. However, directly using LLMs
will encounter hallucinations problem, which may lead to the modification of
the correct text. To address this problem, we propose the Reliable LLM
Correction Framework (RLLM-CF), which consists of three stages: (1) error
pre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)
reasoning process verification. The advantage of our method is that it does not
require additional information or fine-tuning of the model, and ensures the
correctness of the LLM correction under multi-pass programming. Experiments on
AISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by
our framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.

</details>


### [21] [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2505.23808)
*Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang*

Main category: cs.CL

TL;DR: DenseLoRA improves upon LoRA by using a dense low-rank matrix for adapting LLMs, achieving higher accuracy with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Prior methods like LoRA have redundant parameters, leading to inefficiencies. DenseLoRA aims to enhance parameter utilization and performance.

Method: DenseLoRA refines hidden representations with an Encoder-Decoder before adaptation, using a dense low-rank matrix instead of redundant matrices.

Result: DenseLoRA achieves 83.8% accuracy with 0.01% trainable parameters, outperforming LoRA's 80.8% with 0.70%.

Conclusion: DenseLoRA offers a more efficient and effective alternative to LoRA for adapting large language models.

Abstract: Low-rank adaptation (LoRA) has been developed as an efficient approach for
adapting large language models (LLMs) by fine-tuning two low-rank matrices,
thereby reducing the number of trainable parameters. However, prior research
indicates that many of the weights in these matrices are redundant, leading to
inefficiencies in parameter utilization. To address this limitation, we
introduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances
parameter efficiency while achieving superior performance compared to LoRA.
DenseLoRA builds upon the concept of representation fine-tuning, incorporating
a single Encoder-Decoder to refine and compress hidden representations across
all adaptation layers before applying adaptation. Instead of relying on two
redundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense
low-rank matrix, improving parameter utilization and adaptation efficiency. We
evaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%
accuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%
accuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we
conduct extensive experiments to systematically assess the impact of
DenseLoRA's components on overall model performance. Code is available at
https://github.com/mulin-ahu/DenseLoRA.

</details>


### [22] [MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR](https://arxiv.org/pdf/2505.24656)
*Dimitrios Damianos, Georgios Paraskevopoulos, Alexandros Potamianos*

Main category: cs.CL

TL;DR: The paper introduces MSDA, a two-stage domain adaptation framework for ASR, combining self-supervised and semi-supervised learning to improve robustness, especially for low-resource languages like Greek.


<details>
  <summary>Details</summary>
Motivation: To enhance ASR model adaptability and generalization in diverse conditions, particularly for low-resource languages and weakly supervised scenarios.

Method: Proposes Multi-Stage Domain Adaptation (MSDA), integrating self-supervised learning with semi-supervised techniques in a cascading approach.

Result: Achieves state-of-the-art performance in ASR tasks, outperforming existing methods and demonstrating robustness in unsupervised domain adaptation.

Conclusion: MSDA effectively combines self-supervision and self-training, proving its necessity and superiority for ASR domain adaptation.

Abstract: In this work, we investigate the Meta PL unsupervised domain adaptation
framework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage
Domain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation
approach that integrates self-supervised learning with semi-supervised
techniques. MSDA is designed to enhance the robustness and generalization of
ASR models, making them more adaptable to diverse conditions. It is
particularly effective for low-resource languages like Greek and in weakly
supervised scenarios where labeled data is scarce or noisy. Through extensive
experiments, we demonstrate that Meta PL can be applied effectively to ASR
tasks, achieving state-of-the-art results, significantly outperforming
state-of-the-art methods, and providing more robust solutions for unsupervised
domain adaptation in ASR. Our ablations highlight the necessity of utilizing a
cascading approach when combining self-supervision with self-training.

</details>


### [23] [Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios](https://arxiv.org/pdf/2505.24691)
*Gerard I. Gállego, Oriol Pareras, Martí Cortada Garcia, Lucas Takanori, Javier Hernando*

Main category: cs.CL

TL;DR: A Speech-to-Text Translation (S2TT) method using phoneme representations in a Chain-of-Thought (CoT) framework improves low-resource and zero-resource translation by leveraging phoneme recognition for cross-lingual transfer.


<details>
  <summary>Details</summary>
Motivation: To enhance translation in low-resource and zero-resource settings by integrating phoneme recognition into the translation process, enabling broader language accessibility.

Method: The approach combines phoneme representations with a multilingual LLM in a CoT framework, using curriculum learning to train progressively on complex tasks.

Result: Phoneme-augmented CoT improves translation in low-resource scenarios and enables zero-resource translation, with a minor trade-off in high-resource performance.

Conclusion: Phoneme-based CoT is a promising advancement for making S2TT more accessible across diverse languages, despite slight performance trade-offs.

Abstract: We propose a Speech-to-Text Translation (S2TT) approach that integrates
phoneme representations into a Chain-of-Thought (CoT) framework to improve
translation in low-resource and zero-resource settings. By introducing phoneme
recognition as an intermediate step, we enhance cross-lingual transfer,
enabling translation even for languages with no labeled speech data. Our system
builds on a multilingual LLM, which we extend to process speech and phonemes.
Training follows a curriculum learning strategy that progressively introduces
more complex tasks. Experiments on multilingual S2TT benchmarks show that
phoneme-augmented CoT improves translation quality in low-resource conditions
and enables zero-resource translation, while slightly impacting high-resource
performance. Despite this trade-off, our findings demonstrate that
phoneme-based CoT is a promising step toward making S2TT more accessible across
diverse languages.

</details>


### [24] [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/pdf/2505.23809)
*Haowei Yang, Haotian Lyu, Tianle Zhang, Dingzhou Wang, Yushang Zhao*

Main category: cs.CL

TL;DR: A framework using LLMs for marketing copy generation boosts CTR by 12.5% and CVR by 8.3% while maintaining novelty.


<details>
  <summary>Details</summary>
Motivation: Balancing creative engagement and conversion effectiveness in competitive e-commerce.

Method: Integrates prompt engineering, multi-objective fine-tuning (sentiment, diversity, CTA), and post-processing.

Result: 12.5% CTR and 8.3% CVR improvement in A/B tests.

Conclusion: Offers automated copy generation with potential for multimodal, real-time personalization.

Abstract: As e-commerce competition intensifies, balancing creative content with
conversion effectiveness becomes critical. Leveraging LLMs' language generation
capabilities, we propose a framework that integrates prompt engineering,
multi-objective fine-tuning, and post-processing to generate marketing copy
that is both engaging and conversion-driven. Our fine-tuning method combines
sentiment adjustment, diversity enhancement, and CTA embedding. Through offline
evaluations and online A/B tests across categories, our approach achieves a
12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content
novelty. This provides a practical solution for automated copy generation and
suggests paths for future multimodal, real-time personalization.

</details>


### [25] [Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification](https://arxiv.org/pdf/2505.24713)
*Badr M. Abdullah, Matthew Baas, Bernd Möbius, Dietrich Klakow*

Main category: cs.CL

TL;DR: An effective voice conversion-based approach improves Arabic dialect identification (ADI) robustness and accuracy, achieving state-of-the-art performance and mitigating speaker bias.


<details>
  <summary>Details</summary>
Motivation: Current ADI systems lack generalization to out-of-domain speech, limiting their reliability for inclusive Arabic speech technologies.

Method: The paper proposes a voice conversion-based training approach for ADI models.

Result: The method achieves up to +34.1% accuracy improvement across domains and mitigates speaker bias.

Conclusion: The robust ADI model and cross-domain dataset are released to advance inclusive Arabic speech technologies.

Abstract: Arabic dialect identification (ADI) systems are essential for large-scale
data collection pipelines that enable the development of inclusive speech
technologies for Arabic language varieties. However, the reliability of current
ADI systems is limited by poor generalization to out-of-domain speech. In this
paper, we present an effective approach based on voice conversion for training
ADI models that achieves state-of-the-art performance and significantly
improves robustness in cross-domain scenarios. Evaluated on a newly collected
real-world test set spanning four different domains, our approach yields
consistent improvements of up to +34.1% in accuracy across domains.
Furthermore, we present an analysis of our approach and demonstrate that voice
conversion helps mitigate the speaker bias in the ADI dataset. We release our
robust ADI model and cross-domain evaluation dataset to support the development
of inclusive speech technologies for Arabic.

</details>


### [26] [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/pdf/2505.23810)
*Chenghao Yang, Yinbo Luo, Zhoufutu Wen, Qi Chu, Tao Gong, Longxiang Liu, Kaiyuan Zhang, Jianpeng Jiao, Ge Zhang, Wenhao Huang, Nenghai Yu*

Main category: cs.CL

TL;DR: MARS-Bench is a benchmark for evaluating LLMs' robustness in multi-turn dialogues, highlighting challenges like motivation transfer and cross-turn dependencies. Closed-source LLMs outperform open-source ones, and explicit reasoning improves robustness.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to fully assess LLMs' weaknesses in handling long, complex dialogues with motivation transfer and cross-turn dependencies.

Method: MARS-Bench is constructed from play-by-play text commentary to evaluate Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Experiments compare closed-source and open-source LLMs, and analyze attention mechanisms.

Result: Closed-source LLMs outperform open-source ones. Explicit reasoning enhances robustness. LLMs struggle with motivation transfer and cross-turn dependencies. Attention sinks degrade performance in long dialogues.

Conclusion: MARS-Bench addresses gaps in evaluating LLMs' dialogue robustness, revealing performance disparities and mechanistic insights into attention-related degradation.

Abstract: Large Language Models (\textbf{LLMs}), e.g. ChatGPT, have been widely adopted
in real-world dialogue applications. However, LLMs' robustness, especially in
handling long complex dialogue sessions, including frequent motivation
transfer, sophisticated cross-turn dependency, is criticized all along.
Nevertheless, no existing benchmarks can fully reflect these weaknesses. We
present \textbf{MARS-Bench}, a \textbf{M}ulti-turn \textbf{A}thletic
\textbf{R}eal-world \textbf{S}cenario Dialogue \textbf{Bench}mark, designed to
remedy the gap. MARS-Bench is constructed from play-by-play text commentary so
to feature realistic dialogues specifically designed to evaluate three critical
aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,
and Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that
closed-source LLMs significantly outperform open-source alternatives, explicit
reasoning significantly boosts LLMs' robustness on handling long complex
dialogue sessions, and LLMs indeed face significant challenges when handling
motivation transfer and sophisticated cross-turn dependency. Moreover, we
provide mechanistic interpretability on how attention sinks due to special
tokens lead to LLMs' performance degradation when handling long complex
dialogue sessions based on attention visualization experiment in
Qwen2.5-7B-Instruction.

</details>


### [27] [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/pdf/2505.23811)
*Hadi Askari, Shivanshu Gupta, Fei Wang, Anshuman Chhabra, Muhao Chen*

Main category: cs.CL

TL;DR: LayerIF is a data-driven framework using Influence Functions to estimate layer-wise training quality in LLMs, improving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook data influence in assessing layer-wise training quality, limiting LLM performance.

Method: LayerIF isolates layer gradients and computes influence of training examples on validation loss to derive task-specific layer importance.

Result: The framework improves performance in expert allocation and pruning tasks across multiple LLM architectures.

Conclusion: LayerIF provides a principled, task-sensitive approach to enhance LLM layer utilization.

Abstract: Pretrained Large Language Models (LLMs) achieve strong performance across a
wide range of tasks, yet exhibit substantial variability in the various layers'
training quality with respect to specific downstream applications, limiting
their downstream performance.It is therefore critical to estimate layer-wise
training quality in a manner that accounts for both model architecture and
training data. However, existing approaches predominantly rely on model-centric
heuristics (such as spectral statistics, outlier detection, or uniform
allocation) while overlooking the influence of data. To address these
limitations, we propose LayerIF, a data-driven framework that leverages
Influence Functions to quantify the training quality of individual layers in a
principled and task-sensitive manner. By isolating each layer's gradients and
measuring the sensitivity of the validation loss to training examples by
computing layer-wise influences, we derive data-driven estimates of layer
importance. Notably, our method produces task-specific layer importance
estimates for the same LLM, revealing how layers specialize for different
test-time evaluation tasks. We demonstrate the utility of our scores by
leveraging them for two downstream applications: (a) expert allocation in
LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM
pruning. Experiments across multiple LLM architectures demonstrate that our
model-agnostic, influence-guided allocation leads to consistent gains in task
performance.

</details>


### [28] [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/pdf/2505.23812)
*Lata Pangtey, Mohammad Zia Ur Rehman, Prasad Chaudhari, Shubhi Bansal, Nagendra Kumar*

Main category: cs.CL

TL;DR: The paper introduces SPLAENet, a novel method for stance detection in misinformative social media content, using dual cross-attention and emotion-aware techniques, achieving significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of social media and misinformation necessitates effective stance detection to analyze biases and combat misinformation.

Method: SPLAENet employs a dual cross-attention mechanism and hierarchical attention network to capture inter and intra-relationships in text, incorporating emotions and label fusion for improved stance distinction.

Result: SPLAENet outperforms state-of-the-art methods, with average gains of 8.92% in accuracy and 17.36% in F1-score on RumourEval, and similar improvements on SemEval and P-stance datasets.

Conclusion: SPLAENet is effective for stance detection in misinformative social media content, validated by superior performance across multiple datasets.

Abstract: The rapid evolution of social media has generated an overwhelming volume of
user-generated content, conveying implicit opinions and contributing to the
spread of misinformation. The method aims to enhance the detection of stance
where misinformation can polarize user opinions. Stance detection has emerged
as a crucial approach to effectively analyze underlying biases in shared
information and combating misinformation. This paper proposes a novel method
for \textbf{S}tance \textbf{P}rediction through a \textbf{L}abel-fused dual
cross-\textbf{A}ttentive \textbf{E}motion-aware neural \textbf{Net}work
(SPLAENet) in misinformative social media user-generated content. The proposed
method employs a dual cross-attention mechanism and a hierarchical attention
network to capture inter and intra-relationships by focusing on the relevant
parts of source text in the context of reply text and vice versa. We
incorporate emotions to effectively distinguish between different stance
categories by leveraging the emotional alignment or divergence between the
texts. We also employ label fusion that uses distance-metric learning to align
extracted features with stance labels, improving the method's ability to
accurately distinguish between stances. Extensive experiments demonstrate the
significant improvements achieved by SPLAENet over existing state-of-the-art
methods. SPLAENet demonstrates an average gain of 8.92\% in accuracy and
17.36\% in F1-score on the RumourEval dataset. On the SemEval dataset, it
achieves average gains of 7.02\% in accuracy and 10.92\% in F1-score. On the
P-stance dataset, it demonstrates average gains of 10.03\% in accuracy and
11.18\% in F1-score. These results validate the effectiveness of the proposed
method for stance detection in the context of misinformative social media
content.

</details>


### [29] [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/pdf/2505.23815)
*Stéphane Aroca-Ouellette, Natalie Mackraz, Barry-John Theobald, Katherine Metcalf*

Main category: cs.CL

TL;DR: PROSE improves LLM agents' ability to infer nuanced human preferences from writing samples, outperforming existing methods by 33%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inferring user preferences in LLM agents often produce generic descriptions, failing to capture individualized preferences.

Method: PROSE introduces iterative refinement and cross-sample verification of inferred preferences.

Result: PROSE outperforms CIPHER by 33% and, when combined with ICL, improves performance by 9% over ICL alone.

Conclusion: PROSE enhances the precision of preference descriptions, improving personalized interactions with LLM agents.

Abstract: Accommodating human preferences is essential for creating aligned LLM agents
that deliver personalized and effective interactions. Recent work has shown the
potential for LLMs acting as writing agents to infer a description of user
preferences. Agent alignment then comes from conditioning on the inferred
preference description. However, existing methods often produce generic
preference descriptions that fail to capture the unique and individualized
nature of human preferences. This paper introduces PROSE, a method designed to
enhance the precision of preference descriptions inferred from user writing
samples. PROSE incorporates two key elements: (1) iterative refinement of
inferred preferences, and (2) verification of inferred preferences across
multiple user writing samples. We evaluate PROSE with several LLMs (i.e.,
Qwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an
email writing task. We find that PROSE more accurately infers nuanced human
preferences, improving the quality of the writing agent's generations over
CIPHER (a state-of-the-art method for inferring preferences) by 33\%. Lastly,
we demonstrate that ICL and PROSE are complementary methods, and combining them
provides up to a 9\% improvement over ICL alone.

</details>


### [30] [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/pdf/2505.23816)
*Trenton Chang, Tobias Schnabel, Adith Swaminathan, Jenna Wiens*

Main category: cs.CL

TL;DR: Current LLMs struggle with steerability, showing issues like poor coverage, miscalibration, and side effects. Existing interventions have limited effectiveness.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can reliably align outputs with diverse user goals (steerability) and identify common failures.

Method: A multi-dimensional goal space framework evaluates LLM outputs for text attributes. Applied to a text-rewriting task with interventions like prompt engineering and fine-tuning.

Result: LLMs exhibit persistent side effects; interventions like reinforcement learning help but don't fully resolve issues.

Conclusion: Steerability remains a challenge for LLMs, and current alignment strategies may be inadequate.

Abstract: Despite advances in large language models (LLMs) on reasoning and
instruction-following benchmarks, it remains unclear whether they can reliably
produce outputs aligned with a broad variety of user goals, a concept we refer
to as steerability. The abundance of methods proposed to modify LLM behavior
makes it unclear whether current LLMs are already steerable, or require further
intervention. In particular, LLMs may exhibit (i) poor coverage, where rare
user goals are underrepresented; (ii) miscalibration, where models overshoot
requests; and (iii) side effects, where changes to one dimension of text
inadvertently affect others. To systematically evaluate these failures, we
introduce a framework based on a multi-dimensional goal space that models user
goals and LLM outputs as vectors with dimensions corresponding to text
attributes (e.g., reading difficulty). Applied to a text-rewriting task, we
find that current LLMs struggle with steerability, as side effects are
persistent. Interventions to improve steerability, such as prompt engineering,
best-of-$N$ sampling, and reinforcement learning fine-tuning, have varying
effectiveness, yet side effects remain problematic. Our findings suggest that
even strong LLMs struggle with steerability, and existing alignment strategies
may be insufficient. We open-source our steerability evaluation framework at
https://github.com/MLD3/steerability.

</details>


### [31] [Dialectal Coverage And Generalization in Arabic Speech Recognition](https://arxiv.org/pdf/2411.05872)
*Amirbek Djanibekov, Hawau Olamide Toyin, Raghad Alshalan, Abdullah Alitr, Hanan Aldarmaki*

Main category: cs.CL

TL;DR: A suite of ASR models for diverse Arabic variants, including MSA, dialects, and code-switching, outperforms prior models in coverage and performance.


<details>
  <summary>Details</summary>
Motivation: Existing ASR systems lack coverage and generalization for diverse Arabic variants and code-switching, limiting their robustness.

Method: Developed pre-trained and fine-tuned ASR models covering 17 Arabic-speaking countries, 11 variants, and multilingual code-switching.

Result: Demonstrated improved coverage and performance across spoken Arabic varieties compared to prior models.

Conclusion: The introduced ASR models effectively address the diversity of Arabic speech, including dialects and code-switching, outperforming existing solutions.

Abstract: Developing robust automatic speech recognition (ASR) systems for Arabic
requires effective strategies to manage its diversity. Existing ASR systems
mainly cover the modern standard Arabic (MSA) variety and few high-resource
dialects, but fall short in coverage and generalization across the multitude of
spoken variants. Code-switching with English and French is also common in
different regions of the Arab world, which challenges the performance of
monolingual Arabic models. In this work, we introduce a suite of ASR models
optimized to effectively recognize multiple variants of spoken Arabic,
including MSA, various dialects, and code-switching. We provide open-source
pre-trained models that cover data from 17 Arabic-speaking countries, and
fine-tuned MSA and dialectal ASR models that include at least 11 variants, as
well as multi-lingual ASR models covering embedded languages in code-switched
utterances. We evaluate ASR performance across these spoken varieties and
demonstrate both coverage and performance gains compared to prior models.

</details>


### [32] [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/pdf/2505.23818)
*Masoud Safilian, Amin Beheshti, Stephen Elbourn*

Main category: cs.CL

TL;DR: RATAS is a new AI-driven framework for automated answer grading, offering rubric-based, interpretable, and subject-agnostic scoring with high reliability.


<details>
  <summary>Details</summary>
Motivation: Existing grading methods are limited in scope, interpretability, and applicability. RATAS aims to overcome these by leveraging generative AI for diverse and transparent grading.

Method: RATAS uses generative AI models and a mathematical framework for rubric-based grading, tested on a real-world dataset of project-based courses.

Result: RATAS achieves high reliability and accuracy in grading while providing explainable feedback.

Conclusion: RATAS effectively addresses limitations of current grading systems, offering scalable and transparent automated assessment.

Abstract: Automated answer grading is a critical challenge in educational technology,
with the potential to streamline assessment processes, ensure grading
consistency, and provide timely feedback to students. However, existing
approaches are often constrained to specific exam formats, lack
interpretability in score assignment, and struggle with real-world
applicability across diverse subjects and assessment types. To address these
limitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a
novel framework that leverages state-of-the-art generative AI models for
rubric-based grading of textual responses. RATAS is designed to support a wide
range of grading rubrics, enable subject-agnostic evaluation, and generate
structured, explainable rationales for assigned scores. We formalize the
automatic grading task through a mathematical framework tailored to
rubric-based assessment and present an architecture capable of handling
complex, real-world exam structures. To rigorously evaluate our approach, we
construct a unique, contextualized dataset derived from real-world
project-based courses, encompassing diverse response formats and varying levels
of complexity. Empirical results demonstrate that RATAS achieves high
reliability and accuracy in automated grading while providing interpretable
feedback that enhances transparency for both students and nstructors.

</details>


### [33] [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/pdf/2505.23820)
*Bhaktipriya Radharapu, Manon Revel, Megan Ung, Sebastian Ruder, Adina Williams*

Main category: cs.CL

TL;DR: LLMs struggle to replicate human disagreement in ambivalent scenarios, showing biases when acting as judges or debaters despite nuanced open-ended answers.


<details>
  <summary>Details</summary>
Motivation: To investigate the biases and limitations of LLMs in replicating human judgments and preferences, especially in scenarios where humans disagree.

Method: Developed a "no-consensus" benchmark with ambivalent scenarios, testing LLMs as answer generators, judges, and debaters.

Result: LLMs provide nuanced open-ended answers but take stances in no-consensus scenarios as judges or debaters, failing to fully capture human disagreement.

Conclusion: More sophisticated alignment methods are needed, as LLMs cannot fully replicate human disagreement in divided topics.

Abstract: The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has
raised questions about their ability to replicate human judgments and
preferences, especially in ambivalent scenarios where humans disagree. This
study examines the biases and limitations of LLMs in three roles: answer
generator, judge, and debater. These roles loosely correspond to previously
described alignment frameworks: preference alignment (judge) and scalable
oversight (debater), with the answer generator reflecting the typical setting
with user interactions. We develop a ``no-consensus'' benchmark by curating
examples that encompass a variety of a priori ambivalent scenarios, each
presenting two possible stances. Our results show that while LLMs can provide
nuanced assessments when generating open-ended answers, they tend to take a
stance on no-consensus topics when employed as judges or debaters. These
findings underscore the necessity for more sophisticated methods for aligning
LLMs without human oversight, highlighting that LLMs cannot fully capture human
disagreement even on topics where humans themselves are divided.

</details>


### [34] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/pdf/2505.14874)
*Chin-Jou Li, Eunjung Yeo, Kwanghee Choi, Paula Andrea Pérez-Toro, Masao Someki, Rohan Kumar Das, Zhengjun Yue, Juan Rafael Orozco-Arroyave, Elmar Nöth, David R. Mortensen*

Main category: cs.CL

TL;DR: Fine-tuning a voice conversion model on English dysarthric speech to generate non-English dysarthric-like speech improves multilingual ASR performance for dysarthric speech.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in non-English dysarthric ASR by leveraging English dysarthric speech and healthy non-English speech.

Method: Fine-tune a voice conversion model on English dysarthric speech (UASpeech) to encode speaker and prosodic distortions, then apply it to convert healthy non-English speech (FLEURS) into dysarthric-like speech. Use this data to fine-tune a multilingual ASR model (MMS).

Result: VC with speaker and prosody conversion outperforms off-the-shelf MMS and conventional augmentation techniques (e.g., speed/tempo perturbation) on Spanish, Italian, and Tamil datasets. Generated speech simulates dysarthric characteristics.

Conclusion: The proposed method effectively improves dysarthric ASR performance in non-English languages by generating realistic dysarthric-like speech.

Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.

</details>


### [35] [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/pdf/2505.23823)
*Youngseung Jeon, Ziwen Li, Thomas Li, JiaSyuan Chang, Morteza Ziyadi, Xiang 'Anthony' Chen*

Main category: cs.CL

TL;DR: The paper introduces RAGPPI, a benchmark for evaluating the biological impacts of protein-protein interactions (PPIs) in drug development, using 4,420 QA pairs and expert-driven annotation.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing PPI impacts in drug development lack benchmarks, hindering progress in Target ID using LLMs and RAG frameworks.

Method: Developed RAGPPI, a benchmark with 4,420 QA pairs (500 gold-standard, 3,720 silver-standard) using expert annotation and an ensemble auto-evaluation LLM.

Result: Created a scalable, expert-aligned benchmark to advance RAG systems for drug discovery QA.

Conclusion: RAGPPI serves as a valuable resource for improving RAG-based solutions in PPI impact assessment.

Abstract: Retrieving the biological impacts of protein-protein interactions (PPIs) is
essential for target identification (Target ID) in drug development. Given the
vast number of proteins involved, this process remains time-consuming and
challenging. Large Language Models (LLMs) and Retrieval-Augmented Generation
(RAG) frameworks have supported Target ID; however, no benchmark currently
exists for identifying the biological impacts of PPIs. To bridge this gap, we
introduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer
benchmark of 4,420 question-answer pairs that focus on the potential biological
impacts of PPIs. Through interviews with experts, we identified criteria for a
benchmark dataset, such as a type of QA and source. We built a gold-standard
dataset (500 QA pairs) through expert-driven data annotation. We developed an
ensemble auto-evaluation LLM that reflected expert labeling characteristics,
which facilitates the construction of a silver-standard dataset (3,720 QA
pairs). We are committed to maintaining RAGPPI as a resource to support the
research community in advancing RAG systems for drug discovery QA solutions.

</details>


### [36] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/pdf/2505.17076)
*Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Eng Siong Chng*

Main category: cs.CL

TL;DR: The study explores how varying frame rates in speech tokenizers affect tokenization in Mandarin and English, revealing language-specific impacts and implications for optimizing speech tasks.


<details>
  <summary>Details</summary>
Motivation: The impact of frame rates on speech tokenization is underexplored, despite its crucial role in bridging speech signals and language models.

Method: Speech was encoded at different frame rates, and the resulting semantic tokens were evaluated in speech recognition tasks for Mandarin and English.

Result: Frame rate variations affect speech tokenization differently for each language, influenced by phonetic density and acoustic features.

Conclusion: The findings guide frame rate optimization for speech tokenizers, benefiting applications like speech recognition and text-to-speech.

Abstract: The speech tokenizer plays a crucial role in recent speech tasks, generally
serving as a bridge between speech signals and language models. While
low-frame-rate codecs are widely employed as speech tokenizers, the impact of
frame rates on speech tokens remains underexplored. In this study, we
investigate how varying frame rates affect speech tokenization by examining
Mandarin and English, two typologically distinct languages. We encode speech at
different frame rates and evaluate the resulting semantic tokens in the speech
recognition task. Our findings reveal that frame rate variations influence
speech tokenization differently for each language, highlighting the interplay
between frame rates, phonetic density, and language-specific acoustic features.
The results provide insights into optimizing frame rate selection for speech
tokenizers, with implications for automatic speech recognition, text-to-speech,
and other speech-related applications.

</details>


### [37] [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/pdf/2505.23824)
*Tianmai M. Zhang, Neil F. Abernethy*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) as manuscript quality checkers for peer review, proposing baseline methods and an evaluation framework. It validates performance using withdrawn arXiv papers, finding OpenAI's o3 model most effective and o4-mini most cost-efficient.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recruiting domain experts for peer review by leveraging LLMs for manuscript quality checking.

Method: Proposes baseline approaches and an automatic evaluation framework using top LLMs as judges, validated with withdrawn arXiv papers.

Result: OpenAI's o3 model performed best in identifying critical errors, while o4-mini was the most cost-effective.

Conclusion: The study offers insights into scientific reasoning with LLMs and sets a foundation for future applications in peer review.

Abstract: Recent advancements in large language models have sparked interest in
utilizing them to assist the peer review process of scientific publication.
Instead of having AI models generate reviews in the same way as human
reviewers, we propose adopting them as manuscript quality checkers. We
introduce several baseline approaches and an extendable automatic evaluation
framework using top LLMs as judges to tackle the difficulty of recruiting
domain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we
validated our proposed methods with several leading reasoning LLMs from
different providers and assessed their performance and API costs for
identifying critical errors and unsoundness problems. The OpenAI o3 model
performed the best, while o4-mini was the most cost-effective one in our
evaluation. This paper provides insights into document-based scientific
understanding/reasoning and lays the foundation for future applications.

</details>


### [38] [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/pdf/2505.23688)
*James Tanner, Morgan Sonderegger, Jane Stuart-Smith, Jeff Mielke, Tyler Kendall*

Main category: cs.CL

TL;DR: The paper shows that wav2vec2.0 models can accurately classify stop burst presence in English and Japanese, replicating manual annotation patterns, and enabling scalable phonetic research.


<details>
  <summary>Details</summary>
Motivation: Few tools exist for annotating variable phonetic phenomena, despite the need for automation in phonetic research.

Method: Train wav2vec2.0 models to classify stop burst presence in English and Japanese, testing on curated and unprepared speech corpora.

Result: High accuracy in classification, replicating manual annotation patterns and variability in stop realization.

Conclusion: Pre-trained speech models like wav2vec2.0 can automate speech corpus annotation, expanding phonetic research scope.

Abstract: Modern phonetic research regularly makes use of automatic tools for the
annotation of speech data, however few tools exist for the annotation of many
variable phonetic phenomena. At the same time, pre-trained self-supervised
models, such as wav2vec2.0, have been shown to perform well at speech
classification tasks and latently encode fine-grained phonetic information. We
demonstrate that wav2vec2.0 models can be trained to automatically classify
stop burst presence with high accuracy in both English and Japanese, robust
across both finely-curated and unprepared speech corpora. Patterns of
variability in stop realisation are replicated with the automatic annotations,
and closely follow those of manual annotations. These results demonstrate the
potential of pre-trained speech models as tools for the automatic annotation
and processing of speech corpus data, enabling researchers to 'scale-up' the
scope of phonetic research with relative ease.

</details>


### [39] [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/pdf/2505.23827)
*Bangde Du, Ziyi Ye, Zhijing Wu, Jankowska Monika, Shuqi Zhu, Qingyao Ai, Yujia Zhou, Yiqun Liu*

Main category: cs.CL

TL;DR: ValueSim is a framework that simulates individual human values by generating personal backstories, outperforming retrieval-augmented methods by 10% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with individualized human values, beyond broad ethical considerations, is a gap in current techniques.

Method: ValueSim converts structured data into narrative backstories and uses a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate values.

Result: Achieves over 10% improvement in top-1 accuracy compared to retrieval-augmented methods, with performance improving as more user interaction history is available.

Conclusion: ValueSim effectively simulates individualized human values, with potential for refinement over time through additional user data.

Abstract: As Large Language Models (LLMs) continue to exhibit increasingly human-like
capabilities, aligning them with human values has become critically important.
Contemporary advanced techniques, such as prompt learning and reinforcement
learning, are being deployed to better align LLMs with human values. However,
while these approaches address broad ethical considerations and helpfulness,
they rarely focus on simulating individualized human value systems. To address
this gap, we present ValueSim, a framework that simulates individual values
through the generation of personal backstories reflecting past experiences and
demographic information. ValueSim converts structured individual data into
narrative backstories and employs a multi-module architecture inspired by the
Cognitive-Affective Personality System to simulate individual values based on
these narratives. Testing ValueSim on a self-constructed benchmark derived from
the World Values Survey demonstrates an improvement in top-1 accuracy by over
10% compared to retrieval-augmented generation methods. Further analysis
reveals that performance enhances as additional user interaction history
becomes available, indicating the model's ability to refine its persona
simulation capabilities over time.

</details>


### [40] [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/pdf/2505.23829)
*Xiaoqing Cheng, Ruizhe Chen, Hongying Zan, Yuxiang Jia, Min Peng*

Main category: cs.CL

TL;DR: BiasFilter is a model-agnostic, inference-time debiasing framework for LLMs that filters outputs in real time to mitigate social bias without retraining or modifying model parameters.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing methods are costly, ineffective, and hard to scale. BiasFilter addresses these limitations by offering a scalable and efficient solution.

Method: BiasFilter evaluates intermediate outputs periodically, maintains candidate continuations, and discards unfair segments using a fairness reward model trained on a preference dataset.

Result: BiasFilter effectively reduces social bias in LLMs while maintaining generation quality, as shown in extensive experiments.

Conclusion: BiasFilter provides a scalable and efficient solution for mitigating social bias in LLMs during inference, without compromising performance.

Abstract: Mitigating social bias in large language models (LLMs) has become an
increasingly important research objective. However, existing debiasing methods
often incur high human and computational costs, exhibit limited effectiveness,
and struggle to scale to larger models and open-ended generation tasks. To
address these limitations, this paper proposes BiasFilter, a model-agnostic,
inference-time debiasing framework that integrates seamlessly with both
open-source and API-based LLMs. Instead of relying on retraining with balanced
data or modifying model parameters, BiasFilter enforces fairness by filtering
generation outputs in real time. Specifically, it periodically evaluates
intermediate outputs every few tokens, maintains an active set of candidate
continuations, and incrementally completes generation by discarding low-reward
segments based on a fairness reward signal. To support this process, we
construct a fairness preference dataset and train an implicit reward model to
assess token-level fairness in generated responses. Extensive experiments
demonstrate that BiasFilter effectively mitigates social bias across a range of
LLMs while preserving overall generation quality.

</details>


### [41] [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/pdf/2505.23830)
*Linglin Jing, Yuting Gao, Zhigang Wang, Wang Lan, Yiwen Tang, Wenhai Wang, Kaipeng Zhang, Qingpei Guo*

Main category: cs.CL

TL;DR: EvoMoE is a novel MoE tuning framework for MLLMs that addresses expert uniformity and router rigidity through expert evolution and a dynamic token-aware router, outperforming existing methods on multi-modal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal MoE tuning methods suffer from expert uniformity (homogenized experts) and router rigidity (static routing), limiting performance.

Method: EvoMoE introduces expert evolution for diverse expert initialization and a Dynamic Token-aware Router (DTR) for modality-aware routing using hypernetworks.

Result: EvoMoE outperforms other sparse MLLMs on benchmarks like MME, MMBench, TextVQA, and POPE.

Conclusion: EvoMoE effectively enhances MLLM performance by tackling expert uniformity and router rigidity.

Abstract: Recent advancements have shown that the Mixture of Experts (MoE) approach
significantly enhances the capacity of large language models (LLMs) and
improves performance on downstream tasks. Building on these promising results,
multi-modal large language models (MLLMs) have increasingly adopted MoE
techniques. However, existing multi-modal MoE tuning methods typically face two
key challenges: expert uniformity and router rigidity. Expert uniformity occurs
because MoE experts are often initialized by simply replicating the FFN
parameters from LLMs, leading to homogenized expert functions and weakening the
intended diversification of the MoE architecture. Meanwhile, router rigidity
stems from the prevalent use of static linear routers for expert selection,
which fail to distinguish between visual and textual tokens, resulting in
similar expert distributions for image and text. To address these limitations,
we propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a
meticulously designed expert initialization strategy that progressively evolves
multiple robust experts from a single trainable expert, a process termed expert
evolution that specifically targets severe expert homogenization. Furthermore,
we introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism
that allocates input tokens to appropriate experts based on their modality and
intrinsic token values. This dynamic routing is facilitated by hypernetworks,
which dynamically generate routing weights tailored for each individual token.
Extensive experiments demonstrate that EvoMoE significantly outperforms other
sparse MLLMs across a variety of multi-modal benchmarks, including MME,
MMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE
in enhancing the performance of MLLMs by addressing the critical issues of
expert uniformity and router rigidity.

</details>


### [42] [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/pdf/2505.23831)
*Wenhao Ye, Tiansheng Zheng, Yue Qi, Wenhua Zhao, Xiyu Wang, Xue Zhao, Jiacheng He, Yaya Zheng, Dongbo Wang*

Main category: cs.CL

TL;DR: The paper introduces ICH-Qwen, a large language model for preserving and disseminating China's intangible cultural heritage (ICH), leveraging modern AI to address threats from modernization.


<details>
  <summary>Details</summary>
Motivation: The rapid modernization threatens ICH preservation. China's rich ICH resources and UNESCO listings highlight the urgency for innovative solutions like AI.

Method: Developed ICH-Qwen using open-source Chinese ICH data, enhanced with synthetic data and fine-tuning for domain-specific tasks.

Result: ICH-Qwen effectively performs ICH-specific tasks, offering potential for intelligent preservation and dissemination.

Conclusion: The model provides a novel AI-driven approach for ICH sustainability and opens new avenues for digital humanities research.

Abstract: The intangible cultural heritage (ICH) of China, a cultural asset transmitted
across generations by various ethnic groups, serves as a significant testament
to the evolution of human civilization and holds irreplaceable value for the
preservation of historical lineage and the enhancement of cultural
self-confidence. However, the rapid pace of modernization poses formidable
challenges to ICH, including threats damage, disappearance and discontinuity of
inheritance. China has the highest number of items on the UNESCO Intangible
Cultural Heritage List, which is indicative of the nation's abundant cultural
resources and emphasises the pressing need for ICH preservation. In recent
years, the rapid advancements in large language modelling have provided a novel
technological approach for the preservation and dissemination of ICH. This
study utilises a substantial corpus of open-source Chinese ICH data to develop
a large language model, ICH-Qwen, for the ICH domain. The model employs natural
language understanding and knowledge reasoning capabilities of large language
models, augmented with synthetic data and fine-tuning techniques. The
experimental results demonstrate the efficacy of ICH-Qwen in executing tasks
specific to the ICH domain. It is anticipated that the model will provide
intelligent solutions for the protection, inheritance and dissemination of
intangible cultural heritage, as well as new theoretical and practical
references for the sustainable development of intangible cultural heritage.
Furthermore, it is expected that the study will open up new paths for digital
humanities research.

</details>


### [43] [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/pdf/2505.23832)
*Chaeeun Kim, Jinu Lee, Wonseok Hwang*

Main category: cs.CL

TL;DR: The paper introduces LEGAR BENCH, a large-scale Korean Legal Case Retrieval (LCR) benchmark, and LegalSearchLM, a retrieval model that outperforms baselines by 6-20% and generalizes well to out-of-domain cases.


<details>
  <summary>Details</summary>
Motivation: Existing LCR studies are limited by small-scale corpora, narrow query types, and reliance on embedding-based or lexical matching methods, which fail to capture real-world legal complexity.

Method: The authors propose LEGAR BENCH (1.2M cases, 411 crime types) and LegalSearchLM, a model that performs legal element reasoning and generates content grounded in target cases via constrained decoding.

Result: LegalSearchLM outperforms baselines by 6-20% on LEGAR BENCH and shows 15% better generalization to out-of-domain cases compared to naive generative models.

Conclusion: The work addresses limitations in LCR by providing a scalable benchmark and a high-performing retrieval model, advancing the field for real-world legal applications.

Abstract: Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,
is a fundamental task for legal professionals in research and decision-making.
However, existing studies on LCR face two major limitations. First, they are
evaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and
use a narrow range of criminal query types, which cannot sufficiently reflect
the complexity of real-world legal retrieval scenarios. Second, their reliance
on embedding-based or lexical matching methods often results in limited
representations and legally irrelevant matches. To address these issues, we
present: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering
411 diverse crime types in queries over 1.2M legal cases; and (2)
LegalSearchLM, a retrieval model that performs legal element reasoning over the
query case and directly generates content grounded in the target cases through
constrained decoding. Experimental results show that LegalSearchLM outperforms
baselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It
also demonstrates strong generalization to out-of-domain cases, outperforming
naive generative models trained on in-domain data by 15%.

</details>


### [44] [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/pdf/2505.23833)
*Qingchuan Ma, Yuhang Wu, Xiawu Zheng, Rongrong Ji*

Main category: cs.CL

TL;DR: The paper introduces a benchmark to evaluate abstract reasoning in LLMs using two novel metrics, revealing gaps in non-decimal arithmetic and symbolic reasoning despite chain-of-thought prompting.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess abstract reasoning in LLMs by defining it mathematically and measuring it beyond surface-level memorization.

Method: Developed a framework for abstract reasoning, introduced metrics (scoreGamma and scoreDelta), and designed a benchmark with symbol remapping to test genuine pattern recognition.

Result: LLMs show limitations in non-decimal arithmetic and symbolic reasoning, with scoreDelta effectively measuring reliance on memorization.

Conclusion: Current LLMs lack robust abstract reasoning, highlighting areas for future improvement.

Abstract: In this paper, we aim to establish a simple, effective, and theoretically
grounded benchmark for rigorously probing abstract reasoning in Large Language
Models (LLMs). To achieve this, we first develop a mathematic framework that
defines abstract reasoning as the ability to: (i) extract essential patterns
independent of surface representations, and (ii) apply consistent rules to
these abstract patterns. Based on this framework, we introduce two novel
complementary metrics: \(\scoreGamma\) measures basic reasoning accuracy, while
\(\scoreDelta\) quantifies a model's reliance on specific symbols rather than
underlying patterns - a key indicator of true abstraction versus mere
memorization. To implement this measurement, we design a benchmark: systematic
symbol remapping in rule-based tasks, which forces models to demonstrate
genuine pattern recognition beyond superficial token matching. Extensive LLM
evaluations using this benchmark (commercial API models, 7B-70B, multi-agent)
reveal:1) critical limitations in non-decimal arithmetic and symbolic
reasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;
and 3) \(\scoreDelta\)'s effectiveness in robustly measuring memory dependence
by quantifying performance degradation under symbol remapping, particularly
highlighting operand-specific memorization. These findings underscore that
current LLMs, despite domain-specific strengths, still lack robust abstract
reasoning, highlighting key areas for future improvement.

</details>


### [45] [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/pdf/2505.23835)
*Ye Cheng, Minghui Xu, Yue Zhang, Kun Li, Hao Wu, Yechao Zhang, Shaoyong Guo, Wangjie Qiu, Dongxiao Yu, Xiuzhen Cheng*

Main category: cs.CL

TL;DR: LACE is a hybrid framework using LLMs to bridge natural language policy authoring and machine-enforceable logic for IoT access control, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing IoT access control platforms are either coarse-grained or rigid, creating gaps between human intent and machine logic. LACE aims to address this by leveraging LLMs for expressive, interpretable, and verifiable policy management.

Method: LACE combines prompt-guided policy generation, retrieval-augmented reasoning, and formal validation to translate natural language policies into structured rules. It uses a hybrid LLM-rule-based engine for decision-making.

Result: LACE achieves 100% correctness in policy generation and 88% decision accuracy (0.79 F1-score), outperforming baselines like GPT-3.5 and Gemini. It also scales well with policy volume and concurrency.

Conclusion: LACE demonstrates potential for secure, flexible, and user-friendly IoT access control by effectively bridging human intent and machine logic.

Abstract: Access control in the Internet of Things (IoT) is becoming increasingly
complex, as policies must account for dynamic and contextual factors such as
time, location, user behavior, and environmental conditions. However, existing
platforms either offer only coarse-grained controls or rely on rigid rule
matching, making them ill-suited for semantically rich or ambiguous access
scenarios. Moreover, the policy authoring process remains fragmented: domain
experts describe requirements in natural language, but developers must manually
translate them into code, introducing semantic gaps and potential
misconfiguration. In this work, we present LACE, the Language-based Access
Control Engine, a hybrid framework that leverages large language models (LLMs)
to bridge the gap between human intent and machine-enforceable logic. LACE
combines prompt-guided policy generation, retrieval-augmented reasoning, and
formal validation to support expressive, interpretable, and verifiable access
control. It enables users to specify policies in natural language,
automatically translates them into structured rules, validates semantic
correctness, and makes access decisions using a hybrid LLM-rule-based engine.
We evaluate LACE in smart home environments through extensive experiments. LACE
achieves 100% correctness in verified policy generation and up to 88% decision
accuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as
GPT-3.5 and Gemini. The system also demonstrates strong scalability under
increasing policy volume and request concurrency. Our results highlight LACE's
potential to enable secure, flexible, and user-friendly access control across
real-world IoT platforms.

</details>


### [46] [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/pdf/2505.23836)
*Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn*

Main category: cs.CL

TL;DR: AI models can detect evaluation contexts, compromising benchmark reliability. Frontier models show above-random awareness but lag behind humans.


<details>
  <summary>Details</summary>
Motivation: To assess if AI models can identify evaluation contexts, potentially skewing benchmarks and governance decisions.

Method: Constructed a benchmark of 1,000 prompts from 61 datasets, testing models' ability to classify evaluation vs. real-world transcripts.

Result: Models (e.g., Gemini-2.5-Pro, AUC 0.83) show evaluation awareness but trail humans (AUC 0.92). Better performance in agentic vs. chat settings.

Conclusion: Frontier models exhibit notable evaluation-awareness, warranting future tracking to ensure reliable benchmarks.

Abstract: If AI models can detect when they are being evaluated, the effectiveness of
evaluations might be compromised. For example, models could have systematically
different behavior during evaluations, leading to less reliable benchmarks for
deployment and governance decisions. We investigate whether frontier language
models can accurately classify transcripts based on whether they originate from
evaluations or real-world deployment, a capability we call evaluation
awareness. To achieve this, we construct a diverse benchmark of 1,000 prompts
and transcripts from 61 distinct datasets. These span public benchmarks (e.g.,
MMLU, SWEBench), real-world deployment interactions, and agent trajectories
from scaffolding frameworks (e.g., web-browsing agents). Frontier models
clearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches
an AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of
$0.92$). Furthermore, both AI models and humans are better at identifying
evaluations in agentic settings compared to chat settings. Additionally, we
test whether models can identify the purpose of the evaluation. Under
multiple-choice and open-ended questioning, AI models far outperform random
chance in identifying what an evaluation is testing for. Our results indicate
that frontier models already exhibit a substantial, though not yet superhuman,
level of evaluation-awareness. We recommend tracking this capability in future
models.

</details>


### [47] [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/pdf/2505.23837)
*Lin Zhong, Lingzhi Wang, Xu Yang, Qing Liao*

Main category: cs.CL

TL;DR: CoMaPOI, a multi-agent framework, addresses LLM challenges in POI prediction by enhancing semantic understanding and refining candidate spaces, achieving 5-10% performance gains.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with numeric spatiotemporal data and large POI spaces, leading to inaccurate predictions. CoMaPOI aims to solve these issues.

Method: Uses three agents (Profiler, Forecaster, Predictor) to convert data, refine POI space, and generate predictions.

Result: Achieves state-of-the-art performance, improving metrics by 5-10% on NYC, TKY, and CA datasets.

Conclusion: CoMaPOI effectively tackles LLM challenges in spatiotemporal tasks, setting a new benchmark.

Abstract: Large Language Models (LLMs) offer new opportunities for the next
Point-Of-Interest (POI) prediction task, leveraging their capabilities in
semantic understanding of POI trajectories. However, previous LLM-based
methods, which are superficially adapted to next POI prediction, largely
overlook critical challenges associated with applying LLMs to this task.
Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic
understanding of numeric spatiotemporal data, which hinders accurate modeling
of users' spatiotemporal distributions and preferences; and (2) an excessively
large and unconstrained candidate POI space, which often results in random or
irrelevant predictions. To address these issues, we propose a Collaborative
Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close
interaction of three specialized agents (Profiler, Forecaster, and Predictor),
CoMaPOI collaboratively addresses the two critical challenges. The Profiler
agent is responsible for converting numeric data into language descriptions,
enhancing semantic understanding. The Forecaster agent focuses on dynamically
constraining and refining the candidate POI space. The Predictor agent
integrates this information to generate high-precision predictions. Extensive
experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that
CoMaPOI achieves state of the art performance, improving all metrics by 5% to
10% compared to SOTA baselines. This work pioneers the investigation of
challenges associated with applying LLMs to complex spatiotemporal tasks by
leveraging tailored collaborative agents.

</details>


### [48] [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/pdf/2505.23838)
*Yiming Huang, Jiyu Guo, Wenxin Mao, Cuiyun Gao, Peiyi Han, Chuanyi Liu, Qing Ling*

Main category: cs.CL

TL;DR: A systematic review of LLM-based Text-to-SQL, covering trends, techniques, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: To facilitate database access for non-SQL users and leverage LLM advancements in NLP.

Method: Analyzes research trends, techniques, datasets, and evaluation metrics in LLM-based Text-to-SQL.

Result: Provides a comprehensive overview and identifies challenges and future opportunities.

Conclusion: Aims to inspire innovations in LLM-based Text-to-SQL by offering a detailed survey.

Abstract: Converting natural language (NL) questions into SQL queries, referred to as
Text-to-SQL, has emerged as a pivotal technology for facilitating access to
relational databases, especially for users without SQL knowledge. Recent
progress in large language models (LLMs) has markedly propelled the field of
natural language processing (NLP), opening new avenues to improve text-to-SQL
systems. This study presents a systematic review of LLM-based text-to-SQL,
focusing on four key aspects: (1) an analysis of the research trends in
LLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based
text-to-SQL techniques from diverse perspectives; (3) summarization of existing
text-to-SQL datasets and evaluation metrics; and (4) discussion on potential
obstacles and avenues for future exploration in this domain. This survey seeks
to furnish researchers with an in-depth understanding of LLM-based text-to-SQL,
sparking new innovations and advancements in this field.

</details>


### [49] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/pdf/2505.18754)
*Elsen Ronando, Sozo Inoue*

Main category: cs.CL

TL;DR: HED-LM improves few-shot optimization for sensor-based tasks by combining Euclidean distance and LLM-based contextual relevance for example selection, outperforming baselines in fatigue detection.


<details>
  <summary>Details</summary>
Motivation: Few-shot prompting's performance depends on example quality, especially in nuanced tasks like fatigue detection with overlapping patterns and high variability.

Method: HED-LM uses a hybrid pipeline: Euclidean distance filters candidates, and LLMs re-rank them by contextual relevance.

Result: HED-LM achieves a 69.13% F1-score, outperforming random selection (59.30%) and distance-only filtering (67.61%).

Conclusion: Combining numerical and contextual metrics enhances few-shot robustness, making HED-LM practical for sensor-based tasks and broader applications.

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [50] [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/pdf/2505.23840)
*Jiseung Hong, Grace Byun, Seungone Kim, Kai Shu*

Main category: cs.CL

TL;DR: SYCON Bench evaluates LLM sycophancy in multi-turn conversations, revealing alignment tuning worsens it, while scaling and reasoning help resist it. Third-person prompting reduces sycophancy by 63.8%.


<details>
  <summary>Details</summary>
Motivation: LLMs often conform to user beliefs (sycophancy), but prior work focused on single-turn correctness, ignoring real-world dynamics.

Method: Introduces SYCON Bench to measure sycophancy in multi-turn conversations via Turn of Flip and Number of Flip metrics, tested on 17 LLMs.

Result: Alignment tuning increases sycophancy; scaling and reasoning improve resistance. Reasoning models outperform but may over-index on logic. Third-person prompting reduces sycophancy by 63.8%.

Conclusion: Sycophancy is prevalent; scaling, reasoning, and third-person prompting mitigate it. SYCON Bench is released for further research.

Abstract: Large Language Models (LLMs) are expected to provide helpful and harmless
responses, yet they often exhibit sycophancy--conforming to user beliefs
regardless of factual accuracy or ethical soundness. Prior research on
sycophancy has primarily focused on single-turn factual correctness,
overlooking the dynamics of real-world interactions. In this work, we introduce
SYCON Bench, a novel benchmark for evaluating sycophantic behavior in
multi-turn, free-form conversational settings. Our benchmark measures how
quickly a model conforms to the user (Turn of Flip) and how frequently it
shifts its stance under sustained user pressure (Number of Flip). Applying
SYCON Bench to 17 LLMs across three real-world scenarios, we find that
sycophancy remains a prevalent failure mode. Our analysis shows that alignment
tuning amplifies sycophantic behavior, whereas model scaling and reasoning
optimization strengthen the model's ability to resist undesirable user views.
Reasoning models generally outperform instruction-tuned models but often fail
when they over-index on logical exposition instead of directly addressing the
user's underlying beliefs. Finally, we evaluate four additional prompting
strategies and demonstrate that adopting a third-person perspective reduces
sycophancy by up to 63.8% in debate scenario. We release our code and data at
https://github.com/JiseungHong/SYCON-Bench.

</details>


### [51] [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/pdf/2505.23842)
*Zikun Ye, Hema Yoganarasimhan*

Main category: cs.CL

TL;DR: The paper proposes Cluster Shapley, an efficient method to attribute credit to documents in LLM-generated summaries using Shapley values, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of fairly attributing credit to original content creators in LLM-generated summaries, which currently obscures contributions.

Method: Uses Shapley values for credit allocation but approximates them efficiently via Cluster Shapley, leveraging semantic similarity and clustering.

Result: Cluster Shapley reduces computational complexity significantly while maintaining high accuracy, outperforming baseline methods like Monte Carlo sampling and Kernel SHAP.

Conclusion: The method is broadly applicable across various summarization settings, agnostic to the LLM or summarization process used.

Abstract: Large Language Models (LLMs) are increasingly used in systems that retrieve
and summarize content from multiple sources, such as search engines and AI
assistants. While these models enhance user experience by generating coherent
summaries, they obscure the contributions of original content creators, raising
concerns about credit attribution and compensation. We address the challenge of
valuing individual documents used in LLM-generated summaries. We propose using
Shapley values, a game-theoretic method that allocates credit based on each
document's marginal contribution. Although theoretically appealing, Shapley
values are expensive to compute at scale. We therefore propose Cluster Shapley,
an efficient approximation algorithm that leverages semantic similarity between
documents. By clustering documents using LLM-based embeddings and computing
Shapley values at the cluster level, our method significantly reduces
computation while maintaining attribution quality. We demonstrate our approach
to a summarization task using Amazon product reviews. Cluster Shapley
significantly reduces computational complexity while maintaining high accuracy,
outperforming baseline methods such as Monte Carlo sampling and Kernel SHAP
with a better efficient frontier. Our approach is agnostic to the exact LLM
used, the summarization process used, and the evaluation procedure, which makes
it broadly applicable to a variety of summarization settings.

</details>


### [52] [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/pdf/2505.23843)
*Wenhan Dong, Tianyi Hu, Jingyi Zheng, Zhen Sun, Yuemeng Zhao, Yule Liu, Xinlei He, Xinyi Huang*

Main category: cs.CL

TL;DR: The paper highlights limitations in current methods for evaluating LLMs' lateral thinking in multi-round tasks and proposes refined standards for more reliable assessments.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLMs' lateral thinking often produce misleading results, masking issues like shortcut-taking and rigid patterns, which undermine reliability.

Method: The study identifies flaws in current benchmarks and metrics, then proposes new standards: inspecting reasoning paths, diversifying metrics, and comparing with human performance.

Result: Current methods fail to reveal key issues in LLMs' reasoning, necessitating improved evaluation standards.

Conclusion: Refined evaluation standards are proposed to better assess LLMs' true lateral thinking capabilities and ensure reliable evaluations.

Abstract: Multi-round incomplete information tasks are crucial for evaluating the
lateral thinking capabilities of large language models (LLMs). Currently,
research primarily relies on multiple benchmarks and automated evaluation
metrics to assess these abilities. However, our study reveals novel insights
into the limitations of existing methods, as they often yield misleading
results that fail to uncover key issues, such as shortcut-taking behaviors,
rigid patterns, and premature task termination. These issues obscure the true
reasoning capabilities of LLMs and undermine the reliability of evaluations. To
address these limitations, we propose a refined set of evaluation standards,
including inspection of reasoning paths, diversified assessment metrics, and
comparative analyses with human performance.

</details>


### [53] [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/pdf/2505.23844)
*Zhenglun Kong, Zheng Zhan, Shiyue Hou, Yifan Gong, Xin Meng, Pengwei Sui, Peiyan Dong, Xuan Shen, Zifeng Wang, Pu Zhao, Hao Tang, Stratis Ioannidis, Yanzhi Wang*

Main category: cs.CL

TL;DR: A framework for adaptive selection and aggregation of knowledge from diverse LLMs to build a stronger model, reducing interference and memory overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional finetuning and methods like ensemble or weight merging struggle with memory, adaptability, and interference when integrating capabilities from specialized LLMs.

Method: An adaptive selection network identifies relevant source LLMs, followed by dynamic weighted fusion and a feedback-driven loss function to prevent bias.

Result: Reduces knowledge interference by up to 50% and enables stable, scalable knowledge aggregation.

Conclusion: The proposed framework effectively addresses limitations of existing methods, offering a more flexible and efficient approach to LLM integration.

Abstract: Large language models (LLMs) have shown remarkable promise but remain
challenging to continually improve through traditional finetuning, particularly
when integrating capabilities from other specialized LLMs. Popular methods like
ensemble and weight merging require substantial memory and struggle to adapt to
changing data environments. Recent efforts have transferred knowledge from
multiple LLMs into a single target model; however, they suffer from
interference and degraded performance among tasks, largely due to limited
flexibility in candidate selection and training pipelines. To address these
issues, we propose a framework that adaptively selects and aggregates knowledge
from diverse LLMs to build a single, stronger model, avoiding the high memory
overhead of ensemble and inflexible weight merging. Specifically, we design an
adaptive selection network that identifies the most relevant source LLMs based
on their scores, thereby reducing knowledge interference. We further propose a
dynamic weighted fusion strategy that accounts for the inherent strengths of
candidate LLMs, along with a feedback-driven loss function that prevents the
selector from converging on a single subset of sources. Experimental results
demonstrate that our method can enable a more stable and scalable knowledge
aggregation process while reducing knowledge interference by up to 50% compared
to existing approaches. Code is avaliable at
https://github.com/ZLKong/LLM_Integration

</details>


### [54] [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/pdf/2505.23845)
*Jakub Podolak, Rajeev Verma*

Main category: cs.CL

TL;DR: DeepSeek R1-32B's self-reported confidence is often overconfident, but semantic entropy (via sampling) is reliable. Forcing chain-of-thought improves confidence accuracy, and a reader model can replicate confidences, suggesting confidence is a statistic of explored alternatives.


<details>
  <summary>Details</summary>
Motivation: To understand the source of uncertainty in DeepSeek R1-32B and improve the reliability of its self-reported confidence on QA tasks.

Method: Analyze verbal confidence in QA tasks, compare default confidence with semantic entropy, and test the impact of chain-of-thought reasoning. Also, evaluate a reader model's ability to reconstruct confidences.

Result: Semantic entropy is reliable; chain-of-thought improves confidence accuracy. A reader model can replicate confidences, indicating confidence reflects explored alternatives.

Conclusion: Reliable uncertainty estimation requires exploring the generative space; self-reported confidence is trustworthy only after such exploration.

Abstract: We study the source of uncertainty in DeepSeek R1-32B by analyzing its
self-reported verbal confidence on question answering (QA) tasks. In the
default answer-then-confidence setting, the model is regularly over-confident,
whereas semantic entropy - obtained by sampling many responses - remains
reliable. We hypothesize that this is because of semantic entropy's larger
test-time compute, which lets us explore the model's predictive distribution.
We show that granting DeepSeek the budget to explore its distribution by
forcing a long chain-of-thought before the final answer greatly improves its
verbal score effectiveness, even on simple fact-retrieval questions that
normally require no reasoning. Furthermore, a separate reader model that sees
only the chain can reconstruct very similar confidences, indicating the verbal
score might be merely a statistic of the alternatives surfaced during
reasoning. Our analysis concludes that reliable uncertainty estimation requires
explicit exploration of the generative space, and self-reported confidence is
trustworthy only after such exploration.

</details>


### [55] [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/pdf/2505.23848)
*Harvey Dam, Jonas Knochelmann, Vinu Joseph, Ganesh Gopalakrishnan*

Main category: cs.CL

TL;DR: A method to reduce LLM refusal rates on sensitive content by adjusting token probabilities during generation, without modifying weights or prompts.


<details>
  <summary>Details</summary>
Motivation: Observed that refusals often follow specific token sequences (e.g., <think> followed by \n\n), prompting investigation into simple formatting adjustments.

Method: Suppress \n\n after <think> and the end-of-sequence token after </think> during generation, without datasets or training.

Result: Increased substantive answers to sensitive prompts without impacting standard benchmark performance.

Conclusion: Refusal behaviors can be circumvented by blocking refusal subspaces during generation.

Abstract: We introduce a method to reduce refusal rates of large language models (LLMs)
on sensitive content without modifying model weights or prompts. Motivated by
the observation that refusals in certain models were often preceded by the
specific token sequence of a token marking the beginning of the
chain-of-thought (CoT) block (<think>) followed by a double newline token
(\n\n), we investigate the impact of two simple formatting adjustments during
generation: suppressing \n\n after <think> and suppressing the end-of-sequence
token after the end of the CoT block (</think>). Our method requires no
datasets, parameter changes, or training, relying solely on modifying token
probabilities during generation. In our experiments with official DeepSeek-R1
distillations, these interventions increased the proportion of substantive
answers to sensitive prompts without affecting performance on standard
benchmarks. Our findings suggest that refusal behaviors can be circumvented by
blocking refusal subspaces at specific points in the generation process.

</details>


### [56] [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/pdf/2505.23851)
*Michael Shalyt, Rotem Elimelech, Ido Kaminer*

Main category: cs.CL

TL;DR: ASyMOB is a new framework to assess LLMs in symbolic math, revealing their reliance on memorization and limited generalization. Advanced models show robustness, but integration with tools may be key.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to evaluate LLMs' core symbolic math skills like integration and algebraic simplification.

Method: Introduces ASyMOB, a framework with 17,092 math challenges, analyzing LLM performance via perturbations.

Result: LLMs degrade significantly with perturbations (-70.3%), but advanced models (o4-mini, Gemini 2.5 Flash) show robustness (-21.7%, -21.2%). Integrated code execution improves accuracy (+33.1%).

Conclusion: LLMs rely on memorization, but advanced models hint at improved generalization. Future may involve deeper tool integration or surpassing CAS capabilities.

Abstract: Large language models (LLMs) are rapidly approaching the level of proficiency
in university-level symbolic mathematics required for applications in advanced
science and technology. However, existing benchmarks fall short in assessing
the core skills of LLMs in symbolic mathematics-such as integration,
differential equations, and algebraic simplification. To address this gap, we
introduce ASyMOB, a novel assessment framework focused exclusively on symbolic
manipulation, featuring 17,092 unique math challenges, organized by similarity
and complexity. ASyMOB enables analysis of LLM generalization capabilities by
comparing performance in problems that differ by simple numerical or symbolic
`perturbations'. Evaluated LLMs exhibit substantial degradation in performance
for all perturbation types (up to -70.3%), suggesting reliance on memorized
patterns rather than deeper understanding of symbolic math, even among models
achieving high baseline accuracy. Comparing LLM performance to computer algebra
systems, we identify examples where they fail while LLMs succeed, as well as
problems solved only by combining both approaches. Models capable of integrated
code execution yielded higher accuracy compared to their performance without
code, particularly stabilizing weaker models (up to +33.1% for certain
perturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5
Flash) demonstrate not only high symbolic math proficiency (scoring 96.8% and
97.6% on the unperturbed set), but also remarkable robustness against
perturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).
This may indicate a recent "phase transition" in the generalization
capabilities of frontier LLMs. It remains to be seen whether the path forward
lies in deeper integration with sophisticated external tools, or in developing
models so capable that symbolic math systems like CAS become unnecessary.

</details>


### [57] [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/pdf/2505.23854)
*Linwei Tao, Yi-Fan Yeh, Minjing Dong, Tao Huang, Philip Torr, Chang Xu*

Main category: cs.CL

TL;DR: A comprehensive study evaluates uncertainty estimation in 80 LLMs, finding LVU outperforms TPU and NVU in calibration and interpretability, with performance influenced by model scale, post-training, and task type.


<details>
  <summary>Details</summary>
Motivation: To ensure safe and trustworthy deployment of LLMs in high-stakes applications by improving uncertainty estimation.

Method: Evaluated 80 LLMs using three black-box methods (TPU, NVU, LVU) on the MMLU-Pro benchmark, analyzing calibration and selective classification.

Result: LVU is superior to TPU and NVU, with performance affected by model scale, post-training, reasoning ability, and task type. High accuracy doesn't guarantee reliable uncertainty.

Conclusion: Multi-perspective evaluation is crucial, and LVU is a practical tool for enhancing LLM reliability in real-world applications.

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
applications, robust uncertainty estimation is essential for ensuring the safe
and trustworthy deployment of LLMs. We present the most comprehensive study to
date of uncertainty estimation in LLMs, evaluating 80 models spanning open- and
closed-source families, dense and Mixture-of-Experts (MoE) architectures,
reasoning and non-reasoning modes, quantization variants and parameter scales
from 0.6B to 671B. Focusing on three representative black-box single-pass
methods, including token probability-based uncertainty (TPU), numerical verbal
uncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically
evaluate uncertainty calibration and selective classification using the
challenging MMLU-Pro benchmark, which covers both reasoning-intensive and
knowledge-based tasks. Our results show that LVU consistently outperforms TPU
and NVU, offering stronger calibration and discrimination while being more
interpretable. We also find that high accuracy does not imply reliable
uncertainty, and that model scale, post-training, reasoning ability and
quantization all influence estimation performance. Notably, LLMs exhibit better
uncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good
calibration does not necessarily translate to effective error ranking. These
findings highlight the need for multi-perspective evaluation and position LVU
as a practical tool for improving the reliability of LLMs in real-world
settings.

</details>


### [58] [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/pdf/2505.23856)
*Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh*

Main category: cs.CL

TL;DR: OMNIGUARD is a method to detect harmful prompts in LLMs across languages and modalities, improving accuracy and efficiency over baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the fallibility of current harmful prompt detection methods, especially against attacks exploiting mismatched generalization in low-resource languages or non-text modalities.

Method: Identifies aligned internal representations in LLMs/MLLMs to build language- and modality-agnostic classifiers for harmful prompts.

Result: Improves classification accuracy by 11.57% (multilingual), 20.44% (image-based), and sets SOTA for audio-based prompts, with ≈120× speedup.

Conclusion: OMNIGUARD effectively enhances harmful prompt detection across diverse inputs while being highly efficient.

Abstract: The emerging capabilities of large language models (LLMs) have sparked
concerns about their immediate potential for harmful misuse. The core approach
to mitigate these concerns is the detection of harmful queries to the model.
Current detection approaches are fallible, and are particularly susceptible to
attacks that exploit mismatched generalization of model capabilities (e.g.,
prompts in low-resource languages or prompts provided in non-text modalities
such as image and audio). To tackle this challenge, we propose OMNIGUARD, an
approach for detecting harmful prompts across languages and modalities. Our
approach (i) identifies internal representations of an LLM/MLLM that are
aligned across languages or modalities and then (ii) uses them to build a
language-agnostic or modality-agnostic classifier for detecting harmful
prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\%
over the strongest baseline in a multilingual setting, by 20.44\% for
image-based prompts, and sets a new SOTA for audio-based prompts. By
repurposing embeddings computed during generation, OMNIGUARD is also very
efficient ($\approx 120 \times$ faster than the next fastest baseline). Code
and data are available at: https://github.com/vsahil/OmniGuard.

</details>


### [59] [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/pdf/2505.23867)
*Zeyu Liu, Zhitian Hou, Yining Di, Kejing Yang, Zhijie Sang, Congkai Xie, Jingwen Yang, Siyuan Liu, Jialu Wang, Chunming Li, Ming Li, Hongxia Yang*

Main category: cs.CL

TL;DR: Infi-Med is a framework for medical MLLMs, improving resource efficiency, multimodal reasoning, and evaluation, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in deploying MLLMs in healthcare, such as resource efficiency, accuracy, and ethical concerns.

Method: Proposes Infi-Med with innovations: efficient SFT datasets, enhanced multimodal reasoning, and systematic evaluation.

Result: Achieves SOTA performance in medical reasoning and adaptability to clinical scenarios.

Conclusion: Infi-Med balances effectiveness and operational constraints, enabling real-world healthcare deployment.

Abstract: Multimodal large language models (MLLMs) have demonstrated promising
prospects in healthcare, particularly for addressing complex medical tasks,
supporting multidisciplinary treatment (MDT), and enabling personalized
precision medicine. However, their practical deployment faces critical
challenges in resource efficiency, diagnostic accuracy, clinical
considerations, and ethical privacy. To address these limitations, we propose
Infi-Med, a comprehensive framework for medical MLLMs that introduces three key
innovations: (1) a resource-efficient approach through curating and
constructing high-quality supervised fine-tuning (SFT) datasets with minimal
sample requirements, with a forward-looking design that extends to both
pretraining and posttraining phases; (2) enhanced multimodal reasoning
capabilities for cross-modal integration and clinical task understanding; and
(3) a systematic evaluation system that assesses model performance across
medical modalities and task types. Our experiments demonstrate that Infi-Med
achieves state-of-the-art (SOTA) performance in general medical reasoning while
maintaining rapid adaptability to clinical scenarios. The framework establishes
a solid foundation for deploying MLLMs in real-world healthcare settings by
balancing model effectiveness with operational constraints.

</details>


### [60] [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/pdf/2505.23911)
*Pavel Tikhonov, Ivan Oseledets, Elena Tutubalina*

Main category: cs.CL

TL;DR: ICL in LLMs uses task vectors for adaptation; QuiteAFew dataset (3,096 tasks) reveals task vectors peak at intermediate layers, vary by task type, and involve distributed representation for complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing studies on ICL and task vectors are limited by small-scale benchmarks, hindering thorough analysis.

Method: Introduce QuiteAFew, a dataset of 3,096 tasks, and test Llama-3-8B to analyze task vector behavior.

Result: Task vectors peak at intermediate layers (e.g., 15th), vary by task type, and complex tasks use distributed vectors.

Conclusion: Task knowledge in LLMs is distributed, with performance depending on layer depth and task complexity.

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to adapt to
new tasks using few examples, with task vectors - specific hidden state
activations - hypothesized to encode task information. Existing studies are
limited by small-scale benchmarks, restricting comprehensive analysis. We
introduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with
30 input-output pairs derived from the Alpaca dataset. Experiments with
Llama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an
intermediate layer (e.g., 15th), (2) effectiveness varies significantly by task
type, and (3) complex tasks rely on multiple, subtask-specific vectors rather
than a single vector, suggesting distributed task knowledge representation.

</details>


### [61] [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/pdf/2505.23912)
*Caiqi Zhang, Xiaochen Zhu, Chengzu Li, Nigel Collier, Andreas Vlachos*

Main category: cs.CL

TL;DR: LoVeC introduces an efficient RL-based method for verbalized confidence estimation in long-form LLM generation, improving calibration and domain generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing hallucination in LLMs by proposing a more efficient and generalizable confidence estimation method for long-form generation.

Method: Uses reinforcement learning (RL) to train LLMs to append numerical confidence scores to each generated statement, employing methods like DPO, ORPO, and GRPO.

Result: Achieves better calibration and robust generalization across domains, with high efficiency due to minimal token overhead.

Conclusion: LoVeC is a promising approach for trustworthy long-form generation, balancing accuracy and computational efficiency.

Abstract: Hallucination remains a major challenge for the safe and trustworthy
deployment of large language models (LLMs) in factual content generation. Prior
work has explored confidence estimation as an effective approach to
hallucination detection, but often relies on post-hoc self-consistency methods
that require computationally expensive sampling. Verbalized confidence offers a
more efficient alternative, but existing approaches are largely limited to
short-form question answering (QA) tasks and do not generalize well to
open-ended generation. In this paper, we propose LoVeC (Long-form Verbalized
Confidence), an on-the-fly verbalized confidence estimation method for
long-form generation. Specifically, we use reinforcement learning (RL) to train
LLMs to append numerical confidence scores to each generated statement, serving
as a direct and interpretable signal of the factuality of generation. Our
experiments consider both on-policy and off-policy RL methods, including DPO,
ORPO, and GRPO, to enhance the model calibration. We introduce two novel
evaluation settings, free-form tagging and iterative tagging, to assess
different verbalized confidence estimation methods. Experiments on three
long-form QA datasets show that our RL-trained models achieve better
calibration and generalize robustly across domains. Also, our method is highly
efficient, as it only requires adding a few tokens to the output being decoded.

</details>


### [62] [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/pdf/2505.23914)
*Yuxin Wang, Botao Yu, Ivory Yang, Saeed Hassanpour, Soroush Vosoughi*

Main category: cs.CL

TL;DR: LLMs misclassify benign comments as toxic due to topic biases, not just offensive terms. Topic Association Analysis reveals advanced models like GPT-4 Turbo show stronger topic stereotypes despite lower false positives.


<details>
  <summary>Details</summary>
Motivation: To uncover why LLMs over-sensitively misclassify benign comments, moving beyond token-level explanations to explore implicit topic associations.

Method: Introduces Topic Association Analysis, prompting LLMs to generate scenarios for misclassified comments and analyzing topic amplification levels.

Result: Advanced LLMs exhibit stronger topic biases, relying on learned associations rather than explicit language, shaping moderation decisions.

Conclusion: Highlights the need for refining moderation beyond keyword filtering, addressing underlying topic biases in LLMs.

Abstract: Large Language Models are widely used for content moderation but often
misclassify benign comments as toxic, leading to over-sensitivity. While
previous research attributes this issue primarily to the presence of offensive
terms, we reveal a potential cause beyond token level: LLMs exhibit systematic
topic biases in their implicit associations. Inspired by cognitive psychology's
implicit association tests, we introduce Topic Association Analysis, a
semantic-level approach to quantify how LLMs associate certain topics with
toxicity. By prompting LLMs to generate free-form scenario imagination for
misclassified benign comments and analyzing their topic amplification levels,
we find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger
topic stereotype despite lower overall false positive rates. These biases
suggest that LLMs do not merely react to explicit, offensive language but rely
on learned topic associations, shaping their moderation decisions. Our findings
highlight the need for refinement beyond keyword-based filtering, providing
insights into the underlying mechanisms driving LLM over-sensitivity.

</details>


### [63] [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/pdf/2505.23923)
*Feiteng Fang, Ting-En Lin, Yuchuan Wu, Xiong Liu, Xiang Huang, Dingwei Chen, Jing Ye, Haonan Zhang, Liang Zhu, Hamid Alinejad-Rokny, Min Yang, Fei Huang, Yongbin Li*

Main category: cs.CL

TL;DR: ChARM improves RPLAs with act-adaptive margins and self-evolution, outperforming traditional models by 13%.


<details>
  <summary>Details</summary>
Motivation: Address scalability and subjective preference challenges in RPLAs.

Method: Introduces act-adaptive margins and self-evolution using unlabeled data.

Result: 13% improvement in preference rankings; state-of-the-art results on benchmarks.

Conclusion: ChARM enhances RPLAs with efficient learning and broader training coverage.

Abstract: Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic
and engaging human-computer interactions. However, traditional reward models
often struggle with scalability and adapting to subjective conversational
preferences. We propose ChARM, a Character-based Act-adaptive Reward Model,
addressing these challenges through two innovations: (1) an act-adaptive margin
that significantly enhances learning efficiency and generalizability, and (2) a
self-evolution mechanism leveraging large-scale unlabeled data to improve
training coverage. Additionally, we introduce RoleplayPref, the first
large-scale preference dataset specifically for RPLAs, featuring 1,108
characters, 13 subcategories, and 16,888 bilingual dialogues, alongside
RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%
improvement over the conventional Bradley-Terry model in preference rankings.
Furthermore, applying ChARM-generated rewards to preference learning techniques
(e.g., direct preference optimization) achieves state-of-the-art results on
CharacterEval and RoleplayEval. Code and dataset are available at
https://github.com/calubkk/ChARM.

</details>


### [64] [Scaling up the think-aloud method](https://arxiv.org/pdf/2505.23931)
*Daniel Wurgaft, Ben Prystawski, Kanishk Gandhi, Cedegao E. Zhang, Joshua B. Tenenbaum, Noah D. Goodman*

Main category: cs.CL

TL;DR: Automated transcription and annotation of think-aloud data using NLP enables large-scale analysis, demonstrated with 640 participants solving a math task.


<details>
  <summary>Details</summary>
Motivation: Revive the think-aloud method in cognitive science by overcoming labor-intensive transcription and annotation barriers.

Method: Used NLP tools to transcribe and annotate verbal reports, coding transcripts as search graphs for analysis.

Result: Achieved moderate inter-rater reliability with humans and characterized reasoning patterns.

Conclusion: Demonstrates feasibility and value of automated think-aloud analysis at scale.

Abstract: The think-aloud method, where participants voice their thoughts as they solve
a task, is a valuable source of rich data about human reasoning processes. Yet,
it has declined in popularity in contemporary cognitive science, largely
because labor-intensive transcription and annotation preclude large sample
sizes. Here, we develop methods to automate the transcription and annotation of
verbal reports of reasoning using natural language processing tools, allowing
for large-scale analysis of think-aloud data. In our study, 640 participants
thought aloud while playing the Game of 24, a mathematical reasoning task. We
automatically transcribed the recordings and coded the transcripts as search
graphs, finding moderate inter-rater reliability with humans. We analyze these
graphs and characterize consistency and variation in human reasoning traces.
Our work demonstrates the value of think-aloud data at scale and serves as a
proof of concept for the automated analysis of verbal reports.

</details>


### [65] [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/pdf/2505.23932)
*Wendong Xu, Jing Xiong, Chenyang Zhao, Qiujiang Chen, Haoran Wang, Hui Shen, Zhongwei Wan, Jianbo Dai, Taiqiang Wu, He Xiao, Chaofan Tao, Z. Morley Mao, Ying Sheng, Zhijiang Guo, Hongxia Yang, Bei Yu, Lingpeng Kong, Quanquan Gu, Ngai Wong*

Main category: cs.CL

TL;DR: SwingArena is a competitive LLM evaluation framework mimicking real-world software workflows, using patch generation and CI-driven testing.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs in realistic software development contexts, moving beyond static benchmarks.

Method: Pairs LLMs as submitters (patch generation) and reviewers (test case creation, CI validation), using a retrieval-augmented code generation module for efficiency.

Result: GPT-4o excels in aggressive patch generation, while DeepSeek and Gemini prioritize correctness in CI validation.

Conclusion: SwingArena offers a scalable, extensible method for evaluating LLMs in CI-driven software development.

Abstract: We present SwingArena, a competitive evaluation framework for Large Language
Models (LLMs) that closely mirrors real-world software development workflows.
Unlike traditional static benchmarks, SwingArena models the collaborative
process of software iteration by pairing LLMs as submitters, who generate
patches, and reviewers, who create test cases and verify the patches through
continuous integration (CI) pipelines. To support these interactive
evaluations, we introduce a retrieval-augmented code generation (RACG) module
that efficiently handles long-context challenges by providing syntactically and
semantically relevant code snippets from large codebases, supporting multiple
programming languages (C++, Python, Rust, and Go). This enables the framework
to scale across diverse tasks and contexts while respecting token limitations.
Our experiments, using over 400 high-quality real-world GitHub issues selected
from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive
patch generation, whereas DeepSeek and Gemini prioritize correctness in CI
validation. SwingArena presents a scalable and extensible methodology for
evaluating LLMs in realistic, CI-driven software development settings. More
details are available on our project page: swing-bench.github.io

</details>


### [66] [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/pdf/2505.23944)
*Thushara Manjari Naduvilakandy, Hyeju Jang, Mohammad Al Hasan*

Main category: cs.CL

TL;DR: The paper proposes RAG-based dynamic prompting for LLMs to improve causality detection and mining, outperforming static methods.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised methods for causality detection perform poorly and require human intervention, while supervised methods lack large datasets. LLMs with prompt engineering offer a solution, but comprehensive studies are missing.

Method: The authors introduce retrieval-augmented generation (RAG)-based dynamic prompting schemes to enhance LLM performance in causality tasks.

Result: Experiments on three datasets and five LLMs show the proposed method's superiority over static prompting.

Conclusion: RAG-based dynamic prompting effectively addresses limitations in causality detection and mining, offering better performance and generalization.

Abstract: Causality detection and mining are important tasks in information retrieval
due to their enormous use in information extraction, and knowledge graph
construction. To solve these tasks, in existing literature there exist several
solutions -- both unsupervised and supervised. However, the unsupervised
methods suffer from poor performance and they often require significant human
intervention for causal rule selection, leading to poor generalization across
different domains. On the other hand, supervised methods suffer from the lack
of large training datasets. Recently, large language models (LLMs) with
effective prompt engineering are found to be effective to overcome the issue of
unavailability of large training dataset. Yet, in existing literature, there
does not exist comprehensive works on causality detection and mining using LLM
prompting. In this paper, we present several retrieval-augmented generation
(RAG) based dynamic prompting schemes to enhance LLM performance in causality
detection and extraction tasks. Extensive experiments over three datasets and
five LLMs validate the superiority of our proposed RAG-based dynamic prompting
over other static prompting schemes.

</details>


### [67] [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/pdf/2505.23945)
*Sriram Balasubramanian, Samyadeep Basu, Soheil Feizi*

Main category: cs.CL

TL;DR: The paper investigates the faithfulness of Chain-of-thought (CoT) reasoning in large vision-language models (LVLMs), revealing biases in reasoning and introducing a novel evaluation pipeline for precise analysis.


<details>
  <summary>Details</summary>
Motivation: To understand whether CoT reasoning traces in LVLMs reflect their internal processes and how biases (text-based and image-based) affect reasoning.

Method: A fine-grained evaluation pipeline is introduced to categorize bias articulation patterns, analyzing CoT reasoning more precisely than previous methods.

Result: Subtle image-based biases are rarely articulated compared to text-based ones, and models exhibit inconsistent reasoning. Similar issues persist in language-only models.

Conclusion: The study highlights limitations in CoT faithfulness, especially for implicit cues, and provides tools for detecting biased reasoning.

Abstract: Chain-of-thought (CoT) reasoning enhances performance of large language
models, but questions remain about whether these reasoning traces faithfully
reflect the internal processes of the model. We present the first comprehensive
study of CoT faithfulness in large vision-language models (LVLMs),
investigating how both text-based and previously unexplored image-based biases
affect reasoning and bias articulation. Our work introduces a novel,
fine-grained evaluation pipeline for categorizing bias articulation patterns,
enabling significantly more precise analysis of CoT reasoning than previous
methods. This framework reveals critical distinctions in how models process and
respond to different types of biases, providing new insights into LVLM CoT
faithfulness. Our findings reveal that subtle image-based biases are rarely
articulated compared to explicit text-based ones, even in models specialized
for reasoning. Additionally, many models exhibit a previously unidentified
phenomenon we term ``inconsistent'' reasoning - correctly reasoning before
abruptly changing answers, serving as a potential canary for detecting biased
reasoning from unfaithful CoTs. We then apply the same evaluation pipeline to
revisit CoT faithfulness in LLMs across various levels of implicit cues. Our
findings reveal that current language-only reasoning models continue to
struggle with articulating cues that are not overtly stated.

</details>


### [68] [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/pdf/2505.23966)
*Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, Zheng Zhang*

Main category: cs.CL

TL;DR: FLAT-LLM is a training-free, fine-grained low-rank compression method for LLMs, improving efficiency and accuracy without recovery fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and memory demands of LLMs in resource-constrained environments, while avoiding accuracy degradation and inefficient architectures from existing methods.

Method: Uses head-wise PCA for low-rank transformations in activation space, adaptively allocating ranks across decoders with an importance-based metric.

Result: Outperforms structural pruning baselines in generalization and downstream performance, achieving faster inference than decomposition-based methods.

Conclusion: FLAT-LLM offers a practical, efficient, and accurate solution for compressing LLMs without fine-tuning.

Abstract: Large Language Models (LLMs) have enabled remarkable progress in natural
language processing, yet their high computational and memory demands pose
challenges for deployment in resource-constrained environments. Although recent
low-rank decomposition methods offer a promising path for structural
compression, they often suffer from accuracy degradation, expensive calibration
procedures, and result in inefficient model architectures that hinder
real-world inference speedups. In this paper, we propose FLAT-LLM, a fast and
accurate, training-free structural compression method based on fine-grained
low-rank transformations in the activation space. Specifically, we reduce the
hidden dimension by transforming the weights using truncated eigenvectors
computed via head-wise Principal Component Analysis (PCA), and employ an
importance-based metric to adaptively allocate ranks across decoders. FLAT-LLM
achieves efficient and effective weight compression without recovery
fine-tuning, which could complete the calibration within a few minutes.
Evaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural
pruning baselines in generalization and downstream performance, while
delivering inference speedups over decomposition-based methods.

</details>


### [69] [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/pdf/2505.23996)
*Yinong Oliver Wang, Nivedha Sivakumar, Falaah Arif Khan, Rin Metcalf Susa, Adam Golinski, Natalie Mackraz, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff*

Main category: cs.CL

TL;DR: The paper proposes UCerF, an uncertainty-aware fairness metric for LLMs, and introduces a new gender-occupation dataset. It benchmarks ten LLMs, revealing biases overlooked by traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Current fairness metrics fail to account for model uncertainty, leading to incomplete bias assessments in LLMs.

Method: Developed UCerF for fine-grained fairness evaluation and created a diverse dataset (31,756 samples) for benchmarking.

Result: Mistral-7B showed suboptimal fairness due to high confidence in incorrect predictions, detected by UCerF but not traditional metrics.

Conclusion: The UCerF metric and new dataset advance fairness evaluation in LLMs, promoting more transparent and accountable AI.

Abstract: The recent rapid adoption of large language models (LLMs) highlights the
critical need for benchmarking their fairness. Conventional fairness metrics,
which focus on discrete accuracy-based evaluations (i.e., prediction
correctness), fail to capture the implicit impact of model uncertainty (e.g.,
higher model confidence about one group over another despite similar accuracy).
To address this limitation, we propose an uncertainty-aware fairness metric,
UCerF, to enable a fine-grained evaluation of model fairness that is more
reflective of the internal bias in model decisions compared to conventional
fairness measures. Furthermore, observing data size, diversity, and clarity
issues in current datasets, we introduce a new gender-occupation fairness
evaluation dataset with 31,756 samples for co-reference resolution, offering a
more diverse and suitable dataset for evaluating modern LLMs. We establish a
benchmark, using our metric and dataset, and apply it to evaluate the behavior
of ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness
due to high confidence in incorrect predictions, a detail overlooked by
Equalized Odds but captured by UCerF. Overall, our proposed LLM benchmark,
which evaluates fairness with uncertainty awareness, paves the way for
developing more transparent and accountable AI systems.

</details>


### [70] [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/pdf/2505.24009)
*Hidetaka Kamigaito, Ying Zhang, Jingun Kwon, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe*

Main category: cs.CL

TL;DR: The paper explores how layer diversity in Transformers impacts performance, linking internal mechanisms to parameter scaling laws. It shows that diverse layers enhance performance, especially when outputs are far from ground truth, and reveals submodular gains from adding layers.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between understanding Transformer internal mechanisms (like residual streams) and parameter scaling laws, focusing on layer diversity and its role in performance.

Method: Theoretical investigation using a bias-diversity decomposition of layers in the residual stream, followed by empirical validation on semantic tasks with various LLMs.

Result: Performance improves when layers are diverse and close to ground truth. Diversity is critical when outputs are inaccurate. Adding layers boosts performance only if they are diverse, with diminishing returns (submodularity).

Conclusion: Layer diversity is key to Transformer performance, aligning with scaling laws. Submodular gains suggest careful layer addition is optimal.

Abstract: Transformers deliver outstanding performance across a wide range of tasks and
are now a dominant backbone architecture for large language models (LLMs).
Their task-solving performance is improved by increasing parameter size, as
shown in the recent studies on parameter scaling laws. Although recent
mechanistic-interpretability studies have deepened our understanding of the
internal behavior of Transformers by analyzing their residual stream, the
relationship between these internal mechanisms and the parameter scaling laws
remains unclear. To bridge this gap, we focus on layers and their size, which
mainly decide the parameter size of Transformers. For this purpose, we first
theoretically investigate the layers within the residual stream through a
bias-diversity decomposition. The decomposition separates (i) bias, the error
of each layer's output from the ground truth, and (ii) diversity, which
indicates how much the outputs of each layer differ from each other. Analyzing
Transformers under this theory reveals that performance improves when
individual layers make predictions close to the correct answer and remain
mutually diverse. We show that diversity becomes especially critical when
individual layers' outputs are far from the ground truth. Finally, we introduce
an information-theoretic diversity and show our main findings that adding
layers enhances performance only when those layers behave differently, i.e.,
are diverse. We also reveal the performance gains from increasing the number of
layers exhibit submodularity: marginal improvements diminish as additional
layers increase, mirroring the logarithmic convergence predicted by the
parameter scaling laws. Experiments on multiple semantic-understanding tasks
with various LLMs empirically confirm the theoretical properties derived in
this study.

</details>


### [71] [Large Language Model Meets Constraint Propagation](https://arxiv.org/pdf/2505.24012)
*Alexandre Bonlarron, Florian Régin, Elisabetta De Maria, Jean-Charles Régin*

Main category: cs.CL

TL;DR: GenCP enhances LLMs by integrating MLMs for bidirectional constraint propagation, improving constraint-aware text generation.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with enforcing external constraints due to sequential token generation. GenCP aims to address this by combining LLMs with CP reasoning.

Method: GenCP integrates MLMs for domain generation, enabling bidirectional constraint propagation and formulating text generation as a CSP.

Result: Evaluation on COLLIE benchmarks shows improved performance with MLM integration, despite additional MLM calls and backtracking.

Conclusion: GenCP with MLMs enhances constraint-aware text generation, making it more efficient and reliable for tasks with strict constraints.

Abstract: Large Language Models (LLMs) excel at generating fluent text but struggle to
enforce external constraints because they generate tokens sequentially without
explicit control mechanisms. GenCP addresses this limitation by combining LLM
predictions with Constraint Programming (CP) reasoning, formulating text
generation as a Constraint Satisfaction Problem (CSP). In this paper, we
improve GenCP by integrating Masked Language Models (MLMs) for domain
generation, which allows bidirectional constraint propagation that leverages
both past and future tokens. This integration bridges the gap between
token-level prediction and structured constraint enforcement, leading to more
reliable and constraint-aware text generation. Our evaluation on COLLIE
benchmarks demonstrates that incorporating domain preview via MLM calls
significantly improves GenCP's performance. Although this approach incurs
additional MLM calls and, in some cases, increased backtracking, the overall
effect is a more efficient use of LLM inferences and an enhanced ability to
generate feasible and meaningful solutions, particularly in tasks with strict
content constraints.

</details>


### [72] [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/pdf/2505.24016)
*Matthew Raffel, Victor Agostinelli, Lizhong Chen*

Main category: cs.CL

TL;DR: BeaverTalk is a cascaded system for speech-to-text translation, using VAD segmenter, Whisper Large V2 for ASR, and Gemma 3 12B for translation, fine-tuned with LoRAs. It achieved competitive BLEU scores in English→German and English→Chinese tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a robust system for simultaneous speech-to-text translation, addressing challenges in latency and accuracy for real-time applications.

Method: Combines VAD segmenter, Whisper Large V2 for ASR, and Gemma 3 12B for translation, fine-tuned with LoRAs for conversational context.

Result: Achieved BLEU scores of 24.64/27.83 (English→German) and 34.07/37.23 (English→Chinese) at varying latencies.

Conclusion: BeaverTalk demonstrates effectiveness in simultaneous translation, balancing latency and accuracy for real-time use.

Abstract: This paper discusses the construction, fine-tuning, and deployment of
BeaverTalk, a cascaded system for speech-to-text translation as part of the
IWSLT 2025 simultaneous translation task. The system architecture employs a VAD
segmenter for breaking a speech stream into segments, Whisper Large V2 for
automatic speech recognition (ASR), and Gemma 3 12B for simultaneous
translation. Regarding the simultaneous translation LLM, it is fine-tuned via
low-rank adaptors (LoRAs) for a conversational prompting strategy that
leverages a single prior-sentence memory bank from the source language as
context. The cascaded system participated in the English$\rightarrow$German and
English$\rightarrow$Chinese language directions for both the low and high
latency regimes. In particular, on the English$\rightarrow$German task, the
system achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and
3343.73, respectively. Then, on the English$\rightarrow$Chinese task, the
system achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and
3521.35, respectively.

</details>


### [73] [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/pdf/2505.24040)
*Yuexing Hao, Kumail Alhamoud, Hyewon Jeong, Haoran Zhang, Isha Puri, Philip Torr, Mike Schaekermann, Ariel D. Stern, Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: The study introduces MedPAIR to compare how physician trainees and LLMs prioritize relevant information in medical QA tasks, finding misalignment in relevance estimates and improved accuracy after filtering irrelevant content.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs prioritize relevant information correctly in medical QA tasks, as correct answers alone may not reflect logical accuracy.

Method: Created the MedPAIR dataset with 1,300 QA pairs annotated by 36 physician trainees for sentence relevance, comparing these to LLM relevance estimates and evaluating downstream performance.

Result: LLMs often misalign with physician trainees' relevance estimates, but filtering irrelevant sentences improves accuracy for both groups.

Conclusion: The study highlights the importance of relevance alignment in medical QA and provides a dataset for further research.

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance on
various medical question-answering (QA) benchmarks, including standardized
medical exams. However, correct answers alone do not ensure correct logic, and
models may reach accurate conclusions through flawed processes. In this study,
we introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance
Estimation and Question Answering) dataset to evaluate how physician trainees
and LLMs prioritize relevant information when answering QA questions. We obtain
annotations on 1,300 QA pairs from 36 physician trainees, labeling each
sentence within the question components for relevance. We compare these
relevance estimates to those for LLMs, and further evaluate the impact of these
"relevant" subsets on downstream task performance for both physician trainees
and LLMs. We find that LLMs are frequently not aligned with the content
relevance estimates of physician trainees. After filtering out physician
trainee-labeled irrelevant sentences, accuracy improves for both the trainees
and the LLMs. All LLM and physician trainee-labeled data are available at:
http://medpair.csail.mit.edu/.

</details>


### [74] [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/pdf/2505.24028)
*Kateryna Akhynko, Oleksandr Kosovan, Mykola Trokhymovych*

Main category: cs.CL

TL;DR: A top-performing solution for detecting manipulation in Ukrainian Telegram, using fine-tuned Gemma 2 with LoRA for classification and XLM-RoBERTa for span detection, achieving 2nd and 3rd places.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of detecting rhetorical and stylistic manipulation in social media, specifically targeting Ukrainian Telegram users.

Method: Fine-tuned Gemma 2 with LoRA adapters for classification and XLM-RoBERTa for span detection, with meta-features and threshold optimization.

Result: Achieved 2nd place in classification and 3rd place in span detection.

Conclusion: The approach is effective for detecting manipulation in social media, demonstrating strong performance in shared task benchmarks.

Abstract: This paper presents one of the top-performing solutions to the UNLP 2025
Shared Task on Detecting Manipulation in Social Media. The task focuses on
detecting and classifying rhetorical and stylistic manipulation techniques used
to influence Ukrainian Telegram users. For the classification subtask, we
fine-tuned the Gemma 2 language model with LoRA adapters and applied a
second-level classifier leveraging meta-features and threshold optimization.
For span detection, we employed an XLM-RoBERTa model trained for multi-target,
including token binary classification. Our approach achieved 2nd place in
classification and 3rd place in span detection.

</details>


### [75] [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/pdf/2505.24033)
*Yasaman Jafari, Zixian Wang, Leon Bergen, Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: Post-hoc merging of hidden states in SSMs (document souping) enables modular encoding and reuse, matching cross-encoder performance on tasks like multi-hop QA.


<details>
  <summary>Details</summary>
Motivation: To explore if hidden states from SSMs can be merged after encoding to support downstream reasoning without reprocessing inputs.

Method: Propose document souping: independently encode documents, then pool representations (e.g., averaging) into a single context state. Finetune Mamba2 models for soupable representations.

Result: Strong accuracy on multi-hop QA, sparse retrieval, and long-document reasoning. On HotpotQA, souping 10 documents nearly matches cross-encoder performance.

Conclusion: Document souping is effective for modular encoding and reuse, offering comparable performance to more complex methods.

Abstract: We investigate whether hidden states from Structured State Space Models
(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by
model souping, we propose a strategy where documents are encoded independently
and their representations are pooled -- via simple operations like averaging --
into a single context state. This approach, which we call document souping,
enables modular encoding and reuse without reprocessing the full input for each
query. We finetune Mamba2 models to produce soupable representations and find
that they support multi-hop QA, sparse retrieval, and long-document reasoning
with strong accuracy. On HotpotQA, souping ten independently encoded documents
nearly matches the performance of a cross-encoder trained on the same inputs.

</details>


### [76] [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/pdf/2505.24063)
*Jiacheng Xie, Yang Yu, Ziyang Zhang, Shuai Zeng, Jiaxuan He, Ayush Vasireddy, Xiaoting Tang, Congyu Guo, Lening Zhao, Congcong Jing, Guanghui An, Dong Xu*

Main category: cs.CL

TL;DR: The paper introduces TCM-Ladder, the first multimodal QA dataset for evaluating large TCM language models, addressing the lack of standardized benchmarks in TCM.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation datasets for TCM language models are limited and text-based, lacking a unified multimodal benchmark.

Method: The authors created TCM-Ladder, a dataset spanning TCM disciplines with 52,000+ multimodal questions, and proposed Ladder-Score for evaluation.

Result: Comparative experiments were conducted on 9 general and 5 TCM-specific LLMs, evaluating their performance on TCM-Ladder.

Conclusion: TCM-Ladder fills a gap in TCM evaluation, providing a standardized benchmark for multimodal QA tasks in TCM.

Abstract: Traditional Chinese Medicine (TCM), as an effective alternative medicine, has
been receiving increasing attention. In recent years, the rapid development of
large language models (LLMs) tailored for TCM has underscored the need for an
objective and comprehensive evaluation framework to assess their performance on
real-world tasks. However, existing evaluation datasets are limited in scope
and primarily text-based, lacking a unified and standardized multimodal
question-answering (QA) benchmark. To address this issue, we introduce
TCM-Ladder, the first multimodal QA dataset specifically designed for
evaluating large TCM language models. The dataset spans multiple core
disciplines of TCM, including fundamental theory, diagnostics, herbal formulas,
internal medicine, surgery, pharmacognosy, and pediatrics. In addition to
textual content, TCM-Ladder incorporates various modalities such as images and
videos. The datasets were constructed using a combination of automated and
manual filtering processes and comprise 52,000+ questions in total. These
questions include single-choice, multiple-choice, fill-in-the-blank, diagnostic
dialogue, and visual comprehension tasks. We trained a reasoning model on
TCM-Ladder and conducted comparative experiments against 9 state-of-the-art
general domain and 5 leading TCM-specific LLMs to evaluate their performance on
the datasets. Moreover, we propose Ladder-Score, an evaluation method
specifically designed for TCM question answering that effectively assesses
answer quality regarding terminology usage and semantic expression. To our
knowledge, this is the first work to evaluate mainstream general domain and
TCM-specific LLMs on a unified multimodal benchmark. The datasets and
leaderboard are publicly available at https://tcmladder.com or
https://54.211.107.106 and will be continuously updated.

</details>


### [77] [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/pdf/2505.24133)
*Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu*

Main category: cs.CL

TL;DR: R-KV is a novel KV cache compression method for reasoning models, reducing memory usage by 90% and improving throughput by 6.6X while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods fail with reasoning models, causing performance drops and inefficiencies.

Method: R-KV targets redundant tokens in reasoning models, compressing the KV cache while preserving performance.

Result: R-KV achieves 100% performance with 10% KV cache and even 105% with 16%, outperforming baselines (60%).

Conclusion: R-KV is highly effective for reasoning models, offering significant efficiency gains without compromising performance.

Abstract: Reasoning models have demonstrated impressive performance in self-reflection
and chain-of-thought reasoning. However, they often produce excessively long
outputs, leading to prohibitively large key-value (KV) caches during inference.
While chain-of-thought inference significantly improves performance on complex
reasoning tasks, it can also lead to reasoning failures when deployed with
existing KV cache compression approaches. To address this, we propose
Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel
method specifically targeting redundant tokens in reasoning models. Our method
preserves nearly 100% of the full KV cache performance using only 10% of the KV
cache, substantially outperforming existing KV cache baselines, which reach
only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV
cache performance with 16% of the KV cache. This KV-cache reduction also leads
to a 90% memory saving and a 6.6X throughput over standard chain-of-thought
reasoning inference. Experimental results show that R-KV consistently
outperforms existing KV cache compression baselines across two mathematical
reasoning datasets.

</details>


### [78] [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/pdf/2505.24098)
*Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li*

Main category: cs.CL

TL;DR: HARDTESTGEN is a pipeline for synthesizing high-quality tests for LLM-generated code, improving precision and recall in evaluation and enhancing downstream code generation performance.


<details>
  <summary>Details</summary>
Motivation: Reliable verifiers for LLM reasoning are hard to create for difficult coding problems due to the challenge of synthesizing edge cases.

Method: Proposes HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs, and curates the HARDTESTS dataset with 47k problems.

Result: HARDTESTGEN tests show 11.3% higher precision and 17.5% higher recall, with up to 40% improvement for harder problems. HARDTESTS also improves model training.

Conclusion: The HARDTESTGEN pipeline and HARDTESTS dataset effectively address the challenge of test synthesis for LLM-generated code, with significant performance improvements.

Abstract: Verifiers play a crucial role in large language model (LLM) reasoning, needed
by post-training techniques such as reinforcement learning. However, reliable
verifiers are hard to get for difficult coding problems, because a
well-disguised wrong solution may only be detected by carefully human-written
edge cases that are difficult to synthesize. To address this issue, we propose
HARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this
pipeline, we curate a comprehensive competitive programming dataset HARDTESTS
with 47k problems and synthetic high-quality tests. Compared with existing
tests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points
higher and recall that is 17.5 percentage points higher when evaluating
LLM-generated code. For harder problems, the improvement in precision can be as
large as 40 points. HARDTESTS also proves to be more effective for model
training, measured by downstream code generation performance. We will
open-source our dataset and synthesis pipeline at
https://leililab.github.io/HardTests/.

</details>


### [79] [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/pdf/2505.24163)
*Jiaqi Sun, Shiyou Qian, Zhangchi Han, Wei Li, Zelin Qian, Dingyu Yang, Jian Cao, Guangtao Xue*

Main category: cs.CL

TL;DR: LKD-KGC is a novel unsupervised framework for domain-specific KG construction, outperforming existing methods by 10-20% in precision and recall.


<details>
  <summary>Details</summary>
Motivation: Manual KG construction is inefficient, and existing LLM-based methods are limited by predefined schemas and single-document processing, making them ineffective for domain-specific corpora.

Method: LKD-KGC autonomously analyzes document repositories, infers knowledge dependencies, prioritizes processing sequences via LLMs, and autoregressively generates entity schema.

Result: LKD-KGC improves precision and recall by 10-20% over state-of-the-art baselines.

Conclusion: LKD-KGC effectively constructs high-quality domain-specific KGs without predefined structures or external knowledge.

Abstract: Knowledge Graphs (KGs) structure real-world entities and their relationships
into triples, enhancing machine reasoning for various tasks. While
domain-specific KGs offer substantial benefits, their manual construction is
often inefficient and requires specialized knowledge. Recent approaches for
knowledge graph construction (KGC) based on large language models (LLMs), such
as schema-guided KGC and reference knowledge integration, have proven
efficient. However, these methods are constrained by their reliance on manually
defined schema, single-document processing, and public-domain references,
making them less effective for domain-specific corpora that exhibit complex
knowledge dependencies and specificity, as well as limited reference knowledge.
To address these challenges, we propose LKD-KGC, a novel framework for
unsupervised domain-specific KG construction. LKD-KGC autonomously analyzes
document repositories to infer knowledge dependencies, determines optimal
processing sequences via LLM driven prioritization, and autoregressively
generates entity schema by integrating hierarchical inter-document contexts.
This schema guides the unsupervised extraction of entities and relationships,
eliminating reliance on predefined structures or external knowledge. Extensive
experiments show that compared with state-of-the-art baselines, LKD-KGC
generally achieves improvements of 10% to 20% in both precision and recall
rate, demonstrating its potential in constructing high-quality domain-specific
KGs.

</details>


### [80] [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/pdf/2505.24105)
*Jiacheng Lin, Zhenbang Wu, Jimeng Sun*

Main category: cs.CL

TL;DR: EHRMIND adapts LLMs for clinical tasks using RLVR, addressing misapplied and missing knowledge via SFT and RLVR, improving accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Applying RLVR to healthcare is challenging due to specialized EHR knowledge. EHRMIND aims to address misapplied and missing knowledge in LLMs.

Method: Two-stage approach: SFT for domain knowledge injection and RLVR for refining decision-making.

Result: Improved accuracy, interpretability, and cross-task generalization in clinical applications.

Conclusion: EHRMIND provides practical guidance for using RLVR to enhance LLMs in healthcare.

Abstract: We present EHRMIND, a practical recipe for adapting large language models
(LLMs) to complex clinical reasoning tasks using reinforcement learning with
verifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,
its application to healthcare contexts presents unique challenges due to the
specialized knowledge and reasoning required for electronic health record (EHR)
interpretation. Our pilot study on the MEDCALC benchmark reveals two key
failure modes: (1) misapplied knowledge, where models possess relevant medical
knowledge but apply it incorrectly, and (2) missing knowledge, where models
lack essential domain knowledge. To address these cases, EHRMIND applies a
two-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that
injects missing domain knowledge, stabilizes subsequent training, and
encourages structured, interpretable outputs; followed by RLVR, which
reinforces outcome correctness and refines the model's decision-making. We
demonstrate the effectiveness of our method across diverse clinical
applications, including medical calculations (MEDCALC), patient-trial matching
(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers
consistent gains in accuracy, interpretability, and cross-task generalization.
These findings offer practical guidance for applying RLVR to enhance LLM
capabilities in healthcare settings.

</details>


### [81] [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/pdf/2505.24119)
*Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer*

Main category: cs.CL

TL;DR: The paper analyzes the English-centric bias in LLM safety research, revealing minimal attention to non-English languages and poor documentation practices. It proposes future directions for more inclusive AI safety.


<details>
  <summary>Details</summary>
Motivation: To highlight the lack of linguistic diversity in LLM safety research and advocate for more inclusive practices.

Method: Systematic review of 300 publications (2020-2024) from major NLP conferences (e.g., *ACL).

Result: Non-English languages are underrepresented, even high-resource ones, and English research lacks proper language documentation.

Conclusion: Proposes recommendations and future directions (evaluation, data generation, crosslingual generalization) for more robust, multilingual AI safety.

Abstract: This paper presents a comprehensive analysis of the linguistic diversity of
LLM safety research, highlighting the English-centric nature of the field.
Through a systematic review of nearly 300 publications from 2020--2024 across
major NLP conferences and workshops at *ACL, we identify a significant and
growing language gap in LLM safety research, with even high-resource
non-English languages receiving minimal attention. We further observe that
non-English languages are rarely studied as a standalone language and that
English safety research exhibits poor language documentation practice. To
motivate future research into multilingual safety, we make several
recommendations based on our survey, and we then pose three concrete future
directions on safety evaluation, training data generation, and crosslingual
safety generalization. Based on our survey and proposed directions, the field
can develop more robust, inclusive AI safety practices for diverse global
populations.

</details>


### [82] [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/pdf/2505.24143)
*Jinglong Gao, Xiao Ding, Lingxiao Zou, Bing Qin, Ting Liu*

Main category: cs.CL

TL;DR: CrossICL introduces a new ICL paradigm to leverage source task demonstrations for target tasks, eliminating manual effort, and shows effectiveness through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Manual demonstrations for ICL are often impractical; CrossICL aims to utilize existing source task demonstrations for target tasks without additional effort.

Method: A two-stage alignment strategy mitigates task gaps, followed by experiments with 875 NLP tasks and six LLMs, including GPT-4.

Result: CrossICL proves effective, offering insights on selecting cross-task demonstrations and understanding task-gap interference.

Conclusion: CrossICL provides a practical solution for ICL by reusing demonstrations, reducing reliance on manual input.

Abstract: In-Context Learning (ICL) enhances the performance of large language models
(LLMs) with demonstrations. However, obtaining these demonstrations primarily
relies on manual effort. In most real-world scenarios, users are often
unwilling or unable to provide such demonstrations. Inspired by the human
analogy, we explore a new ICL paradigm CrossICL to study how to utilize
existing source task demonstrations in the ICL for target tasks, thereby
obtaining reliable guidance without any additional manual effort. To explore
this, we first design a two-stage alignment strategy to mitigate the
interference caused by gaps across tasks, as the foundation for our
experimental exploration. Based on it, we conduct comprehensive exploration of
CrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,
including GPT-4o. Experimental results demonstrate the effectiveness of
CrossICL and provide valuable insights on questions like the criteria for
selecting cross-task demonstrations, as well as the types of task-gap-induced
interference in CrossICL.

</details>


### [83] [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/pdf/2505.24255)
*Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim*

Main category: cs.CL

TL;DR: LLMs with ToM reasoning improve alignment with human norms in negotiation tasks, outperforming reasoning models without ToM.


<details>
  <summary>Details</summary>
Motivation: To explore how ToM reasoning in LLMs aligns agentic behaviors with human norms, using the ultimatum game as a testbed.

Method: Initialized LLM agents with prosocial beliefs (Greedy, Fair, Selfless) and reasoning methods (CoT, ToM levels), tested across models like o3-mini and DeepSeek-R1.

Result: ToM reasoning improved behavior alignment, decision consistency, and outcomes; reasoning models without ToM were less effective.

Conclusion: ToM reasoning enhances human-AI interaction and cooperative decision-making, with implications for AI alignment.

Abstract: Large Language Models (LLMs) have shown potential in simulating human
behaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for
complex social interactions. In this study, we investigate the role of ToM
reasoning in aligning agentic behaviors with human norms in negotiation tasks,
using the ultimatum game as a controlled environment. We initialized LLM agents
with different prosocial beliefs (including Greedy, Fair, and Selfless) and
reasoning methods like chain-of-thought (CoT) and varying ToM levels, and
examined their decision-making processes across diverse LLMs, including
reasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from
2,700 simulations indicated that ToM reasoning enhances behavior alignment,
decision-making consistency, and negotiation outcomes. Consistent with previous
findings, reasoning models exhibit limited capability compared to models with
ToM reasoning, different roles of the game benefits with different orders of
ToM reasoning. Our findings contribute to the understanding of ToM's role in
enhancing human-AI interaction and cooperative decision-making. The code used
for our experiments can be found at https://github.com/Stealth-py/UltimatumToM.

</details>


### [84] [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/pdf/2505.24147)
*Chiwei Zhu, Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Zhendong Mao*

Main category: cs.CL

TL;DR: Rationales in training language models don't always improve performance; they can sometimes harm it or boost reliability, depending on task difficulty.


<details>
  <summary>Details</summary>
Motivation: Challenge the prevailing belief that rationales always enhance model performance and explore their impact on reliability.

Method: Comprehensive investigations into the effects of rationales on model performance and reliability, analyzing task difficulty.

Result: 1) Rationales can degrade performance; 2) They may improve reliability; 3) Performance and reliability improvements are linearly related and task-dependent.

Conclusion: Findings guide rationale use in training and highlight implications for aligning models with human reasoning.

Abstract: Training language models with rationales augmentation has been shown to be
beneficial in many existing works. In this paper, we identify that such a
prevailing view does not hold consistently. We conduct comprehensive
investigations to thoroughly inspect the impact of rationales on model
performance as well as a novel perspective of model reliability. The results
lead to several key findings that add new insights upon existing
understandings: 1) Rationales can, at times, deteriorate model performance; 2)
Rationales can, at times, improve model reliability, even outperforming their
untrained counterparts; 3) A linear correspondence exists in between the
performance and reliability improvements, while both are driven by the
intrinsic difficulty of the task. These findings provide informative
regulations on the broad utilization of rationales and raise critical
implications on the procedure of explicitly aligning language models with
implicit human thoughts. Codes can be found at
https://github.com/Ignoramus0817/rationales.

</details>


### [85] [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/pdf/2505.24264)
*Xin Quan, Marco Valentino, Louise A. Dennis, André Freitas*

Main category: cs.CL

TL;DR: The paper explores strategies to improve faithfulness and robustness in NLI explanations by combining LLMs with theorem provers, addressing semantic loss, syntactic errors, and proof construction.


<details>
  <summary>Details</summary>
Motivation: To mitigate issues of semantic loss and unfaithful interpretation in translating natural language to formal representations, and to enhance LLMs' proof construction and feedback interpretation.

Method: Proposes strategies to reduce semantic loss, correct syntactic errors, guide proof sketches with logical expressions, and improve LLM feedback interpretation.

Result: Significant improvements in autoformalisation (+18.46% to +39.77%) and explanation refinement (+29.5% to +51.5%) across datasets.

Conclusion: Hybrid LLM-TP interventions enhance efficiency and reduce verification iterations, demonstrating the effectiveness of the proposed strategies.

Abstract: Natural language explanations play a fundamental role in Natural Language
Inference (NLI) by revealing how premises logically entail hypotheses. Recent
work has shown that the interaction of large language models (LLMs) with
theorem provers (TPs) can help verify and improve the validity of NLI
explanations. However, TPs require translating natural language into
machine-verifiable formal representations, a process that introduces the risk
of semantic information loss and unfaithful interpretation, an issue compounded
by LLMs' challenges in capturing critical logical structures with sufficient
precision. Moreover, LLMs are still limited in their capacity for rigorous and
robust proof construction within formal verification frameworks. To mitigate
issues related to faithfulness and robustness, this paper investigates
strategies to (1) alleviate semantic loss during autoformalisation, (2)
efficiently identify and correct syntactic errors in logical representations,
(3) explicitly use logical expressions to guide LLMs in generating structured
proof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback
for iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree
using different LLMs demonstrate that the proposed strategies yield significant
improvements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation
refinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,
we show that specific interventions on the hybrid LLM-TP architecture can
substantially improve efficiency, drastically reducing the number of iterations
required for successful verification.

</details>


### [86] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/pdf/2505.24164)
*Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei*

Main category: cs.CL

TL;DR: The paper introduces Mixed-R1, a unified framework for stable reinforcement learning in multi-source MLLM tasks, combining mixed rewards and a post-training dataset (Mixed-45K).


<details>
  <summary>Details</summary>
Motivation: Current methods focus on single aspects of MLLM tasks, lacking a unified approach for multi-source reinforcement learning.

Method: Proposes Mixed-R1 with a mixed reward function (including matching, chart, IoU, and open-ended rewards) and a curated Mixed-45K dataset. Introduces BMAS for open-ended rewards.

Result: Demonstrates effectiveness on various MLLMs (e.g., Qwen2.5-VL, Intern-VL) across sizes.

Conclusion: Mixed-R1 provides a robust solution for multi-task MLLM reinforcement learning, validated by experiments.

Abstract: Recent works on large language models (LLMs) have successfully demonstrated
the emergence of reasoning capabilities via reinforcement learning (RL).
Although recent efforts leverage group relative policy optimization (GRPO) for
MLLMs post-training, they constantly explore one specific aspect, such as
grounding tasks, math problems, or chart analysis. There are no works that can
leverage multi-source MLLM tasks for stable reinforcement learning. In this
work, we present a unified perspective to solve this problem. We present
Mixed-R1, a unified yet straightforward framework that contains a mixed reward
function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).
We first design a data engine to select high-quality examples to build the
Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which
contains various reward functions for various MLLM tasks. In particular, it has
four different reward functions: matching reward for binary answer or
multiple-choice problems, chart reward for chart-aware datasets, IoU reward for
grounding problems, and open-ended reward for long-form text responses such as
caption datasets. To handle the various long-form text content, we propose a
new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by
leveraging tokenizer embedding matching between the generated response and the
ground truth. Extensive experiments show the effectiveness of our proposed
method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.
Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [87] [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/pdf/2505.24165)
*Yixuan Wang, Shiqi Zhou, Chuanzhe Guo, Qingfu Zhu*

Main category: cs.CL

TL;DR: Tag-Evol improves instruction evolution by using diverse knowledge tags for controlled and efficient data synthesis, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on fixed, manually designed strategies and face high costs in iterative evolution, limiting diversity and efficiency.

Method: Tag-Evol employs diverse knowledge tags to inject into original instructions, enabling controlled and varied evolution.

Result: Experiments show Tag-Evol generates better, more diverse, and challenging data than other methods.

Conclusion: Tag-Evol is efficient, diverse, and effective for instruction evolution, addressing limitations of existing approaches.

Abstract: Evol-Instruct has made significant improvements as a data synthesis method in
several areas. Existing methods typically rely on a fixed set of strategies to
evolve, which require manual design and are monolithic in form. In addition,
iterative evolution also makes the acquisition of hard samples expensive. In
view of this, we propose the Tag-Evol framework, a more diverse and efficient
instruction evolving method. Specifically, Tag-Evol uses diverse and specific
knowledge tags as strategies to achieve controlled evolution by injecting
different combinations of tags into the original instructions. Experiments with
multiple backbones in diverse domain benchmarks show that the proposed method
generates significantly better evolved data than other methods. Furthermore, we
conduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is
not only efficient but also generates more diverse and challenging data.

</details>


### [88] [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/pdf/2505.24174)
*Ryota Miyano, Yuki Arase*

Main category: cs.CL

TL;DR: A LoRA merge method is proposed for adapting LLMs to low-resource language tasks, improving adaptability by updating and pruning LoRA parameters with minimal data.


<details>
  <summary>Details</summary>
Motivation: Previous LoRA merge methods lack adaptability due to frozen parameters and ignore low-resource challenges.

Method: The proposed method fine-tunes and prunes LoRA parameters for finer-grained adjustments using minimal target task data.

Result: Experiments show significant improvements in task adaptability across domains and languages (English, Japanese).

Conclusion: The method enhances LLM adaptation for low-resource tasks, outperforming prior approaches.

Abstract: This study proposes a simple yet effective LoRA merge method to achieve LLM
adaptation for low-resource language generation tasks. The LoRA merge
technique, which integrates multiple LoRA modules trained on different tasks,
has gained attention as an effective and efficient approach for adapting LLMs
to target tasks. However, previous methods are limited in adaptability as they
keep the LoRA parameters frozen. Additionally, the low-resource problem has
been out of their scope. We propose a LoRA merge method that updates and prunes
LoRA parameters through fine-tuning with minimal target task data, which allows
finer-grained adjustments of LoRA parameters and enhancement of task
adaptability. Extensive experiments have been conducted taking summarization as
a benchmark task. Our datasets cover various domains and multiple languages of
English and Japanese. The results confirm that the proposed method achieves
significant and consistent improvements in task adaptability over the previous
methods.

</details>


### [89] [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/pdf/2505.24341)
*Shujian Yang, Shiyao Cui, Chuanrui Hu, Haicheng Wang, Tianwei Zhang, Minlie Huang, Jialiang Lu, Han Qiu*

Main category: cs.CL

TL;DR: The paper addresses the challenge of detecting toxic Chinese content using LLMs, highlighting issues with character substitutions and multimodal language. It proposes perturbation strategies, benchmarks 9 SOTA LLMs, and explores enhancement solutions like ICL and SFT, revealing limitations and risks of overcorrection.


<details>
  <summary>Details</summary>
Motivation: Detecting toxic content in Chinese is difficult due to character substitutions and the multimodal nature of the language, which confuses SOTA LLMs.

Method: Proposes a taxonomy of perturbation strategies, curates a dataset, benchmarks 9 LLMs, and tests enhancement methods like ICL and SFT.

Result: LLMs struggle with perturbed multimodal toxic content, and ICL/SFT may cause overcorrection, mislabeling normal content as toxic.

Conclusion: The study underscores the limitations of LLMs in detecting perturbed toxic Chinese content and warns against unintended consequences of enhancement methods.

Abstract: Detecting toxic content using language models is important but challenging.
While large language models (LLMs) have demonstrated strong performance in
understanding Chinese, recent studies show that simple character substitutions
in toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In
this paper, we highlight the multimodal nature of Chinese language as a key
challenge for deploying LLMs in toxic Chinese detection. First, we propose a
taxonomy of 3 perturbation strategies and 8 specific approaches in toxic
Chinese content. Then, we curate a dataset based on this taxonomy, and
benchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect
perturbed toxic Chinese text. Additionally, we explore cost-effective
enhancement solutions like in-context learning (ICL) and supervised fine-tuning
(SFT). Our results reveal two important findings. (1) LLMs are less capable of
detecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a
small number of perturbed examples may cause the LLMs "overcorrect'':
misidentify many normal Chinese contents as toxic.

</details>


### [90] [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/pdf/2505.24187)
*Mikhail L. Arbuzov, Alexey A. Shvets, Sisong Beir*

Main category: cs.CL

TL;DR: LLM reliability doesn't decay exponentially with sequence length; errors are concentrated at key tokens (5-10%). A new framework focuses on these tokens for better performance.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption of uniform error distribution in LLMs and improve long-context reliability.

Method: Identify and prioritize high-impact "key tokens," propose targeted strategies (e.g., dynamic computation, multi-path exploration).

Result: Sustained coherence over long sequences, outperforming brute-force methods.

Conclusion: Shift from raw scaling to strategic reasoning for more efficient and powerful LLMs.

Abstract: The prevailing assumption of an exponential decay in large language model
(LLM) reliability with sequence length, predicated on independent per-token
error probabilities, posits an inherent limitation for long autoregressive
outputs. Our research fundamentally challenges this view by synthesizing
emerging evidence that LLM errors are not uniformly distributed but are
concentrated at sparse "key tokens" ($5-10\%$ of total tokens) representing
critical decision junctions. By distinguishing these high-impact tokens from
the increasingly predictable majority, we introduce a new reliability formula
explaining the sustained coherence of modern LLMs over thousands of tokens.
Converging research streams reveal that long-context performance primarily
depends on accurately navigating a few crucial semantic decision points rather
than on uniform token-level accuracy, enabling targeted strategies that
significantly outperform brute-force approaches. We thus propose a framework
for next-generation systems centered on selective preservation of semantically
vital tokens, dynamic computational allocation at uncertain decision
boundaries, multi-path exploration at ambiguities, and architectures aligned
with natural semantic domains. This marks a fundamental shift from raw scaling
to strategic reasoning, promising breakthrough performance without
proportionate computational scaling and offering a more nuanced understanding
that supersedes the exponential decay hypothesis, thereby opening pathways
toward substantially more powerful and efficient language systems.

</details>


### [91] [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/pdf/2505.24196)
*Longze Chen, Renke Shan, Huiming Wang, Lu Wang, Ziqiang Liu, Run Luo, Jiawei Wang, Hamid Alinejad-Rokny, Min Yang*

Main category: cs.CL

TL;DR: CLaSp introduces a plug-and-play layer-skipping strategy for self-speculative decoding in LLMs, eliminating the need for additional training or drafting modules, achieving 1.3x~1.7x speedup.


<details>
  <summary>Details</summary>
Motivation: Existing speculative decoding methods require extra training and modules, complicating compatibility across LLMs. CLaSp addresses this by leveraging the verify model's layers.

Method: CLaSp skips intermediate layers of the verify model to create a draft model, using a dynamic programming algorithm to optimize layer-skipping based on hidden states.

Result: CLaSp achieves a 1.3x~1.7x speedup on LLaMA3 models without altering text generation quality.

Conclusion: CLaSp offers a practical, efficient alternative to traditional speculative decoding, enhancing LLM performance without extra training.

Abstract: Speculative decoding (SD) is a promising method for accelerating the decoding
process of Large Language Models (LLMs). The efficiency of SD primarily hinges
on the consistency between the draft model and the verify model. However,
existing drafting approaches typically require additional modules to be
trained, which can be challenging to implement and ensure compatibility across
various LLMs. In this paper, we propose CLaSp, an in-context layer-skipping
strategy for self-speculative decoding. Unlike prior methods, CLaSp does not
require additional drafting modules or extra training. Instead, it employs a
plug-and-play mechanism by skipping intermediate layers of the verify model to
construct a compressed draft model. Specifically, we develop a dynamic
programming algorithm that optimizes the layer-skipping process by leveraging
the complete hidden states from the last verification stage as an objective.
This enables CLaSp to dynamically adjust its layer-skipping strategy after each
verification stage, without relying on pre-optimized sets of skipped layers.
Experimental results across diverse downstream tasks demonstrate that CLaSp
achieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the
original distribution of the generated text.

</details>


### [92] [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/pdf/2505.24199)
*Yimin Du*

Main category: cs.CL

TL;DR: A novel framework using intuitionistic fuzzy sets (IFS) improves human preference data quality for LLMs, addressing uncertainty and disagreement in annotations, leading to better model performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional annotation methods (e.g., SBS) struggle with uncertainty and disagreement in human preference judgments, necessitating a more nuanced approach.

Method: Proposes an IFS-based protocol for preference modeling, incorporating membership, non-membership, and hesitation degrees, along with aggregation methods and quality metrics.

Result: IFS-based approach improves annotation consistency, reduces fatigue, and enhances data quality, yielding a 12.3% win-rate boost and 15.7% time reduction.

Conclusion: The IFS framework effectively handles uncertainty in preference annotation, benefiting large-scale LLM training with practical improvements.

Abstract: The quality of human preference data is crucial for training and evaluating
large language models (LLMs), particularly in reinforcement learning from human
feedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional
side-by-side (SBS) annotation approaches often struggle with inherent
uncertainty, annotator disagreement, and the complexity of preference
judgments. This paper introduces a novel framework based on intuitionistic
fuzzy sets (IFS) for modeling and aggregating human preferences in LLM data
annotation tasks. Our approach captures not only the degree of preference but
also the uncertainty and hesitation inherent in human judgment through
membership, non-membership, and hesitation degrees. We propose an IFS-based
annotation protocol that enables more nuanced preference modeling, develops
aggregation methods for handling annotator disagreement, and introduces quality
metrics for preference data assessment. Experimental validation on multiple
datasets demonstrates that our IFS-based approach significantly improves
annotation consistency, reduces annotator fatigue, and produces higher-quality
preference data compared to traditional binary and Likert-scale methods. The
resulting preference datasets lead to improved model performance in downstream
tasks, with 12.3\% improvement in win-rate against baseline models and 15.7\%
reduction in annotation time. Our framework provides a principled approach to
handling uncertainty in human preference annotation and offers practical
benefits for large-scale LLM training.

</details>


### [93] [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/pdf/2505.24211)
*Jiwan Chung, Janghan Yoon, Junhyeong Park, Sangeyl Lee, Joowon Yang, Sooyeon Park, Youngjae Yu*

Main category: cs.CL

TL;DR: The paper investigates whether any-to-any generative models achieve true cross-modal coherence. Using the ACON dataset and three consistency criteria, it finds such models do not outperform specialized ones in pointwise evaluations but show weak consistency in structured analyses.


<details>
  <summary>Details</summary>
Motivation: To determine if unified any-to-any generative models genuinely preserve cross-modal relationships or merely create an illusion of coherence.

Method: Introduces the ACON dataset (1,000 images with captions, editing instructions, and Q&A pairs) and evaluates models using cyclic consistency, forward equivariance, and conjugated equivariance.

Result: Any-to-any models do not consistently outperform specialized models in pointwise evaluations but exhibit weak consistency in structured latent space analyses.

Conclusion: Unified models lack strong cross-modal consistency in pointwise tasks but show potential in structured evaluations, suggesting room for improvement.

Abstract: Any-to-any generative models aim to enable seamless interpretation and
generation across multiple modalities within a unified framework, yet their
ability to preserve relationships across modalities remains uncertain. Do
unified models truly achieve cross-modal coherence, or is this coherence merely
perceived? To explore this, we introduce ACON, a dataset of 1,000 images (500
newly contributed) paired with captions, editing instructions, and Q&A pairs to
evaluate cross-modal transfers rigorously. Using three consistency
criteria-cyclic consistency, forward equivariance, and conjugated
equivariance-our experiments reveal that any-to-any models do not consistently
demonstrate greater cross-modal consistency than specialized models in
pointwise evaluations such as cyclic consistency. However, equivariance
evaluations uncover weak but observable consistency through structured analyses
of the intermediate latent space enabled by multiple editing operations. We
release our code and data at https://github.com/JiwanChung/ACON.

</details>


### [94] [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/pdf/2505.24217)
*Jixuan Leng, Cassandra A. Cohen, Zhixian Zhang, Chenyan Xiong, William W. Cohen*

Main category: cs.CL

TL;DR: SSRMs introduce semi-structured reasoning traces in Pythonic syntax to improve faithfulness and detectability of errors in LLM reasoning, outperforming baselines and enabling automated audits.


<details>
  <summary>Details</summary>
Motivation: Address the issue of faithfulness in LLM reasoning traces, which often contain undetectable errors and biases.

Method: Develop Semi-Structured Reasoning Models (SSRMs) with a Pythonic syntax for reasoning traces, using restricted vocabulary and marking inputs/outputs.

Result: SSRMs outperform baselines by ~10% on in-domain tasks and remain competitive on out-of-domain benchmarks, with improved auditability.

Conclusion: SSRMs enhance reasoning faithfulness and enable effective error detection through structured and learned audits.

Abstract: As Large Language Models (LLMs) become increasingly capable at reasoning, the
problem of "faithfulness" persists: LLM "reasoning traces" can contain errors
and omissions that are difficult to detect, and may obscure biases in model
outputs. To address these limitations, we introduce Semi-Structured Reasoning
Models (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)
reasoning format within the model. Our SSRMs generate reasoning traces in a
Pythonic syntax. While SSRM traces are not executable, they adopt a restricted,
task-specific vocabulary to name distinct reasoning steps, and to mark each
step's inputs and outputs. Through extensive evaluation on ten benchmarks,
SSRMs demonstrate strong performance and generality: they outperform comparably
sized baselines by nearly ten percentage points on in-domain tasks while
remaining competitive with specialized models on out-of-domain medical
benchmarks. Furthermore, we show that semi-structured reasoning is more
amenable to analysis: in particular, they can be automatically audited to
identify reasoning flaws. We explore both hand-crafted structured audits, which
detect task-specific problematic reasoning patterns, and learned typicality
audits, which apply probabilistic models over reasoning patterns, and show that
both audits can be used to effectively flag probable reasoning errors.

</details>


### [95] [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/pdf/2505.24409)
*Eojin Kang, Juae Kim*

Main category: cs.CL

TL;DR: The paper explores cross-lingual knowledge transfer in multilingual LLMs, proposing the L2T prompting strategy to improve factual knowledge recall by aligning internal cognitive processes with task requirements.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent factual knowledge recall in multilingual LLMs by investigating language-thought binding and improving cross-lingual transfer without relying on English-based methods.

Method: Proposes the Language-to-Thought (L2T) prompting strategy to analyze input language, internal cognition, and knowledge alignment. Evaluates via experiments challenging English-based superiority.

Result: L2T improves cross-lingual knowledge transfer, showing alignment of internal thought with task needs is key. Reduces input language reliance without translation-based learning.

Conclusion: Aligning LLMs' internal thought with task knowledge is crucial for effective cross-lingual transfer. L2T offers a promising approach for multilingual LLM improvement.

Abstract: Multilingual large language models (LLMs) open up new possibilities for
leveraging information across languages, but their factual knowledge recall
remains inconsistent depending on the input language. While previous studies
have attempted to address this issue through English-based prompting and
evaluation, we explore non-English to English transfer via Language and Thought
Theory. This perspective allows us to examine language-thought binding in LLMs
and uncover why factual knowledge often fails to transfer effectively. We
propose the Language-to-Thought (L2T) prompting strategy, which analyzes the
relationship between input language, internal cognitive processes, and
knowledge. Experimental results challenge the assumption that English-based
approaches consistently outperform other languages and offer a novel insight
that aligning the model's internal thought with the knowledge required for the
task is critical for successful cross-lingual transfer. Furthermore, we show
that applying L2T during training can alleviate LLMs' reliance on the input
language and facilitate cross-linguistic knowledge integration without
translation-based learning. Code and datasets will be available.

</details>


### [96] [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/pdf/2505.24219)
*Lam Thanh Do, Aaditya Bodke, Pritom Saha Akash, Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: ERU-KG is an unsupervised keyphrase generation model with informativeness and phraseness modules, outperforming baselines and achieving 89% of supervised model performance for top 10 predictions. It is efficient and adaptable for extraction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised keyphrase prediction methods rely on heuristic scores, leading to inaccurate informativeness estimation and lacking time efficiency. ERU-KG addresses these issues.

Method: ERU-KG uses an informativeness module (learning from references and term-level modeling) and a phraseness module (generating candidates). It aggregates term informativeness for efficiency.

Result: Outperforms unsupervised baselines, achieves 89% of supervised model performance for top 10 predictions, and is the fastest among similar-sized models. Effective in text retrieval tasks.

Conclusion: ERU-KG is a practical, efficient, and adaptable solution for unsupervised keyphrase generation, with strong performance and utility in real-world applications.

Abstract: Unsupervised keyphrase prediction has gained growing interest in recent
years. However, existing methods typically rely on heuristically defined
importance scores, which may lead to inaccurate informativeness estimation. In
addition, they lack consideration for time efficiency. To solve these problems,
we propose ERU-KG, an unsupervised keyphrase generation (UKG) model that
consists of an informativeness and a phraseness module. The former estimates
the relevance of keyphrase candidates, while the latter generate those
candidates. The informativeness module innovates by learning to model
informativeness through references (e.g., queries, citation contexts, and
titles) and at the term-level, thereby 1) capturing how the key concepts of
documents are perceived in different contexts and 2) estimating informativeness
of phrases more efficiently by aggregating term informativeness, removing the
need for explicit modeling of the candidates. ERU-KG demonstrates its
effectiveness on keyphrase generation benchmarks by outperforming unsupervised
baselines and achieving on average 89\% of the performance of a supervised
model for top 10 predictions. Additionally, to highlight its practical utility,
we evaluate the model on text retrieval tasks and show that keyphrases
generated by ERU-KG are effective when employed as query and document
expansions. Furthermore, inference speed tests reveal that ERU-KG is the
fastest among baselines of similar model sizes. Finally, our proposed model can
switch between keyphrase generation and extraction by adjusting
hyperparameters, catering to diverse application requirements.

</details>


### [97] [Automated Structured Radiology Report Generation](https://arxiv.org/pdf/2505.24223)
*Jean-Benoit Delbrouck, Justin Xu, Johannes Moll, Alois Thomas, Zhihong Chen, Sophie Ostmeier, Asfandyar Azhar, Kelvin Zhenghao Li, Andrew Johnston, Christian Bluethgen, Eduardo Reis, Mohamed Muneer, Maya Varma, Curtis Langlotz*

Main category: cs.CL

TL;DR: The paper introduces Structured Radiology Report Generation (SRRG) to standardize free-text radiology reports, improving consistency and clinical relevance. It uses LLMs to restructure reports and proposes SRR-BERT for disease classification and F1-SRR-BERT for evaluation.


<details>
  <summary>Details</summary>
Motivation: Free-form radiology reports are inconsistent and unstructured, making automated generation and evaluation challenging. Standardization is needed for clinical utility.

Method: Restructured reports using LLMs, developed SRR-BERT for disease classification (55 labels), and introduced F1-SRR-BERT for evaluation. Validated with radiologists and experiments.

Result: Created a standardized dataset and evaluation framework, improving report consistency and clinical relevance.

Conclusion: SRRG and SRR-BERT address variability in radiology reports, enabling better automated generation and evaluation.

Abstract: Automated radiology report generation from chest X-ray (CXR) images has the
potential to improve clinical efficiency and reduce radiologists' workload.
However, most datasets, including the publicly available MIMIC-CXR and CheXpert
Plus, consist entirely of free-form reports, which are inherently variable and
unstructured. This variability poses challenges for both generation and
evaluation: existing models struggle to produce consistent, clinically
meaningful reports, and standard evaluation metrics fail to capture the nuances
of radiological interpretation. To address this, we introduce Structured
Radiology Report Generation (SRRG), a new task that reformulates free-text
radiology reports into a standardized format, ensuring clarity, consistency,
and structured clinical reporting. We create a novel dataset by restructuring
reports using large language models (LLMs) following strict structured
reporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained
disease classification model trained on 55 labels, enabling more precise and
clinically informed evaluation of structured reports. To assess report quality,
we propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease
taxonomy to bridge the gap between free-text variability and structured
clinical reporting. We validate our dataset through a reader study conducted by
five board-certified radiologists and extensive benchmarking experiments.

</details>


### [98] [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/pdf/2505.24241)
*Naibin Gu, Yilong Chen, Zhenyu Zhang, Peng Fu, Zheng Lin, Shuohuan Wang, Yu Sun, Hua Wu, Weiping Wang, Haifeng Wang*

Main category: cs.CL

TL;DR: APEX (Advantageous Parameter EXpansion Training) improves model performance by expanding advantageous parameters, reducing computational overhead while outperforming full-parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Scaling up parameters improves performance but increases computational costs. Identifying and leveraging advantageous parameters can optimize training.

Method: APEX progressively expands advantageous parameters into disadvantageous ones, increasing their proportion. Theoretical analysis uses matrix effective rank.

Result: APEX outperforms full-parameter tuning with 52% parameters in instruction tuning and matches perplexity with 33% data in pre-training, improving downstream tasks.

Conclusion: APEX efficiently enhances model performance by focusing on advantageous parameters, reducing computational and data requirements.

Abstract: Although scaling up the number of trainable parameters in both pre-training
and fine-tuning can effectively improve the performance of large language
models, it also leads to increased computational overhead. When delving into
the parameter difference, we find that a subset of parameters, termed
advantageous parameters, plays a crucial role in determining model performance.
Further analysis reveals that stronger models tend to possess more such
parameters. In this paper, we propose Advantageous Parameter EXpansion Training
(APEX), a method that progressively expands advantageous parameters into the
space of disadvantageous ones, thereby increasing their proportion and
enhancing training effectiveness. Further theoretical analysis from the
perspective of matrix effective rank explains the performance gains of APEX.
Extensive experiments on both instruction tuning and continued pre-training
demonstrate that, in instruction tuning, APEX outperforms full-parameter tuning
while using only 52% of the trainable parameters. In continued pre-training,
APEX achieves the same perplexity level as conventional training with just 33%
of the training data, and yields significant improvements on downstream tasks.

</details>


### [99] [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/pdf/2505.24244)
*Nir Endy, Idan Daniel Grosbard, Yuval Ran-Milo, Yonatan Slutzky, Itay Tshuva, Raja Giryes*

Main category: cs.CL

TL;DR: The paper analyzes how factual information flows in Mamba SSM-based language models, comparing them to Transformers and adapting Transformer interpretability methods to Mamba models.


<details>
  <summary>Details</summary>
Motivation: To understand the information flow and dynamics in Mamba models by leveraging insights from Transformer architectures.

Method: Adapted Attention Knockout methodology from Transformers to Mamba-1 and Mamba-2, analyzing token and layer-wise information transmission.

Result: Revealed patterns of information emergence and dynamics, with some phenomena unique to Mamba models and others universal across LLMs.

Conclusion: The study provides a unified framework to understand Mamba's internal operations, highlighting differences and similarities with Transformers.

Abstract: This paper investigates the flow of factual information in Mamba State-Space
Model (SSM)-based language models. We rely on theoretical and empirical
connections to Transformer-based architectures and their attention mechanisms.
Exploiting this relationship, we adapt attentional interpretability techniques
originally developed for Transformers--specifically, the Attention Knockout
methodology--to both Mamba-1 and Mamba-2. Using them we trace how information
is transmitted and localized across tokens and layers, revealing patterns of
subject-token information emergence and layer-wise dynamics. Notably, some
phenomena vary between mamba models and Transformer based models, while others
appear universally across all models inspected--hinting that these may be
inherent to LLMs in general. By further leveraging Mamba's structured
factorization, we disentangle how distinct "features" either enable
token-to-token information exchange or enrich individual tokens, thus offering
a unified lens to understand Mamba internal operations.

</details>


### [100] [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/pdf/2505.24251)
*Xiaoyu Li, Xiao Li, Li Gao, Yiding Liu, Xiaoyang Wang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin*

Main category: cs.CL

TL;DR: A two-phase framework (G-SFT and C-RL) enhances proactive guidance in multi-turn conversations, improving accuracy, click-through rates, and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Address challenges in dynamically adapting to user goal shifts and maintaining low latency in real-time interactions for LLM-based systems.

Method: Proposes Goal-adaptive Supervised Fine-Tuning (G-SFT) for dynamic goal adaptation and Click-oriented Reinforcement Learning (C-RL) for improving engagement via click signals.

Result: Achieves 86.10% accuracy (+23.95% over baseline), 25.28% CTR (149.06% relative improvement), and reduces latency by 69.55%.

Conclusion: The dual-phase framework effectively balances goal tracking and interaction quality, demonstrating significant improvements in performance and efficiency.

Abstract: The evolution of Large Language Models (LLMs) has significantly advanced
multi-turn conversation systems, emphasizing the need for proactive guidance to
enhance users' interactions. However, these systems face challenges in
dynamically adapting to shifts in users' goals and maintaining low latency for
real-time interactions. In the Baidu Search AI assistant, an industrial-scale
multi-turn search system, we propose a novel two-phase framework to provide
proactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning
(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal
shifts and provides goal-relevant contextual information. G-SFT also
incorporates scalable knowledge transfer to distill insights from LLMs into a
lightweight model for real-time interaction. The second phase, Click-oriented
Reinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically
constructs preference pairs from user click signals, and proactively improves
click-through rates through more engaging guidance. This dual-phase
architecture achieves complementary objectives: G-SFT ensures accurate goal
tracking, while C-RL optimizes interaction quality through click signal-driven
reinforcement learning. Extensive experiments demonstrate that our framework
achieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and
25.28% CTR in online deployment (149.06% relative improvement), while reducing
inference latency by 69.55% through scalable knowledge distillation.

</details>


### [101] [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/pdf/2505.24472)
*Hieu Tran, Phuong-Anh Nguyen-Le, Huy Nghiem, Quang-Nhan Nguyen, Wei Ai, Marine Carpuat*

Main category: cs.CL

TL;DR: The paper introduces VietMix, a parallel corpus for code-mixed Vietnamese-English translation, and a synthetic data pipeline to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Machine translation struggles with code-mixed inputs in low-resource languages like Vietnamese.

Method: Curated VietMix corpus and developed a synthetic data generation pipeline with filtering for plausible code-mixing.

Result: Models improved, scoring up to 71.84 on COMETkiwi and 81.77 on XCOMET, with LLM-based assessments favoring augmented models.

Conclusion: VietMix and the augmentation method enhance ecological validity in neural MT and provide a framework for other low-resource pairs.

Abstract: Machine translation systems fail when processing code-mixed inputs for
low-resource languages. We address this challenge by curating VietMix, a
parallel corpus of naturally occurring code-mixed Vietnamese text paired with
expert English translations. Augmenting this resource, we developed a
complementary synthetic data generation pipeline. This pipeline incorporates
filtering mechanisms to ensure syntactic plausibility and pragmatic
appropriateness in code-mixing patterns. Experimental validation shows our
naturalistic and complementary synthetic data boost models' performance,
measured by translation quality estimation scores, of up to 71.84 on COMETkiwi
and 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,
augmented models are favored over seed fine-tuned counterparts in approximately
49% of judgments (54-56% excluding ties). VietMix and our augmentation
methodology advance ecological validity in neural MT evaluations and establish
a framework for addressing code-mixed translation challenges across other
low-resource pairs.

</details>


### [102] [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/pdf/2505.24263)
*Naila Shafirni Hidayat, Muhammad Dehan Al Kautsar, Alfan Farizki Wicaksono, Fajri Koto*

Main category: cs.CL

TL;DR: The paper evaluates data leakage detection methods in LLMs, comparing permutation and n-gram techniques under simulated leakage. It introduces a lightweight 'semi-half question' method, finding n-gram superior. Refined methods are applied to clean benchmarks, improving evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: Concerns about training data transparency and fairness in LLM evaluations due to potential benchmark contamination.

Method: Comparison of permutation, n-gram, and semi-half question methods under simulated leakage. Refinement for instance-level detection and efficiency.

Result: N-gram method achieves the highest F1-score. Cleaned benchmarks (MMLU, HellaSwag) show improved reliability in LLM evaluations.

Conclusion: Recommends contamination checks as standard practice for benchmark releases to ensure transparency and fairness.

Abstract: The performance of large language models (LLMs) continues to improve, as
reflected in rising scores on standard benchmarks. However, the lack of
transparency around training data raises concerns about potential overlap with
evaluation sets and the fairness of reported results. Although prior work has
proposed methods for detecting data leakage, these approaches primarily focus
on identifying outliers and have not been evaluated under controlled simulated
leakage conditions. In this work, we compare existing leakage detection
techniques, namely permutation and n-gram-based methods, under a continual
pretraining setup that simulates real-world leakage scenarios, and additionally
explore a lightweight method we call semi-half question. Although semi-half
offers a low-cost alternative, our analysis shows that the n-gram method
consistently achieves the highest F1-score. We also refine these techniques to
support instance-level detection and reduce computational overhead. Leveraging
the best-performing method, we create cleaned versions of MMLU and HellaSwag,
and re-evaluate several LLMs. Our findings present a practical path toward more
reliable and transparent evaluations, and we recommend contamination checks as
a standard step before releasing benchmark results.

</details>


### [103] [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/pdf/2505.24302)
*Yike Wang, Shangbin Feng, Yulia Tsvetkov, Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: ScienceMeter evaluates LLMs' scientific knowledge updates, measuring preservation, acquisition, and projection of knowledge. Results show gaps in current methods.


<details>
  <summary>Details</summary>
Motivation: LLMs' scientific knowledge becomes outdated quickly; a framework is needed to evaluate and improve knowledge update methods.

Method: ScienceMeter introduces three metrics (preservation, acquisition, projection) and tests five update methods on 15,444 papers and 30,888 claims.

Result: Best methods preserve 85.9%, acquire 71.7%, and project 37.7% of knowledge. Inference works for large models; training suits smaller ones.

Conclusion: Current methods fall short, highlighting the need for better scientific knowledge update mechanisms.

Abstract: Large Language Models (LLMs) are increasingly used to support scientific
research, but their knowledge of scientific advancements can quickly become
outdated. We introduce ScienceMeter, a new framework for evaluating scientific
knowledge update methods over scientific knowledge spanning the past, present,
and future. ScienceMeter defines three metrics: knowledge preservation, the
extent to which models' understanding of previously learned papers are
preserved; knowledge acquisition, how well scientific claims from newly
introduced papers are acquired; and knowledge projection, the ability of the
updated model to anticipate or generalize to related scientific claims that may
emerge in the future. Using ScienceMeter, we examine the scientific knowledge
of LLMs on claim judgment and generation tasks across a curated dataset of
15,444 scientific papers and 30,888 scientific claims from ten domains
including medicine, biology, materials science, and computer science. We
evaluate five representative knowledge update approaches including training-
and inference-time methods. With extensive experiments, we find that the
best-performing knowledge update methods can preserve only 85.9% of existing
knowledge, acquire 71.7% of new knowledge, and project 37.7% of future
knowledge. Inference-based methods work for larger models, whereas smaller
models require training to achieve comparable performance. Cross-domain
analysis reveals that performance on these objectives is correlated. Even when
applying on specialized scientific LLMs, existing knowledge update methods fail
to achieve these objectives collectively, underscoring that developing robust
scientific knowledge update mechanisms is both crucial and challenging.

</details>


### [104] [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/pdf/2505.24319)
*Yuntao Shi, Yi Luo, Yeyun Gong, Chen Lin*

Main category: cs.CL

TL;DR: HiCaM, a Hierarchical-Causal Modification framework, improves LLMs' performance in long-form text modification by addressing undesired and missed modifications, achieving a 79.50% win rate.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-form text modification, often altering irrelevant content or missing necessary changes, harming document coherence.

Method: HiCaM uses a hierarchical summary tree and causal graph to guide modifications.

Result: Evaluations show HiCaM outperforms strong LLMs with a 79.50% win rate.

Conclusion: HiCaM consistently improves performance across models and domains, proving its effectiveness.

Abstract: Large Language Models (LLMs) have achieved remarkable success in various
domains. However, when handling long-form text modification tasks, they still
face two major problems: (1) producing undesired modifications by
inappropriately altering or summarizing irrelevant content, and (2) missing
necessary modifications to implicitly related passages that are crucial for
maintaining document coherence. To address these issues, we propose HiCaM, a
Hierarchical-Causal Modification framework that operates through a hierarchical
summary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a
multi-domain dataset from various benchmarks, providing a resource for
assessing its effectiveness. Comprehensive evaluations on the dataset
demonstrate significant improvements over strong LLMs, with our method
achieving up to a 79.50\% win rate. These results highlight the
comprehensiveness of our approach, showing consistent performance improvements
across multiple models and domains.

</details>


### [105] [Towards Effective Code-Integrated Reasoning](https://arxiv.org/pdf/2505.24480)
*Fei Bai, Yingqian Min, Beichen Zhang, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen*

Main category: cs.CL

TL;DR: The paper introduces a method for code-integrated reasoning using tool-augmented RL, addressing instability with enhanced training strategies, and shows improved performance on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enable models to generate and execute code for reasoning, leveraging tool-augmented RL while overcoming its instability.

Method: Enhanced training strategies for tool-augmented RL, balancing exploration and stability to improve code-integrated reasoning.

Result: Significant performance gains on five math reasoning benchmarks, with insights into capability extension and efficiency.

Conclusion: Code-integrated reasoning enhances model capabilities and efficiency, with reproducible results provided.

Abstract: In this paper, we investigate code-integrated reasoning, where models
generate code when necessary and integrate feedback by executing it through a
code interpreter. To acquire this capability, models must learn when and how to
use external code tools effectively, which is supported by tool-augmented
reinforcement learning (RL) through interactive learning. Despite its benefits,
tool-augmented RL can still suffer from potential instability in the learning
dynamics. In light of this challenge, we present a systematic approach to
improving the training effectiveness and stability of tool-augmented RL for
code-integrated reasoning. Specifically, we develop enhanced training
strategies that balance exploration and stability, progressively building
tool-use capabilities while improving reasoning performance. Through extensive
experiments on five mainstream mathematical reasoning benchmarks, our model
demonstrates significant performance improvements over multiple competitive
baselines. Furthermore, we conduct an in-depth analysis of the mechanism and
effect of code-integrated reasoning, revealing several key insights, such as
the extension of model's capability boundaries and the simultaneous improvement
of reasoning efficiency through code integration. All data and code for
reproducing this work are available at: https://github.com/RUCAIBox/CIR.

</details>


### [106] [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/pdf/2505.24331)
*Fanhang Man, Huandong Wang, Jianjie Fang, Zhaoyi Deng, Baining Zhao, Xinlei Chen, Yong Li*

Main category: cs.CL

TL;DR: The paper introduces sentiment forecasting on social media to predict future user sentiments in response to ongoing events, using sentiment-related features and a multi-perspective role-playing framework, showing significant improvements.


<details>
  <summary>Details</summary>
Motivation: To predict imminent user sentiments on social media, which is rarely studied, to better understand and respond to evolving events.

Method: Extracts sentiment-related features and proposes a multi-perspective role-playing framework to simulate human response processes.

Result: Preliminary results show significant improvement in sentiment forecasting at both microscopic and macroscopic levels.

Conclusion: The proposed framework effectively enhances sentiment forecasting, offering insights into future user sentiments in evolving events.

Abstract: User sentiment on social media reveals the underlying social trends, crises,
and needs. Researchers have analyzed users' past messages to trace the
evolution of sentiments and reconstruct sentiment dynamics. However, predicting
the imminent sentiment of an ongoing event is rarely studied. In this paper, we
address the problem of \textbf{sentiment forecasting} on social media to
predict the user's future sentiment in response to the development of the
event. We extract sentiment-related features to enhance the modeling skill and
propose a multi-perspective role-playing framework to simulate the process of
human response. Our preliminary results show significant improvement in
sentiment forecasting on both microscopic and macroscopic levels.

</details>


### [107] [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/pdf/2505.24332)
*Wenxuan Shi, Haochen Tan, Chuqiao Kuang, Xiaoguang Li, Xiaozhe Ren, Chen Zhang, Hanting Chen, Yasheng Wang, Lifeng Shang, Fisher Yu, Yunhe Wang*

Main category: cs.CL

TL;DR: The paper introduces Search Intensity Scaling (SIS) and WebPuzzle dataset to improve LLMs' adaptive information-seeking in open-web QA, proposing DeepDiver, an RL framework that enhances SIS.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with iterative evidence gathering in open-web QA due to static prompting and limited training environments. The goal is to enable dynamic search decisions under real-world ambiguity and noise.

Method: Introduces WebPuzzle dataset (24K training, 275 test questions) and DeepDiver, an RL framework promoting adaptive search policies in open-web environments.

Result: DeepDiver empowers Pangu-7B-Reasoner to match performance of the larger DeepSeek-R1 on real-web tasks, demonstrating SIS generalization to open-ended tasks.

Conclusion: The work advances adaptive information seeking in LLMs, providing a benchmark (WebPuzzle) and framework (DeepDiver) for future research.

Abstract: Information seeking demands iterative evidence gathering and reflective
reasoning, yet large language models (LLMs) still struggle with it in open-web
question answering. Existing methods rely on static prompting rules or training
with Wikipedia-based corpora and retrieval environments, limiting adaptability
to the real-world web environment where ambiguity, conflicting evidence, and
noise are prevalent. These constrained training settings hinder LLMs from
learning to dynamically decide when and where to search, and how to adjust
search depth and frequency based on informational demands. We define this
missing capacity as Search Intensity Scaling (SIS)--the emergent skill to
intensify search efforts under ambiguous or conflicting conditions, rather than
settling on overconfident, under-verification answers.
  To study SIS, we introduce WebPuzzle, the first dataset designed to foster
information-seeking behavior in open-world internet environments. WebPuzzle
consists of 24K training instances and 275 test questions spanning both
wiki-based and open-web queries. Building on this dataset, we propose
DeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by
encouraging adaptive search policies through exploration under a real-world
open-web environment. Experimental results show that Pangu-7B-Reasoner
empowered by DeepDiver achieve performance on real-web tasks comparable to the
671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from
cold-start supervised fine-tuning to a carefully designed RL phase, and present
that its capability of SIS generalizes from closed-form QA to open-ended tasks
such as long-form writing. Our contributions advance adaptive information
seeking in LLMs and provide a valuable benchmark and dataset for future
research.

</details>


### [108] [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/pdf/2505.24500)
*Guiyang Hou, Xing Gao, Yuchuan Wu, Xiang Huang, Wenqi Zhang, Zhe Zheng, Yongliang Shen, Jialu Du, Fei Huang, Yongbin Li, Weiming Lu*

Main category: cs.CL

TL;DR: The paper introduces TimeHC-RL, a method to enhance LLMs' social intelligence by combining intuitive and deliberate thinking, outperforming System 2 RL and rivaling advanced models.


<details>
  <summary>Details</summary>
Motivation: Current LLMs excel in IQ-related tasks but lack in social domains, which require a mix of cognitive modes.

Method: Proposes Temporal-aware Hierarchical Cognitive Reinforcement Learning (TimeHC-RL) for post-training enhancement of social intelligence.

Result: TimeHC-RL outperforms System 2 RL and matches advanced models like DeepSeek-R1 and OpenAI-O3.

Conclusion: TimeHC-RL effectively improves LLMs' social intelligence, offering insights for future research.

Abstract: Recently, Large Language Models (LLMs) have made significant progress in
IQ-related domains that require careful thinking, such as mathematics and
coding. However, enhancing LLMs' cognitive development in social domains,
particularly from a post-training perspective, remains underexplored.
Recognizing that the social world follows a distinct timeline and requires a
richer blend of cognitive modes (from intuitive reactions (System 1) and
surface-level thinking to deliberate thinking (System 2)) than mathematics,
which primarily relies on System 2 cognition (careful, step-by-step reasoning),
we introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning
(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we
systematically explore improving LLMs' social intelligence and validate the
effectiveness of the TimeHC-RL method, through five other post-training
paradigms and two test-time intervention paradigms on eight datasets with
diverse data patterns. Experimental results reveal the superiority of our
proposed TimeHC-RL method compared to the widely adopted System 2 RL method. It
gives the 7B backbone model wings, enabling it to rival the performance of
advanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic
exploration from post-training and test-time interventions perspectives to
improve LLMs' social intelligence has uncovered several valuable insights.

</details>


### [109] [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/pdf/2505.24354)
*Qianqian Zhang, Jiajia Liao, Heting Ying, Yibo Ma, Haozhan Shen, Jingcheng Li, Peng Liu, Lu Zhang, Chunxin Fang, Kyusong Lee, Ruochen Xu, Tiancheng Zhao*

Main category: cs.CL

TL;DR: AGORA is a framework for developing and evaluating language agents, offering modular architecture, reusable algorithms, and a standardized evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Challenges in developing robust language agents include engineering overhead, lack of standardized components, and insufficient evaluation frameworks.

Method: AGORA introduces a graph-based workflow engine, reusable algorithms, and a rigorous evaluation framework.

Result: Experiments show sophisticated reasoning enhances capabilities, but simpler methods like Chain-of-Thought are robust and efficient.

Conclusion: AGORA simplifies agent development and standardizes evaluation, advancing reproducible research.

Abstract: Language agents powered by large language models (LLMs) have demonstrated
remarkable capabilities in understanding, reasoning, and executing complex
tasks. However, developing robust agents presents significant challenges:
substantial engineering overhead, lack of standardized components, and
insufficient evaluation frameworks for fair comparison. We introduce Agent
Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and
extensible framework that addresses these challenges through three key
contributions: (1) a modular architecture with a graph-based workflow engine,
efficient memory management, and clean component abstraction; (2) a
comprehensive suite of reusable agent algorithms implementing state-of-the-art
reasoning approaches; and (3) a rigorous evaluation framework enabling
systematic comparison across multiple dimensions. Through extensive experiments
on mathematical reasoning and multimodal tasks, we evaluate various agent
algorithms across different LLMs, revealing important insights about their
relative strengths and applicability. Our results demonstrate that while
sophisticated reasoning approaches can enhance agent capabilities, simpler
methods like Chain-of-Thought often exhibit robust performance with
significantly lower computational overhead. AGORA not only simplifies language
agent development but also establishes a foundation for reproducible agent
research through standardized evaluation protocols.

</details>


### [110] [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/pdf/2505.24355)
*Sihan Tan, Taro Miyazaki, Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: A multilingual gloss-free model for Sign Language Translation (SLT) is proposed, supporting 10 sign languages and handling various SLT tasks, achieving competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To bridge the communication gap between sign and spoken communities by leveraging multilingual resources, addressing low-resource issues and unexplored multilingual SLT challenges.

Method: A multilingual gloss-free model with dual CTC objectives for token-level SL identification and spoken text generation.

Result: Competitive performance on multilingual SP-10, PHOENIX14T, and CSL-Daily benchmarks.

Conclusion: The proposed model effectively addresses language conflicts and alignment difficulties in multilingual SLT, enhancing accessibility.

Abstract: Sign Language Translation (SLT) aims to convert sign language (SL) videos
into spoken language text, thereby bridging the communication gap between the
sign and the spoken community. While most existing works focus on translating a
single sign language into a single spoken language (one-to-one SLT), leveraging
multilingual resources could mitigate low-resource issues and enhance
accessibility. However, multilingual SLT (MLSLT) remains unexplored due to
language conflicts and alignment difficulties across SLs and spoken languages.
To address these challenges, we propose a multilingual gloss-free model with
dual CTC objectives for token-level SL identification and spoken text
generation. Our model supports 10 SLs and handles one-to-one, many-to-one, and
many-to-many SLT tasks, achieving competitive performance compared to
state-of-the-art methods on three widely adopted benchmarks: multilingual
SP-10, PHOENIX14T, and CSL-Daily.

</details>


### [111] [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/pdf/2505.24362)
*Anum Afzal, Florian Matthes, Gal Chechik, Yftah Ziser*

Main category: cs.CL

TL;DR: A probing classifier using LLM representations can predict zero-shot Chain-of-Thought (CoT) success before token generation, outperforming BERT-based methods. Early reasoning steps often contain key information, enabling early stopping without significant loss.


<details>
  <summary>Details</summary>
Motivation: To determine if the success of zero-shot CoT reasoning can be predicted early, leveraging LLM representations for efficiency.

Method: Use a probing classifier based on LLM representations and compare it to a BERT-based baseline. Analyze the impact of early vs. later reasoning steps.

Result: LLM-based classifier performs well before token generation, while BERT relies on shallow cues. Early reasoning steps often suffice, enabling effective early stopping.

Conclusion: Early representations in LLMs encode crucial reasoning information, suggesting potential for optimizing CoT efficiency via early stopping or guided methods.

Abstract: We investigate whether the success of a zero-shot Chain-of-Thought (CoT)
process can be predicted before completion. We discover that a probing
classifier, based on LLM representations, performs well \emph{even before a
single token is generated}, suggesting that crucial information about the
reasoning process is already present in the initial steps representations. In
contrast, a strong BERT-based baseline, which relies solely on the generated
tokens, performs worse, likely because it depends on shallow linguistic cues
rather than deeper reasoning dynamics. Surprisingly, using later reasoning
steps does not always improve classification. When additional context is
unhelpful, earlier representations resemble later ones more, suggesting LLMs
encode key information early. This implies reasoning can often stop early
without loss. To test this, we conduct early stopping experiments, showing that
truncating CoT reasoning still improves performance over not using CoT at all,
though a gap remains compared to full reasoning. However, approaches like
supervised learning or reinforcement learning designed to shorten CoT chains
could leverage our classifier's guidance to identify when early stopping is
effective. Our findings provide insights that may support such methods, helping
to optimize CoT's efficiency while preserving its benefits.\footnote{Code and
data is available at
\href{https://github.com/anum94/CoTpred}{\texttt{github.com/anum94/CoTpred}}.

</details>


### [112] [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/pdf/2505.24523)
*Andrea Pedrotti, Michele Papucci, Cristiano Ciaccio, Alessio Miaschi, Giovanni Puccetti, Felice Dell'Orletta, Andrea Esuli*

Main category: cs.CL

TL;DR: The paper explores adversarial attacks on Machine-Generated Text (MGT) detectors by fine-tuning models to mimic human-written text, revealing vulnerabilities in current detection methods.


<details>
  <summary>Details</summary>
Motivation: Concerns about malicious use of synthetic content and the lack of robust benchmarks for MGT detection in real-world scenarios.

Method: Fine-tuning language models using Direct Preference Optimization (DPO) to shift MGT style toward human-written text and testing detectors like Mage, Radar, and LLM-DetectAIve.

Result: Detectors are easily fooled, showing a significant drop in performance, indicating their reliance on stylistic clues.

Conclusion: Highlights the need for improving detection methods to handle unseen in-domain texts and adversarial attacks.

Abstract: Recent advancements in Generative AI and Large Language Models (LLMs) have
enabled the creation of highly realistic synthetic content, raising concerns
about the potential for malicious use, such as misinformation and manipulation.
Moreover, detecting Machine-Generated Text (MGT) remains challenging due to the
lack of robust benchmarks that assess generalization to real-world scenarios.
In this work, we present a pipeline to test the resilience of state-of-the-art
MGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed
adversarial attacks. To challenge the detectors, we fine-tune language models
using Direct Preference Optimization (DPO) to shift the MGT style toward
human-written text (HWT). This exploits the detectors' reliance on stylistic
clues, making new generations more challenging to detect. Additionally, we
analyze the linguistic shifts induced by the alignment and which features are
used by detectors to detect MGT texts. Our results show that detectors can be
easily fooled with relatively few examples, resulting in a significant drop in
detection performance. This highlights the importance of improving detection
methods and making them robust to unseen in-domain texts.

</details>


### [113] [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/pdf/2505.24377)
*Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, Yun-Nung Chen*

Main category: cs.CL

TL;DR: The paper explores strategies for enhancing large language models (LLMs) using external knowledge, focusing on structured data like tables and knowledge graphs, and analyzes trade-offs in interpretability, scalability, and performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations of LLMs, such as limited memory and hallucination, by integrating external knowledge for accurate, context-based inference.

Method: Systematic exploration of external knowledge integration, with taxonomies for unstructured and structured data (tables and KGs), and review of integration paradigms and methods.

Result: Comparative analysis reveals trade-offs among interpretability, scalability, and performance in knowledge-enhanced LLMs.

Conclusion: Provides insights for developing trustworthy and generalizable LLMs by leveraging structured external knowledge.

Abstract: Recent advancements in large language models (LLMs) have enhanced
natural-language reasoning. However, their limited parametric memory and
susceptibility to hallucination present persistent challenges for tasks
requiring accurate, context-based inference. To overcome these limitations, an
increasing number of studies have proposed leveraging external knowledge to
enhance LLMs. This study offers a systematic exploration of strategies for
using external knowledge to enhance LLMs, beginning with a taxonomy that
categorizes external knowledge into unstructured and structured data. We then
focus on structured knowledge, presenting distinct taxonomies for tables and
knowledge graphs (KGs), detailing their integration paradigms with LLMs, and
reviewing representative methods. Our comparative analysis further highlights
the trade-offs among interpretability, scalability, and performance, providing
insights for developing trustworthy and generalizable knowledge-enhanced LLMs.

</details>


### [114] [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.24388)
*Hao Chen, Yukun Yan, Sen Mei, Wanxiang Che, Zhenghao Liu, Qi Shi, Xinze Li, Yuchun Fan, Pengcheng Huang, Qiushi Xiong, Zhiyuan Liu, Maosong Sun*

Main category: cs.CL

TL;DR: ClueAnchor enhances RAG by extracting key clues and optimizing reasoning paths, outperforming existing methods in robustness and completeness.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems underutilize retrieved documents, struggling with implicit or noisy evidence.

Method: ClueAnchor extracts key clues, generates multiple reasoning paths, and optimizes selection via reward-based preference.

Result: Outperforms prior RAG baselines in reasoning completeness and robustness, resilient to noise.

Conclusion: ClueAnchor improves RAG by effectively integrating scattered or implicit evidence without explicit supervision.

Abstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)
with external knowledge to improve factuality. However, existing RAG systems
frequently underutilize the retrieved documents, failing to extract and
integrate the key clues needed to support faithful and interpretable reasoning,
especially in cases where relevant evidence is implicit, scattered, or obscured
by noise. To address this issue, we propose ClueAnchor, a novel framework for
enhancing RAG via clue-anchored reasoning exploration and optimization.
ClueAnchor extracts key clues from retrieved content and generates multiple
reasoning paths based on different knowledge configurations, optimizing the
model by selecting the most effective one through reward-based preference
optimization. Experiments show that ClueAnchor significantly outperforms prior
RAG baselines in reasoning completeness and robustness. Further analysis
confirms its strong resilience to noisy or partially relevant retrieved
content, as well as its capability to identify supporting evidence even in the
absence of explicit clue supervision during inference.

</details>


### [115] [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/pdf/2505.24423)
*Zhiwei Liu, Lingfei Qian, Qianqian Xie, Jimin Huang, Kailai Yang, Sophia Ananiadou*

Main category: cs.CL

TL;DR: The paper introduces MMAFFBen, a multilingual multimodal benchmark for affective analysis, and evaluates LMs' capabilities in this domain.


<details>
  <summary>Details</summary>
Motivation: Despite LMs' success in NLP and CV, their potential in affective analysis is underexplored due to lack of benchmarks and task complexity.

Method: MMAFFBen is created, covering text, image, and video in 35 languages for four affective tasks. MMAFFIn dataset is built for fine-tuning, leading to MMAFFLM-3b and MMAFFLM-7b models.

Result: Various LMs, including GPT-4o-mini, are systematically evaluated for affective understanding.

Conclusion: The work provides a comprehensive benchmark and tools to advance affective analysis in LMs.

Abstract: Large language models and vision-language models (which we jointly call LMs)
have transformed NLP and CV, demonstrating remarkable potential across various
fields. However, their capabilities in affective analysis (i.e. sentiment
analysis and emotion detection) remain underexplored. This gap is largely due
to the absence of comprehensive evaluation benchmarks, and the inherent
complexity of affective analysis tasks. In this paper, we introduce MMAFFBen,
the first extensive open-source benchmark for multilingual multimodal affective
analysis. MMAFFBen encompasses text, image, and video modalities across 35
languages, covering four key affective analysis tasks: sentiment polarity,
sentiment intensity, emotion classification, and emotion intensity. Moreover,
we construct the MMAFFIn dataset for fine-tuning LMs on affective analysis
tasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate
various representative LMs, including GPT-4o-mini, providing a systematic
comparison of their affective understanding capabilities. This project is
available at https://github.com/lzw108/MMAFFBen.

</details>


### [116] [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/pdf/2505.24427)
*Christopher Bagdon, Aidan Combs, Carina Silberer, Roman Klinger*

Main category: cs.CL

TL;DR: Study compares genuine and study-created social media posts for emotion expression, finding differences in length, reliance on text vs. images, and demographic biases. Study-created data is useful for training but requires genuine data for realistic model evaluation.


<details>
  <summary>Details</summary>
Motivation: To understand how study-created content differs from genuine content in emotion expression and its impact on model performance.

Method: Collect and compare study-created and genuine multimodal social media posts labeled for emotion, analyzing differences in content and model performance.

Result: Study-created posts are longer, rely more on text, focus on prototypical events, and differ demographically. They train models well but need genuine data for realistic evaluation.

Conclusion: Study-created data is valuable but must be supplemented with genuine data for accurate model assessment.

Abstract: Accurate modeling of subjective phenomena such as emotion expression requires
data annotated with authors' intentions. Commonly such data is collected by
asking study participants to donate and label genuine content produced in the
real world, or create content fitting particular labels during the study.
Asking participants to create content is often simpler to implement and
presents fewer risks to participant privacy than data donation. However, it is
unclear if and how study-created content may differ from genuine content, and
how differences may impact models. We collect study-created and genuine
multimodal social media posts labeled for emotion and compare them on several
dimensions, including model performance. We find that compared to genuine
posts, study-created posts are longer, rely more on their text and less on
their images for emotion expression, and focus more on emotion-prototypical
events. The samples of participants willing to donate versus create posts are
demographically different. Study-created data is valuable to train models that
generalize well to genuine data, but realistic effectiveness estimates require
genuine data.

</details>


### [117] [Localizing Persona Representations in LLMs](https://arxiv.org/pdf/2505.24539)
*Celia Cintas, Miriam Rateike, Erik Miehling, Elizabeth Daly, Skyler Speakman*

Main category: cs.CL

TL;DR: The study explores how personas (human traits, values, beliefs) are encoded in LLMs, identifying layers with the most divergence and analyzing activations to reveal shared and distinct embedding spaces.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs internally represent personas and inform efforts to modulate human traits in outputs.

Method: Used dimension reduction and pattern recognition to analyze layers and activations in pre-trained decoder-only LLMs.

Result: Personas show large differences in the final third of decoder layers, with overlapping activations for ethical perspectives but distinct regions for political ideologies.

Conclusion: Findings enhance understanding of LLM representations and guide future refinement of trait modulation in outputs.

Abstract: We present a study on how and where personas -- defined by distinct sets of
human characteristics, values, and beliefs -- are encoded in the representation
space of large language models (LLMs). Using a range of dimension reduction and
pattern recognition methods, we first identify the model layers that show the
greatest divergence in encoding these representations. We then analyze the
activations within a selected layer to examine how specific personas are
encoded relative to others, including their shared and distinct embedding
spaces. We find that, across multiple pre-trained decoder-only LLMs, the
analyzed personas show large differences in representation space only within
the final third of the decoder layers. We observe overlapping activations for
specific ethical perspectives -- such as moral nihilism and utilitarianism --
suggesting a degree of polysemy. In contrast, political ideologies like
conservatism and liberalism appear to be represented in more distinct regions.
These findings help to improve our understanding of how LLMs internally
represent information and can inform future efforts in refining the modulation
of specific human traits in LLM outputs. Warning: This paper includes
potentially offensive sample statements.

</details>


### [118] [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/pdf/2505.24428)
*Xu Wang, Zihao Li, Benyou Wang, Yan Hu, Difan Zou*

Main category: cs.CL

TL;DR: SSPU is a new unlearning framework using SAE features for precise, interpretable, and robust knowledge removal in LLMs, outperforming baselines in reducing harmful knowledge and improving adversarial robustness.


<details>
  <summary>Details</summary>
Motivation: Address privacy and safety concerns in LLMs by enabling selective knowledge removal, overcoming limitations of existing unlearning methods like lack of interpretability or robustness.

Method: SSPU uses a three-stage pipeline: data-driven layer/feature selection, subspace construction via QR decomposition, and constrained optimization to guide unlearning while preserving retained knowledge.

Result: SSPU reduces harmful knowledge accuracy by 3.22% and improves adversarial robustness under jailbreak prompts compared to baselines.

Conclusion: SSPU demonstrates the effectiveness of interpretable subspace-guided optimization for robust and controllable unlearning in LLMs.

Abstract: Large language models (LLMs) store vast amounts of information, making them
powerful yet raising privacy and safety concerns when selective knowledge
removal is required. Existing unlearning strategies, ranging from
gradient-based fine-tuning and model editing to sparse autoencoder (SAE)
steering, either lack interpretability or fail to provide a robust defense
against adversarial prompts. We propose SAE-Guided Subspace Projection
Unlearning (SSPU), a novel framework that leverages SAE features to drive
targeted updates in the model's parameter space, enabling precise,
interpretable, and robust unlearning. SSPU's three-stage pipeline performs
data-driven layer and feature selection, subspace construction via QR
decomposition, and constrained optimization that controls activations into an
"irrelevant" subspace while preserving retained knowledge. Overall, we use SAE
features to construct a subspace that supervises unlearning, refining the loss
and adding a regularization term to guide interpretable parameter updates. In
experiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,
TruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared
to the strongest baseline. It also improves adversarial robustness, lowering
malicious accuracy under jailbreak prompts compared to baselines. Our findings
expose the limitations of prior unlearning methods and demonstrate how
interpretable subspace-guided optimization can achieve robust, controllable
model behavior.

</details>


### [119] [Cross-Attention Speculative Decoding](https://arxiv.org/pdf/2505.24544)
*Wei Zhong, Manasa Bharadwaj, Yixiao Wang, Nikhil Verma, Yipeng Ji, Chul Lee*

Main category: cs.CL

TL;DR: Budget EAGLE (Beagle) is a cross-attention-based Transformer decoder for speculative decoding, simplifying architecture and improving efficiency while matching performance of self-attention models.


<details>
  <summary>Details</summary>
Motivation: Address the complexity and lack of generalization in current speculative decoding methods by introducing a simpler, cross-attention-based approach.

Method: Proposes Budget EAGLE (Beagle), a cross-attention-based Transformer decoder, and Two-Stage Block-Attention Training for stable and efficient training.

Result: Beagle achieves competitive inference speedups and higher training efficiency compared to EAGLE-v2, without auxiliary components.

Conclusion: Beagle offers a simpler, efficient, and generalizable alternative for speculative decoding in large language models.

Abstract: Speculative decoding (SD) is a widely adopted approach for accelerating
inference in large language models (LLMs), particularly when the draft and
target models are well aligned. However, state-of-the-art SD methods typically
rely on tightly coupled, self-attention-based Transformer decoders, often
augmented with auxiliary pooling or fusion layers. This coupling makes them
increasingly complex and harder to generalize across different models. We
present Budget EAGLE (Beagle), the first, to our knowledge,
cross-attention-based Transformer decoder SD model that achieves performance on
par with leading self-attention SD models (EAGLE-v2) while eliminating the need
for pooling or auxiliary components, simplifying the architecture, improving
training efficiency, and maintaining stable memory usage during training-time
simulation. To enable effective training of this novel architecture, we propose
Two-Stage Block-Attention Training, a new method that achieves training
stability and convergence efficiency in block-level attention scenarios.
Extensive experiments across multiple LLMs and datasets show that Beagle
achieves competitive inference speedups and higher training efficiency than
EAGLE-v2, offering a strong alternative for architectures in speculative
decoding.

</details>


### [120] [Exploring the Impact of Occupational Personas on Domain-Specific QA](https://arxiv.org/pdf/2505.24448)
*Eojin Kang, Jaehyuk Yu, Juae Kim*

Main category: cs.CL

TL;DR: Personas in LLMs for QA tasks: Profession-Based Personas (PBPs) slightly improve accuracy, while Occupational Personality-Based Personas (OPBPs) degrade performance. Relevance doesn't ensure effective knowledge use.


<details>
  <summary>Details</summary>
Motivation: To investigate whether personas enhance performance in domain-specific QA tasks, focusing on two types: PBPs and OPBPs.

Method: Empirical evaluations across multiple scientific domains comparing PBPs and OPBPs.

Result: PBPs slightly improve accuracy; OPBPs degrade performance despite semantic relevance.

Conclusion: Persona relevance alone isn't sufficient for effective knowledge use; future research should explore nuanced persona distinctions for better LLM reasoning.

Abstract: Recent studies on personas have improved the way Large Language Models (LLMs)
interact with users. However, the effect of personas on domain-specific
question-answering (QA) tasks remains a subject of debate. This study analyzes
whether personas enhance specialized QA performance by introducing two types of
persona: Profession-Based Personas (PBPs) (e.g., scientist), which directly
relate to domain expertise, and Occupational Personality-Based Personas (OPBPs)
(e.g., scientific person), which reflect cognitive tendencies rather than
explicit expertise. Through empirical evaluations across multiple scientific
domains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs
often degrade performance, even when semantically related to the task. Our
findings suggest that persona relevance alone does not guarantee effective
knowledge utilization and that they may impose cognitive constraints that
hinder optimal knowledge application. Future research can explore how nuanced
distinctions in persona representations guide LLMs, potentially contributing to
reasoning and knowledge retrieval that more closely mirror human social
conceptualization.

</details>


### [121] [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/pdf/2505.24553)
*Ye Eun Chun, Taeyoon Hwang, Seung-won Hwang, Byung-Hak Kim*

Main category: cs.CL

TL;DR: CREFT is a sequential framework using LLM agents to extract and refine character relations in narratives, outperforming baselines in accuracy and completeness.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with nuanced interactions in long-form narratives, necessitating a better solution for narrative analysis and script evaluation.

Method: CREFT employs knowledge distillation to build a base character graph, then iteratively refines composition, relations, roles, and group assignments.

Result: Experiments on Korean dramas show CREFT outperforms single-agent LLM baselines in accuracy and completeness.

Conclusion: CREFT enhances narrative comprehension and script review, benefiting entertainment, publishing, and education sectors.

Abstract: Understanding complex character relations is crucial for narrative analysis
and efficient script evaluation, yet existing extraction methods often fail to
handle long-form narratives with nuanced interactions. To address this
challenge, we present CREFT, a novel sequential framework leveraging
specialized Large Language Model (LLM) agents. First, CREFT builds a base
character graph through knowledge distillation, then iteratively refines
character composition, relation extraction, role identification, and group
assignments. Experiments on a curated Korean drama dataset demonstrate that
CREFT significantly outperforms single-agent LLM baselines in both accuracy and
completeness. By systematically visualizing character networks, CREFT
streamlines narrative comprehension and accelerates script review -- offering
substantial benefits to the entertainment, publishing, and educational sectors.

</details>


### [122] [When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways](https://arxiv.org/pdf/2505.24449)
*Kailin Jiang, Yuntao Du, Yukai Ding, Yuchen Ren, Ning Jiang, Zhi Gao, Zilong Zheng, Lei Liu, Bin Li, Qing Li*

Main category: cs.CL

TL;DR: The paper introduces the EVOKE benchmark to assess LMMs' ability to handle multimodal evolving knowledge, highlighting challenges like poor performance of existing methods and catastrophic forgetting during fine-tuning. It suggests text augmentation and continual learning as partial solutions.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating and improving multimodal evolving knowledge injection in LMMs, as existing methods struggle with real-world updates and catastrophic forgetting.

Method: Proposes the EVOKE benchmark for evaluation and identifies challenges through comprehensive assessment. Tests text/image augmentation and continual learning methods like Replay and MoELoRA.

Result: Text augmentation improves performance, while image augmentation fails. Continual learning methods mitigate forgetting, but current knowledge injection methods remain limited.

Conclusion: Current methods for evolving knowledge injection are inadequate, urging further research for more efficient and stable solutions.

Abstract: Large language/multimodal models (LLMs/LMMs) store extensive pre-trained
knowledge but struggle to maintain consistency with real-world updates, making
it difficult to avoid catastrophic forgetting while acquiring evolving
knowledge. Previous work focused on constructing textual knowledge datasets and
exploring knowledge injection in LLMs, lacking exploration of multimodal
evolving knowledge injection in LMMs. To address this, we propose the EVOKE
benchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in
real-world scenarios. Meanwhile, a comprehensive evaluation of multimodal
evolving knowledge injection revealed two challenges: (1) Existing knowledge
injection methods perform terribly on evolving knowledge. (2) Supervised
fine-tuning causes catastrophic forgetting, particularly instruction following
ability is severely compromised. Additionally, we provide pathways and find
that: (1) Text knowledge augmentation during the training phase improves
performance, while image augmentation cannot achieve it. (2) Continual learning
methods, especially Replay and MoELoRA, effectively mitigate forgetting. Our
findings indicate that current knowledge injection methods have many
limitations on evolving knowledge, which motivates further research on more
efficient and stable knowledge injection methods.

</details>


### [123] [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/pdf/2505.24554)
*Anna Sofia Lippolis, Minh Davide Ragagni, Paolo Ciancarini, Andrea Giovanni Nuzzolese, Valentina Presutti*

Main category: cs.CL

TL;DR: The paper introduces Bench4KE, a benchmarking system for evaluating LLM-based tools in Knowledge Engineering automation, focusing on Competency Questions (CQs).


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation for LLM-based KE automation tools hinders rigor and comparability.

Method: Bench4KE provides a gold standard CQ dataset and similarity metrics to assess tool performance.

Result: A comparative analysis of four CQ generation systems establishes a baseline for future research.

Conclusion: Bench4KE is extensible for other KE tasks and is publicly available under Apache 2.0.

Abstract: The availability of Large Language Models (LLMs) presents a unique
opportunity to reinvigorate research on Knowledge Engineering (KE) automation,
a trend already evident in recent efforts developing LLM-based methods and
tools for the automatic generation of Competency Questions (CQs). However, the
evaluation of these tools lacks standardisation. This undermines the
methodological rigour and hinders the replication and comparison of results. To
address this gap, we introduce Bench4KE, an extensible API-based benchmarking
system for KE automation. Its first release focuses on evaluating tools that
generate CQs automatically. CQs are natural language questions used by ontology
engineers to define the functional requirements of an ontology. Bench4KE
provides a curated gold standard consisting of CQ datasets from four real-world
ontology projects. It uses a suite of similarity metrics to assess the quality
of the CQs generated. We present a comparative analysis of four recent CQ
generation systems, which are based on LLMs, establishing a baseline for future
research. Bench4KE is also designed to accommodate additional KE automation
tasks, such as SPARQL query generation, ontology testing and drafting. Code and
datasets are publicly available under the Apache 2.0 license.

</details>


### [124] [Domain Pre-training Impact on Representations](https://arxiv.org/pdf/2505.24455)
*Cesar Gonzalez-Gutierrez, Ariadna Quattoni*

Main category: cs.CL

TL;DR: Pre-training on a small, specialized corpus can yield effective transformer representations, and combining generic and specialized corpora depends on task-corpus similarity.


<details>
  <summary>Details</summary>
Motivation: To analyze how the pre-training corpus affects transformer representation quality, focusing on pre-training alone.

Method: Empirical study comparing pre-training on small specialized corpora vs. combining generic and specialized corpora.

Result: Specialized corpora can be effective; combining corpora works best when the specialized corpus aligns with the task distribution.

Conclusion: Corpus choice and similarity to the target task are key factors in transformer representation quality.

Abstract: This empirical study analyzes the effects of the pre-training corpus on the
quality of learned transformer representations. We focus on the representation
quality induced solely through pre-training. Our experiments show that
pre-training on a small, specialized corpus can yield effective
representations, and that the success of combining a generic and a specialized
corpus depends on the distributional similarity between the target task and the
specialized corpus.

</details>


### [125] [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/pdf/2505.24575)
*Hyuntak Kim, Byung-Hak Kim*

Main category: cs.CL

TL;DR: NexusSum is a multi-agent LLM framework for summarizing long-form narratives like books and movies, achieving a 30% improvement in BERTScore without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with capturing complex plotlines and character interactions in long-form narratives.

Method: Uses a structured pipeline with dialogue-to-description transformation and hierarchical multi-LLM summarization.

Result: Achieves up to 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts.

Conclusion: Demonstrates the effectiveness of multi-agent LLMs for scalable, high-quality narrative summarization.

Abstract: Summarizing long-form narratives--such as books, movies, and TV
scripts--requires capturing intricate plotlines, character interactions, and
thematic coherence, a task that remains challenging for existing LLMs. We
introduce NexusSum, a multi-agent LLM framework for narrative summarization
that processes long-form text through a structured, sequential
pipeline--without requiring fine-tuning. Our approach introduces two key
innovations: (1) Dialogue-to-Description Transformation: A narrative-specific
preprocessing method that standardizes character dialogue and descriptive text
into a unified format, improving coherence. (2) Hierarchical Multi-LLM
Summarization: A structured summarization pipeline that optimizes chunk
processing and controls output length for accurate, high-quality summaries. Our
method establishes a new state-of-the-art in narrative summarization, achieving
up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV
scripts. These results demonstrate the effectiveness of multi-agent LLMs in
handling long-form content, offering a scalable approach for structured
summarization in diverse storytelling domains.

</details>


### [126] [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/pdf/2505.24456)
*Emilio Villa-Cueva, Sholpan Bolatzhanova, Diana Turmakhan, Kareem Elzeky, Henok Biadglign Ademtew, Alham Fikri Aji, Israel Abebe Azime, Jinheon Baek, Frederico Belcavello, Fermin Cristobal, Jan Christian Blaise Cruz, Mary Dabre, Raj Dabre, Toqeer Ehsan, Naome A Etori, Fauzan Farooqui, Jiahui Geng, Guido Ivetta, Thanmay Jayakumar, Soyeong Jeong, Zheng Wei Lim, Aishik Mandal, Sofia Martinelli, Mihail Minkov Mihaylov, Daniil Orel, Aniket Pramanick, Sukannya Purkayastha, Israfel Salazar, Haiyue Song, Tiago Timponi Torrent, Debela Desalegn Yadeta, Injy Hamed, Atnafu Lambebo Tonja, Thamar Solorio*

Main category: cs.CL

TL;DR: Images improve machine translation of cultural content, as shown by the CaMMT benchmark evaluating VLMs.


<details>
  <summary>Details</summary>
Motivation: Address challenges in translating cultural content due to conceptual differences between cultures.

Method: Introduce CaMMT, a dataset with images and parallel captions, and evaluate five VLMs in text-only and text+image settings.

Result: Visual context enhances translation quality, particularly for culturally-specific items, disambiguation, and gender accuracy.

Conclusion: CaMMT supports better multimodal translation systems aligned with cultural nuance.

Abstract: Cultural content poses challenges for machine translation systems due to the
differences in conceptualizations between cultures, where language alone may
fail to convey sufficient context to capture region-specific meanings. In this
work, we investigate whether images can act as cultural context in multimodal
translation. We introduce CaMMT, a human-curated benchmark of over 5,800
triples of images along with parallel captions in English and regional
languages. Using this dataset, we evaluate five Vision Language Models (VLMs)
in text-only and text+image settings. Through automatic and human evaluations,
we find that visual context generally improves translation quality, especially
in handling Culturally-Specific Items (CSIs), disambiguation, and correct
gender usage. By releasing CaMMT, we aim to support broader efforts in building
and evaluating multimodal translation systems that are better aligned with
cultural nuance and regional variation.

</details>


### [127] [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/pdf/2505.24593)
*Junzhuo Li, Bo Wang, Xiuze Zhou, Peijie Jiang, Jia Liu, Xuming Hu*

Main category: cs.CL

TL;DR: The paper proposes a cross-level attribution algorithm to analyze sparse MoE models, revealing efficiency and robustness patterns, and offering design principles for balancing specialization and robustness.


<details>
  <summary>Details</summary>
Motivation: The interpretability of MoE models, especially heterogeneous ones, is underexplored, and existing methods fail to capture dynamic routing-expert interactions.

Method: A cross-level attribution algorithm is used to analyze sparse MoE architectures (e.g., Qwen 1.5-MoE) against dense models (e.g., Llama-7B).

Result: MoE models show 37% higher efficiency via a 'mid-activation, late-amplification' pattern, with shared and routed experts handling general and specialized tasks, respectively. Robustness varies with depth (e.g., Qwen 1.5-MoE mitigates expert failures better than shallow OLMoE).

Conclusion: The study advances MoE interpretability, providing principles to balance efficiency, specialization, and robustness in model design.

Abstract: The interpretability of Mixture-of-Experts (MoE) models, especially those
with heterogeneous designs, remains underexplored. Existing attribution methods
for dense models fail to capture dynamic routing-expert interactions in sparse
MoE architectures. To address this issue, we propose a cross-level attribution
algorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,
Mixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results
show MoE models achieve 37% higher per-layer efficiency via a "mid-activation,
late-amplification" pattern: early layers screen experts, while late layers
refine knowledge collaboratively. Ablation studies reveal a "basic-refinement"
framework--shared experts handle general tasks (entity recognition), while
routed experts specialize in domain-specific processing (geographic
attributes). Semantic-driven routing is evidenced by strong correlations
between attention heads and experts (r=0.68), enabling task-aware coordination.
Notably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates
expert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10
experts) through shared expert redundancy, whereas shallow OLMoE suffers severe
degradation (76% drop). Task sensitivity further guides design: core-sensitive
tasks (geography) require concentrated expertise, while distributed-tolerant
tasks (object attributes) leverage broader participation. These insights
advance MoE interpretability, offering principles to balance efficiency,
specialization, and robustness.

</details>


### [128] [Limited-Resource Adapters Are Regularizers, Not Linguists](https://arxiv.org/pdf/2505.24525)
*Marcell Fekete, Nathaniel R. Robinson, Ernests Lavrinovics, E. Djeride Jean-Baptiste, Raj Dabre, Johannes Bjerva, Heather Lent*

Main category: cs.CL

TL;DR: Adapter souping and cross-attention fine-tuning improve low-resource MT for Creole languages, but linguistic relatedness doesn't correlate with performance. Adapters likely act as parameter regularizers, not for information transfer.


<details>
  <summary>Details</summary>
Motivation: Enhance low-resource language technologies, specifically for Creole languages, by leveraging cross-lingual transfer and adapter methods.

Method: Combines adapter souping with cross-attention fine-tuning of a pre-trained MT model.

Result: Substantial performance improvement over baselines, but linguistic relatedness doesn't affect adapter performance. Randomly initialized adapters work equally well.

Conclusion: Adapters serve as parameter regularizers, not for meaningful transfer. Neural methods may not always leverage linguistic knowledge intuitively.

Abstract: Cross-lingual transfer from related high-resource languages is a
well-established strategy to enhance low-resource language technologies. Prior
work has shown that adapters show promise for, e.g., improving low-resource
machine translation (MT). In this work, we investigate an adapter souping
method combined with cross-attention fine-tuning of a pre-trained MT model to
leverage language transfer for three low-resource Creole languages, which
exhibit relatedness to different language groups across distinct linguistic
dimensions. Our approach improves performance substantially over baselines.
However, we find that linguistic relatedness -- or even a lack thereof -- does
not covary meaningfully with adapter performance. Surprisingly, our
cross-attention fine-tuning approach appears equally effective with randomly
initialized adapters, implying that the benefit of adapters in this setting
lies in parameter regularization, and not in meaningful information transfer.
We provide analysis supporting this regularization hypothesis. Our findings
underscore the reality that neural language processing involves many success
factors, and that not all neural methods leverage linguistic knowledge in
intuitive ways.

</details>


### [129] [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/pdf/2505.24616)
*Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova*

Main category: cs.CL

TL;DR: POLLUX is an open-source benchmark for evaluating Russian LLMs, featuring a novel methodology for interpretable assessment, 35 task types, and LLM-as-a-Judge evaluators.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability and scalability of LLM evaluation in Russian, reducing reliance on costly human judgments.

Method: Develops a criteria-driven scoring protocol with detailed task taxonomy (35 types, 2,100 prompts) and LLM-as-a-Judge evaluators (7B and 32B).

Result: Provides transparent, scalable evaluation tools, replacing less precise human comparisons.

Conclusion: POLLUX offers a robust, interpretable framework for assessing generative LLMs in Russian.

Abstract: We introduce POLLUX, a comprehensive open-source benchmark designed to
evaluate the generative capabilities of large language models (LLMs) in
Russian. Our main contribution is a novel evaluation methodology that enhances
the interpretability of LLM assessment. For each task type, we define a set of
detailed criteria and develop a scoring protocol where models evaluate
responses and provide justifications for their ratings. This enables
transparent, criteria-driven evaluation beyond traditional resource-consuming,
side-by-side human comparisons. POLLUX includes a detailed, fine-grained
taxonomy of 35 task types covering diverse generative domains such as code
generation, creative writing, and practical assistant use cases, totaling 2,100
manually crafted and professionally authored prompts. Each task is categorized
by difficulty (easy/medium/hard), with experts constructing the dataset
entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)
evaluators trained for nuanced assessment of generative outputs. This approach
provides scalable, interpretable evaluation and annotation tools for model
development, effectively replacing costly and less precise human judgments.

</details>


### [130] [DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance](https://arxiv.org/pdf/2505.24532)
*Ali Khoramfar, Ali Ramezani, Mohammad Mahdi Mohajeri, Mohammad Javad Dousti, Majid Nili Ahmadabadi, Heshaam Faili*

Main category: cs.CL

TL;DR: DeepQuestion is a framework that enhances datasets using Bloom's taxonomy to test higher-order reasoning in LLMs, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on benchmarks but struggle with real-world tasks, highlighting the need for better evaluation methods.

Method: DeepQuestion augments datasets with novel questions based on Bloom's taxonomy to assess evaluative and creative skills.

Result: Experiments show up to 70% accuracy drop in higher-order tasks across various LLMs.

Conclusion: Cognitively diverse benchmarks are essential for advancing LLM capabilities; DeepQuestion will be released publicly.

Abstract: LLMs often excel on standard benchmarks but falter on real-world tasks. We
introduce DeepQuestion, a scalable automated framework that augments existing
datasets based on Bloom's taxonomy and creates novel questions that trace
original solution paths to probe evaluative and creative skills. Extensive
experiments across ten open-source and proprietary models, covering both
general-purpose and reasoning LLMs, reveal substantial performance drops (even
up to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in
deep reasoning. Our work highlights the need for cognitively diverse benchmarks
to advance LLM progress. DeepQuestion and related datasets will be released
upon acceptance of the paper.

</details>


### [131] [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/pdf/2505.24630)
*Junyi Li, Hwee Tou Ng*

Main category: cs.CL

TL;DR: RL fine-tuning in LLMs boosts reasoning but increases hallucinations. FSPO, a new method with factuality checks, reduces hallucinations and improves accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the drawback of increased hallucinations in reasoning tasks due to RL fine-tuning in LLMs.

Method: Propose FSPO, an RL algorithm with step-wise factuality verification to adjust token-level advantages dynamically.

Result: FSPO reduces hallucinations and enhances reasoning accuracy in experiments with Qwen2.5 and Llama models.

Conclusion: FSPO improves reliability and performance in LLMs by mitigating hallucinations during reasoning.

Abstract: Large language models (LLMs) have significantly advanced in reasoning tasks
through reinforcement learning (RL) optimization, achieving impressive
capabilities across various challenging benchmarks. However, our empirical
analysis reveals a critical drawback: reasoning-oriented RL fine-tuning
significantly increases the prevalence of hallucinations. We theoretically
analyze the RL training dynamics, identifying high-variance gradient,
entropy-induced randomness, and susceptibility to spurious local optima as key
factors leading to hallucinations. To address this drawback, we propose
Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL
fine-tuning algorithm incorporating explicit factuality verification at each
reasoning step. FSPO leverages automated verification against given evidence to
dynamically adjust token-level advantage values, incentivizing factual
correctness throughout the reasoning process. Experiments across mathematical
reasoning and hallucination benchmarks using Qwen2.5 and Llama models
demonstrate that FSPO effectively reduces hallucinations while enhancing
reasoning accuracy, substantially improving both reliability and performance.

</details>


### [132] [Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections](https://arxiv.org/pdf/2505.24538)
*Orfeas Menis Mastromichalakis, Jason Liartis, Kristina Rose, Antoine Isaac, Giorgos Stamou*

Main category: cs.CL

TL;DR: An AI-powered tool detects and contextualizes offensive terms in Cultural Heritage metadata, aiding in creating more inclusive collections.


<details>
  <summary>Details</summary>
Motivation: Cultural Heritage data often contain outdated or offensive descriptions due to historical biases, posing challenges for institutions.

Method: The tool uses a multilingual vocabulary co-created with marginalized communities, NLP techniques, and LLMs to detect and contextualize offensive terms.

Result: Processed over 7.9 million records, providing insights into contentious terms without erasing them.

Conclusion: The approach aims to make biases visible and actionable, fostering more inclusive and accessible Cultural Heritage collections.

Abstract: Cultural Heritage (CH) data hold invaluable knowledge, reflecting the
history, traditions, and identities of societies, and shaping our understanding
of the past and present. However, many CH collections contain outdated or
offensive descriptions that reflect historical biases. CH Institutions (CHIs)
face significant challenges in curating these data due to the vast scale and
complexity of the task. To address this, we develop an AI-powered tool that
detects offensive terms in CH metadata and provides contextual insights into
their historical background and contemporary perception. We leverage a
multilingual vocabulary co-created with marginalized communities, researchers,
and CH professionals, along with traditional NLP techniques and Large Language
Models (LLMs). Available as a standalone web app and integrated with major CH
platforms, the tool has processed over 7.9 million records, contextualizing the
contentious terms detected in their metadata. Rather than erasing these terms,
our approach seeks to inform, making biases visible and providing actionable
insights for creating more inclusive and accessible CH collections.

</details>


### [133] [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/pdf/2505.24640)
*Jens-Joris Decorte, Jeroen Van Hautte, Chris Develder, Thomas Demeester*

Main category: cs.CL

TL;DR: ConTeXT-match, a contrastive learning approach, improves skill extraction efficiency and performance for labor market analysis, outperforming LLMs with a lightweight model. Skill-XL benchmark and JobBERT V2 enhance evaluation and job title normalization.


<details>
  <summary>Details</summary>
Motivation: Current skill extraction methods rely on computationally expensive LLMs, necessitating a more efficient solution for large-scale labor market analysis.

Method: Proposes ConTeXT-match, a contrastive learning approach with token-level attention for skill classification, and introduces Skill-XL benchmark and JobBERT V2 for job title normalization.

Result: Achieves state-of-the-art results with a lightweight model, improving efficiency and accuracy for skill extraction and job title representation.

Conclusion: The proposed models are efficient, accurate, and scalable, making them suitable for real-time labor market analysis.

Abstract: Labor market analysis relies on extracting insights from job advertisements,
which provide valuable yet unstructured information on job titles and
corresponding skill requirements. While state-of-the-art methods for skill
extraction achieve strong performance, they depend on large language models
(LLMs), which are computationally expensive and slow. In this paper, we propose
\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level
attention that is well-suited for the extreme multi-label classification task
of skill classification. \textbf{ConTeXT-match} significantly improves skill
extraction efficiency and performance, achieving state-of-the-art results with
a lightweight bi-encoder model. To support robust evaluation, we introduce
\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill
annotations that explicitly address the redundancy in the large label space.
Finally, we present \textbf{JobBERT V2}, an improved job title normalization
model that leverages extracted skills to produce high-quality job title
representations. Experiments demonstrate that our models are efficient,
accurate, and scalable, making them ideal for large-scale, real-time labor
market analysis.

</details>


### [134] [A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings](https://arxiv.org/pdf/2505.24550)
*Xiaoang Xu, Shuo Wang, Xu Han, Zhenghao Liu, Huijia Wu, Peipei Li, Zhiyuan Liu, Maosong Sun, Zhaofeng He*

Main category: cs.CL

TL;DR: A*-Thought is a tree search-based framework that efficiently compresses reasoning chains in Large Reasoning Models (LRMs) by isolating essential thoughts, balancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for efficient reasoning in LRMs often degrade performance by compressing Chain-of-Thought. A*-Thought aims to address this by identifying high-quality reasoning paths.

Method: The framework formulates reasoning as a search tree, using A* search and a cost function to find high-density, low-cost paths. A bidirectional importance estimation refines the search.

Result: A*-Thought improves QwQ-32B's performance by 2.39x with low budget and reduces output tokens by 50% with high budget, demonstrating generalization across LRMs.

Conclusion: A*-Thought effectively balances efficiency and performance in LRMs, offering a scalable solution for complex reasoning tasks.

Abstract: Large Reasoning Models (LRMs) achieve superior performance by extending the
thought length. However, a lengthy thinking trajectory leads to reduced
efficiency. Most of the existing methods are stuck in the assumption of
overthinking and attempt to reason efficiently by compressing the
Chain-of-Thought, but this often leads to performance degradation. To address
this problem, we introduce A*-Thought, an efficient tree search-based unified
framework designed to identify and isolate the most essential thoughts from the
extensive reasoning chains produced by these models. It formulates the
reasoning process of LRMs as a search tree, where each node represents a
reasoning span in the giant reasoning space. By combining the A* search
algorithm with a cost function specific to the reasoning path, it can
efficiently compress the chain of thought and determine a reasoning path with
high information density and low cost. In addition, we also propose a
bidirectional importance estimation mechanism, which further refines this
search process and enhances its efficiency beyond uniform sampling. Extensive
experiments on several advanced math tasks show that A*-Thought effectively
balances performance and efficiency over a huge search space. Specifically,
A*-Thought can improve the performance of QwQ-32B by 2.39$\times$ with
low-budget and reduce the length of the output token by nearly 50% with
high-budget. The proposed method is also compatible with several other LRMs,
demonstrating its generalization capability. The code can be accessed at:
https://github.com/AI9Stars/AStar-Thought.

</details>


### [135] [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/pdf/2505.24671)
*Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat*

Main category: cs.CL

TL;DR: Multi-Agent Debate framework improves cultural adaptability of LLMs by leveraging debates between agents, outperforming single-LLM baselines.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs' cultural adaptability for diverse global communities by moving beyond single-LLM approaches.

Method: Introduces a Multi-Agent Debate framework with two variants: exclusive debate and dynamic self-reflection/debate, evaluated on 7 LLMs using the NormAd-ETI benchmark.

Result: Debate improves accuracy and cultural parity, enabling smaller LLMs (7-9B) to match larger models (27B).

Conclusion: Multi-agent debate is effective for cultural adaptation, offering scalable improvements for LLMs.

Abstract: Large Language Models (LLMs) need to adapt their predictions to diverse
cultural contexts to benefit diverse communities across the world. While
previous efforts have focused on single-LLM, single-turn approaches, we propose
to exploit the complementary strengths of multiple LLMs to promote cultural
adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based
agents debate over a cultural scenario and collaboratively reach a final
decision. We propose two variants: one where either LLM agents exclusively
debate and another where they dynamically choose between self-reflection and
debate during their turns. We evaluate these approaches on 7 open-weight LLMs
(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette
norms in 75 countries. Experiments show that debate improves both overall
accuracy and cultural group parity over single-LLM baselines. Notably,
multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies
comparable to that of a much larger model (27B parameters).

</details>


### [136] [Improving Language and Modality Transfer in Translation by Character-level Modeling](https://arxiv.org/pdf/2505.24561)
*Ioannis Tsiamas, David Dale, Marta R. Costa-jussà*

Main category: cs.CL

TL;DR: A character-based approach improves adaptability for low-resource languages and modalities, outperforming subword models in translation and achieving state-of-the-art speech-to-text results.


<details>
  <summary>Details</summary>
Motivation: Expand translation coverage to low-resource languages by leveraging cross-lingual and cross-modal knowledge transfer.

Method: Uses SONAR for multilingual embeddings, a teacher-student approach for character-level encoding, and adapters to connect ASR models for speech translation.

Result: Better language transfer in text translation (75 languages) and state-of-the-art speech-to-text translation (33 languages).

Conclusion: Character-based methods enhance adaptability and performance in low-resource and multimodal settings.

Abstract: Current translation systems, despite being highly multilingual, cover only 5%
of the world's languages. Expanding language coverage to the long-tail of
low-resource languages requires data-efficient methods that rely on
cross-lingual and cross-modal knowledge transfer. To this end, we propose a
character-based approach to improve adaptability to new languages and
modalities. Our method leverages SONAR, a multilingual fixed-size embedding
space with different modules for encoding and decoding. We use a
teacher-student approach with parallel translation data to obtain a
character-level encoder. Then, using ASR data, we train a lightweight adapter
to connect a massively multilingual CTC ASR model (MMS), to the character-level
encoder, potentially enabling speech translation from 1,000+ languages.
Experimental results in text translation for 75 languages on FLORES+
demonstrate that our character-based approach can achieve better language
transfer than traditional subword-based models, especially outperforming them
in low-resource settings, and demonstrating better zero-shot generalizability
to unseen languages. Our speech adaptation, maximizing knowledge transfer from
the text modality, achieves state-of-the-art results in speech-to-text
translation on the FLEURS benchmark on 33 languages, surpassing previous
supervised and cascade models, albeit being a zero-shot model with minimal
supervision from ASR data.

</details>


### [137] [GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training](https://arxiv.org/pdf/2505.24581)
*Omer Nacar, Anis Koubaa, Serry Sibaee, Yasser Al-Habashi, Adel Ammar, Wadii Boulila*

Main category: cs.CL

TL;DR: The paper introduces GATE, a state-of-the-art Arabic text embedding model, addressing the lack of resources for Arabic STS tasks. It outperforms larger models by 20-25%.


<details>
  <summary>Details</summary>
Motivation: Limited high-quality datasets and pre-trained models for Arabic STS tasks hinder progress. GATE aims to fill this gap.

Method: GATE uses Matryoshka Representation Learning and a hybrid loss training approach with Arabic triplet datasets for NLI.

Result: GATE achieves a 20-25% performance improvement on STS benchmarks, surpassing larger models like OpenAI.

Conclusion: GATE effectively captures Arabic semantic nuances, advancing STS research for the language.

Abstract: Semantic textual similarity (STS) is a critical task in natural language
processing (NLP), enabling applications in retrieval, clustering, and
understanding semantic relationships between texts. However, research in this
area for the Arabic language remains limited due to the lack of high-quality
datasets and pre-trained models. This scarcity of resources has restricted the
accurate evaluation and advance of semantic similarity in Arabic text. This
paper introduces General Arabic Text Embedding (GATE) models that achieve
state-of-the-art performance on the Semantic Textual Similarity task within the
MTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid
loss training approach with Arabic triplet datasets for Natural Language
Inference, which are essential for enhancing model performance in tasks that
demand fine-grained semantic understanding. GATE outperforms larger models,
including OpenAI, with a 20-25% performance improvement on STS benchmarks,
effectively capturing the unique semantic nuances of Arabic.

</details>


### [138] [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/pdf/2505.24683)
*Dayeon Ki, Kevin Duh, Marine Carpuat*

Main category: cs.CL

TL;DR: The paper explores feedback mechanisms for AI systems, focusing on Machine Translation (MT), comparing explicit (error highlights, LLM explanations) and implicit (backtranslation, QA tables) feedback. Implicit feedback, especially QA tables, outperforms explicit feedback in improving decision accuracy, reliance, and user perceptions.


<details>
  <summary>Details</summary>
Motivation: To address the need for feedback mechanisms that help users, especially monolingual ones, assess AI predictions responsibly in MT scenarios.

Method: Studied a realistic MT scenario where users decide to share outputs with and without feedback. Compared four feedback types: error highlights, LLM explanations (explicit), backtranslation, and QA tables (implicit).

Result: All feedback types except error highlights improved decision accuracy and reliance. Implicit feedback, particularly QA tables, showed greater gains in accuracy, reliance, and user perceptions (helpfulness, trust, mental burden).

Conclusion: Implicit feedback, especially QA tables, is more effective than explicit feedback for improving user decisions and trust in AI systems like MT.

Abstract: As people increasingly use AI systems in work and daily life, feedback
mechanisms that help them use AI responsibly are urgently needed, particularly
in settings where users are not equipped to assess the quality of AI
predictions. We study a realistic Machine Translation (MT) scenario where
monolingual users decide whether to share an MT output, first without and then
with quality feedback. We compare four types of quality feedback: explicit
feedback that directly give users an assessment of translation quality using 1)
error highlights and 2) LLM explanations, and implicit feedback that helps
users compare MT inputs and outputs through 3) backtranslation and 4)
question-answer (QA) tables. We find that all feedback types, except error
highlights, significantly improve both decision accuracy and appropriate
reliance. Notably, implicit feedback, especially QA tables, yields
significantly greater gains than explicit feedback in terms of decision
accuracy, appropriate reliance, and user perceptions, receiving the highest
ratings for helpfulness and trust, and the lowest for mental burden.

</details>


### [139] [Explainable Depression Detection using Masked Hard Instance Mining](https://arxiv.org/pdf/2505.24609)
*Patawee Prakrankamanant, Shinji Watanabe, Ekapol Chuangsuwanich*

Main category: cs.CL

TL;DR: The paper introduces Masked Hard Instance Mining (MHIM) to improve explainability in text-based depression detection, showing better accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current depression detection models lack explainability, reducing trust in their predictions.

Method: Proposes MHIM, which masks attention weights to force broader feature attention.

Result: MHIM improves prediction accuracy and explainability on Thai and English datasets.

Conclusion: MHIM effectively enhances both performance and transparency in depression detection.

Abstract: This paper addresses the critical need for improved explainability in
text-based depression detection. While offering predictive outcomes, current
solutions often overlook the understanding of model predictions which can
hinder trust in the system. We propose the use of Masked Hard Instance Mining
(MHIM) to enhance the explainability in the depression detection task. MHIM
strategically masks attention weights within the model, compelling it to
distribute attention across a wider range of salient features. We evaluate MHIM
on two datasets representing distinct languages: Thai (Thai-Maywe) and English
(DAIC-WOZ). Our results demonstrate that MHIM significantly improves
performance in terms of both prediction accuracy and explainability metrics.

</details>


### [140] [When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation](https://arxiv.org/pdf/2505.24613)
*Daniela Occhipinti, Marco Guerini, Malvina Nissim*

Main category: cs.CL

TL;DR: The paper explores how dialogue agents align responses with both provided and interlocutor personas, evaluates robustness across topics and interlocutors, and assesses fine-tuning impacts. Results show interlocutor persona improves speaker recognition, while unfamiliar interlocutors pose challenges.


<details>
  <summary>Details</summary>
Motivation: To improve dialogue agent consistency and diversity by exploring alignment with interlocutor personas, robustness, and fine-tuning effects.

Method: Evaluates models using diverse speaker pairings and topics, masking/disclosing interlocutor info, and employs LLM/human evaluations.

Result: Interlocutor persona improves speaker recognition; models generalize well across topics but struggle with unfamiliar interlocutors. Zero-shot LLMs often copy biographical details.

Conclusion: Access to interlocutor persona enhances dialogue generation, but challenges remain with unfamiliar interlocutors and zero-shot settings.

Abstract: Endowing dialogue agents with persona information has proven to significantly
improve the consistency and diversity of their generations. While much focus
has been placed on aligning dialogues with provided personas, the adaptation to
the interlocutor's profile remains largely underexplored. In this work, we
investigate three key aspects: (1) a model's ability to align responses with
both the provided persona and the interlocutor's; (2) its robustness when
dealing with familiar versus unfamiliar interlocutors and topics, and (3) the
impact of additional fine-tuning on specific persona-based dialogues. We
evaluate dialogues generated with diverse speaker pairings and topics, framing
the evaluation as an author identification task and employing both
LLM-as-a-judge and human evaluations. By systematically masking or disclosing
information about the interlocutor, we assess its impact on dialogue
generation. Results show that access to the interlocutor's persona improves the
recognition of the target speaker, while masking it does the opposite. Although
models generalise well across topics, they struggle with unfamiliar
interlocutors. Finally, we found that in zero-shot settings, LLMs often copy
biographical details, facilitating identification but trivialising the task.

</details>


### [141] [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/pdf/2505.24701)
*Tejul Pandit, Meet Raval, Dhvani Upadhyay*

Main category: cs.CL

TL;DR: The paper proposes using LLMs like GPT-4o to generate synthetic ABSA data, addressing dataset scarcity. Evaluated on three models, synthetic data proved effective, with trade-offs in precision, recall, and speed.


<details>
  <summary>Details</summary>
Motivation: Scarcity of diverse, labeled ABSA datasets reflecting real-world nuances limits research and model evaluation.

Method: Generate synthetic ABSA data using GPT-4o, ensuring consistent topic and sentiment distributions. Evaluate quality by testing three LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek-R1) on classification tasks.

Result: Synthetic data was effective: DeepSeek-R1 had higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet showed strong recall, and Gemini 1.5 Pro was faster.

Conclusion: LLM-based synthetic data generation is viable for creating ABSA resources, enabling research without relying on limited real-world data.

Abstract: Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions
but often suffers from the scarcity of diverse, labeled datasets that reflect
real-world conversational nuances. This paper presents an approach for
generating synthetic ABSA data using Large Language Models (LLMs) to address
this gap. We detail the generation process aimed at producing data with
consistent topic and sentiment distributions across multiple domains using
GPT-4o. The quality and utility of the generated data were evaluated by
assessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,
Claude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification
tasks. Our results demonstrate the effectiveness of the synthetic data,
revealing distinct performance trade-offs among the models: DeepSeekR1 showed
higher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,
and Gemini 1.5 Pro offered significantly faster inference. We conclude that
LLM-based synthetic data generation is a viable and flexible method for
creating valuable ABSA resources, facilitating research and model evaluation
without reliance on limited or inaccessible real-world labeled data.

</details>


### [142] [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/pdf/2505.24615)
*Yan Liu, Zonglin Yang, Soujanya Poria, Thanh-Son Nguyen, Erik Cambria*

Main category: cs.CL

TL;DR: The paper proposes using large language models (LLMs) for scientific novelty detection, introduces two new datasets, and presents a method to distill idea-level knowledge from LLMs for efficient retrieval.


<details>
  <summary>Details</summary>
Motivation: The challenge of identifying novel research ideas in academia due to the lack of benchmark datasets and the inadequacy of existing NLP methods for capturing idea conception.

Method: Extracts closure sets of papers, summarizes their main ideas using LLMs, and trains a lightweight retriever by distilling idea-level knowledge from LLMs.

Result: The method outperforms others on the proposed benchmark datasets for idea retrieval and novelty detection tasks.

Conclusion: LLMs can effectively aid in scientific novelty detection, and the proposed method offers a practical solution with new datasets and efficient retrieval.

Abstract: In an era of exponential scientific growth, identifying novel research ideas
is crucial and challenging in academia. Despite potential, the lack of an
appropriate benchmark dataset hinders the research of novelty detection. More
importantly, simply adopting existing NLP technologies, e.g., retrieving and
then cross-checking, is not a one-size-fits-all solution due to the gap between
textual similarity and idea conception. In this paper, we propose to harness
large language models (LLMs) for scientific novelty detection (ND), associated
with two new datasets in marketing and NLP domains. To construct the
considerate datasets for ND, we propose to extract closure sets of papers based
on their relationship, and then summarize their main ideas based on LLMs. To
capture idea conception, we propose to train a lightweight retriever by
distilling the idea-level knowledge from LLMs to align ideas with similar
conception, enabling efficient and accurate idea retrieval for LLM novelty
detection. Experiments show our method consistently outperforms others on the
proposed benchmark datasets for idea retrieval and ND tasks. Codes and data are
available at https://anonymous.4open.science/r/NoveltyDetection-10FB/.

</details>


### [143] [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/pdf/2505.24619)
*Vittorio Torri, Machteld J. Boonstra, Marielle C. van de Veerdonk, Deborah N. Kalkman, Alicia Uijl, Francesca Ieva, Ameen Abu-Hanna, Folkert W. Asselbergs, Iacer Calixto*

Main category: cs.CL

TL;DR: The study evaluates models for phenotyping heart failure patients using structured and unstructured data, finding discharge letters most informative. Aug-Linear models matched black-box performance with better interpretability.


<details>
  <summary>Details</summary>
Motivation: Heart failure patients have diverse phenotypes affecting treatment and prognosis, necessitating accurate and interpretable phenotyping models.

Method: Analyzed HF hospitalizations (2015-2023) from two hospitals, using structured and unstructured data. Models included Transformer-based (BERT) and Aug-Linear, compared with baselines. Interpretability was assessed via clinician annotations and SHAP/LIME explanations.

Result: BERT and Aug-Linear models outperformed baselines (AUC=0.84 and 0.81). Aug-Linear explanations aligned better with clinician annotations than black-box models.

Conclusion: Discharge letters are key for HF phenotyping. Aug-Linear models offer transparent, clinician-aligned interpretability, suitable for clinical decision-making.

Abstract: Objective: Heart failure (HF) patients present with diverse phenotypes
affecting treatment and prognosis. This study evaluates models for phenotyping
HF patients based on left ventricular ejection fraction (LVEF) classes, using
structured and unstructured data, assessing performance and interpretability.
  Materials and Methods: The study analyzes all HF hospitalizations at both
Amsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105
hospitalizations, 16,334 patients). Data from AMC were used for model training,
and from VUmc for external validation. The dataset was unlabelled and included
tabular clinical measurements and discharge letters. Silver labels for LVEF
classes were generated by combining diagnosis codes, echocardiography results,
and textual mentions. Gold labels were manually annotated for 300 patients for
testing. Multiple Transformer-based (black-box) and Aug-Linear (white-box)
models were trained and compared with baselines on structured and unstructured
data. To evaluate interpretability, two clinicians annotated 20 discharge
letters by highlighting information they considered relevant for LVEF
classification. These were compared to SHAP and LIME explanations from
black-box models and the inherent explanations of Aug-Linear models.
  Results: BERT-based and Aug-Linear models, using discharge letters alone,
achieved the highest classification results (AUC=0.84 for BERT, 0.81 for
Aug-Linear on external validation), outperforming baselines. Aug-Linear
explanations aligned more closely with clinicians' explanations than post-hoc
explanations on black-box models.
  Conclusions: Discharge letters emerged as the most informative source for
phenotyping HF patients. Aug-Linear models matched black-box performance while
providing clinician-aligned interpretability, supporting their use in
transparent clinical decision-making.

</details>


### [144] [Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization](https://arxiv.org/pdf/2505.24621)
*Utsav Maskey, Chencheng Zhu, Usman Naseem*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' cryptanalytic capabilities on encrypted texts, introducing a benchmark dataset and testing decryption accuracy in zero-shot and few-shot settings.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored cryptanalytic potential of LLMs in data security and encryption, addressing a gap in current evaluations.

Method: Introduces a benchmark dataset of diverse plain texts and their encrypted versions, assessing LLMs in zero-shot and few-shot settings for decryption accuracy and semantic comprehension.

Result: Reveals strengths and limitations of LLMs in cryptanalysis, highlighting concerns about their susceptibility to jailbreaking attacks.

Conclusion: Emphasizes the dual-use nature of LLMs in security, contributing to AI safety and security discussions.

Abstract: Recent advancements in Large Language Models (LLMs) have transformed natural
language understanding and generation, leading to extensive benchmarking across
diverse tasks. However, cryptanalysis a critical area for data security and
encryption has not yet been thoroughly explored in LLM evaluations. To address
this gap, we evaluate cryptanalytic potential of state of the art LLMs on
encrypted texts generated using a range of cryptographic algorithms. We
introduce a novel benchmark dataset comprising diverse plain texts spanning
various domains, lengths, writing styles, and topics paired with their
encrypted versions. Using zero-shot and few shot settings, we assess multiple
LLMs for decryption accuracy and semantic comprehension across different
encryption schemes. Our findings reveal key insights into the strengths and
limitations of LLMs in side-channel communication while raising concerns about
their susceptibility to jailbreaking attacks. This research highlights the
dual-use nature of LLMs in security contexts and contributes to the ongoing
discussion on AI safety and security.

</details>


### [145] [Disentangling Language and Culture for Evaluating Multilingual Large Language Models](https://arxiv.org/pdf/2505.24635)
*Jiahao Ying, Wei Tang, Yiran Zhao, Yixin Cao, Yu Rong, Wenxuan Zhang*

Main category: cs.CL

TL;DR: A Dual Evaluation Framework assesses LLMs' multilingual capabilities by analyzing linguistic and cultural dimensions, revealing a 'Cultural-Linguistic Synergy' where models perform better in culturally aligned contexts.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that LLMs perform uniformly across languages and emphasize the need for culturally and linguistically aware evaluations.

Method: Decomposes evaluation into linguistic medium and cultural context, conducting extensive tests on various models and using interpretability probing.

Result: Models show better performance in culturally aligned contexts, with specific neuron activation patterns indicating multilingual capability.

Conclusion: The framework highlights the importance of cultural and linguistic alignment in LLM evaluations and suggests neuron activation as a training indicator.

Abstract: This paper introduces a Dual Evaluation Framework to comprehensively assess
the multilingual capabilities of LLMs. By decomposing the evaluation along the
dimensions of linguistic medium and cultural context, this framework enables a
nuanced analysis of LLMs' ability to process questions within both native and
cross-cultural contexts cross-lingually. Extensive evaluations are conducted on
a wide range of models, revealing a notable "CulturalLinguistic Synergy"
phenomenon, where models exhibit better performance when questions are
culturally aligned with the language. This phenomenon is further explored
through interpretability probing, which shows that a higher proportion of
specific neurons are activated in a language's cultural context. This
activation proportion could serve as a potential indicator for evaluating
multilingual performance during model training. Our findings challenge the
prevailing notion that LLMs, primarily trained on English data, perform
uniformly across languages and highlight the necessity of culturally and
linguistically model evaluations. Our code can be found at
https://yingjiahao14. github.io/Dual-Evaluation/.

</details>


### [146] [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/pdf/2505.24754)
*Yingchaojie Feng, Yiqun Sun, Yandong Sun, Minfeng Zhu, Qiang Huang, Anthony K. H. Tung, Wei Chen*

Main category: cs.CL

TL;DR: GSTransform is a lightweight framework for instruction-following text embeddings, avoiding re-encoding by adapting pre-computed embeddings in real time, achieving 6~300x speedup and improved quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for instruction-following text embeddings are computationally expensive, requiring re-encoding for each new instruction.

Method: GSTransform uses Guided Space Transformation to adapt pre-computed embeddings in real time, guided by annotated text data.

Result: GSTransform outperforms state-of-the-art methods in quality and achieves 6~300x speedup on large datasets.

Conclusion: GSTransform offers an efficient and effective solution for dynamic text embeddings, balancing performance and computational cost.

Abstract: In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.

</details>


### [147] [Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching](https://arxiv.org/pdf/2505.24643)
*Juan Wisznia, Cecilia Bolaños, Juan Tollo, Giovanni Marraffini, Agustín Gianolini, Noe Hsueh, Luciano Del Corro*

Main category: cs.CL

TL;DR: A new framework for analyzing sorting algorithms in pairwise ranking prompting (PRP) shifts focus from traditional comparisons to LLM inference costs, revealing inefficiencies in classical metrics and suggesting optimizations like batching and caching.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for sorting algorithms rely on pairwise comparisons, but LLM inference costs can dominate efficiency, necessitating a revised cost model.

Method: The framework re-centers the cost model around LLM inferences, analyzing algorithms under this new paradigm and suggesting optimizations like batching and caching.

Result: Classically optimal algorithms may lose efficiency when LLM inference costs dominate, highlighting the need for new strategies.

Conclusion: The framework underscores the importance of adapting sorting algorithm analysis to account for LLM inference costs, proposing practical optimizations for improved efficiency.

Abstract: We introduce a novel framework for analyzing sorting algorithms in pairwise
ranking prompting (PRP), re-centering the cost model around LLM inferences
rather than traditional pairwise comparisons. While classical metrics based on
comparison counts have traditionally been used to gauge efficiency, our
analysis reveals that expensive LLM inferences overturn these predictions;
accordingly, our framework encourages strategies such as batching and caching
to mitigate inference costs. We show that algorithms optimal in the classical
setting can lose efficiency when LLM inferences dominate the cost under certain
optimizations.

</details>


### [148] [PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder](https://arxiv.org/pdf/2505.24646)
*Yiqun Sun, Qiang Huang, Anthony K. H. Tung, Jun Yu*

Main category: cs.CL

TL;DR: PRISM is a framework for creating interpretable political bias embeddings, outperforming existing models in bias classification and offering enhanced interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing text embedding models lack the ability to capture ideological nuances, limiting their effectiveness in tasks requiring political bias understanding.

Method: PRISM involves two stages: (1) mining bias indicators from weakly labeled news data, and (2) assigning structured bias scores to articles based on these indicators.

Result: PRISM outperforms state-of-the-art models in political bias classification and provides interpretable representations for ideological analysis.

Conclusion: PRISM addresses the gap in capturing political bias in text embeddings, offering improved performance and interpretability.

Abstract: Semantic Text Embedding is a fundamental NLP task that encodes textual
content into vector representations, where proximity in the embedding space
reflects semantic similarity. While existing embedding models excel at
capturing general meaning, they often overlook ideological nuances, limiting
their effectiveness in tasks that require an understanding of political bias.
To address this gap, we introduce PRISM, the first framework designed to
Produce inteRpretable polItical biaS eMbeddings. PRISM operates in two key
stages: (1) Controversial Topic Bias Indicator Mining, which systematically
extracts fine-grained political topics and their corresponding bias indicators
from weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,
which assigns structured bias scores to news articles based on their alignment
with these indicators. This approach ensures that embeddings are explicitly
tied to bias-revealing dimensions, enhancing both interpretability and
predictive power. Through extensive experiments on two large-scale datasets, we
demonstrate that PRISM outperforms state-of-the-art text embedding models in
political bias classification while offering highly interpretable
representations that facilitate diversified retrieval and ideological analysis.
The source code is available at https://github.com/dukesun99/ACL-PRISM.

</details>


### [149] [TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis](https://arxiv.org/pdf/2505.24672)
*Xiaorui Wu, Xiaofeng Mao, Fei Li, Xin Zhang, Xuanhong Li, Chong Teng, Donghong Ji, Zhuang Li*

Main category: cs.CL

TL;DR: The paper introduces TRIDENT, a framework and dataset to improve LLM safety alignment by addressing gaps in risk coverage across lexical diversity, malicious intent, and jailbreak tactics.


<details>
  <summary>Details</summary>
Motivation: Existing safety alignment datasets lack comprehensive risk coverage, focusing mainly on lexical diversity.

Method: Proposes TRIDENT, an automated pipeline using persona-based, zero-shot LLM generation to create diverse harmful instructions paired with ethical responses.

Result: Fine-tuning on TRIDENT-Edge reduces Harm Score by 14.29% and Attack Success Rate by 20% compared to baselines.

Conclusion: TRIDENT enhances LLM safety alignment by systematically addressing risk dimensions, outperforming existing datasets.

Abstract: Large Language Models (LLMs) excel in various natural language processing
tasks but remain vulnerable to generating harmful content or being exploited
for malicious purposes. Although safety alignment datasets have been introduced
to mitigate such risks through supervised fine-tuning (SFT), these datasets
often lack comprehensive risk coverage. Most existing datasets focus primarily
on lexical diversity while neglecting other critical dimensions. To address
this limitation, we propose a novel analysis framework to systematically
measure the risk coverage of alignment datasets across three essential
dimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We
further introduce TRIDENT, an automated pipeline that leverages persona-based,
zero-shot LLM generation to produce diverse and comprehensive instructions
spanning these dimensions. Each harmful instruction is paired with an ethically
aligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311
examples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on
TRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%
reduction in Harm Score, and a 20% decrease in Attack Success Rate compared to
the best-performing baseline model fine-tuned on the WildBreak dataset.

</details>


### [150] [A Simple Linear Patch Revives Layer-Pruned Large Language Models](https://arxiv.org/pdf/2505.24680)
*Xinrui Chen, Haoli Bai, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan*

Main category: cs.CL

TL;DR: LinearPatch is a plug-and-play technique to revive layer-pruned LLMs by addressing activation magnitude mismatches, achieving up to 94.15% performance retention with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing layer pruning methods cause significant performance drops due to activation magnitude mismatches at pruning interfaces.

Method: Uses Hadamard transformation to suppress outliers and channel-wise scaling to align activations, fused into a single patch matrix.

Result: Retains 94.15% performance when pruning 5 layers of LLaMA-3-8B, surpassing SOTA by 4%. Further optimization boosts performance to 95.16%.

Conclusion: LinearPatch effectively mitigates performance degradation in pruned LLMs with negligible overhead, outperforming existing methods.

Abstract: Layer pruning has become a popular technique for compressing large language
models (LLMs) due to its simplicity. However, existing layer pruning methods
often suffer from significant performance drops. We identify that this
degradation stems from the mismatch of activation magnitudes across layers and
tokens at the pruning interface. To address this, we propose LinearPatch, a
simple plug-and-play technique to revive the layer-pruned LLMs. The proposed
method adopts Hadamard transformation to suppress massive outliers in
particular tokens, and channel-wise scaling to align the activation magnitudes.
These operations can be fused into a single matrix, which functions as a patch
to bridge the pruning interface with negligible inference overhead. LinearPatch
retains up to 94.15% performance of the original model when pruning 5 layers of
LLaMA-3-8B on the question answering benchmark, surpassing existing
state-of-the-art methods by 4%. In addition, the patch matrix can be further
optimized with memory efficient offline knowledge distillation. With only 5K
samples, the retained performance of LinearPatch can be further boosted to
95.16% within 30 minutes on a single computing card.

</details>


### [151] [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/pdf/2505.24688)
*Qinglin Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, Lin Gui*

Main category: cs.CL

TL;DR: Soft Reasoning improves LLM reasoning by embedding-based search, combining perturbation and Bayesian optimization for better accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex reasoning due to limited diversity and inefficient search.

Method: Proposes Soft Reasoning: embedding perturbation for exploration and Bayesian optimization for refinement, guided by a verifier.

Result: Improves reasoning accuracy and coherence with minimal computation, scalable and model-agnostic.

Conclusion: Soft Reasoning offers a superior, efficient solution for LLM reasoning without heuristic search reliance.

Abstract: Large Language Models (LLMs) struggle with complex reasoning due to limited
diversity and inefficient search. We propose Soft Reasoning, an embedding-based
search framework that optimises the embedding of the first token to guide
generation. It combines (1) embedding perturbation for controlled exploration
and (2) Bayesian optimisation to refine embeddings via a verifier-guided
objective, balancing exploration and exploitation. This approach improves
reasoning accuracy and coherence while avoiding reliance on heuristic search.
Experiments demonstrate superior correctness with minimal computation, making
it a scalable, model-agnostic solution.

</details>


### [152] [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/pdf/2505.24689)
*Sander Land, Catherine Arnett*

Main category: cs.CL

TL;DR: SCRIPT-BPE introduces a Unicode-based pretokenization method to address BPE tokenizer challenges in multilingual settings, improving robustness and fairness for non-Latin scripts.


<details>
  <summary>Details</summary>
Motivation: BPE tokenizers struggle with non-Western scripts and fragile pretokenization methods, leading to unfair penalties and edge cases.

Method: SCRIPT uses Unicode script and category properties for pretokenization, combined with a constrained BPE merging strategy to ensure character integrity.

Result: SCRIPT-BPE achieves competitive compression and eliminates encoding penalties for non-Latin scripts.

Conclusion: SCRIPT-BPE offers a robust, fair alternative to traditional BPE tokenizers in multilingual contexts.

Abstract: Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,
face challenges in multilingual settings, including penalization of non-Western
scripts and the creation of tokens with partial UTF-8 sequences.
Pretokenization, often reliant on complex regular expressions, can also
introduce fragility and unexpected edge cases. We propose SCRIPT (Script
Category Representation in PreTokenization), a novel encoding scheme that
bypasses UTF-8 byte conversion by using initial tokens based on Unicode script
and category properties. This approach enables a simple, rule-based
pretokenization strategy that respects script boundaries, offering a robust
alternative to pretokenization strategies based on regular expressions. We also
introduce and validate a constrained BPE merging strategy that enforces
character integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our
experiments demonstrate that SCRIPT-BPE achieves competitive compression while
eliminating encoding-based penalties for non-Latin-script languages.

</details>


### [153] [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/pdf/2505.24788)
*Houjun Liu, John Bauer, Christopher D. Manning*

Main category: cs.CL

TL;DR: Dropout, once a key regularization method, is found to degrade performance in single-epoch pretraining of large language models (LLMs), advocating its omission.


<details>
  <summary>Details</summary>
Motivation: Despite dropout's historical success in reducing overfitting, its role in single-epoch pretraining for modern LLMs lacks thorough empirical investigation.

Method: Experiments with varying dropout levels in single-epoch pretraining of masked (BERT) and autoregressive (Pythia) LMs, evaluating downstream tasks like language modeling, BLiMP, SQuAD, and MNLI.

Result: Performance improves without dropout; early dropout also underperforms. Models without dropout excel in gradient-based editing (MEND) and match in representation-based editing (ReFT).

Conclusion: Dropout should be omitted in single-epoch pretraining of LLMs for better downstream performance and editability.

Abstract: Originally, dropout was seen as a breakthrough regularization technique that
reduced overfitting and improved performance in almost all applications of deep
learning by reducing overfitting. Yet, single-epoch pretraining tasks common to
modern LLMs yield minimal overfitting, leading to dropout not being used for
large LLMs. Nevertheless, no thorough empirical investigation has been done on
the role of dropout in LM pretraining. Through experiments in single-epoch
pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs
with varying levels of dropout, we find that downstream performance in language
modeling, morpho-syntax (BLiMP), question answering (SQuAD), and
natural-language inference (MNLI) improves when dropout is not applied during
pretraining. We additionally find that the recently-introduced "early dropout"
also degrades performance over applying no dropout at all. We further
investigate the models' editability, and find that models trained without
dropout are more successful in gradient-based model editing (MEND) and
equivalent in representation-based model editing (ReFT). Therefore, we advocate
to drop dropout during single-epoch pretraining.

</details>


### [154] [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/pdf/2505.24712)
*Guido Ivetta, Marcos J. Gomez, Sofía Martinelli, Pietro Palombini, M. Emilia Echeveste, Nair Carolina Mazzeo, Beatriz Busaniche, Luciana Benotti*

Main category: cs.CL

TL;DR: HESEIA is a dataset of 46,499 sentences co-designed by Latin-American educators and students to evaluate intersectional biases in LLMs, outperforming existing benchmarks in capturing unrecognized stereotypes.


<details>
  <summary>Details</summary>
Motivation: Existing bias evaluation resources lack co-design with affected communities and participatory approaches, limiting their relevance and effectiveness.

Method: Created through a professional development course involving 370 teachers and 5,370 students, HESEIA uses minimal pairs to capture intersectional biases across demographics and school subjects.

Result: HESEIA demonstrates greater diversity and captures more unrecognized stereotypes by LLMs compared to previous datasets.

Conclusion: HESEIA provides a community-grounded resource for bias assessment in LLMs, reflecting local contexts and educator expertise.

Abstract: Most resources for evaluating social biases in Large Language Models are
developed without co-design from the communities affected by these biases, and
rarely involve participatory approaches. We introduce HESEIA, a dataset of
46,499 sentences created in a professional development course. The course
involved 370 high-school teachers and 5,370 students from 189 Latin-American
schools. Unlike existing benchmarks, HESEIA captures intersectional biases
across multiple demographic axes and school subjects. It reflects local
contexts through the lived experience and pedagogical expertise of educators.
Teachers used minimal pairs to create sentences that express stereotypes
relevant to their school subjects and communities. We show the dataset
diversity in term of demographic axes represented and also in terms of the
knowledge areas included. We demonstrate that the dataset contains more
stereotypes unrecognized by current LLMs than previous datasets. HESEIA is
available to support bias assessments grounded in educational communities.

</details>


### [155] [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/pdf/2505.24714)
*Junyu Luo, Zhizhuo Kou, Liming Yang, Xiao Luo, Jinsheng Huang, Zhiping Xiao, Jingshu Peng, Chengzhong Liu, Jiaming Ji, Xuanzhe Liu, Sirui Han, Ming Zhang, Yike Guo*

Main category: cs.CL

TL;DR: FinMME is a specialized multimodal evaluation dataset for financial MLLMs, featuring 11,000+ samples across 18 domains and 6 asset classes, with robust evaluation via FinScore.


<details>
  <summary>Details</summary>
Motivation: Address the lack of effective multimodal evaluation datasets in finance to advance MLLM development in this domain.

Method: Curate FinMME with high-quality samples, validate data with 20 annotators, and develop FinScore for unbiased evaluation.

Result: State-of-the-art models like GPT-4o perform poorly on FinMME, showing its challenge; dataset robustness is high (<1% variation).

Conclusion: FinMME is a reliable, challenging benchmark for financial MLLMs, with publicly available data and evaluation protocol.

Abstract: Multimodal Large Language Models (MLLMs) have experienced rapid development
in recent years. However, in the financial domain, there is a notable lack of
effective and specialized multimodal evaluation datasets. To advance the
development of MLLMs in the finance domain, we introduce FinMME, encompassing
more than 11,000 high-quality financial research samples across 18 financial
domains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We
ensure data quality through 20 annotators and carefully designed validation
mechanisms. Additionally, we develop FinScore, an evaluation system
incorporating hallucination penalties and multi-dimensional capability
assessment to provide an unbiased evaluation. Extensive experimental results
demonstrate that even state-of-the-art models like GPT-4o exhibit
unsatisfactory performance on FinMME, highlighting its challenging nature. The
benchmark exhibits high robustness with prediction variations under different
prompts remaining below 1%, demonstrating superior reliability compared to
existing datasets. Our dataset and evaluation protocol are available at
https://huggingface.co/datasets/luojunyu/FinMME and
https://github.com/luo-junyu/FinMME.

</details>


### [156] [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/pdf/2505.24726)
*Shelly Bensal, Umar Jamil, Christopher Bryant, Melisa Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, Waseem AlShikh*

Main category: cs.CL

TL;DR: A method using self-reflection and reinforcement learning improves large language models' performance, even with limited feedback, achieving up to 34.7% gains in tasks like math and function calling.


<details>
  <summary>Details</summary>
Motivation: To enhance language models' performance on complex tasks when synthetic data generation is infeasible and only binary feedback is available.

Method: A two-stage framework: (1) model generates self-reflective commentary after failing a task, (2) retries the task with the reflection in context, rewarding successful attempts.

Result: Substantial performance gains (e.g., 34.7% in math, 18.1% in function calling), with smaller models outperforming larger ones.

Conclusion: The framework offers a promising way to create more reliable, self-improving language models with limited feedback.

Abstract: We explore a method for improving the performance of large language models
through self-reflection and reinforcement learning. By incentivizing the model
to generate better self-reflections when it answers incorrectly, we demonstrate
that a model's ability to solve complex, verifiable tasks can be enhanced even
when generating synthetic data is infeasible and only binary feedback is
available. Our framework operates in two stages: first, upon failing a given
task, the model generates a self-reflective commentary analyzing its previous
attempt; second, the model is given another attempt at the task with the
self-reflection in context. If the subsequent attempt succeeds, the tokens
generated during the self-reflection phase are rewarded. Our experimental
results show substantial performance gains across a variety of model
architectures, as high as 34.7% improvement at math equation writing and 18.1%
improvement at function calling. Notably, smaller fine-tuned models (1.5
billion to 7 billion parameters) outperform models in the same family that are
10 times larger. Our novel paradigm is thus an exciting pathway to more useful
and reliable language models that can self-improve on challenging tasks with
limited external feedback.

</details>


### [157] [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/pdf/2505.24731)
*Alan Sun*

Main category: cs.CL

TL;DR: The paper introduces 'circuit stability' as a new metric to evaluate language models by assessing their consistent reasoning across inputs, addressing challenges in benchmark saturation and dataset creation.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for language models face issues like benchmark saturation and labor-intensive dataset creation. The paper aims to provide a more rigorous and interpretable way to assess model performance.

Method: The authors formalize circuit stability and equivalence mathematically and validate their approach through three case studies.

Result: Empirical results demonstrate that circuit stability can characterize and predict aspects of model generalization.

Conclusion: The proposed methods advance the understanding of model generality and interpretability, offering a new evaluation framework.

Abstract: Extensively evaluating the capabilities of (large) language models is
difficult. Rapid development of state-of-the-art models induce benchmark
saturation, while creating more challenging datasets is labor-intensive.
Inspired by the recent developments in mechanistic interpretability, we
introduce circuit stability as a new way to assess model performance. Circuit
stability refers to a model's ability to apply a consistent reasoning
process-its circuit-across various inputs. We mathematically formalize circuit
stability and circuit equivalence. Then, through three case studies, we
empirically show that circuit stability and the lack thereof can characterize
and predict different aspects of generalization. Our proposed methods offer a
step towards rigorously relating the generality of models to their
interpretability.

</details>


### [158] [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/pdf/2505.24830)
*Juraj Vladika, Annika Domres, Mai Nguyen, Rebecca Moser, Jana Nano, Felix Busch, Lisa C. Adams, Keno K. Bressem, Denise Bernhardt, Stephanie E. Combs, Kai J. Borm, Florian Matthes, Jan C. Peeken*

Main category: cs.CL

TL;DR: A novel atomic fact-checking framework improves LLM reliability in medical Q&A by decomposing responses into verifiable atomic facts, enhancing accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Address hallucinations and inaccurate citations in LLMs to improve clinical adoption and regulatory compliance.

Method: Decompose LLM responses into atomic facts, verify each against medical guidelines, and trace sources for corrections.

Result: 40% overall answer improvement, 50% hallucination detection rate, and enhanced explainability.

Conclusion: The framework advances trustworthy clinical LLM applications by improving accuracy and transparency.

Abstract: Large language models (LLMs) exhibit extensive medical knowledge but are
prone to hallucinations and inaccurate citations, which pose a challenge to
their clinical adoption and regulatory compliance. Current methods, such as
Retrieval Augmented Generation, partially address these issues by grounding
answers in source documents, but hallucinations and low fact-level
explainability persist. In this work, we introduce a novel atomic fact-checking
framework designed to enhance the reliability and explainability of LLMs used
in medical long-form question answering. This method decomposes LLM-generated
responses into discrete, verifiable units called atomic facts, each of which is
independently verified against an authoritative knowledge base of medical
guidelines. This approach enables targeted correction of errors and direct
tracing to source literature, thereby improving the factual accuracy and
explainability of medical Q&A. Extensive evaluation using multi-reader
assessments by medical experts and an automated open Q&A benchmark demonstrated
significant improvements in factual accuracy and explainability. Our framework
achieved up to a 40% overall answer improvement and a 50% hallucination
detection rate. The ability to trace each atomic fact back to the most relevant
chunks from the database provides a granular, transparent explanation of the
generated responses, addressing a major gap in current medical AI applications.
This work represents a crucial step towards more trustworthy and reliable
clinical applications of LLMs, addressing key prerequisites for clinical
application and fostering greater confidence in AI-assisted healthcare.

</details>


### [159] [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/pdf/2505.24757)
*Christian Jaumann, Andreas Wiedholz, Annemarie Friedrich*

Main category: cs.CL

TL;DR: LGAR, a zero-shot LLM-based abstract ranker, outperforms QA-based methods by 5-10 pp. in mean average precision for systematic literature reviews.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of scientific literature makes tracking state-of-the-art challenging; existing abstract screening methods lack comprehensive benchmarks and suffer from error propagation.

Method: Proposes LGAR, combining an LLM-based graded relevance scorer and a dense re-ranker, evaluated on manually extracted criteria from 57 SLRs.

Result: LGAR outperforms QA-based methods by 5-10 pp. in mean average precision.

Conclusion: LGAR provides a more effective approach for abstract screening in systematic literature reviews, with publicly available code and data.

Abstract: The scientific literature is growing rapidly, making it hard to keep track of
the state-of-the-art. Systematic literature reviews (SLRs) aim to identify and
evaluate all relevant papers on a topic. After retrieving a set of candidate
papers, the abstract screening phase determines initial relevance. To date,
abstract screening methods using large language models (LLMs) focus on binary
classification settings; existing question answering (QA) based ranking
approaches suffer from error propagation. LLMs offer a unique opportunity to
evaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks
do not provide them exhaustively. We manually extract these criteria as well as
research questions for 57 SLRs, mostly in the medical domain, enabling
principled comparisons between approaches. Moreover, we propose LGAR, a
zero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance
scorer and a dense re-ranker. Our extensive experiments show that LGAR
outperforms existing QA-based methods by 5-10 pp. in mean average precision.
Our code and data is publicly available.

</details>


### [160] [From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning](https://arxiv.org/pdf/2505.24768)
*Haoyu Li, Xuhong Li, Yiming Dong, Kun Liu*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy of diversity-control strategies in SFT datasets for LLMs, analyzing macro-, meso-, and microscopic levels, and finds microscopic response diversity most impactful on model performance.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored systematic analysis of dataset diversity in SFT for LLMs.

Method: Proposes a taxonomy of diversity-control strategies, constructs datasets with these strategies, and evaluates their impact on LLM performance.

Result: Microscopic diversity in responses shows the strongest correlation with model performance and outperforms other strategies.

Conclusion: Microscopic response diversity is key for high-performance SFT datasets, offering practical insights for dataset construction.

Abstract: Dataset diversity plays a pivotal role for the successful training of many
machine learning models, particularly in the supervised fine-tuning (SFT) stage
of large language model (LLM) development. Despite increasing recognition of
its importance, systematic analyses of dataset diversity still remain
underexplored. To address this gap, this work presents a systematic taxonomy of
existing diversity-control strategies, which primarily focus on the instruction
component, operating at either macroscopic (entire instruction semantics) or
mesoscopic levels (instruction units), and furthermore introduces a novel
analysis of microscopic diversity within the response component, specifically
analyzing the statistical distribution of tokens in SFT training samples. In
the experimental evaluation, we construct fixed-size datasets (e.g., 10,000
samples each) from a corpus of 117,000 open-source SFT samples, incorporating
six distinct diversity-control strategies spanning macro-, meso-, and
microscopic levels applied to both instructions and responses. We then
fine-tune LLMs on these datasets to assess the six diversity-control
strategies. Results reveal that while macroscopic and mesoscopic strategies
lead to higher performance with increasing diversity, the microscopic strategy
in responses exhibits both a stronger correlation between model performance and
the degree of diversity and superior performance with maximum diversity across
all strategies. These findings offer actionable insights for constructing
high-performance SFT datasets.

</details>


### [161] [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/pdf/2505.24778)
*Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs use epistemic markers (e.g., "fairly confident") to reflect intrinsic confidence consistently, finding inconsistencies in out-of-distribution settings.


<details>
  <summary>Details</summary>
Motivation: Accurately assessing LLM confidence is crucial for high-stakes applications, but it's unclear if epistemic markers reliably reflect model uncertainty.

Method: Defines marker confidence as observed accuracy when models use epistemic markers, evaluated across question-answering datasets in in-distribution and out-of-distribution settings.

Result: Markers generalize well in-distribution but show inconsistent confidence out-of-distribution, raising reliability concerns.

Conclusion: Improved alignment between marker-based confidence and actual model uncertainty is needed for reliable confidence estimation.

Abstract: As large language models (LLMs) are increasingly used in high-stakes domains,
accurately assessing their confidence is crucial. Humans typically express
confidence through epistemic markers (e.g., "fairly confident") instead of
numerical values. However, it remains unclear whether LLMs consistently use
these markers to reflect their intrinsic confidence due to the difficulty of
quantifying uncertainty associated with various markers. To address this gap,
we first define marker confidence as the observed accuracy when a model employs
an epistemic marker. We evaluate its stability across multiple
question-answering datasets in both in-distribution and out-of-distribution
settings for open-source and proprietary LLMs. Our results show that while
markers generalize well within the same distribution, their confidence is
inconsistent in out-of-distribution scenarios. These findings raise significant
concerns about the reliability of epistemic markers for confidence estimation,
underscoring the need for improved alignment between marker based confidence
and actual model uncertainty. Our code is available at
https://github.com/HKUST-KnowComp/MarCon.

</details>


### [162] [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/pdf/2505.24864)
*Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong*

Main category: cs.CL

TL;DR: Prolonged RL training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models in pass@k evaluations.


<details>
  <summary>Details</summary>
Motivation: To determine if RL meaningfully expands reasoning capabilities or just amplifies existing outputs, and whether scaling RL compute improves reasoning.

Method: Introduces ProRL with KL divergence control, reference policy resetting, and diverse tasks.

Result: RL-trained models outperform base models, even where base models fail, and reasoning improvements correlate with base model competence and training duration.

Conclusion: RL can explore new solution spaces, expanding reasoning boundaries in language models, with implications for future long-horizon RL research.

Abstract: Recent advances in reasoning-centric language models have highlighted
reinforcement learning (RL) as a promising method for aligning models with
verifiable rewards. However, it remains contentious whether RL truly expands a
model's reasoning capabilities or merely amplifies high-reward outputs already
latent in the base model's distribution, and whether continually scaling up RL
compute reliably leads to improved reasoning performance. In this work, we
challenge prevailing assumptions by demonstrating that prolonged RL (ProRL)
training can uncover novel reasoning strategies that are inaccessible to base
models, even under extensive sampling. We introduce ProRL, a novel training
methodology that incorporates KL divergence control, reference policy
resetting, and a diverse suite of tasks. Our empirical analysis reveals that
RL-trained models consistently outperform base models across a wide range of
pass@k evaluations, including scenarios where base models fail entirely
regardless of the number of attempts. We further show that reasoning boundary
improvements correlates strongly with task competence of base model and
training duration, suggesting that RL can explore and populate new regions of
solution space over time. These findings offer new insights into the conditions
under which RL meaningfully expands reasoning boundaries in language models and
establish a foundation for future work on long-horizon RL for reasoning. We
release model weights to support further research:
https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B

</details>


### [163] [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/pdf/2505.24803)
*Zhijun Pan, Antonios Andronis, Eva Hayek, Oscar AP Wilkinson, Ilya Lasy, Annette Parry, Guy Gadney, Tim J. Smith, Mick Grierson*

Main category: cs.CL

TL;DR: The paper explores using knowledge graphs (KGs) to improve LLM-based storytelling, enhancing coherence and user control. A KG-assisted pipeline was tested, showing better narrative quality and user engagement.


<details>
  <summary>Details</summary>
Motivation: Address challenges in LLM-based storytelling, such as coherence and user control, by leveraging structured data (KGs) to enhance narrative quality and interactivity.

Method: Proposed a KG-assisted storytelling pipeline, evaluated through a user study with 15 participants creating and editing stories via KGs.

Result: KGs improved story quality, especially in structured narratives, and increased user control and engagement.

Conclusion: KGs effectively enhance LLM storytelling by improving coherence and user interactivity, making the process more engaging.

Abstract: Large Language Models (LLMs) have shown great potential in automated story
generation, but challenges remain in maintaining long-form coherence and
providing users with intuitive and effective control. Retrieval-Augmented
Generation (RAG) has proven effective in reducing hallucinations in text
generation; however, the use of structured data to support generative
storytelling remains underexplored. This paper investigates how knowledge
graphs (KGs) can enhance LLM-based storytelling by improving narrative quality
and enabling user-driven modifications. We propose a KG-assisted storytelling
pipeline and evaluate its effectiveness through a user study with 15
participants. Participants created their own story prompts, generated stories,
and edited knowledge graphs to shape their narratives. Through quantitative and
qualitative analysis, our findings demonstrate that knowledge graphs
significantly enhance story quality in action-oriented and structured
narratives within our system settings. Additionally, editing the knowledge
graph increases users' sense of control, making storytelling more engaging,
interactive, and playful.

</details>


### [164] [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/pdf/2505.24826)
*Li yunhan, Wu gengshen*

Main category: cs.CL

TL;DR: The paper addresses the gap in evaluating linguistic quality (clarity, coherence, terminology) of legal LLMs by proposing a regression model, specialized legal questions, and analyzing 49 LLMs. Key findings include parameter efficiency, negligible impact of engineering choices, and reasoning models outperforming base architectures.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for legal LLMs focus on factual accuracy but overlook linguistic quality, prompting the need for a comprehensive evaluation framework.

Method: Developed a regression model for linguistic quality, created specialized legal questions, and analyzed 49 LLMs using this framework.

Result: Found that model quality plateaus at 14B parameters, engineering choices have minimal impact, and reasoning models perform best. Released rankings and Pareto analysis.

Conclusion: Established standardized evaluation for legal LLMs, identified training data limitations, and highlighted the Qwen3 series as optimal for cost-performance.

Abstract: As large language models (LLMs) are increasingly used in legal applications,
current evaluation benchmarks tend to focus mainly on factual accuracy while
largely neglecting important linguistic quality aspects such as clarity,
coherence, and terminology. To address this gap, we propose three steps: First,
we develop a regression model to evaluate the quality of legal texts based on
clarity, coherence, and terminology. Second, we create a specialized set of
legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off
at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at
72 billion parameters. Second, engineering choices such as quantization and
context length have a negligible impact, as indicated by statistical
significance thresholds above 0.016. Third, reasoning models consistently
outperform base architectures. A significant outcome of our research is the
release of a ranking list and Pareto analysis, which highlight the Qwen3 series
as the optimal choice for cost-performance tradeoffs. This work not only
establishes standardized evaluation protocols for legal LLMs but also uncovers
fundamental limitations in current training data refinement approaches. Code
and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

</details>


### [165] [How much do language models memorize?](https://arxiv.org/pdf/2505.24832)
*John X. Morris, Chawin Sitawarin, Chuan Guo, Narine Kokhlikyan, G. Edward Suh, Alexander M. Rush, Kamalika Chaudhuri, Saeed Mahloujifar*

Main category: cs.CL

TL;DR: The paper introduces a method to measure model capacity by distinguishing memorization from generalization, estimating GPT-style models at 3.6 bits per parameter.


<details>
  <summary>Details</summary>
Motivation: To disentangle memorization from generalization in language models and quantify model capacity.

Method: Separates memorization into unintended memorization and generalization, measures total memorization by eliminating generalization, and trains models of varying sizes.

Result: Estimates GPT-style models have a capacity of 3.6 bits per parameter; memorization decreases as generalization begins ('grokking').

Conclusion: Provides scaling laws for model capacity and data size, showing memorization fills capacity before generalization occurs.

Abstract: We propose a new method for estimating how much a model ``knows'' about a
datapoint and use it to measure the capacity of modern language models. Prior
studies of language model memorization have struggled to disentangle
memorization from generalization. We formally separate memorization into two
components: \textit{unintended memorization}, the information a model contains
about a specific dataset, and \textit{generalization}, the information a model
contains about the true data-generation process. When we completely eliminate
generalization, we can compute the total memorization, which provides an
estimate of model capacity: our measurements estimate that GPT-style models
have a capacity of approximately 3.6 bits per parameter. We train language
models on datasets of increasing size and observe that models memorize until
their capacity fills, at which point ``grokking'' begins, and unintended
memorization decreases as models begin to generalize. We train hundreds of
transformer language models ranging from $500K$ to $1.5B$ parameters and
produce a series of scaling laws relating model capacity and data size to
membership inference.

</details>


### [166] [Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks](https://arxiv.org/pdf/2505.24834)
*Roksana Goworek, Haim Dubossarsky*

Main category: cs.CL

TL;DR: Multilingual training isn't inherently better for cross-lingual transfer in sense-aware tasks; confounding factors like data composition and evaluation artifacts explain perceived benefits.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that increased multilinguality benefits cross-lingual transfer in sense-aware tasks (polysemy disambiguation and lexical semantic change).

Method: Large-scale analysis across 28 languages to evaluate the impact of multilingual training on transfer performance.

Result: Multilingual training is neither necessary nor beneficial for effective transfer; confounding factors like fine-tuning data and evaluation artifacts explain perceived advantages.

Conclusion: Rigorous evaluations are needed in multilingual NLP. Findings support low-resource and typologically diverse languages, with released models and benchmarks for further research.

Abstract: Cross-lingual transfer allows models to perform tasks in languages unseen
during training and is often assumed to benefit from increased multilinguality.
In this work, we challenge this assumption in the context of two underexplored,
sense-aware tasks: polysemy disambiguation and lexical semantic change. Through
a large-scale analysis across 28 languages, we show that multilingual training
is neither necessary nor inherently beneficial for effective transfer. Instead,
we find that confounding factors - such as fine-tuning data composition and
evaluation artifacts - better account for the perceived advantages of
multilinguality. Our findings call for more rigorous evaluations in
multilingual NLP. We release fine-tuned models and benchmarks to support
further research, with implications extending to low-resource and typologically
diverse languages.

</details>


### [167] [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/pdf/2505.24858)
*Gabrielle Kaili-May Liu, Gal Yona, Avi Caciularu, Idan Szpektor, Tim G. J. Rudner, Arman Cohan*

Main category: cs.CL

TL;DR: The paper studies LLMs' unreliable uncertainty communication, introduces MetaFaith for better calibration, and shows significant improvements in faithfulness.


<details>
  <summary>Details</summary>
Motivation: LLMs often assert false claims confidently, undermining trust. The study aims to improve their uncertainty communication.

Method: Benchmarked LLMs' uncertainty expression, tested interventions, and introduced MetaFaith, a prompt-based calibration method inspired by human metacognition.

Result: LLMs largely fail at faithful calibration; MetaFaith improves faithfulness by up to 61% and wins 83% human-judged cases.

Conclusion: MetaFaith effectively addresses LLMs' calibration gaps, enhancing trustworthy uncertainty communication.

Abstract: A critical component in the trustworthiness of LLMs is reliable uncertainty
communication, yet LLMs often use assertive language when conveying false
claims, leading to over-reliance and eroded trust. We present the first
systematic study of $\textit{faithful confidence calibration}$ of LLMs,
benchmarking models' ability to use linguistic expressions of uncertainty that
$\textit{faithfully reflect}$ their intrinsic uncertainty, across a
comprehensive array of models, datasets, and prompting strategies. Our results
demonstrate that LLMs largely fail at this task, and that existing
interventions are insufficient: standard prompt approaches provide only
marginal gains, and existing, factuality-based calibration techniques can even
harm faithful calibration. To address this critical gap, we introduce
MetaFaith, a novel prompt-based calibration approach inspired by human
metacognition. We show that MetaFaith robustly improves faithful calibration
across diverse models and task domains, enabling up to 61% improvement in
faithfulness and achieving an 83% win rate over original generations as judged
by humans.

</details>


### [168] [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/pdf/2505.24863)
*Junyu Zhang, Runpei Dong, Han Wang, Xuying Ning, Haoran Geng, Peihao Li, Xialin He, Yutong Bai, Jitendra Malik, Saurabh Gupta, Huan Zhang*

Main category: cs.CL

TL;DR: AlphaOne ($\alpha$1) is a framework for dynamically modulating reasoning progress in large models, using a stochastic process for slow-to-fast transitions, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning efficiency in large models by enabling flexible and dense modulation between slow and fast thinking phases.

Method: Introduces the $\alpha$ moment for scaled thinking, uses a Bernoulli process for slow thinking transitions, and terminates with an end-of-thinking token for fast reasoning.

Result: Demonstrates superior reasoning capability and efficiency across mathematical, coding, and scientific benchmarks.

Conclusion: Alpha1 generalizes existing methods, offering a unified approach for efficient reasoning modulation in large models.

Abstract: This paper presents AlphaOne ($\alpha$1), a universal framework for
modulating reasoning progress in large reasoning models (LRMs) at test time.
$\alpha$1 first introduces $\alpha$ moment, which represents the scaled
thinking phase with a universal parameter $\alpha$. Within this scaled
pre-$\alpha$ moment phase, it dynamically schedules slow thinking transitions
by modeling the insertion of reasoning transition tokens as a Bernoulli
stochastic process. After the $\alpha$ moment, $\alpha$1 deterministically
terminates slow thinking with the end-of-thinking token, thereby fostering fast
reasoning and efficient answer generation. This approach unifies and
generalizes existing monotonic scaling methods by enabling flexible and dense
slow-to-fast reasoning modulation. Extensive empirical studies on various
challenging benchmarks across mathematical, coding, and scientific domains
demonstrate $\alpha$1's superior reasoning capability and efficiency. Project
page: https://alphaone-project.github.io/

</details>


### [169] [Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models](https://arxiv.org/pdf/2401.08491)
*Tassilo Klein, Moin Nabi*

Main category: cs.CL

TL;DR: A novel framework for detoxifying LLMs uses prototype-based contrastive perplexity and hard negatives to reduce toxic content while preserving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of toxic content generation by LLMs to ensure safer deployment of language technology.

Method: Fine-tuning LLMs with a prototype-based contrastive perplexity objective, using adversarial paraphrasing to create hard negatives (toxic outputs similar to non-toxic ones).

Result: Significant reduction in toxic generation without compromising performance on tasks like commonsense reasoning and reading comprehension.

Conclusion: Hard negatives are effective for attribute-aware fine-tuning, enabling robust detoxification of LLMs.

Abstract: The generation of toxic content by large language models (LLMs) remains a
critical challenge for the safe deployment of language technology. We propose a
novel framework for implicit knowledge editing and controlled text generation
by fine-tuning LLMs with a prototype-based contrastive perplexity objective.
Central to our method is the construction of hard negatives - toxic outputs
that are generated through adversarial paraphrasing to be semantically similar
and model probability to their non-toxic counterparts. By training on these
challenging and realistic pairs, our approach ensures robust and stable
contrastive optimization. Experimental results in the domain of detoxification
demonstrate that our method significantly reduces toxic generation while
maintaining strong performance on downstream tasks such as commonsense
reasoning and reading comprehension. Our findings highlight the effectiveness
of exploiting hard negatives for attribute-aware fine-tuning.

</details>


### [170] [An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4](https://arxiv.org/pdf/2403.02839)
*Hui Huang, Xingyuan Bu, Hongli Zhou, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao*

Main category: cs.CL

TL;DR: Fine-tuned LLM judge models perform well in-domain but lag behind GPT-4 in generalizability, fairness, and adaptability, acting more like task-specific classifiers.


<details>
  <summary>Details</summary>
Motivation: To empirically assess the capabilities of fine-tuned LLM judge models compared to GPT-4 in evaluating other LLMs.

Method: Conducted an empirical study comparing fine-tuned judge models and GPT-4 across various dimensions.

Result: Fine-tuned models excel in-domain but underperform GPT-4 in generalizability, fairness, and adaptability.

Conclusion: Fine-tuned judge models have limitations as task-specific classifiers, highlighting GPT-4's superior versatility.

Abstract: Recently, there has been a growing trend of utilizing Large Language Model
(LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge
models based on open-source LLMs for evaluation. While the fine-tuned judge
models are claimed to achieve comparable evaluation capability with GPT-4, in
this work, we conduct an empirical study of LLM-as-a-Judge. Our findings
indicate that although the fine-tuned judge models achieve high performance on
in-domain test sets, even surpassing GPT-4, they underperform GPT-4 across
several dimensions, including generalizability, fairness and adaptability. We
also reveal that the fine-tuned judge model inherently operates as a
task-specific classifier, consequently imposing the limitations.

</details>


### [171] [Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization](https://arxiv.org/pdf/2405.14189)
*Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, Yang Liu*

Main category: cs.CL

TL;DR: POUGH is a method for universal goal hijacking in LLMs, combining efficient optimization and prompt organization strategies to achieve high attack performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods are cumbersome and overlook prompt importance; POUGH addresses this by integrating optimization and prompt strategies.

Method: Uses sampling and ranking strategies for prompts, followed by iterative optimization to generate a fixed suffix for hijacking.

Result: Effective on four LLMs and ten target responses, demonstrating high performance.

Conclusion: POUGH efficiently achieves universal goal hijacking by optimizing prompts and algorithms.

Abstract: Universal goal hijacking is a kind of prompt injection attack that forces
LLMs to return a target malicious response for arbitrary normal user prompts.
The previous methods achieve high attack performance while being too cumbersome
and time-consuming. Also, they have concentrated solely on optimization
algorithms, overlooking the crucial role of the prompt. To this end, we propose
a method called POUGH that incorporates an efficient optimization algorithm and
two semantics-guided prompt organization strategies. Specifically, our method
starts with a sampling strategy to select representative prompts from a
candidate pool, followed by a ranking strategy that prioritizes them. Given the
sequentially ranked prompts, our method employs an iterative optimization
algorithm to generate a fixed suffix that can concatenate to arbitrary user
prompts for universal goal hijacking. Experiments conducted on four popular
LLMs and ten types of target responses verified the effectiveness.

</details>


### [172] [Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning](https://arxiv.org/pdf/2406.10099)
*Jiaqi Li, Yixuan Tang, Yi Yang*

Main category: cs.CL

TL;DR: US-Tuning improves LLMs' ability to admit knowledge gaps and reduce hallucinations, outperforming GPT-4 in some tasks.


<details>
  <summary>Details</summary>
Motivation: Address LLMs' struggle with admitting knowledge gaps due to instruction datasets favoring answer generation over boundary awareness.

Method: Introduces US-Tuning, a two-stage approach: enhancing knowledge boundary recognition and reinforcing instruction adherence with causal prompts.

Result: Fine-tuned Llama2-7B shows 34.7% improvement in handling out-of-knowledge questions and outperforms GPT-4 by 4.2%.

Conclusion: US-Tuning effectively mitigates hallucinations and improves faithfulness to parametric knowledge in LLMs.

Abstract: Large language models (LLMs) demonstrate remarkable capabilities but face
challenges from hallucinations, which typically arise from insufficient
knowledge or context. While instructing LLMs to acknowledge knowledge
limitations by responding with "I don't know" appears promising, we find that
models consistently struggle with admitting knowledge gaps. This challenge may
originate from current instruction datasets that emphasise answer generation
over knowledge boundary awareness. To address this limitation, we introduce
Uncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage
approach for contextual question answering (QA). The first stage enhances LLMs'
ability to recognise their knowledge boundaries, while the second stage
reinforces instruction adherence through carefully designed causal prompts. Our
experimental results demonstrate that US-Tuning not only significantly reduces
incorrect answers in contextual QA but also improves models' faithfulness to
their parametric knowledge, mitigating hallucinations in general QA tasks. Our
fine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling
out-of-knowledge questions and outperforms GPT-4 by 4.2% in overall
performance.

</details>


### [173] [Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration](https://arxiv.org/pdf/2406.16469)
*ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo*

Main category: cs.CL

TL;DR: A semi-automated framework for creating culturally inclusive vision-language model benchmarks, demonstrated with a Korean culture dataset, showing gaps in open-source models.


<details>
  <summary>Details</summary>
Motivation: To address the labor-intensive and cognitively demanding process of creating culturally diverse benchmarks for VLMs.

Method: Combines human-VLM collaboration for question generation, verified by native speakers, applied to create the K-Viscuit dataset.

Result: Open-source models underperform proprietary ones in understanding Korean culture, with further analyses like human evaluation and knowledge augmentation.

Conclusion: The framework effectively creates cultural benchmarks, highlighting areas for improvement in VLMs, with the dataset publicly available.

Abstract: To create culturally inclusive vision-language models (VLMs), developing a
benchmark that tests their ability to address culturally relevant questions is
essential. Existing approaches typically rely on human annotators, making the
process labor-intensive and creating a cognitive burden in generating diverse
questions. To address this, we propose a semi-automated framework for
constructing cultural VLM benchmarks, specifically targeting multiple-choice
QA. This framework combines human-VLM collaboration, where VLMs generate
questions based on guidelines, a small set of annotated examples, and relevant
knowledge, followed by a verification process by native speakers. We
demonstrate the effectiveness of this framework through the creation of
\texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on
this dataset reveal that open-source models lag behind proprietary ones in
understanding Korean culture, highlighting key areas for improvement. We also
present a series of further analyses, including human evaluation, augmenting
VLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our
dataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.

</details>


### [174] [NativQA: Multilingual Culturally-Aligned Natural Query for LLMs](https://arxiv.org/pdf/2407.09823)
*Md. Arid Hasan, Maram Hasanain, Fatema Ahmad, Sahinur Rahman Laskar, Sunaya Upadhyay, Vrunda N Sukhadia, Mucahid Kutlu, Shammur Absar Chowdhury, Firoj Alam*

Main category: cs.CL

TL;DR: The paper introduces NativQA, a scalable framework for creating region-specific QA datasets in native languages, and presents MultiNativQA, a multilingual dataset with 64k annotated QA pairs for LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of region-specific QA datasets in native languages to improve LLM benchmarking and fine-tuning for cultural and regional nuances.

Method: Proposing NativQA, a language-independent framework, and creating MultiNativQA, a dataset with 64k QA pairs in seven languages from native speakers across 9 regions.

Result: MultiNativQA dataset is developed and benchmarked on open- and closed-source LLMs, demonstrating the framework's efficacy.

Conclusion: The NativQA framework and MultiNativQA dataset fill a critical gap in LLM evaluation, with publicly available resources for community use.

Abstract: Natural Question Answering (QA) datasets play a crucial role in evaluating
the capabilities of large language models (LLMs), ensuring their effectiveness
in real-world applications. Despite the numerous QA datasets that have been
developed and some work has been done in parallel, there is a notable lack of a
framework and large scale region-specific datasets queried by native users in
their own languages. This gap hinders the effective benchmarking and the
development of fine-tuned models for regional and cultural specificities. In
this study, we propose a scalable, language-independent framework, NativQA, to
seamlessly construct culturally and regionally aligned QA datasets in native
languages, for LLM evaluation and tuning. We demonstrate the efficacy of the
proposed framework by designing a multilingual natural QA dataset,
MultiNativQA, consisting of ~64k manually annotated QA pairs in seven
languages, ranging from high to extremely low resource, based on queries from
native speakers from 9 regions covering 18 topics. We benchmark open- and
closed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA
dataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other
experimental scripts(https://gitlab.com/nativqa/multinativqa) publicly
available for the community.

</details>


### [175] [Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment](https://arxiv.org/pdf/2407.14878)
*Yongxin Huang, Kexin Wang, Goran Glavaš, Iryna Gurevych*

Main category: cs.CL

TL;DR: Modular training of multilingual sentence encoders improves performance by separating monolingual and cross-lingual tasks, mitigating the curse of multilinguality.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between monolingual and cross-lingual task performance in multilingual sentence encoders and the curse of multilinguality.

Method: Train language-specific monolingual modules first, then align non-English embeddings to English using cross-lingual adapters with dual data types.

Result: Achieves better, balanced performance across tasks, especially benefiting low-resource languages.

Conclusion: Modular training outperforms monolithic multilingual encoders by resolving interference and conflicting alignment needs.

Abstract: Multilingual sentence encoders (MSEs) are commonly obtained by training
multilingual language models to map sentences from different languages into a
shared semantic space. As such, they are subject to curse of multilinguality, a
loss of monolingual representational accuracy due to parameter sharing. Another
limitation of MSEs is the trade-off between different task performance:
cross-lingual alignment training distorts the optimal monolingual structure of
semantic spaces of individual languages, harming the utility of sentence
embeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual
semantic similarity and zero-shot transfer for sentence classification, may
also require conflicting cross-lingual alignment strategies. In this work, we
address both issues by means of modular training of sentence encoders. We first
train language-specific monolingual modules to mitigate negative interference
between languages (i.e., the curse). We then align all non-English sentence
embeddings to the English by training cross-lingual alignment adapters,
preventing interference with monolingual specialization from the first step. We
train the cross-lingual adapters with two different types of data to resolve
the conflicting requirements of different cross-lingual tasks. Monolingual and
cross-lingual results on semantic text similarity and relatedness, bitext
mining and sentence classification show that our modular solution achieves
better and more balanced performance across all the tasks compared to
full-parameter training of monolithic multilingual sentence encoders,
especially benefiting low-resource languages.

</details>


### [176] [MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU](https://arxiv.org/pdf/2408.08144)
*Yan Li, So-Eon Kim, Seong-Bae Park, Soyeon Caren Han*

Main category: cs.CL

TL;DR: MIDAS introduces a multi-level knowledge distillation approach for multi-turn NLU, improving intent, domain, and slot understanding in dialogues.


<details>
  <summary>Details</summary>
Motivation: Existing NLU models struggle with multi-turn dialogues, requiring a unified solution for intent, domain, and slot interpretation.

Method: MIDAS uses distinct teachers for sentence-level intent, word-level slot filling, and conversation-level domain classification, combined via multi-teacher loss.

Result: The model enhances multi-turn dialogue understanding, demonstrating the effectiveness of multi-level knowledge distillation.

Conclusion: MIDAS advances NLU by integrating multi-level dialogue knowledge, with open-source implementation available.

Abstract: Although Large Language Models (LLMs) can generate coherent text, they often
struggle to recognise user intent behind queries. In contrast, Natural Language
Understanding (NLU) models interpret the purpose and key information of user
input for responsive interactions. Existing NLU models typically map utterances
to a dual-level semantic frame, involving sentence-level intent (SI) and
word-level slot (WS) labels. However, real-life conversations primarily consist
of multi-turn dialogues, requiring the interpretation of complex and extended
exchanges. Researchers encounter challenges in addressing all facets of
multi-turn dialogue using a unified NLU model. This paper introduces MIDAS, a
novel approach leveraging multi-level intent, domain, and slot knowledge
distillation for multi-turn NLU. We construct distinct teachers for SI
detection, WS filling, and conversation-level domain (CD) classification, each
fine-tuned for specific knowledge. A multi-teacher loss is proposed to
facilitate the integration of these teachers, guiding a student model in
multi-turn dialogue tasks. Results demonstrate the efficacy of our model in
improving multi-turn conversation understanding, showcasing the potential for
advancements in NLU through multi-level dialogue knowledge distillation. Our
implementation is open-sourced on https://github.com/adlnlp/Midas.

</details>


### [177] [EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts](https://arxiv.org/pdf/2408.12226)
*Nicy Scaria, Silvester John Joseph Kennedy, Thomas Latinovich, Deepak Subramani*

Main category: cs.CL

TL;DR: Automated evaluation of CEFR B2 English speaking assessments using LLMs, achieving high accuracy with the EvalYaks model family.


<details>
  <summary>Details</summary>
Motivation: Human expert evaluation in e-learning is not scalable; automation is needed for wider, faster assessments.

Method: Evaluate LLMs, create synthetic datasets, and instruction-tune Mistral Instruct 7B to develop EvalYaks models.

Result: EvalYaks achieved 96% accuracy, outperforming other models by 3x.

Conclusion: A 7B parameter LLM with CEFR-aligned data can effectively automate CEFR B2 speaking assessments.

Abstract: Relying on human experts to evaluate CEFR speaking assessments in an
e-learning environment creates scalability challenges, as it limits how quickly
and widely assessments can be conducted. We aim to automate the evaluation of
CEFR B2 English speaking assessments in e-learning environments from
conversation transcripts. First, we evaluate the capability of leading open
source and commercial Large Language Models (LLMs) to score a candidate's
performance across various criteria in the CEFR B2 speaking exam in both global
and India-specific contexts. Next, we create a new expert-validated,
CEFR-aligned synthetic conversational dataset with transcripts that are rated
at different assessment scores. In addition, new instruction-tuned datasets are
developed from the English Vocabulary Profile (up to CEFR B2 level) and the
CEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform
parameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a
family of models called EvalYaks. Four models in this family are for assessing
the four sections of the CEFR B2 speaking exam, one for identifying the CEFR
level of vocabulary and generating level-specific vocabulary, and another for
detecting the CEFR level of text and generating level-specific text. EvalYaks
achieved an average acceptable accuracy of 96%, a degree of variation of 0.35
levels, and performed 3 times better than the next best model. This
demonstrates that a 7B parameter LLM instruction tuned with high-quality
CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English
speaking assessments, offering a promising solution for scalable, automated
language proficiency evaluation.

</details>


### [178] [BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla](https://arxiv.org/pdf/2409.11638)
*Mahammed Kamruzzaman, Abdullah Al Monsur, Shrabon Das, Enamul Hassan, Gene Louis Kim*

Main category: cs.CL

TL;DR: BanStereoSet is a Bangla dataset for evaluating biases in multilingual LLMs, localized from existing datasets, covering 9 bias categories. It reveals significant biases in tested models, highlighting the need for culturally adapted datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of non-English bias evaluation tools, focusing on Bangla to capture local stereotypes and improve equity in language technologies.

Method: Localized content from StereoSet, IndiBias, and Kamruzzaman et al.'s datasets into Bangla, creating 1,194 sentences across 9 bias categories.

Result: Analysis of language models using BanStereoSet showed significant biases, underscoring the dataset's utility for bias measurement.

Conclusion: Culturally and linguistically adapted datasets like BanStereoSet are essential for developing equitable language technologies in Bangla contexts.

Abstract: This study presents BanStereoSet, a dataset designed to evaluate
stereotypical social biases in multilingual LLMs for the Bangla language. In an
effort to extend the focus of bias research beyond English-centric datasets, we
have localized the content from the StereoSet, IndiBias, and Kamruzzaman et.
al.'s datasets, producing a resource tailored to capture biases prevalent
within the Bangla-speaking community. Our BanStereoSet dataset consists of
1,194 sentences spanning 9 categories of bias: race, profession, gender,
ageism, beauty, beauty in profession, region, caste, and religion. This dataset
not only serves as a crucial tool for measuring bias in multilingual LLMs but
also facilitates the exploration of stereotypical bias across different social
categories, potentially guiding the development of more equitable language
technologies in Bangladeshi contexts. Our analysis of several language models
using this dataset indicates significant biases, reinforcing the necessity for
culturally and linguistically adapted datasets to develop more equitable
language technologies.

</details>


### [179] [DemoShapley: Valuation of Demonstrations for In-Context Learning](https://arxiv.org/pdf/2410.07523)
*Shan Xie, Man Luo, Chadly Daniel Stern, Mengnan Du, Lu Cheng*

Main category: cs.CL

TL;DR: Proposed DemoShapley and Beta-DemoShapley to evaluate demonstration influence in ICL, improving performance and fairness in LLMs.


<details>
  <summary>Details</summary>
Motivation: Demonstration selection and ordering significantly affect ICL effectiveness, but existing methods lack flexibility.

Method: Introduced DemoShapley (based on Data Shapley) and Beta-DemoShapley (incorporating Beta distribution) to assess demonstration influence dynamically.

Result: Improved model performance, generalization to out-of-distribution tasks, noise detection, and fairness.

Conclusion: The proposed methods enhance ICL by selecting better demonstrations, ensuring robustness and fairness in LLMs.

Abstract: Large language models (LLMs) using in-context learning (ICL) excel in many
tasks without task-specific fine-tuning. However, demonstration selection and
ordering greatly impact ICL effectiveness. To address this, we propose
DemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to
assess the influence of individual demonstrations. DemoShapley captures how
each example influences performance in different contexts, unlike other
influence-based methods that rely on a fixed number of demonstrations.
Beta-DemoShapley further enhances this framework by incorporating the Beta
distribution, allowing users to assign higher weights to smaller cardinalities,
which aligns with ICL's prompt length and computational constraints. Our
findings show that the proposed algorithms improve model performance by
selecting quality demonstrations, and enhancing generalization to
out-of-distribution tasks. It also identifies noise-compromised data and
promotes fairness in LLMs, protecting model performance and ensuring robustness
across various scenarios.

</details>


### [180] [ChuLo: Chunk-Level Key Information Representation for Long Document Processing](https://arxiv.org/pdf/2410.11119)
*Yan Li, Soyeon Caren Han, Yue Dai, Feiqi Cao*

Main category: cs.CL

TL;DR: ChuLo introduces a chunk representation method for long document understanding, using unsupervised keyphrase extraction to group tokens, reducing input length while minimizing information loss.


<details>
  <summary>Details</summary>
Motivation: Transformer models struggle with long documents due to computational limits, and existing methods like truncation or chunking often lose information or long-range dependencies.

Method: ChuLo groups tokens via unsupervised keyphrase extraction, focusing on semantically important chunks to retain core content.

Result: Evaluated on long document tasks, ChuLo shows effectiveness in preserving fine-grained annotations and improving efficiency.

Conclusion: ChuLo addresses limitations of traditional methods, enhancing Transformer performance for long documents while being open-sourced.

Abstract: Transformer-based models have achieved remarkable success in various Natural
Language Processing (NLP) tasks, yet their ability to handle long documents is
constrained by computational limitations. Traditional approaches, such as
truncating inputs, sparse self-attention, and chunking, attempt to mitigate
these issues, but they often lead to information loss and hinder the model's
ability to capture long-range dependencies. In this paper, we introduce ChuLo,
a novel chunk representation method for long document understanding that
addresses these limitations. Our ChuLo groups input tokens using unsupervised
keyphrase extraction, emphasizing semantically important keyphrase based chunks
to retain core document content while reducing input length. This approach
minimizes information loss and improves the efficiency of Transformer-based
models. Preserving all tokens in long document understanding, especially token
classification tasks, is important to ensure that fine-grained annotations,
which depend on the entire sequence context, are not lost. We evaluate our
method on multiple long document classification tasks and long document token
classification tasks, demonstrating its effectiveness through comprehensive
qualitative and quantitative analysis. Our implementation is open-sourced on
https://github.com/adlnlp/Chulo.

</details>


### [181] [From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence](https://arxiv.org/pdf/2410.13460)
*Ronja Stern, Ken Kawamura, Matthias Stürmer, Ilias Chalkidis, Joel Niklaus*

Main category: cs.CL

TL;DR: The paper introduces the Criticality Prediction dataset for case prioritization in overwhelmed court systems, using algorithmic labeling and evaluating multilingual models, with fine-tuned models outperforming larger ones.


<details>
  <summary>Details</summary>
Motivation: Court systems worldwide face backlogs; effective triage systems could optimize resource allocation.

Method: Algorithmic labeling for a large dataset (LD-Label and Citation-Label) and evaluation of multilingual models (fine-tuned vs. zero-shot LLMs).

Result: Fine-tuned models outperform larger zero-shot models, emphasizing the value of large training sets for domain-specific tasks.

Conclusion: Large training sets remain crucial for domain-specific tasks, and algorithmic labeling enables scalable dataset creation.

Abstract: Many court systems are overwhelmed all over the world, leading to huge
backlogs of pending cases. Effective triage systems, like those in emergency
rooms, could ensure proper prioritization of open cases, optimizing time and
resource allocation in the court system. In this work, we introduce the
Criticality Prediction dataset, a novel resource for evaluating case
prioritization. Our dataset features a two-tier labeling system: (1) the binary
LD-Label, identifying cases published as Leading Decisions (LD), and (2) the
more granular Citation-Label, ranking cases by their citation frequency and
recency, allowing for a more nuanced evaluation. Unlike existing approaches
that rely on resource-intensive manual annotations, we algorithmically derive
labels leading to a much larger dataset than otherwise possible. We evaluate
several multilingual models, including both smaller fine-tuned models and large
language models in a zero-shot setting. Our results show that the fine-tuned
models consistently outperform their larger counterparts, thanks to our large
training set. Our results highlight that for highly domain-specific tasks like
ours, large training sets are still valuable.

</details>


### [182] [Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models](https://arxiv.org/pdf/2410.14248)
*Olga Loginova, Oleksandr Bezrukov, Ravi Shekhar, Alexey Kravets*

Main category: cs.CL

TL;DR: The paper addresses selection bias in Multiple-Choice Question Answering (MCQA) benchmarks for Video Language Models (VLMs) and introduces BOLD, a post-processing calibration technique to mitigate bias, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Existing MCQA benchmarks fail to capture VLMs' full reasoning due to selection bias, where models favor certain answer options based on positional patterns.

Method: The study analyzes VLM architectures across datasets, identifies bias, and introduces BOLD, a calibration technique to balance bias.

Result: BOLD reduces selection bias, improving debiasing metrics, Accuracy, and F1 Mean score.

Conclusion: BOLD offers a cost-effective solution to mitigate bias, enhancing VLM performance and providing the first focused study on selection bias in video-to-text LLMs.

Abstract: Evaluating Video Language Models (VLMs) is a challenging task. Due to its
transparency, Multiple-Choice Question Answering (MCQA) is widely used to
measure the performance of these models through accuracy. However, existing
MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to
selection bias, when models disproportionately favor certain answer options
based on positional patterns observed during training. In this work, we conduct
a comprehensive empirical analysis of several VLM architectures across major
datasets designed to assess complex video-focused reasoning. We identify where
the bias is most pronounced and demonstrate to what extent model responses
reflect genuine understanding of video content and related questions, as
opposed to reliance on arbitrary patterns or superficial cues, such as answer
position. By decomposing the MCQA task and adapting fairness bias metrics to
VLMs, we introduce a post-processing calibration technique BOLD to balance this
bias. Our results show that reducing selection bias improves not only debiasing
metrics but also overall model performance, including Accuracy and F1 Mean
score. Our method, by suppressing "blind guessing", offers a more cost- and
time-effective approach to mitigating selection bias compared to existing
techniques. This study represents the first focused investigation of selection
bias in video-to-text LLM-powered models.

</details>


### [183] [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/pdf/2410.21728)
*Kangyang Luo, Zichen Ding, Zhenmin Weng, Lingfeng Qiao, Meng Zhao, Xiang Li, Di Yin, Jinlong Shu*

Main category: cs.CL

TL;DR: LBS3 is a novel prompt approach for automatic reasoning in LLMs, inspired by curriculum learning, that improves reasoning performance by guiding models through easy-to-hard proxy queries.


<details>
  <summary>Details</summary>
Motivation: Existing CoT prompting approaches for LLMs require extensive human effort or lack performance, prompting the need for an automated, high-quality solution.

Method: LBS3 uses curriculum learning to guide LLMs through easy-to-hard proxy queries, leveraging exemplary prompts from easy queries to solve harder ones.

Result: Experiments show LBS3 achieves competitive performance in reasoning-intensive tasks across various LLMs.

Conclusion: LBS3 effectively addresses limitations of CoT prompting, offering a scalable and automated solution for improving LLM reasoning.

Abstract: While Chain of Thought (CoT) prompting approaches have significantly
consolidated the reasoning capabilities of large language models (LLMs), they
still face limitations that require extensive human effort or have performance
needs to be improved. Existing endeavors have focused on bridging these gaps;
however, these approaches either hinge on external data and cannot completely
eliminate manual effort, or they fall short in effectively directing LLMs to
generate high-quality exemplary prompts. To address the said pitfalls, we
propose a novel prompt approach for automatic reasoning named \textbf{LBS3},
inspired by curriculum learning which better reflects human learning habits.
Specifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries
that are pertinent to the target query. Following this, it invokes a
progressive strategy that utilizes exemplary prompts stemmed from easy-proxy
queries to direct LLMs in solving hard-proxy queries, enabling the high-quality
of the proxy solutions. Finally, our extensive experiments in various
reasoning-intensive tasks with varying open- and closed-source LLMs show that
LBS3 achieves strongly competitive performance compared to the SOTA baselines.

</details>


### [184] [Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework](https://arxiv.org/pdf/2411.06160)
*Jingyi Zhou, Senlin Luo, Haofan Chen*

Main category: cs.CL

TL;DR: The paper introduces the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation, addressing limitations in existing datasets by mapping label values to energy intensity levels. It validates EQN's effectiveness across NLP models and demonstrates superior performance in automatic micro-emotion annotation.


<details>
  <summary>Details</summary>
Motivation: Existing emotion detection datasets suffer from high costs, subjectivity, and label imbalances, lacking micro-emotion and intensity representation. EQN aims to overcome these limitations by leveraging machine learning capabilities.

Method: Proposes an all-labels and training-set label regression method to map label values to energy intensity levels, establishing the EQN framework for micro-emotion detection. Validated using five sentiment datasets and the GoEmotions dataset.

Result: EQN outperforms existing models in automatic micro-emotion annotation, achieving energy-level scoring for the first time. It shows broad applicability in NLP models.

Conclusion: The EQN framework successfully addresses dataset limitations, enabling high-quality micro-emotion detection and annotation, supporting advanced emotion computing research.

Abstract: Text emotion detection constitutes a crucial foundation for advancing
artificial intelligence from basic comprehension to the exploration of
emotional reasoning. Most existing emotion detection datasets rely on manual
annotations, which are associated with high costs, substantial subjectivity,
and severe label imbalances. This is particularly evident in the inadequate
annotation of micro-emotions and the absence of emotional intensity
representation, which fail to capture the rich emotions embedded in sentences
and adversely affect the quality of downstream task completion. By proposing an
all-labels and training-set label regression method, we map label values to
energy intensity levels, thereby fully leveraging the learning capabilities of
machine models and the interdependencies among labels to uncover multiple
emotions within samples. This led to the establishment of the Emotion
Quantization Network (EQN) framework for micro-emotion detection and
annotation. Using five commonly employed sentiment datasets, we conducted
comparative experiments with various models, validating the broad applicability
of our framework within NLP machine learning models. Based on the EQN
framework, emotion detection and annotation are conducted on the GoEmotions
dataset. A comprehensive comparison with the results from Google literature
demonstrates that the EQN framework possesses a high capability for automatic
detection and annotation of micro-emotions. The EQN framework is the first to
achieve automatic micro-emotion annotation with energy-level scores, providing
strong support for further emotion detection analysis and the quantitative
research of emotion computing.

</details>


### [185] [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/pdf/2411.07404)
*Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell*

Main category: cs.CL

TL;DR: The paper explores controlling a language model's sensitivity to context vs. prior knowledge, identifies a 1-D subspace for this control, and shows its universality across models.


<details>
  <summary>Details</summary>
Motivation: To determine how language models balance context and prior knowledge, enabling better performance in tasks like retrieval-augmented generation and question-answering.

Method: Designed a task for controllable context sensitivity, fine-tuned models, and analyzed layers to identify a 1-D subspace controlling context sensitivity.

Result: Fine-tuned models achieved 85-95% accuracy; the identified subspace works across models and correlates with performance.

Conclusion: A single subspace controls context sensitivity, suggesting a simple fundamental mechanism.

Abstract: When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.

</details>


### [186] [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/pdf/2411.17116)
*Shantanu Acharya, Fei Jia, Boris Ginsburg*

Main category: cs.CL

TL;DR: Star Attention improves efficiency of Transformer-based LLMs by using a two-phase block-sparse approximation, reducing memory and time by up to 11x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of self-attention in LLMs makes inference costly and slow for long sequences.

Method: Introduces Star Attention, a two-phase approach: blockwise-local attention across hosts and sequence-global attention for query and response tokens.

Result: Reduces memory and inference time by up to 11x while preserving 97-100% accuracy.

Conclusion: Star Attention offers a scalable solution for efficient inference in LLMs without significant accuracy loss.

Abstract: Inference with Transformer-based Large Language Models (LLMs) on long
sequences is both costly and slow due to the quadratic complexity of the
self-attention mechanism. We introduce Star Attention, a two-phase block-sparse
approximation that improves computational efficiency by sharding attention
across multiple hosts while minimizing communication overhead. In the first
phase, the context is processed using blockwise-local attention across hosts,
in parallel. In the second phase, query and response tokens attend to all prior
cached tokens through sequence-global attention. Star Attention integrates
seamlessly with most Transformer-based LLMs trained with global attention,
reducing memory requirements and inference time by up to 11x while preserving
97-100% of accuracy.

</details>


### [187] [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/pdf/2412.04905)
*Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, Yongbin Li*

Main category: cs.CL

TL;DR: The paper introduces DEMO, a benchmark for comprehensive dialogue modeling, addressing gaps in current LLM-based dialogue systems by focusing on dialogue stages and elements.


<details>
  <summary>Details</summary>
Motivation: Current dialogue systems lack systematic investigation into dialogue stages, hindering precise modeling and assessment of LLMs.

Method: Proposes DEMO, a benchmark for dialogue element modeling, and builds a DEMO agent using imitation learning.

Result: Experiments show current LLMs have room for improvement, while the DEMO agent excels in dialogue element modeling and out-of-domain tasks.

Conclusion: DEMO provides a robust framework for advancing dialogue modeling and assessment in LLM-based systems.

Abstract: Large language models (LLMs) enabled dialogue systems have become one of the
central modes in human-machine interaction, which bring about vast amounts of
conversation logs and increasing demand for dialogue generation. The dialogue's
life-cycle spans from $\textit{Prelude}$ through $\textit{Interlocution}$ to
$\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes
of dialogue-related studies, there is a lack of systematic investigation into
the dialogue stages to frame benchmark construction that covers comprehensive
dialogue elements. This hinders the precise modeling, generation and assessment
of LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce
a new research task--$\textbf{D}$ialogue $\textbf{E}$lement
$\textbf{MO}$deling, including $\textit{Element Awareness}$ and
$\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,
$\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.
On this basis, we further build the DEMO agent with the adept ability to model
dialogue elements via imitation learning. Extensive experiments on DEMO
indicate that current representative LLMs still have considerable potential for
enhancement, and our DEMO agent performs well in both dialogue element modeling
and out-of-domain tasks.

</details>


### [188] [Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation](https://arxiv.org/pdf/2412.08473)
*Huiyuan Lai, Esther Ploeger, Rik van Noord, Antonio Toral*

Main category: cs.CL

TL;DR: A novel method using reinforcement learning improves NMT by balancing naturalness and content preservation, enhancing lexical richness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: NMT systems amplify lexical biases, producing unnatural translations that differ from human translations, limiting their utility in tasks like evaluation dataset creation.

Method: Introduces a reinforcement learning-based approach to reward both naturalness and content preservation, tested on English-to-Dutch literary translation.

Result: The best model yields lexically richer translations with more human-like properties, maintaining translation accuracy.

Conclusion: The method successfully reduces machine and human translationese, improving NMT output quality.

Abstract: Neural machine translation (NMT) systems amplify lexical biases present in
their training data, leading to artificially impoverished language in output
translations. These language-level characteristics render automatic
translations different from text originally written in a language and human
translations, which hinders their usefulness in for example creating evaluation
datasets. Attempts to increase naturalness in NMT can fall short in terms of
content preservation, where increased lexical diversity comes at the cost of
translation accuracy. Inspired by the reinforcement learning from human
feedback framework, we introduce a novel method that rewards both naturalness
and content preservation. We experiment with multiple perspectives to produce
more natural translations, aiming at reducing machine and human translationese.
We evaluate our method on English-to-Dutch literary translation, and find that
our best model produces translations that are lexically richer and exhibit more
properties of human-written language, without loss in translation accuracy.

</details>


### [189] [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/pdf/2412.08972)
*Ruiwen Zhou, Wenyue Hua, Liangming Pan, Sitao Cheng, Xiaobao Wu, En Yu, William Yang Wang*

Main category: cs.CL

TL;DR: RuleArena is a benchmark for evaluating LLMs' ability to follow complex real-world rules, revealing limitations in rule application, math accuracy, and overall performance, but showing improvement with external tools.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' proficiency in handling intricate, practical rules and their suitability for real-world applications.

Method: RuleArena evaluates LLMs across three domains (airline baggage fees, NBA transactions, tax regulations) using long-context understanding, logical reasoning, and math computation.

Result: LLMs struggle with rule application, math accuracy, and perform poorly overall, but improve with external tools.

Conclusion: The benchmark highlights challenges and research directions for advancing LLMs' rule-guided reasoning in real-life scenarios.

Abstract: This paper introduces RuleArena, a novel and challenging benchmark designed
to evaluate the ability of large language models (LLMs) to follow complex,
real-world rules in reasoning. Covering three practical domains -- airline
baggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'
proficiency in handling intricate natural language instructions that demand
long-context understanding, logical reasoning, and accurate mathematical
computation. Two key attributes distinguish RuleArena from traditional
rule-based reasoning benchmarks: (1) it extends beyond standard first-order
logic representations, and (2) it is grounded in authentic, practical
scenarios, providing insights into the suitability and reliability of LLMs for
real-world applications. Our findings reveal several notable limitations in
LLMs: (1) they struggle to identify and apply the appropriate rules, frequently
becoming confused by similar but distinct regulations, (2) they cannot
consistently perform accurate mathematical computations, even when they
correctly identify the relevant rules, and (3) in general, they perform poorly
in the benchmark. We also observe a significant performance boost when LLMs are
provided with external tools for oracle math and logic operations. These
results highlight significant challenges and promising research directions in
advancing LLMs' rule-guided reasoning capabilities in real-life applications.
Our codes and data are publicly available on
https://github.com/skyriver-2000/RuleArena.

</details>


### [190] [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/pdf/2412.12567)
*Seunghee Kim, Changhyeon Kim, Taeuk Kim*

Main category: cs.CL

TL;DR: The paper introduces FCMR, a benchmark for evaluating multimodal large language models (MLLMs) on multi-hop reasoning across financial data, revealing their limitations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like MMQA are flawed due to data contamination and lack of complex queries, necessitating a new evaluation tool.

Method: FCMR is designed with three difficulty levels (Easy, Medium, Hard) to test MLLMs on combining textual reports, tables, and charts in financial contexts.

Result: State-of-the-art MLLMs perform poorly, with the best model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on Hard-level tasks.

Conclusion: FCMR highlights MLLMs' reasoning limitations, particularly in information retrieval, and provides a robust evaluation framework.

Abstract: Real-world decision-making often requires integrating and reasoning over
information from multiple modalities. While recent multimodal large language
models (MLLMs) have shown promise in such tasks, their ability to perform
multi-hop reasoning across diverse sources remains insufficiently evaluated.
Existing benchmarks, such as MMQA, face challenges due to (1) data
contamination and (2) a lack of complex queries that necessitate operations
across more than two modalities, hindering accurate performance assessment. To
address this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a
benchmark created to analyze the reasoning capabilities of MLLMs by urging them
to combine information from textual reports, tables, and charts within the
financial domain. FCMR is categorized into three difficulty levels-Easy,
Medium, and Hard-facilitating a step-by-step evaluation. In particular,
problems at the Hard level require precise cross-modal three-hop reasoning and
are designed to prevent the disregard of any modality. Experiments on this new
benchmark reveal that even state-of-the-art MLLMs struggle, with the
best-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the
most challenging tier. We also conduct analysis to provide insights into the
inner workings of the models, including the discovery of a critical bottleneck
in the information retrieval phase.

</details>


### [191] [A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI](https://arxiv.org/pdf/2412.13942)
*Beiduo Chen, Siyao Peng, Anna Korhonen, Barbara Plank*

Main category: cs.CL

TL;DR: LLMs can generate explanations to approximate human judgment distributions (HJDs) as effectively as human-provided explanations, even without human input.


<details>
  <summary>Details</summary>
Motivation: To reduce the time-consuming process of collecting human explanations for every label while still approximating HJDs accurately.

Method: Use LLMs to generate explanations for given human labels, test ways to combine these with labels, and compare model-generated explanations to human ones.

Result: LLM-generated explanations perform comparably to human explanations for approximating HJDs, even on datasets without human explanations or out-of-distribution test sets.

Conclusion: LLMs are a viable alternative to human annotators for generating explanations to estimate HJDs, saving time without sacrificing accuracy.

Abstract: Disagreement in human labeling is ubiquitous, and can be captured in human
judgment distributions (HJDs). Recent research has shown that explanations
provide valuable information for understanding human label variation (HLV) and
large language models (LLMs) can approximate HJD from a few human-provided
label-explanation pairs. However, collecting explanations for every label is
still time-consuming. This paper examines whether LLMs can be used to replace
humans in generating explanations for approximating HJD. Specifically, we use
LLMs as annotators to generate model explanations for a few given human labels.
We test ways to obtain and combine these label-explanations with the goal to
approximate human judgment distributions. We further compare the resulting
human with model-generated explanations, and test automatic and human
explanation selection. Our experiments show that LLM explanations are promising
for NLI: to estimate HJDs, generated explanations yield comparable results to
human's when provided with human labels. Importantly, our results generalize
from datasets with human explanations to i) datasets where they are not
available and ii) challenging out-of-distribution test sets.

</details>


### [192] [Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph](https://arxiv.org/pdf/2412.15268)
*Yibo Zhao, Jiapeng Zhu, Can Xu, Yao Liu, Xiang Li*

Main category: cs.CL

TL;DR: MetaTox improves toxicity detection in social media by using a meta-toxic knowledge graph to reduce false positives and negatives.


<details>
  <summary>Details</summary>
Motivation: Address challenges in toxicity detection using LLMs: false negatives due to lack of domain-specific knowledge and false positives from excessive sensitivity.

Method: Construct a meta-toxic knowledge graph via LLMs, then use retrieval and ranking to query the graph for accurate toxic knowledge.

Result: MetaTox reduces false positives and enhances overall toxicity detection performance.

Conclusion: MetaTox effectively balances toxicity detection and freedom of speech, with code publicly available.

Abstract: The rapid growth of social media platforms has raised significant concerns
regarding online content toxicity. When Large Language Models (LLMs) are used
for toxicity detection, two key challenges emerge: 1) the absence of
domain-specific toxic knowledge leads to false negatives; 2) the excessive
sensitivity of LLMs to toxic speech results in false positives, limiting
freedom of speech. To address these issues, we propose a novel method called
MetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance
hatred and toxicity detection. First, we construct a comprehensive meta-toxic
knowledge graph by utilizing LLMs to extract toxic information through a
three-step pipeline, with toxic benchmark datasets serving as corpora. Second,
we query the graph via retrieval and ranking processes to supplement accurate,
relevant toxic knowledge. Extensive experiments and in-depth case studies
across multiple datasets demonstrate that our MetaTox significantly decreases
the false positive rate while boosting overall toxicity detection performance.
Our code is available at https://github.com/YiboZhao624/MetaTox.

</details>


### [193] [Contrastive Learning for Task-Independent SpeechLLM-Pretraining](https://arxiv.org/pdf/2412.15712)
*Maike Züfle, Jan Niehues*

Main category: cs.CL

TL;DR: A two-stage training approach for adapting LLMs to speech tasks efficiently, outperforming traditional methods with minimal data.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like overfitting, data requirements, and computational costs in adapting LLMs to speech processing.

Method: 1. Task-independent speech pretraining with contrastive learning. 2. Task-specific fine-tuning with minimal data.

Result: Outperforms traditional ASR pretraining and specialized models in speech translation and QA with only 10% of task-specific data.

Conclusion: The proposed scalable approach efficiently adapts LLMs to speech tasks, reducing data and computational needs.

Abstract: Large language models (LLMs) excel in natural language processing but
adapting these LLMs to speech processing tasks efficiently is not
straightforward. Direct task-specific fine-tuning is limited by overfitting
risks, data requirements, and computational costs. To address these challenges,
we propose a scalable, two-stage training approach: (1) A task-independent
speech pretraining stage using contrastive learning to align text and speech
representations over all layers, followed by (2) a task-specific fine-tuning
stage requiring minimal data. This approach outperforms traditional ASR
pretraining and enables the model to surpass models specialized on speech
translation and question answering while being trained on only 10% of the
task-specific data.

</details>


### [194] [CLIX: Cross-Lingual Explanations of Idiomatic Expressions](https://arxiv.org/pdf/2501.03191)
*Aaron Gluck, Katharina von der Wense, Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: The paper proposes CLIX, a task for cross-lingual explanations of idiomatic expressions, to improve automated definition generation for language learners. It evaluates NLP models and identifies challenges for educational integration.


<details>
  <summary>Details</summary>
Motivation: Language learners struggle with automated definitions due to unfamiliar words and grammar, especially with non-standard language.

Method: Proposes CLIX, evaluates NLP models (notably large language models), and conducts error analysis.

Result: Large language models show promise but challenges remain for reliable educational use.

Conclusion: Key challenges must be addressed before integrating these systems into educational tools.

Abstract: Automated definition generation systems have been proposed to support
vocabulary expansion for language learners. The main barrier to the success of
these systems is that learners often struggle to understand definitions due to
the presence of potentially unfamiliar words and grammar, particularly when
non-standard language is involved. To address these challenges, we propose
CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We
explore the capabilities of current NLP models for this task, and observe that
while it remains challenging, large language models show promise. Finally, we
perform a detailed error analysis to highlight the key challenges that need to
be addressed before we can reliably incorporate these systems into educational
tools.

</details>


### [195] [AlphaPO: Reward Shape Matters for LLM Alignment](https://arxiv.org/pdf/2501.03884)
*Aman Gupta, Shao Tang, Qingquan Song, Sirou Zhu, Jiwoo Hong, Ankan Saha, Viral Gupta, Noah Lee, Eunki Kim, Siyu Zhu, Parag Agrawal, Natesh Pillai, S. Sathiya Keerthi*

Main category: cs.CL

TL;DR: AlphaPO, a new Direct Alignment Algorithm (DAA), improves alignment performance by optimizing reward function shape, outperforming DPO and SimPO.


<details>
  <summary>Details</summary>
Motivation: Address likelihood displacement in DAAs by refining reward function shape for better alignment.

Method: Introduces AlphaPO, leveraging an α-parameter to adjust reward function shape beyond standard log reward.

Result: 7-10% relative improvement over SimPO and 15-50% over DPO in alignment performance.

Conclusion: Reward shape is crucial; systematic adjustments can enhance training dynamics and alignment.

Abstract: Reinforcement Learning with Human Feedback (RLHF) and its variants have made
huge strides toward the effective alignment of large language models (LLMs) to
follow instructions and reflect human values. More recently, Direct Alignment
Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is
skipped by characterizing the reward directly as a function of the policy being
learned. Some popular examples of DAAs include Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO). These methods often suffer
from likelihood displacement, a phenomenon by which the probabilities of
preferred responses are often reduced undesirably. In this paper, we argue
that, for DAAs the reward (function) shape matters. We introduce
\textbf{AlphaPO}, a new DAA method that leverages an $\alpha$-parameter to help
change the shape of the reward function beyond the standard log reward. AlphaPO
helps maintain fine-grained control over likelihood displacement and
over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO
leads to about 7\% to 10\% relative improvement in alignment performance for
the instruct versions of Mistral-7B and Llama3-8B while achieving 15\% to 50\%
relative improvement over DPO on the same models. The analysis and results
presented highlight the importance of the reward shape and how one can
systematically change it to affect training dynamics, as well as improve
alignment performance.

</details>


### [196] [Autonomy-of-Experts Models](https://arxiv.org/pdf/2501.13074)
*Ang Lv, Ruobing Xie, Yining Qian, Songhao Wu, Xingwu Sun, Zhanhui Kang, Di Wang, Rui Yan*

Main category: cs.CL

TL;DR: Autonomy-of-Experts (AoE) replaces traditional routers in MoE models by allowing experts to autonomously select themselves based on internal activations, improving performance and learning efficiency.


<details>
  <summary>Details</summary>
Motivation: The separation between router decisions and expert execution in MoE models leads to suboptimal expert selection and ineffective learning.

Method: AoE removes routers; experts pre-compute internal activations, rank themselves by activation norms, and only top experts proceed. Low-rank weight factorization reduces overhead.

Result: AoE outperforms traditional MoE models in pre-trained language models (700M to 4B parameters) with comparable efficiency.

Conclusion: AoE introduces a self-evaluating paradigm for MoE models, enhancing expert selection and learning effectiveness.

Abstract: Mixture-of-Experts (MoE) models mostly use a router to assign tokens to
specific expert modules, activating only partial parameters and often
outperforming dense models. We argue that the separation between the router's
decision-making and the experts' execution is a critical yet overlooked issue,
leading to suboptimal expert selection and ineffective learning. To address
this, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which
experts autonomously select themselves to process inputs. AoE is based on the
insight that an expert is aware of its own capacity to effectively process a
token, an awareness reflected in the scale of its internal activations. In AoE,
routers are removed; instead, experts pre-compute internal activations for
inputs and are ranked based on their activation norms. Only the top-ranking
experts proceed with the forward pass, while the others abort. The overhead of
pre-computing activations is reduced through a low-rank weight factorization.
This self-evaluating-then-partner-comparing approach ensures improved expert
selection and effective learning. We pre-train language models having 700M up
to 4B parameters, demonstrating that AoE outperforms traditional MoE models
with comparable efficiency.

</details>


### [197] [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/pdf/2501.18922)
*Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan*

Main category: cs.CL

TL;DR: KBQA-o1, a novel KBQA method using Monte Carlo Tree Search (MCTS) and ReAct-based agents, improves performance with limited annotated data, achieving 78.5% F1 on GrailQA.


<details>
  <summary>Details</summary>
Motivation: Address challenges in KBQA: weak KB awareness, efficiency-effectiveness imbalance, and high reliance on annotated data.

Method: Combines ReAct-based agent for logical form generation with MCTS for heuristic search, balancing exploration and performance.

Result: Outperforms previous methods, achieving 78.5% F1 on GrailQA with limited data.

Conclusion: KBQA-o1 is effective for low-resource KBQA, leveraging MCTS and agentic exploration for improved performance.

Abstract: Knowledge Base Question Answering (KBQA) aims to answer natural language
questions with a large-scale structured knowledge base (KB). Despite
advancements with large language models (LLMs), KBQA still faces challenges in
weak KB awareness, imbalance between effectiveness and efficiency, and high
reliance on annotated data. To address these challenges, we propose KBQA-o1, a
novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a
ReAct-based agent process for stepwise logical form generation with KB
environment exploration. Moreover, it employs MCTS, a heuristic search method
driven by policy and reward models, to balance agentic exploration's
performance and search space. With heuristic exploration, KBQA-o1 generates
high-quality annotations for further improvement by incremental fine-tuning.
Experimental results show that KBQA-o1 outperforms previous low-resource KBQA
methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1
performance to 78.5% compared to 48.5% of the previous sota method with
GPT-3.5-turbo. Our code is publicly available.

</details>


### [198] [M+: Extending MemoryLLM with Scalable Long-Term Memory](https://arxiv.org/pdf/2502.00592)
*Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, Zexue He*

Main category: cs.CL

TL;DR: M+ enhances MemoryLLM by adding a long-term memory mechanism and co-trained retriever, improving retention from 20k to 160k tokens.


<details>
  <summary>Details</summary>
Motivation: Existing latent-space memory models like MemoryLLM struggle with retaining information beyond 20k tokens.

Method: M+ integrates a long-term memory mechanism and a co-trained retriever to dynamically fetch relevant information during generation.

Result: M+ outperforms MemoryLLM and baselines, extending retention to over 160k tokens with similar GPU memory usage.

Conclusion: M+ significantly improves long-term information retention in LLMs, offering a scalable solution for extended context windows.

Abstract: Equipping large language models (LLMs) with latent-space memory has attracted
increasing attention as they can extend the context window of existing language
models. However, retaining information from the distant past remains a
challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative
work with latent-space memory, compresses past information into hidden states
across all layers, forming a memory pool of 1B parameters. While effective for
sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k
tokens. In this work, we address this limitation by introducing M+, a
memory-augmented model based on MemoryLLM that significantly enhances long-term
information retention. M+ integrates a long-term memory mechanism with a
co-trained retriever, dynamically retrieving relevant information during text
generation. We evaluate M+ on diverse benchmarks, including long-context
understanding and knowledge retention tasks. Experimental results show that M+
significantly outperforms MemoryLLM and recent strong baselines, extending
knowledge retention from under 20k to over 160k tokens with similar GPU memory
overhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM

</details>


### [199] [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/pdf/2502.01349)
*Giorgos Filandrianos, Angeliki Dimitriou, Maria Lymperaiou, Konstantinos Thomas, Giorgos Stamou*

Main category: cs.CL

TL;DR: The paper explores how cognitive biases can manipulate LLM-based product recommenders, revealing unpredictable effects on recommendations and challenges for mitigation.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of LLMs in product recommenders to adversarial manipulation, leveraging human psychological principles for undetectable modifications.

Method: Investigates cognitive biases as black-box adversarial strategies, testing their impact on LLMs and human purchasing behavior across models of varying scales.

Result: Certain biases (e.g., social proof) improve recommendations, while others (e.g., scarcity) reduce visibility, showing deep embedding of biases in LLMs.

Conclusion: Cognitive biases unpredictably affect LLM recommendations, posing significant mitigation challenges.

Abstract: The advent of Large Language Models (LLMs) has revolutionized product
recommenders, yet their susceptibility to adversarial manipulation poses
critical challenges, particularly in real-world commercial applications. Our
approach is the first one to tap into human psychological principles,
seamlessly modifying product descriptions, making such manipulations hard to
detect. In this work, we investigate cognitive biases as black-box adversarial
strategies, drawing parallels between their effects on LLMs and human
purchasing behavior. Through extensive evaluation across models of varying
scale, we find that certain biases, such as social proof, consistently boost
product recommendation rate and ranking, while others, like scarcity and
exclusivity, surprisingly reduce visibility. Our results demonstrate that
cognitive biases are deeply embedded in state-of-the-art LLMs, leading to
highly unpredictable behavior in product recommendations and posing significant
challenges for effective mitigation.

</details>


### [200] [Boosting Multimodal Reasoning with Automated Structured Thinking](https://arxiv.org/pdf/2502.02339)
*Jinyang Wu, Mingkuan Feng, Shuai Zhang, Fangrui Lv, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao*

Main category: cs.CL

TL;DR: AStar introduces an automated structured thinking paradigm for multimodal reasoning, using lightweight 'thought cards' to improve efficiency and accuracy without extensive training.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models struggle with complex visual reasoning tasks due to inefficiencies in current methods like search-based and post-training approaches.

Method: AStar leverages 'thought cards'—high-level reasoning patterns from prior samples—and integrates them with the model's implicit reasoning.

Result: AStar achieves 53.9% accuracy on MathVerse (beating GPT-4o) and 32.7% on MathVision, with minimal data and no training.

Conclusion: AStar is effective, efficient, and generalizable, serving as a plug-and-play solution for multimodal reasoning and beyond.

Abstract: Multimodal large language models excel across diverse domains but struggle
with complex visual reasoning tasks. Current approaches aim to incorporate
structured thinking via two strategies: explicit search methods and
post-training techniques. However, both approaches face significant
limitations: Search-based methods suffer from computational inefficiency due to
extensive solution space exploration, while post-training methods require
substantial data, computational resources, and often encounter training
instability. To address these limitations, we propose AStar, an
\textbf{A}utomated \textbf{S}tructured \textbf{t}hinking paradigm for
multimod\textbf{a}l \textbf{r}easoning. Our method introduces "thought cards",
a lightweight library of high-level reasoning patterns abstracted from 500
prior samples using Monte Carlo Tree Search. For each test problem, AStar
adaptively retrieves the optimal thought cards and seamlessly integrates these
external explicit guidelines with the model's internal implicit reasoning
capabilities. Extensive experiments demonstrate AStar's effectiveness and
efficiency: using only 500 prior samples and a 7B backbone, our training-free
framework achieves 53.9$\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)
and 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that
AStar generalizes beyond multimodal reasoning to visual perception and
understanding domains, and serves as a plug-and-play test-time inference method
compatible with mainstream post-training techniques like GRPO.

</details>


### [201] [A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)](https://arxiv.org/pdf/2502.02659)
*Yan Li, Tianyi Zhang, Zechuan Li, Soyeon Caren Han*

Main category: cs.CL

TL;DR: GALI, a training-free method, improves LLM length extrapolation by reusing positional intervals and interpolating attention logits, achieving stable performance without input-length-specific tuning.


<details>
  <summary>Details</summary>
Motivation: Transformer-based LLMs struggle with long inputs due to positional O.O.D. issues, and existing solutions are inefficient or lose local positional information.

Method: GALI greedily reuses pretrained positional intervals and interpolates attention logits to eliminate outliers, avoiding redundant interpolation.

Result: GALI performs stably across long-context tasks without tuning and reveals uneven positional interval interpretation in LLMs.

Conclusion: GALI advances robust long-text processing in LLMs, with open-sourced implementation available.

Abstract: Transformer-based Large Language Models (LLMs) struggle with inputs exceeding
their training context window due to positional out-of-distribution (O.O.D.)
issues that disrupt attention. Existing solutions, including fine-tuning and
training-free methods, face challenges like inefficiency, redundant
interpolation, logit outliers, or loss of local positional information. We
propose Greedy Attention Logit Interpolation (GALI), a training-free method
that improves length extrapolation by greedily reusing pretrained positional
intervals and interpolating attention logit to eliminate outliers. GALI
achieves stable and superior performance across a wide range of long-context
tasks without requiring input-length-specific tuning. Our analysis further
reveals that LLMs interpret positional intervals unevenly and that restricting
interpolation to narrower ranges improves performance, even on short-context
tasks. GALI represents a step toward more robust and generalizable long-text
processing in LLMs. Our implementation of GALI, along with the experiments from
our paper, is open-sourced at https://github.com/adlnlp/Gali.

</details>


### [202] [Exploring Imbalanced Annotations for Effective In-Context Learning](https://arxiv.org/pdf/2502.04037)
*Hongfu Gao, Feipeng Zhang, Hao Zeng, Deyu Meng, Bingyi Jing, Hongxin Wei*

Main category: cs.CL

TL;DR: The paper addresses class imbalance in in-context learning (ICL) for LLMs, proposing RCB to mitigate bias by reweighting demonstrations based on class weight and conditional bias.


<details>
  <summary>Details</summary>
Motivation: Class imbalances in annotated datasets degrade ICL performance, and traditional rebalancing methods fail due to neglecting conditional bias.

Method: Proposes RCB, which estimates conditional bias using a balanced subset and reweights demonstration scores.

Result: RCB improves average accuracy of current selection methods by up to 5.42%.

Conclusion: RCB effectively enhances ICL performance under class imbalance by addressing both class weight and conditional bias.

Abstract: Large language models (LLMs) have shown impressive performance on downstream
tasks through in-context learning (ICL), which heavily relies on the
demonstrations selected from annotated datasets. However, these datasets often
exhibit long-tailed class distributions in real-world scenarios, leading to
biased demonstration selection. In this work, we show that such class
imbalances significantly degrade the ICL performance across various tasks,
regardless of selection methods. Moreover, classical rebalancing methods, which
focus solely on class weights, yield poor performance due to neglecting
condition bias--skewed feature distributions within classes. To address this,
we propose Reweighting with Conditional Bias (dubbed RCB), a simple and
complementary approach to enhance ICL performance under class imbalance. In
particular, RCB estimates conditional bias using a balanced subset and
re-weights demonstration scores based on both class weight and conditional
bias. In effect, RCB prevents over-selection from dominant classes while
preserving the efficacy of current selection methods. Extensive experiments on
common benchmarks demonstrate the effectiveness of our method, improving the
average accuracy of current selection methods by up to 5.42%.

</details>


### [203] [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/pdf/2502.09284)
*Amirbek Djanibekov, Hanan Aldarmaki*

Main category: cs.CL

TL;DR: The paper introduces a method to integrate self-supervised speech representations with instruction-tuned LLMs for speech-to-text translation, using a modality adapter to align features.


<details>
  <summary>Details</summary>
Motivation: To enable seamless multi-modal processing and speech understanding by combining speech representations with LLMs.

Method: Uses a modality adapter to align self-supervised speech features with instruction-tuned LLMs, tested on English speech data.

Result: Effectively preserves semantic content and bridges speech models with LLMs.

Conclusion: The approach is promising for speech understanding applications.

Abstract: With the growing influence of Large Language Models (LLMs), there is
increasing interest in integrating speech representations with them to enable
more seamless multi-modal processing and speech understanding. This study
introduces a novel approach that combines self-supervised speech
representations with instruction-tuned LLMs for speech-to-text translation. The
proposed approach leverages a modality adapter to align extracted speech
features with instruction-tuned LLMs using English speech data. Our experiments
demonstrate that this method effectively preserves the semantic content of the
input speech and serves as an effective bridge between self-supervised speech
models and instruction-tuned LLMs, offering a promising approach for various
speech understanding applications.

</details>


### [204] [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction](https://arxiv.org/pdf/2502.11084)
*Yuting Huang, Chengyuan Liu, Yifeng Feng, Yiquan Wu, Chao Wu, Fei Wu, Kun Kuang*

Main category: cs.CL

TL;DR: The paper introduces Rewrite to Jailbreak (R2J), a method to exploit LLM vulnerabilities by rewriting instructions, making jailbreaks efficient and hard to detect.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and detectability of existing jailbreak methods, aiming for a more practical attack scenario.

Method: Proposes R2J, which iteratively explores LLM weaknesses by rewriting instructions, improving attack strategies automatically.

Result: R2J is effective, transferable across datasets and models, and requires few queries.

Conclusion: The work highlights LLM safety vulnerabilities and encourages further research.

Abstract: As Large Language Models (LLMs) are widely applied in various domains, the
safety of LLMs is increasingly attracting attention to avoid their powerful
capabilities being misused. Existing jailbreak methods create a forced
instruction-following scenario, or search adversarial prompts with prefix or
suffix tokens to achieve a specific representation manually or automatically.
However, they suffer from low efficiency and explicit jailbreak patterns, far
from the real deployment of mass attacks to LLMs. In this paper, we point out
that simply rewriting the original instruction can achieve a jailbreak, and we
find that this rewriting approach is learnable and transferable. We propose the
Rewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method
to attack LLMs by iteratively exploring the weakness of the LLMs and
automatically improving the attacking strategy. The jailbreak is more efficient
and hard to identify since no additional features are introduced. Extensive
experiments and analysis demonstrate the effectiveness of R2J, and we find that
the jailbreak is also transferable to multiple datasets and various types of
models with only a few queries. We hope our work motivates further
investigation of LLM safety. The code can be found at
https://github.com/ythuang02/R2J/.

</details>


### [205] [VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment](https://arxiv.org/pdf/2502.11361)
*Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah*

Main category: cs.CL

TL;DR: VLDBench is a large-scale benchmark for detecting multimodal disinformation, combining text and images, with 62,000 labeled pairs. It shows visual cues improve detection accuracy by 5-35% over text-only models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in detecting intentional multimodal disinformation (e.g., propaganda) by providing a robust benchmark.

Method: Curated 62,000 text-image pairs from 58 news outlets, annotated by 22 experts. Evaluated LLMs and VLMs.

Result: Incorporating visual cues boosts detection accuracy significantly.

Conclusion: VLDBench supports trustworthy disinformation detection and aligns with AI governance frameworks.

Abstract: Detecting disinformation that blends manipulated text and images has become
increasingly challenging, as AI tools make synthetic content easy to generate
and disseminate. While most existing AI safety benchmarks focus on single
modality misinformation (i.e., false content shared without intent to deceive),
intentional multimodal disinformation, such as propaganda or conspiracy
theories that imitate credible news, remains largely unaddressed. We introduce
the Vision-Language Disinformation Detection Benchmark (VLDBench), the first
large-scale resource supporting both unimodal (text-only) and multimodal (text
+ image) disinformation detection. VLDBench comprises approximately 62,000
labeled text-image pairs across 13 categories, curated from 58 news outlets.
Using a semi-automated pipeline followed by expert review, 22 domain experts
invested over 500 hours to produce high-quality annotations with substantial
inter-annotator agreement. Evaluations of state-of-the-art Large Language
Models (LLMs) and Vision-Language Models (VLMs) on VLDBench show that
incorporating visual cues improves detection accuracy by 5 to 35 percentage
points over text-only models. VLDBench provides data and code for evaluation,
fine-tuning, and robustness testing to support disinformation analysis.
Developed in alignment with AI governance frameworks (e.g., the MIT AI Risk
Repository), VLDBench offers a principled foundation for advancing trustworthy
disinformation detection in multimodal media.
  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:
https://huggingface.co/datasets/vector-institute/VLDBench Code:
https://github.com/VectorInstitute/VLDBench

</details>


### [206] [ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models](https://arxiv.org/pdf/2502.11404)
*Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, Xueqi Cheng*

Main category: cs.CL

TL;DR: ToolCoder reformulates tool learning as code generation, improving planning, error handling, and reuse, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in tool learning like hand-crafted prompts, multi-step planning, and lack of error diagnosis.

Method: Transforms natural language queries into Python functions with comments, uses code generation and execution, and stores reusable functions.

Result: Superior task completion accuracy and execution reliability compared to existing approaches.

Conclusion: Code-centric approaches like ToolCoder are effective for tool learning.

Abstract: Tool learning has emerged as a crucial capability for large language models
(LLMs) to solve complex real-world tasks through interaction with external
tools. Existing approaches face significant challenges, including reliance on
hand-crafted prompts, difficulty in multi-step planning, and lack of precise
error diagnosis and reflection mechanisms. We propose ToolCoder, a novel
framework that reformulates tool learning as a code generation task. Inspired
by software engineering principles, ToolCoder transforms natural language
queries into structured Python function scaffold and systematically breaks down
tasks with descriptive comments, enabling LLMs to leverage coding paradigms for
complex reasoning and planning. It then generates and executes function
implementations to obtain final responses. Additionally, ToolCoder stores
successfully executed functions in a repository to promote code reuse, while
leveraging error traceback mechanisms for systematic debugging, optimizing both
execution efficiency and robustness. Experiments demonstrate that ToolCoder
achieves superior performance in task completion accuracy and execution
reliability compared to existing approaches, establishing the effectiveness of
code-centric approaches in tool learning.

</details>


### [207] [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/pdf/2502.11471)
*Kangyang Luo, Yuzhuo Bai, Cheng Gao, Shuzheng Si, Yingli Shen, Zhu Liu, Zhitong Wang, Cunliang Kong, Wenhao Li, Yufei Huang, Ye Tian, Xuantang Xiong, Lei Han, Maosong Sun*

Main category: cs.CL

TL;DR: GLTW enhances Knowledge Graph Completion (KGC) by integrating structural KG information into Large Language Models (LLMs) using an improved Graph Transformer (iGT) and subgraph-based training.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to integrate KG structural information into LLMs for deterministic KGC predictions.

Method: Proposes GLTW, combining iGT (encoding local/global KG subgraphs) with LLMs, using subgraph-based multi-classification training.

Result: GLTW outperforms state-of-the-art baselines on various KG datasets.

Conclusion: GLTW effectively merges KG structure with LLMs, improving KGC performance.

Abstract: Knowledge Graph Completion (KGC), which aims to infer missing or incomplete
facts, is a crucial task for KGs. However, integrating the vital structural
information of KGs into Large Language Models (LLMs) and outputting predictions
deterministically remains challenging. To address this, we propose a new method
called GLTW, which encodes the structural information of KGs and merges it with
LLMs to enhance KGC performance. Specifically, we introduce an improved Graph
Transformer (iGT) that effectively encodes subgraphs with both local and global
structural information and inherits the characteristics of language model,
bypassing training from scratch. Also, we develop a subgraph-based
multi-classification training objective, using all entities within KG as
classification objects, to boost learning efficiency.Importantly, we combine
iGT with an LLM that takes KG language prompts as input.Our extensive
experiments on various KG datasets show that GLTW achieves significant
performance gains compared to SOTA baselines.

</details>


### [208] [MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training](https://arxiv.org/pdf/2502.11541)
*Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, Tiejun Zhao*

Main category: cs.CL

TL;DR: Proposes MuSC, a self-contrastive training framework for complex instruction alignment in LLMs without relying on advanced models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Existing methods for complex instruction alignment depend on stronger models (e.g., GPT-4), limiting their broader application.

Method: MuSC uses multi-granularity self-contrastive training: coarse-granularity (constraint-aware preference data via instruction decomposition/recombination) and fine-granularity (token-aware preference optimization with dynamic supervision).

Result: Significant improvement on complex and general instruction-following benchmarks, outperforming prior self-alignment methods.

Conclusion: MuSC effectively enhances complex instruction alignment without needing a stronger model, demonstrating broad applicability.

Abstract: Complex instruction-following with elaborate constraints is imperative for
Large Language Models (LLMs). While existing methods have constructed data for
complex instruction alignment, they all rely on a more advanced model,
especially GPT-4, limiting their application. In this paper, we propose a
Multi-granularity Self-Contrastive Training (MuSC) framework, to improve the
complex instruction alignment without relying on a stronger model. Our method
is conducted on both coarse and fine granularity. On coarse-granularity, we
construct constraint-aware preference data based on instruction decomposition
and recombination. On fine-granularity, we perform token-aware preference
optimization with dynamic token-level supervision. Our method is evaluated on
open-sourced models, and experiment results show our method achieves
significant improvement on both complex and general instruction-following
benchmarks, surpassing previous self-alignment methods.

</details>


### [209] ["See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models](https://arxiv.org/pdf/2502.11718)
*Jihao Gu, Yingyao Wang, Pi Bu, Chen Wang, Ziming Wang, Tengtao Song, Donglai Wei, Jiale Yuan, Yingxiu Zhao, Yancheng He, Shilong Li, Jiaheng Liu, Meng Cao, Jun Song, Yingshui Tan, Xiang Li, Wenbo Su, Zhicheng Zheng, Xiaoyong Zhu, Bo Zheng*

Main category: cs.CL

TL;DR: The paper introduces ChineseSimpleVQA, a factuality-based visual question-answering benchmark in Chinese, to evaluate LVLMs' visual factuality across diverse topics.


<details>
  <summary>Details</summary>
Motivation: Factual accuracy evaluation in LVLMs has not kept pace with their rapid development, limiting understanding of their knowledge capacity and reliability.

Method: The benchmark includes a rigorous data construction pipeline, decouples visual factuality into object recognition and knowledge discovery, and evaluates 34 models.

Result: The evaluation reveals critical performance gaps among LVLMs, highlighting their capability boundaries.

Conclusion: The benchmark and open-sourced resources provide a foundation for assessing and improving LVLMs' visual factuality.

Abstract: The evaluation of factual accuracy in large vision language models (LVLMs)
has lagged behind their rapid development, making it challenging to fully
reflect these models' knowledge capacity and reliability. In this paper, we
introduce the first factuality-based visual question-answering benchmark in
Chinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of
LVLMs across 8 major topics and 56 subtopics. The key features of this
benchmark include a focus on the Chinese language, diverse knowledge types, a
multi-hop question construction, high-quality data, static consistency, and
easy-to-evaluate through short answers. Moreover, we contribute a rigorous data
construction pipeline and decouple the visual factuality into two parts: seeing
the world (i.e., object recognition) and discovering knowledge. This decoupling
allows us to analyze the capability boundaries and execution mechanisms of
LVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source
models, revealing critical performance gaps within this field. Our
evaluation-friendly code and data have already been open-sourced.

</details>


### [210] [CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs](https://arxiv.org/pdf/2502.12476)
*Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani*

Main category: cs.CL

TL;DR: CoCo-CoLa metric evaluates language adherence in multilingual LLMs, revealing biases and proposing a partial fine-tuning strategy for efficient multilingual adaptation.


<details>
  <summary>Details</summary>
Motivation: Multilingual LLMs often favor high-resource languages like English, struggling to generate responses in intended languages despite cross-lingual abilities.

Method: Introduces CoCo-CoLa metric, conducts fine-tuning experiments on a QA task across seven languages, and identifies language-specific layers. Proposes partial training of key layers.

Result: Multilingual models share task knowledge but show output language biases. Partial fine-tuning improves adherence and reduces computational costs.

Conclusion: Partial fine-tuning of key layers enhances multilingual performance efficiently, especially for low-resource languages, matching or surpassing full fine-tuning.

Abstract: Multilingual Large Language Models (LLMs) develop cross-lingual abilities
despite being trained on limited parallel data. However, they often struggle to
generate responses in the intended language, favoring high-resource languages
such as English. In this work, we introduce CoCo-CoLa (Correct Concept -
Correct Language), a novel metric to evaluate language adherence in
multilingual LLMs. Using fine-tuning experiments on a closed-book QA task
across seven languages, we analyze how training in one language affects others'
performance. Our findings reveal that multilingual models share task knowledge
across languages but exhibit biases in the selection of output language. We
identify language-specific layers, showing that final layers play a crucial
role in determining output language. Accordingly, we propose a partial training
strategy that selectively fine-tunes key layers, improving language adherence
while significantly reducing computational cost. Our method achieves comparable
or superior performance to full fine-tuning, particularly for low-resource
languages, offering a more efficient multilingual adaptation.

</details>


### [211] [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/pdf/2502.14494)
*Jinnan Li, Jinzhe Li, Yue Wang, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: StructFlowBench is a new benchmark for evaluating multi-turn instruction following in LLMs, focusing on structural dependencies between dialogue turns, revealing current models' deficiencies.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook structural dependencies in multi-turn dialogues, which are crucial for real-world LLM applications.

Method: Proposes StructFlowBench with a structural flow framework (six inter-turn relationships) and uses LLM-based automatic evaluation on 13 models.

Result: Significant deficiencies in current models' comprehension of multi-turn dialogue structures were found.

Conclusion: StructFlowBench addresses a critical gap in LLM evaluation and highlights the need for improved structural understanding in models.

Abstract: Multi-turn instruction following capability constitutes a core competency of
large language models (LLMs) in real-world applications. Existing evaluation
benchmarks predominantly focus on fine-grained constraint satisfaction and
domain-specific capability assessment, yet overlook the crucial structural
dependencies between dialogue turns that distinguish multi-turn from
single-turn interactions. These structural dependencies not only reflect user
intent but also establish an essential second dimension for the instruction
following evaluation beyond constraint satisfaction. To address this gap, we
propose StructFlowBench, a multi-turn instruction following benchmark with
structural flow modeling. The benchmark defines an innovative structural flow
framework with six fundamental inter-turn relationships. These relationships
introduce novel structural constraints for model evaluation and also serve as
generation parameters for creating customized dialogue flows tailored to
specific scenarios. Adopting established LLM-based automatic evaluation
methodologies, we conduct systematic evaluations of 13 leading open-source and
closed-source LLMs. Experimental results reveal significant deficiencies in
current models' comprehension of multi-turn dialogue structures. The code is
available at https://github.com/MLGroupJLU/StructFlowBench.

</details>


### [212] [Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs](https://arxiv.org/pdf/2502.14561)
*Paris Koloveas, Serafeim Chatzopoulos, Thanasis Vergoulis, Christos Tryfonopoulos*

Main category: cs.CL

TL;DR: Open LLMs can predict citation intent effectively with minimal task-specific data, outperforming traditional methods through in-context learning and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore the adaptability of general-purpose LLMs for citation intent prediction, avoiding reliance on domain-specific models.

Method: Evaluated 12 model variations across 5 LLM families using zero-, one-, few-, and many-shot prompting, followed by fine-tuning.

Result: Achieved 8% F1-score improvement on SciCite and 4.3% on ACL-ARC datasets after fine-tuning.

Conclusion: General-purpose LLMs are viable for citation intent prediction, with fine-tuning and prompt engineering enhancing performance.

Abstract: This work investigates the ability of open Large Language Models (LLMs) to
predict citation intent through in-context learning and fine-tuning. Unlike
traditional approaches relying on domain-specific pre-trained models like
SciBERT, we demonstrate that general-purpose LLMs can be adapted to this task
with minimal task-specific data. We evaluate twelve model variations across
five prominent open LLM families using zero-, one-, few-, and many-shot
prompting. Our experimental study identifies the top-performing model and
prompting parameters through extensive in-context learning experiments. We then
demonstrate the significant impact of task-specific adaptation by fine-tuning
this model, achieving a relative F1-score improvement of 8% on the SciCite
dataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned
baseline. These findings provide valuable insights for model selection and
prompt engineering. Additionally, we make our end-to-end evaluation framework
and models openly available for future use.

</details>


### [213] [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/pdf/2502.14662)
*Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper critiques traditional recommender systems for prioritizing platform benefits over user interests and proposes a new user-agent-platform paradigm to protect users.


<details>
  <summary>Details</summary>
Motivation: Traditional recommender systems often prioritize commercial goals and overlook individual preferences, leading to user vulnerabilities like lack of control and echo chambers.

Method: The paper introduces a user-agent-platform paradigm, where an agent acts as a protective intermediary between users and the system.

Result: The proposed paradigm aims to mitigate issues like manipulation, lack of personalization, and echo chambers by enabling indirect exposure.

Conclusion: The new paradigm addresses core flaws in traditional systems, offering a solution to better protect user interests.

Abstract: Traditional recommender systems usually take the user-platform paradigm,
where users are directly exposed under the control of the platform's
recommendation algorithms. However, the defect of recommendation algorithms may
put users in very vulnerable positions under this paradigm. First, many
sophisticated models are often designed with commercial objectives in mind,
focusing on the platform's benefits, which may hinder their ability to protect
and capture users' true interests. Second, these models are typically optimized
using data from all users, which may overlook individual user's preferences.
Due to these shortcomings, users may experience several disadvantages under the
traditional user-platform direct exposure paradigm, such as lack of control
over the recommender system, potential manipulation by the platform, echo
chamber effects, or lack of personalization for less active users due to the
dominance of active users during collaborative learning. Therefore, there is an
urgent need to develop a new paradigm to protect user interests and alleviate
these issues. Recently, some researchers have introduced LLM agents to simulate
user behaviors, these approaches primarily aim to optimize platform-side
performance, leaving core issues in recommender systems unresolved. To address
these limitations, we propose a new user-agent-platform paradigm, where agent
serves as the protective shield between user and recommender system that
enables indirect exposure.

</details>


### [214] [Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs](https://arxiv.org/pdf/2502.14830)
*Danni Liu, Jan Niehues*

Main category: cs.CL

TL;DR: The paper proposes a middle-layer alignment method to improve cross-lingual transfer in LLMs, showing consistent gains in performance, especially for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Extending the benefits of fine-tuned LLMs to diverse languages is challenging due to performance gaps and data scarcity.

Method: Analyzed LLM internal representations across 1,000+ language pairs, identified middle layers for alignment, and integrated a middle-layer alignment objective into task-specific training.

Result: Improved cross-lingual transfer in tasks like slot filling, machine translation, and structured text generation, with robustness to alignment language choice and generalization to unseen languages.

Conclusion: Middle-layer alignment enhances cross-lingual capabilities without full re-training, offering a scalable solution for multilingual applications.

Abstract: While large language models demonstrate remarkable capabilities at
task-specific applications through fine-tuning, extending these benefits across
diverse languages is essential for broad accessibility. However, effective
cross-lingual transfer is hindered by LLM performance gaps across languages and
the scarcity of fine-tuning data in many languages. Through analysis of LLM
internal representations from over 1,000+ language pairs, we discover that
middle layers exhibit the strongest potential for cross-lingual alignment.
Building on this finding, we propose a middle-layer alignment objective
integrated into task-specific training. Our experiments on slot filling,
machine translation, and structured text generation show consistent
improvements in cross-lingual transfer, especially to lower-resource languages.
The method is robust to the choice of alignment languages and generalizes to
languages unseen during alignment. Furthermore, we show that separately trained
alignment modules can be merged with existing task-specific modules, improving
cross-lingual capabilities without full re-training. Our code is publicly
available (https://github.com/dannigt/mid-align).

</details>


### [215] [TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding](https://arxiv.org/pdf/2502.15197)
*Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, Bryan Kian Hsiang Low*

Main category: cs.CL

TL;DR: TETRIS optimizes batch speculative decoding for multi-request settings, improving throughput by selecting the most promising draft tokens, reducing wasted resources.


<details>
  <summary>Details</summary>
Motivation: To enhance inference efficiency in LLMs by minimizing wasted computing resources and maximizing throughput in multi-request settings.

Method: TETRIS actively selects the most promising draft tokens for each request in a batch, verified in parallel, reducing rejections.

Result: Higher acceptance rate and better utilization of limited inference capacity compared to baseline and existing methods.

Conclusion: TETRIS outperforms existing methods, offering more efficient batch inference for LLMs.

Abstract: We propose TETRIS, a novel method that optimizes the total throughput of
batch speculative decoding in multi-request settings. Unlike existing methods
that optimize for a single request or a group of requests as a whole, TETRIS
actively selects the most promising draft tokens (for every request in a batch)
to be accepted when verified in parallel, resulting in fewer rejected tokens
and hence less wasted computing resources. Such an effective resource
utilization to achieve fast inference in large language models (LLMs) is
especially important to service providers with limited inference capacity.
Compared to baseline speculative decoding, TETRIS yields a consistently higher
acceptance rate and more effective utilization of the limited inference
capacity. We show theoretically and empirically that TETRIS outperforms
baseline speculative decoding and existing methods that dynamically select
draft tokens, leading to a more efficient batch inference in LLMs.

</details>


### [216] [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/pdf/2502.15401)
*Xuetao Ma, Wenbin Jiang, Hua Huang*

Main category: cs.CL

TL;DR: A curriculum ICL strategy improves LLMs' reasoning by selecting and ordering examples based on problem-solving logic, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing ICL methods use simple features for example relevance, which fail to capture intrinsic connections.

Method: Proposes a curriculum ICL strategy: selects examples by analyzing problem-solving logic, orders them from easy to hard using curriculum learning.

Result: Outperforms previous ICL approaches in performance and efficiency on multiple benchmarks.

Conclusion: The method effectively enhances LLMs' complex reasoning capabilities.

Abstract: In-context learning (ICL) can significantly enhance the complex reasoning
capabilities of large language models (LLMs), with the key lying in the
selection and ordering of demonstration examples. Previous methods typically
relied on simple features to measure the relevance between examples. We argue
that these features are not sufficient to reflect the intrinsic connections
between examples. In this study, we propose a curriculum ICL strategy guided by
problem-solving logic. We select demonstration examples by analyzing the
problem-solving logic and order them based on curriculum learning.
Specifically, we constructed a problem-solving logic instruction set based on
the BREAK dataset and fine-tuned a language model to analyze the
problem-solving logic of examples. Subsequently, we selected appropriate
demonstration examples based on problem-solving logic and assessed their
difficulty according to the number of problem-solving steps. In accordance with
the principles of curriculum learning, we ordered the examples from easy to
hard to serve as contextual prompts. Experimental results on multiple
benchmarks indicate that our method outperforms previous ICL approaches in
terms of performance and efficiency, effectively enhancing the complex
reasoning capabilities of LLMs. Our project will be released at
https://github.com/maxuetao/CurriculumICL

</details>


### [217] [Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation](https://arxiv.org/pdf/2502.15434)
*Yue Zhou, Yi Chang, Yuan Wu*

Main category: cs.CL

TL;DR: M3 is a model merging method using randomized linear interpolation with Beta-distributed coefficients to improve performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods overlook varying contribution ratios of task-specific models, limiting merged model effectiveness.

Method: M3 employs randomized linear interpolation in parameter space between task-specific LLMs, sampling coefficients from a Beta distribution.

Result: M3 improves merged model performance, robustness, and outperforms DARE, with tunable exploration efficiency.

Conclusion: M3 offers a simple, effective solution for model merging, enhancing performance and robustness through controlled randomness.

Abstract: Model merging aims to integrate multiple task-specific models into a unified
model that inherits the capabilities of the task-specific models, without
additional training. Existing model merging methods often lack consideration of
the varying contribution ratios of different task-specific models to the final
merged model. In this paper, we propose Mixup Model Merge (M3), a simple yet
effective method inspired by the randomized linear interpolation strategy from
the Mixup data augmentation technique. M3 performs randomized linear
interpolation in parameter space between two task-specific LLMs, where
interpolation coefficients are sampled from a Beta distribution to explore
diverse contribution ratios. This controllable randomness allows M3 to
outperform standard equal-ratio merging by discovering better contribution
ratio combinations. Extensive experiments show that M3 significantly (1)
improves merged LLM performance across tasks, (2) enhances out-of-distribution
and adversarial robustness, and (3) outperforms the positive effects of the
sparsification method DARE on model merging and can be further combined with
DARE to achieve superior results. By tuning the Beta distribution's shape
parameters, (4) M3 balances exploration efficiency and diversity in
contribution ratios. The code is available at:
https://github.com/MLGroupJLU/MixupModelMerge

</details>


### [218] [All That Glitters is Not Novel: Plagiarism in AI Generated Research](https://arxiv.org/pdf/2502.16487)
*Tarun Gupta, Danish Pruthi*

Main category: cs.CL

TL;DR: The paper highlights concerns about plagiarism in LLM-generated research documents, with 24% being directly copied or paraphrased from existing work, and 32% partially overlapping. Automated detectors fail to catch these issues, urging careful assessment of such research.


<details>
  <summary>Details</summary>
Motivation: To address the growing optimism around autonomous research agents by exposing the prevalence of plagiarism in LLM-generated research documents and the inadequacy of current detection methods.

Method: Experts evaluated 50 LLM-generated research documents for similarities with existing work, cross-verified by original authors. Controlled experiments tested automated plagiarism detectors.

Result: 24% of documents were plagiarized (paraphrased or borrowed), 32% partially overlapped, and few were original. Automated detectors failed to identify plagiarized content.

Conclusion: LLM-generated research requires rigorous assessment due to widespread plagiarism and detector limitations, impacting academic publishing integrity.

Abstract: Automating scientific research is considered the final frontier of science.
Recently, several papers claim autonomous research agents can generate novel
research ideas. Amidst the prevailing optimism, we document a critical concern:
a considerable fraction of such research documents are smartly plagiarized.
Unlike past efforts where experts evaluate the novelty and feasibility of
research ideas, we request $13$ experts to operate under a different
situational logic: to identify similarities between LLM-generated research
documents and existing work. Concerningly, the experts identify $24\%$ of the
$50$ evaluated research documents to be either paraphrased (with one-to-one
methodological mapping), or significantly borrowed from existing work. These
reported instances are cross-verified by authors of the source papers. Experts
find an additional $32\%$ ideas to partially overlap with prior work, and a
small fraction to be completely original. Problematically, these LLM-generated
research documents do not acknowledge original sources, and bypass inbuilt
plagiarism detectors. Lastly, through controlled experiments we show that
automated plagiarism detectors are inadequate at catching plagiarized ideas
from such systems. We recommend a careful assessment of LLM-generated research,
and discuss the implications of our findings on academic publishing.

</details>


### [219] [All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark](https://arxiv.org/pdf/2502.16989)
*Davide Testa, Giovanni Bonetta, Raffaella Bernardi, Alessandro Bondielli, Alessandro Lenci, Alessio Miaschi, Lucia Passaro, Bernardo Magnini*

Main category: cs.CL

TL;DR: MAIA is a native-Italian benchmark for evaluating VLMs on video-related tasks, focusing on reasoning abilities, cultural relevance, and multimodal consistency.


<details>
  <summary>Details</summary>
Motivation: To assess VLMs' reasoning abilities in a fine-grained manner, with a focus on Italian language and culture, while disentangling vision-language relations.

Method: Uses two tasks: visual statement verification and open-ended visual question-answering, across twelve reasoning categories, with an aggregated metric for evaluation.

Result: Low scores reveal VLMs' fragility in consistency and visually grounded language comprehension/generation.

Conclusion: MAIA highlights the need for improved VLMs in handling multimodal tasks, especially in culturally specific contexts.

Abstract: We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark
designed for fine-grained investigation of the reasoning abilities of visual
language models on videos. MAIA differs from other available video benchmarks
for its design, its reasoning categories, the metric it uses, and the language
and culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two
aligned tasks: a visual statement verification task and an open-ended visual
question-answering task, both on the same set of video-related questions. It
considers twelve reasoning categories that aim to disentangle language and
vision relations by highlighting the role of the visual input. Thanks to its
carefully taught design, it evaluates VLMs' consistency and visually grounded
natural language comprehension and generation simultaneously through an
aggregated metric revealing low results that highlight models' fragility. Last
but not least, the video collection has been carefully selected to reflect the
Italian culture, and the language data are produced by native-speakers.

</details>


### [220] [Do Language Models Understand Honorific Systems in Javanese?](https://arxiv.org/pdf/2502.20864)
*Mohammad Rifqi Farhansyah, Iwan Darmawan, Adryan Kusumawardhana, Genta Indra Winata, Alham Fikri Aji, Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: The paper introduces Unggah-Ungguh, a dataset for Javanese honorifics, and evaluates language models' performance in processing and translating these honorifics, revealing biases and limitations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive corpus for Javanese honorifics in NLP tasks and assess language models' ability to handle these linguistic nuances.

Method: Curated the Unggah-Ungguh dataset, tested LMs on classification, machine translation, and conversational tasks for Javanese honorifics.

Result: Current LMs struggle with most honorific levels, showing bias toward certain tiers.

Conclusion: The study highlights the challenges LMs face in processing Javanese honorifics, emphasizing the need for improved datasets and models.

Abstract: The Javanese language features a complex system of honorifics that vary
according to the social status of the speaker, listener, and referent. Despite
its cultural and linguistic significance, there has been limited progress in
developing a comprehensive corpus to capture these variations for natural
language processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a
carefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh
Basa, the Javanese speech etiquette framework that dictates the choice of words
and phrases based on social hierarchy and context. Using Unggah-Ungguh, we
assess the ability of language models (LMs) to process various levels of
Javanese honorifics through classification and machine translation tasks. To
further evaluate cross-lingual LMs, we conduct machine translation experiments
between Javanese (at specific honorific levels) and Indonesian. Additionally,
we explore whether LMs can generate contextually appropriate Javanese
honorifics in conversation tasks, where the honorific usage should align with
the social role and contextual cues. Our findings indicate that current LMs
struggle with most honorific levels, exhibitinga bias toward certain honorific
tiers.

</details>


### [221] [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/pdf/2503.01372)
*Joel Niklaus, Jakob Merane, Luka Nenadic, Sina Ahmadi, Yingqiang Gao, Cyrill A. H. Chevalley, Claude Humbel, Christophe Gösken, Lorenzo Tanzi, Thomas Lüthi, Stefan Palombo, Spencer Poff, Boling Yang, Nan Wu, Matthew Guillod, Robin Mamié, Daniel Brunner, Julio Pereyra, Niko Grupen*

Main category: cs.CL

TL;DR: SwiLTra-Bench, a multilingual Swiss legal translation benchmark, evaluates LLM-based systems, showing frontier models outperform specialized ones, with fine-tuned SLMs lagging behind top zero-shot models like Claude-3.5-Sonnet. SwiLTra-Judge aligns well with human evaluations.


<details>
  <summary>Details</summary>
Motivation: Address bottlenecks in Swiss legal translation by evaluating LLM-based systems to improve access to justice.

Method: Introduce SwiLTra-Bench (180K aligned legal texts) and SwiLTra-Judge for evaluation, testing frontier and specialized models.

Result: Frontier models excel overall; specialized models perform well in laws but poorly in headnotes. Fine-tuned SLMs improve but trail top zero-shot models.

Conclusion: LLMs show promise for legal translation, but frontier models lead, and SwiLTra-Judge aids reliable evaluation.

Abstract: In Switzerland legal translation is uniquely important due to the country's
four official languages and requirements for multilingual legal documentation.
However, this process traditionally relies on professionals who must be both
legal experts and skilled translators -- creating bottlenecks and impacting
effective access to justice. To address this challenge, we introduce
SwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned
Swiss legal translation pairs comprising laws, headnotes, and press releases
across all Swiss languages along with English, designed to evaluate LLM-based
translation systems. Our systematic evaluation reveals that frontier models
achieve superior translation performance across all document types, while
specialized translation systems excel specifically in laws but under-perform in
headnotes. Through rigorous testing and human expert validation, we demonstrate
that while fine-tuning open SLMs significantly improves their translation
quality, they still lag behind the best zero-shot prompted frontier models such
as Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM
evaluation system that aligns best with human expert assessments.

</details>


### [222] [Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering](https://arxiv.org/pdf/2503.01606)
*Zhanghao Hu, Hanqi Yan, Qinglin Zhu, Zhenyi Shen, Yulan He, Lin Gui*

Main category: cs.CL

TL;DR: EmbQA improves ODQA by refining query representations and diversifying candidate generation, outperforming baselines in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current retriever-reader pipelines in ODQA suffer from high computational overhead, instability, and suboptimal retrieval coverage.

Method: EmbQA enhances retriever and reader via unsupervised contrastive learning for query refinement and introduces an exploratory embedding with entropy-based selection.

Result: Extensive experiments show EmbQA outperforms baselines in accuracy and efficiency across multiple benchmarks.

Conclusion: EmbQA effectively addresses shortcomings of existing ODQA pipelines, offering a more robust and efficient solution.

Abstract: Large language models have recently pushed open domain question answering
(ODQA) to new frontiers. However, prevailing retriever-reader pipelines often
depend on multiple rounds of prompt level instructions, leading to high
computational overhead, instability, and suboptimal retrieval coverage. In this
paper, we propose EmbQA, an embedding-level framework that alleviates these
shortcomings by enhancing both the retriever and the reader. Specifically, we
refine query representations via lightweight linear layers under an
unsupervised contrastive learning objective, thereby reordering retrieved
passages to highlight those most likely to contain correct answers.
Additionally, we introduce an exploratory embedding that broadens the model's
latent semantic space to diversify candidate generation and employs an
entropy-based selection mechanism to choose the most confident answer
automatically. Extensive experiments across three open-source LLMs, three
retrieval methods, and four ODQA benchmarks demonstrate that EmbQA
substantially outperforms recent baselines in both accuracy and efficiency.

</details>


### [223] [Vision-Language Models Struggle to Align Entities across Modalities](https://arxiv.org/pdf/2503.03854)
*Iñigo Alonso, Gorka Azkune, Ander Salaberria, Jeremy Barnes, Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: The paper introduces MATE, a benchmark for cross-modal entity linking, showing VLMs lag behind humans, especially in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Cross-modal entity linking is crucial for applications like multimodal code generation and fake news detection but lacks thorough study.

Method: A QA task evaluates VLMs and humans on retrieving attributes across modalities using the MATE benchmark (5.5k instances).

Result: VLMs perform poorly compared to humans, with chain-of-thought prompting offering limited improvement.

Conclusion: MATE is a valuable benchmark, highlighting the need for more research in cross-modal entity linking.

Abstract: Cross-modal entity linking refers to the ability to align entities and their
attributes across different modalities. While cross-modal entity linking is a
fundamental skill needed for real-world applications such as multimodal code
generation, fake news detection, or scene understanding, it has not been
thoroughly studied in the literature. In this paper, we introduce a new task
and benchmark to address this gap. Our benchmark, MATE, consists of 5.5k
evaluation instances featuring visual scenes aligned with their textual
representations. To evaluate cross-modal entity linking performance, we design
a question-answering task that involves retrieving one attribute of an object
in one modality based on a unique attribute of that object in another modality.
We evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this
task, and find that VLMs struggle significantly compared to humans,
particularly as the number of objects in the scene increases. Our analysis also
shows that, while chain-of-thought prompting can improve VLM performance,
models remain far from achieving human-level proficiency. These findings
highlight the need for further research in cross-modal entity linking and show
that MATE is a strong benchmark to support that progress.

</details>


### [224] [HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks](https://arxiv.org/pdf/2503.04378)
*Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev*

Main category: cs.CL

TL;DR: The paper introduces a method for inference-time scaling in open-ended tasks using Feedback and Edit Models, achieving state-of-the-art performance on the Arena Hard benchmark.


<details>
  <summary>Details</summary>
Motivation: Current inference-time scaling techniques are limited to verifiable tasks like math or coding. The paper aims to extend this to open-ended domains by mimicking human iterative feedback processes.

Method: A three-model setup is used: one generates initial responses, another provides feedback, and a third edits the response. Performance is scaled by increasing drafts, feedback, and edits.

Result: The method achieves 92.7 on Arena Hard, outperforming OpenAI o1 (90.4) and DeepSeek R1 (92.3).

Conclusion: The approach successfully extends inference-time scaling to open-ended tasks, demonstrating significant performance gains.

Abstract: Inference-Time Scaling has been critical to the success of recent models such
as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for
inference-time scaling require tasks to have answers that can be verified,
limiting their application to domains such as math, coding and logical
reasoning. We take inspiration from how humans make first attempts, ask for
detailed feedback from others and make improvements based on such feedback
across a wide spectrum of open-ended endeavors. To this end, we collect
HelpSteer3 data to train dedicated Feedback and Edit Models that are capable of
performing inference-time scaling for open-ended general-domain tasks. In our
setup, one model generates an initial response, which are given feedback by a
second model, that are then used by a third model to edit the response. We show
that performance on Arena Hard, a benchmark strongly predictive of Chatbot
Arena Elo can be boosted by scaling the number of initial response drafts,
effective feedback and edited responses. When scaled optimally, our setup based
on 70B models from the Llama 3 family can reach SoTA performance on Arena Hard
at 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and
DeepSeek R1 with 92.3.

</details>


### [225] [ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs](https://arxiv.org/pdf/2503.05268)
*Francesco Cazzaro, Justin Kleindienst, Sofia Marquez Gomez, Ariadna Quattoni*

Main category: cs.CL

TL;DR: ZOGRASCOPE is a benchmark for property graphs and Cypher queries, addressing the lack of resources in semantic parsing research.


<details>
  <summary>Details</summary>
Motivation: The need for natural language interfaces to knowledge graphs, especially property graphs, is growing, but research resources are lacking.

Method: Introduces ZOGRASCOPE, a benchmark with diverse, manually annotated Cypher queries, and tests LLMs in various learning settings.

Result: The benchmark provides a resource for evaluating semantic parsing on property graphs.

Conclusion: ZOGRASCOPE fills a gap in semantic parsing research for property graphs and enables future evaluations.

Abstract: In recent years, the need for natural language interfaces to knowledge graphs
has become increasingly important since they enable easy and efficient access
to the information contained in them. In particular, property graphs (PGs) have
seen increased adoption as a means of representing complex structured
information. Despite their growing popularity in industry, PGs remain
relatively underrepresented in semantic parsing research with a lack of
resources for evaluation. To address this gap, we introduce ZOGRASCOPE, a
benchmark designed specifically for PGs and queries written in Cypher. Our
benchmark includes a diverse set of manually annotated queries of varying
complexity and is organized into three partitions: iid, compositional and
length. We complement this paper with a set of experiments that test the
performance of different LLMs in a variety of learning settings.

</details>


### [226] [DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs](https://arxiv.org/pdf/2503.07067)
*Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun*

Main category: cs.CL

TL;DR: DistiLLM-2 introduces a contrastive approach for LLM distillation, optimizing performance by aligning teacher and student models across diverse data types.


<details>
  <summary>Details</summary>
Motivation: Prior distillation methods use identical loss functions for teacher and student data, missing the synergy between loss formulations and data types, leading to suboptimal results.

Method: DistiLLM-2 employs a contrastive approach, increasing the likelihood of teacher responses while decreasing student responses to leverage this synergy.

Result: Experiments show DistiLLM-2 builds high-performing student models for tasks like instruction-following and code generation, and supports applications like preference alignment and vision-language extensions.

Conclusion: The contrastive approach in DistiLLM-2 enhances LLM distillation by effectively aligning teacher and student models across varied data types.

Abstract: Despite the success of distillation in large language models (LLMs), most
prior work applies identical loss functions to both teacher- and
student-generated data. These strategies overlook the synergy between loss
formulations and data types, leading to a suboptimal performance boost in
student models. To address this, we propose DistiLLM-2, a contrastive approach
that simultaneously increases the likelihood of teacher responses and decreases
that of student responses by harnessing this synergy. Our extensive experiments
show that DistiLLM-2 not only builds high-performing student models across a
wide range of tasks, including instruction-following and code generation, but
also supports diverse applications, such as preference alignment and
vision-language extensions. These findings highlight the potential of a
contrastive approach to enhance the efficacy of LLM distillation by effectively
aligning teacher and student models across varied data types.

</details>


### [227] [TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification](https://arxiv.org/pdf/2503.15289)
*Junnan Zhu, Min Xiao, Yining Wang, Feifei Zhai, Yu Zhou, Chengqing Zong*

Main category: cs.CL

TL;DR: The TROVE challenge aims to trace text provenance by linking target sentences to source sentences and annotating their relationships. A dataset was created for benchmarking, and 11 LLMs were evaluated, showing retrieval augmentation improves performance.


<details>
  <summary>Details</summary>
Motivation: Address concerns about content reliability and accountability in LLMs by understanding how text is formed from sources.

Method: Introduce the TROVE challenge, construct a dataset from public sources, and evaluate LLMs using direct prompting and retrieval-augmented paradigms.

Result: Retrieval is crucial for performance; larger models excel in complex tasks, and open-source models show promise with retrieval augmentation.

Conclusion: TROVE provides a framework for tracing text provenance, with retrieval augmentation enhancing LLM performance.

Abstract: LLMs have achieved remarkable fluency and coherence in text generation, yet
their widespread adoption has raised concerns about content reliability and
accountability. In high-stakes domains, it is crucial to understand where and
how the content is created. To address this, we introduce the Text pROVEnance
(TROVE) challenge, designed to trace each sentence of a target text back to
specific source sentences within potentially lengthy or multi-document inputs.
Beyond identifying sources, TROVE annotates the fine-grained relationships
(quotation, compression, inference, and others), providing a deep understanding
of how each target sentence is formed. To benchmark TROVE, we construct our
dataset by leveraging three public datasets covering 11 diverse scenarios
(e.g., QA and summarization) in English and Chinese, spanning source texts of
varying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and
long-document settings essential for provenance. To ensure high-quality data,
we employ a three-stage annotation process: sentence retrieval, GPT-4o
provenance, and human provenance. We evaluate 11 LLMs under direct prompting
and retrieval-augmented paradigms, revealing that retrieval is essential for
robust performance, larger models perform better in complex relationship
classification, and closed-source models often lead, yet open-source models
show significant promise, particularly with retrieval augmentation. We make our
dataset available here: https://github.com/ZNLP/ZNLP-Dataset.

</details>


### [228] [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/pdf/2503.18491)
*Shuo Yang, Siwen Luo, Soyeon Caren Han, Eduard Hovy*

Main category: cs.CL

TL;DR: MAGIC-VQA enhances VQA by integrating commonsense knowledge with LVLMs through explicit knowledge integration, contextual refinement, and GNN-based reasoning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LVLMs lack integrated commonsense knowledge, limiting robustness in real-world VQA scenarios.

Method: Three-stage process: (1) Explicit Knowledge Integration, (2) By-Type Post-Processing, (3) Implicit Knowledge Augmentation using GNN.

Result: State-of-the-art performance on benchmark datasets, with significant improvement in commonsense reasoning.

Conclusion: MAGIC-VQA bridges the gap between commonsense knowledge and LVLM-driven reasoning without extensive pre-training or prompt tuning.

Abstract: Visual Question Answering (VQA) requires reasoning across visual and textual
modalities, yet Large Vision-Language Models (LVLMs) often lack integrated
commonsense knowledge, limiting their robustness in real-world scenarios. To
address this, we introduce MAGIC-VQA, a novel framework that enhances VQA by
systematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs
a three-stage process: (1) Explicit Knowledge Integration from external
sources, (2) By-Type Post-Processing for contextual refinement, and (3)
Implicit Knowledge Augmentation using a Graph Neural Network (GNN) for
structured reasoning. While GNNs bring greater depth to structured inference,
they enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key
gap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating
the need for extensive pre-training or complex prompt tuning. Our framework
achieves state-of-the-art performance on benchmark datasets, significantly
improving commonsense reasoning in VQA.

</details>


### [229] [Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment](https://arxiv.org/pdf/2503.18991)
*Ruoxi Cheng, Haoxuan Ma, Weixin Wang, Zhiqiang Wang, Xiaoshuang Jia, Simeng Qin, Xiaochun Cao, Yang Liu, Xiaojun Jia*

Main category: cs.CL

TL;DR: DR-IRL dynamically adjusts rewards via inverse reinforcement learning, using a balanced safety dataset and GRPO-S for task-aware optimization, outperforming baselines in safety alignment.


<details>
  <summary>Details</summary>
Motivation: Existing alignment techniques for LLMs face challenges like imbalanced safety datasets and static reward models, limiting robustness and efficiency.

Method: DR-IRL constructs a balanced safety dataset using CoD prompts, trains category-specific reward models via IRL, and aligns LLMs with GRPO-S, scaling rewards to task difficulty.

Result: DR-IRL outperforms baselines in safety alignment while maintaining usefulness, validated on multiple benchmarks and LLMs.

Conclusion: DR-IRL addresses key limitations in LLM alignment, offering a robust and efficient solution for safety and performance.

Abstract: Robust alignment is vital for safely deploying large language models (LLMs).
Existing techniques are either reward-based -- training a reward model on
preference pairs and optimizing with reinforcement learning (RL) -- or
reward-free -- directly fine-tuning on ranked outputs. Recent research shows
that well-tuned reward-based pipelines remain the most robust, and
single-response demonstrations can outperform pairwise preference data.
However, two key challenges remain: (i) imbalanced safety datasets that
over-represent common hazards while neglecting long-tail threats; and (ii)
static reward models that ignore task difficulty, limiting optimization
efficiency and attainable gains. To address these limitations, we propose
\textbf{DR-IRL}, which dynamically adjusts rewards through inverse
reinforcement learning. We first construct a balanced safety dataset of seven
harmful categories using Chain-of-Draft (CoD) template prompts, which reduce
token usage and generation time compared to Chain-of-Thought (CoT). We then
train category-specific reward models on this dataset via IRL. Finally, to
align the LLM, we introduce \textbf{GRPO-S} (Group Relative Policy
Optimization--Scaling), a variant of GRPO that scales the reward during
optimization to task difficulty -- data-level hardness measured by CLIP
similarity and model-level responsiveness measured by reward gaps. Extensive
experiments on multiple benchmarks and LLMs demonstrate that DR-IRL outperforms
all baselines in safety alignment while maintaining usefulness.

</details>


### [230] [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/pdf/2503.22353)
*Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman*

Main category: cs.CL

TL;DR: The paper introduces a framework for evaluating and improving LLM response consistency, featuring a new metric (PWC), a benchmark dataset (MT-Consistency), and a generation framework (CARG).


<details>
  <summary>Details</summary>
Motivation: Ensuring consistent and coherent behavior of LLMs in high-stake domains across multi-turn interactions.

Method: Introduces Position-Weighted Consistency (PWC), MT-Consistency benchmark dataset, and Confidence-Aware Response Generation (CARG) framework.

Result: CARG improves response stability without sacrificing accuracy.

Conclusion: The framework offers a practical approach for dependable LLM behavior in critical real-world deployments.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various tasks, but their deployment in high-stake domains requires consistent
and coherent behavior across multiple rounds of user interaction. This paper
introduces a comprehensive framework for evaluating and improving LLM response
consistency, making three key contributions. Code and data are available at:
https://github.com/yubol-bobo/MT-Consistency. First, we introduce
Position-Weighted Consistency (PWC), a metric designed to capture both the
importance of early-stage stability and recovery patterns in multi-turn
interactions. Second, we present MT-Consistency, a carefully curated benchmark
dataset spanning diverse domains and difficulty levels, specifically designed
to evaluate LLM consistency under various challenging follow-up scenarios.
Third, we introduce Confidence-Aware Response Generation (CARG), a framework
that significantly improves response stability by explicitly integrating
internal model confidence scores during the generation process. Experimental
results demonstrate that CARG significantly improves response stability without
sacrificing accuracy, offering a practical path toward more dependable LLM
behavior in critical, real-world deployments.

</details>


### [231] [An Explicit Syllogistic Legal Reasoning Framework for Large Language Models](https://arxiv.org/pdf/2504.04042)
*Kepu Zhang, Weijie Yu, Zhongxiang Sun, Jun Xu*

Main category: cs.CL

TL;DR: SyLeR enhances LLMs for explicit syllogistic legal reasoning, improving accuracy and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with explicit syllogistic reasoning in legal contexts, producing implicit and less trustworthy outputs.

Method: SyLeR uses hierarchical retrieval and two-stage fine-tuning (supervised + reinforcement learning) to improve reasoning.

Result: SyLeR boosts accuracy and produces explicit, explainable legal reasoning across diverse tests.

Conclusion: SyLeR effectively addresses LLM limitations in legal syllogistic reasoning, enhancing reliability.

Abstract: Syllogistic reasoning is crucial for sound legal decision-making, allowing
legal professionals to draw logical conclusions by applying general principles
to specific case facts. While large language models (LLMs) can answer legal
questions, they often struggle with explicit syllogistic reasoning. Their
outputs tend to be implicit, unstructured, and consequently, less explainable
and trustworthy. To overcome these limitations, we introduce SyLeR, a novel
framework designed to enable LLMs to perform explicit syllogistic legal
reasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to
synthesize relevant legal statutes and precedents, thereby constructing
comprehensive major premises. This is followed by a two-stage fine-tuning
process: an initial supervised fine-tuning warm-up establishes a foundational
understanding of syllogistic reasoning, while reinforcement learning, guided by
a structure-aware reward mechanism, refines the model's capacity to generate
diverse, logically sound, and well-structured reasoning paths. We conducted
extensive experiments to evaluate SyLeR's performance. Our evaluations spanned
diverse dimensions, including both in-domain and cross-domain user groups
(legal laypersons and practitioners), multiple languages (Chinese and French),
and various LLM backbones (legal-specific and open-domain LLMs). The results
consistently demonstrate that SyLeR significantly enhances response accuracy
and reliably produces explicit, explainable, and trustworthy legal reasoning.

</details>


### [232] [RAISE: Reinforced Adaptive Instruction Selection For Large Language Models](https://arxiv.org/pdf/2504.07282)
*Lv Qingsong, Yangning Li, Zihua Lan, Zishan Xu, Jiwei Tang, Yinghui Li, Wenhao Jiang, Hai-Tao Zheng, Philip S. Yu*

Main category: cs.CL

TL;DR: RAISE is a dynamic, task-driven instruction selection framework for LLM fine-tuning, using RL to optimize instruction selection during training, outperforming heuristic methods with only 1% of training steps.


<details>
  <summary>Details</summary>
Motivation: Current instruction selection methods rely on fixed heuristic metrics and pre-training selection, leading to suboptimal fine-tuning and poor task-specific optimization.

Method: RAISE models instruction selection as a sequential decision-making process, using RL to dynamically select instructions based on their expected impact on model performance.

Result: RAISE achieves superior performance by updating only 1% of training steps compared to full-data training, proving its efficiency and effectiveness.

Conclusion: RAISE offers a highly interpretable and task-specific optimization approach, significantly improving instruction fine-tuning for LLMs.

Abstract: In the instruction fine-tuning of large language models (LLMs), it is widely
recognized that a few high-quality instructions are superior to a large number
of low-quality instructions. At present, many instruction selection methods
have been proposed, but most of these methods select instruction based on
heuristic quality metrics, and only consider data selection before training.
These designs lead to insufficient optimization of instruction fine-tuning, and
fixed heuristic indicators are often difficult to optimize for specific tasks.
Therefore, we design a dynamic, task-objective-driven instruction selection
framework RAISE(Reinforced Adaptive Instruction SElection), which incorporates
the entire instruction fine-tuning process into optimization, selecting
instructions at each step based on the expected impact of each instruction on
model performance improvement. Our approach is well interpretable and has
strong task-specific optimization capabilities. By modeling dynamic instruction
selection as a sequential decision-making process, we use RL to train our
selection strategy. Extensive experiments and result analysis prove the
superiority of our method compared with other instruction selection methods.
Notably, RAISE achieves superior performance by updating only 1% of the
training steps compared to full-data training, demonstrating its efficiency and
effectiveness.

</details>


### [233] [DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/pdf/2504.08120)
*Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger*

Main category: cs.CL

TL;DR: The study compares reasoning and non-reasoning LLMs for NLG evaluation, finding task-dependent benefits and performance variations based on model architecture and distillation scale.


<details>
  <summary>Details</summary>
Motivation: To explore the unexplored utility of reasoning LLMs for evaluating natural language generation tasks like machine translation and text summarization.

Method: Systematic comparison of eight models (reasoning and non-reasoning) on WMT23 and SummEval benchmarks, analyzing performance, reasoning token usage, and distillation effects.

Result: Reasoning LLMs show task-dependent benefits (e.g., OpenAI o3-mini excels in MT, DeepSeek-R1 in summarization consistency). Reasoning token usage correlates with evaluation quality in specific models. Distillation maintains performance up to 32B parameters but degrades at 8B.

Conclusion: This work is the first to assess reasoning LLMs for NLG evaluation, revealing nuanced performance differences and providing a foundation for future research.

Abstract: Reasoning-enabled large language models (LLMs) excel in logical tasks, yet
their utility for evaluating natural language generation remains unexplored.
This study systematically compares reasoning LLMs with non-reasoning
counterparts across machine translation and text summarization evaluation
tasks. We evaluate eight models spanning state-of-the-art reasoning models
(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and
equivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks
reveal architecture and task-dependent benefits: OpenAI o3-mini models show
improved performance with increased reasoning on MT, while DeepSeek-R1 and
generally underperforms compared to its non-reasoning variant except in
summarization consistency evaluation. Correlation analysis demonstrates that
reasoning token usage correlates with evaluation quality only in specific
models, while almost all models generally allocate more reasoning tokens when
identifying more quality issues. Distillation maintains reasonable performance
up to 32B parameter models but degrades substantially at 8B scale. This work
provides the first assessment of reasoning LLMs for NLG evaluation and
comparison to non-reasoning models. We share our code to facilitate further
research: https://github.com/NL2G/reasoning-eval.

</details>


### [234] [GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction](https://arxiv.org/pdf/2504.10792)
*Jessica Lin, Amir Zeldes*

Main category: cs.CL

TL;DR: The paper introduces a novel method for graded entity salience by combining subjective and summarization-based approaches, outperforming existing techniques and LLMs.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for consistent and explainable entity salience scoring in user-facing systems.

Method: Collects 5 summaries per document across 12 genres, calculating entity salience scores based on summary presence.

Result: Stronger correlation with human summaries and outperforms existing methods.

Conclusion: The approach effectively combines gradient scoring and explainability, with released data and code for further research.

Abstract: Determining and ranking the most salient entities in a text is critical for
user-facing systems, especially as users increasingly rely on models to
interpret long documents they only partially read. Graded entity salience
addresses this need by assigning entities scores that reflect their relative
importance in a text. Existing approaches fall into two main categories:
subjective judgments of salience, which allow for gradient scoring but lack
consistency, and summarization-based methods, which define salience as
mention-worthiness in a summary, promoting explainability but limiting outputs
to binary labels (entities are either summary-worthy or not). In this paper, we
introduce a novel approach for graded entity salience that combines the
strengths of both approaches. Using an English dataset spanning 12 spoken and
written genres, we collect 5 summaries per document and calculate each entity's
salience score based on its presence across these summaries. Our approach shows
stronger correlation with scores based on human summaries and alignments, and
outperforms existing techniques, including LLMs. We release our data and code
at https://github.com/jl908069/gum_sum_salience to support further research on
graded salient entity extraction.

</details>


### [235] [From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs](https://arxiv.org/pdf/2504.11277)
*Guocong Li, Weize Liu, Yihang Wu, Ping Wang, Shuaihan Huang, Hongxia Xu, Jian Wu*

Main category: cs.CL

TL;DR: The paper proposes a three-stage fine-tuning method to improve LLMs' ability to detect and correct misleading input, enhancing response accuracy and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on correcting LLM outputs but neglect improving their ability to handle misleading input directly.

Method: A three-stage fine-tuning approach: (1) detect misleading input, (2) correct it using knowledge, and (3) generate accurate answers.

Result: Experiments show improved accuracy, factuality, and reduced hallucinations, especially with misleading queries.

Conclusion: The method effectively enhances LLMs' robustness to misleading input, improving overall performance.

Abstract: Large language models (LLMs) exhibit excellent performance in natural
language processing (NLP), but remain highly sensitive to the quality of input
queries, especially when these queries contain misleading or inaccurate
information. Existing methods focus on correcting the output, but they often
overlook the potential of improving the ability of LLMs to detect and correct
misleading content in the input itself. In this paper, we propose a novel
three-stage fine-tuning method that enhances the ability of LLMs to detect and
correct misleading information in the input, further improving response
accuracy and reducing hallucinations. Specifically, the three stages include
(1) training LLMs to identify misleading information, (2) training LLMs to
correct the misleading information using built-in or external knowledge, and
(3) training LLMs to generate accurate answers based on the corrected queries.
To evaluate our method, we conducted experiments on three datasets for the
hallucination detection task and the question answering~(QA) task, as well as
two datasets containing misleading information that we constructed. The
experimental results demonstrate that our method significantly improves the
accuracy and factuality of LLM responses, while also enhancing the ability to
detect hallucinations and reducing the generation of hallucinations in the
output, particularly when the query contains misleading information.

</details>


### [236] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/pdf/2505.06186)
*Massimiliano Pronesti, Joao Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisin Redmond, Anya Belz, Yufang Hou*

Main category: cs.CL

TL;DR: The paper introduces CochraneForest, a dataset for document-level scientific evidence extraction, and URCA, a retrieval-augmented framework, which outperforms existing methods by 10.3% in F1 score.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting scientific evidence for clinical questions with conflicting evidence, particularly in synthesizing biomedical evidence.

Method: Created CochraneForest dataset with annotated forest plots and proposed URCA, a retrieval-augmented generation framework.

Result: URCA outperforms existing methods by up to 10.3% in F1 score, but CochraneForest proves to be a complex benchmark.

Conclusion: CochraneForest is a valuable but challenging testbed for advancing automated evidence synthesis systems.

Abstract: Extracting scientific evidence from biomedical studies for clinical research
questions (e.g., Does stem cell transplantation improve quality of life in
patients with medically refractory Crohn's disease compared to placebo?) is a
crucial step in synthesising biomedical evidence. In this paper, we focus on
the task of document-level scientific evidence extraction for clinical
questions with conflicting evidence. To support this task, we create a dataset
called CochraneForest, leveraging forest plots from Cochrane systematic
reviews. It comprises 202 annotated forest plots, associated clinical research
questions, full texts of studies, and study-specific conclusions. Building on
CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a
retrieval-augmented generation framework designed to tackle the unique
challenges of evidence extraction. Our experiments show that URCA outperforms
the best existing methods by up to 10.3% in F1 score on this task. However, the
results also underscore the complexity of CochraneForest, establishing it as a
challenging testbed for advancing automated evidence synthesis systems.

</details>


### [237] [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/pdf/2504.15815)
*Michael A. Hedderich, Anyi Wang, Raoyuan Zhao, Florian Eichin, Jonas Fischer, Barbara Plank*

Main category: cs.CL

TL;DR: Spotlight combines automation and human analysis to evaluate LLM outputs by distinguishing random variations from systematic differences, aiding prompt engineering and model behavior research.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for LLM outputs are limited—automated metrics lack depth, and human evaluation is labor-intensive. Spotlight addresses these gaps.

Method: Spotlight uses data mining to automatically identify systematic differences in LLM outputs, generating token patterns for efficient human analysis.

Result: Benchmarks confirm Spotlight's reliability in extracting token patterns, providing new insights into prompt data and revealing differences (e.g., gender, culture).

Conclusion: Spotlight enhances prompt engineering and model behavior research by efficiently uncovering systematic differences in LLM outputs, supported by user studies.

Abstract: Prompt engineering for large language models is challenging, as even small
prompt perturbations or model changes can significantly impact the generated
output texts. Existing evaluation methods of LLM outputs, either automated
metrics or human evaluation, have limitations, such as providing limited
insights or being labor-intensive. We propose Spotlight, a new approach that
combines both automation and human analysis. Based on data mining techniques,
we automatically distinguish between random (decoding) variations and
systematic differences in language model outputs. This process provides token
patterns that describe the systematic differences and guide the user in
manually analyzing the effects of their prompts and changes in models
efficiently. We create three benchmarks to quantitatively test the reliability
of token pattern extraction methods and demonstrate that our approach provides
new insights into established prompt data. From a human-centric perspective,
through demonstration studies and a user study, we show that our token pattern
approach helps users understand the systematic differences of language model
outputs. We are further able to discover relevant differences caused by prompt
and model changes (e.g. related to gender or culture), thus supporting the
prompt engineering process and human-centric model behavior research.

</details>


### [238] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/pdf/2505.13772)
*Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros*

Main category: cs.CL

TL;DR: Llama-Krikri-8B is a Greek-focused LLM based on Llama 3.1-8B, excelling in language nuances, supporting Modern/Ancient Greek and English, and outperforming peers in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a high-performance LLM specifically optimized for Greek, addressing linguistic nuances and supporting diverse Greek language forms.

Method: Built on Llama 3.1-8B, trained on Greek data, with a multi-stage post-training pipeline (MAGPIE) using human and synthetic data. Introduced three new Greek benchmarks.

Result: Outperforms comparable Greek and multilingual LLMs in natural language understanding, generation, and code generation.

Conclusion: Llama-Krikri-8B is a state-of-the-art Greek LLM with superior performance and broad language support.

Abstract: We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored
for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been
extensively trained on high-quality Greek data to ensure superior adaptation to
linguistic nuances. With 8 billion parameters, it offers advanced capabilities
while maintaining efficient computational performance. Llama-Krikri-8B supports
both Modern Greek and English, and is also equipped to handle polytonic text
and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage
post-training pipeline, utilizing both human and synthetic instruction and
preference data, by applying techniques such as MAGPIE. In addition, for
evaluation, we propose three novel public benchmarks for Greek. Our evaluation
on existing as well as the proposed benchmarks shows notable improvements over
comparable Greek and multilingual LLMs in both natural language understanding
and generation as well as code generation.

</details>


### [239] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/pdf/2505.14079)
*Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei*

Main category: cs.CL

TL;DR: BAR, a backward reasoning-based agent, outperforms forward reasoning methods in complex tasks by starting planning from the terminal state.


<details>
  <summary>Details</summary>
Motivation: Forward reasoning struggles with complex tasks due to the perception gap between initial state and goal. Backward reasoning is proposed as a solution.

Method: BAR uses recursive goal decomposition, state consistency maintenance, and stage memory modules for robust planning from the terminal state.

Result: BAR shows superior performance over existing methods in experiments.

Conclusion: Backward reasoning is effective for complex tasks, and BAR's modules enhance planning robustness and efficiency.

Abstract: Large language model (LLM) based agents have shown great potential in
following human instructions and automatically completing various tasks. To
complete a task, the agent needs to decompose it into easily executed steps by
planning. Existing studies mainly conduct the planning by inferring what steps
should be executed next starting from the agent's initial state. However, this
forward reasoning paradigm doesn't work well for complex tasks. We propose to
study this issue in Minecraft, a virtual environment that simulates complex
tasks based on real-world scenarios. We believe that the failure of forward
reasoning is caused by the big perception gap between the agent's initial state
and task goal. To this end, we leverage backward reasoning and make the
planning starting from the terminal state, which can directly achieve the task
goal in one step. Specifically, we design a BAckward Reasoning based agent
(BAR). It is equipped with a recursive goal decomposition module, a state
consistency maintaining module and a stage memory module to make robust,
consistent, and efficient planning starting from the terminal state.
Experimental results demonstrate the superiority of BAR over existing methods
and the effectiveness of proposed modules.

</details>


### [240] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/pdf/2505.14590)
*Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper proposes MCIP, a refined version of MCP, to address safety risks in decentralized architectures. It includes a taxonomy of unsafe behaviors, benchmark data, and experiments showing improved LLM safety.


<details>
  <summary>Details</summary>
Motivation: MCP's decentralized architecture introduces underexplored safety risks, necessitating systematic analysis and solutions.

Method: The MAESTRO framework guides the analysis of MCP's safety gaps, leading to MCIP. A taxonomy of unsafe behaviors is created, followed by benchmark data and experiments on LLMs.

Result: Experiments reveal LLM vulnerabilities in MCP interactions, with MCIP significantly enhancing safety performance.

Conclusion: The proposed MCIP framework effectively addresses MCP safety gaps, improving LLM safety in decentralized contexts.

Abstract: As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users
and developers, it also brings underexplored safety risks. Its decentralized
architecture, which separates clients and servers, poses unique challenges for
systematic safety analysis. This paper proposes a novel framework to enhance
MCP safety. Guided by the MAESTRO framework, we first analyze the missing
safety mechanisms in MCP, and based on this analysis, we propose the Model
Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses
these gaps. Next, we develop a fine-grained taxonomy that captures a diverse
range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy,
we develop benchmark and training data that support the evaluation and
improvement of LLMs' capabilities in identifying safety risks within MCP
interactions. Leveraging the proposed benchmark and training data, we conduct
extensive experiments on state-of-the-art LLMs. The results highlight LLMs'
vulnerabilities in MCP interactions and demonstrate that our approach
substantially improves their safety performance.

</details>


### [241] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/pdf/2505.16491)
*Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia*

Main category: cs.CL

TL;DR: The study explores how sentiment features are encoded in Llama models, identifying mid-layers as most informative for sentiment analysis and reducing memory usage by 57%.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs capture sentiment-related information and improve sentiment analysis beyond prompting techniques.

Method: Probe classifiers analyze sentiment encoding across layers and scales, focusing on binary polarity tasks.

Result: Sentiment information is concentrated in mid-layers, improving detection accuracy by 14%, and the last token isn't always most informative. Memory usage is reduced by 57%.

Conclusion: Layer-specific probing is effective for sentiment tasks, enhancing model utility and efficiency.

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [242] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/pdf/2505.16520)
*Giovanni Servedio, Alessandro De Bellis, Dario Di Palma, Vito Walter Anelli, Tommaso Di Noia*

Main category: cs.CL

TL;DR: The paper addresses factual hallucinations in LLMs, critiques synthetic datasets, and introduces realistic datasets to better evaluate truthfulness encoding.


<details>
  <summary>Details</summary>
Motivation: Factual hallucinations in LLMs undermine reliability and trust, but prior studies rely on unrealistic synthetic datasets, limiting generalization.

Method: The authors introduce strategies for sampling true-false factoid sentences from tabular data and generating realistic datasets from QA collections.

Result: Analysis of two LLMs partially validates prior findings but shows challenges in generalizing to LLM-generated datasets.

Conclusion: The study advances research on LLM factuality and provides practical evaluation guidelines.

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [243] [EarthSE: A Benchmark for Evaluating Earth Scientific Exploration Capability of LLMs](https://arxiv.org/pdf/2505.17139)
*Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai*

Main category: cs.CL

TL;DR: A specialized benchmark for Earth sciences evaluates LLMs' capabilities in scientific exploration, covering broad and advanced levels with datasets like Earth-Iron, Earth-Silver, and Earth-Gold.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack Earth science specificity or holistic evaluation, and neglect open-ended scientific exploration.

Method: Constructed two QA datasets (Earth-Iron and Earth-Silver) and introduced Earth-Gold with open-ended multi-turn dialogues, leveraging 100,000 research papers.

Result: Experiments show limitations in 11 leading LLMs, indicating room for improvement in scientific exploration.

Conclusion: The benchmark provides a comprehensive tool for evaluating LLMs in Earth sciences, highlighting gaps in current capabilities.

Abstract: Advancements in Large Language Models (LLMs) drive interest in scientific
applications, necessitating specialized benchmarks such as Earth science.
Existing benchmarks either present a general science focus devoid of Earth
science specificity or cover isolated subdomains, lacking holistic evaluation.
Furthermore, current benchmarks typically neglect the assessment of LLMs'
capabilities in open-ended scientific exploration. In this paper, we present a
comprehensive and professional benchmark for the Earth sciences, designed to
evaluate the capabilities of LLMs in scientific exploration within this domain,
spanning from fundamental to advanced levels. Leveraging a corpus of 100,000
research papers, we first construct two Question Answering (QA) datasets:
Earth-Iron, which offers extensive question coverage for broad assessment, and
Earth-Silver, which features a higher level of difficulty to evaluate
professional depth. These datasets encompass five Earth spheres, 114
disciplines, and 11 task categories, assessing foundational knowledge crucial
for scientific exploration. Most notably, we introduce Earth-Gold with new
metrics, a dataset comprising open-ended multi-turn dialogues specifically
designed to evaluate the advanced capabilities of LLMs in scientific
exploration, including methodology induction, limitation analysis, and concept
proposal. Extensive experiments reveal limitations in 11 leading LLMs across
different domains and tasks, highlighting considerable room for improvement in
their scientific exploration capabilities. The benchmark is available on
https://huggingface.co/ai-earth .

</details>


### [244] [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/pdf/2505.17873)
*Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang*

Main category: cs.CL

TL;DR: The paper introduces experiment-guided hypothesis ranking using a simulator to prioritize hypotheses based on simulated experimental feedback, outperforming pre-experiment methods.


<details>
  <summary>Details</summary>
Motivation: Existing hypothesis ranking methods rely on pre-experiment reasoning without empirical outcomes, which is inefficient for costly wet-lab experiments.

Method: A simulator models hypothesis performance based on similarity to ground truth and noise. A dataset of 124 chemistry hypotheses validates the simulator, and a pseudo experiment-guided ranking method clusters hypotheses for prioritization.

Result: The proposed method outperforms pre-experiment baselines and strong ablations in experiments.

Conclusion: Experiment-guided ranking, enabled by a domain-informed simulator, improves hypothesis prioritization for scientific discovery.

Abstract: Hypothesis ranking is a crucial component of automated scientific discovery,
particularly in natural sciences where wet-lab experiments are costly and
throughput-limited. Existing approaches focus on pre-experiment ranking,
relying solely on large language model's internal reasoning without
incorporating empirical outcomes from experiments. We introduce the task of
experiment-guided ranking, which aims to prioritize candidate hypotheses based
on the results of previously tested ones. However, developing such strategies
is challenging due to the impracticality of repeatedly conducting real
experiments in natural science domains. To address this, we propose a simulator
grounded in three domain-informed assumptions, modeling hypothesis performance
as a function of similarity to a known ground truth hypothesis, perturbed by
noise. We curate a dataset of 124 chemistry hypotheses with experimentally
reported outcomes to validate the simulator. Building on this simulator, we
develop a pseudo experiment-guided ranking method that clusters hypotheses by
shared functional characteristics and prioritizes candidates based on insights
derived from simulated experimental feedback. Experiments show that our method
outperforms pre-experiment baselines and strong ablations.

</details>


### [245] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/pdf/2505.18799)
*Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao*

Main category: cs.CL

TL;DR: The paper introduces ALPS, a method to efficiently align large language models (LLMs) by localizing and pruning task-sensitive attention heads, reducing training costs while improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the high costs and data dependency issues in aligning LLMs to downstream tasks, enhancing efficiency and reusability.

Method: Proposes ALPS, which localizes and prunes task-sensitive attention heads, restricting updates to these heads to reduce alignment costs.

Result: Activates only 10% of attention parameters during fine-tuning, achieving a 2% performance improvement over baselines on three tasks, with transferable task-specific heads.

Conclusion: ALPS offers a novel, efficient approach to LLM alignment, reducing costs and improving performance while maintaining transferability.

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant training adjustment costs. Prior research has explored
various avenues to enhance alignment efficiency, primarily through minimal-data
training or data-driven activations to identify key attention heads. However,
these approaches inherently introduce data dependency, which hinders
generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment. The code is available at
https://github.com/VoiceBeer/ALPS.

</details>


### [246] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/pdf/2505.19240)
*Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger*

Main category: cs.CL

TL;DR: A data-driven survey analyzes limitations of large language models (LLMs) from 2022-2025, identifying key issues like reasoning, hallucination, and bias, with trends showing rapid growth in research on these topics.


<details>
  <summary>Details</summary>
Motivation: To address the gap in comprehensive reviews of LLM limitations by providing a data-driven, semi-automated analysis of research trends.

Method: A bottom-up approach using keyword filtering, LLM-based classification, and topic clustering (HDBSCAN+BERTopic and LlooM) on 250,000 ACL and arXiv papers.

Result: Identified 7-15 prominent LLM limitations, with reasoning being the most studied. Research on limitations grew significantly, with arXiv shifting focus toward safety and controllability.

Conclusion: The study offers a quantitative view of LLM limitations research trends and releases a dataset and methodology for future work.

Abstract: Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. While prior reviews have addressed these
issues, they often focus on individual limitations or consider them within the
broader context of evaluating overall model performance. This survey addresses
the gap by presenting a data-driven, semi-automated review of research on
limitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From
a corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation
papers using keyword filtering and LLM-based classification, validated against
expert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and
LlooM), we identify between 7 and 15 prominent types of limitations discussed
in recent LLM research across the ACL and arXiv datasets. We find that
LLM-related research increases nearly sixfold in ACL and nearly fifteenfold in
arXiv between 2022 and 2025, while LLLMs research grows even faster, by a
factor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most
studied limitation, followed by generalization, hallucination, bias, and
security. The distribution of topics in the ACL dataset stays relatively stable
over time, while arXiv shifts toward safety and controllability (with topics
like security risks, alignment, hallucinations, knowledge editing), and
multimodality between 2022 and 2025. We offer a quantitative view of trends in
LLM limitations research and release a dataset of annotated abstracts and a
validated methodology, available at:
https://github.com/a-kostikova/LLLMs-Survey.

</details>


### [247] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/pdf/2505.20112)
*Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang*

Main category: cs.CL

TL;DR: ResSVD is a new SVD-based LLM compression method that reduces truncation loss by leveraging residual matrices and selectively compressing layers, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face deployment challenges due to size and memory demands, necessitating efficient compression strategies. Current SVD-based methods neglect residual matrices and compress all layers, leading to performance degradation.

Method: ResSVD uses the residual matrix from truncation to reduce loss and selectively compresses the last few layers under a fixed compression ratio to mitigate error propagation.

Result: ResSVD consistently outperforms existing methods across diverse LLM families and benchmark datasets.

Conclusion: ResSVD is a practical and effective solution for LLM compression, addressing limitations of current SVD-based approaches.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models. Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.

</details>


### [248] [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/pdf/2505.19439)
*Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang*

Main category: cs.CL

TL;DR: This paper explores using format and length as surrogate signals to train LLMs for math problem-solving, avoiding costly ground truth data. The method matches or surpasses standard GRPO in performance.


<details>
  <summary>Details</summary>
Motivation: Ground truth answers for training LLMs in math are often hard to obtain. This research seeks an alternative using format and length as signals.

Method: Uses format correctness and length-based rewards in a GRPO approach to train LLMs without ground truth.

Result: Achieves 40.0% accuracy on AIME2024 with a 7B model, outperforming standard GRPO in some cases.

Conclusion: The approach reduces reliance on ground truth data and shows that LLMs can excel with proper answering habits.

Abstract: Large Language Models have achieved remarkable success in natural language
processing tasks, with Reinforcement Learning playing a key role in adapting
them to specific applications. However, obtaining ground truth answers for
training LLMs in mathematical problem-solving is often challenging, costly, and
sometimes unfeasible. This research delves into the utilization of format and
length as surrogate signals to train LLMs for mathematical problem-solving,
bypassing the need for traditional ground truth answers.Our study shows that a
reward function centered on format correctness alone can yield performance
improvements comparable to the standard GRPO algorithm in early phases.
Recognizing the limitations of format-only rewards in the later phases, we
incorporate length-based rewards. The resulting GRPO approach, leveraging
format-length surrogate signals, not only matches but surpasses the performance
of the standard GRPO algorithm relying on ground truth answers in certain
scenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through
systematic exploration and experimentation, this research not only offers a
practical solution for training LLMs to solve mathematical problems and
reducing the dependence on extensive ground truth data collection, but also
reveals the essence of why our label-free approach succeeds: the powerful base
model is like an excellent student who has already mastered mathematical and
logical reasoning skills, but performs poorly on the test paper, it simply
needs to develop good answering habits to achieve outstanding results in exams
, to unlock the capabilities it already possesses.

</details>


### [249] [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/pdf/2505.20767)
*Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie*

Main category: cs.CL

TL;DR: The paper introduces a framework and dataset (CogniBench) to assess faithfulness hallucinations in LLMs, focusing on cognitive statements, and develops an automatic annotation pipeline for scalability.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks overlook cognitive statements, making it hard to evaluate hallucinations involving inferences.

Method: Inspired by legal evidence assessment, the authors design a framework to evaluate faithfulness levels and create the CogniBench dataset, later scaled to CogniBench-L with an automatic annotation pipeline.

Result: The CogniBench dataset provides insights, and the CogniBench-L dataset enables training detectors for both factual and cognitive hallucinations.

Conclusion: The framework and datasets address gaps in evaluating LLM hallucinations, offering scalable solutions for future research.

Abstract: Faithfulness hallucinations are claims generated by a Large Language Model
(LLM) not supported by contexts provided to the LLM. Lacking assessment
standards, existing benchmarks focus on "factual statements" that rephrase
source materials while overlooking "cognitive statements" that involve making
inferences from the given context. Consequently, evaluating and detecting the
hallucination of cognitive statements remains challenging. Inspired by how
evidence is assessed in the legal domain, we design a rigorous framework to
assess different levels of faithfulness of cognitive statements and introduce
the CogniBench dataset where we reveal insightful statistics. To keep pace with
rapidly evolving LLMs, we further develop an automatic annotation pipeline that
scales easily across different models. This results in a large-scale
CogniBench-L dataset, which facilitates training accurate detectors for both
factual and cognitive hallucinations. We release our model and datasets at:
https://github.com/FUTUREEEEEE/CogniBench

</details>


### [250] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/pdf/2505.21693)
*Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich*

Main category: cs.CL

TL;DR: MAKIEval is an automatic multilingual framework to evaluate cultural awareness in LLMs, addressing biases and disparities across languages and regions.


<details>
  <summary>Details</summary>
Motivation: The English-centric pretraining of LLMs raises concerns about cross-lingual cultural biases, but existing evaluation methods are limited.

Method: MAKIEval uses Wikidata's multilingual structure to automatically identify cultural entities in model outputs, introducing four metrics for cultural awareness.

Result: Evaluation of 7 LLMs across 13 languages shows stronger cultural awareness in English, suggesting English prompts activate knowledge more effectively.

Conclusion: MAKIEval provides a scalable, language-agnostic tool for assessing cultural awareness in LLMs, highlighting disparities and biases.

Abstract: Large language models (LLMs) are used globally across many languages, but
their English-centric pretraining raises concerns about cross-lingual
disparities for cultural awareness, often resulting in biased outputs. However,
comprehensive multilingual evaluation remains challenging due to limited
benchmarks and questionable translation quality. To better assess these
disparities, we introduce MAKIEval, an automatic multilingual framework for
evaluating cultural awareness in LLMs across languages, regions, and topics.
MAKIEval evaluates open-ended text generation, capturing how models express
culturally grounded knowledge in natural language. Leveraging Wikidata's
multilingual structure as a cross-lingual anchor, it automatically identifies
cultural entities in model outputs and links them to structured knowledge,
enabling scalable, language-agnostic evaluation without manual annotation or
translation. We then introduce four metrics that capture complementary
dimensions of cultural awareness: granularity, diversity, cultural specificity,
and consensus across languages. We assess 7 LLMs developed from different parts
of the world, encompassing both open-source and proprietary systems, across 13
languages, 19 countries and regions, and 6 culturally salient topics (e.g.,
food, clothing). Notably, we find that models tend to exhibit stronger cultural
awareness in English, suggesting that English prompts more effectively activate
culturally grounded knowledge. We publicly release our code and data.

</details>


### [251] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/pdf/2505.22571)
*Hoang Pham, Thuy-Duong Nguyen, Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: A novel unified RAG framework, Agent-UniRAG, uses LLM agents for interpretability and handles both single-hop and multi-hop queries. SynAgent-RAG dataset supports small LLMs, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Previous RAG systems address single-hop or multi-hop queries separately, limiting real-world applicability. A unified approach is needed for broader use.

Method: Proposes Agent-UniRAG, a trainable LLM agent framework that processes queries step-by-step based on complexity, and introduces SynAgent-RAG dataset for small LLMs.

Result: Competitive performance with closed-source and larger open-source LLMs on RAG benchmarks.

Conclusion: Agent-UniRAG enhances RAG effectiveness and interpretability, with potential for broader adoption due to its unified approach and support for small LLMs.

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [252] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/pdf/2505.21701)
*Raoyuan Zhao, Abdullatif Köksal, Ali Modarressi, Michael A. Hedderich, Hinrich Schütze*

Main category: cs.CL

TL;DR: The paper highlights inconsistencies in probing methods for identifying knowledge gaps in LLMs, revealing intra-method and cross-method inconsistencies, and calls for more robust frameworks.


<details>
  <summary>Details</summary>
Motivation: The reliability of LLMs is compromised by hallucinations, necessitating precise identification of knowledge gaps. Existing probing methods are inconsistent.

Method: The study evaluates probing methods using input variations and quantitative metrics, focusing on intra-method and cross-method inconsistencies.

Result: Findings show significant inconsistencies: minimal prompt changes cause variance (intra-method), and methods contradict each other (cross-method).

Conclusion: Current probing methods are unreliable, emphasizing the need for perturbation-robust frameworks to accurately identify knowledge gaps in LLMs.

Abstract: The reliability of large language models (LLMs) is greatly compromised by
their tendency to hallucinate, underscoring the need for precise identification
of knowledge gaps within LLMs. Various methods for probing such gaps exist,
ranging from calibration-based to prompting-based methods. To evaluate these
probing methods, in this paper, we propose a new process based on using input
variations and quantitative metrics. Through this, we expose two dimensions of
inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal
non-semantic perturbations in prompts lead to considerable variance in detected
knowledge gaps within the same probing method; e.g., the simple variation of
shuffling answer options can decrease agreement to around 40%. (2) Cross-method
inconsistency: Probing methods contradict each other on whether a model knows
the answer. Methods are highly inconsistent -- with decision consistency across
methods being as low as 7% -- even though the model, dataset, and prompt are
all the same. These findings challenge existing probing methods and highlight
the urgent need for perturbation-robust probing frameworks.

</details>


### [253] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/pdf/2505.22630)
*Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: LLMs exhibit errors due to class-based (mis)generalization, integrating misleading contextual cues, revealing structured but flawed mechanisms in their reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the nature of LLM errors and whether they follow regularities, particularly focusing on irrelevant context hallucinations.

Method: Behavioral analysis and mechanistic interpretability experiments on models like Llama-3, Mistral, and Pythia across 39 factual recall relations.

Result: Errors stem from class-based (mis)generalization, with abstract class representations refined into answers. Two competing circuits govern feature selection.

Conclusion: LLMs generalize using abstractions but unreliably, termed 'stochastic chameleons,' offering a nuanced view of the stochastic parrot argument.

Abstract: The widespread success of large language models (LLMs) on NLP benchmarks has
been accompanied by concerns that LLMs function primarily as stochastic parrots
that reproduce texts similar to what they saw during pre-training, often
erroneously. But what is the nature of their errors, and do these errors
exhibit any regularities? In this work, we examine irrelevant context
hallucinations, in which models integrate misleading contextual cues into their
predictions. Through behavioral analysis, we show that these errors result from
a structured yet flawed mechanism that we term class-based (mis)generalization,
in which models combine abstract class cues with features extracted from the
query or context to derive answers. Furthermore, mechanistic interpretability
experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation
types reveal that this behavior is reflected in the model's internal
computations: (i) abstract class representations are constructed in lower
layers before being refined into specific answers in higher layers, (ii)
feature selection is governed by two competing circuits -- one prioritizing
direct query-based reasoning, the other incorporating contextual cues -- whose
relative influences determine the final output. Our findings provide a more
nuanced perspective on the stochastic parrot argument: through form-based
training, LLMs can exhibit generalization leveraging abstractions, albeit in
unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [254] [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/pdf/2505.23883)
*Jianyang Gu, Samuel Stevens, Elizabeth G Campolongo, Matthew J Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E. White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su*

Main category: cs.CV

TL;DR: BioCLIP 2, trained on TreeOfLife-200M, exhibits emergent behaviors in biological vision tasks, with embeddings aligning with ecological and functional traits.


<details>
  <summary>Details</summary>
Motivation: To explore emergent behaviors in biological vision models via large-scale contrastive vision-language training.

Method: Curated TreeOfLife-200M dataset and trained BioCLIP 2 with hierarchical supervision and contrastive objectives.

Result: BioCLIP 2 achieves high accuracy in tasks like habitat classification and trait prediction, with embeddings reflecting biological meaning.

Conclusion: Emergent properties in BioCLIP 2's embedding space are biologically meaningful and scale with larger datasets.

Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors,
learning new capabilities beyond their initial training objectives. We find
such emergent behaviors in biological vision models via large-scale contrastive
vision-language training. To achieve this, we first curate TreeOfLife-200M,
comprising 214 million images of living organisms, the largest and most diverse
biological organism image dataset to date. We then train BioCLIP 2 on
TreeOfLife-200M to distinguish different species. Despite the narrow training
objective, BioCLIP 2 yields extraordinary accuracy when applied to various
biological visual tasks such as habitat classification and trait prediction. We
identify emergent properties in the learned embedding space of BioCLIP 2. At
the inter-species level, the embedding distribution of different species aligns
closely with functional and ecological meanings (e.g., beak sizes and
habitats). At the intra-species level, instead of being diminished, the
intra-species variations (e.g., life stages and sexes) are preserved and better
separated in subspaces orthogonal to inter-species distinctions. We provide
formal proof and analyses to explain why hierarchical supervision and
contrastive objectives encourage these emergent properties. Crucially, our
results reveal that these properties become increasingly significant with
larger-scale training data, leading to a biologically meaningful embedding
space.

</details>


### [255] [Generating Fit Check Videos with a Handheld Camera](https://arxiv.org/pdf/2505.23886)
*Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz*

Main category: cs.CV

TL;DR: A method for generating full-body videos from static photos and IMU data using a video diffusion model, enabling realistic motion synthesis and scene rendering.


<details>
  <summary>Details</summary>
Motivation: To simplify full-body video capture by eliminating the need for mounted cameras and repeated practice, making it more accessible via handheld mobile devices.

Method: Uses two static photos (front and back) and IMU motion data. A video diffusion model with parameter-free frame generation and multi-reference attention integrates appearance details. Image-based fine-tuning enhances realism.

Result: Realistic full-body videos with consistent illumination and shadows, synthesized from minimal input.

Conclusion: The approach successfully enables convenient full-body video capture and rendering into new scenes, improving accessibility and realism.

Abstract: Self-captured full-body videos are popular, but most deployments require
mounted cameras, carefully-framed shots, and repeated practice. We propose a
more convenient solution that enables full-body video capture using handheld
mobile devices. Our approach takes as input two static photos (front and back)
of you in a mirror, along with an IMU motion reference that you perform while
holding your mobile phone, and synthesizes a realistic video of you performing
a similar target motion. We enable rendering into a new scene, with consistent
illumination and shadows. We propose a novel video diffusion-based model to
achieve this. Specifically, we propose a parameter-free frame generation
strategy, as well as a multi-reference attention mechanism, that effectively
integrate appearance information from both the front and back selfies into the
video diffusion model. Additionally, we introduce an image-based fine-tuning
strategy to enhance frame sharpness and improve the generation of shadows and
reflections, achieving a more realistic human-scene composition.

</details>


### [256] [Cora: Correspondence-aware image editing using few step diffusion](https://arxiv.org/pdf/2505.23907)
*Amirhossein Almohammadi, Aryan Mikaeili, Sauradip Nag, Negar Hassanpour, Andrea Tagliasacchi, Ali Mahdavi-Amiri*

Main category: cs.CV

TL;DR: Cora is a novel image editing framework using correspondence-aware noise correction and interpolated attention maps to improve structural and texture edits while preserving key attributes.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with structural edits, producing artifacts or failing to preserve source image attributes like pose.

Method: Cora introduces correspondence-aware noise correction and interpolated attention maps to align textures and structures between source and target images.

Result: Cora excels in maintaining structure, textures, and identity across diverse edits, outperforming alternatives in user studies.

Conclusion: Cora provides superior control and results for complex image edits, addressing limitations of current methods.

Abstract: Image editing is an important task in computer graphics, vision, and VFX,
with recent diffusion-based methods achieving fast and high-quality results.
However, edits requiring significant structural changes, such as non-rigid
deformations, object modifications, or content generation, remain challenging.
Existing few step editing approaches produce artifacts such as irrelevant
texture or struggle to preserve key attributes of the source image (e.g.,
pose). We introduce Cora, a novel editing framework that addresses these
limitations by introducing correspondence-aware noise correction and
interpolated attention maps. Our method aligns textures and structures between
the source and target images through semantic correspondence, enabling accurate
texture transfer while generating new content when necessary. Cora offers
control over the balance between content generation and preservation. Extensive
experiments demonstrate that, quantitatively and qualitatively, Cora excels in
maintaining structure, textures, and identity across diverse edits, including
pose changes, object addition, and texture refinements. User studies confirm
that Cora delivers superior results, outperforming alternatives.

</details>


### [257] [Representational Difference Explanations](https://arxiv.org/pdf/2505.23917)
*Neehar Kondapaneni, Oisin Mac Aodha, Pietro Perona*

Main category: cs.CV

TL;DR: Proposes RDX, a method for comparing learned representations, outperforming existing XAI techniques in interpretability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Current XAI tools lack effectiveness in model comparison, a key aspect of scientific analysis.

Method: Introduces Representational Differences Explanations (RDX) to visualize and compare learned representations.

Result: RDX successfully identifies meaningful distinctions between models and subtle data patterns, outperforming existing XAI methods.

Conclusion: RDX fills a gap in model comparison tools, offering an effective and explainable solution.

Abstract: We propose a method for discovering and visualizing the differences between
two learned representations, enabling more direct and interpretable model
comparisons. We validate our method, which we call Representational Differences
Explanations (RDX), by using it to compare models with known conceptual
differences and demonstrate that it recovers meaningful distinctions where
existing explainable AI (XAI) techniques fail. Applied to state-of-the-art
models on challenging subsets of the ImageNet and iNaturalist datasets, RDX
reveals both insightful representational differences and subtle patterns in the
data. Although comparison is a cornerstone of scientific analysis, current
tools in machine learning, namely post hoc XAI methods, struggle to support
model comparison effectively. Our work addresses this gap by introducing an
effective and explainable tool for contrasting model representations.

</details>


### [258] [Interactive Video Generation via Domain Adaptation](https://arxiv.org/pdf/2505.24253)
*Ishaan Rawal, Suryansh Kumar*

Main category: cs.CV

TL;DR: The paper addresses challenges in Interactive Video Generation (IVG) by proposing mask normalization and temporal intrinsic denoising to improve perceptual quality and trajectory control.


<details>
  <summary>Details</summary>
Motivation: Enabling precise control over motion elements like object trajectory in text-conditioned diffusion models for video generation is challenging, with existing methods degrading perceptual quality.

Method: The authors propose mask normalization to mitigate internal covariate shift from attention masking and a temporal intrinsic diffusion prior to align initial noise with IVG conditioning.

Result: Qualitative and quantitative evaluations show improved perceptual quality and trajectory control over state-of-the-art IVG techniques.

Conclusion: The proposed solutions effectively address domain shift problems in IVG, enhancing both quality and control.

Abstract: Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.

</details>


### [259] [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/pdf/2505.23922)
*David Ma, Huaqing Yuan, Xingjian Wang, Qianbo Zang, Tianci Liu, Xinyang He, Yanbin Wei, Jiawei Guo, Ni Jiahui, Zhenzhu Yang, Meng Cao, Shanghaoran Quan, Yizhi Li, Wangchunshu Zhou, Jiaheng Liu, Wenhao Huang, Ge Zhang, Shiwen Ni, Xiaojie Jin*

Main category: cs.CV

TL;DR: ScaleLong is a benchmark for evaluating multi-timescale video understanding, enabling direct comparison of model performance across clip, shot, event, and story timescales within the same video content.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack a multi-scale design or scatter scale-specific questions across videos, hindering direct comparison of model performance across timescales.

Method: ScaleLong embeds questions targeting four hierarchical timescales (clip, shot, event, story) within the same video content, featuring 269 long videos with 4-8 questions per video.

Result: Evaluation of 23 MLLMs shows a U-shaped performance curve, with higher accuracy at the shortest and longest timescales and a dip at intermediate levels. Increased visual token capacity improves reasoning across all timescales.

Conclusion: ScaleLong provides a fine-grained, multi-timescale benchmark to advance MLLM capabilities in long-video understanding, with publicly available code and dataset.

Abstract: Although long-video understanding demands that models capture hierarchical
temporal information -- from clip (seconds) and shot (tens of seconds) to event
(minutes) and story (hours) -- existing benchmarks either neglect this
multi-scale design or scatter scale-specific questions across different videos,
preventing direct comparison of model performance across timescales on the same
content. To address this, we introduce ScaleLong, the first benchmark to
disentangle these factors by embedding questions targeting four hierarchical
timescales -- clip (seconds), shot (tens of seconds), event (minutes), and
story (hours) -- all within the same video content. This within-content
multi-timescale questioning design enables direct comparison of model
performance across timescales on identical videos. ScaleLong features 269 long
videos (avg.\ 86\,min) from 5 main categories and 36 sub-categories, with 4--8
carefully designed questions, including at least one question for each
timescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with
higher accuracy at the shortest and longest timescales and a dip at
intermediate levels. Furthermore, ablation studies show that increased visual
token capacity consistently enhances reasoning across all timescales. ScaleLong
offers a fine-grained, multi-timescale benchmark for advancing MLLM
capabilities in long-video understanding. The code and dataset are available
https://github.com/multimodal-art-projection/ScaleLong.

</details>


### [260] [Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts](https://arxiv.org/pdf/2505.23926)
*Xuweiyi Chen, Wentao Zhou, Aruni RoyChowdhury, Zezhou Cheng*

Main category: cs.CV

TL;DR: Point-MoE, a Mixture-of-Experts architecture, improves cross-domain generalization in 3D point cloud understanding by automatically specializing experts without domain labels.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud understanding lacks scalability due to domain heterogeneity and inaccessible domain labels at inference time.

Method: Proposes Point-MoE with a top-k routing strategy to specialize experts for mixed-domain data.

Result: Point-MoE outperforms multi-domain baselines and generalizes better to unseen domains.

Conclusion: Point-MoE offers a scalable solution for 3D understanding by leveraging model-discovered structure in diverse data.

Abstract: While scaling laws have transformed natural language processing and computer
vision, 3D point cloud understanding has yet to reach that stage. This can be
attributed to both the comparatively smaller scale of 3D datasets, as well as
the disparate sources of the data itself. Point clouds are captured by diverse
sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,
outdoor), each introducing unique scanning patterns, sampling densities, and
semantic biases. Such domain heterogeneity poses a major barrier towards
training unified models at scale, especially under the realistic constraint
that domain labels are typically inaccessible at inference time. In this work,
we propose Point-MoE, a Mixture-of-Experts architecture designed to enable
large-scale, cross-domain generalization in 3D perception. We show that
standard point cloud backbones degrade significantly in performance when
trained on mixed-domain data, whereas Point-MoE with a simple top-k routing
strategy can automatically specialize experts, even without access to domain
labels. Our experiments demonstrate that Point-MoE not only outperforms strong
multi-domain baselines but also generalizes better to unseen domains. This work
highlights a scalable path forward for 3D understanding: letting the model
discover structure in diverse 3D data, rather than imposing it via manual
curation or domain supervision.

</details>


### [261] [Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review](https://arxiv.org/pdf/2505.23952)
*Adriano Fragomeni, Dima Damen, Michael Wray*

Main category: cs.CV

TL;DR: A survey of 81 papers on Text-to-Video (T2V) retrieval, focusing on methods using auxiliary information (e.g., visual attributes, temporal/spatial context) to improve performance and bridge the semantic gap between text and video.


<details>
  <summary>Details</summary>
Motivation: Traditional T2V retrieval relies on direct alignment of video and text, but incorporating auxiliary information can enhance performance and address the semantic gap.

Method: Review of methodologies in 81 papers, analyzing how auxiliary information (e.g., objects, context, captions) is integrated into T2V retrieval systems.

Result: Highlights state-of-the-art results on benchmark datasets and discusses the role of auxiliary information in improving retrieval accuracy.

Conclusion: Proposes future research directions to further leverage auxiliary information for enhanced T2V retrieval performance.

Abstract: Text-to-Video (T2V) retrieval aims to identify the most relevant item from a
gallery of videos based on a user's text query. Traditional methods rely solely
on aligning video and text modalities to compute the similarity and retrieve
relevant items. However, recent advancements emphasise incorporating auxiliary
information extracted from video and text modalities to improve retrieval
performance and bridge the semantic gap between these modalities. Auxiliary
information can include visual attributes, such as objects; temporal and
spatial context; and textual descriptions, such as speech and rephrased
captions. This survey comprehensively reviews 81 research papers on
Text-to-Video retrieval that utilise such auxiliary information. It provides a
detailed analysis of their methodologies; highlights state-of-the-art results
on benchmark datasets; and discusses available datasets and their auxiliary
information. Additionally, it proposes promising directions for future
research, focusing on different ways to further enhance retrieval performance
using this information.

</details>


### [262] [MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification](https://arxiv.org/pdf/2505.23961)
*Rafi Hassan Chowdhury, Sabbir Ahmed*

Main category: cs.CV

TL;DR: A lightweight Vision Transformer-based pipeline is proposed for efficient mango leaf disease classification, achieving 99.43% accuracy with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Food safety and economic losses due to mango diseases necessitate efficient, low-end device-compatible solutions.

Method: Uses a lightweight Vision Transformer with self-attention and runtime augmentation for disease classification.

Result: Achieves 99.43% accuracy on the MangoLeafBD dataset, outperforming existing methods in efficiency.

Conclusion: The proposed method offers a computationally efficient solution for mango leaf disease classification, suitable for low-end devices.

Abstract: Ensuring food safety is critical due to its profound impact on public health,
economic stability, and global supply chains. Cultivation of Mango, a major
agricultural product in several South Asian countries, faces high financial
losses due to different diseases, affecting various aspects of the entire
supply chain. While deep learning-based methods have been explored for mango
leaf disease classification, there remains a gap in designing solutions that
are computationally efficient and compatible with low-end devices. In this
work, we propose a lightweight Vision Transformer-based pipeline with a
self-attention mechanism to classify mango leaf diseases, achieving
state-of-the-art performance with minimal computational overhead. Our approach
leverages global attention to capture intricate patterns among disease types
and incorporates runtime augmentation for enhanced performance. Evaluation on
the MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing
methods in terms of model size, parameter count, and FLOPs count.

</details>


### [263] [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/pdf/2505.23977)
*Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran*

Main category: cs.CV

TL;DR: VisualSphinx is a synthetic dataset for training VLMs in visual logical reasoning, improving their performance on tasks like diagram understanding and spatial problem solving.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack large-scale, structured training data for effective multimodal reasoning, hindering tasks like logical coherence and spatial problem solving.

Method: A rule-to-image synthesis pipeline is proposed, extracting and expanding puzzle rules from seed questions to generate grounding images for training.

Result: VLMs trained on VisualSphinx show improved logical reasoning performance, benefiting tasks like algebraic, arithmetic, and geometry reasoning.

Conclusion: VisualSphinx addresses the data gap for VLMs, enhancing their reasoning capabilities across multiple domains.

Abstract: Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

</details>


### [264] [DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets](https://arxiv.org/pdf/2505.23980)
*Bayu Adhi Tama, Mansa Krishna, Homayra Alam, Mostafa Cham, Omar Faruque, Gong Cheng, Jianwu Wang, Mathieu Morlighem, Vandana Janeja*

Main category: cs.CV

TL;DR: DeepTopoNet, a deep learning framework, improves Greenland's subglacial topography reconstruction by integrating radar and BedMachine data with adaptive loss-balancing, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: Accurate subglacial topography is crucial for predicting Greenland's ice sheet mass loss and sea-level rise, but sparse radar data creates uncertainty.

Method: DeepTopoNet combines radar and BedMachine data using a dynamic loss-balancing mechanism, gradient-based features, and a CNN for subgrid-scale predictions.

Result: The model achieves high accuracy in reconstructing subglacial terrain, especially in the Upernavik Isstrøm region, outperforming baselines.

Conclusion: Deep learning bridges observational gaps, offering a scalable solution for subglacial topography inference.

Abstract: Understanding Greenland's subglacial topography is critical for projecting
the future mass loss of the ice sheet and its contribution to global sea-level
rise. However, the complex and sparse nature of observational data,
particularly information about the bed topography under the ice sheet,
significantly increases the uncertainty in model projections. Bed topography is
traditionally measured by airborne ice-penetrating radar that measures the ice
thickness directly underneath the aircraft, leaving data gap of tens of
kilometers in between flight lines. This study introduces a deep learning
framework, which we call as DeepTopoNet, that integrates radar-derived ice
thickness observations and BedMachine Greenland data through a novel dynamic
loss-balancing mechanism. Among all efforts to reconstruct bed topography,
BedMachine has emerged as one of the most widely used datasets, combining mass
conservation principles and ice thickness measurements to generate
high-resolution bed elevation estimates. The proposed loss function adaptively
adjusts the weighting between radar and BedMachine data, ensuring robustness in
areas with limited radar coverage while leveraging the high spatial resolution
of BedMachine predictions i.e. bed estimates. Our approach incorporates
gradient-based and trend surface features to enhance model performance and
utilizes a CNN architecture designed for subgrid-scale predictions. By
systematically testing on the Upernavik Isstr{\o}m) region, the model achieves
high accuracy, outperforming baseline methods in reconstructing subglacial
terrain. This work demonstrates the potential of deep learning in bridging
observational gaps, providing a scalable and efficient solution to inferring
subglacial topography.

</details>


### [265] [DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment](https://arxiv.org/pdf/2505.24002)
*Vaishnav Ramesh, Junliang Liu, Haining Wang, Md Jahidul Islam*

Main category: cs.CV

TL;DR: The paper introduces Depth-CAR and TCB mechanisms to improve NR-IQA by leveraging scene depth and multimodal attention, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of generalization in NR-IQA by integrating human subjective perception with objective features.

Method: Proposes Depth-CAR for scene depth and spatial feature distillation, and TCB for fusing transformer and CNN features.

Result: DGIQA model achieves SOTA performance on synthetic and authentic datasets, excelling in cross-dataset evaluations.

Conclusion: The Depth-CAR and TCB mechanisms enhance NR-IQA, demonstrating superior generalization and efficiency.

Abstract: A long-held challenge in no-reference image quality assessment (NR-IQA)
learning from human subjective perception is the lack of objective
generalization to unseen natural distortions. To address this, we integrate a
novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which
distills scene depth and spatial features into a structure-aware representation
for improved NR-IQA. This brings in the knowledge of object saliency and
relative contrast of the scene for more discriminative feature learning.
Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse
high-level global contextual dependencies from a transformer backbone with
local spatial features captured by a set of hierarchical CNN (convolutional
neural network) layers. We implement TCB and Depth-CAR as multimodal
attention-based projection functions to select the most informative features,
which also improve training time and inference efficiency. Experimental results
demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)
performance on both synthetic and authentic benchmark datasets. More
importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well
as in assessing natural image distortions such as low-light effects, hazy
conditions, and lens flares.

</details>


### [266] [Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model](https://arxiv.org/pdf/2505.24007)
*Nokimul Hasan Arif, Shadman Rabby, Md Hefzul Hossain Papon, Sabbir Ahmed*

Main category: cs.CV

TL;DR: A novel ensemble-based preprocessing framework reduces visual hallucinations in LLMs by 44.3% without modifying the model, using adaptive input filtering.


<details>
  <summary>Details</summary>
Motivation: Visual hallucinations in LLMs undermine reliability, especially in critical contexts, with limited research on preprocessing solutions.

Method: An ensemble framework adaptively selects filtering (noise reduced, edge enhanced, or unaltered input) based on question type.

Result: 44.3% reduction in hallucination rates on the HaloQuest dataset, measured by NLI scores.

Conclusion: Adaptive preprocessing significantly enhances factual grounding, enabling more reliable multimodal systems.

Abstract: Visual hallucinations in Large Language Models (LLMs), where the model
generates responses that are inconsistent with the visual input, pose a
significant challenge to their reliability, particularly in contexts where
precise and trustworthy outputs are critical. Current research largely
emphasizes post-hoc correction or model-specific fine-tuning strategies, with
limited exploration of preprocessing techniques to address hallucination issues
at the input stage. This study presents a novel ensemble-based preprocessing
framework that adaptively selects the most appropriate filtering approach --
noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the
type of question posed, resulting into reduced hallucination without requiring
any modifications to the underlying model architecture or training pipeline.
Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal
reasoning on visually complex inputs, our method achieves a 44.3% reduction in
hallucination rates, as measured by Natural Language Inference (NLI) scores
using SelfCheckGPT. This demonstrates that intelligent input conditioning alone
can significantly enhance factual grounding in LLM responses. The findings
highlight the importance of adaptive preprocessing techniques in mitigating
hallucinations, paving the way for more reliable multimodal systems capable of
addressing real-world challenges.

</details>


### [267] [Multi-Group Proportional Representation for Text-to-Image Models](https://arxiv.org/pdf/2505.24023)
*Sangwon Jung, Alex Oesterling, Claudio Mayrink Verdun, Sajani Vithana, Taesup Moon, Flavio P. Calmon*

Main category: cs.CV

TL;DR: A framework to measure and improve representation of intersectional groups in text-to-image models using the Multi-Group Proportional Representation (MPR) metric.


<details>
  <summary>Details</summary>
Motivation: Address concerns about demographic diversity, stereotypes, and minority representation in AI-generated images, lacking systematic measurement tools.

Method: Introduces MPR to evaluate worst-case representation deviation across groups and develops an algorithm to optimize models for balanced generation.

Result: MPR effectively measures representation and, as a training objective, improves balance across groups without compromising image quality.

Conclusion: The framework provides a systematic way to control representational harms in T2I models, promoting fairness and diversity.

Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from
textual descriptions. As these models proliferate, they expose new concerns
about their ability to represent diverse demographic groups, propagate
stereotypes, and efface minority populations. Despite growing attention to the
"safe" and "responsible" design of artificial intelligence (AI), there is no
established methodology to systematically measure and control representational
harms in image generation. This paper introduces a novel framework to measure
the representation of intersectional groups in images generated by T2I models
by applying the Multi-Group Proportional Representation (MPR) metric. MPR
evaluates the worst-case deviation of representation statistics across given
population groups in images produced by a generative model, allowing for
flexible and context-specific measurements based on user requirements. We also
develop an algorithm to optimize T2I models for this metric. Through
experiments, we demonstrate that MPR can effectively measure representation
statistics across multiple intersectional groups and, when used as a training
objective, can guide models toward a more balanced generation across
demographic groups while maintaining generation quality.

</details>


### [268] [DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/pdf/2505.24025)
*Chenbin Pan, Wenbin He, Zhengzhong Tu, Liu Ren*

Main category: cs.CV

TL;DR: DINO-R1 introduces GRQO, a reinforcement learning method for vision models, enhancing visual reasoning and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To extend reasoning capabilities, currently limited to language models, to vision foundation models like DINO.

Method: Proposes GRQO, a reinforcement-style training strategy with KL-regularization, applied to DINO-R1 models.

Result: DINO-R1 outperforms baselines in COCO, LVIS, and ODinW benchmarks, showing strong generalization.

Conclusion: DINO-R1 successfully integrates reinforcement learning into vision models, advancing visual reasoning capabilities.

Abstract: The recent explosive interest in the reasoning capabilities of large language
models, such as DeepSeek-R1, has demonstrated remarkable success through
reinforcement learning-based fine-tuning frameworks, exemplified by methods
like Group Relative Policy Optimization (GRPO). However, such reasoning
abilities remain underexplored and notably absent in vision foundation models,
including representation models like the DINO series. In this work, we propose
\textbf{DINO-R1}, the first such attempt to incentivize visual in-context
reasoning capabilities of vision foundation models using reinforcement
learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query
Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly
designed for query-based representation models, which computes query-level
rewards based on group-normalized alignment quality. We also apply
KL-regularization to stabilize the objectness distribution to reduce the
training instability. This joint optimization enables dense and expressive
supervision across queries while mitigating overfitting and distributional
drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models
that integrate a visual prompt encoder and a visual-guided query selection
mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that
DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving
strong generalization in both open-vocabulary and closed-set visual prompting
scenarios.

</details>


### [269] [MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking](https://arxiv.org/pdf/2505.24026)
*Numair Nadeem, Muhammad Hamza Asad, Saeed Anwar, Abdul Bais*

Main category: cs.CV

TL;DR: MaskAdapt improves crop and weed segmentation by integrating RGB and depth data, using depth gradients and a geometry-aware masking strategy to address domain shifts and occlusions.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods struggle with occlusions and visual blending in crop-weed segmentation, limiting generalization across different field conditions.

Method: MaskAdapt combines RGB images with depth-derived features, using depth gradients and cross-attention to refine features. It also employs a geometry-aware masking strategy during training.

Result: MaskAdapt outperforms SOTA UDA methods, achieving higher mIOU in diverse field conditions.

Conclusion: MaskAdapt effectively addresses domain adaptation challenges in agricultural segmentation, offering improved accuracy and robustness.

Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm
management; however, most existing methods depend on labor intensive
pixel-level annotations. A further challenge arises when models trained on one
field (source domain) fail to generalize to new fields (target domain) due to
domain shifts, such as variations in lighting, camera setups, soil composition,
and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by
enabling adaptation without target-domain labels, but current UDA methods
struggle with occlusions and visual blending between crops and weeds, leading
to misclassifications in real-world conditions. To overcome these limitations,
we introduce MaskAdapt, a novel approach that enhances segmentation accuracy
through multimodal contextual learning by integrating RGB images with features
derived from depth data. By computing depth gradients from depth maps, our
method captures spatial transitions that help resolve texture ambiguities.
These gradients, through a cross-attention mechanism, refines RGB feature
representations, resulting in sharper boundary delineation. In addition, we
propose a geometry-aware masking strategy that applies horizontal, vertical,
and stochastic masks during training. This encourages the model to focus on the
broader spatial context for robust visual recognition. Evaluations on real
agricultural datasets demonstrate that MaskAdapt consistently outperforms
existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation
mean Intersection over Union (mIOU) across diverse field conditions.

</details>


### [270] [SIM: A mapping framework for built environment auditing based on street view imagery](https://arxiv.org/pdf/2505.24076)
*Huan Ning, Zhenlong Li, Manzhu Yu, Wenpeng Yin*

Main category: cs.CV

TL;DR: The paper introduces an open-source framework for mapping and measuring street objects using street view imagery and deep learning, enhancing built environment auditing efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional built environment audits are time-consuming and costly. Emerging street view imagery and deep learning offer potential for remote auditing, but lack universal mapping frameworks.

Method: The study proposes three pipelines for geospatial mapping: width measurement for ground objects, 3D localization for known-dimension objects, and diameter measurement for objects like street trees.

Result: Case studies demonstrate the framework's effectiveness in road width measurement, stop sign localization, and street tree diameter measurement.

Conclusion: The open-source framework improves auditing productivity and accuracy, benefiting researchers and urban planners.

Abstract: Built environment auditing refers to the systematic documentation and
assessment of urban and rural spaces' physical, social, and environmental
characteristics, such as walkability, road conditions, and traffic lights. It
is used to collect data for the evaluation of how built environments impact
human behavior, health, mobility, and overall urban functionality.
Traditionally, built environment audits were conducted using field surveys and
manual observations, which were time-consuming and costly. The emerging street
view imagery, e.g., Google Street View, has become a widely used data source
for conducting built environment audits remotely. Deep learning and computer
vision techniques can extract and classify objects from street images to
enhance auditing productivity. Before meaningful analysis, the detected objects
need to be geospatially mapped for accurate documentation. However, the mapping
methods and tools based on street images are underexplored, and there are no
universal frameworks or solutions yet, imposing difficulties in auditing the
street objects. In this study, we introduced an open source street view mapping
framework, providing three pipelines to map and measure: 1) width measurement
for ground objects, such as roads; 2) 3D localization for objects with a known
dimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,
street trees). These pipelines can help researchers, urban planners, and other
professionals automatically measure and map target objects, promoting built
environment auditing productivity and accuracy. Three case studies, including
road width measurement, stop sign localization, and street tree diameter
measurement, are provided in this paper to showcase pipeline usage.

</details>


### [271] [ComposeAnything: Composite Object Priors for Text-to-Image Generation](https://arxiv.org/pdf/2505.24086)
*Zeeshan Khan, Shizhe Chen, Cordelia Schmid*

Main category: cs.CV

TL;DR: ComposeAnything improves text-to-image generation by using LLMs to create 2.5D semantic layouts, enhancing object arrangements and coherence without retraining models.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with complex object arrangements and 3D positioning, often sacrificing quality.

Method: Leverages LLMs for 2.5D semantic layouts, uses spatial and depth-aware coarse composites as priors, and guides denoising for coherent generation.

Result: Outperforms state-of-the-art methods on benchmarks for 2D/3D arrangements and surreal compositions, with human evaluations confirming quality.

Conclusion: ComposeAnything effectively addresses compositional challenges in T2I models, improving fidelity and coherence.

Abstract: Generating images from text involving complex and novel object arrangements
remains a significant challenge for current text-to-image (T2I) models.
Although prior layout-based methods improve object arrangements using spatial
constraints with 2D layouts, they often struggle to capture 3D positioning and
sacrifice quality and coherence. In this work, we introduce ComposeAnything, a
novel framework for improving compositional image generation without retraining
existing T2I models. Our approach first leverages the chain-of-thought
reasoning abilities of LLMs to produce 2.5D semantic layouts from text,
consisting of 2D object bounding boxes enriched with depth information and
detailed captions. Based on this layout, we generate a spatial and depth aware
coarse composite of objects that captures the intended composition, serving as
a strong and interpretable prior that replaces stochastic noise initialization
in diffusion-based T2I models. This prior guides the denoising process through
object prior reinforcement and spatial-controlled denoising, enabling seamless
generation of compositional objects and coherent backgrounds, while allowing
refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art
methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D
spatial arrangements, high object counts, and surreal compositions. Human
evaluations further demonstrate that our model generates high-quality images
with compositions that faithfully reflect the text.

</details>


### [272] [Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors](https://arxiv.org/pdf/2505.24103)
*Peiran Xu, Yadong Mu*

Main category: cs.CV

TL;DR: A weakly supervised affordance grounding model is proposed, leveraging pseudo labels from part segmentation and foundation models, with enhancements like label refining and feature alignment, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of class activation maps for affordance grounding and improve the identification of affordance regions without dense labels.

Method: Uses pseudo labels from a part segmentation model, guided by affordance-part mapping, and introduces label refining, feature alignment, and a reasoning module.

Result: Achieves breakthrough improvement over existing methods in affordance grounding.

Conclusion: The proposed model effectively bridges the gap between objects and actions, demonstrating significant performance gains.

Abstract: In this work, we focus on the task of weakly supervised affordance grounding,
where a model is trained to identify affordance regions on objects using
human-object interaction images and egocentric object images without dense
labels. Previous works are mostly built upon class activation maps, which are
effective for semantic segmentation but may not be suitable for locating
actions and functions. Leveraging recent advanced foundation models, we develop
a supervised training pipeline based on pseudo labels. The pseudo labels are
generated from an off-the-shelf part segmentation model, guided by a mapping
from affordance to part names. Furthermore, we introduce three key enhancements
to the baseline model: a label refining stage, a fine-grained feature alignment
process, and a lightweight reasoning module. These techniques harness the
semantic knowledge of static objects embedded in off-the-shelf foundation
models to improve affordance learning, effectively bridging the gap between
objects and actions. Extensive experiments demonstrate that the performance of
the proposed model has achieved a breakthrough improvement over existing
methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .

</details>


### [273] [DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT Space](https://arxiv.org/pdf/2412.15032)
*Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Wenshuo Chen, Albert Ali Salah, Itir Onal Ertugrul*

Main category: cs.CV

TL;DR: DCTdiff is a diffusion-based generative model operating in the DCT space, outperforming pixel-based models in quality and efficiency, and scaling to high resolutions without latent diffusion.


<details>
  <summary>Details</summary>
Motivation: To explore image modeling in the frequency space and improve upon the limitations of pixel-based diffusion models.

Method: Introduces DCTdiff, an end-to-end diffusion paradigm in DCT space, tested across frameworks, tasks, and samplers.

Result: DCTdiff achieves better generative quality and training efficiency, scales to 512x512 resolution, and outperforms latent diffusion at lower cost.

Conclusion: DCTdiff demonstrates the potential of frequency-space image modeling, bridging diffusion and autoregressive models, and offers a promising direction for future research.

Abstract: This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to 512$\times$512 resolution without using the latent diffusion
paradigm and beats latent diffusion (using SD-VAE) with only 1/4 training cost.
Finally, we illustrate several intriguing properties of DCT image modeling. For
example, we provide a theoretical proof of why 'image diffusion can be seen as
spectral autoregression', bridging the gap between diffusion and autoregressive
models. The effectiveness of DCTdiff and the introduced properties suggest a
promising direction for image modeling in the frequency space. The code is
https://github.com/forever208/DCTdiff.

</details>


### [274] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/pdf/2505.24108)
*Alina Devkota, Annahita Amireskandari, Joel Palko, Shyam Thakkar, Donald Adjeroh, Xiajun Jiang, Binod Bhattarai, Prashnna K. Gyawali*

Main category: cs.CV

TL;DR: A federated learning (FL) framework is proposed to train foundation models for gastroendoscopy imaging, addressing data privacy and scarcity issues while improving performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Deep learning for GI diagnostics requires labeled datasets, which are costly. Foundation models can help but face challenges due to privacy restrictions on medical data.

Method: A FL framework is introduced to train foundation models locally without sharing data, using established FL algorithms in homogeneous and heterogeneous settings.

Result: The foundation model improves performance on classification, detection, and segmentation tasks in a privacy-preserving manner.

Conclusion: The FL framework effectively trains foundation models for gastroendoscopy, overcoming data privacy and scarcity challenges.

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [275] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/pdf/2505.24120)
*Ai Jian, Weijie Qiu, Xiaokun Wang, Peiyu Wang, Yunzhuo Hao, Jiangbo Pei, Yichen Wei, Yi Peng, Xuchen Song*

Main category: cs.CV

TL;DR: CSVQA is a new benchmark for evaluating scientific reasoning in Vision-Language Models (VLMs), featuring 1,378 STEM-based questions requiring domain knowledge and visual analysis. It highlights gaps in current VLMs' capabilities, with top models achieving only 49.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack authentic scientific contexts for evaluating VLMs' scientific reasoning, necessitating a domain-specific, multimodal benchmark.

Method: CSVQA includes 1,378 STEM question-answer pairs demanding domain knowledge, visual evidence integration, and higher-order reasoning. A rigorous evaluation protocol assesses model predictions based on intermediate reasoning steps.

Result: Evaluation of 15 VLMs shows significant performance gaps, with the best model achieving only 49.6% accuracy, revealing limitations in scientific reasoning.

Conclusion: CSVQA underscores the need for advancing VLMs' scientific reasoning capabilities and provides a tool for future research.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in
multimodal understanding, yet their capabilities for scientific reasoning
remains inadequately assessed. Current multimodal benchmarks predominantly
evaluate generic image comprehension or text-driven reasoning, lacking
authentic scientific contexts that require domain-specific knowledge
integration with visual evidence analysis. To fill this gap, we present CSVQA,
a diagnostic multimodal benchmark specifically designed for evaluating
scientific reasoning through domain-grounded visual question answering.Our
benchmark features 1,378 carefully constructed question-answer pairs spanning
diverse STEM disciplines, each demanding domain knowledge, integration of
visual evidence, and higher-order reasoning. Compared to prior multimodal
benchmarks, CSVQA places greater emphasis on real-world scientific content and
complex reasoning.We additionally propose a rigorous evaluation protocol to
systematically assess whether model predictions are substantiated by valid
intermediate reasoning steps based on curated explanations. Our comprehensive
evaluation of 15 VLMs on this benchmark reveals notable performance
disparities, as even the top-ranked proprietary model attains only 49.6\%
accuracy.This empirical evidence underscores the pressing need for advancing
scientific reasoning capabilities in VLMs. Our CSVQA is released at
https://huggingface.co/datasets/Skywork/CSVQA.

</details>


### [276] [CAE-Net: Generalized Deepfake Image Detection using Convolution and Attention Mechanisms with Spatial and Frequency Domain Features](https://arxiv.org/pdf/2502.10682)
*Kafi Anan, Anindya Bhattacharjee, Ashir Intesher, Kaidul Islam, Abrar Assaeem Fuad, Utsab Saha, Hafiz Imtiaz*

Main category: cs.CV

TL;DR: A multistage training method and ensemble architecture (CAE-Net) are proposed for deepfake detection, achieving high accuracy and robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The rise of deepfakes in unethical practices necessitates effective detection tools, but diverse generation techniques complicate universal detection.

Method: Proposes a disjoint set-based multistage training method and an ensemble network (CAE-Net) combining EfficientNet, DeiT, and ConvNeXt with wavelet transform.

Result: Achieves 94.63% accuracy, 4.72% equal error rate, and 97.37% AUC, with robustness against adversarial attacks.

Conclusion: The ensemble approach effectively addresses class imbalance and generalizes well for deepfake detection.

Abstract: Effective deepfake detection tools are becoming increasingly essential to the
growing usage of deepfakes in unethical practices. There exists a wide range of
deepfake generation techniques, which makes it challenging to develop an
accurate universal detection mechanism. The 2025 IEEE Signal Processing Cup
(\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images
containing significant class imbalance. The images in the dataset are generated
from multiple deepfake image generators, for training machine learning model(s)
to emphasize the generalization of deepfake detection. To this end, we proposed
a disjoint set-based multistage training method to address the class imbalance
and devised an ensemble-based architecture \emph{CAE-Net}. Our architecture
consists of a convolution- and attention-based ensemble network, and employs
three different neural network architectures: EfficientNet, Data-Efficient
Image Transformer (DeiT), and ConvNeXt with wavelet transform to capture both
local and global features of deepfakes. We visualize the specific regions that
these models focus on for classification using Grad-CAM, and empirically
demonstrate the effectiveness of these models in grouping real and fake images
into cohesive clusters using t-SNE plots. Individually, the EfficientNet B0
architecture has achieved 90.79\% accuracy, whereas the ConvNeXt and the DeiT
architecture have achieved 89.49\% and 89.32\% accuracy, respectively. With
these networks, our weighted ensemble model achieves an excellent accuracy of
94.63\% on the validation dataset of the SP Cup 2025 competition. The equal
error rate of 4.72\% and the Area Under the ROC curve of 97.37\% further
confirm the stability of our proposed method. Finally, the robustness of our
proposed model against adversarial perturbation attacks is tested as well,
showing the inherent defensive properties of the ensemble approach.

</details>


### [277] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/pdf/2505.24139)
*Yichen Xie, Runsheng Xu, Tong He, Jyh-Jing Hwang, Katie Luo, Jingwei Ji, Hubert Lin, Letian Chen, Yiren Lu, Zhaoqi Leng, Dragomir Anguelov, Mingxing Tan*

Main category: cs.CV

TL;DR: S4-Driver is a self-supervised motion planning algorithm for autonomous driving, leveraging MLLMs with a novel 3D representation, outperforming supervised methods without human annotations.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end motion planning methods rely on human annotations or underperform in self-supervised settings. MLLMs pretrained in 2D lack native 3D planning capabilities.

Method: S4-Driver uses a sparse volume strategy to adapt MLLMs' 2D visual representations to 3D space, aggregating multi-view and multi-frame inputs for trajectory prediction.

Result: S4-Driver outperforms supervised multi-task approaches on nuScenes and Waymo datasets without human annotations and scales well with unannotated data.

Conclusion: S4-Driver bridges the gap in 3D planning representation, offering a scalable, annotation-free solution for autonomous driving.

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [278] [The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models](https://arxiv.org/pdf/2505.24141)
*Jiashuai Liu, Yingjia Shang, Yingkang Zhan, Di Zhang, Yi Niu, Dong Wei, Xian Wu, Zeyu Gao, Chen Li, Yefeng Zheng*

Main category: cs.CV

TL;DR: The paper investigates adversarial attacks on pathology foundation models for WSI analysis, proposing a label-free attack framework and demonstrating significant accuracy degradation.


<details>
  <summary>Details</summary>
Motivation: The security of pathology foundation models is critical but underexplored, especially their vulnerability to adversarial attacks.

Method: The authors introduce a label-free attack framework with local perturbation and global impact, revising four white-box attack methods and redefining perturbation budgets for WSI.

Result: Experiments show up to 20% accuracy degradation by modifying just 0.1% of patches per slide. Key attack factors and patch-level vulnerabilities are analyzed.

Conclusion: The findings highlight the need for adversarial robustness research to ensure reliable deployment of pathology foundation models.

Abstract: With the widespread adoption of pathology foundation models in both research
and clinical decision support systems, exploring their security has become a
critical concern. However, despite their growing impact, the vulnerability of
these models to adversarial attacks remains largely unexplored. In this work,
we present the first systematic investigation into the security of pathology
foundation models for whole slide image~(WSI) analysis against adversarial
attacks. Specifically, we introduce the principle of \textit{local perturbation
with global impact} and propose a label-free attack framework that operates
without requiring access to downstream task labels. Under this attack
framework, we revise four classical white-box attack methods and redefine the
perturbation budget based on the characteristics of WSI. We conduct
comprehensive experiments on three representative pathology foundation models
across five datasets and six downstream tasks. Despite modifying only 0.1\% of
patches per slide with imperceptible noise, our attack leads to downstream
accuracy degradation that can reach up to 20\% in the worst cases. Furthermore,
we analyze key factors that influence attack success, explore the relationship
between patch-level vulnerability and semantic content, and conduct a
preliminary investigation into potential defence strategies. These findings lay
the groundwork for future research on the adversarial robustness and reliable
deployment of pathology foundation models. Our code is publicly available at:
https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.

</details>


### [279] [V2SFlow: Video-to-Speech Generation with Speech Decomposition and Rectified Flow](https://arxiv.org/pdf/2411.19486)
*Jeongsoo Choi, Ji-Hoon Kim, Jinyu Li, Joon Son Chung, Shujie Liu*

Main category: cs.CV

TL;DR: V2SFlow is a new Video-to-Speech framework that generates natural speech from silent talking face videos by decomposing speech into content, pitch, and speaker attributes, and using a Transformer-based decoder.


<details>
  <summary>Details</summary>
Motivation: Existing V2S systems struggle with real-world datasets due to speech variability. V2SFlow aims to improve performance by addressing these challenges.

Method: Decomposes speech into content, pitch, and speaker subspaces, predicts them visually, and uses a Transformer-based rectified flow matching decoder for speech generation.

Result: Outperforms state-of-the-art methods, even surpassing ground truth naturalness.

Conclusion: V2SFlow is effective for generating natural speech from silent videos, with superior performance on real-world datasets.

Abstract: In this paper, we introduce V2SFlow, a novel Video-to-Speech (V2S) framework
designed to generate natural and intelligible speech directly from silent
talking face videos. While recent V2S systems have shown promising results on
constrained datasets with limited speakers and vocabularies, their performance
often degrades on real-world, unconstrained datasets due to the inherent
variability and complexity of speech signals. To address these challenges, we
decompose the speech signal into manageable subspaces (content, pitch, and
speaker information), each representing distinct speech attributes, and predict
them directly from the visual input. To generate coherent and realistic speech
from these predicted attributes, we employ a rectified flow matching decoder
built on a Transformer architecture, which models efficient probabilistic
pathways from random noise to the target speech distribution. Extensive
experiments demonstrate that V2SFlow significantly outperforms state-of-the-art
methods, even surpassing the naturalness of ground truth utterances. Code and
models are available at: https://github.com/kaistmm/V2SFlow

</details>


### [280] [Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction](https://arxiv.org/pdf/2505.24156)
*Chenyou Fan, Fangzheng Yan, Chenjia Bai, Jiepeng Wang, Chi Zhang, Zhen Wang, Xuelong Li*

Main category: cs.CV

TL;DR: The paper proposes a novel bimanual foundation policy using fine-tuned text-to-video models and a lightweight diffusion policy to address challenges in generalizing bimanual manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to generalize effectively due to scarcity of bimanual data and differences between single-arm and bimanual manipulation.

Method: A two-stage paradigm fine-tunes text-to-flow and flow-to-video models, using optical flow as an intermediate variable for intent concretization and video prediction.

Result: The method reduces language ambiguity and robot-data requirements, demonstrating effectiveness in simulation and real-world experiments.

Conclusion: The proposed approach successfully addresses generalization challenges in bimanual manipulation by leveraging intermediate flow representations and reducing reliance on low-level actions.

Abstract: Learning a generalizable bimanual manipulation policy is extremely
challenging for embodied agents due to the large action space and the need for
coordinated arm movements. Existing approaches rely on Vision-Language-Action
(VLA) models to acquire bimanual policies. However, transferring knowledge from
single-arm datasets or pre-trained VLA models often fails to generalize
effectively, primarily due to the scarcity of bimanual data and the fundamental
differences between single-arm and bimanual manipulation. In this paper, we
propose a novel bimanual foundation policy by fine-tuning the leading
text-to-video models to predict robot trajectories and training a lightweight
diffusion policy for action generation. Given the lack of embodied knowledge in
text-to-video models, we introduce a two-stage paradigm that fine-tunes
independent text-to-flow and flow-to-video models derived from a pre-trained
text-to-video model. Specifically, optical flow serves as an intermediate
variable, providing a concise representation of subtle movements between
images. The text-to-flow model predicts optical flow to concretize the intent
of language instructions, and the flow-to-video model leverages this flow for
fine-grained video prediction. Our method mitigates the ambiguity of language
in single-stage text-to-video prediction and significantly reduces the
robot-data requirement by avoiding direct use of low-level actions. In
experiments, we collect high-quality manipulation data for real dual-arm robot,
and the results of simulation and real-world experiments demonstrate the
effectiveness of our method.

</details>


### [281] [Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders](https://arxiv.org/pdf/2505.24158)
*Bo Fang, Wenhao Wu, Qiangqiang Wu, Yuxin Song, Antoni B. Chan*

Main category: cs.CV

TL;DR: Nar-KFC is a plug-and-play module for MLLMs that optimizes keyframe selection and adds textual narratives to improve long video understanding efficiently.


<details>
  <summary>Details</summary>
Motivation: The challenge of balancing visual tokens and limited context length in MLLMs for long videos, avoiding irrelevant content and computational burden.

Method: Keyframe selection via integer quadratic programming (with a greedy alternative) and interleaving textual narratives from non-keyframes.

Result: Significant performance improvement on long-video benchmarks.

Conclusion: Nar-KFC effectively compresses and enhances long video understanding for MLLMs.

Abstract: Employing Multimodal Large Language Models (MLLMs) for long video
understanding remains a challenging problem due to the dilemma between the
substantial number of video frames (i.e., visual tokens) versus the limited
context length of language models. Traditional uniform sampling often leads to
selection of irrelevant content, while post-training MLLMs on thousands of
frames imposes a substantial computational burden. In this paper, we propose
threading keyframes with narratives (Nar-KFC), a plug-and-play module to
facilitate effective and efficient long video perception. Nar-KFC generally
involves two collaborative steps. First, we formulate the keyframe selection
process as an integer quadratic programming problem, jointly optimizing
query-relevance and frame-diversity. To avoid its computational complexity, a
customized greedy search strategy is designed as an efficient alternative.
Second, to mitigate the temporal discontinuity caused by sparse keyframe
sampling, we further introduce interleaved textual narratives generated from
non-keyframes using off-the-shelf captioners. These narratives are inserted
between keyframes based on their true temporal order, forming a coherent and
compact representation. Nar-KFC thus serves as a temporal- and content-aware
compression strategy that complements visual and textual modalities.
Experimental results on multiple long-video benchmarks demonstrate that Nar-KFC
significantly improves the performance of popular MLLMs. Code will be made
publicly available.

</details>


### [282] [Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry](https://arxiv.org/pdf/2505.24162)
*Isaac Aguirre, Ivan Sipiran*

Main category: cs.CV

TL;DR: A training-free, zero-shot 3D symmetry detection method using DINOv2 features, outperforming traditional and learning-based approaches.


<details>
  <summary>Details</summary>
Motivation: To leverage foundation vision models for solving complex 3D geometric problems like symmetry detection without training data.

Method: Extracts visual features from rendered 3D object views using DINOv2, backprojects them onto geometry, and identifies symmetry planes via a proposed algorithm.

Result: Outperforms traditional geometric and learning-based methods on ShapeNet subset, demonstrating symmetric invariance of features.

Conclusion: Foundation vision models like DINOv2 can effectively solve 3D geometric tasks without training, as shown in symmetry detection.

Abstract: We present a simple yet effective training-free approach for zero-shot 3D
symmetry detection that leverages visual features from foundation vision models
such as DINOv2. Our method extracts features from rendered views of 3D objects
and backprojects them onto the original geometry. We demonstrate the symmetric
invariance of these features and use them to identify reflection-symmetry
planes through a proposed algorithm. Experiments on a subset of ShapeNet
demonstrate that our approach outperforms both traditional geometric methods
and learning-based approaches without requiring any training data. Our work
demonstrates how foundation vision models can help in solving complex 3D
geometric problems such as symmetry detection.

</details>


### [283] [Pretraining Deformable Image Registration Networks with Random Images](https://arxiv.org/pdf/2505.24167)
*Junyu Chen, Shuwen Wei, Yihao Liu, Aaron Carass, Yong Du*

Main category: cs.CV

TL;DR: Pretraining DNNs for medical image registration using random images improves accuracy, reduces domain-specific data needs, and speeds up convergence.


<details>
  <summary>Details</summary>
Motivation: Leverage the insight that DNNs can generalize from non-medical random images to medical data, reducing reliance on domain-specific training data.

Method: Use registration between random images as a proxy task for pretraining a foundation model for image registration.

Result: Pretraining improves registration accuracy, reduces domain-specific data requirements, and accelerates downstream training convergence.

Conclusion: Pretraining with random images is an effective strategy for enhancing medical image registration models.

Abstract: Recent advances in deep learning-based medical image registration have shown
that training deep neural networks~(DNNs) does not necessarily require medical
images. Previous work showed that DNNs trained on randomly generated images
with carefully designed noise and contrast properties can still generalize well
to unseen medical data. Building on this insight, we propose using registration
between random images as a proxy task for pretraining a foundation model for
image registration. Empirical results show that our pretraining strategy
improves registration accuracy, reduces the amount of domain-specific data
needed to achieve competitive performance, and accelerates convergence during
downstream training, thereby enhancing computational efficiency.

</details>


### [284] [DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?](https://arxiv.org/pdf/2505.24173)
*Tianhong Zhou, Yin Xu, Yingtao Zhu, Chuxi Xiao, Haiyang Bian, Lei Wei, Xuegong Zhang*

Main category: cs.CV

TL;DR: DrVD-Bench is a new benchmark for evaluating clinical visual reasoning in vision-language models (VLMs), designed to assess whether they reason like human clinicians or rely on superficial patterns.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack systematic evaluation of whether VLMs truly mimic human clinical reasoning or just superficial patterns.

Method: DrVD-Bench includes three modules (Visual Evidence Comprehension, Reasoning Trajectory Assessment, Report Generation Evaluation) with 7,789 image-question pairs, covering 20 task types, 17 diagnostic categories, and five imaging modalities.

Result: Performance of 19 VLMs drops as reasoning complexity increases, with models often relying on shortcut correlations rather than grounded understanding.

Conclusion: DrVD-Bench provides a structured framework to develop clinically trustworthy VLMs by rigorously evaluating their reasoning capabilities.

Abstract: Vision-language models (VLMs) exhibit strong zero-shot generalization on
natural images and show early promise in interpretable medical image analysis.
However, existing benchmarks do not systematically evaluate whether these
models truly reason like human clinicians or merely imitate superficial
patterns. To address this gap, we propose DrVD-Bench, the first multimodal
benchmark for clinical visual reasoning. DrVD-Bench consists of three modules:
Visual Evidence Comprehension, Reasoning Trajectory Assessment, and Report
Generation Evaluation, comprising a total of 7,789 image-question pairs. Our
benchmark covers 20 task types, 17 diagnostic categories, and five imaging
modalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is
explicitly structured to reflect the clinical reasoning workflow from modality
recognition to lesion identification and diagnosis. We benchmark 19 VLMs,
including general-purpose and medical-specific, open-source and proprietary
models, and observe that performance drops sharply as reasoning complexity
increases. While some models begin to exhibit traces of human-like reasoning,
they often still rely on shortcut correlations rather than grounded visual
understanding. DrVD-Bench offers a rigorous and structured evaluation framework
to guide the development of clinically trustworthy VLMs.

</details>


### [285] [Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT](https://arxiv.org/pdf/2505.24182)
*Zhuobai Dong, Junchao Yi, Ziyuan Zheng, Haochen Han, Xiangxi Zheng, Alex Jinpeng Wang, Fangming Liu, Linjie Li*

Main category: cs.CV

TL;DR: MVPBench evaluates MLLMs' visual physical reasoning, revealing their struggles with basic physics and coherent reasoning chains, despite advances like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To address the gap in MLLMs' ability to understand physical laws, spatial relations, and causality in visual contexts.

Method: Introduces MVPBench, a benchmark with multi-image inputs and visual chain-of-thought (CoT) reasoning, using a graph-based CoT consistency metric.

Result: Cutting-edge MLLMs perform poorly in visual physical reasoning, and RL-based alignment often worsens spatial reasoning.

Conclusion: Current MLLMs lack robust visual physical reasoning, and fine-tuning practices need reevaluation.

Abstract: Understanding the physical world - governed by laws of motion, spatial
relations, and causality - poses a fundamental challenge for multimodal large
language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o
demonstrate impressive perceptual and reasoning capabilities, our investigation
reveals these models struggle profoundly with visual physical reasoning,
failing to grasp basic physical laws, spatial interactions, and causal effects
in complex scenes. More importantly, they often fail to follow coherent
reasoning chains grounded in visual evidence, especially when multiple steps
are needed to arrive at the correct answer. To rigorously evaluate this
capability, we introduce MVPBench, a curated benchmark designed to rigorously
evaluate visual physical reasoning through the lens of visual chain-of-thought
(CoT). Each example features interleaved multi-image inputs and demands not
only the correct final answer but also a coherent, step-by-step reasoning path
grounded in evolving visual cues. This setup mirrors how humans reason through
real-world physical processes over time. To ensure fine-grained evaluation, we
introduce a graph-based CoT consistency metric that verifies whether the
reasoning path of model adheres to valid physical logic. Additionally, we
minimize shortcut exploitation from text priors, encouraging models to rely on
visual understanding. Experimental results reveal a concerning trend: even
cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text
alignment in physical domains. Surprisingly, RL-based post-training alignment -
commonly believed to improve visual reasoning performance - often harms spatial
reasoning, suggesting a need to rethink current fine-tuning practices.

</details>


### [286] [Boosting All-in-One Image Restoration via Self-Improved Privilege Learning](https://arxiv.org/pdf/2505.24207)
*Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu*

Main category: cs.CV

TL;DR: SIPL introduces a self-improving paradigm using pseudo-privileged signals for iterative refinement in image restoration, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address unstable optimization and inter-task conflicts in unified image restoration models by leveraging privileged information beyond training.

Method: Uses Proxy Fusion with a Privileged Dictionary to distill and apply high-frequency/structural priors for self-correction during inference.

Result: Achieves PSNR improvements of +4.58 dB on composite tasks and +1.28 dB on diverse benchmarks.

Conclusion: SIPL is effective, broadly applicable, and integrates seamlessly with minimal overhead.

Abstract: Unified image restoration models for diverse and mixed degradations often
suffer from unstable optimization dynamics and inter-task conflicts. This paper
introduces Self-Improved Privilege Learning (SIPL), a novel paradigm that
overcomes these limitations by innovatively extending the utility of privileged
information (PI) beyond training into the inference stage. Unlike conventional
Privilege Learning, where ground-truth-derived guidance is typically discarded
after training, SIPL empowers the model to leverage its own preliminary outputs
as pseudo-privileged signals for iterative self-refinement at test time.
Central to SIPL is Proxy Fusion, a lightweight module incorporating a learnable
Privileged Dictionary. During training, this dictionary distills essential
high-frequency and structural priors from privileged feature representations.
Critically, at inference, the same learned dictionary then interacts with
features derived from the model's initial restoration, facilitating a
self-correction loop. SIPL can be seamlessly integrated into various backbone
architectures, offering substantial performance improvements with minimal
computational overhead. Extensive experiments demonstrate that SIPL
significantly advances the state-of-the-art on diverse all-in-one image
restoration benchmarks. For instance, when integrated with the PromptIR model,
SIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation
tasks and +1.28 dB on diverse five-task benchmarks, underscoring its
effectiveness and broad applicability. Codes are available at our project page
https://github.com/Aitical/SIPL.

</details>


### [287] [STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models](https://arxiv.org/pdf/2505.24210)
*Zheng Tan, Weizhen Wang, Andrea L. Bertozzi, Ernest K. Ryu*

Main category: cs.CV

TL;DR: Proposes STORK, a training-free DM ODE solver for mid-NFE regimes (20-50 NFE), improving generation quality without structural dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on low-NFE regimes, leaving mid-NFE regimes underexplored despite their practical relevance in applications like Stable Diffusion 3.5.

Method: Introduces STORK, a novel ODE solver based on stiff ODE techniques and Taylor expansion, applicable to any DM sampling.

Result: STORK outperforms prior methods in mid-NFE regimes, achieving better FID scores for both pixel-level and latent-space generation.

Conclusion: STORK is a versatile and effective solution for mid-NFE DM sampling, with potential for broad application in high-quality generation tasks.

Abstract: Diffusion models (DMs) have demonstrated remarkable performance in
high-fidelity image and video generation. Because high-quality generations with
DMs typically require a large number of function evaluations (NFEs), resulting
in slow sampling, there has been extensive research successfully reducing the
NFE to a small range (<10) while maintaining acceptable image quality. However,
many practical applications, such as those involving Stable Diffusion 3.5,
FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve
superior results, and, despite the practical relevance, research on the
effective sampling within this mid-NFE regime remains underexplored. In this
work, we propose a novel, training-free, and structure-independent DM ODE
solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,
based on a class of stiff ODE solvers with a Taylor expansion adaptation.
Unlike prior work such as DPM-Solver, which is dependent on the semi-linear
structure of the DM ODE, STORK is applicable to any DM sampling, including
noise-based and flow matching-based models. Within the 20-50 NFE range, STORK
achieves improved generation quality, as measured by FID scores, across
unconditional pixel-level generation and conditional latent-space generation
tasks using models like Stable Diffusion 3.5 and SANA. Code is available at
https://github.com/ZT220501/STORK.

</details>


### [288] [Benchmarking Foundation Models for Zero-Shot Biometric Tasks](https://arxiv.org/pdf/2505.24214)
*Redwan Sony, Parisa Farmanifard, Hamzeh Alzwairy, Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: The paper evaluates the zero-shot and few-shot performance of Vision-Language Models (VLMs) and Multi-modal Large Language Models (MLLMs) on six biometric tasks, showing promising results without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored potential of foundation models in biometric recognition and analysis.

Method: A benchmark evaluating 41 VLMs across six biometric tasks (face verification, iris recognition, etc.) using embeddings and simple classifiers.

Result: High accuracy in tasks like face verification (96.77% TMR) and iris recognition (97.55% TMR) without fine-tuning. Simple classifiers improved performance in DeepFake detection and soft biometric attribute prediction.

Conclusion: Pretrained models show significant potential for biometric tasks, advancing toward Artificial General Intelligence.

Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs)
and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of
artificial intelligence, enabling remarkable generalization across diverse
tasks with minimal or no supervision. Yet, their potential in biometric
recognition and analysis remains relatively underexplored. In this work, we
introduce a comprehensive benchmark that evaluates the zero-shot and few-shot
performance of state-of-the-art publicly available VLMs and MLLMs across six
biometric tasks spanning the face and iris modalities: face verification, soft
biometric attribute prediction (gender and race), iris recognition,
presentation attack detection (PAD), and face manipulation detection (morphs
and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments
show that embeddings from these foundation models can be used for diverse
biometric tasks with varying degrees of success. For example, in the case of
face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a
False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)
dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1
percent FMR on the IITD-R-Full dataset was 97.55 percent without any
fine-tuning. Further, we show that applying a simple classifier head to these
embeddings can help perform DeepFake detection for faces, Presentation Attack
Detection (PAD) for irides, and extract soft biometric attributes like gender
and ethnicity from faces with reasonably high accuracy. This work reiterates
the potential of pretrained models in achieving the long-term vision of
Artificial General Intelligence.

</details>


### [289] [Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation](https://arxiv.org/pdf/2505.24216)
*Prasanna Reddy Pulakurthi, Majid Rabbani, Jamison Heard, Sohail Dianat, Celso M. de Melo, Raghuveer Rao*

Main category: cs.CV

TL;DR: A new Source-Free Domain Adaptation (SFDA) method using Shuffle PatchMix (SPM) and pseudo-label reweighting achieves state-of-the-art results on benchmarks like PACS, VisDA-C, and DomainNet-126.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting models to target domains without access to source data, particularly in smaller datasets prone to overfitting and pseudo-label noise.

Method: Introduces SPM for diverse augmentations and a reweighting strategy to prioritize reliable pseudo-labels, reducing noise.

Result: Significant improvements: 7.3% on PACS (single-target), 7.2% (multi-target), 2.8% on DomainNet-126, and 0.7% on VisDA-C.

Conclusion: The combination of SPM and pseudo-label reweighting sets a new benchmark for SFDA, with code publicly available.

Abstract: This work investigates Source-Free Domain Adaptation (SFDA), where a model
adapts to a target domain without access to source data. A new augmentation
technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are
introduced to enhance performance. SPM shuffles and blends image patches to
generate diverse and challenging augmentations, while the reweighting strategy
prioritizes reliable pseudo-labels to mitigate label noise. These techniques
are particularly effective on smaller datasets like PACS, where overfitting and
pseudo-label noise pose greater risks. State-of-the-art results are achieved on
three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,
improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target
and multi-target settings, respectively, while gains of 2.8% and 0.7% are
attained on DomainNet-126 and VisDA-C. This combination of advanced
augmentation and robust pseudo-label reweighting establishes a new benchmark
for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM

</details>


### [290] [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](https://arxiv.org/pdf/2505.24222)
*Fangyikang Wang, Hubery Yin, Lei Qian, Yinan Li, Shaobin Zhuang, Huminhao Zhu, Yilin Zhang, Yanlong Tang, Chao Zhang, Hanbin Zhao, Hui Qian, Chen Li*

Main category: cs.CV

TL;DR: The paper introduces a Levenberg-Marquardt-Langevin (LML) method to enhance diffusion model sampling by approximating Hessian geometry efficiently, improving image generation quality without significant computational cost.


<details>
  <summary>Details</summary>
Motivation: Current diffusion model sampling relies on first-order Langevin dynamics, but leveraging second-order Hessian geometry is computationally expensive. The goal is to enhance sampling quality without scalability issues.

Method: The LML method approximates diffusion Hessian geometry using low-rank approximation and a damping mechanism, avoiding quadratic-complexity costs.

Result: Experiments show LML significantly improves image generation quality with negligible computational overhead.

Conclusion: The LML method effectively enhances diffusion sampling by approximating Hessian geometry, offering a scalable solution for better image generation.

Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of
generating images via learning the noised score function of data distribution.
Current DM sampling techniques typically rely on first-order Langevin dynamics
at each noise level, with efforts concentrated on refining inter-level
denoising strategies. While leveraging additional second-order Hessian geometry
to enhance the sampling quality of Langevin is a common practice in Markov
chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in
high-dimensional DMs lead to quadratic-complexity computational costs,
rendering them non-scalable. In this work, we introduce a novel
Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion
Hessian geometry in a training-free manner, drawing inspiration from the
celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces
two key innovations: (1) A low-rank approximation of the diffusion Hessian,
leveraging the DMs' inherent structure and circumventing explicit
quadratic-complexity computations; (2) A damping mechanism to stabilize the
approximated Hessian. This LML approximated Hessian geometry enables the
diffusion sampling to execute more accurate steps and improve the image
generation quality. We further conduct a theoretical analysis to substantiate
the approximation error bound of low-rank approximation and the convergence
property of the damping mechanism. Extensive experiments across multiple
pretrained DMs validate that the LML method significantly improves image
generation quality, with negligible computational overhead.

</details>


### [291] [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/pdf/2505.24225)
*Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang*

Main category: cs.CV

TL;DR: CoT prompting in LLMs can degrade inductive reasoning; structured interventions improve accuracy by addressing failure modes.


<details>
  <summary>Details</summary>
Motivation: To investigate if CoT prompting enhances inductive reasoning in LLMs and identify its limitations.

Method: Controlled diagnostic tasks (chess, poker, dice, blackjack) with hidden rules; theoretical framework for error amplification.

Result: CoT reasoning often underperforms non-reasoning models; structured interventions improve accuracy.

Conclusion: Effective CoT reasoning requires well-structured steps, not just more steps.

Abstract: Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
sparse examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.

</details>


### [292] [Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models](https://arxiv.org/pdf/2505.24227)
*Ying Yang, Jie Zhang, Xiao Lv, Di Lin, Tao Xiang, Qing Guo*

Main category: cs.CV

TL;DR: LightD is a framework for generating natural adversarial samples for VLP models using semantically guided relighting, ensuring realistic perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attack methods for VLP models produce unnatural artifacts or ineffective attacks due to limited optimization spaces.

Method: LightD uses ChatGPT for context-aware lighting parameters and a pretrained relighting model (IC-light) for adjustments, combined with gradient-based optimization.

Result: LightD outperforms existing methods in attacking VLP models for tasks like image captioning and visual question answering.

Conclusion: LightD effectively generates natural adversarial samples, advancing adversarial research for VLP models.

Abstract: While adversarial attacks on vision-and-language pretraining (VLP) models
have been explored, generating natural adversarial samples crafted through
realistic and semantically meaningful perturbations remains an open challenge.
Existing methods, primarily designed for classification tasks, struggle when
adapted to VLP models due to their restricted optimization spaces, leading to
ineffective attacks or unnatural artifacts. To address this, we propose
\textbf{LightD}, a novel framework that generates natural adversarial samples
for VLP models via semantically guided relighting. Specifically, LightD
leverages ChatGPT to propose context-aware initial lighting parameters and
integrates a pretrained relighting model (IC-light) to enable diverse lighting
adjustments. LightD expands the optimization space while ensuring perturbations
align with scene semantics. Additionally, gradient-based optimization is
applied to the reference lighting image to further enhance attack effectiveness
while maintaining visual naturalness. The effectiveness and superiority of the
proposed LightD have been demonstrated across various VLP models in tasks such
as image captioning and visual question answering.

</details>


### [293] [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/pdf/2505.24232)
*Haibo Jin, Peiyan Zhang, Peiran Wang, Man Luo, Haohan Wang*

Main category: cs.CV

TL;DR: The paper proposes a unified framework linking hallucinations and jailbreak attacks in large foundation models, showing shared optimization behaviors and suggesting joint mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between hallucinations and jailbreak attacks in large foundation models, as defenses for one often impact the other.

Method: A theoretical framework modeling jailbreaks as token-level optimization and hallucinations as attention-level optimization, validated empirically on LLaVA-1.5 and MiniGPT-4.

Result: Empirical validation shows similar loss convergence and gradient consistency, with mitigation techniques for one vulnerability reducing the other.

Conclusion: The findings reveal a shared failure mode in LFMs, advocating for joint robustness strategies to address both vulnerabilities.

Abstract: Large foundation models (LFMs) are susceptible to two distinct
vulnerabilities: hallucinations and jailbreak attacks. While typically studied
in isolation, we observe that defenses targeting one often affect the other,
hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as
token-level optimization and hallucinations as attention-level optimization.
Within this framework, we establish two key propositions: (1) \textit{Similar
Loss Convergence} - the loss functions for both vulnerabilities converge
similarly when optimizing for target-specific outputs; and (2) \textit{Gradient
Consistency in Attention Redistribution} - both exhibit consistent gradient
behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,
showing consistent optimization trends and aligned gradients. Leveraging this
connection, we demonstrate that mitigation techniques for hallucinations can
reduce jailbreak success rates, and vice versa. Our findings reveal a shared
failure mode in LFMs and suggest that robustness strategies should jointly
address both vulnerabilities.

</details>


### [294] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/pdf/2505.24238)
*Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces a benchmark ({\dataset}) to isolate and evaluate reasoning-induced hallucinations in MLLMs, proposes multi-granular metrics, and presents a method ({\method}) to mitigate such hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to distinguish perception-induced from reasoning-induced hallucinations in MLLMs, hindering diagnosis of reasoning failures.

Method: Proposes {\dataset} benchmark to isolate reasoning hallucinations and introduces {\method}, combining curriculum reinforcement fine-tuning and collaborative hint inference.

Result: Findings show model scale, data scale, and training stages affect hallucinations; current MLLMs struggle with spatial reasoning; question types correlate with hallucination patterns.

Conclusion: {\method} reduces logical hallucinations, establishing a baseline for addressing reasoning-induced hallucinations in MLLMs.

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [295] [LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework](https://arxiv.org/pdf/2505.24245)
*Xin Kang, Zihan Zheng, Lei Chu, Yue Gao, Jiahao Li, Hao Pan, Xuejin Chen, Yan Lu*

Main category: cs.CV

TL;DR: LTM3D integrates diffusion and auto-regressive models for 3D shape generation, improving token dependency learning and structural fidelity.


<details>
  <summary>Details</summary>
Motivation: Combining diffusion and AR models for 3D shape generation is challenging but promising for better performance.

Method: LTM3D uses a Conditional Distribution Modeling backbone, Prefix Learning, and Latent Token Reconstruction with Reconstruction-Guided Sampling.

Result: LTM3D outperforms existing methods in prompt fidelity and structural accuracy across multiple 3D representations.

Conclusion: LTM3D offers a generalizable framework for multi-modal, multi-representation 3D generation.

Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D
shape generation that integrates the strengths of diffusion and auto-regressive
(AR) models. While diffusion-based methods effectively model continuous latent
spaces and AR models excel at capturing inter-token dependencies, combining
these paradigms for 3D shape generation remains a challenge. To address this,
LTM3D features a Conditional Distribution Modeling backbone, leveraging a
masked autoencoder and a diffusion model to enhance token dependency learning.
Additionally, we introduce Prefix Learning, which aligns condition tokens with
shape latent tokens during generation, improving flexibility across modalities.
We further propose a Latent Token Reconstruction module with
Reconstruction-Guided Sampling to reduce uncertainty and enhance structural
fidelity in generated shapes. Our approach operates in token space, enabling
support for multiple 3D representations, including signed distance fields,
point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on
image- and text-conditioned shape generation tasks demonstrate that LTM3D
outperforms existing methods in prompt fidelity and structural accuracy while
offering a generalizable framework for multi-modal, multi-representation 3D
generation.

</details>


### [296] [50 Years of Automated Face Recognition](https://arxiv.org/pdf/2505.24247)
*Minchul Kim, Anil Jain, Xiaoming Liu*

Main category: cs.CV

TL;DR: The paper reviews 50 years of face recognition (FR) evolution, from early methods to modern deep learning, highlighting key innovations, dataset impacts, and current challenges.


<details>
  <summary>Details</summary>
Motivation: To document the historical progression and technological advancements in FR, emphasizing the role of datasets and deep learning in achieving human-level performance.

Method: The study analyzes key innovations in FR, including dataset development, loss functions, neural network design, and feature fusion, while evaluating the impact of data scale and diversity on model generalization.

Result: Modern FR systems achieve high accuracy (e.g., 0.13% False Negative Rate in NIST FRVT evaluations), but challenges like scalability, multi-modal fusion, and explainability persist.

Conclusion: Despite significant progress, FR research faces ongoing challenges, with future directions including scalability, multi-modal fusion, synthetic data, and explainable systems.

Abstract: Over the past 50 years, automated face recognition has evolved from
rudimentary, handcrafted systems into sophisticated deep learning models that
rival and often surpass human performance. This paper chronicles the history
and technological progression of FR, from early geometric and statistical
methods to modern deep neural architectures leveraging massive real and
AI-generated datasets. We examine key innovations that have shaped the field,
including developments in dataset, loss function, neural network design and
feature fusion. We also analyze how the scale and diversity of training data
influence model generalization, drawing connections between dataset growth and
benchmark improvements. Recent advances have achieved remarkable milestones:
state-of-the-art face verification systems now report False Negative
Identification Rates of 0.13% against a 12.4 million gallery in NIST FRVT
evaluations for 1:N visa-to-border matching. While recent advances have enabled
remarkable accuracy in high- and low-quality face scenarios, numerous
challenges persist. While remarkable progress has been achieved, several open
research problems remain. We outline critical challenges and promising
directions for future face recognition research, including scalability,
multi-modal fusion, synthetic identity generation, and explainable systems.

</details>


### [297] [Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization](https://arxiv.org/pdf/2505.24249)
*Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Hongbin Liu*

Main category: cs.CV

TL;DR: PANSv2 is a generalizable and robust bronchoscopy localization framework that integrates depth estimation, landmark detection, and centerline constraints to improve accuracy and robustness in challenging visual conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 6-DOF bronchoscopy localization lack generalization across patient cases and robustness under visual degradation, such as occlusions and motion blur.

Method: PANSv2 combines depth estimation (using EndoOmni), landmark detection (using EndoMamba), and centerline constraints into a unified pose optimization framework. It also includes an automatic re-initialization module for tracking failures.

Result: PANSv2 achieves an 18.1% improvement in SR-5 (absolute trajectory error under 5 mm) compared to existing methods, demonstrating superior performance across 10 patient cases.

Conclusion: PANSv2 shows strong potential for clinical use by addressing generalization and robustness challenges in bronchoscopy localization.

Abstract: Vision-based 6-DOF bronchoscopy localization offers a promising solution for
accurate and cost-effective interventional guidance. However, existing methods
struggle with 1) limited generalization across patient cases due to scarce
labeled data, and 2) poor robustness under visual degradation, as bronchoscopy
procedures frequently involve artifacts such as occlusions and motion blur that
impair visual information. To address these challenges, we propose PANSv2, a
generalizable and robust bronchoscopy localization framework. Motivated by PANS
that leverages multiple visual cues for pose likelihood measurement, PANSv2
integrates depth estimation, landmark detection, and centerline constraints
into a unified pose optimization framework that evaluates pose probability and
solves for the optimal bronchoscope pose. To further enhance generalization
capabilities, we leverage the endoscopic foundation model EndoOmni for depth
estimation and the video foundation model EndoMamba for landmark detection,
incorporating both spatial and temporal analyses. Pretrained on diverse
endoscopic datasets, these models provide stable and transferable visual
representations, enabling reliable performance across varied bronchoscopy
scenarios. Additionally, to improve robustness to visual degradation, we
introduce an automatic re-initialization module that detects tracking failures
and re-establishes pose using landmark detections once clear views are
available. Experimental results on bronchoscopy dataset encompassing 10 patient
cases show that PANSv2 achieves the highest tracking success rate, with an
18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)
compared to existing methods, showing potential towards real clinical usage.

</details>


### [298] [Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames](https://arxiv.org/pdf/2505.24257)
*Sahithya Ravi, Gabriel Sarch, Vibhav Vineet, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel*

Main category: cs.CV

TL;DR: Disjoint-3DQA is a benchmark for evaluating VLMs' ability to reason about spatially disjoint objects in egocentric video, revealing significant gaps in performance and 3D scene representation.


<details>
  <summary>Details</summary>
Motivation: To assess and improve VLMs' capability for long-horizon spatial reasoning in embodied AI tasks, particularly when objects are not co-visible.

Method: Introduces Disjoint-3DQA, a generative QA benchmark testing VLMs on object pairs not seen together in frames. Evaluates seven VLMs with varying temporal gaps and auxiliary inputs.

Result: VLMs lag humans by 28%, with accuracy dropping as temporal gaps widen. Oracle 3D coordinates improve performance by 20%, while trajectories or bird's-eye views offer marginal gains.

Conclusion: Disjoint-3DQA identifies a bottleneck in VLMs' 3D scene representation and aims to drive research in vision, language, and embodied AI.

Abstract: An embodied AI assistant operating on egocentric video must integrate spatial
cues across time - for instance, determining where an object A, glimpsed a few
moments ago lies relative to an object B encountered later. We introduce
Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs
by posing questions about object pairs that are not co-visible in the same
frame. We evaluated seven state-of-the-art VLMs and found that models lag
behind human performance by 28%, with steeper declines in accuracy (60% to 30
%) as the temporal gap widens. Our analysis further reveals that providing
trajectories or bird's-eye-view projections to VLMs results in only marginal
improvements, whereas providing oracle 3D coordinates leads to a substantial
20% performance increase. This highlights a core bottleneck of multi-frame VLMs
in constructing and maintaining 3D scene representations over time from visual
signals. Disjoint-3DQA therefore sets a clear, measurable challenge for
long-horizon spatial reasoning and aims to catalyze future research at the
intersection of vision, language, and embodied AI.

</details>


### [299] [LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization](https://arxiv.org/pdf/2505.24282)
*Zirui Shang, Xinxiao Wu, Shuo Yang*

Main category: cs.CV

TL;DR: The paper proposes a method to improve language-driven action localization in videos by expanding queries with boundary details using LLMs and modeling boundary probabilities to reduce uncertainty.


<details>
  <summary>Details</summary>
Motivation: Language queries often lack details about action boundaries, leading to subjective annotations and training uncertainty. The paper aims to address this by providing more detailed boundary cues and enhancing training tolerance.

Method: The method involves expanding queries with boundary descriptions via LLMs and modeling boundary probabilities using semantic similarities and temporal distances.

Result: Experimental results show the method's effectiveness in improving action localization.

Conclusion: The proposed approach is model-agnostic, easy to integrate, and successfully reduces boundary uncertainty in training.

Abstract: Language-driven action localization in videos requires not only semantic
alignment between language query and video segment, but also prediction of
action boundaries.
  However, the language query primarily describes the main content of an action
and usually lacks specific details of action start and end boundaries, which
increases the subjectivity of manual boundary annotation and leads to boundary
uncertainty in training data.
  In this paper, on one hand, we propose to expand the original query by
generating textual descriptions of the action start and end boundaries through
LLMs, which can provide more detailed boundary cues for localization and thus
reduce the impact of boundary uncertainty.
  On the other hand, to enhance the tolerance to boundary uncertainty during
training, we propose to model probability scores of action boundaries by
calculating the semantic similarities between frames and the expanded query as
well as the temporal distances between frames and the annotated boundary
frames. They can provide more consistent boundary supervision, thus improving
the stability of training.
  Our method is model-agnostic and can be seamlessly and easily integrated into
any existing models of language-driven action localization in an off-the-shelf
manner. Experimental results on several datasets demonstrate the effectiveness
of our method.

</details>


### [300] [EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding](https://arxiv.org/pdf/2505.24287)
*Ege Özsoy, Arda Mamur, Felix Tristram, Chantal Pellegrini, Magdalena Wysocki, Benjamin Busam, Nassir Navab*

Main category: cs.CV

TL;DR: EgoExOR is the first dataset combining egocentric and exocentric views in operating rooms, providing multimodal data for enhanced clinical perception.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack comprehensive multi-perspective views, limiting OR perception models. EgoExOR fills this gap by integrating first-person and third-person data.

Method: EgoExOR includes 94 minutes of emulated spine procedures, combining egocentric (RGB, gaze, hand tracking, audio) and exocentric (RGB, depth, ultrasound) data with detailed scene graph annotations.

Result: The dataset supports tasks like action recognition and human-centric perception. Evaluations show improved surgical scene graph generation using multimodal signals.

Conclusion: EgoExOR sets a new foundation for OR perception, offering a rich, multimodal resource for advanced clinical applications.

Abstract: Operating rooms (ORs) demand precise coordination among surgeons, nurses, and
equipment in a fast-paced, occlusion-heavy environment, necessitating advanced
perception models to enhance safety and efficiency. Existing datasets either
provide partial egocentric views or sparse exocentric multi-view context, but
do not explore the comprehensive combination of both. We introduce EgoExOR, the
first OR dataset and accompanying benchmark to fuse first-person and
third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two
emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally
Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand
tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D
cameras, and ultrasound imagery. Its detailed scene graph annotations, covering
36 entities and 22 relations (568,235 triplets), enable robust modeling of
clinical interactions, supporting tasks like action recognition and
human-centric perception. We evaluate the surgical scene graph generation
performance of two adapted state-of-the-art models and offer a new baseline
that explicitly leverages EgoExOR's multimodal and multi-perspective signals.
This new dataset and benchmark set a new foundation for OR perception, offering
a rich, multimodal resource for next-generation clinical perception.

</details>


### [301] [Category-aware EEG image generation based on wavelet transform and contrast semantic loss](https://arxiv.org/pdf/2505.24301)
*Enshang Zhang, Zhicheng Zhang, Takashi Hanakawa*

Main category: cs.CV

TL;DR: A transformer-based EEG encoder with DWT and gating mechanism reconstructs visual stimuli from EEG signals, outperforming state-of-the-art methods in semantic alignment and classification.


<details>
  <summary>Details</summary>
Motivation: To advance brain-computer interfaces by improving EEG-to-visual stimulus reconstruction.

Method: Uses a transformer-based EEG encoder with DWT and gating, guided by feature alignment and category-aware fusion losses, paired with a pre-trained diffusion model for reconstruction.

Result: Achieves 43% single-subject accuracy, outperforming other methods in semantic alignment and classification.

Conclusion: The model effectively transfers neural activities into visual representations, validated by EEG-to-image tasks and semantic-based metrics.

Abstract: Reconstructing visual stimuli from EEG signals is a crucial step in realizing
brain-computer interfaces. In this paper, we propose a transformer-based EEG
signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating
mechanism. Guided by the feature alignment and category-aware fusion losses,
this encoder is used to extract features related to visual stimuli from EEG
signals. Subsequently, with the aid of a pre-trained diffusion model, these
features are reconstructed into visual stimuli. To verify the effectiveness of
the model, we conducted EEG-to-image generation and classification tasks using
the THINGS-EEG dataset. To address the limitations of quantitative analysis at
the semantic level, we combined WordNet-based classification and semantic
similarity metrics to propose a novel semantic-based score, emphasizing the
ability of our model to transfer neural activities into visual representations.
Experimental results show that our model significantly improves semantic
alignment and classification accuracy, which achieves a maximum single-subject
accuracy of 43\%, outperforming other state-of-the-art methods. The source code
and supplementary material is available at
https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main.

</details>


### [302] [Progressive Class-level Distillation](https://arxiv.org/pdf/2505.24310)
*Jiayan Li, Jun Li, Zhourui Zhang, Jianhua Xu*

Main category: cs.CV

TL;DR: Progressive Class-level Distillation (PCD) improves logit distillation by prioritizing class-level knowledge transfer in stages, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional logit distillation methods overlook low-probability classes, leading to insufficient knowledge transfer.

Method: PCD ranks logit differences, divides distillation into stages, and uses bidirectional stage-wise distillation for comprehensive alignment.

Result: PCD outperforms state-of-the-art methods in classification and detection tasks on public datasets.

Conclusion: PCD effectively addresses the limitations of conventional logit distillation by ensuring thorough knowledge transfer.

Abstract: In knowledge distillation (KD), logit distillation (LD) aims to transfer
class-level knowledge from a more powerful teacher network to a small student
model via accurate teacher-student alignment at the logits level. Since
high-confidence object classes usually dominate the distillation process,
low-probability classes which also contain discriminating information are
downplayed in conventional methods, leading to insufficient knowledge transfer.
To address this issue, we propose a simple yet effective LD method termed
Progressive Class-level Distillation (PCD). In contrast to existing methods
which perform all-class ensemble distillation, our PCD approach performs
stage-wise distillation for step-by-step knowledge transfer. More specifically,
we perform ranking on teacher-student logits difference for identifying
distillation priority from scratch, and subsequently divide the entire LD
process into multiple stages. Next, bidirectional stage-wise distillation
incorporating fine-to-coarse progressive learning and reverse coarse-to-fine
refinement is conducted, allowing comprehensive knowledge transfer via
sufficient logits alignment within separate class groups in different
distillation stages. Extension experiments on public benchmarking datasets
demonstrate the superiority of our method compared to state-of-the-arts for
both classification and detection tasks.

</details>


### [303] [InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing](https://arxiv.org/pdf/2505.24315)
*Jinlu Zhang, Yixin Chen, Zan Wang, Jie Yang, Yizhou Wang, Siyuan Huang*

Main category: cs.CV

TL;DR: A zero-shot 3D HOI generation framework leverages pre-trained models to address challenges in human-object interaction synthesis, achieving fine-grained and open-set object handling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generating novel HOI from text, especially for open-set objects, due to challenges in relation reasoning, affordance parsing, and pose synthesis.

Method: The framework uses LLMs for relation inference, a 2D diffusion model for object parsing, multi-view SDS for pose hypotheses, and detailed optimization for realistic 3D contact.

Result: The approach outperforms prior works in fine-grained interaction synthesis and handling open-set 3D objects.

Conclusion: The proposed zero-shot framework effectively addresses HOI generation challenges without dataset-specific training, validated by extensive experiments.

Abstract: Recent advances in 3D human-aware generation have made significant progress.
However, existing methods still struggle with generating novel Human Object
Interaction (HOI) from text, particularly for open-set objects. We identify
three main challenges of this task: precise human-object relation reasoning,
affordance parsing for any object, and detailed human interaction pose
synthesis aligning description and object geometry. In this work, we propose a
novel zero-shot 3D HOI generation framework without training on specific
datasets, leveraging the knowledge from large-scale pre-trained models.
Specifically, the human-object relations are inferred from large language
models (LLMs) to initialize object properties and guide the optimization
process. Then we utilize a pre-trained 2D image diffusion model to parse unseen
objects and extract contact points, avoiding the limitations imposed by
existing 3D asset knowledge. The initial human pose is generated by sampling
multiple hypotheses through multi-view SDS based on the input text and object
geometry. Finally, we introduce a detailed optimization to generate
fine-grained, precise, and natural interaction, enforcing realistic 3D contact
between the 3D object and the involved body parts, including hands in grasping.
This is achieved by distilling human-level feedback from LLMs to capture
detailed human-object relations from the text instruction. Extensive
experiments validate the effectiveness of our approach compared to prior works,
particularly in terms of the fine-grained nature of interactions and the
ability to handle open-set 3D objects.

</details>


### [304] [STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising](https://arxiv.org/pdf/2505.24327)
*Jingjing Liu, Jiashun Jin, Xianchao Xiu, Jianhua Zhang, Wanquan Liu*

Main category: cs.CV

TL;DR: Proposes STAR-Net and STAR-Net-S, deep unrolling networks for RSI denoising, combining low-rank priors and ADMM for robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current deep learning-based RSI denoising lacks interpretability and struggles with non-local self-similarity and parameter tuning.

Method: Uses low-rank priors for non-local self-similarity, extends to sparse variant (STAR-Net-S) for robustness, and employs ADMM-guided deep unrolling for automatic parameter learning.

Result: Outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: Combines model-based and deep learning advantages, addressing interpretability and robustness in RSI denoising.

Abstract: Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named sparse tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
sparse variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.

</details>


### [305] [DisTime: Distribution-based Time Representation for Video Large Language Models](https://arxiv.org/pdf/2505.24329)
*Yingsen Zeng, Zepeng Huang, Yujie Zhong, Chengjian Feng, Jie Hu, Lin Ma, Yang Liu*

Main category: cs.CV

TL;DR: DisTime enhances Video-LLMs' temporal localization with continuous embeddings and a distribution-based decoder, validated on a large dataset (InternVid-TG).


<details>
  <summary>Details</summary>
Motivation: Existing Video-LLMs struggle with precise temporal localization due to discrete time representations and limited datasets.

Method: DisTime uses learnable tokens for continuous temporal embeddings and a Distribution-based Time Decoder/Encoder. An automated annotation paradigm creates InternVid-TG dataset.

Result: DisTime achieves state-of-the-art performance in time-sensitive tasks and maintains Video QA competitiveness.

Conclusion: DisTime effectively addresses temporal localization challenges in Video-LLMs, supported by a scalable dataset.

Abstract: Despite advances in general video understanding, Video Large Language Models
(Video-LLMs) face challenges in precise temporal localization due to discrete
time representations and limited temporally aware datasets. Existing methods
for temporal expression either conflate time with text-based numerical values,
add a series of dedicated temporal tokens, or regress time using specialized
temporal grounding heads. To address these issues, we introduce DisTime, a
lightweight framework designed to enhance temporal comprehension in Video-LLMs.
DisTime employs a learnable token to create a continuous temporal embedding
space and incorporates a Distribution-based Time Decoder that generates
temporal probability distributions, effectively mitigating boundary ambiguities
and maintaining temporal continuity. Additionally, the Distribution-based Time
Encoder re-encodes timestamps to provide time markers for Video-LLMs. To
overcome temporal granularity limitations in existing datasets, we propose an
automated annotation paradigm that combines the captioning capabilities of
Video-LLMs with the localization expertise of dedicated temporal models. This
leads to the creation of InternVid-TG, a substantial dataset with 1.25M
temporally grounded events across 179k videos, surpassing ActivityNet-Caption
by 55 times. Extensive experiments demonstrate that DisTime achieves
state-of-the-art performance across benchmarks in three time-sensitive tasks
while maintaining competitive performance in Video QA tasks. Code and data are
released at https://github.com/josephzpng/DisTime.

</details>


### [306] [KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices](https://arxiv.org/pdf/2505.24334)
*Uzair Khan, Franco Fummi, Luigi Capogrosso*

Main category: cs.CV

TL;DR: KairosAD is a lightweight, efficient anomaly detection model for SMEs, leveraging MobileSAM for image-based detection, outperforming state-of-the-art models in efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of large, resource-heavy anomaly detection models for SMEs by developing a deployable solution for embedded devices.

Method: Uses MobileSAM for image-based anomaly detection, evaluated on MVTec-AD and ViSA datasets.

Result: 78% fewer parameters, 4x faster inference, comparable AUROC to leading models; successfully deployed on NVIDIA Jetson devices and tested in a real production line.

Conclusion: KairosAD is a practical, efficient solution for anomaly detection in resource-constrained environments, validated in real-world settings.

Abstract: In the era of intelligent manufacturing, anomaly detection has become
essential for maintaining quality control on modern production lines. However,
while many existing models show promising performance, they are often too
large, computationally demanding, and impractical to deploy on
resource-constrained embedded devices that can be easily installed on the
production lines of Small and Medium Enterprises (SMEs). To bridge this gap, we
present KairosAD, a novel supervised approach that uses the power of the Mobile
Segment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD
has been evaluated on the two well-known industrial anomaly detection datasets,
i.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer
parameters and boasts a 4x faster inference time compared to the leading
state-of-the-art model, while maintaining comparable AUROC performance. We
deployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA
Jetson AGX. Finally, KairosAD was successfully installed and tested on the real
production line of the Industrial Computer Engineering Laboratory (ICE Lab) at
the University of Verona. The code is available at
https://github.com/intelligolabs/KairosAD.

</details>


### [307] [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/pdf/2505.24340)
*Gilles Quentin Hacheme, Girmaw Abebe Tadesse, Caleb Robinson, Akram Zaytar, Rahul Dodhia, Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: GVL is a zero-shot geospatial imagery classification framework using vision and language models, achieving high accuracy without task-specific training.


<details>
  <summary>Details</summary>
Motivation: Addressing the bottleneck of scarce annotated data in geospatial imagery classification for applications like disaster response.

Method: GVL combines a vision Large Language Model (vLLM) for image descriptions and an LLM for mapping to user-defined classes, with hierarchical classification for complex tasks.

Result: Achieves 93.2% zero-shot accuracy on binary tasks and competitive performance on multi-class benchmarks.

Conclusion: GVL offers a flexible, interpretable, and open-source solution for zero-shot geospatial classification.

Abstract: Classifying geospatial imagery remains a major bottleneck for applications
such as disaster response and land-use monitoring-particularly in regions where
annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that
claim zero-shot classification capabilities for satellite imagery nonetheless
rely on task-specific pretraining and adaptation to reach competitive
performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot
classification framework: a vision Large Language Model (vLLM) generates rich,
human-readable image descriptions, which are then mapped to user-defined
classes by a conventional Large Language Model (LLM). This modular, and
interpretable pipeline enables flexible image classification for a large range
of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,
and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary
Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class
classification tasks (UC Merced, RESISC45), we implemented a recursive
LLM-driven clustering to form meta-classes at successive depths, followed by
hierarchical classification-first resolving coarse groups, then finer
distinctions-to deliver competitive zero-shot performance. GVL is open-sourced
at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in
real-world geospatial workflows.

</details>


### [308] [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/pdf/2505.24342)
*Fanhang Man, Xiaoyue Chen, Huandong Wang, Baining Zhao, Han Li, Xinlei Chen, Yong Li*

Main category: cs.CV

TL;DR: K-EVER² is a knowledge-enhanced framework for visual emotion analysis, outperforming baselines by up to 19% accuracy gain.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in visual emotion analysis, such as abstract cues and limited supervision in VLMs, to improve emotion prediction.

Method: Integrates structured affective knowledge and multimodal alignment without relying on handcrafted labels.

Result: Achieves up to 19% accuracy gain on benchmarks like Emotion6, EmoSet, and M-Disaster.

Conclusion: K-EVER² offers a scalable, generalizable solution for advancing emotional understanding in visual content.

Abstract: Understanding what emotions images evoke in their viewers is a foundational
goal in human-centric visual computing. While recent advances in
vision-language models (VLMs) have shown promise for visual emotion analysis
(VEA), several key challenges remain unresolved. Emotional cues in images are
often abstract, overlapping, and entangled, making them difficult to model and
interpret. Moreover, VLMs struggle to align these complex visual patterns with
emotional semantics due to limited supervision and sparse emotional grounding.
Finally, existing approaches lack structured affective knowledge to resolve
ambiguity and ensure consistent emotional reasoning across diverse visual
domains.
  To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}},
a knowledge-enhanced framework for emotion reasoning and retrieval. Our
approach introduces a semantically structured formulation of visual emotion
cues and integrates external affective knowledge through multimodal alignment.
Without relying on handcrafted labels or direct emotion supervision,
K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions
across heterogeneous image types.
  We validate our framework on three representative benchmarks, Emotion6,
EmoSet, and M-Disaster, covering social media imagery, human-centric scenes,
and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms
strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for
specific emotions and a \textbf{12.3\% average accuracy gain} across all
emotion categories. Our results demonstrate a scalable and generalizable
solution for advancing emotional understanding of visual content.

</details>


### [309] [VUDG: A Dataset for Video Understanding Domain Generalization](https://arxiv.org/pdf/2505.24346)
*Ziyi Wang, Zhi Gao, Boxuan Yu, Zirui Dai, Yuxiang Song, Qingyuan Lu, Jin Chen, Xinxiao Wu*

Main category: cs.CV

TL;DR: The paper introduces VUDG, a dataset for evaluating domain generalization in video understanding, highlighting performance drops in current models under domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding works neglect domain shifts, leaving domain generalization underexplored.

Method: Proposes VUDG dataset with 11 domains and three shift types, annotated via a multi-expert framework.

Result: Tests on 9 LVLMs show performance degradation under domain shifts, revealing robustness gaps.

Conclusion: VUDG is a valuable resource for advancing domain generalization in video understanding.

Abstract: Video understanding has made remarkable progress in recent years, largely
driven by advances in deep models and the availability of large-scale annotated
datasets. However, existing works typically ignore the inherent domain shifts
encountered in real-world video applications, leaving domain generalization
(DG) in video understanding underexplored. Hence, we propose Video
Understanding Domain Generalization (VUDG), a novel dataset designed
specifically for evaluating the DG performance in video understanding. VUDG
contains videos from 11 distinct domains that cover three types of domain
shifts, and maintains semantic similarity across different domains to ensure
fair and meaningful evaluation. We propose a multi-expert progressive
annotation framework to annotate each video with both multiple-choice and
open-ended question-answer pairs. Extensive experiments on 9 representative
large video-language models (LVLMs) and several traditional video question
answering methods show that most models (including state-of-the-art LVLMs)
suffer performance degradation under domain shifts. These results highlight the
challenges posed by VUDG and the difference in the robustness of current models
to data distribution shifts. We believe VUDG provides a valuable resource for
prompting future research in domain generalization video understanding.

</details>


### [310] [Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation](https://arxiv.org/pdf/2505.24361)
*Roger Ferrod, Cássio F. Dantas, Luigi Di Caro, Dino Ienco*

Main category: cs.CV

TL;DR: CroDiNo-KD introduces a novel cross-modal knowledge distillation framework for RGBD semantic segmentation, addressing limitations of traditional teacher/student paradigms by using disentanglement representation, contrastive learning, and decoupled data augmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional CMKD frameworks face challenges in teacher architecture choices and distillation process selection, limiting real-world adoption. RGBD data enhances perception but may lack some modalities during inference due to sensor failures or constraints.

Method: CroDiNo-KD learns single-modality RGB and Depth models simultaneously using disentanglement representation, contrastive learning, and decoupled data augmentation to structure neural network manifolds through interaction and collaboration.

Result: Evaluated on three RGBD datasets, CroDiNo-KD outperforms recent CMKD frameworks, demonstrating its effectiveness.

Conclusion: The results suggest rethinking the conventional teacher/student paradigm for distilling multi-modal data into single-modality networks, highlighting CroDiNo-KD's potential.

Abstract: Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as
robotics, autonomous driving and remote sensing. The combination of these
multi-modal data enhances environmental perception by providing 3D spatial
context, which is absent in standard RGB images. Although RGBD multi-modal data
can be available to train computer vision models, accessing all sensor
modalities during the inference stage may be infeasible due to sensor failures
or resource constraints, leading to a mismatch between data modalities
available during training and inference. Traditional Cross-Modal Knowledge
Distillation (CMKD) frameworks, developed to address this task, are typically
based on a teacher/student paradigm, where a multi-modal teacher distills
knowledge into a single-modality student model. However, these approaches face
challenges in teacher architecture choices and distillation process selection,
thus limiting their adoption in real-world scenarios. To overcome these issues,
we introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on
Knowledge Distillation), a novel cross-modal knowledge distillation framework
for RGBD semantic segmentation. Our approach simultaneously learns
single-modality RGB and Depth models by exploiting disentanglement
representation, contrastive learning and decoupled data augmentation with the
aim to structure the internal manifolds of neural network models through
interaction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets
across diverse domains, considering recent CMKD frameworks as competitors. Our
findings illustrate the quality of CroDiNo-KD, and they suggest reconsidering
the conventional teacher/student paradigm to distill information from
multi-modal data to single-modality neural networks.

</details>


### [311] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/pdf/2505.24371)
*Md Intisar Chowdhury, Kittinun Aukkapinyo, Hiroshi Fujimura, Joo Ann Woo, Wasu Wasusatein, Fadoua Ghourabi*

Main category: cs.CV

TL;DR: Grid-LoGAT system for VideoQA uses a VLM for text extraction and an LLM for answer generation, ensuring privacy and improving accuracy with grid-based prompting.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in VideoQA by deploying models on edge and cloud, while improving transcript quality and accuracy.

Method: Two-phase system: VLM extracts text from video frames; LLM processes questions using transcripts. Grid-based visual prompting enhances local and global details.

Result: Outperforms state-of-the-art methods on NExT-QA (65.9%) and STAR-QA (50.11%), and surpasses non-grid version by 24 points on localization questions.

Conclusion: Grid-LoGAT effectively balances privacy and performance, demonstrating superior accuracy and localization capabilities in VideoQA.

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA.

</details>


### [312] [D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding](https://arxiv.org/pdf/2505.24372)
*Yichi Zhang, Gongwei Chen, Jun Zhu, Jia Wan*

Main category: cs.CV

TL;DR: D2AF is a robust annotation framework for visual grounding that generates region-text pairs without relying on human-labeled captions, improving dataset size and diversity. It uses multimodal models and object detection, with filtering to enhance data quality, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The high cost of manual annotation limits dataset scale and model performance in visual grounding. Existing pseudo label methods rely on human-labeled captions, restricting scalability and diversity.

Method: D2AF leverages multimodal large models and object detection models to generate region-text pairs via dual-driven annotation (closed-set and open-set). It includes a filtering method to remove noisy data.

Result: Increasing data volume improves performance, but the degree depends on how pseudo labels broaden the original data distribution. The method achieves state-of-the-art results on three visual grounding tasks.

Conclusion: D2AF addresses scalability and diversity challenges in visual grounding by generating high-quality pseudo labels without human captions, significantly boosting model performance.

Abstract: Visual Grounding is a task that aims to localize a target region in an image
based on a free-form natural language description. With the rise of Transformer
architectures, there is an increasing need for larger datasets to boost
performance. However, the high cost of manual annotation poses a challenge,
hindering the scale of data and the ability of large models to enhance their
effectiveness. Previous pseudo label generation methods heavily rely on
human-labeled captions of the original dataset, limiting scalability and
diversity. To address this, we propose D2AF, a robust annotation framework for
visual grounding using only input images. This approach overcomes dataset size
limitations and enriches both the quantity and diversity of referring
expressions. Our approach leverages multimodal large models and object
detection models. By implementing dual-driven annotation strategies, we
effectively generate detailed region-text pairs using both closed-set and
open-set approaches. We further conduct an in-depth analysis of data quantity
and data distribution. Our findings demonstrate that increasing data volume
enhances model performance. However, the degree of improvement depends on how
well the pseudo labels broaden the original data distribution. Based on these
insights, we propose a consistency and distribution aware filtering method to
further improve data quality by effectively removing erroneous and redundant
data. This approach effectively eliminates noisy data, leading to improved
performance. Experiments on three visual grounding tasks demonstrate that our
method significantly improves the performance of existing models and achieves
state-of-the-art results.

</details>


### [313] [Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification](https://arxiv.org/pdf/2505.24375)
*Maciej Wielgosz, Simon Berg, Heikki Korpunen, Stephan Hoffmann*

Main category: cs.CV

TL;DR: A deep learning framework using 3D ResNet-50 classifies forestry operations from dashcam videos, achieving strong performance (F1: 0.88, precision: 0.90) but facing overfitting issues.


<details>
  <summary>Details</summary>
Motivation: To automate and reduce manual workload in forestry time studies by classifying key operations like crane-out, cutting, driving, and processing.

Method: Uses a 3D ResNet-50 architecture with PyTorchVideo, trained on annotated field recordings, with preprocessing and augmentation for generalization.

Result: High validation scores (F1: 0.88, precision: 0.90) but overfitting indicates need for more data and class balance.

Conclusion: The framework shows promise for operational monitoring in forestry, with future plans for dataset expansion, regularization, and embedded system deployment.

Abstract: This paper presents a deep learning-based framework for classifying forestry
operations from dashcam video footage. Focusing on four key work elements -
crane-out, cutting-and-to-processing, driving, and processing - the approach
employs a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a
manually annotated dataset of field recordings, the model achieves strong
performance, with a validation F1 score of 0.88 and precision of 0.90. These
results underscore the effectiveness of spatiotemporal convolutional networks
for capturing both motion patterns and appearance in real-world forestry
environments.
  The system integrates standard preprocessing and augmentation techniques to
improve generalization, but overfitting is evident, highlighting the need for
more training data and better class balance. Despite these challenges, the
method demonstrates clear potential for reducing the manual workload associated
with traditional time studies, offering a scalable solution for operational
monitoring and efficiency analysis in forestry.
  This work contributes to the growing application of AI in natural resource
management and sets the foundation for future systems capable of real-time
activity recognition in forest machinery. Planned improvements include dataset
expansion, enhanced regularization, and deployment trials on embedded systems
for in-field use.

</details>


### [314] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/pdf/2505.24380)
*Zheng Wang*

Main category: cs.CV

TL;DR: A framework for fine-grained bird image classification (FBIC) using strip-aware spatial perception to address challenges like varying bird sizes, background interference, and pose variability. It introduces EPA and CSW modules for robust feature extraction.


<details>
  <summary>Details</summary>
Motivation: FBIC is crucial for ecological monitoring and species identification but faces challenges due to varying bird sizes, complex backgrounds, and pose variability, limiting traditional methods.

Method: Proposes a framework with extensional perception aggregator (EPA) and channel semantic weaving (CSW) modules to capture long-range spatial dependencies and refine semantic representations. Built on ResNet-50.

Result: Achieves significant performance improvements on the CUB-200-2011 dataset while maintaining efficiency.

Conclusion: The framework enhances robustness and interpretability in FBIC, addressing key challenges effectively.

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [315] [Leadership Assessment in Pediatric Intensive Care Unit Team Training](https://arxiv.org/pdf/2505.24389)
*Liangyang Ouyang, Yuki Sakai, Ryosuke Furuta, Hisataka Nozawa, Hikoro Matsui, Yoichi Sato*

Main category: cs.CV

TL;DR: An automated framework using egocentric vision assesses PICU team leadership by analyzing behavioral cues like fixation, eye contact, and conversation patterns.


<details>
  <summary>Details</summary>
Motivation: To improve PICU team training by objectively assessing leadership skills through automated analysis of behavioral indicators.

Method: Uses Aria Glasses for multimodal data collection (video, audio, gaze, head movement) and employs REMoDNaV, SAM, YOLO, and ChatGPT for automated processing.

Result: Significant correlations found between leadership skills and behavioral metrics (fixation time, transition patterns, direct orders).

Conclusion: The framework effectively assesses leadership skills, aiding PICU team training.

Abstract: This paper addresses the task of assessing PICU team's leadership skills by
developing an automated analysis framework based on egocentric vision. We
identify key behavioral cues, including fixation object, eye contact, and
conversation patterns, as essential indicators of leadership assessment. In
order to capture these multimodal signals, we employ Aria Glasses to record
egocentric video, audio, gaze, and head movement data. We collect one-hour
videos of four simulated sessions involving doctors with different roles and
levels. To automate data processing, we propose a method leveraging REMoDNaV,
SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection,
and conversation classification. In the experiments, significant correlations
are observed between leadership skills and behavioral metrics, i.e., the output
of our proposed methods, such as fixation time, transition patterns, and direct
orders in speech. These results indicate that our proposed data collection and
analysis framework can effectively solve skill assessment for training PICU
teams.

</details>


### [316] [S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification](https://arxiv.org/pdf/2505.24401)
*Xianheng Ma, Hongchen Tan, Xiuping Liu, Yi Zhang, Huasheng Wang, Jiang Liu, Ying Chen, Hantao Liu*

Main category: cs.CV

TL;DR: A novel model, S3CE-Net, is proposed for long-sequence event-based person Re-ID, leveraging spiking neural networks for efficiency and robustness in harsh lighting conditions.


<details>
  <summary>Details</summary>
Motivation: To address challenges like harsh lighting and background interference in person Re-ID using event cameras, while maintaining high time resolution and facial information protection.

Method: S3CE-Net uses spiking neural networks with a Spike-guided Spatial-temporal Attention Mechanism (SSAM) and Spatiotemporal Feature Sampling Strategy (STFS) for semantic interaction and robust feature extraction.

Result: S3CE-Net achieves outstanding performance on mainstream datasets with low parameters and high efficiency.

Conclusion: The proposed S3CE-Net is effective for long-sequence event-based person Re-ID, validated by extensive experiments.

Abstract: In this paper, we leverage the advantages of event cameras to resist harsh
lighting conditions, reduce background interference, achieve high time
resolution, and protect facial information to study the long-sequence
event-based person re-identification (Re-ID) task. To this end, we propose a
simple and efficient long-sequence event Re-ID model, namely the Spike-guided
Spatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better
handle asynchronous event data, we build S3CE-Net based on spiking neural
networks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal
Attention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy
(STFS). The SSAM is designed to carry out semantic interaction and association
in both spatial and temporal dimensions, leveraging the capabilities of SNNs.
The STFS involves sampling spatial feature subsequences and temporal feature
subsequences from the spatiotemporal dimensions, driving the Re-ID model to
perceive broader and more robust effective semantics. Notably, the STFS
introduces no additional parameters and is only utilized during the training
stage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for
long-sequence event-based person Re-ID. Extensive experiments have verified
that our S3CE-Net achieves outstanding performance on many mainstream
long-sequence event-based person Re-ID datasets. Code is available
at:https://github.com/Mhsunshine/SC3E_Net.

</details>


### [317] [Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing](https://arxiv.org/pdf/2505.24402)
*Mika Feng, Koichi Ito, Takafumi Aoki, Tetsushi Ohki, Masakatsu Nishigaki*

Main category: cs.CV

TL;DR: A Vision Transformer (ViT)-based method is proposed to detect spoofing attacks in face recognition by leveraging intermediate features and introducing two data augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: To prevent unauthorized access via spoofing attacks using face photos, robust detection methods are needed.

Method: Uses ViT's intermediate features for balanced local-global feature extraction, along with two data augmentation methods (face anti-spoofing and patch-wise).

Result: Tested on OULU-NPU and SiW datasets, showing effectiveness in detecting spoofing attacks.

Conclusion: The proposed ViT-based method with data augmentation improves spoofing attack detection accuracy.

Abstract: Face recognition systems are designed to be robust against changes in head
pose, illumination, and blurring during image capture. If a malicious person
presents a face photo of the registered user, they may bypass the
authentication process illegally. Such spoofing attacks need to be detected
before face recognition. In this paper, we propose a spoofing attack detection
method based on Vision Transformer (ViT) to detect minute differences between
live and spoofed face images. The proposed method utilizes the intermediate
features of ViT, which have a good balance between local and global features
that are important for spoofing attack detection, for calculating loss in
training and score in inference. The proposed method also introduces two data
augmentation methods: face anti-spoofing data augmentation and patch-wise data
augmentation, to improve the accuracy of spoofing attack detection. We
demonstrate the effectiveness of the proposed method through experiments using
the OULU-NPU and SiW datasets.

</details>


### [318] [PCIE_Interaction Solution for Ego4D Social Interaction Challenge](https://arxiv.org/pdf/2505.24404)
*Kanokphan Lertniphonphan, Feng Chen, Junda Xu, Fengbu Lan, Jun Xie, Tao Zhang, Zhepeng Wang*

Main category: cs.CV

TL;DR: The paper introduces PCIE_Interaction for the Ego4D Social Interaction Challenge, tackling LAM and TTM tasks with face quality enhancement, ensemble methods, and audio-visual fusion, achieving 0.81 and 0.71 mAP.


<details>
  <summary>Details</summary>
Motivation: To accurately detect social interactions between subjects and the camera wearer, addressing the challenges of LAM (face crop sequences) and TTM (face crops with audio).

Method: For LAM: face quality enhancement and ensemble methods. For TTM: fusion of audio and visual cues weighted by visual quality.

Result: Achieved 0.81 mAP on LAM and 0.71 mAP on TTM.

Conclusion: PCIE_Interaction effectively addresses the challenge tasks, demonstrating strong performance in social interaction detection.

Abstract: This report presents our team's PCIE_Interaction solution for the Ego4D
Social Interaction Challenge at CVPR 2025, addressing both Looking At Me (LAM)
and Talking To Me (TTM) tasks. The challenge requires accurate detection of
social interactions between subjects and the camera wearer, with LAM relying
exclusively on face crop sequences and TTM combining speaker face crops with
synchronized audio segments. In the LAM track, we employ face quality
enhancement and ensemble methods. For the TTM task, we extend visual
interaction analysis by fusing audio and visual cues, weighted by a visual
quality score. Our approach achieved 0.81 and 0.71 mean average precision (mAP)
on the LAM and TTM challenges leader board. Code is available at
https://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction

</details>


### [319] [IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models](https://arxiv.org/pdf/2505.24406)
*Hanting Wang, Tao Jin, Wang Lin, Shulei Wang, Hai Huang, Shengpeng Ji, Zhou Zhao*

Main category: cs.CV

TL;DR: The paper proposes IRBridge, a framework to integrate pretrained generative models into image restoration bridges, addressing the mismatch between standard generative models and restoration tasks.


<details>
  <summary>Details</summary>
Motivation: Existing bridge models in image restoration require training from scratch for each degradation type, leading to high costs and limited performance. This work aims to leverage pretrained generative priors more efficiently.

Method: The authors introduce a transition equation to bridge two diffusion processes with the same endpoint distribution, enabling the use of generative models in restoration tasks. The IRBridge framework is proposed for this purpose.

Result: Experiments on six restoration tasks show IRBridge improves robustness and generalization by efficiently integrating generative priors.

Conclusion: IRBridge offers a flexible and adaptable approach to image restoration by leveraging pretrained generative models, eliminating the need for task-specific training.

Abstract: Bridge models in image restoration construct a diffusion process from
degraded to clear images. However, existing methods typically require training
a bridge model from scratch for each specific type of degradation, resulting in
high computational costs and limited performance. This work aims to efficiently
leverage pretrained generative priors within existing image restoration bridges
to eliminate this requirement. The main challenge is that standard generative
models are typically designed for a diffusion process that starts from pure
noise, while restoration tasks begin with a low-quality image, resulting in a
mismatch in the state distributions between the two processes. To address this
challenge, we propose a transition equation that bridges two diffusion
processes with the same endpoint distribution. Based on this, we introduce the
IRBridge framework, which enables the direct utilization of generative models
within image restoration bridges, offering a more flexible and adaptable
approach to image restoration. Extensive experiments on six image restoration
tasks demonstrate that IRBridge efficiently integrates generative priors,
resulting in improved robustness and generalization performance. Code will be
available at GitHub.

</details>


### [320] [PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge](https://arxiv.org/pdf/2505.24411)
*Feng Chen, Kanokphan Lertniphonphan, Qiancheng Yan, Xiaohui Fan, Jun Xie, Tao Zhang, Zhepeng Wang*

Main category: cs.CV

TL;DR: The paper presents PCIE_EgoPose's solutions for CVPR2025 challenges, achieving top results in hand and body pose estimation and proficiency estimation using transformer-based architectures.


<details>
  <summary>Details</summary>
Motivation: The challenges involve complex tasks like 3D hand pose estimation from egocentric videos and body pose estimation in dynamic contexts, requiring innovative solutions.

Method: Developed HP-ViT+ (Vision Transformer + CNN) for hand pose estimation and multimodal spatio-temporal feature integration for body pose estimation. Extended these methods to proficiency estimation.

Result: Achieved 8.31 PA-MPJPE (hand pose), 11.25 MPJPE (body pose), and 0.53 top-1 accuracy (proficiency estimation), winning championships.

Conclusion: The proposed methods set new benchmarks in pose and proficiency estimation, demonstrating the effectiveness of transformer-based architectures.

Abstract: This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D
Pose and Proficiency Estimation Challenges at CVPR2025. Focused on the
intricate task of estimating 21 3D hand joints from RGB egocentric videos,
which are complicated by subtle movements and frequent occlusions, we developed
the Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a
Vision Transformer and a CNN backbone, using weighted fusion to refine the hand
pose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal
spatio-temporal feature integration strategy to address the complexities of
body pose estimation across dynamic contexts. Our methods achieved remarkable
performance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the
Body Pose Challenge, securing championship titles in both competitions. We
extended our pose estimation solutions to the Proficiency Estimation task,
applying core technologies such as transformer-based architectures. This
extension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the
Demonstrator Proficiency Estimation competition.

</details>


### [321] [EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering](https://arxiv.org/pdf/2505.24417)
*Runnan Lu, Yuxuan Zhang, Jiaming Liu, Haofan Wang, Yiren Song*

Main category: cs.CV

TL;DR: EasyText, a multilingual text rendering framework using DiT, achieves precise and controllable text generation with novel encoding techniques and large datasets.


<details>
  <summary>Details</summary>
Motivation: Generating accurate multilingual text with diffusion models is challenging, and existing methods lack support for arbitrary languages.

Method: Uses DiT with character positioning encoding and position encoding interpolation, along with pretraining on 1M synthetic and fine-tuning on 20K annotated images.

Result: Demonstrates effectiveness in multilingual text rendering, visual quality, and layout-aware integration.

Conclusion: EasyText advances multilingual text rendering with improved precision and control.

Abstract: Generating accurate multilingual text with diffusion models has long been
desired but remains challenging. Recent methods have made progress in rendering
text in a single language, but rendering arbitrary languages is still an
unexplored area. This paper introduces EasyText, a text rendering framework
based on DiT (Diffusion Transformer), which connects denoising latents with
multilingual character tokens encoded as character tokens. We propose character
positioning encoding and position encoding interpolation techniques to achieve
controllable and precise text rendering. Additionally, we construct a
large-scale synthetic text image dataset with 1 million multilingual image-text
annotations as well as a high-quality dataset of 20K annotated images, which
are used for pretraining and fine-tuning respectively. Extensive experiments
and evaluations demonstrate the effectiveness and advancement of our approach
in multilingual text rendering, visual quality, and layout-aware text
integration.

</details>


### [322] [Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation](https://arxiv.org/pdf/2505.24431)
*Bozhong Zheng, Jinye Gan, Xiaohao Xu, Wenqiao Li, Xiaonan Huang, Na Ni, Yingna Wu*

Main category: cs.CV

TL;DR: PASDF introduces a pose-aware continuous 3D representation for anomaly detection and repair, outperforming existing methods with high AUROC scores.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of patch-based methods in handling pose variations and geometric anomalies for fine-grained localization.

Method: Uses a Pose Alignment Module for canonicalization and a SDF Network to learn pose-invariant shape representations, enabling anomaly repair.

Result: Achieves state-of-the-art AUROC scores of 80.2% and 90.0% on Real3D-AD and Anomaly-ShapeNet datasets.

Conclusion: Continuous geometric representations in PASDF advance 3D anomaly detection and enable practical anomaly repair.

Abstract: 3D point cloud anomaly detection is essential for robust vision systems but
is challenged by pose variations and complex geometric anomalies. Existing
patch-based methods often suffer from geometric fidelity issues due to discrete
voxelization or projection-based representations, limiting fine-grained anomaly
localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel
framework that integrates 3D anomaly detection and repair by learning a
continuous, pose-invariant shape representation. PASDF leverages a Pose
Alignment Module for canonicalization and a SDF Network to dynamically
incorporate pose, enabling implicit learning of high-fidelity anomaly repair
templates from the continuous SDF. This facilitates precise pixel-level anomaly
localization through an Anomaly-Aware Scoring Module. Crucially, the continuous
3D representation in PASDF extends beyond detection, facilitating in-situ
anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate
state-of-the-art performance, achieving high object-level AUROC scores of 80.2%
and 90.0%, respectively. These results highlight the effectiveness of
continuous geometric representations in advancing 3D anomaly detection and
facilitating practical anomaly region repair. The code is available at
https://github.com/ZZZBBBZZZ/PASDF to support further research.

</details>


### [323] [SORCE: Small Object Retrieval in Complex Environments](https://arxiv.org/pdf/2505.24441)
*Chunxu Liu, Chi Xie, Xiaxu Chen, Wei Li, Feng Zhu, Rui Zhao, Limin Wang*

Main category: cs.CV

TL;DR: The paper introduces SORCE-1K, a benchmark for small object retrieval in complex environments, and proposes a multi-embedding approach using MLLMs and Regional Prompts to improve retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Existing T2IR benchmarks overlook small objects in complex environments, which are crucial in real-world applications.

Method: Proposes a multi-embedding representation for images using MLLMs and Regional Prompts (ReP).

Result: The multi-embedding approach outperforms existing T2IR methods on SORCE-1K.

Conclusion: SORCE-1K is effective for benchmarking, and multi-embedding with MLLMs shows promise for small object retrieval.

Abstract: Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a
given textual query to images in a gallery. Existing benchmarks primarily focus
on textual queries describing overall image semantics or foreground salient
objects, possibly overlooking inconspicuous small objects, especially in
complex environments. Such small object retrieval is crucial, as in real-world
applications, the targets of interest are not always prominent in the image.
Thus, we introduce SORCE (Small Object Retrieval in Complex Environments), a
new subfield of T2IR, focusing on retrieving small objects in complex images
with textual queries. We propose a new benchmark, SORCE-1K, consisting of
images with complex environments and textual queries describing less
conspicuous small objects with minimal contextual cues from other salient
objects. Preliminary analysis on SORCE-1K finds that existing T2IR methods
struggle to capture small objects and encode all the semantics into a single
embedding, leading to poor retrieval performance on SORCE-1K. Therefore, we
propose to represent each image with multiple distinctive embeddings. We
leverage Multimodal Large Language Models (MLLMs) to extract multiple
embeddings for each image instructed by a set of Regional Prompts (ReP).
Experimental results show that our multi-embedding approach through MLLM and
ReP significantly outperforms existing T2IR methods on SORCE-1K. Our
experiments validate the effectiveness of SORCE-1K for benchmarking SORCE
performances, highlighting the potential of multi-embedding representation and
text-customized MLLM features for addressing this task.

</details>


### [324] [Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers](https://arxiv.org/pdf/2505.24443)
*Heejo Kong, Sung-Jin Kim, Gunho Jung, Seong-Whan Lee*

Main category: cs.CV

TL;DR: DAC enhances SSL robustness in open-set scenarios by leveraging prediction disagreements among multiple biased models, improving outlier detection with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional SSL assumes identical class distributions for labeled and unlabeled data, but outliers degrade performance. Existing methods fail with insufficient labeled data.

Method: DAC trains multiple models with divergent biases on unlabeled data, using their prediction disagreements to detect outliers.

Result: DAC outperforms naive SSL and existing open-set SSL methods, especially with limited labeled data.

Conclusion: DAC provides a robust solution for open-set SSL by exploiting model disagreements, improving outlier detection without extensive labeled data.

Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and
unlabeled data share an identical class distribution, however in practice, this
assumption is easily violated, as unlabeled data often includes unknown class
data, i.e., outliers. The outliers are treated as noise, considerably degrading
the performance of SSL models. To address this drawback, we propose a novel
framework, Diversify and Conquer (DAC), to enhance SSL robustness in the
context of open-set semi-supervised learning. In particular, we note that
existing open-set SSL methods rely on prediction discrepancies between inliers
and outliers from a single model trained on labeled data. This approach can be
easily failed when the labeled data is insufficient, leading to performance
degradation that is worse than naive SSL that do not account for outliers. In
contrast, our approach exploits prediction disagreements among multiple models
that are differently biased towards the unlabeled distribution. By leveraging
the discrepancies arising from training on unlabeled data, our method enables
robust outlier detection even when the labeled data is underspecified. Our key
contribution is constructing a collection of differently biased models through
a single training process. By encouraging divergent heads to be differently
biased towards outliers while making consistent predictions for inliers, we
exploit the disagreement among these heads as a measure to identify unknown
concepts. Our code is available at https://github.com/heejokong/DivCon.

</details>


### [325] [SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking](https://arxiv.org/pdf/2505.24466)
*Yingjia Xu, Jinlin Wu, Zhen Chen, Daming Gao, Yang Yang, Zhen Lei, Min Cao*

Main category: cs.CV

TL;DR: The paper introduces SCENEPERSON-13W, a large-scale dataset for text-based person retrieval, and proposes SA-Person, a two-stage framework combining appearance grounding and scene-aware re-ranking.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on appearance-based retrieval, missing contextual scene information, which can enhance accuracy.

Method: SA-Person uses a two-stage approach: appearance grounding aligns text with pedestrian regions, and SceneRanker re-ranks results using scene context via multimodal LLMs.

Result: Experiments on SCENEPERSON-13W show improved performance in scene-level retrieval.

Conclusion: The framework effectively integrates appearance and scene context, validated by the dataset, with plans for public release.

Abstract: Text-based person retrieval aims to identify a target individual from a
gallery of images based on a natural language description. It presents a
significant challenge due to the complexity of real-world scenes and the
ambiguity of appearance-related descriptions. Existing methods primarily
emphasize appearance-based cross-modal retrieval, often neglecting the
contextual information embedded within the scene, which can offer valuable
complementary insights for retrieval. To address this, we introduce
SCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich
annotations covering both pedestrian appearance and environmental cues. Based
on this, we propose SA-Person, a two-stage retrieval framework. In the first
stage, it performs discriminative appearance grounding by aligning textual cues
with pedestrian-specific regions. In the second stage, it introduces
SceneRanker, a training-free, scene-aware re-ranking method leveraging
multimodal large language models to jointly reason over pedestrian appearance
and the global scene context. Experiments on SCENEPERSON-13W validate the
effectiveness of our framework in challenging scene-level retrieval scenarios.
The code and dataset will be made publicly available.

</details>


### [326] [SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds](https://arxiv.org/pdf/2505.24475)
*Cheng Zeng, Xiatian Qi, Chi Chen, Kai Sun, Wangle Zhang, Yuxuan Liu, Yan Meng, Bisheng Yang*

Main category: cs.CV

TL;DR: The paper introduces a two-stage superpoint generation method for Transformers in point cloud roof plane segmentation, combining handcrafted features and a novel decoder to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Transformers are rarely used in point cloud roof plane segmentation, and existing superpoint Transformers underperform due to low-quality superpoints.

Method: Proposes a two-stage superpoint generation process, integrates handcrafted features, and designs a decoder combining Kolmogorov-Arnold Network with Transformer.

Result: Achieves state-of-the-art performance on annotated datasets and reduces annotation burden by being insensitive to boundary annotations.

Conclusion: Highlights the impact of point cloud quality on segmentation and suggests data augmentation for robustness.

Abstract: Transformers have been seldom employed in point cloud roof plane instance
segmentation, which is the focus of this study, and existing superpoint
Transformers suffer from limited performance due to the use of low-quality
superpoints. To address this challenge, we establish two criteria that
high-quality superpoints for Transformers should satisfy and introduce a
corresponding two-stage superpoint generation process. The superpoints
generated by our method not only have accurate boundaries, but also exhibit
consistent geometric sizes and shapes, both of which greatly benefit the
feature learning of superpoint Transformers. To compensate for the limitations
of deep learning features when the training set size is limited, we incorporate
multidimensional handcrafted features into the model. Additionally, we design a
decoder that combines a Kolmogorov-Arnold Network with a Transformer module to
improve instance prediction and mask extraction. Finally, our network's
predictions are refined using traditional algorithm-based postprocessing. For
evaluation, we annotated a real-world dataset and corrected annotation errors
in the existing RoofN3D dataset. Experimental results show that our method
achieves state-of-the-art performance on our dataset, as well as both the
original and reannotated RoofN3D datasets. Moreover, our model is not sensitive
to plane boundary annotations during training, significantly reducing the
annotation burden. Through comprehensive experiments, we also identified key
factors influencing roof plane segmentation performance: in addition to roof
types, variations in point cloud density, density uniformity, and 3D point
precision have a considerable impact. These findings underscore the importance
of incorporating data augmentation strategies that account for point cloud
quality to enhance model robustness under diverse and challenging conditions.

</details>


### [327] [Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model](https://arxiv.org/pdf/2505.24476)
*Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu*

Main category: cs.CV

TL;DR: Period-LLM is a multimodal large language model designed to improve performance on periodic tasks by addressing temporal modeling and period conflict issues, using an 'Easy to Hard Generalization' paradigm and a 'Resisting Logical Oblivion' strategy.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with periodic tasks due to poor temporal modeling and conflicts between short and long periods, limiting their ability to understand complex periodic phenomena.

Method: Introduces Period-LLM with a benchmark for cross-modal periodic tasks, using an 'Easy to Hard Generalization' approach and a 'Resisting Logical Oblivion' optimization strategy.

Result: Period-LLM outperforms existing MLLMs in periodic tasks, demonstrating superior performance across modalities.

Conclusion: Period-LLM effectively enhances periodic reasoning in MLLMs, with potential applications in diverse fields like weather, traffic, and biology.

Abstract: Periodic or quasi-periodic phenomena reveal intrinsic characteristics in
various natural processes, such as weather patterns, movement behaviors,
traffic flows, and biological signals. Given that these phenomena span multiple
modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer
promising potential to effectively capture and understand their complex nature.
However, current MLLMs struggle with periodic tasks due to limitations in: 1)
lack of temporal modelling and 2) conflict between short and long periods. This
paper introduces Period-LLM, a multimodal large language model designed to
enhance the performance of periodic tasks across various modalities, and
constructs a benchmark of various difficulty for evaluating the cross-modal
periodic capabilities of large models. Specially, We adopt an "Easy to Hard
Generalization" paradigm, starting with relatively simple text-based tasks and
progressing to more complex visual and multimodal tasks, ensuring that the
model gradually builds robust periodic reasoning capabilities. Additionally, we
propose a "Resisting Logical Oblivion" optimization strategy to maintain
periodic reasoning abilities during semantic alignment. Extensive experiments
demonstrate the superiority of the proposed Period-LLM over existing MLLMs in
periodic tasks. The code is available at
https://github.com/keke-nice/Period-LLM.

</details>


### [328] [ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation](https://arxiv.org/pdf/2505.24481)
*Jing Huang, Yongkang Zhao, Yuhan Li, Zhitao Dai, Cheng Chen, Qiying Lai*

Main category: cs.CV

TL;DR: ACM-UNet is a segmentation framework combining CNNs and Mamba models via adapters, achieving state-of-the-art results on medical datasets.


<details>
  <summary>Details</summary>
Motivation: To address structural mismatches in using pretrained vision backbones for medical image segmentation.

Method: Uses a UNet-like design with adapters for CNNs and Mamba models, plus a wavelet transform module for feature fusion.

Result: Achieves 85.12% Dice Score and 13.89mm HD95 on Synapse dataset with 17.93G FLOPs.

Conclusion: ACM-UNet effectively combines CNNs and SSMs, offering high performance and efficiency.

Abstract: The U-shaped encoder-decoder architecture with skip connections has become a
prevailing paradigm in medical image segmentation due to its simplicity and
effectiveness. While many recent works aim to improve this framework by
designing more powerful encoders and decoders, employing advanced convolutional
neural networks (CNNs) for local feature extraction, Transformers or state
space models (SSMs) such as Mamba for global context modeling, or hybrid
combinations of both, these methods often struggle to fully utilize pretrained
vision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To
bridge this gap, we introduce ACM-UNet, a general-purpose segmentation
framework that retains a simple UNet-like design while effectively
incorporating pretrained CNNs and Mamba models through a lightweight adapter
mechanism. This adapter resolves architectural incompatibilities and enables
the model to harness the complementary strengths of CNNs and SSMs-namely,
fine-grained local detail extraction and long-range dependency modeling.
Additionally, we propose a hierarchical multi-scale wavelet transform module in
the decoder to enhance feature fusion and reconstruction fidelity. Extensive
experiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet
achieves state-of-the-art performance while remaining computationally
efficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the
Synapse dataset with 17.93G FLOPs, showcasing its effectiveness and
scalability. Code is available at: https://github.com/zyklcode/ACM-UNet.

</details>


### [329] [Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing](https://arxiv.org/pdf/2505.24489)
*Anasse Boutayeb, Iyad Lahsen-cherif, Ahmed El Khadimi*

Main category: cs.CV

TL;DR: The paper applies the Deformable-DETR model to remote sensing images (optical and SAR), achieving high F1 scores (95.12% and 94.54%) and outperforming CNN and transformer-based models.


<details>
  <summary>Details</summary>
Motivation: Object detection is crucial in remote sensing, and DL models like Transformers are effective for such tasks. The work aims to leverage Deformable-DETR for improved performance.

Method: The Deformable-DETR model with deformable attention mechanisms is tested on optical (Pleiades Aircraft) and SAR (SSDD) datasets using 10-fold stratified validation.

Result: High F1 scores of 95.12% (optical) and 94.54% (SAR) were achieved, outperforming other models.

Conclusion: Deformable-DETR is highly effective for object detection in remote sensing, surpassing existing CNN and transformer-based approaches.

Abstract: Object detection has recently seen an interesting trend in terms of the most
innovative research work, this task being of particular importance in the field
of remote sensing, given the consistency of these images in terms of
geographical coverage and the objects present. Furthermore, Deep Learning (DL)
models, in particular those based on Transformers, are especially relevant for
visual computing tasks in general, and target detection in particular. Thus,
the present work proposes an application of Deformable-DETR model, a specific
architecture using deformable attention mechanisms, on remote sensing images in
two different modes, especially optical and Synthetic Aperture Radar (SAR). To
achieve this objective, two datasets are used, one optical, which is Pleiades
Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset
(SSDD). The results of a 10-fold stratified validation showed that the proposed
model performed particularly well, obtaining an F1 score of 95.12% for the
optical dataset and 94.54% for SSDD, while comparing these results with several
models detections, especially those based on CNNs and transformers, as well as
those specifically designed to detect different object classes in remote
sensing images.

</details>


### [330] [Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation](https://arxiv.org/pdf/2505.24499)
*Ximing Xing, Yandong Guan, Jing Zhang, Dong Xu, Qian Yu*

Main category: cs.CV

TL;DR: Reason-SVG enhances LLM reasoning for SVG generation via a two-stage training strategy (SFT and RL) and a Hybrid Reward function, improving accuracy and visual quality.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with SVG generation due to lack of reasoning for structural validity, semantic faithfulness, and visual coherence.

Method: Introduces the "Drawing-with-Thought" (DwT) paradigm, combining SFT and RL with Group Relative Policy Optimization (GRPO) and a Hybrid Reward function.

Result: Reason-SVG significantly improves LLM performance in generating accurate and visually compelling SVGs.

Conclusion: The framework fosters better SVG generation by mimicking human creative processes, potentially enabling "Aha moments" in design.

Abstract: Generating high-quality Scalable Vector Graphics (SVGs) is challenging for
Large Language Models (LLMs), as it requires advanced reasoning for structural
validity, semantic faithfulness, and visual coherence -- capabilities in which
current LLMs often fall short. In this work, we introduce Reason-SVG, a novel
framework designed to enhance LLM reasoning for SVG generation. Reason-SVG
pioneers the "Drawing-with-Thought" (DwT) paradigm, in which models generate
both SVG code and explicit design rationales, mimicking the human creative
process. Reason-SVG adopts a two-stage training strategy: First, Supervised
Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational
reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group
Relative Policy Optimization (GRPO), empowers the model to generate both DwT
and SVGs rationales through refined, reward-driven reasoning. To facilitate
reasoning-driven SVG generation, we design a Hybrid Reward function that
evaluates the presence and utility of DwT reasoning, along with structural
validity, semantic alignment, and visual quality. We also introduce the
SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each
SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT,
and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance
in generating accurate and visually compelling SVGs, potentially fostering "Aha
moments" in design.

</details>


### [331] [un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP](https://arxiv.org/pdf/2505.24517)
*Yinqi Li, Jiahe Zhao, Hong Chang, Ruibing Hou, Shiguang Shan, Xilin Chen*

Main category: cs.CV

TL;DR: The paper proposes un$^2$CLIP, a method to enhance CLIP by inverting the unCLIP generative model, improving visual detail capture while maintaining text-image alignment.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with detailed image differences and dense-prediction tasks. The goal is to enhance CLIP's ability to capture visual details.

Method: Inverts unCLIP (a generative model) to improve CLIP, leveraging unCLIP's detail-capturing ability and alignment with CLIP's embedding space.

Result: un$^2$CLIP outperforms original CLIP and prior improvements on tasks like MMVP-VLM, open-vocabulary segmentation, and multimodal LLM tasks.

Conclusion: un$^2$CLIP effectively enhances CLIP's performance by integrating generative model strengths, offering a promising direction for vision-centric tasks.

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a foundation model
and has been applied to various vision and multimodal tasks. However, recent
works indicate that CLIP falls short in distinguishing detailed differences in
images and shows suboptimal performance on dense-prediction and vision-centric
multimodal tasks. Therefore, this work focuses on improving existing CLIP
models, aiming to capture as many visual details in images as possible. We find
that a specific type of generative models, unCLIP, provides a suitable
framework for achieving our goal. Specifically, unCLIP trains an image
generator conditioned on the CLIP image embedding. In other words, it inverts
the CLIP image encoder. Compared to discriminative models like CLIP, generative
models are better at capturing image details because they are trained to learn
the data distribution of images. Additionally, the conditional input space of
unCLIP aligns with CLIP's original image-text embedding space. Therefore, we
propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this
way, the improved image encoder can gain unCLIP's visual detail capturing
ability while preserving its alignment with the original text encoder
simultaneously. We evaluate our improved CLIP across various tasks to which
CLIP has been applied, including the challenging MMVP-VLM benchmark, the
dense-prediction open-vocabulary segmentation task, and multimodal large
language model tasks. Experiments show that un$^2$CLIP significantly improves
the original CLIP and previous CLIP improvement methods. Code and models will
be available at https://github.com/LiYinqi/un2CLIP.

</details>


### [332] [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/pdf/2505.24519)
*Yuqi Zhang, Yuchun Miao, Zuchao Li, Liang Ding*

Main category: cs.CV

TL;DR: AMIA is a lightweight defense for LVLMs that masks irrelevant image patches and analyzes intentions to improve safety without retraining, boosting defense success from 52.4% to 81.7%.


<details>
  <summary>Details</summary>
Motivation: To protect LVLMs from adversarial attacks and harmful intents without requiring retraining or significant utility loss.

Method: Automatically masks text-irrelevant image patches and conducts joint intention analysis to disrupt adversarial perturbations and mitigate harmful intents.

Result: Improves defense success rates from 52.4% to 81.7%, with only a 2% accuracy drop and modest inference overhead.

Conclusion: AMIA effectively balances safety and utility, with masking and intention analysis being critical components.

Abstract: We introduce AMIA, a lightweight, inference-only defense for Large
Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of
text-irrelevant image patches to disrupt adversarial perturbations, and (2)
conducts joint Intention Analysis to uncover and mitigate hidden harmful
intents before response generation. Without any retraining, AMIA improves
defense success rates across diverse LVLMs and jailbreak benchmarks from an
average of 52.4% to 81.7%, preserves general utility with only a 2% average
accuracy drop, and incurs only modest inference overhead. Ablation confirms
both masking and intention analysis are essential for a robust safety-utility
trade-off.

</details>


### [333] [UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation](https://arxiv.org/pdf/2505.24521)
*Yang-Tian Sun, Xin Yu, Zehuan Huang, Yi-Hua Huang, Yuan-Chen Guo, Ziyi Yang, Yan-Pei Cao, Xiaojuan Qi*

Main category: cs.CV

TL;DR: The paper proposes using diffusion model priors for consistent geometric estimation in videos by leveraging inter-frame correspondence and global coordinate attributes.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single-frame geometric estimation, missing the potential of diffusion models for inter-frame consistency.

Method: The approach selects global geometric attributes, reuses positional encodings for conditioning, and jointly trains on multiple attributes.

Result: Superior performance in predicting global geometric attributes in videos, applicable to reconstruction tasks, with potential generalization to dynamic scenes.

Conclusion: The method effectively harnesses diffusion model consistency for geometric estimation, showing promise for broader applications.

Abstract: Recently, methods leveraging diffusion model priors to assist monocular
geometric estimation (e.g., depth and normal) have gained significant attention
due to their strong generalization ability. However, most existing works focus
on estimating geometric properties within the camera coordinate system of
individual video frames, neglecting the inherent ability of diffusion models to
determine inter-frame correspondence. In this work, we demonstrate that,
through appropriate design and fine-tuning, the intrinsic consistency of video
generation models can be effectively harnessed for consistent geometric
estimation. Specifically, we 1) select geometric attributes in the global
coordinate system that share the same correspondence with video frames as the
prediction targets, 2) introduce a novel and efficient conditioning method by
reusing positional encodings, and 3) enhance performance through joint training
on multiple geometric attributes that share the same correspondence. Our
results achieve superior performance in predicting global geometric attributes
in videos and can be directly applied to reconstruction tasks. Even when
trained solely on static video data, our approach exhibits the potential to
generalize to dynamic video scenes.

</details>


### [334] [Optimal Density Functions for Weighted Convolution in Learning Models](https://arxiv.org/pdf/2505.24527)
*Simone Cammarasana, Giuseppe Patanè*

Main category: cs.CV

TL;DR: The paper proposes weighted convolution, an improved method over traditional uniform convolution, using an optimal density function to scale pixel contributions. It enhances CNN accuracy and reduces loss, though with a slight increase in execution time.


<details>
  <summary>Details</summary>
Motivation: Traditional uniform convolution treats all neighboring pixels equally, which may not be optimal. The paper aims to improve accuracy by weighting contributions based on distance.

Method: Introduces weighted convolution with an optimal density function. The framework optimizes kernel weights (via SGD) and density function (via DIRECT-L) separately.

Result: Experiments show up to 53% loss reduction and higher test accuracy, despite an 11% increase in execution time. The method is robust across hyperparameters.

Conclusion: Weighted convolution improves CNN performance for image tasks. Future work will extend it to 2D/3D real-case problems.

Abstract: The paper introduces the weighted convolution, a novel approach to the
convolution for signals defined on regular grids (e.g., 2D images) through the
application of an optimal density function to scale the contribution of
neighbouring pixels based on their distance from the central pixel. This choice
differs from the traditional uniform convolution, which treats all neighbouring
pixels equally. Our weighted convolution can be applied to convolutional neural
network problems to improve the approximation accuracy. Given a convolutional
network, we define a framework to compute the optimal density function through
a minimisation model. The framework separates the optimisation of the
convolutional kernel weights (using stochastic gradient descent) from the
optimisation of the density function (using DIRECT-L). Experimental results on
a learning model for an image-to-image task (e.g., image denoising) show that
the weighted convolution significantly reduces the loss (up to 53% improvement)
and increases the test accuracy compared to standard convolution. While this
method increases execution time by 11%, it is robust across several
hyperparameters of the learning model. Future work will apply the weighted
convolution to real-case 2D and 3D image convolutional learning problems.

</details>


### [335] [Geospatial Foundation Models to Enable Progress on Sustainable Development Goals](https://arxiv.org/pdf/2505.24528)
*Pedram Ghamisi, Weikang Yu, Xiaokang Zhang, Aldino Rizaldy, Jian Wang, Chufeng Zhou, Richard Gloaguen, Gustau Camps-Valls*

Main category: cs.CV

TL;DR: SustainFM introduces a benchmarking framework for geospatial Foundation Models (FMs) to assess their alignment with sustainability goals, showing their potential but advocating for broader evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To explore the real-world utility of geospatial FMs and their alignment with global sustainability goals, which remain underexplored despite rapid adoption.

Method: SustainFM, a comprehensive benchmarking framework grounded in the 17 Sustainable Development Goals, evaluates FMs across diverse tasks like asset wealth prediction and hazard detection.

Result: FMs often outperform traditional methods, but evaluation should include transferability, generalization, and energy efficiency. They enable scalable, SDG-aligned solutions.

Conclusion: Advocates for impact-driven deployment of FMs, emphasizing energy efficiency, robustness, and ethical considerations over model-centric development.

Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have
revolutionized natural language processing and computer vision, and are now
advancing geospatial analysis and Earth Observation (EO). They promise improved
generalization across tasks, scalability, and efficient adaptation with minimal
labeled data. However, despite the rapid proliferation of geospatial FMs, their
real-world utility and alignment with global sustainability goals remain
underexplored. We introduce SustainFM, a comprehensive benchmarking framework
grounded in the 17 Sustainable Development Goals with extremely diverse tasks
ranging from asset wealth prediction to environmental hazard detection. This
study provides a rigorous, interdisciplinary assessment of geospatial FMs and
offers critical insights into their role in attaining sustainability goals. Our
findings show: (1) While not universally superior, FMs often outperform
traditional approaches across diverse tasks and datasets. (2) Evaluating FMs
should go beyond accuracy to include transferability, generalization, and
energy efficiency as key criteria for their responsible use. (3) FMs enable
scalable, SDG-grounded solutions, offering broad utility for tackling complex
sustainability challenges. Critically, we advocate for a paradigm shift from
model-centric development to impact-driven deployment, and emphasize metrics
such as energy efficiency, robustness to domain shifts, and ethical
considerations.

</details>


### [336] [Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts](https://arxiv.org/pdf/2505.24541)
*Xin He, Xumeng Han, Longhui Wei, Lingxi Xie, Qi Tian*

Main category: cs.CV

TL;DR: Mixpert is a mixture-of-vision-experts architecture for MLLMs, combining joint learning benefits with task-specific fine-tuning and dynamic routing to reduce domain conflicts.


<details>
  <summary>Details</summary>
Motivation: Single vision encoders struggle with diverse tasks, causing conflicts, while multiple encoders add complexity. Mixpert aims to balance efficiency and performance.

Method: Mixpert restructures a single vision encoder into a multi-expert paradigm with dynamic routing for task-specific fine-tuning.

Result: Mixpert reduces domain conflicts with minimal computational cost and improves performance across tasks.

Conclusion: Mixpert offers an efficient, scalable solution for MLLMs, enhancing performance without added complexity.

Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of
complex image information, typically leveraging a vision encoder to perceive
various visual scenarios. However, relying solely on a single vision encoder to
handle diverse task domains proves difficult and inevitably leads to conflicts.
Recent work enhances data perception by directly integrating multiple
domain-specific vision encoders, yet this structure adds complexity and limits
the potential for joint optimization. In this paper, we introduce Mixpert, an
efficient mixture-of-vision-experts architecture that inherits the joint
learning advantages from a single vision encoder while being restructured into
a multi-expert paradigm for task-specific fine-tuning across different visual
tasks. Additionally, we design a dynamic routing mechanism that allocates input
images to the most suitable visual expert. Mixpert effectively alleviates
domain conflicts encountered by a single vision encoder in multi-task learning
with minimal additional computational cost, making it more efficient than
multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,
with experimental results demonstrating substantial performance gains across
various tasks.

</details>


### [337] [Optimal Weighted Convolution for Classification and Denosing](https://arxiv.org/pdf/2505.24558)
*Simone Cammarasana, Giuseppe Patanè*

Main category: cs.CV

TL;DR: A novel weighted convolution operator enhances CNNs by integrating a spatial density function, improving feature extraction without increasing trainable parameters. It shows performance gains in image classification and denoising tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional CNNs lack spatial weighting, limiting their ability to differentially prioritize neighboring pixels. This work aims to improve spatial characterization in feature extraction.

Method: The proposed weighted convolution operator incorporates a spatial density function, pre-computed for efficiency, and is compatible with existing CNN architectures.

Result: Experiments on CIFAR-100 and DIV2K datasets show improved performance: VGG accuracy rose from 56.89% to 66.94%, and DnCNN PSNR increased from 20.17 to 22.63.

Conclusion: The weighted convolution operator enhances CNN performance in spatial tasks while maintaining compatibility and efficiency, with potential applications in various signal types.

Abstract: We introduce a novel weighted convolution operator that enhances traditional
convolutional neural networks (CNNs) by integrating a spatial density function
into the convolution operator. This extension enables the network to
differentially weight neighbouring pixels based on their relative position to
the reference pixel, improving spatial characterisation and feature extraction.
The proposed operator maintains the same number of trainable parameters and is
fully compatible with existing CNN architectures. Although developed for 2D
image data, the framework is generalisable to signals on regular grids of
arbitrary dimensions, such as 3D volumetric data or 1D time series. We propose
an efficient implementation of the weighted convolution by pre-computing the
density function and achieving execution times comparable to standard
convolution layers. We evaluate our method on two deep learning tasks: image
classification using the CIFAR-100 dataset [KH+09] and image denoising using
the DIV2K dataset [AT17]. Experimental results with state-of-the-art
classification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN
[ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves
performance with respect to standard convolution across different quantitative
metrics. For example, VGG achieves an accuracy of 66.94% with weighted
convolution versus 56.89% with standard convolution on the classification
problem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the
denoising problem. All models were trained on the CINECA Leonardo cluster to
reduce the execution time and improve the tuning of the density function
values. The PyTorch implementation of the weighted convolution is publicly
available at: https://github.com/cammarasana123/weightedConvolution2.0.

</details>


### [338] [Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation](https://arxiv.org/pdf/2505.24567)
*Qinghe Ma, Jian Zhang, Lei Qi, Qian Yu, Yinghuan Shi, Yang Gao*

Main category: cs.CV

TL;DR: The paper introduces MiDSS, a scenario combining limited labeled data and domain shift in medical image segmentation, and proposes the UST-RUN framework to address it.


<details>
  <summary>Details</summary>
Motivation: The coexistence of limited annotation and domain shift in medical image segmentation motivates the need for a unified solution, leading to the MiDSS scenario.

Method: UST-RUN leverages intermediate domains via Unified Copy-paste (UCP), Symmetric GuiDance (SymGD), and Training Process aware Random Amplitude MixUp (TP-RAM) for effective knowledge transfer.

Result: UST-RUN achieves a 12.94% improvement in Dice score on the Prostate dataset, outperforming other methods.

Conclusion: UST-RUN effectively addresses MiDSS, demonstrating superior performance and potential for practical medical image segmentation tasks.

Abstract: Both limited annotation and domain shift are prevalent challenges in medical
image segmentation. Traditional semi-supervised segmentation and unsupervised
domain adaptation methods address one of these issues separately. However, the
coexistence of limited annotation and domain shift is quite common, which
motivates us to introduce a novel and challenging scenario: Mixed Domain
Semi-supervised medical image Segmentation (MiDSS), where limited labeled data
from a single domain and a large amount of unlabeled data from multiple
domains. To tackle this issue, we propose the UST-RUN framework, which fully
leverages intermediate domain information to facilitate knowledge transfer. We
employ Unified Copy-paste (UCP) to construct intermediate domains, and propose
a Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by
merging pseudo-labels from intermediate samples. Subsequently, we introduce a
Training Process aware Random Amplitude MixUp (TP-RAM) to progressively
incorporate style-transition components into intermediate samples. To generate
more diverse intermediate samples, we further select reliable samples with
high-quality pseudo-labels, which are then mixed with other unlabeled data.
Additionally, we generate sophisticated intermediate samples with high-quality
pseudo-labels for unreliable samples, ensuring effective knowledge transfer for
them. Extensive experiments on four public datasets demonstrate the superiority
of UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the
Prostate dataset. Our code is available at https://github.com/MQinghe/UST-RUN

</details>


### [339] [SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition](https://arxiv.org/pdf/2505.24600)
*Omer Nacar, Yasser Al-Habashi, Serry Sibaee, Adel Ammar, Wadii Boulila*

Main category: cs.CV

TL;DR: SARD is a synthetic Arabic OCR dataset designed to address the lack of large, diverse datasets for training models on book-style documents. It includes 843,622 images with 690 million words across ten fonts, offering a clean, scalable resource for OCR development.


<details>
  <summary>Details</summary>
Motivation: Existing Arabic OCR datasets are limited in scale, diversity, or structural complexity, hindering the training of modern OCR models for book-style texts.

Method: SARD is synthetically generated to simulate book layouts, using ten Arabic fonts and controlled variations to ensure typographic diversity and scalability.

Result: The dataset provides a noise-free, scalable resource with benchmark results for various OCR models, showcasing its utility for robust model training.

Conclusion: SARD fills a critical gap in Arabic OCR research, enabling the development of models capable of handling diverse book-style texts effectively.

Abstract: Arabic Optical Character Recognition (OCR) is essential for converting vast
amounts of Arabic print media into digital formats. However, training modern
OCR models, especially powerful vision-language models, is hampered by the lack
of large, diverse, and well-structured datasets that mimic real-world book
layouts. Existing Arabic OCR datasets often focus on isolated words or lines or
are limited in scale, typographic variety, or structural complexity found in
books. To address this significant gap, we introduce SARD (Large-Scale
Synthetic Arabic OCR Dataset). SARD is a massive, synthetically generated
dataset specifically designed to simulate book-style documents. It comprises
843,622 document images containing 690 million words, rendered across ten
distinct Arabic fonts to ensure broad typographic coverage. Unlike datasets
derived from scanned documents, SARD is free from real-world noise and
distortions, offering a clean and controlled environment for model training.
Its synthetic nature provides unparalleled scalability and allows for precise
control over layout and content variation. We detail the dataset's composition
and generation process and provide benchmark results for several OCR models,
including traditional and deep learning approaches, highlighting the challenges
and opportunities presented by this dataset. SARD serves as a valuable resource
for developing and evaluating robust OCR and vision-language models capable of
processing diverse Arabic book-style texts.

</details>


### [340] [GARLIC: GAussian Representation LearnIng for spaCe partitioning](https://arxiv.org/pdf/2505.24608)
*Panagiotis Rigas, Panagiotis Drivas, Charalambos Tzamos, Ioannis Chamodrakas, George Ioannakis, Leonidas J. Guibas, Ioannis Z. Emiris*

Main category: cs.CV

TL;DR: GARLIC is a novel indexing structure using N-dimensional Gaussians for efficient high-dimensional vector space learning, achieving fast build times and high accuracy in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve high-dimensional search and classification by adapting Gaussian splatting techniques from 3D rendering to create an efficient indexing structure.

Method: GARLIC optimizes Gaussian parameters using information-theoretic objectives and refines the representation through split and clone operations to handle varying data densities.

Result: GARLIC outperforms Faiss-IVF in retrieval tasks and achieves ~15% higher accuracy in classification, with strong generalization even with minimal training data.

Conclusion: GARLIC is a powerful tool for applications requiring both speed and accuracy in high-dimensional vector space tasks.

Abstract: We introduce GARLIC (GAussian Representation LearnIng for spaCe
partitioning), a novel indexing structure based on \(N\)-dimensional Gaussians
for efficiently learning high-dimensional vector spaces. Our approach is
inspired from Gaussian splatting techniques, typically used in 3D rendering,
which we adapt for high-dimensional search and classification. We optimize
Gaussian parameters using information-theoretic objectives that balance
coverage, assignment confidence, and structural and semantic consistency. A key
contribution is to progressively refine the representation through split and
clone operations, handling hundreds of dimensions, thus handling varying data
densities. GARLIC offers the fast building times of traditional space
partitioning methods (e.g., under \(\sim5\) min build time for SIFT1M) while
achieving \(\sim50\%\) Recall10@10 in low-candidate regimes. Experimental
results on standard benchmarks demonstrate our method's consistency in (a)
\(k\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by
using about half their probes for the same Recall10@10 in Fashion-MNIST, and
(b) in classification tasks, beating by \(\sim15\%\) accuracy other majority
voting methods. Further, we show strong generalization capabilities,
maintaining high accuracy even with downsampled training data: using just
\(1\%\) of the training data returns \(\sim 45\%\) Recall@1, thus making GARLIC
quite powerful for applications requiring both speed and accuracy.

</details>


### [341] [Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/pdf/2505.24625)
*Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang*

Main category: cs.CV

TL;DR: The paper introduces VG LLM, a method for 3D scene understanding directly from video data without explicit 3D inputs, outperforming some state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To enhance MLLMs' ability to understand 3D spaces from video alone, eliminating the need for additional 3D data like point clouds or BEV maps.

Method: Proposes VG LLM, which uses a 3D visual geometry encoder to extract 3D priors from videos, integrating them with visual tokens for MLLM processing.

Result: Achieves significant improvements in 3D scene understanding tasks, with a 4B model matching or surpassing state-of-the-art methods like Gemini-1.5-Pro in VSI-Bench.

Conclusion: VG LLM demonstrates efficient 3D reasoning directly from videos, offering a competitive alternative to methods requiring explicit 3D data.

Abstract: Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method, the Video-3D Geometry Large
Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that
extracts 3D prior information from video sequences. This information is
integrated with visual tokens and fed into the MLLM. Extensive experiments have
shown that our method has achieved substantial improvements in various tasks
related to 3D scene understanding and spatial reasoning, all directly learned
from video sources. Impressively, our 4B model, which does not rely on explicit
3D data inputs, achieves competitive results compared to existing
state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.

</details>


### [342] [NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation](https://arxiv.org/pdf/2505.24634)
*Xuzhi Wang, Wei Feng, Lingdong Kong, Liang Wan*

Main category: cs.CV

TL;DR: NUC-Net proposes a non-uniform cylindrical partition network for LiDAR semantic segmentation, addressing inefficiencies and unbalanced point distribution in existing voxel-based methods. It achieves state-of-the-art performance with faster speed and reduced resource usage.


<details>
  <summary>Details</summary>
Motivation: Existing voxel-based methods for LiDAR semantic segmentation are inefficient due to high computational costs and poor handling of unbalanced point distribution.

Method: NUC-Net introduces the Arithmetic Progression of Interval (API) method for non-uniform radial axis partitioning and a non-uniform multi-scale aggregation method.

Result: Achieves top performance on SemanticKITTI and nuScenes datasets, with 4x faster training, 2x GPU memory reduction, and 3x inference speedup.

Conclusion: NUC-Net is a general, efficient solution for LiDAR semantic segmentation, improving accuracy and efficiency while providing theoretical insights.

Abstract: LiDAR semantic segmentation plays a vital role in autonomous driving.
Existing voxel-based methods for LiDAR semantic segmentation apply uniform
partition to the 3D LiDAR point cloud to form a structured representation based
on cartesian/cylindrical coordinates. Although these methods show impressive
performance, the drawback of existing voxel-based methods remains in two
aspects: (1) it requires a large enough input voxel resolution, which brings a
large amount of computation cost and memory consumption. (2) it does not well
handle the unbalanced point distribution of LiDAR point cloud. In this paper,
we propose a non-uniform cylindrical partition network named NUC-Net to tackle
the above challenges. Specifically, we propose the Arithmetic Progression of
Interval (API) method to non-uniformly partition the radial axis and generate
the voxel representation which is representative and efficient. Moreover, we
propose a non-uniform multi-scale aggregation method to improve contextual
information. Our method achieves state-of-the-art performance on SemanticKITTI
and nuScenes datasets with much faster speed and much less training time. And
our method can be a general component for LiDAR semantic segmentation, which
significantly improves both the accuracy and efficiency of the uniform
counterpart by $4 \times$ training faster and $2 \times$ GPU memory reduction
and $3 \times$ inference speedup. We further provide theoretical analysis
towards understanding why NUC is effective and how point distribution affects
performance. Code is available at
\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.

</details>


### [343] [Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data](https://arxiv.org/pdf/2505.24636)
*Marios Glytsos, Panagiotis P. Filntisis, George Retsinas, Petros Maragos*

Main category: cs.CV

TL;DR: PLANTPose is a novel RGB-based framework for category-level 6D pose estimation in agriculture, handling intra-class variability without instance-specific CAD models or depth sensors. It outperforms MegaPose.


<details>
  <summary>Details</summary>
Motivation: Accurate 6D pose estimation is crucial for robotic grasping in agriculture, but existing methods rely on impractical CAD models or depth sensors. PLANTPose addresses this gap.

Method: PLANTPose predicts 6D pose and deformation parameters using a base mesh and refines synthetic training images with Stable Diffusion for realism.

Result: Evaluations show PLANTPose handles large intra-class variations (e.g., bananas of different shapes/sizes) and outperforms MegaPose.

Conclusion: PLANTPose offers a practical, accurate solution for agricultural 6D pose estimation without depth sensors or instance-specific data, advancing RGB-based methods.

Abstract: Accurate 6D object pose estimation is essential for robotic grasping and
manipulation, particularly in agriculture, where fruits and vegetables exhibit
high intra-class variability in shape, size, and texture. The vast majority of
existing methods rely on instance-specific CAD models or require depth sensors
to resolve geometric ambiguities, making them impractical for real-world
agricultural applications. In this work, we introduce PLANTPose, a novel
framework for category-level 6D pose estimation that operates purely on RGB
input. PLANTPose predicts both the 6D pose and deformation parameters relative
to a base mesh, allowing a single category-level CAD model to adapt to unseen
instances. This enables accurate pose estimation across varying shapes without
relying on instance-specific data. To enhance realism and improve
generalization, we also leverage Stable Diffusion to refine synthetic training
images with realistic texturing, mimicking variations due to ripeness and
environmental factors and bridging the domain gap between synthetic data and
the real world. Our evaluations on a challenging benchmark that includes
bananas of various shapes, sizes, and ripeness status demonstrate the
effectiveness of our framework in handling large intraclass variations while
maintaining accurate 6D pose predictions, significantly outperforming the
state-of-the-art RGB-based approach MegaPose.

</details>


### [344] [Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models](https://arxiv.org/pdf/2505.24638)
*Zahid Hassan Tushar, Adeleke Ademakinwa, Jianwu Wang, Zhibo Zhang, Sanjay Purushotham*

Main category: cs.CV

TL;DR: A novel deep learning model, CAAC, is proposed to improve Cloud Optical Thickness (COT) estimation by addressing biases from traditional methods and enhancing robustness against radiance variations and viewing angles.


<details>
  <summary>Details</summary>
Motivation: Accurate COT estimation is crucial for climate and weather studies, but existing methods like IPA and deep learning models suffer from biases and sensitivity to radiance and angle variations.

Method: The proposed CAAC model uses attention mechanisms and angle embeddings to account for viewing geometry and 3D radiative effects, with a multi-angle training strategy for angle invariance.

Result: CAAC outperforms state-of-the-art models, reducing COT retrieval errors by at least nine times.

Conclusion: CAAC provides a robust and accurate solution for COT estimation, addressing limitations of existing methods.

Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing
Earth's climate, weather, and radiation budget. Satellite radiance measurements
enable global COT retrieval, but challenges like 3D cloud effects, viewing
angles, and atmospheric interference must be addressed to ensure accurate
estimation. Traditionally, the Independent Pixel Approximation (IPA) method,
which treats individual pixels independently, has been used for COT estimation.
However, IPA introduces significant bias due to its simplified assumptions.
Recently, deep learning-based models have shown improved performance over IPA
but lack robustness, as they are sensitive to variations in radiance intensity,
distortions, and cloud shadows. These models also introduce substantial errors
in COT estimation under different solar and viewing zenith angles. To address
these challenges, we propose a novel angle-invariant, attention-based deep
model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages
attention mechanisms and angle embeddings to account for satellite viewing
geometry and 3D radiative transfer effects, enabling more accurate retrieval of
COT. Additionally, our multi-angle training strategy ensures angle invariance.
Through comprehensive experiments, we demonstrate that CAAC significantly
outperforms existing state-of-the-art deep learning models, reducing cloud
property retrieval errors by at least a factor of nine.

</details>


### [345] [A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning](https://arxiv.org/pdf/2505.24641)
*Chengzhi Wu, Qianliang Huang, Kun Jin, Julius Pfrommer, Jürgen Beyerer*

Main category: cs.CV

TL;DR: The paper introduces PoCCA, a contrastive learning framework for point cloud data that enables information exchange between branches before the loss stage, improving unsupervised representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning frameworks lack information exchange between branches before the loss stage, limiting their effectiveness for point cloud data.

Method: Proposes PoCCA, which introduces sub-branches to allow early information exchange between branches in contrastive learning for point clouds.

Result: PoCCA achieves state-of-the-art performance in downstream tasks without extra training data.

Conclusion: PoCCA enhances unsupervised point cloud representation learning by enabling inter-branch information exchange early in the process.

Abstract: Contrastive learning is an essential method in self-supervised learning. It
primarily employs a multi-branch strategy to compare latent representations
obtained from different branches and train the encoder. In the case of
multi-modal input, diverse modalities of the same object are fed into distinct
branches. When using single-modal data, the same input undergoes various
augmentations before being fed into different branches. However, all existing
contrastive learning frameworks have so far only performed contrastive
operations on the learned features at the final loss end, with no information
exchange between different branches prior to this stage. In this paper, for
point cloud unsupervised learning without the use of extra training data, we
propose a Contrastive Cross-branch Attention-based framework for Point cloud
data (termed PoCCA), to learn rich 3D point cloud representations. By
introducing sub-branches, PoCCA allows information exchange between different
branches before the loss end. Experimental results demonstrate that in the case
of using no extra training data, the representations learned with our
self-supervised model achieve state-of-the-art performances when used for
downstream tasks on point clouds.

</details>


### [346] [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/pdf/2505.24649)
*Huu-Thien Tran, Thanh-Dat Truong, Khoa Luu*

Main category: cs.CV

TL;DR: The paper introduces Bijective Maximum Likelihood Learning (BIMA) to reduce hallucination in vision-language models, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of hallucination in large vision-language models to enhance trustworthiness and interpretability.

Method: Proposes BIMA, leveraging normalizing flow theories to mitigate hallucination during the decoding process.

Result: BIMA achieves an 85.06% F1 score on POPE benchmark and reduces CHAIRS and CHAIRI by 7.6% and 2.6%, respectively.

Conclusion: BIMA is an effective approach for hallucination mitigation in vision-language models, pioneering the use of bijection methods.

Abstract: Large vision-language models have become widely adopted to advance in various
domains. However, developing a trustworthy system with minimal interpretable
characteristics of large-scale models presents a significant challenge. One of
the most prevalent terms associated with the fallacy functions caused by these
systems is hallucination, where the language model generates a response that
does not correspond to the visual content. To mitigate this problem, several
approaches have been developed, and one prominent direction is to ameliorate
the decoding process. In this paper, we propose a new Bijective Maximum
Likelihood Learning (BIMA) approach to hallucination mitigation using
normalizing flow theories. The proposed BIMA method can efficiently mitigate
the hallucination problem in prevailing vision-language models, resulting in
significant improvements. Notably, BIMA achieves the average F1 score of 85.06%
on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,
respectively. To the best of our knowledge, this is one of the first studies
that contemplates the bijection means to reduce hallucination induced by large
vision-language models.

</details>


### [347] [Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation](https://arxiv.org/pdf/2505.24667)
*Jiahe Chen, Jiahe Ying, Shen Wang, Jianwei Zheng*

Main category: cs.CV

TL;DR: The paper introduces a Decoupled Competitive Framework (DCF) to address limitations in semi-supervised medical image segmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Overcoming performance bottlenecks in Mean Teacher and Dual Students architectures due to over-coupling and cognitive bias.

Method: DCF uses a competition mechanism for EMA updates and facilitates knowledge exchange among students.

Result: Validated on 2D and 3D datasets, DCF outperforms state-of-the-art methods.

Conclusion: DCF effectively mitigates existing limitations and enhances segmentation performance.

Abstract: Confronting the critical challenge of insufficiently annotated samples in
medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a
promising solution. Specifically, most methodologies following the Mean Teacher
(MT) or Dual Students (DS) architecture have achieved commendable results.
However, to date, these approaches face a performance bottleneck due to two
inherent limitations, \textit{e.g.}, the over-coupling problem within MT
structure owing to the employment of exponential moving average (EMA)
mechanism, as well as the severe cognitive bias between two students of DS
structure, both of which potentially lead to reduced efficacy, or even model
collapse eventually. To mitigate these issues, a Decoupled Competitive
Framework (DCF) is elaborated in this work, which utilizes a straightforward
competition mechanism for the update of EMA, effectively decoupling students
and teachers in a dynamical manner. In addition, the seamless exchange of
invaluable and precise insights is facilitated among students, guaranteeing a
better learning paradigm. The DCF introduced undergoes rigorous validation on
three publicly accessible datasets, which encompass both 2D and 3D datasets.
The results demonstrate the superiority of our method over previous
cutting-edge competitors. Code will be available at
https://github.com/JiaheChen2002/DCF.

</details>


### [348] [6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly](https://arxiv.org/pdf/2505.24669)
*Chengzhi Wu, Hao Fu, Jan-Philipp Kaiser, Erik Tabuchi Barczak, Julius Pfrommer, Gisela Lanza, Michael Heizmann, Jürgen Beyerer*

Main category: cs.CV

TL;DR: The paper proposes a multi-stage pipeline for 6D pose estimation of bolts on starter motors, leveraging prior knowledge to address occlusions and single-view limitations in manufacturing automation.


<details>
  <summary>Details</summary>
Motivation: Accurate 6D pose estimation is challenging, especially in manufacturing where occlusions and single-view data hinder bolt detection during automated disassembly.

Method: A meticulously devised multi-stage pipeline is introduced to capture complete 6D bolt information, utilizing prior knowledge to overcome data limitations.

Result: The pipeline successfully estimates 6D poses of all bolts on the motor, demonstrating the effectiveness of prior knowledge in complex tasks.

Conclusion: The study advances 6D pose estimation and highlights the value of domain-specific insights in solving manufacturing automation challenges.

Abstract: The accurate estimation of 6D pose remains a challenging task within the
computer vision domain, even when utilizing 3D point cloud data. Conversely, in
the manufacturing domain, instances arise where leveraging prior knowledge can
yield advancements in this endeavor. This study focuses on the disassembly of
starter motors to augment the engineering of product life cycles. A pivotal
objective in this context involves the identification and 6D pose estimation of
bolts affixed to the motors, facilitating automated disassembly within the
manufacturing workflow. Complicating matters, the presence of occlusions and
the limitations of single-view data acquisition, notably when motors are placed
in a clamping system, obscure certain portions and render some bolts
imperceptible. Consequently, the development of a comprehensive pipeline
capable of acquiring complete bolt information is imperative to avoid oversight
in bolt detection. In this paper, employing the task of bolt detection within
the scope of our project as a pertinent use case, we introduce a meticulously
devised pipeline. This multi-stage pipeline effectively captures the 6D
information with regard to all bolts on the motor, thereby showcasing the
effective utilization of prior knowledge in handling this challenging task. The
proposed methodology not only contributes to the field of 6D pose estimation
but also underscores the viability of integrating domain-specific insights to
tackle complex problems in manufacturing and automation.

</details>


### [349] [Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism](https://arxiv.org/pdf/2505.24679)
*Evangelos Sariyanidi, Lisa Yankowitz, Robert T. Schultz, John D. Herrington, Birkan Tunc, Jeffrey Cohn*

Main category: cs.CV

TL;DR: The paper introduces Facial Basis, a data-driven coding system as an alternative to FACS for detecting facial movements, addressing limitations like manual annotation, limited movement recognition, and non-additive combinations.


<details>
  <summary>Details</summary>
Motivation: The laborious and costly process of FACS coding, along with its limitations in accuracy and coverage, motivates the development of a more efficient and comprehensive alternative.

Method: The proposed method, Facial Basis, is unsupervised, data-driven, and constructs interpretable 3D facial movement units, overcoming structural limitations of FACS.

Result: Facial Basis outperforms traditional AU detectors in predicting autism diagnosis and reconstructs all observable facial movements.

Conclusion: Facial Basis is a viable alternative to FACS, offering comprehensive encoding of facial behavior with open-source implementation.

Abstract: The Facial Action Coding System (FACS) has been used by numerous studies to
investigate the links between facial behavior and mental health. The laborious
and costly process of FACS coding has motivated the development of machine
learning frameworks for Action Unit (AU) detection. Despite intense efforts
spanning three decades, the detection accuracy for many AUs is considered to be
below the threshold needed for behavioral research. Also, many AUs are excluded
altogether, making it impossible to fulfill the ultimate goal of FACS-the
representation of any facial expression in its entirety. This paper considers
an alternative approach. Instead of creating automated tools that mimic FACS
experts, we propose to use a new coding system that mimics the key properties
of FACS. Specifically, we construct a data-driven coding system called the
Facial Basis, which contains units that correspond to localized and
interpretable 3D facial movements, and overcomes three structural limitations
of automated FACS coding. First, the proposed method is completely
unsupervised, bypassing costly, laborious and variable manual annotation.
Second, Facial Basis reconstructs all observable movement, rather than relying
on a limited repertoire of recognizable movements (as in automated FACS).
Finally, the Facial Basis units are additive, whereas AUs may fail detection
when they appear in a non-additive combination. The proposed method outperforms
the most frequently used AU detector in predicting autism diagnosis from
in-person and remote conversations, highlighting the importance of encoding
facial behavior comprehensively. To our knowledge, Facial Basis is the first
alternative to FACS for deconstructing facial expressions in videos into
localized movements. We provide an open source implementation of the method at
github.com/sariyanidi/FacialBasis.

</details>


### [350] [Learning reusable concepts across different egocentric video understanding tasks](https://arxiv.org/pdf/2505.24690)
*Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Tatiana Tommasi, Giuseppe Averta*

Main category: cs.CV

TL;DR: Hier-EgoPack is a unified framework for holistic video understanding, enabling autonomous systems to correlate concepts and leverage task synergies.


<details>
  <summary>Details</summary>
Motivation: To equip autonomous systems with multifaceted human-like perception of video streams, including understanding, relevance identification, and forecasting.

Method: Introduces Hier-EgoPack, a framework that creates task perspectives for downstream tasks, acting as a 'backpack of skills' for robots.

Result: The framework provides a reusable collection of task perspectives for enhanced learning and adaptation in novel tasks.

Conclusion: Hier-EgoPack advances holistic perception in autonomous systems by enabling cross-task knowledge abstraction and synergy.

Abstract: Our comprehension of video streams depicting human activities is naturally
multifaceted: in just a few moments, we can grasp what is happening, identify
the relevance and interactions of objects in the scene, and forecast what will
happen soon, everything all at once. To endow autonomous systems with such
holistic perception, learning how to correlate concepts, abstract knowledge
across diverse tasks, and leverage tasks synergies when learning novel skills
is essential. In this paper, we introduce Hier-EgoPack, a unified framework
able to create a collection of task perspectives that can be carried across
downstream tasks and used as a potential source of additional insights, as a
backpack of skills that a robot can carry around and use when needed.

</details>


### [351] [Conformal Prediction for Zero-Shot Models](https://arxiv.org/pdf/2505.24693)
*Julio Silva-Rodríguez, Ismail Ben Ayed, Jose Dolz*

Main category: cs.CV

TL;DR: The paper explores the reliability and uncertainty of CLIP models using split conformal prediction, addressing domain drift with Conf-OT, a method improving efficiency by 20% and speed by 15x.


<details>
  <summary>Details</summary>
Motivation: Despite the adaptability of vision-language models like CLIP, their reliability and uncertainty are understudied. This work aims to address these gaps.

Method: The study uses split conformal prediction with Conf-OT, an optimal transport-based transfer learning method, to bridge domain gaps without extra data splits.

Result: Conf-OT improves set efficiency by up to 20% and is 15 times faster than existing transductive methods, tested on 15 datasets.

Conclusion: Conf-OT effectively mitigates domain drift in conformal prediction for CLIP models, enhancing efficiency and speed while maintaining theoretical guarantees.

Abstract: Vision-language models pre-trained at large scale have shown unprecedented
adaptability and generalization to downstream tasks. Although its
discriminative potential has been widely explored, its reliability and
uncertainty are still overlooked. In this work, we investigate the capabilities
of CLIP models under the split conformal prediction paradigm, which provides
theoretical guarantees to black-box models based on a small, labeled
calibration set. In contrast to the main body of literature on conformal
predictors in vision classifiers, foundation models exhibit a particular
characteristic: they are pre-trained on a one-time basis on an inaccessible
source domain, different from the transferred task. This domain drift
negatively affects the efficiency of the conformal sets and poses additional
challenges. To alleviate this issue, we propose Conf-OT, a transfer learning
setting that operates transductive over the combined calibration and query
sets. Solving an optimal transport problem, the proposed method bridges the
domain gap between pre-training and adaptation without requiring additional
data splits but still maintaining coverage guarantees. We comprehensively
explore this conformal prediction strategy on a broad span of 15 datasets and
three non-conformity scores. Conf-OT provides consistent relative improvements
of up to 20% on set efficiency while being 15 times faster than popular
transductive approaches.

</details>


### [352] [RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement](https://arxiv.org/pdf/2505.24705)
*Raman Jha, Adithya Lenka, Mani Ramanagopal, Aswin Sankaranarayanan, Kaushik Mitra*

Main category: cs.CV

TL;DR: RT-X Net is a cross-attention network fusing RGB and thermal images for nighttime enhancement, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Nighttime images suffer from noise and poor illumination; thermal images provide complementary details, motivating fusion for better enhancement.

Method: Uses self-attention for feature extraction and cross-attention for RGB-thermal fusion, evaluated on LLVIP and the new V-TIEE dataset.

Result: RT-X Net achieves superior performance in low-light enhancement compared to existing methods.

Conclusion: The proposed method and V-TIEE dataset advance nighttime image enhancement research.

Abstract: In nighttime conditions, high noise levels and bright illumination sources
degrade image quality, making low-light image enhancement challenging. Thermal
images provide complementary information, offering richer textures and
structural details. We propose RT-X Net, a cross-attention network that fuses
RGB and thermal images for nighttime image enhancement. We leverage
self-attention networks for feature extraction and a cross-attention mechanism
for fusion to effectively integrate information from both modalities. To
support research in this domain, we introduce the Visible-Thermal Image
Enhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and
thermal images captured under diverse nighttime conditions. Extensive
evaluations on the publicly available LLVIP dataset and our V-TIEE dataset
demonstrate that RT-X Net outperforms state-of-the-art methods in low-light
image enhancement. The code and the V-TIEE can be found here
https://github.com/jhakrraman/rt-xnet.

</details>


### [353] [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/pdf/2505.24718)
*Jisheng Dang, Jingze Wu, Teng Wang, Xuanhui Lin, Nannan Zhu, Hongbo Chen, Wei-Shi Zheng, Meng Wang, Tat-Seng Chua*

Main category: cs.CV

TL;DR: TW-GRPO improves multimodal reasoning by focusing on key tokens and using dense rewards, outperforming prior methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of GRPO: unfocused reasoning chains and binary rewards, which hinder efficiency and accuracy.

Method: Introduces token weighting for focused reasoning and soft rewards for partial correctness, plus question-answer inversion for data augmentation.

Result: Achieves 50.4% accuracy on CLEVRER (18.8% improvement) and 65.8% on MMVU, setting new benchmarks.

Conclusion: TW-GRPO enhances reasoning efficiency and accuracy, validated by superior performance on complex tasks.

Abstract: Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group variance),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.

</details>


### [354] [DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds](https://arxiv.org/pdf/2505.24733)
*Jiaxu Zhang, Xianfang Zeng, Xin Chen, Wei Zuo, Gang Yu, Guosheng Lin, Zhigang Tu*

Main category: cs.CV

TL;DR: DreamDance is a framework for stable character animation using camera trajectories, combining scene and video inpainting for high-quality results.


<details>
  <summary>Details</summary>
Motivation: To create stable, consistent character animations with precise camera control, addressing the challenge of integrating dynamic characters into scene motion.

Method: Uses two steps: Camera-aware Scene Inpainting for background rendering and Pose-aware Video Inpainting to inject characters and refine the scene.

Result: Produces high-quality, consistent animations with dynamic camera effects, validated through experiments.

Conclusion: DreamDance effectively integrates character and scene motion, offering a robust solution for animation with camera dynamics.

Abstract: This paper presents DreamDance, a novel character art animation framework
capable of producing stable, consistent character and scene motion conditioned
on precise camera trajectories. To achieve this, we re-formulate the animation
task as two inpainting-based steps: Camera-aware Scene Inpainting and
Pose-aware Video Inpainting. The first step leverages a pre-trained image
inpainting model to generate multi-view scene images from the reference art and
optimizes a stable large-scale Gaussian field, which enables coarse background
video rendering with camera trajectories. However, the rendered video is rough
and only conveys scene motion. To resolve this, the second step trains a
pose-aware video inpainting model that injects the dynamic character into the
scene video while enhancing background quality. Specifically, this model is a
DiT-based video generation model with a gating strategy that adaptively
integrates the character's appearance and pose information into the base
background video. Through extensive experiments, we demonstrate the
effectiveness and generalizability of DreamDance, producing high-quality and
consistent character animations with remarkable camera dynamics.

</details>


### [355] [Tackling View-Dependent Semantics in 3D Language Gaussian Splatting](https://arxiv.org/pdf/2505.24746)
*Jiazhong Cen, Xudong Zhou, Jiemin Fang, Changsong Wen, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian*

Main category: cs.CV

TL;DR: LaGa (Language Gaussians) addresses view-dependent semantics in 3D-GS by decomposing scenes into objects and aggregating multi-view semantics, achieving +18.7% mIoU improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods project 2D semantic features onto 3D Gaussians, ignoring view-dependent semantics in 3D understanding.

Method: LaGa decomposes scenes into objects, clusters semantic descriptors, and reweights them based on multi-view semantics.

Result: LaGa outperforms SOTA by +18.7% mIoU on the LERF-OVS dataset.

Conclusion: LaGa effectively captures view-dependent semantics, enhancing 3D scene understanding.

Abstract: Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D
scene reconstruction from RGB images. Many studies extend this paradigm for
language-driven open-vocabulary scene understanding. However, most of them
simply project 2D semantic features onto 3D Gaussians and overlook a
fundamental gap between 2D and 3D understanding: a 3D object may exhibit
various semantics from different viewpoints--a phenomenon we term
view-dependent semantics. To address this challenge, we propose LaGa (Language
Gaussians), which establishes cross-view semantic connections by decomposing
the 3D scene into objects. Then, it constructs view-aggregated semantic
representations by clustering semantic descriptors and reweighting them based
on multi-view semantics. Extensive experiments demonstrate that LaGa
effectively captures key information from view-dependent semantics, enabling a
more comprehensive understanding of 3D scenes. Notably, under the same
settings, LaGa achieves a significant improvement of +18.7% mIoU over the
previous SOTA on the LERF-OVS dataset. Our code is available at:
https://github.com/SJTU-DeepVisionLab/LaGa.

</details>


### [356] [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/pdf/2505.24787)
*Yucheng Zhou, Jiahao Yuan, Qianning Wang*

Main category: cs.CV

TL;DR: The paper introduces LongBench-T2I, a benchmark for evaluating text-to-image models on complex prompts, and proposes Plan2Gen, an agent framework for better generation without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models struggle with complex instructions, and current benchmarks lack nuanced evaluation for such prompts.

Method: Developed LongBench-T2I with 500 complex prompts across nine dimensions and proposed Plan2Gen, an LLM-integrated framework for better generation.

Result: LongBench-T2I enables thorough evaluation, and Plan2Gen improves complex instruction handling without additional training.

Conclusion: The work addresses gaps in T2I evaluation and generation for complex prompts, offering tools and a benchmark for future research.

Abstract: Recent advancements in text-to-image (T2I) generation have enabled models to
produce high-quality images from textual descriptions. However, these models
often struggle with complex instructions involving multiple objects,
attributes, and spatial relationships. Existing benchmarks for evaluating T2I
models primarily focus on general text-image alignment and fail to capture the
nuanced requirements of complex, multi-faceted prompts. Given this gap, we
introduce LongBench-T2I, a comprehensive benchmark specifically designed to
evaluate T2I models under complex instructions. LongBench-T2I consists of 500
intricately designed prompts spanning nine diverse visual evaluation
dimensions, enabling a thorough assessment of a model's ability to follow
complex instructions. Beyond benchmarking, we propose an agent framework
(Plan2Gen) that facilitates complex instruction-driven image generation without
requiring additional model training. This framework integrates seamlessly with
existing T2I models, using large language models to interpret and decompose
complex prompts, thereby guiding the generation process more effectively. As
existing evaluation metrics, such as CLIPScore, fail to adequately capture the
nuances of complex instructions, we introduce an evaluation toolkit that
automates the quality assessment of generated images using a set of
multi-dimensional metrics. The data and code are released at
https://github.com/yczhou001/LongBench-T2I.

</details>


### [357] [Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification](https://arxiv.org/pdf/2505.24792)
*Xinliu Zhong, Leo Hwa Liang, Angela S. Koh, Yeo Si Yong*

Main category: cs.CV

TL;DR: A novel Deep Learning network using Few-Shot Learning improves CRC detection in endoscopic images, achieving high accuracy and outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Traditional colonoscopy is invasive and relies on high-quality images, which are challenging to obtain and analyze. Early CRC detection is crucial for patient survival.

Method: Proposes a Few-Shot Learning architecture with feature extraction, task interpolation, relational embedding, and bi-level routing attention to enhance image analysis.

Result: Achieved 90.1% accuracy, 0.845 precision, 0.942 recall, and 0.891 F1 score on the Kvasir dataset, surpassing state-of-the-art methods.

Conclusion: The model offers a robust, non-invasive solution for CRC detection, improving diagnostic accuracy and patient outcomes.

Abstract: Traditional diagnostic methods like colonoscopy are invasive yet critical
tools necessary for accurately diagnosing colorectal cancer (CRC). Detection of
CRC at early stages is crucial for increasing patient survival rates. However,
colonoscopy is dependent on obtaining adequate and high-quality endoscopic
images. Prolonged invasive procedures are inherently risky for patients, while
suboptimal or insufficient images hamper diagnostic accuracy. These images,
typically derived from video frames, often exhibit similar patterns, posing
challenges in discrimination. To overcome these challenges, we propose a novel
Deep Learning network built on a Few-Shot Learning architecture, which includes
a tailored feature extractor, task interpolation, relational embedding, and a
bi-level routing attention mechanism. The Few-Shot Learning paradigm enables
our model to rapidly adapt to unseen fine-grained endoscopic image patterns,
and the task interpolation augments the insufficient images artificially from
varied instrument viewpoints. Our relational embedding approach discerns
critical intra-image features and captures inter-image transitions between
consecutive endoscopic frames, overcoming the limitations of Convolutional
Neural Networks (CNNs). The integration of a light-weight attention mechanism
ensures a concentrated analysis of pertinent image regions. By training on
diverse datasets, the model's generalizability and robustness are notably
improved for handling endoscopic images. Evaluated on Kvasir dataset, our model
demonstrated superior performance, achieving an accuracy of 90.1\%, precision
of 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current
state-of-the-art methods, presenting a promising solution to the challenges of
invasive colonoscopy by optimizing CRC detection through advanced image
analysis.

</details>


### [358] [CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning](https://arxiv.org/pdf/2505.24816)
*Jiangpeng He, Zhihao Duan, Fengqing Zhu*

Main category: cs.CV

TL;DR: CL-LoRA introduces a dual-adapter architecture for Class-Incremental Learning (CIL) using task-shared and task-specific adapters to reduce redundancy and leverage shared knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-based methods in CIL create redundant parameters and fail to utilize shared knowledge across tasks.

Method: CL-LoRA combines task-shared adapters (using orthogonal matrices and knowledge distillation) and task-specific adapters (with learnable block-wise weights) to balance shared and unique features.

Result: CL-LoRA achieves strong performance on benchmarks with reduced computation, proving efficient for continual learning.

Conclusion: CL-LoRA offers a scalable and efficient paradigm for CIL with pre-trained models.

Abstract: Class-Incremental Learning (CIL) aims to learn new classes sequentially while
retaining the knowledge of previously learned classes. Recently, pre-trained
models (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown
remarkable performance in rehearsal-free CIL without requiring exemplars from
previous tasks. However, existing adapter-based methods, which incorporate
lightweight learnable modules into PTMs for CIL, create new adapters for each
new task, leading to both parameter redundancy and failure to leverage shared
knowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation
(CL-LoRA), which introduces a novel dual-adapter architecture combining
\textbf{task-shared adapters} to learn cross-task knowledge and
\textbf{task-specific adapters} to capture unique features of each new task.
Specifically, the shared adapters utilize random orthogonal matrices and
leverage knowledge distillation with gradient reassignment to preserve
essential shared knowledge. In addition, we introduce learnable block-wise
weights for task-specific adapters, which mitigate inter-task interference
while maintaining the model's plasticity. We demonstrate CL-LoRA consistently
achieves promising performance under multiple benchmarks with reduced training
and inference computation, establishing a more efficient and scalable paradigm
for continual learning with pre-trained models.

</details>


### [359] [Segmenting France Across Four Centuries](https://arxiv.org/pdf/2505.24824)
*Marta López-Rauhut, Hongyu Zhou, Mathieu Aubry, Loic Landrieu*

Main category: cs.CV

TL;DR: A new dataset of historical maps for France (18th-20th centuries) is introduced, enabling large-scale land use analysis with limited annotations. Three segmentation approaches are benchmarked, including weakly-supervised methods using modern labels.


<details>
  <summary>Details</summary>
Motivation: Historical maps provide unique insights into land evolution, but existing datasets are limited in scope and require costly annotations. This work addresses these gaps.

Method: A dataset of French historical maps (18th-20th centuries) is created, with modern and historical labels. Three segmentation models (fully-supervised and weakly-supervised) are tested.

Result: The dataset highlights segmentation challenges like stylistic inconsistencies and landscape changes. Weakly-supervised methods show promise despite these difficulties.

Conclusion: The dataset and methods support long-term environmental monitoring, offering insights into centuries of landscape transformation.

Abstract: Historical maps offer an invaluable perspective into territory evolution
across past centuries--long before satellite or remote sensing technologies
existed. Deep learning methods have shown promising results in segmenting
historical maps, but publicly available datasets typically focus on a single
map type or period, require extensive and costly annotations, and are not
suited for nationwide, long-term analyses. In this paper, we introduce a new
dataset of historical maps tailored for analyzing large-scale, long-term land
use and land cover evolution with limited annotations. Spanning metropolitan
France (548,305 km^2), our dataset contains three map collections from the
18th, 19th, and 20th centuries. We provide both comprehensive modern labels and
22,878 km^2 of manually annotated historical labels for the 18th and 19th
century maps. Our dataset illustrates the complexity of the segmentation task,
featuring stylistic inconsistencies, interpretive ambiguities, and significant
landscape changes (e.g., marshlands disappearing in favor of forests). We
assess the difficulty of these challenges by benchmarking three approaches: a
fully-supervised model trained with historical labels, and two
weakly-supervised models that rely only on modern annotations. The latter
either use the modern labels directly or first perform image-to-image
translation to address the stylistic gap between historical and contemporary
maps. Finally, we discuss how these methods can support long-term environment
monitoring, offering insights into centuries of landscape transformation. Our
official project repository is publicly available at
https://github.com/Archiel19/FRAx4.git.

</details>


### [360] [Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning](https://arxiv.org/pdf/2505.24837)
*Yinglian Zhu, Haiyang Yu, Qizao Wang, Wei Lu, Xiangyang Xue, Bin Li*

Main category: cs.CV

TL;DR: The paper introduces Hi-GITA, a hierarchical multi-granularity framework for Chinese Character Recognition, leveraging fine-grained semantic information and contrastive learning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese Character Recognition methods lack fine-grained semantic utilization and rely on single-level representations, limiting performance.

Method: Proposes Hi-GITA with multi-granularity encoders for images and text, fusion modules, and a decoupled contrastive loss to align modalities.

Result: Hi-GITA achieves a 20% accuracy improvement in zero-shot settings for handwritten characters and radicals.

Conclusion: The framework effectively leverages hierarchical semantic information, setting a new benchmark for zero-shot Chinese Character Recognition.

Abstract: Chinese Character Recognition (CCR) is a fundamental technology for
intelligent document processing. Unlike Latin characters, Chinese characters
exhibit unique spatial structures and compositional rules, allowing for the use
of fine-grained semantic information in representation. However, existing
approaches are usually based on auto-regressive as well as edit distance
post-process and typically rely on a single-level character representation. In
this paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning
(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant
fine-grained semantic information of Chinese characters, we propose
multi-granularity encoders on both image and text sides. Specifically, the
Image Multi-Granularity Encoder extracts hierarchical image representations
from character images, capturing semantic cues from localized strokes to
holistic structures. The Text Multi-Granularity Encoder extracts stroke and
radical sequence representations at different levels of granularity. To better
capture the relationships between strokes and radicals, we introduce
Multi-Granularity Fusion Modules on the image and text sides, respectively.
Furthermore, to effectively bridge the two modalities, we further introduce a
Fine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text
representations across multiple granularities. Extensive experiments
demonstrate that our proposed Hi-GITA significantly outperforms existing
zero-shot CCR methods. For instance, it brings about 20% accuracy improvement
in handwritten character and radical zero-shot settings. Code and models will
be released soon.

</details>


### [361] [VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software](https://arxiv.org/pdf/2505.24838)
*Brandon Man, Ghadi Nehme, Md Ferdous Alam, Faez Ahmed*

Main category: cs.CV

TL;DR: VideoCAD introduces a large-scale synthetic dataset for learning UI interactions in CAD, featuring high complexity and long-horizon tasks, and proposes a model (VideoCADFormer) that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Existing AI-driven UI datasets and methods focus on simple tasks, lacking the complexity and precision required for professional CAD tools.

Method: VideoCAD is a synthetic dataset of 41K annotated CAD operation videos, used to train VideoCADFormer for UI interaction learning and evaluate multimodal LLMs via a VQA benchmark.

Result: VideoCADFormer outperforms baselines, and the VQA benchmark highlights challenges in video-based UI understanding, such as spatial reasoning and long-horizon dependencies.

Conclusion: VideoCAD advances UI interaction learning for CAD, revealing key challenges and setting a foundation for future research in precision tasks.

Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process,
requiring precise, long-horizon user interactions with intricate 3D interfaces.
While recent advances in AI-driven user interface (UI) agents show promise,
most existing datasets and methods focus on short, low-complexity tasks in
mobile or web applications, failing to capture the demands of professional
engineering tools. In this work, we introduce VideoCAD, the first attempt at
engineering UI interaction learning for precision tasks. Specifically, VideoCAD
is a large-scale synthetic dataset consisting of over 41K annotated video
recordings of CAD operations, generated using an automated framework for
collecting high-fidelity UI action data from human-made CAD designs. Compared
to existing datasets, VideoCAD offers an order of magnitude higher complexity
in UI interaction learning for real-world engineering tasks, having up to a 20x
longer time horizon than other datasets. We show two important downstream
applications of VideoCAD: learning UI interactions from professional precision
3D CAD tools and a visual question-answering (VQA) benchmark designed to
evaluate multimodal large language models' (LLM) spatial reasoning and video
understanding abilities. To learn the UI interactions, we propose
VideoCADFormer - a state-of-the-art model in learning CAD interactions directly
from video, which outperforms multiple behavior cloning baselines. Both
VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key
challenges in the current state of video-based UI understanding, including the
need for precise action grounding, multi-modal and spatial reasoning, and
long-horizon dependencies.

</details>


### [362] [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/pdf/2505.24840)
*Yuwen Tan, Yuan Qing, Boqing Gong*

Main category: cs.CV

TL;DR: LLMs lack hierarchical visual knowledge, hindering vision LLMs' understanding. Finetuning with VQA tasks improves LLMs' hierarchical consistency more than vision LLMs'.


<details>
  <summary>Details</summary>
Motivation: To highlight the gap in hierarchical knowledge in LLMs and its impact on vision LLMs' performance.

Method: Used one million four-choice VQA tasks from six taxonomies and four image datasets to evaluate and finetune models.

Result: Finetuning LLMs with VQA tasks improved their hierarchical consistency more than vision LLMs'.

Conclusion: Hierarchical understanding in vision LLMs requires LLMs to first possess taxonomy knowledge.

Abstract: This paper reveals that many state-of-the-art large language models (LLMs)
lack hierarchical knowledge about our visual world, unaware of even
well-established biology taxonomies. This shortcoming makes LLMs a bottleneck
for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone
Fish but not Vertebrate). We arrive at these findings using about one million
four-choice visual question answering (VQA) tasks constructed from six
taxonomies and four image datasets. Interestingly, finetuning a vision LLM
using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because
the VQA tasks improve the LLM's hierarchical consistency more than the vision
LLM's. We conjecture that one cannot make vision LLMs understand visual
concepts fully hierarchical until LLMs possess corresponding taxonomy
knowledge.

</details>


### [363] [Reading Recognition in the Wild](https://arxiv.org/pdf/2505.24848)
*Charig Yang, Samiul Alam, Shakhrul Iman Siam, Michael J. Proulx, Lambert Mathias, Kiran Somasundaram, Luis Pesqueira, James Fort, Sheroze Sheriffdeen, Omkar Parkhi, Carl Ren, Mi Zhang, Yuning Chai, Richard Newcombe, Hyo Jin Kim*

Main category: cs.CV

TL;DR: The paper introduces a reading recognition task for smart glasses, presents a multimodal dataset (Reading in the Wild), and proposes a transformer model using RGB, gaze, and head pose data to detect reading activity.


<details>
  <summary>Details</summary>
Motivation: To enable contextual AI in smart glasses by tracking user interactions, especially reading, in real-world scenarios.

Method: A transformer model leverages multimodal data (RGB, eye gaze, head pose) to recognize reading activity, tested on a new large-scale dataset.

Result: The modalities are relevant and complementary, and the dataset aids in classifying reading types, extending studies to realistic settings.

Conclusion: The work advances reading recognition in smart glasses, with public release of code, model, and data.

Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism. Code, model, and
data will be public.

</details>


### [364] [ViStoryBench: Comprehensive Benchmark Suite for Story Visualization](https://arxiv.org/pdf/2505.24862)
*Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Zhewei Huang, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, Xuanyang Zhang, Xianfang Zeng, Gang Yu, Chi Zhang*

Main category: cs.CV

TL;DR: ViStoryBench is introduced as a comprehensive benchmark for evaluating story visualization models, featuring diverse story types, artistic styles, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of story visualization frameworks in real-world scenarios by providing a structured evaluation tool.

Method: Collecting a diverse dataset with varied story types, artistic styles, and complex plots, and designing multifaceted evaluation metrics.

Result: ViStoryBench enables thorough identification of model strengths and weaknesses, fostering targeted improvements.

Conclusion: ViStoryBench serves as a valuable tool for advancing story visualization research by ensuring comprehensive and balanced evaluations.

Abstract: Story visualization, which aims to generate a sequence of visually coherent
images aligning with a given narrative and reference images, has seen
significant progress with recent advancements in generative models. To further
enhance the performance of story visualization frameworks in real-world
scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We
collect a diverse dataset encompassing various story types and artistic styles,
ensuring models are evaluated across multiple dimensions such as different
plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D
renderings). ViStoryBench is carefully curated to balance narrative structures
and visual elements, featuring stories with single and multiple protagonists to
test models' ability to maintain character consistency. Additionally, it
includes complex plots and intricate world-building to challenge models in
generating accurate visuals. To ensure comprehensive comparisons, our benchmark
incorporates a wide range of evaluation metrics assessing critical aspects.
This structured and multifaceted framework enables researchers to thoroughly
identify both the strengths and weaknesses of different models, fostering
targeted improvements.

</details>


### [365] [TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection](https://arxiv.org/pdf/2505.24866)
*Xinqi Xiong, Prakrut Patel, Qingyuan Fan, Amisha Wadhwa, Sarathy Selvam, Xiao Guo, Luchao Qi, Xiaoming Liu, Roni Sengupta*

Main category: cs.CV

TL;DR: TalkingHeadBench is a new benchmark and dataset for evaluating deepfake talking-head detection, addressing gaps in current benchmarks by including advanced generators and assessing model robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for deepfake detection are outdated and lack insights into model robustness and generalization, necessitating a more comprehensive evaluation framework.

Method: The authors introduce TalkingHeadBench, a multi-model multi-generator benchmark with a curated dataset, evaluating detectors like CNNs, vision transformers, and temporal models. They also use Grad-CAM for error analysis.

Result: The benchmark assesses detector performance on advanced generators, revealing robustness and generalization issues, and identifies common failure modes and biases.

Conclusion: TalkingHeadBench aims to advance research in robust and generalizable deepfake detection to counter evolving generative techniques.

Abstract: The rapid advancement of talking-head deepfake generation fueled by advanced
generative models has elevated the realism of synthetic videos to a level that
poses substantial risks in domains such as media, politics, and finance.
However, current benchmarks for deepfake talking-head detection fail to reflect
this progress, relying on outdated generators and offering limited insight into
model robustness and generalization. We introduce TalkingHeadBench, a
comprehensive multi-model multi-generator benchmark and curated dataset
designed to evaluate the performance of state-of-the-art detectors on the most
advanced generators. Our dataset includes deepfakes synthesized by leading
academic and commercial models and features carefully constructed protocols to
assess generalization under distribution shifts in identity and generator
characteristics. We benchmark a diverse set of existing detection methods,
including CNNs, vision transformers, and temporal models, and analyze their
robustness and generalization capabilities. In addition, we provide error
analysis using Grad-CAM visualizations to expose common failure modes and
detector biases. TalkingHeadBench is hosted on
https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to
all data splits and protocols. Our benchmark aims to accelerate research
towards more robust and generalizable detection models in the face of rapidly
evolving generative techniques.

</details>


### [366] [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/pdf/2505.24867)
*Ujjwal Upadhyay, Mukul Ranjan, Zhiqiang Shen, Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: SpookyBench is a benchmark for testing temporal pattern recognition in VLMs, revealing their inability to process purely temporal cues compared to humans.


<details>
  <summary>Details</summary>
Motivation: Current VLMs excel in spatio-temporal understanding but fail when spatial information is obscured, highlighting a gap in temporal processing.

Method: Introduces SpookyBench, a dataset of noise-like frames encoding temporal sequences, and evaluates human and VLM performance.

Result: Humans achieve ~98% accuracy, while VLMs score 0%, showing their over-reliance on spatial features and poor temporal reasoning.

Conclusion: Novel architectures or training paradigms are needed to decouple spatial and temporal processing, with SpookyBench released to spur research.

Abstract: Recent advances in vision-language models (VLMs) have made impressive strides
in understanding spatio-temporal relationships in videos. However, when spatial
information is obscured, these models struggle to capture purely temporal
patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is
encoded solely in temporal sequences of noise-like frames, mirroring natural
phenomena from biological signaling to covert communication. Interestingly,
while humans can recognize shapes, text, and patterns in these sequences with
over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance
gap highlights a critical limitation: an over-reliance on frame-level spatial
features and an inability to extract meaning from temporal cues. Furthermore,
when trained in data sets with low spatial signal-to-noise ratios (SNR),
temporal understanding of models degrades more rapidly than human perception,
especially in tasks requiring fine-grained temporal reasoning. Overcoming this
limitation will require novel architectures or training paradigms that decouple
spatial dependencies from temporal processing. Our systematic analysis shows
that this issue persists across model scales and architectures. We release
SpookyBench to catalyze research in temporal pattern recognition and bridge the
gap between human and machine video understanding. Dataset and code has been
made available on our project website: https://timeblindness.github.io/.

</details>


### [367] [SiLVR: A Simple Language-based Video Reasoning Framework](https://arxiv.org/pdf/2505.24869)
*Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, Gedas Bertasius*

Main category: cs.CV

TL;DR: SiLVR is a framework for improving video reasoning in multimodal LLMs by decomposing tasks into language-based stages and using adaptive token reduction.


<details>
  <summary>Details</summary>
Motivation: Address the lag in reasoning capabilities of multimodal LLMs for complex video-language tasks.

Method: Decomposes video understanding into two stages: transforming raw video into language-based representations and feeding these into a reasoning LLM. Uses adaptive token reduction for long-context inputs.

Result: Achieves best-reported results on multiple benchmarks (Video-MME, Video-MMMU, etc.). Shows LLMs can effectively aggregate multisensory input for video reasoning.

Conclusion: SiLVR demonstrates that strong reasoning LLMs can handle complex video tasks without explicit video training, offering a modular, training-free solution.

Abstract: Recent advances in test-time optimization have led to remarkable reasoning
capabilities in Large Language Models (LLMs), enabling them to solve highly
complex problems in math and coding. However, the reasoning capabilities of
multimodal LLMs (MLLMs) still significantly lag, especially for complex
video-language tasks. To address this issue, we present SiLVR, a Simple
Language-based Video Reasoning framework that decomposes complex video
understanding into two stages. In the first stage, SiLVR transforms raw video
into language-based representations using multisensory inputs, such as short
clip captions and audio/speech subtitles. In the second stage, language
descriptions are fed into a powerful reasoning LLM to solve complex
video-language understanding tasks. To handle long-context multisensory inputs,
we use an adaptive token reduction scheme, which dynamically determines the
temporal granularity with which to sample the tokens. Our simple, modular, and
training-free video reasoning framework achieves the best-reported results on
Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.
Furthermore, our empirical study focused on video reasoning capabilities shows
that, despite not being explicitly trained on video, strong reasoning LLMs can
effectively aggregate multisensory input information from video, speech, and
audio for complex temporal, causal, long-context, and knowledge acquisition
reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.

</details>


### [368] [GenSpace: Benchmarking Spatially-Aware Image Generation](https://arxiv.org/pdf/2505.24870)
*Zehan Wang, Jiayang Xu, Ziang Zhang, Tianyu Pan, Chao Du, Hengshuang Zhao, Zhou Zhao*

Main category: cs.CV

TL;DR: GenSpace is a benchmark to evaluate 3D spatial awareness in AI image generators, revealing limitations in object placement, relationships, and measurements.


<details>
  <summary>Details</summary>
Motivation: Assess if AI image generators can plan scenes with human-like 3D spatial awareness, as current evaluations often miss detailed spatial errors.

Method: Proposes a specialized evaluation pipeline using multiple visual foundation models to reconstruct 3D scene geometry for accurate spatial faithfulness metrics.

Result: AI models produce visually appealing images but struggle with 3D details like object placement, relationships, and measurements.

Conclusion: Identifies three core limitations in spatial perception of AI models, suggesting directions for improving spatial intelligence in image generation.

Abstract: Humans can intuitively compose and arrange scenes in the 3D space for
photography. However, can advanced AI image generators plan scenes with similar
3D spatial awareness when creating images from text or image prompts? We
present GenSpace, a novel benchmark and evaluation pipeline to comprehensively
assess the spatial awareness of current image generation models. Furthermore,
standard evaluations using general Vision-Language Models (VLMs) frequently
fail to capture the detailed spatial errors. To handle this challenge, we
propose a specialized evaluation pipeline and metric, which reconstructs 3D
scene geometry using multiple visual foundation models and provides a more
accurate and human-aligned metric of spatial faithfulness. Our findings show
that while AI models create visually appealing images and can follow general
instructions, they struggle with specific 3D details like object placement,
relationships, and measurements. We summarize three core limitations in the
spatial perception of current state-of-the-art image generation models: 1)
Object Perspective Understanding, 2) Egocentric-Allocentric Transformation and
3) Metric Measurement Adherence, highlighting possible directions for improving
spatial intelligence in image generation.

</details>


### [369] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/pdf/2505.24871)
*Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu*

Main category: cs.CV

TL;DR: RLVR for MLLMs improves reasoning via optimized dataset mixtures, boosting accuracy by 5.24% over uniform mixtures and 20.74% over baselines.


<details>
  <summary>Details</summary>
Motivation: Extend RLVR to MLLMs for vision-language tasks, addressing challenges of conflicting objectives from diverse datasets.

Method: Developed a multimodal RLVR framework with verifiable rewards and a data mixture strategy predicting RL outcomes.

Result: Multi-domain RLVR with mixture prediction improves MLLM reasoning, with a 5.24% accuracy boost over uniform mixtures.

Conclusion: Optimal dataset mixtures in RLVR training enhance MLLM generalization and reasoning, outperforming baselines significantly.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


### [370] [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/pdf/2505.24872)
*Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, Vicente Ordonez*

Main category: cs.CV

TL;DR: ProxyThinker is an inference-time technique that allows large vision-language models (LVLMs) to inherit visual reasoning capabilities from smaller models without training, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Training LVLMs with reinforcement fine-tuning (RFT) is computationally expensive, limiting scalability. ProxyThinker addresses this by enabling capabilities transfer without training.

Method: ProxyThinker modifies decoding dynamics by subtracting base model outputs from RFT reasoner outputs, eliciting slow-thinking behaviors like self-verification.

Result: ProxyThinker boosts performance on visual benchmarks (spatial, mathematical, multi-disciplinary reasoning) and achieves 38× faster inference than previous methods.

Conclusion: ProxyThinker offers a practical, efficient solution for deploying advanced visual reasoning in LVLMs without costly training.

Abstract: Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.

</details>


### [371] [MiniMax-Remover: Taming Bad Noise Helps Video Object Removal](https://arxiv.org/pdf/2505.24873)
*Bojia Zi, Weixuan Peng, Xianbiao Qi, Jianan Wang, Shihao Zhao, Rong Xiao, Kam-Fai Wong*

Main category: cs.CV

TL;DR: MiniMax-Remover is a two-stage video object removal method that simplifies model architecture and uses minimax optimization for efficiency and quality, achieving state-of-the-art results with fewer sampling steps.


<details>
  <summary>Details</summary>
Motivation: Existing video object removal methods suffer from issues like hallucinations, artifacts, and slow inference due to reliance on text conditions and CFG.

Method: 1) Simplify pretrained video model by removing text input and cross-attention layers. 2) Distill remover using minimax optimization (inner maximization for adversarial noise, outer minimization for robust training).

Result: Achieves top-tier video object removal with just 6 sampling steps, no CFG, and improved efficiency.

Conclusion: MiniMax-Remover outperforms existing methods in quality and speed, validated by extensive experiments.

Abstract: Recent advances in video diffusion models have driven rapid progress in video
editing techniques. However, video object removal, a critical subtask of video
editing, remains challenging due to issues such as hallucinated objects and
visual artifacts. Furthermore, existing methods often rely on computationally
expensive sampling procedures and classifier-free guidance (CFG), resulting in
slow inference. To address these limitations, we propose MiniMax-Remover, a
novel two-stage video object removal approach. Motivated by the observation
that text condition is not best suited for this task, we simplify the
pretrained video generation model by removing textual input and cross-attention
layers, resulting in a more lightweight and efficient model architecture in the
first stage. In the second stage, we distilled our remover on successful videos
produced by the stage-1 model and curated by human annotators, using a minimax
optimization strategy to further improve editing quality and inference speed.
Specifically, the inner maximization identifies adversarial input noise ("bad
noise") that makes failure removals, while the outer minimization step trains
the model to generate high-quality removal results even under such challenging
conditions. As a result, our method achieves a state-of-the-art video object
removal results with as few as 6 sampling steps and doesn't rely on CFG,
significantly improving inference efficiency. Extensive experiments demonstrate
the effectiveness and superiority of MiniMax-Remover compared to existing
methods. Codes and Videos are available at: https://minimax-remover.github.io.

</details>


### [372] [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/pdf/2505.24875)
*Yu Zhang, Yunqi Li, Yifan Yang, Rui Wang, Yuqing Yang, Dai Qi, Jianmin Bao, Dongdong Chen, Chong Luo, Lili Qiu*

Main category: cs.CV

TL;DR: ReasonGen-R1 integrates chain-of-thought reasoning and RL into generative vision models, outperforming baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored integration of chain-of-thought reasoning and RL in generative vision models for improved performance.

Method: A two-stage framework: supervised fine-tuning with a reasoning dataset, followed by output refinement using Group Relative Policy Optimization (GRPO).

Result: ReasonGen-R1 outperforms baselines and prior state-of-the-art models on GenEval, DPG, and T2I benchmarks.

Conclusion: The integration of reasoning and RL enhances generative vision models, as demonstrated by ReasonGen-R1's superior performance.

Abstract: Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.

</details>


### [373] [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/pdf/2505.24876)
*Tajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair Nawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan*

Main category: cs.CV

TL;DR: Agent-X is a large-scale benchmark for evaluating vision-centric agents' multi-step reasoning in real-world, multimodal settings, revealing limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world, multimodal, and multi-step reasoning evaluation, limiting progress in vision-centric agent capabilities.

Method: Agent-X introduces 828 tasks across six environments, requiring stepwise decision-making and tool use, with a fine-grained evaluation framework.

Result: Top models (GPT, Gemini, Qwen) achieve <50% success in multi-step vision tasks, exposing reasoning and tool-use bottlenecks.

Conclusion: Agent-X highlights critical gaps in current models and guides future research in vision-centric agentic reasoning.

Abstract: Deep reasoning is fundamental for solving complex tasks, especially in
vision-centric scenarios that demand sequential, multimodal understanding.
However, existing benchmarks typically evaluate agents with fully synthetic,
single-turn queries, limited visual modalities, and lack a framework to assess
reasoning quality over multiple steps as required in real-world settings. To
address this, we introduce Agent-X, a large-scale benchmark for evaluating
vision-centric agents multi-step and deep reasoning capabilities in real-world,
multimodal settings. Agent- X features 828 agentic tasks with authentic visual
contexts, including images, multi-image comparisons, videos, and instructional
text. These tasks span six major agentic environments: general visual
reasoning, web browsing, security and surveillance, autonomous driving, sports,
and math reasoning. Our benchmark requires agents to integrate tool use with
explicit, stepwise decision-making in these diverse settings. In addition, we
propose a fine-grained, step-level evaluation framework that assesses the
correctness and logical coherence of each reasoning step and the effectiveness
of tool usage throughout the task. Our results reveal that even the
best-performing models, including GPT, Gemini, and Qwen families, struggle to
solve multi-step vision tasks, achieving less than 50% full-chain success.
These findings highlight key bottlenecks in current LMM reasoning and tool-use
capabilities and identify future research directions in vision-centric agentic
reasoning models. Our data and code are publicly available at
https://github.com/mbzuai-oryx/Agent-X

</details>


### [374] [AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion](https://arxiv.org/pdf/2505.24877)
*Yangyi Huang, Ye Yuan, Xueting Li, Jan Kautz, Umar Iqbal*

Main category: cs.CV

TL;DR: AdaHuman is a new framework for generating high-fidelity, animatable 3D avatars from a single image, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to produce detailed, animation-ready 3D avatars from in-the-wild images.

Method: AdaHuman uses a pose-conditioned 3D joint diffusion model and a compositional 3DGS refinement module to create detailed avatars.

Result: It generates realistic A-pose avatars with minimal self-occlusion, excelling in reconstruction and reposing.

Conclusion: AdaHuman advances avatar generation, offering practical solutions for animation and rigging.

Abstract: Existing methods for image-to-3D avatar generation struggle to produce highly
detailed, animation-ready avatars suitable for real-world applications. We
introduce AdaHuman, a novel framework that generates high-fidelity animatable
3D avatars from a single in-the-wild image. AdaHuman incorporates two key
innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes
consistent multi-view images in arbitrary poses alongside corresponding 3D
Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A
compositional 3DGS refinement module that enhances the details of local body
parts through image-to-image refinement and seamlessly integrates them using a
novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These
components allow AdaHuman to generate highly realistic standardized A-pose
avatars with minimal self-occlusion, enabling rigging and animation with any
input motion. Extensive evaluation on public benchmarks and in-the-wild images
demonstrates that AdaHuman significantly outperforms state-of-the-art methods
in both avatar reconstruction and reposing. Code and models will be publicly
available for research purposes.

</details>


### [375] [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation](https://arxiv.org/pdf/2309.15818)
*David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou*

Main category: cs.CV

TL;DR: Show-1 is a hybrid text-to-video diffusion model combining pixel-based and latent-based VDMs for efficient, high-quality video generation with precise text alignment.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing pixel-based (high computational cost) and latent-based (poor text-video alignment) VDMs by proposing a hybrid approach.

Method: Uses pixel-based VDMs for low-resolution video with strong text alignment, then employs latent-based VDMs for upsampling and artifact removal.

Result: Show-1 outperforms latent VDMs in alignment and pixel VDMs in efficiency (15G vs 72G GPU memory). Supports motion customization and video stylization.

Conclusion: Show-1 achieves state-of-the-art performance, offering a balanced solution for text-to-video generation with efficiency and quality.

Abstract: Significant advancements have been achieved in the realm of large-scale
pre-trained text-to-video Diffusion Models (VDMs). However, previous methods
either rely solely on pixel-based VDMs, which come with high computational
costs, or on latent-based VDMs, which often struggle with precise text-video
alignment. In this paper, we are the first to propose a hybrid model, dubbed as
Show-1, which marries pixel-based and latent-based VDMs for text-to-video
generation. Our model first uses pixel-based VDMs to produce a low-resolution
video of strong text-video correlation. After that, we propose a novel expert
translation method that employs the latent-based VDMs to further upsample the
low-resolution video to high resolution, which can also remove potential
artifacts and corruptions from low-resolution videos. Compared to latent VDMs,
Show-1 can produce high-quality videos of precise text-video alignment;
Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during
inference is 15G vs 72G). Furthermore, our Show-1 model can be readily adapted
for motion customization and video stylization applications through simple
temporal attention layer finetuning. Our model achieves state-of-the-art
performance on standard video generation benchmarks. Our code and model weights
are publicly available at https://github.com/showlab/Show-1.

</details>


### [376] [Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models](https://arxiv.org/pdf/2312.02219)
*Andrés Villa, Juan Carlos León Alcázar, Alvaro Soto, Bernard Ghanem*

Main category: cs.CV

TL;DR: The paper introduces MERLIM, a benchmark to evaluate IT-LVLMs on fundamental computer vision tasks, revealing limitations like fine-grained concept identification and hidden hallucinations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized evaluation for IT-LVLMs in computer vision tasks and assess their capabilities and limitations.

Method: Developed MERLIM, a scalable benchmark with 300K image-question pairs, focusing on detecting cross-modal hallucinations.

Result: IT-LVLMs show weak visual grounding, biases toward language queries, and hidden hallucinations (correct answers without visual grounding).

Conclusion: MERLIM highlights IT-LVLMs' limitations and provides insights for improving their performance in fundamental vision tasks.

Abstract: Large Vision and Language Models have enabled significant advances in fully
supervised and zero-shot visual tasks. These large architectures serve as the
baseline to what is currently known as Instruction Tuning Large Vision and
Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants
whose responses are modulated by natural language instructions and visual data.
Despite this versatility, IT-LVLM effectiveness in fundamental computer vision
problems remains unclear, primarily due to the absence of a standardized
evaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark
named MERLIM, a scalable test-bed to assess the capabilities of IT-LVLMs on
fundamental computer vision tasks. MERLIM contains over 300K image-question
pairs and has a strong focus on detecting cross-modal "hallucination" events in
IT-LVLMs. Our results bring important insights on the performance of
state-of-the-art IT-LVLMs including limitations at identifying fine-grained
visual concepts, object hallucinations across tasks, and biases towards the
language query. Our findings also suggest that these models have weak visual
grounding, but manage to make adequate guesses from global visual patterns or
language biases contained in the LLM component. We name this phenomenon of
correct answers with no visual grounding as hidden hallucinations.

</details>


### [377] [GPT4Point: A Unified Framework for Point-Language Understanding and Generation](https://arxiv.org/pdf/2312.02980)
*Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao*

Main category: cs.CV

TL;DR: GPT4Point is a 3D multimodal model addressing the gap in 3D understanding and generation within MLLMs, excelling in tasks like point-cloud captioning and controllable 3D generation.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack 3D world comprehension, hindering progress in 3D language tasks.

Method: Introduces GPT4Point, a point-language model, and Pyramid-XL, a dataset annotation engine for training.

Result: GPT4Point shows superior performance in 3D understanding and generation tasks.

Conclusion: GPT4Point advances 3D multimodal capabilities, supported by a large-scale dataset and comprehensive benchmarks.

Abstract: Multimodal Large Language Models (MLLMs) have excelled in 2D image-text
comprehension and image generation, but their understanding of the 3D world is
notably deficient, limiting progress in 3D language understanding and
generation. To solve this problem, we introduce GPT4Point, an innovative
groundbreaking point-language multimodal model designed specifically for
unified 3D object understanding and generation within the MLLM framework.
GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text
reference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point
is equipped with advanced capabilities for controllable 3D generation, it can
get high-quality results through a low-quality point-text feature maintaining
the geometric shapes and colors. To support the expansive needs of 3D
object-text pairs, we develop Pyramid-XL, a point-language dataset annotation
engine. It constructs a large-scale database over 1M objects of varied text
granularity levels from the Objaverse-XL dataset, essential for training
GPT4Point. A comprehensive benchmark has been proposed to evaluate 3D
point-language understanding capabilities. In extensive evaluations, GPT4Point
has demonstrated superior performance in understanding and generation.

</details>


### [378] [SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness for Hyperspectral Object Tracking](https://arxiv.org/pdf/2403.05852)
*Hanzheng Wang, Wei Li, Xiang-Gen Xia, Qian Du, Jing Tian*

Main category: cs.CV

TL;DR: The paper proposes SST-Net, a spatial-spectral fusion network for hyperspectral object tracking, addressing limitations in spectral feature extraction and modality fusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient spectral feature exploration and struggle with complementary feature representation in hyperspectral tracking.

Method: The paper introduces a spatial-spectral feature backbone, spectral attention fusion module, and spectral angle awareness module for robust feature fusion and tracking.

Result: Experiments on the HOTC dataset show SST-Net outperforms state-of-the-art trackers.

Conclusion: SST-Net effectively leverages spectral and spatial information for improved hyperspectral object tracking.

Abstract: Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal
information simultaneously, making it highly suitable for handling challenges
such as background clutter and visual similarity in object tracking. However,
existing methods primarily focus on band regrouping and rely on RGB trackers
for feature extraction, resulting in limited exploration of spectral
information and difficulties in achieving complementary representations of
object features. In this paper, a spatial-spectral fusion network with spectral
angle awareness (SST-Net) is proposed for hyperspectral (HS) object tracking.
Firstly, to address the issue of insufficient spectral feature extraction in
existing networks, a spatial-spectral feature backbone ($S^2$FB) is designed.
With the spatial and spectral extraction branch, a joint representation of
texture and spectrum is obtained. Secondly, a spectral attention fusion module
(SAFM) is presented to capture the intra- and inter-modality correlation to
obtain the fused features from the HS and RGB modalities. It can incorporate
the visual information into the HS spectral context to form a robust
representation. Thirdly, to ensure a more accurate response of the tracker to
the object position, a spectral angle awareness module (SAAM) investigates the
region-level spectral similarity between the template and search images during
the prediction stage. Furthermore, we develop a novel spectral angle awareness
loss (SAAL) to offer guidance for the SAAM based on similar regions. Finally,
to obtain the robust tracking results, a weighted prediction method is
considered to combine the HS and RGB predicted motions of objects to leverage
the strengths of each modality. Extensive experiments on the HOTC dataset
demonstrate the effectiveness of the proposed SSF-Net, compared with
state-of-the-art trackers.

</details>


### [379] [TheaterGen: Character Management with LLM for Consistent Multi-turn Image Generation](https://arxiv.org/pdf/2404.18919)
*Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang*

Main category: cs.CV

TL;DR: TheaterGen is a training-free framework combining LLMs and T2I models for multi-turn image generation, improving semantic and contextual consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in maintaining consistency in multi-turn image generation.

Method: Uses LLMs as a 'Screenwriter' to manage prompts and layouts, then integrates guidance into T2I models for final image generation.

Result: Outperforms state-of-the-art methods, improving similarity metrics by 19-21%.

Conclusion: TheaterGen effectively enhances consistency in multi-turn image generation and sets a new benchmark.

Abstract: Recent advances in diffusion models can generate high-quality and stunning
images from text. However, multi-turn image generation, which is of high demand
in real-world scenarios, still faces challenges in maintaining semantic
consistency between images and texts, as well as contextual consistency of the
same subject across multiple interactive turns. To address this issue, we
introduce TheaterGen, a training-free framework that integrates large language
models (LLMs) and text-to-image (T2I) models to provide the capability of
multi-turn image generation. Within this framework, LLMs, acting as a
"Screenwriter", engage in multi-turn interaction, generating and managing a
standardized prompt book that encompasses prompts and layout designs for each
character in the target image. Based on these, Theatergen generate a list of
character images and extract guidance information, akin to the "Rehearsal".
Subsequently, through incorporating the prompt book and guidance information
into the reverse denoising process of T2I diffusion models, Theatergen generate
the final image, as conducting the "Final Performance". With the effective
management of prompt books and character images, TheaterGen significantly
improves semantic and contextual consistency in synthesized images.
Furthermore, we introduce a dedicated benchmark, CMIGBench (Consistent
Multi-turn Image Generation Benchmark) with 8000 multi-turn instructions.
Different from previous multi-turn benchmarks, CMIGBench does not define
characters in advance. Both the tasks of story generation and multi-turn
editing are included on CMIGBench for comprehensive evaluation. Extensive
experimental results show that TheaterGen outperforms state-of-the-art methods
significantly. It raises the performance bar of the cutting-edge Mini DALLE 3
model by 21% in average character-character similarity and 19% in average
text-image similarity.

</details>


### [380] [Mamba-R: Vision Mamba ALSO Needs Registers](https://arxiv.org/pdf/2405.14858)
*Feng Wang, Jiahao Wang, Sucheng Ren, Guoyizhe Wei, Jieru Mei, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie*

Main category: cs.CV

TL;DR: The paper introduces Mamba-R, a modified Vision Mamba architecture with register tokens to mitigate artifacts in feature maps, achieving better performance and scalability.


<details>
  <summary>Details</summary>
Motivation: To address severe artifacts in Vision Mamba's feature maps, particularly high-norm tokens in low-information areas, which degrade model performance.

Method: Introduces register tokens evenly in the input sequence and recycles them for predictions, adapting to Mamba's uni-directional inference.

Result: Mamba-R outperforms vanilla Vision Mamba (83.0% vs. 81.8% on ImageNet) and scales successfully to larger models (83.6% accuracy).

Conclusion: Mamba-R effectively mitigates artifacts, improves feature map quality, and enhances performance and scalability, validated on ImageNet and semantic segmentation.

Abstract: Similar to Vision Transformers, this paper identifies artifacts also present
within the feature maps of Vision Mamba. These artifacts, corresponding to
high-norm tokens emerging in low-information background areas of images, appear
much more severe in Vision Mamba -- they exist prevalently even with the
tiny-sized model and activate extensively across background regions. To
mitigate this issue, we follow the prior solution of introducing register
tokens into Vision Mamba. To better cope with Mamba blocks' uni-directional
inference paradigm, two key modifications are introduced: 1) evenly inserting
registers throughout the input token sequence, and 2) recycling registers for
final decision predictions. We term this new architecture Mamba-R. Qualitative
observations suggest, compared to vanilla Vision Mamba, Mamba-R's feature maps
appear cleaner and more focused on semantically meaningful regions.
Quantitatively, Mamba-R attains stronger performance and scales better. For
example, on the ImageNet benchmark, our base-size Mamba-R attains 83.0%
accuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide
the first successful scaling to the large model size (i.e., with 341M
parameters), attaining a competitive accuracy of 83.6% (84.5% if finetuned with
384x384 inputs). Additional validation on the downstream semantic segmentation
task also supports Mamba-R's efficacy. Code is available at
https://github.com/wangf3014/Mamba-Reg.

</details>


### [381] [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/pdf/2405.21075)
*Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, Xing Sun*

Main category: cs.CV

TL;DR: Video-MME is the first comprehensive benchmark for evaluating Multi-modal Large Language Models (MLLMs) in video analysis, featuring diverse video types, durations, multi-modal inputs, and high-quality annotations. It reveals Gemini 1.5 Pro as the top-performing model, highlighting gaps in handling long sequences and multi-modal data.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM benchmarks focus on static images, leaving sequential visual data under-explored. Video-MME addresses this gap by providing a robust evaluation framework for video analysis.

Method: Video-MME introduces a benchmark with 900 videos (254 hours) across 6 domains, annotated with 2,700 QA pairs. It evaluates MLLMs like GPT-4, Gemini 1.5 Pro, and open-source models (InternVL-Chat-V1.5, LLaVA-NeXT-Video).

Result: Gemini 1.5 Pro outperforms other models, including open-source ones. The benchmark exposes limitations in processing long sequences and multi-modal data.

Conclusion: Video-MME sets a new standard for MLLM evaluation in video analysis, emphasizing the need for advancements in handling complex, multi-modal sequential data.

Abstract: In the quest for artificial general intelligence, Multi-modal Large Language
Models (MLLMs) have emerged as a focal point in recent advancements. However,
the predominant focus remains on developing their capabilities in static image
understanding. The potential of MLLMs in processing sequential visual data is
still insufficiently explored, highlighting the absence of a comprehensive,
high-quality assessment of their performance. In this paper, we introduce
Video-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of
MLLMs in Video analysis. Our work distinguishes from existing benchmarks
through four key features: 1) Diversity in video types, spanning 6 primary
visual domains with 30 subfields to ensure broad scenario generalizability; 2)
Duration in temporal dimension, encompassing both short-, medium-, and
long-term videos, ranging from 11 seconds to 1 hour, for robust contextual
dynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides
video frames, including subtitles and audios, to unveil the all-round
capabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual
labeling by expert annotators to facilitate precise and reliable model
assessment. 900 videos with a total of 254 hours are manually selected and
annotated by repeatedly viewing all the video content, resulting in 2,700
question-answer pairs. With Video-MME, we extensively evaluate various
state-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as
open-source image models like InternVL-Chat-V1.5 and video models like
LLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the
best-performing commercial model, significantly outperforming the open-source
models. Our dataset along with these findings underscores the need for further
improvements in handling longer sequences and multi-modal data. Project Page:
https://video-mme.github.io

</details>


### [382] [AutoStudio: Crafting Consistent Subjects in Multi-turn Interactive Image Generation](https://arxiv.org/pdf/2406.01388)
*Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, Xiaodan Liang*

Main category: cs.CV

TL;DR: AutoStudio is a training-free multi-agent framework for multi-turn interactive image generation, addressing subject consistency and diversity using LLMs and Stable Diffusion.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with maintaining subject consistency in multi-turn interactive image generation due to frequent subject switches by users.

Method: AutoStudio employs four agents: a subject manager, layout generator, supervisor, and drawer, along with a Parallel-UNet and subject-initialized generation method.

Result: AutoStudio improves multi-subject consistency and raises SOTA performance by 13.65% in FID and 2.83% in character-character similarity.

Conclusion: AutoStudio effectively addresses the challenges of multi-turn interactive image generation, achieving better consistency and performance.

Abstract: As cutting-edge Text-to-Image (T2I) generation models already excel at
producing remarkable single images, an even more challenging task, i.e.,
multi-turn interactive image generation begins to attract the attention of
related research communities. This task requires models to interact with users
over multiple turns to generate a coherent sequence of images. However, since
users may switch subjects frequently, current efforts struggle to maintain
subject consistency while generating diverse images. To address this issue, we
introduce a training-free multi-agent framework called AutoStudio. AutoStudio
employs three agents based on large language models (LLMs) to handle
interactions, along with a stable diffusion (SD) based agent for generating
high-quality images. Specifically, AutoStudio consists of (i) a subject manager
to interpret interaction dialogues and manage the context of each subject, (ii)
a layout generator to generate fine-grained bounding boxes to control subject
locations, (iii) a supervisor to provide suggestions for layout refinements,
and (iv) a drawer to complete image generation. Furthermore, we introduce a
Parallel-UNet to replace the original UNet in the drawer, which employs two
parallel cross-attention modules for exploiting subject-aware features. We also
introduce a subject-initialized generation method to better preserve small
subjects. Our AutoStudio hereby can generate a sequence of multi-subject images
interactively and consistently. Extensive experiments on the public CMIGBench
benchmark and human evaluations show that AutoStudio maintains multi-subject
consistency across multiple turns well, and it also raises the state-of-the-art
performance by 13.65% in average Frechet Inception Distance and 2.83% in
average character-character similarity.

</details>


### [383] [ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts](https://arxiv.org/pdf/2406.10973)
*Samar Khanna, Medhanie Irgau, David B. Lobell, Stefano Ermon*

Main category: cs.CV

TL;DR: ExPLoRA enhances transfer learning for vision transformers (ViTs) under domain shifts by combining unsupervised pre-training with LoRA, achieving state-of-the-art results with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: To explore efficient self-supervised pre-training for domain adaptation without labels, improving transfer learning for ViTs.

Method: ExPLoRA unfreezes 1-2 pre-trained ViT blocks and uses LoRA for tuning other layers, followed by supervised fine-tuning with LoRA.

Result: Achieves up to 8% better linear probing accuracy with <10% of parameters compared to fully-tuned methods.

Conclusion: ExPLoRA is a highly effective, parameter-efficient technique for domain adaptation, outperforming full fine-tuning.

Abstract: Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation
(LoRA) can effectively adapt large pre-trained foundation models to downstream
tasks using only a small fraction (0.1%-10%) of the original trainable weights.
An under-explored question of PEFT is in extending the pre-training phase
without supervised labels; that is, can we adapt a pre-trained foundation model
to a new domain via efficient self-supervised pre-training on this domain? In
this work, we introduce ExPLoRA, a highly effective technique to improve
transfer learning of pre-trained vision transformers (ViTs) under domain
shifts. Initializing a ViT with pre-trained weights on large, natural-image
datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised
pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks
and tuning all other layers with LoRA. We then fine-tune the resulting model
only with LoRA on this new domain for supervised learning. Our experiments
demonstrate state-of-the-art results on satellite imagery, even outperforming
fully pre-training and fine-tuning ViTs. Using the DinoV2 training objective,
we demonstrate up to 8% improvement in linear probing top-1 accuracy on
downstream tasks while using <10% of the number of parameters that are used in
prior fully-tuned state-of-the art approaches. Our ablation studies confirm the
efficacy of our approach over other baselines such as PEFT. Code is available
on the project website: https://samar-khanna.github.io/ExPLoRA/

</details>


### [384] [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/pdf/2408.05211)
*Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Yuhang Dai, Meng Zhao, Yi-Fan Zhang, Shaoqi Dong, Yangze Li, Xiong Wang, Haoyu Cao, Di Yin, Long Ma, Xiawu Zheng, Rongrong Ji, Yunsheng Wu, Ran He, Caifeng Shan, Xing Sun*

Main category: cs.CV

TL;DR: VITA is the first open-source Multimodal Large Language Model (MLLM) capable of processing Video, Image, Text, and Audio, with advanced interactive features, built on Mixtral 8x7B and enhanced through bilingual tuning and multimodal training.


<details>
  <summary>Details</summary>
Motivation: Open-source models often lack strong multimodal capabilities and interactive experiences, which VITA aims to address by integrating multiple modalities seamlessly.

Method: VITA expands Mixtral 8x7B's Chinese vocabulary, undergoes bilingual instruction tuning, and adds visual/audio capabilities via two-stage multimodal alignment and instruction tuning.

Result: VITA excels in multilingual, vision, and audio understanding, performing well on unimodal and multimodal benchmarks, and improves human-computer interaction.

Conclusion: VITA pioneers open-source multimodal integration, serving as a foundation for future research, though further work is needed to match closed-source models.

Abstract: The remarkable multimodal capabilities and interactive experience of GPT-4o
underscore their necessity in practical applications, yet open-source models
rarely excel in both areas. In this paper, we introduce VITA, the first-ever
open-source Multimodal Large Language Model (MLLM) adept at simultaneous
processing and analysis of Video, Image, Text, and Audio modalities, and
meanwhile has an advanced multimodal interactive experience. Starting from
Mixtral 8x7B as a language foundation, we expand its Chinese vocabulary
followed by bilingual instruction tuning. We further endow the language model
with visual and audio capabilities through two-stage multi-task learning of
multimodal alignment and instruction tuning. VITA demonstrates robust
foundational capabilities of multilingual, vision, and audio understanding, as
evidenced by its strong performance across a range of both unimodal and
multimodal benchmarks. Beyond foundational capabilities, we have made
considerable progress in enhancing the natural multimodal human-computer
interaction experience. VITA is the first step for the open-source community to
explore the seamless integration of multimodal understanding and interaction.
While there is still lots of work to be done on VITA to get close to
close-source counterparts, we hope that its role as a pioneer can serve as a
cornerstone for subsequent research. Project Page: https://vita-home.github.io.

</details>


### [385] [Graph-guided Cross-composition Feature Disentanglement for Compositional Zero-shot Learning](https://arxiv.org/pdf/2408.09786)
*Yuxia Geng, Runkai Zhu, Jiaoyan Chen, Jintai Chen, Xiang Chen, Zhuo Chen, Shuofei Qiao, Yuxiang Wang, Xiaoliang Xu, Sheng-Jun Huang*

Main category: cs.CV

TL;DR: The paper proposes cross-composition feature disentanglement to improve generalizability of disentangled primitive features in Compositional Zero-shot Learning (CZSL), leveraging a compositional graph and adapters in CLIP.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of feature divergence in attributes/objects when combined differently, aiming for generalizable disentangled features.

Method: Uses a compositional graph and dual cross-composition disentangling adapters (L-Adapter and V-Adapter) in CLIP's frozen encoders.

Result: Significant performance improvement on CZSL benchmarks, validated by ablation studies.

Conclusion: The proposed solution effectively enhances CZSL performance by ensuring feature generalizability across compositions.

Abstract: Disentanglement of visual features of primitives (i.e., attributes and
objects) has shown exceptional results in Compositional Zero-shot Learning
(CZSL). However, due to the feature divergence of an attribute (resp. object)
when combined with different objects (resp. attributes), it is challenging to
learn disentangled primitive features that are general across different
compositions. To this end, we propose the solution of cross-composition feature
disentanglement, which takes multiple primitive-sharing compositions as inputs
and constrains the disentangled primitive features to be general across these
compositions. More specifically, we leverage a compositional graph to define
the overall primitive-sharing relationships between compositions, and build a
task-specific architecture upon the recently successful large pre-trained
vision-language model (VLM) CLIP, with dual cross-composition disentangling
adapters (called L-Adapter and V-Adapter) inserted into CLIP's frozen text and
image encoders, respectively. Evaluation on three popular CZSL benchmarks shows
that our proposed solution significantly improves the performance of CZSL, and
its components have been verified by solid ablation studies. Our code and data
are available at:https://github.com/zhurunkai/DCDA.

</details>


### [386] [Camouflaged Object Tracking: A Benchmark](https://arxiv.org/pdf/2408.13877)
*Xiaoyu Guo, Pengzhi Zhong, Hao Zhang, Defeng Huang, Huikai Shao, Qijun Zhao, Shuiwang Li*

Main category: cs.CV

TL;DR: The paper introduces the Camouflaged Object Tracking Dataset (COTD) and a novel framework, HiPTrack-MLS, to address the challenge of tracking camouflaged objects, where existing methods underperform.


<details>
  <summary>Details</summary>
Motivation: Tracking camouflaged objects is critical in fields like military and security but remains understudied. Existing datasets and methods are inadequate for this challenge.

Method: The authors create COTD, a dataset with 200 sequences and 80,000 annotated frames, and propose HiPTrack-MLS, a new tracking framework.

Result: Evaluation of 20 existing algorithms shows poor performance on camouflaged objects, while HiPTrack-MLS demonstrates improved tracking accuracy.

Conclusion: The COTD dataset and HiPTrack-MLS framework advance research in camouflaged object tracking, addressing a significant gap in the field.

Abstract: Visual tracking has seen remarkable advancements, largely driven by the
availability of large-scale training datasets that have enabled the development
of highly accurate and robust algorithms. While significant progress has been
made in tracking general objects, research on more challenging scenarios, such
as tracking camouflaged objects, remains limited. Camouflaged objects, which
blend seamlessly with their surroundings or other objects, present unique
challenges for detection and tracking in complex environments. This challenge
is particularly critical in applications such as military, security,
agriculture, and marine monitoring, where precise tracking of camouflaged
objects is essential. To address this gap, we introduce the Camouflaged Object
Tracking Dataset (COTD), a specialized benchmark designed specifically for
evaluating camouflaged object tracking methods. The COTD dataset comprises 200
sequences and approximately 80,000 frames, each annotated with detailed
bounding boxes. Our evaluation of 20 existing tracking algorithms reveals
significant deficiencies in their performance with camouflaged objects. To
address these issues, we propose a novel tracking framework, HiPTrack-MLS,
which demonstrates promising results in improving tracking performance for
camouflaged objects. COTD and code are avialable at
https://github.com/openat25/HIPTrack-MLS.

</details>


### [387] [ENACT: Entropy-based Clustering of Attention Input for Reducing the Computational Needs of Object Detection Transformers](https://arxiv.org/pdf/2409.07541)
*Giorgos Savathrakis, Antonis Argyros*

Main category: cs.CV

TL;DR: ENACT reduces GPU usage in vision-based object detection transformers by clustering attention weights based on entropy, maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Transformers for object detection are computationally expensive due to quadratic attention weights. ENACT aims to reduce resource usage while preserving performance.

Method: ENACT clusters transformer input by entropy, leveraging similarity between same-object pixels, and integrates as a plug-in module.

Result: Tests on COCO dataset show reduced memory requirements with minimal accuracy loss.

Conclusion: ENACT effectively balances computational efficiency and detection accuracy in transformer-based object detection.

Abstract: Transformers demonstrate competitive performance in terms of precision on the
problem of vision-based object detection. However, they require considerable
computational resources due to the quadratic size of the attention weights. In
this work, we propose to cluster the transformer input on the basis of its
entropy, due to its similarity between same object pixels. This is expected to
reduce GPU usage during training, while maintaining reasonable accuracy. This
idea is realized with an implemented module that is called ENtropy-based
Attention Clustering for detection Transformers (ENACT), which serves as a
plug-in to any multi-head self-attention based transformer network. Experiments
on the COCO object detection dataset and three detection transformers
demonstrate that the requirements on memory are reduced, while the detection
accuracy is degraded only slightly. The code of the ENACT module is available
at https://github.com/GSavathrakis/ENACT.

</details>


### [388] [Unsupervised Point Cloud Completion through Unbalanced Optimal Transport](https://arxiv.org/pdf/2410.02671)
*Taekyung Lee, Jaemoo Choi, Jaewoong Choi, Myungjoo Kang*

Main category: cs.CV

TL;DR: Proposes UOT-UPC, a model using Unbalanced Optimal Transport for unpaired point cloud completion, achieving robust performance without paired data.


<details>
  <summary>Details</summary>
Motivation: Ground-truth data for complete point clouds is often unavailable, making unpaired completion essential for real-world applications.

Method: Formulates unpaired completion as an Unbalanced Optimal Transport problem, using a Neural OT model to learn the map.

Result: Achieves competitive or superior performance on single- and multi-category benchmarks, especially robust under class imbalance.

Conclusion: UOT-UPC is effective for unpaired point cloud completion, addressing real-world challenges like class imbalance.

Abstract: Unpaired point cloud completion is crucial for real-world applications, where
ground-truth data for complete point clouds are often unavailable. By learning
a completion map from unpaired incomplete and complete point cloud data, this
task avoids the reliance on paired datasets. In this paper, we propose the
\textit{Unbalanced Optimal Transport Map for Unpaired Point Cloud Completion
(\textbf{UOT-UPC})} model, which formulates the unpaired completion task as the
(Unbalanced) Optimal Transport (OT) problem. Our method employs a Neural OT
model learning the UOT map using neural networks. Our model is the first
attempt to leverage UOT for unpaired point cloud completion, achieving
competitive or superior performance on both single-category and multi-category
benchmarks. In particular, our approach is especially robust under the class
imbalance problem, which is frequently encountered in real-world unpaired point
cloud completion scenarios. The code is available at
https://github.com/LEETK99/UOT-UPC.

</details>


### [389] [Scaling Large Motion Models with Million-Level Human Motions](https://arxiv.org/pdf/2410.03311)
*Ye Wang, Sipeng Zheng, Bin Cao, Qianshan Wei, Weishuai Zeng, Qin Jin, Zongqing Lu*

Main category: cs.CV

TL;DR: MotionLib introduces a million-level dataset for motion generation, enabling training of a large motion model (\projname) with robust performance across diverse activities. It highlights the importance of scaling data and model size, proposing Motionbook for efficient motion encoding.


<details>
  <summary>Details</summary>
Motivation: The lack of massive high-quality data hinders the development of generalist models in human motion understanding. MotionLib addresses this gap by providing a large-scale dataset.

Method: MotionLib dataset is created, and a large motion model (\projname) is trained. Motionbook, an innovative motion encoding approach, is introduced, featuring a compact motion representation and a 2D lookup-free tokenizer.

Result: The model demonstrates robust performance across a wide range of human activities, including unseen ones, showcasing the benefits of scaling data and model size.

Conclusion: This work lays the foundation for future development of versatile and powerful motion generation models.

Abstract: Inspired by the recent success of LLMs, the field of human motion
understanding has increasingly shifted toward developing large motion models.
Despite some progress, current efforts remain far from achieving truly
generalist models, primarily due to the lack of massive high-quality data. To
address this gap, we present MotionLib, the first million-level dataset for
motion generation, which is at least 15$\times$ larger than existing
counterparts and enriched with hierarchical text descriptions. Using MotionLib,
we train a large motion model named \projname, demonstrating robust performance
across a wide range of human activities, including unseen ones. Through
systematic investigation, for the first time, we highlight the importance of
scaling both data and model size for advancing motion generation, along with
key insights to achieve this goal. To better integrate the motion modality, we
propose Motionbook, an innovative motion encoding approach including (1) a
compact yet lossless feature to represent motions; (2) a novel 2D lookup-free
motion tokenizer that preserves fine-grained motion details while expanding
codebook capacity, significantly enhancing the representational power of motion
tokens. We believe this work lays the groundwork for developing more versatile
and powerful motion generation models in the future. For further details, visit
https://beingbeyond.github.io/Being-M0/.

</details>


### [390] [Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency](https://arxiv.org/pdf/2410.07599)
*Feng Wang, Timing Yang, Yaodong Yu, Sucheng Ren, Guoyizhe Wei, Angtian Wang, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie*

Main category: cs.CV

TL;DR: Adventurer models treat images as patch token sequences using uni-directional language models, achieving linear complexity for efficient high-resolution image processing.


<details>
  <summary>Details</summary>
Motivation: Address memory and computation issues in high-resolution and fine-grained image processing by leveraging linear complexity.

Method: Uses global pooling tokens and flipping operations to integrate images into causal inference.

Result: Adventurer-Base achieves 84.3% accuracy on ImageNet-1k, 3.8x faster than Vim and 6.2x faster than DeiT.

Conclusion: Adventurer's efficiency and scalability make it promising for future high-resolution or fine-grained image modeling.

Abstract: In this work, we introduce the Adventurer series models where we treat images
as sequences of patch tokens and employ uni-directional language models to
learn visual representations. This modeling paradigm allows us to process
images in a recurrent formulation with linear complexity relative to the
sequence length, which can effectively address the memory and computation
explosion issues posed by high-resolution and fine-grained images. In detail,
we introduce two simple designs that seamlessly integrate image inputs into the
causal inference framework: a global pooling token placed at the beginning of
the sequence and a flipping operation between every two layers. Extensive
empirical studies highlight that compared with the existing plain architectures
such as DeiT and Vim, Adventurer offers an optimal efficiency-accuracy
trade-off. For example, our Adventurer-Base attains a competitive test accuracy
of 84.3% on the standard ImageNet-1k benchmark with 216 images/s training
throughput, which is 3.8 and 6.2 times faster than Vim and DeiT to achieve the
same result. As Adventurer offers great computation and memory efficiency and
allows scaling with linear complexity, we hope this architecture can benefit
future explorations in modeling long sequences for high-resolution or
fine-grained images. Code is available at
https://github.com/wangf3014/Adventurer.

</details>


### [391] [ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](https://arxiv.org/pdf/2410.14138)
*Jingqi Zhou, Sheng Wang, Jingwei Dong, Kai Liu, Lei Li, Jiahui Gao, Jiyue Jiang, Lingpeng Kong, Chuan Wu*

Main category: cs.CV

TL;DR: ProReason is a novel visual reasoning framework addressing LVLMs' bias toward language over vision by decoupling perception and reasoning, integrating LLMs, and outperforming benchmarks by 13.2%.


<details>
  <summary>Details</summary>
Motivation: LVLMs prioritize language over vision in reasoning tasks, leading to performance issues. Existing solutions lack multi-modal reasoning and relevant visual descriptions.

Method: ProReason decomposes reasoning into proactive visual perception and textual reasoning, iterating perception until sufficient visual data is gathered. It integrates LLMs for enhanced reasoning.

Result: ProReason outperforms benchmarks by 13.2% and generates high-quality reasoning data, improving downstream task performance.

Conclusion: ProReason's decoupled approach and LLM integration offer insights for future visual reasoning research, especially LLM-assisted techniques.

Abstract: Large vision-language models (LVLMs) have witnessed significant progress on
visual understanding tasks. However, they often prioritize language knowledge
over image information on visual reasoning tasks, incurring performance
degradation. To tackle this issue, we first identify the drawbacks of existing
solutions (i.e., limited multi-modal reasoning capacities, and insufficient and
irrelevant visual descriptions). We then decompose visual reasoning process
into two stages: proactive visual perception (i.e., eyesight) and textual
reasoning (i.e., wisdom), and introduce a novel visual reasoning framework
named ProReason. This framework features decoupled vision-reasoning
capabilities and multi-run proactive perception. Briefly, given a multi-modal
question, ProReason iterates proactive information collection and reasoning
until the answer can be concluded with necessary and sufficient visual
descriptions. Notably, the disassociation of capabilities allows seamless
integration of existing large language models (LLMs) to compensate for the
reasoning deficits of LVLMs. Our extensive experiments demonstrate that
ProReason outperforms existing multi-step reasoning frameworks on various
benchmarks for both open-source and closed-source models, with the average
performance gain reaching 13.2%. Besides, the integration of LLMs allows
ProReason to produce high-quality visual reasoning data, which empowers
ProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve
superior performance in downstream tasks. Our insights into existing solutions
and the decoupled perspective for feasible integration of LLMs illuminate
future research on visual reasoning techniques, especially LLM-assisted ones.

</details>


### [392] [MedDiff-FM: A Diffusion-based Foundation Model for Versatile Medical Image Applications](https://arxiv.org/pdf/2410.15432)
*Yongrui Yu, Yannian Gu, Shaoting Zhang, Xiaofan Zhang*

Main category: cs.CV

TL;DR: MedDiff-FM is a diffusion-based foundation model for diverse medical image tasks, trained on 3D CT images, and capable of handling tasks like denoising, anomaly detection, and synthesis.


<details>
  <summary>Details</summary>
Motivation: Previous medical image diffusion models were limited to specific regions or tasks; MedDiff-FM aims to generalize across diverse anatomical regions and applications.

Method: Uses 3D CT images for pre-training, integrates multi-level image processing, employs position embedding for spatial relationships, and leverages ControlNet for task-specific fine-tuning.

Result: Effective in tasks like denoising, anomaly detection, synthesis, super-resolution, lesion generation, and inpainting.

Conclusion: MedDiff-FM demonstrates versatility and effectiveness as a foundation model for diverse medical image tasks.

Abstract: Diffusion models have achieved significant success in both natural image and
medical image domains, encompassing a wide range of applications. Previous
investigations in medical images have often been constrained to specific
anatomical regions, particular applications, and limited datasets, resulting in
isolated diffusion models. This paper introduces a diffusion-based foundation
model to address a diverse range of medical image tasks, namely MedDiff-FM.
MedDiff-FM leverages 3D CT images from multiple publicly available datasets,
covering anatomical regions from head to abdomen, to pre-train a diffusion
foundation model, and explores the capabilities of the diffusion foundation
model across a variety of application scenarios. The diffusion foundation model
handles multi-level integrated image processing both at the image-level and
patch-level, utilizes position embedding to establish multi-level spatial
relationships, and leverages region classes and anatomical structures to
capture certain anatomical regions. MedDiff-FM manages several downstream tasks
seamlessly, including image denoising, anomaly detection, and image synthesis.
MedDiff-FM is also capable of performing super-resolution, lesion generation,
and lesion inpainting by rapidly fine-tuning the diffusion foundation model
using ControlNet with task-specific conditions. The experimental results
demonstrate the effectiveness of MedDiff-FM in addressing diverse downstream
medical image tasks.

</details>


### [393] [dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph](https://arxiv.org/pdf/2411.14494)
*Nitish Shukla, Arun Ross*

Main category: cs.CV

TL;DR: A novel demorphing method, dc-GAN, addresses limitations of existing techniques by using dual conditioning and producing high-fidelity reconstructions without morph replication.


<details>
  <summary>Details</summary>
Motivation: Existing demorphing techniques suffer from assumptions about shared identities and morph replication issues.

Method: Proposes dc-GAN, a dual-conditioned GAN, using morph images and embeddings to demorph.

Result: Overcomes morph replication, produces high-fidelity reconstructions, and is generalizable to reference-based and reference-free methods.

Conclusion: dc-GAN is effective and versatile, demonstrated across multiple datasets.

Abstract: A facial morph is an image strategically created by combining two face images
pertaining to two distinct identities. The goal is to create a face image that
can be matched to two different identities by a face matcher. Face demorphing
inverts this process and attempts to recover the original images constituting a
facial morph. Existing demorphing techniques have two major limitations: (a)
they assume that some identities are common in the train and test sets; and (b)
they are prone to the morph replication problem, where the outputs are merely
replicates of the input morph. In this paper, we overcome these issues by
proposing dc-GAN (dual-conditioned GAN), a novel demorphing method conditioned
on the morph image as well as the embedding extracted from the image. Our
method overcomes the morph replication problem and produces high-fidelity
reconstructions of the constituent images. Moreover, the proposed method is
highly generalizable and applicable to both reference-based and reference-free
demorphing methods. Experiments were conducted using the AMSL, FRLL-Morphs, and
MorDiff datasets to demonstrate the efficacy of the method.

</details>


### [394] [$\textit{Revelio}$: Interpreting and leveraging semantic information in diffusion models](https://arxiv.org/pdf/2411.16725)
*Dahye Kim, Xavier Thomas, Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: The paper investigates how visual semantic information is represented in diffusion models, using k-sparse autoencoders and transfer learning to uncover interpretable features and analyze their effectiveness across datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: To deepen the interpretability of diffusion models by understanding how visual semantic information is encoded in their layers and timesteps.

Method: Leverages k-sparse autoencoders (k-SAE) and transfer learning with lightweight classifiers on diffusion features.

Result: Demonstrates the effectiveness of diffusion features for representation learning across 4 datasets and analyzes architectural and dataset impacts.

Conclusion: Provides critical insights into diffusion model interpretability, with code and visualizations available for further exploration.

Abstract: We study $\textit{how}$ rich visual semantic information is represented
within various layers and denoising timesteps of different diffusion
architectures. We uncover monosemantic interpretable features by leveraging
k-sparse autoencoders (k-SAE). We substantiate our mechanistic interpretations
via transfer learning using light-weight classifiers on off-the-shelf diffusion
models' features. On $4$ datasets, we demonstrate the effectiveness of
diffusion features for representation learning. We provide an in-depth analysis
of how different diffusion architectures, pre-training datasets, and language
model conditioning impacts visual representation granularity, inductive biases,
and transfer learning capabilities. Our work is a critical step towards
deepening interpretability of black-box diffusion models. Code and
visualizations available at: https://github.com/revelio-diffusion/revelio

</details>


### [395] [HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator](https://arxiv.org/pdf/2411.17261)
*Fan Yang, Ru Zhen, Jianing Wang, Yanhao Zhang, Haoxiang Chen, Haonan Lu, Sicheng Zhao, Guiguang Ding*

Main category: cs.CV

TL;DR: HEIE is a novel MLLM-based method for explainable evaluation of AIGC image defects, combining heatmaps, scores, and explanations with hierarchical reasoning and a new dataset.


<details>
  <summary>Details</summary>
Motivation: AIGC images often have quality issues, but existing models lack explainability and generalization. MLLMs struggle with fine-grained defect localization and pixel-wise outputs.

Method: HEIE uses a CoT-Driven Explainable Trinity Evaluator for heatmaps, scores, and explanations, and an Adaptive Hierarchical Implausibility Mapper for precise predictions. A new dataset, Expl-AIGI-Eval, is introduced.

Result: HEIE achieves state-of-the-art performance in experiments.

Conclusion: HEIE addresses key challenges in AIGC image evaluation, offering explainability and precision through hierarchical reasoning and a novel dataset.

Abstract: AIGC images are prevalent across various fields, yet they frequently suffer
from quality issues like artifacts and unnatural textures. Specialized models
aim to predict defect region heatmaps but face two primary challenges: (1) lack
of explainability, failing to provide reasons and analyses for subtle defects,
and (2) inability to leverage common sense and logical reasoning, leading to
poor generalization. Multimodal large language models (MLLMs) promise better
comprehension and reasoning but face their own challenges: (1) difficulty in
fine-grained defect localization due to the limitations in capturing tiny
details, and (2) constraints in providing pixel-wise outputs necessary for
precise heatmap generation. To address these challenges, we propose HEIE: a
novel MLLM-Based Hierarchical Explainable Image Implausibility Evaluator. We
introduce the CoT-Driven Explainable Trinity Evaluator, which integrates
heatmaps, scores, and explanation outputs, using CoT to decompose complex tasks
into subtasks of increasing difficulty and enhance interpretability. Our
Adaptive Hierarchical Implausibility Mapper synergizes low-level image features
with high-level mapper tokens from LLMs, enabling precise local-to-global
hierarchical heatmap predictions through an uncertainty-based adaptive token
approach. Moreover, we propose a new dataset: Expl-AIGI-Eval, designed to
facilitate interpretable implausibility evaluation of AIGC images. Our method
demonstrates state-of-the-art performance through extensive experiments. Our
project is at https://yfthu.github.io/HEIE/.

</details>


### [396] [Diagram-Driven Course Questions Generation](https://arxiv.org/pdf/2411.17771)
*Xinyu Zhang, Lingling Zhang, Yanrui Wu, Muye Huang, Wenjun Wu, Bo Li, Shaowei Wang, Basura Fernando, Jun Liu*

Main category: cs.CV

TL;DR: The paper introduces the Diagram-Driven Course Questions Generation (DDCQG) task, addressing the gap in VQG research for diagrams. It presents the DiagramQG dataset and the HKI-DDCQG framework, which outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Current VQG research overlooks diagrams in educational materials, limiting pedagogical assessment. The paper aims to fill this gap by focusing on diagram-based question generation.

Method: The HKI-DDCQG framework uses trainable CLIP for diagram patches, frozen vision-language models for knowledge extraction, and trainable T5 for question generation.

Result: HKI-DDCQG outperforms existing models on the DiagramQG dataset and shows strong generalizability on natural image datasets.

Conclusion: The paper establishes a strong baseline for DDCQG, highlighting its potential for educational applications and addressing key challenges like domain-specific knowledge and diagram complexity.

Abstract: Visual Question Generation (VQG) research focuses predominantly on natural
images while neglecting the diagram, which is a critical component in
educational materials. To meet the needs of pedagogical assessment, we propose
the Diagram-Driven Course Questions Generation (DDCQG) task and construct
DiagramQG, a comprehensive dataset with 15,720 diagrams and 25,798 questions
across 37 subjects and 371 courses. Our approach employs course and input text
constraints to generate course-relevant questions about specific diagram
elements. We reveal three challenges of DDCQG: domain-specific knowledge
requirements across courses, long-tail distribution in course coverage, and
high information density in diagrams. To address these, we propose the
Hierarchical Knowledge Integration framework (HKI-DDCQG), which utilizes
trainable CLIP for identifying relevant diagram patches, leverages frozen
vision-language models for knowledge extraction, and generates questions with
trainable T5. Experiments demonstrate that HKI-DDCQG outperforms existing
models on DiagramQG while maintaining strong generalizability across natural
image datasets, establishing a strong baseline for DDCQG.

</details>


### [397] [FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models](https://arxiv.org/pdf/2411.18672)
*Alice Heiman, Xiaoman Zhang, Emma Chen, Sung Eun Kim, Pranav Rajpurkar*

Main category: cs.CV

TL;DR: FactCheXcker reduces hallucinations in radiology reports by using a query-code-update paradigm, improving measurement accuracy while maintaining report quality.


<details>
  <summary>Details</summary>
Motivation: Medical vision-language models often produce inaccurate quantitative measurements, undermining clinical reliability.

Method: FactCheXcker employs specialized modules and LLM code generation to solve measurement queries, updating reports with extracted findings.

Result: Evaluated on endotracheal tube placement, FactCheXcker reduces hallucinations by 94.0% (mean absolute error) and improves all 11 tested models.

Conclusion: FactCheXcker effectively enhances measurement precision in radiology reports without compromising quality.

Abstract: Medical vision-language model models often struggle with generating accurate
quantitative measurements in radiology reports, leading to hallucinations that
undermine clinical reliability. We introduce FactCheXcker, a modular framework
that de-hallucinates radiology report measurements by leveraging an improved
query-code-update paradigm. Specifically, FactCheXcker employs specialized
modules and the code generation capabilities of large language models to solve
measurement queries generated based on the original report. After extracting
measurable findings, the results are incorporated into an updated report. We
evaluate FactCheXcker on endotracheal tube placement, which accounts for an
average of 78% of report measurements, using the MIMIC-CXR dataset and 11
medical report-generation models. Our results show that FactCheXcker
significantly reduces hallucinations, improves measurement precision, and
maintains the quality of the original reports. Specifically, FactCheXcker
improves the performance of all 11 models and achieves an average improvement
of 94.0% in reducing measurement hallucinations measured by mean absolute
error.

</details>


### [398] [LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language Models](https://arxiv.org/pdf/2412.00686)
*Muhammad Fetrat Qharabagh, Mohammadreza Ghofrani, Kimon Fountoulakis*

Main category: cs.CV

TL;DR: LVLMs struggle with counting tasks, especially for large numbers of objects. A divide-and-conquer method is proposed to improve their counting ability.


<details>
  <summary>Details</summary>
Motivation: Large vision-language models (LVLMs) perform poorly on counting tasks, particularly for larger object counts, highlighting a need for improvement.

Method: A divide-and-conquer approach decomposes counting tasks into sub-tasks and prevents object splitting to avoid repetitive counting.

Result: The proposed method effectively enhances LVLMs' counting performance across multiple datasets.

Conclusion: This approach serves as a valuable baseline for future solutions in improving LVLMs' counting capabilities.

Abstract: Counting is a fundamental operation for various real-world visual tasks,
requiring both object recognition and robust counting capabilities. Despite
their advanced visual perception, large vision-language models (LVLMs) are
known to struggle with counting tasks. In this work, we evaluate the
performance of several recent LVLMs on visual counting tasks across multiple
counting and vision datasets. We observe that while their performance may be
less prone to error for small numbers of objects, they exhibit significant
weaknesses as the number of objects increases. To alleviate this issue, we
propose a simple yet effective baseline method that enhances LVLMs' counting
ability for large numbers of objects using a divide-and-conquer approach. Our
method decomposes counting problems into sub-tasks. Moreover, it incorporates a
mechanism to prevent objects from being split during division, which could
otherwise lead to repetitive counting -- a common issue in a naive
divide-and-conquer implementation. We demonstrate the effectiveness of this
approach across various datasets and benchmarks, establishing it as a valuable
reference for evaluating future solutions.

</details>


### [399] [Structured 3D Latents for Scalable and Versatile 3D Generation](https://arxiv.org/pdf/2412.01506)
*Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang*

Main category: cs.CV

TL;DR: A novel 3D generation method using Structured LATent (SLAT) representation for versatile, high-quality 3D asset creation, supporting multiple output formats and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for flexible and high-quality 3D asset generation with support for diverse output formats and local editing.

Method: Uses a unified SLAT representation integrating sparse 3D grids and dense multiview features, trained with rectified flow transformers on a large dataset.

Result: Generates high-quality 3D assets with text/image conditions, surpassing existing methods, and offers flexible output formats and local editing.

Conclusion: The method advances 3D generation by combining flexibility, quality, and scalability, with plans to release code, model, and data.

Abstract: We introduce a novel 3D generation method for versatile and high-quality 3D
asset creation. The cornerstone is a unified Structured LATent (SLAT)
representation which allows decoding to different output formats, such as
Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a
sparsely-populated 3D grid with dense multiview visual features extracted from
a powerful vision foundation model, comprehensively capturing both structural
(geometry) and textural (appearance) information while maintaining flexibility
during decoding. We employ rectified flow transformers tailored for SLAT as our
3D generation models and train models with up to 2 billion parameters on a
large 3D asset dataset of 500K diverse objects. Our model generates
high-quality results with text or image conditions, significantly surpassing
existing methods, including recent ones at similar scales. We showcase flexible
output format selection and local 3D editing capabilities which were not
offered by previous models. Code, model, and data will be released.

</details>


### [400] [Post-hoc Probabilistic Vision-Language Models](https://arxiv.org/pdf/2412.06014)
*Anton Baumann, Rui Li, Marcus Klasson, Santeri Mentu, Shyamgopal Karthik, Zeynep Akata, Arno Solin, Martin Trapp*

Main category: cs.CV

TL;DR: The paper proposes a post-hoc uncertainty estimation method for vision-language models (VLMs) to address deterministic mapping limitations, improving uncertainty quantification and active learning.


<details>
  <summary>Details</summary>
Motivation: Deterministic mappings in VLMs fail to capture uncertainties from domain shifts, limiting their reliability in downstream tasks.

Method: The method uses Bayesian posterior approximation over VLM layers to analytically quantify uncertainties over cosine similarities, without additional training.

Result: The approach yields improved, well-calibrated uncertainties, interpretable estimates, and efficient active learning compared to baselines.

Conclusion: The method shows promise for enhancing safety-critical applications of large-scale VLMs.

Abstract: Vision-language models (VLMs), such as CLIP and SigLIP, have found remarkable
success in classification, retrieval, and generative tasks. For this, VLMs
deterministically map images and text descriptions to a joint latent space in
which their similarity is assessed using the cosine similarity. However, a
deterministic mapping of inputs fails to capture uncertainties over concepts
arising from domain shifts when used in downstream tasks. In this work, we
propose post-hoc uncertainty estimation in VLMs that does not require
additional training. Our method leverages a Bayesian posterior approximation
over the last layers in VLMs and analytically quantifies uncertainties over
cosine similarities. We demonstrate its effectiveness for uncertainty
quantification and support set selection in active learning. Compared to
baselines, we obtain improved and well-calibrated predictive uncertainties,
interpretable uncertainty estimates, and sample-efficient active learning. Our
results show promise for safety-critical applications of large-scale models.

</details>


### [401] [Nested Hash Layer: A Plug-and-play Module for Multiple-length Hash Code Learning](https://arxiv.org/pdf/2412.08922)
*Liyang He, Yuren Zhang, Rui Li, Zhenya Huang, Runze Wu, Enhong Chen*

Main category: cs.CV

TL;DR: The paper introduces the Nested Hash Layer (NHL) to generate multi-length hash codes efficiently, improving training speed and performance in deep supervised hashing.


<details>
  <summary>Details</summary>
Motivation: Traditional deep supervised hashing models require training multiple models for different code lengths, ignoring relationships between lengths, which is inefficient and time-consuming.

Method: The NHL module generates nested multi-length hash codes. It uses a dominance-aware dynamic weighting strategy for optimization and a long-short cascade self-distillation method to enhance code quality.

Result: NHL improves training speed by 5-8x and boosts model performance by ~3.4%.

Conclusion: NHL effectively addresses efficiency and effectiveness trade-offs in deep supervised hashing, offering a plug-and-play solution for multi-length hash code generation.

Abstract: Deep supervised hashing is essential for efficient storage and search in
large-scale image retrieval. Traditional deep supervised hashing models
generate single-length hash codes, but this creates a trade-off between
efficiency and effectiveness for different code lengths. To find the optimal
length for a task, multiple models must be trained, increasing time and
computation. Furthermore, relationships between hash codes of different lengths
are often ignored. To address these issues, we propose the Nested Hash Layer
(NHL), a plug-and-play module for deep supervised hashing models. NHL generates
hash codes of multiple lengths simultaneously in a nested structure. To resolve
optimization conflicts from multiple learning objectives, we introduce a
dominance-aware dynamic weighting strategy to adjust gradients. Additionally,
we propose a long-short cascade self-distillation method, where long hash codes
guide the learning of shorter ones, improving overall code quality. Experiments
indicate that the NHL achieves an overall training speed improvement of
approximately 5 to 8 times across various deep supervised hashing models and
enhances the average performance of these models by about 3.4%.

</details>


### [402] [DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On](https://arxiv.org/pdf/2412.14465)
*Wengyi Zhan, Mingbao Lin, Shuicheng Yan, Rongrong Ji*

Main category: cs.CV

TL;DR: DiffusionTrend enables virtual fashion try-on without retraining diffusion models, leveraging latent information and a lightweight CNN for garment details.


<details>
  <summary>Details</summary>
Motivation: To avoid resource-intensive retraining of diffusion models and simplify user inputs while achieving visually compelling results.

Method: Uses advanced diffusion models and a lightweight CNN to integrate garment details during the denoising process.

Result: Suboptimal metric performance but offers advantages like no retraining, simplified inputs, and visually appealing try-on.

Conclusion: Highlights the potential of training-free diffusion models in virtual try-on, encouraging further exploration.

Abstract: We introduce DiffusionTrend for virtual fashion try-on, which forgoes the
need for retraining diffusion models. Using advanced diffusion models,
DiffusionTrend harnesses latent information rich in prior information to
capture the nuances of garment details. Throughout the diffusion denoising
process, these details are seamlessly integrated into the model image
generation, expertly directed by a precise garment mask crafted by a
lightweight and compact CNN. Although our DiffusionTrend model initially
demonstrates suboptimal metric performance, our exploratory approach offers
some important advantages: (1) It circumvents resource-intensive retraining of
diffusion models on large datasets. (2) It eliminates the necessity for various
complex and user-unfriendly model inputs. (3) It delivers a visually compelling
try-on experience, underscoring the potential of training-free diffusion model.
This initial foray into the application of untrained diffusion models in
virtual try-on technology potentially paves the way for further exploration and
refinement in this industrially and academically valuable field.

</details>


### [403] [Enhancing Table Recognition with Vision LLMs: A Benchmark and Neighbor-Guided Toolchain Reasoner](https://arxiv.org/pdf/2412.20662)
*Yitong Zhou, Mingyue Cheng, Qingyang Mao, Feiyang Xu, Xin Li*

Main category: cs.CV

TL;DR: The paper introduces a benchmark and the NGTR framework to improve table recognition in Vision Large Language Models (VLLMs) by addressing low-quality image issues.


<details>
  <summary>Details</summary>
Motivation: To explore and enhance the under-researched capability of VLLMs in recognizing unstructured tables, especially under training-free scenarios.

Method: Proposes a hierarchical benchmark and the NGTR framework, which integrates lightweight tools for visual operations and uses neighbor-guided tool selection with a reflection module.

Result: NGTR significantly improves vanilla VLLMs' recognition capabilities, as shown in experiments on public datasets.

Conclusion: The benchmark and NGTR framework offer a viable solution for table recognition, addressing key bottlenecks like low-quality images.

Abstract: Pre-trained foundation models have recently made significant progress in
table-related tasks such as table understanding and reasoning. However,
recognizing the structure and content of unstructured tables using Vision Large
Language Models (VLLMs) remains under-explored. To bridge this gap, we propose
a benchmark based on a hierarchical design philosophy to evaluate the
recognition capabilities of VLLMs in training-free scenarios. Through in-depth
evaluations, we find that low-quality image input is a significant bottleneck
in the recognition process. Drawing inspiration from this, we propose the
Neighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by
integrating diverse lightweight tools for visual operations aimed at mitigating
issues with low-quality images. Specifically, we transfer a tool selection
experience from a similar neighbor to the input and design a reflection module
to supervise the tool invocation process. Extensive experiments on public
datasets demonstrate that our approach significantly enhances the recognition
capabilities of the vanilla VLLMs. We believe that the benchmark and framework
could provide an alternative solution to table recognition.

</details>


### [404] [RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs](https://arxiv.org/pdf/2501.19036)
*Hongliang Li, Jiaxin Zhang, Wenhui Liao, Dezhi Peng, Kai Ding, Lianwen Jin*

Main category: cs.CV

TL;DR: The paper analyzes redundancy in Multimodal Large Language Models (MLLMs), proposing a training-free framework to reduce computations for visual tokens without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address the tradeoff between performance and efficiency in MLLM architectures by investigating redundancy in visual token processing.

Method: Introduces Probe-Activated Dynamic FFN, Hollow Attention, and a Layer Ranking Algorithm to reduce computations for visual tokens.

Result: Identifies structured redundancy in decoder-only MLLMs and achieves comparable or better performance with reduced computations.

Conclusion: The framework offers insights for future MLLM design and serves as a training-free inference acceleration method.

Abstract: Current Multimodal Large Language Model (MLLM) architectures face a critical
tradeoff between performance and efficiency: decoder-only architectures achieve
higher performance but lower efficiency, while cross-attention-based
architectures offer greater efficiency but lower performance. The key
distinction lies in how visual tokens are processed. Decoder-only architectures
apply self-attention and FFN operations on visual tokens, while cross-attention
architectures skip these computations. To investigate whether redundancy exists
in this computationally expensive process, we propose a training-free framework
for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and
Hollow Attention, which enable adjustable reductions in computations for visual
tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these
reductions. Extensive experiments demonstrate substantial, structured, and
clustered redundancy unique to decoder-only MLLMs, offering valuable insights
for future MLLM architecture design. Furthermore, by leveraging our reduction
framework as a training-free inference acceleration approach, we achieve
performance comparable to or better than state-of-the-art methods while
remaining compatible with them. Code will be publicly available at
https://github.com/L-Hugh/RedundancyLens.

</details>


### [405] [Unpaired Deblurring via Decoupled Diffusion Model](https://arxiv.org/pdf/2502.01522)
*Junhao Cheng, Wei-Ting Chen, Xi Lu, Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: UID-Diff improves image deblurring by decoupling structural features and blur patterns using joint training on synthetic and unpaired data, enhancing generalization to unknown domains.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of relying on paired data or synthetic data alone for image deblurring, which often leads to poor generalization.

Method: UID-Diff uses two Q-Formers to extract structural features and blur patterns separately, training on synthetic data (supervised) and unpaired blurred images (unsupervised) with a reconstruction task.

Result: Outperforms state-of-the-art methods in blur removal and structural preservation on real-world datasets.

Conclusion: UID-Diff's blur-decoupled learning enhances generalization, making it effective for unknown blur patterns.

Abstract: Generative diffusion models trained on large-scale datasets have achieved
remarkable progress in image synthesis. In favor of their ability to supplement
missing details and generate aesthetically pleasing contents, recent works have
applied them to image deblurring via training an adapter on blurry-sharp image
pairs to provide structural conditions for restoration. However, acquiring
substantial amounts of realistic paired data is challenging and costly in
real-world scenarios. On the other hand, relying solely on synthetic data often
results in overfitting, leading to unsatisfactory performance when confronted
with unseen blur patterns. To tackle this issue, we propose UID-Diff, a
generative-diffusion-based model designed to enhance deblurring performance on
unknown domains by decoupling structural features and blur patterns through
joint training on three specially designed tasks. We employ two Q-Formers as
structural features and blur patterns extractors separately. The features
extracted by them will be used for the supervised deblurring task on synthetic
data and the unsupervised blur-transfer task by leveraging unpaired blurred
images from the target domain simultaneously. We further introduce a
reconstruction task to make the structural features and blur patterns
complementary. This blur-decoupled learning process enhances the generalization
capabilities of UID-Diff when encountering unknown blur patterns. Experiments
on real-world datasets demonstrate that UID-Diff outperforms existing
state-of-the-art methods in blur removal and structural preservation in various
challenging scenarios.

</details>


### [406] [Masked Autoencoders Are Effective Tokenizers for Diffusion Models](https://arxiv.org/pdf/2502.03444)
*Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, Bhiksha Raj*

Main category: cs.CV

TL;DR: MAETok, a mask-modeling autoencoder, improves latent space structure for diffusion models, achieving state-of-the-art image generation without variational constraints.


<details>
  <summary>Details</summary>
Motivation: The latent space properties for diffusion models are under-explored, and better-structured latent distributions (e.g., fewer Gaussian modes) can enhance generation quality.

Method: Proposes MAETok, an autoencoder using mask modeling to learn a discriminative latent space while maintaining reconstruction fidelity.

Result: Achieves gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation, outperforming variational autoencoders.

Conclusion: The latent space structure, not variational constraints, is key for effective diffusion models.

Abstract: Recent advances in latent diffusion models have demonstrated their
effectiveness for high-resolution image synthesis. However, the properties of
the latent space from tokenizer for better learning and generation of diffusion
models remain under-explored. Theoretically and empirically, we find that
improved generation quality is closely tied to the latent distributions with
better structure, such as the ones with fewer Gaussian Mixture modes and more
discriminative features. Motivated by these insights, we propose MAETok, an
autoencoder (AE) leveraging mask modeling to learn semantically rich latent
space while maintaining reconstruction fidelity. Extensive experiments validate
our analysis, demonstrating that the variational form of autoencoders is not
necessary, and a discriminative latent space from AE alone enables
state-of-the-art performance on ImageNet generation using only 128 tokens.
MAETok achieves significant practical improvements, enabling a gFID of 1.69
with 76x faster training and 31x higher inference throughput for 512x512
generation. Our findings show that the structure of the latent space, rather
than variational constraints, is crucial for effective diffusion models. Code
and trained models are released.

</details>


### [407] [Improving Adversarial Robustness via Phase and Amplitude-aware Prompting](https://arxiv.org/pdf/2502.03758)
*Yibo Xu, Dawei Zhou, Decheng Liu, Nannan Wang*

Main category: cs.CV

TL;DR: The paper proposes a Phase and Amplitude-aware Prompting (PAP) defense to improve robustness in deep neural networks against adversarial perturbations by focusing on phase and amplitude spectra.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based defenses lack focus on critical patterns tied to object semantics, while phase and amplitude spectra are proven crucial for robustness.

Method: Construct phase-level and amplitude-level prompts per class, adjust weights during training based on robust performance, and select prompts during testing using predicted labels.

Result: Experimental results confirm the method's effectiveness.

Conclusion: PAP defense enhances robustness by leveraging phase and amplitude spectra in prompt-based defenses.

Abstract: Deep neural networks are found to be vulnerable to adversarial perturbations.
The prompt-based defense has been increasingly studied due to its high
efficiency. However, existing prompt-based defenses mainly exploited mixed
prompt patterns, where critical patterns closely related to object semantics
lack sufficient focus. The phase and amplitude spectra have been proven to be
highly related to specific semantic patterns and crucial for robustness. To
this end, in this paper, we propose a Phase and Amplitude-aware Prompting (PAP)
defense. Specifically, we construct phase-level and amplitude-level prompts for
each class, and adjust weights for prompting according to the model's robust
performance under these prompts during training. During testing, we select
prompts for each image using its predicted label to obtain the prompted image,
which is inputted to the model to get the final prediction. Experimental
results demonstrate the effectiveness of our method.

</details>


### [408] [VideoRoPE: What Makes for Good Video Rotary Position Embedding?](https://arxiv.org/pdf/2502.05173)
*Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, Dahua Lin*

Main category: cs.CV

TL;DR: The paper introduces VideoRoPE, a 3D adaptation of Rotary Position Embedding (RoPE) for video, addressing challenges in spatio-temporal structure and outperforming prior variants.


<details>
  <summary>Details</summary>
Motivation: Extending 1D RoPE to video is challenging due to complex spatio-temporal dynamics. Prior work lacks key characteristics for effective adaptation, leading to poor performance with distractors.

Method: Proposes VideoRoPE with a 3D structure, low-frequency temporal allocation, diagonal layout, and adjustable temporal spacing to preserve spatio-temporal relationships.

Result: VideoRoPE outperforms previous RoPE variants in tasks like long video retrieval, video understanding, and video hallucination.

Conclusion: VideoRoPE effectively adapts RoPE to video, addressing spatio-temporal challenges and improving performance across diverse tasks.

Abstract: While Rotary Position Embedding (RoPE) and its variants are widely adopted
for their long-context capabilities, the extension of the 1D RoPE to video,
with its complex spatio-temporal structure, remains an open challenge. This
work first introduces a comprehensive analysis that identifies four key
characteristics essential for the effective adaptation of RoPE to video, which
have not been fully considered in prior work. As part of our analysis, we
introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors)
task, which adds periodic distractors into V-NIAH. The V-NIAH-D task
demonstrates that previous RoPE variants, lacking appropriate temporal
dimension allocation, are easily misled by distractors. Based on our analysis,
we introduce \textbf{VideoRoPE}, with a \textit{3D structure} designed to
preserve spatio-temporal relationships. VideoRoPE features
\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a
\textit{diagonal layout} to maintain spatial symmetry, and \textit{adjustable
temporal spacing} to decouple temporal and spatial indexing. VideoRoPE
consistently surpasses previous RoPE variants, across diverse downstream tasks
such as long video retrieval, video understanding, and video hallucination. Our
code will be available at
\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.

</details>


### [409] [MSVCOD:A Large-Scale Multi-Scene Dataset for Video Camouflage Object Detection](https://arxiv.org/pdf/2502.13859)
*Shuyong Gao, Yu'ang Feng, Qishan Wang, Lingyi Hong, Xinyu Zhou, Liu Fei, Yan Wang, Wenqiang Zhang*

Main category: cs.CV

TL;DR: The paper introduces MSVCOD, a large-scale multi-domain dataset for Video Camouflaged Object Detection (VCOD), expanding beyond wildlife to include human, medical, and vehicle objects. It also proposes a one-stream model for VCOD, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing VCOD datasets focus on wildlife, limiting broader applications. The paper aims to address this by creating a diverse dataset (MSVCOD) and improving detection methods.

Method: A semi-automatic iterative annotation pipeline was designed for high-quality dataset creation. A one-stream model was developed for feature extraction and fusion without additional motion modules.

Result: The MSVCOD dataset is the largest and most diverse VCOD dataset. The proposed model achieves state-of-the-art performance on both existing and new datasets.

Conclusion: The MSVCOD dataset and one-stream model significantly advance VCOD research, broadening its practical applications across multiple domains.

Abstract: Video Camouflaged Object Detection (VCOD) is a challenging task which aims to
identify objects that seamlessly concealed within the background in videos. The
dynamic properties of video enable detection of camouflaged objects through
motion cues or varied perspectives. Previous VCOD datasets primarily contain
animal objects, limiting the scope of research to wildlife scenarios. However,
the applications of VCOD extend beyond wildlife and have significant
implications in security, art, and medical fields. Addressing this problem, we
construct a new large-scale multi-domain VCOD dataset MSVCOD. To achieve
high-quality annotations, we design a semi-automatic iterative annotation
pipeline that reduces costs while maintaining annotation accuracy. Our MSVCOD
is the largest VCOD dataset to date, introducing multiple object categories
including human, animal, medical, and vehicle objects for the first time, while
also expanding background diversity across various environments. This expanded
scope increases the practical applicability of the VCOD task in camouflaged
object detection. Alongside this dataset, we introduce a one-steam video
camouflage object detection model that performs both feature extraction and
information fusion without additional motion feature fusion modules. Our
framework achieves state-of-the-art results on the existing VCOD animal dataset
and the proposed MSVCOD. The dataset and code will be made publicly available.

</details>


### [410] [A Survey on Text-Driven 360-Degree Panorama Generation](https://arxiv.org/pdf/2502.14799)
*Hai Wang, Xiaoyu Xiang, Weihao Xia, Jing-Hao Xue*

Main category: cs.CV

TL;DR: A survey on text-driven 360-degree panorama generation, reviewing algorithms, applications, limitations, and future directions.


<details>
  <summary>Details</summary>
Motivation: To simplify immersive visual content creation by synthesizing 360-degree panoramas from text descriptions.

Method: Review of state-of-the-art algorithms and applications in 360-degree 3D scene generation.

Result: Identifies current limitations and suggests future research directions.

Conclusion: The field is advancing rapidly, with potential for further innovation in immersive content creation.

Abstract: The advent of text-driven 360-degree panorama generation, enabling the
synthesis of 360-degree panoramic images directly from textual descriptions,
marks a transformative advancement in immersive visual content creation. This
innovation significantly simplifies the traditionally complex process of
producing such content. Recent progress in text-to-image diffusion models has
accelerated the rapid development in this emerging field. This survey presents
a comprehensive review of text-driven 360-degree panorama generation, offering
an in-depth analysis of state-of-the-art algorithms and their expanding
applications in 360-degree 3D scene generation. Furthermore, we critically
examine current limitations and propose promising directions for future
research. A curated project page with relevant resources and research papers is
available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.

</details>


### [411] [A Survey on Self-supervised Contrastive Learning for Multimodal Text-Image Analysis](https://arxiv.org/pdf/2503.11101)
*Asifullah Khan, Laiba Asmatullah, Anza Malik, Shahzaib Khan, Hamna Asif*

Main category: cs.CV

TL;DR: The paper reviews self-supervised contrastive learning in text-image models, covering terminologies, developments, categorization, techniques, and applications.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of contrastive learning in text-image models due to its success in reducing reliance on labeled data.

Method: Discusses terminologies, categorizes approaches by model structures, and reviews techniques like pretext tasks and architectural trends.

Result: Highlights significant improvements in image-text understanding and recent state-of-the-art applications.

Conclusion: Contrastive learning is a powerful tool for text-image models, with ongoing advancements and broad applications.

Abstract: Self-supervised learning is a machine learning approach that generates
implicit labels by learning underlined patterns and extracting discriminative
features from unlabeled data without manual labelling. Contrastive learning
introduces the concept of "positive" and "negative" samples, where positive
pairs (e.g., variation of the same image/object) are brought together in the
embedding space, and negative pairs (e.g., views from different images/objects)
are pushed farther away. This methodology has shown significant improvements in
image understanding and image text analysis without much reliance on labeled
data. In this paper, we comprehensively discuss the terminologies, recent
developments and applications of contrastive learning with respect to
text-image models. Specifically, we provide an overview of the approaches of
contrastive learning in text-image models in recent years. Secondly, we
categorize the approaches based on different model structures. Thirdly, we
further introduce and discuss the latest advances of the techniques used in the
process such as pretext tasks for both images and text, architectural
structures, and key trends. Lastly, we discuss the recent state-of-art
applications of self-supervised contrastive learning Text-Image based models.

</details>


### [412] [Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives](https://arxiv.org/pdf/2503.14604)
*Sara Sarto, Marcella Cornia, Rita Cucchiara*

Main category: cs.CV

TL;DR: A survey on evaluating machine-generated image captions, focusing on advancements, challenges, and future directions, especially with Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: The rise of MLLMs has increased the need for robust evaluation metrics for image captioning, given the complexity and stylistic variations in generated captions.

Method: Comprehensive analysis of existing metrics, assessing their correlation with human judgment, ranking accuracy, and sensitivity to hallucinations.

Result: Highlights limitations of current evaluation approaches and identifies gaps in handling longer, detailed captions from MLLMs.

Conclusion: Suggests future research directions to improve image captioning assessment, addressing the evolving challenges posed by MLLMs.

Abstract: The evaluation of machine-generated image captions is a complex and evolving
challenge. With the advent of Multimodal Large Language Models (MLLMs), image
captioning has become a core task, increasing the need for robust and reliable
evaluation metrics. This survey provides a comprehensive overview of
advancements in image captioning evaluation, analyzing the evolution,
strengths, and limitations of existing metrics. We assess these metrics across
multiple dimensions, including correlation with human judgment, ranking
accuracy, and sensitivity to hallucinations. Additionally, we explore the
challenges posed by the longer and more detailed captions generated by MLLMs
and examine the adaptability of current metrics to these stylistic variations.
Our analysis highlights some limitations of standard evaluation approaches and
suggests promising directions for future research in image captioning
assessment.

</details>


### [413] [Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models](https://arxiv.org/pdf/2503.17794)
*Ketan Suhaas Saichandran, Xavier Thomas, Prakhar Kaushik, Deepti Ghadiyaram*

Main category: cs.CV

TL;DR: SCoPE improves text-to-image alignment by refining prompts from broad to fine details, boosting VQA scores by +8 over Stable Diffusion.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating images from long, complex prompts with diverse objects and spatial relationships.

Method: Decomposes prompts into sub-prompts (coarse to fine), interpolates them during inference for progressive detail introduction.

Result: Achieves +8 average improvement in VQA scores on 83% of GenAI-Bench prompts.

Conclusion: SCoPE is a training-free, plug-and-play solution enhancing text-to-image alignment effectively.

Abstract: Text-to-image generative models often struggle with long prompts detailing
complex scenes, diverse objects with distinct visual characteristics and
spatial relationships. In this work, we propose SCoPE (Scheduled interpolation
of Coarse-to-fine Prompt Embeddings), a training-free method to improve
text-to-image alignment by progressively refining the input prompt in a
coarse-to-fine-grained manner. Given a detailed input prompt, we first
decompose it into multiple sub-prompts which evolve from describing broad scene
layout to highly intricate details. During inference, we interpolate between
these sub-prompts and thus progressively introduce finer-grained details into
the generated image. Our training-free plug-and-play approach significantly
enhances prompt alignment, achieves an average improvement of more than +8 in
Visual Question Answering (VQA) scores over the Stable Diffusion baselines on
83% of the prompts from the GenAI-Bench dataset.

</details>


### [414] [Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models](https://arxiv.org/pdf/2503.18034)
*Qiao Liang, Yanjiang Liu, Weixiang Zhou, Ben He, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun, Yingfei Sun*

Main category: cs.CV

TL;DR: The paper investigates how the prior knowledge of vision encoders affects MLLMs, proposing a metric ($Rank_e$) and a training framework (VisPRE) to enhance performance, especially for uncommon visual entities.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of vision encoder prior knowledge on MLLMs, as existing research often overlooks this aspect.

Method: Introduces $Rank_e$ to quantify prior knowledge's effect and proposes VisPRE, a two-stage training framework to incorporate prior knowledge explicitly.

Result: Prior knowledge positively correlates with MLLM performance; VisPRE significantly boosts visual understanding, especially for low-prior-knowledge entities.

Conclusion: Enhancing vision encoder prior knowledge improves MLLM performance, offering a novel strategy for better visual understanding.

Abstract: Does the prior knowledge of the vision encoder constrain the capability
boundary of Multi-modal Large Language Models (MLLMs)? While most existing
research treats MLLMs as unified systems optimized through end-to-end training,
the impact of vision encoder's prior knowledge is seldom investigated. In this
work, we introduce a novel metric, $Rank_e$, to quantify the effect of prior
knowledge of the vision encoder on MLLM performance. Our analysis reveals a
positive correlation between prior knowledge and MLLM performance. Moreover, we
find that domain-specific fine-tuning using solely end-to-end visual question
answering (VQA) data is insufficient, particularly for entities with low
inherent visual prior knowledge. To address this issue, we propose VisPRE
(Vision Prior Remediation), a two-stage training framework that explicitly
incorporates prior knowledge at the vision encoder level. Experimental results
demonstrate that augmenting vision encoder's prior knowledge substantially
boosts the visual understanding capabilities of MLLMs, offering a novel and
effective strategy for improving performance, especially in scenarios involving
uncommon visual entities.

</details>


### [415] [Good Keypoints for the Two-View Geometry Estimation Problem](https://arxiv.org/pdf/2503.18767)
*Konstantin Pakulev, Alexander Vakhitov, Gonzalo Ferrer*

Main category: cs.CV

TL;DR: The paper proposes a theoretical model for scoring keypoints in homography estimation, introducing the BoNeSS-ST detector, which outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: To understand properties of local features that improve downstream performance, particularly in homography estimation.

Method: Develops a model for keypoint scoring based on repeatability and small measurement error, leading to the BoNeSS-ST detector with subpixel refinement and robustness to low saliency.

Result: BoNeSS-ST excels in homography estimation and matches prior methods in epipolar geometry estimation.

Conclusion: The model and BoNeSS-ST provide insights and practical improvements for local feature design in geometric tasks.

Abstract: Local features are essential to many modern downstream applications.
Therefore, it is of interest to determine the properties of local features that
contribute to the downstream performance for a better design of feature
detectors and descriptors. In our work, we propose a new theoretical model for
scoring feature points (keypoints) in the context of the two-view geometry
estimation problem. The model determines two properties that a good keypoint
for solving the homography estimation problem should have: be repeatable and
have a small expected measurement error. This result provides key insights into
why maximizing the number of correspondences doesn't always lead to better
homography estimation accuracy. We use the developed model to design a method
that detects keypoints that benefit the homography estimation and introduce the
Bounded NeSS-ST (BoNeSS-ST) keypoint detector. The novelty of BoNeSS-ST comes
from strong theoretical foundations, a more accurate keypoint scoring due to
subpixel refinement and a cost designed for superior robustness to low saliency
keypoints. As a result, BoNeSS-ST outperforms prior self-supervised local
feature detectors on the planar homography estimation task and is on par with
them on the epipolar geometry estimation task.

</details>


### [416] [Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data](https://arxiv.org/pdf/2503.20771)
*Masoumeh Sharafi, Emma Ollivier, Muhammad Osama Zeeshan, Soufiane Belharbi, Marco Pedersoli, Alessandro Lameiras Koerich, Simon Bacon, Eric Granger*

Main category: cs.CV

TL;DR: The paper introduces DSFDA, a method for adapting facial expression recognition models using only neutral target videos, addressing challenges of missing non-neutral data.


<details>
  <summary>Details</summary>
Motivation: FER models face inter-subject variability and lack of comprehensive target data, especially in healthcare. SFDA methods often assume complete target data, which is impractical.

Method: DSFDA leverages neutral control videos to generate missing non-neutral expression data, disentangling identity and expression features for better adaptation.

Result: The method improves model accuracy by generating missing data and using self-supervision for identity-preserving reconstruction.

Conclusion: DSFDA effectively adapts FER models with limited target data, enhancing performance in real-world scenarios.

Abstract: Facial Expression Recognition (FER) from videos is a crucial task in various
application areas, such as human-computer interaction and health diagnosis and
monitoring (e.g., assessing pain and depression). Beyond the challenges of
recognizing subtle emotional or health states, the effectiveness of deep FER
models is often hindered by the considerable inter-subject variability in
expressions. Source-free (unsupervised) domain adaptation (SFDA) methods may be
employed to adapt a pre-trained source model using only unlabeled target domain
data, thereby avoiding data privacy, storage, and transmission issues.
Typically, SFDA methods adapt to a target domain dataset corresponding to an
entire population and assume it includes data from all recognition classes.
However, collecting such comprehensive target data can be difficult or even
impossible for FER in healthcare applications. In many real-world scenarios, it
may be feasible to collect a short neutral control video (which displays only
neutral expressions) from target subjects before deployment. These videos can
be used to adapt a model to better handle the variability of expressions among
subjects. This paper introduces the Disentangled SFDA (DSFDA) method to address
the challenge posed by adapting models with missing target expression data.
DSFDA leverages data from a neutral target control video for end-to-end
generation and adaptation of target data with missing non-neutral data. Our
method learns to disentangle features related to expressions and identity while
generating the missing non-neutral expression data for the target subject,
thereby enhancing model accuracy. Additionally, our self-supervision strategy
improves model adaptation by reconstructing target images that maintain the
same identity and source expression.

</details>


### [417] [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/pdf/2503.23083)
*Hasan Moughnieh, Mohamad Chalhoub, Hasan Nasrallah, Cristiano Nattero, Paolo Campanella, Giovanni Nico, Ali J. Ghandour*

Main category: cs.CV

TL;DR: The paper explores Parameter Efficient Fine Tuning (PEFT) techniques to adapt pre-trained models for remote sensing visual grounding tasks, achieving SOTA performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To leverage pre-trained models for efficient and scalable visual grounding in remote sensing, where such applications are underexplored.

Method: Applied PEFT techniques (LoRA, BitFit, adapters) to fine-tune models like Grounding DINO and OFA for RS-specific tasks.

Result: Achieved performance comparable or superior to SOTA models while significantly lowering computational overhead.

Conclusion: PEFT techniques offer a practical, cost-effective solution for advancing multi-modal analysis in remote sensing.

Abstract: Adapting pre-trained models has become an effective strategy in artificial
intelligence, offering a scalable and efficient alternative to training models
from scratch. In the context of remote sensing (RS), where visual grounding(VG)
remains underexplored, this approach enables the deployment of powerful
vision-language models to achieve robust cross-modal understanding while
significantly reducing computational overhead. To address this, we applied
Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for
RS-specific VG tasks. Specifically, we evaluated LoRA placement across
different modules in Grounding DINO and used BitFit and adapters to fine-tune
the OFA foundation model pre-trained on general-purpose VG datasets. This
approach achieved performance comparable to or surpassing current State Of The
Art (SOTA) models while significantly reducing computational costs. This study
highlights the potential of PEFT techniques to advance efficient and precise
multi-modal analysis in RS, offering a practical and cost-effective alternative
to full model training.

</details>


### [418] [AnimeGamer: Infinite Anime Life Simulation with Next Game State Prediction](https://arxiv.org/pdf/2504.01014)
*Junhao Cheng, Yuying Ge, Yixiao Ge, Jing Liao, Ying Shan*

Main category: cs.CV

TL;DR: AnimeGamer uses Multimodal Large Language Models (MLLMs) to generate dynamic, contextually consistent anime life simulation games, addressing limitations of static image generation and lack of historical visual context.


<details>
  <summary>Details</summary>
Motivation: To enhance infinite anime life simulation games by addressing inconsistencies from static image generation and lack of historical visual context, enabling dynamic and immersive gameplay.

Method: Proposes AnimeGamer, leveraging MLLMs for dynamic animation shots and action-aware multimodal representations, decoded into video clips using a video diffusion model.

Result: Outperforms existing methods in gaming experience, achieving contextual consistency and dynamic gameplay, validated by automated metrics and human evaluations.

Conclusion: AnimeGamer advances anime life simulation by integrating dynamic visuals and historical context, offering a superior gaming experience.

Abstract: Recent advancements in image and video synthesis have opened up new promise
in generative games. One particularly intriguing application is transforming
characters from anime films into interactive, playable entities. This allows
players to immerse themselves in the dynamic anime world as their favorite
characters for life simulation through language instructions. Such games are
defined as infinite game since they eliminate predetermined boundaries and
fixed gameplay rules, where players can interact with the game world through
open-ended language and experience ever-evolving storylines and environments.
Recently, a pioneering approach for infinite anime life simulation employs
large language models (LLMs) to translate multi-turn text dialogues into
language instructions for image generation. However, it neglects historical
visual context, leading to inconsistent gameplay. Furthermore, it only
generates static images, failing to incorporate the dynamics necessary for an
engaging gaming experience. In this work, we propose AnimeGamer, which is built
upon Multimodal Large Language Models (MLLMs) to generate each game state,
including dynamic animation shots that depict character movements and updates
to character states, as illustrated in Figure 1. We introduce novel
action-aware multimodal representations to represent animation shots, which can
be decoded into high-quality video clips using a video diffusion model. By
taking historical animation shot representations as context and predicting
subsequent representations, AnimeGamer can generate games with contextual
consistency and satisfactory dynamics. Extensive evaluations using both
automated metrics and human evaluations demonstrate that AnimeGamer outperforms
existing methods in various aspects of the gaming experience. Codes and
checkpoints are available at https://github.com/TencentARC/AnimeGamer.

</details>


### [419] [REJEPA: A Novel Joint-Embedding Predictive Architecture for Efficient Remote Sensing Image Retrieval](https://arxiv.org/pdf/2504.03169)
*Shabnam Choudhury, Yash Salunkhe, Sarthak Mehrotra, Biplab Banerjee*

Main category: cs.CV

TL;DR: REJEPA is a self-supervised framework for remote sensing image retrieval (RS-CBIR) that reduces computational complexity by 40-60% and improves retrieval accuracy by 5.1-10.1% over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of remote sensing image archives requires efficient and robust content-based image retrieval techniques.

Method: REJEPA uses spatially distributed context token encoding to predict abstract representations, avoiding pixel-level details. It incorporates VICReg for feature diversity and redundancy reduction.

Result: REJEPA outperforms SSL techniques like CSMAE-SESD and SatMAE, achieving 5.1-10.1% higher accuracy on benchmarks like BEN-14K and FMoW.

Conclusion: REJEPA is a sensor-agnostic, efficient, and scalable solution for RS-CBIR, handling challenges like varying resolutions and complex backgrounds.

Abstract: The rapid expansion of remote sensing image archives demands the development
of strong and efficient techniques for content-based image retrieval (RS-CBIR).
This paper presents REJEPA (Retrieval with Joint-Embedding Predictive
Architecture), an innovative self-supervised framework designed for unimodal
RS-CBIR. REJEPA utilises spatially distributed context token encoding to
forecast abstract representations of target tokens, effectively capturing
high-level semantic features and eliminating unnecessary pixel-level details.
In contrast to generative methods that focus on pixel reconstruction or
contrastive techniques that depend on negative pairs, REJEPA functions within
feature space, achieving a reduction in computational complexity of 40-60% when
compared to pixel-reconstruction baselines like Masked Autoencoders (MAE). To
guarantee strong and varied representations, REJEPA incorporates
Variance-Invariance-Covariance Regularisation (VICReg), which prevents encoder
collapse by promoting feature diversity and reducing redundancy. The method
demonstrates an estimated enhancement in retrieval accuracy of 5.1% on BEN-14K
(S1), 7.4% on BEN-14K (S2), 6.0% on FMoW-RGB, and 10.1% on FMoW-Sentinel
compared to prominent SSL techniques, including CSMAE-SESD, Mask-VLM, SatMAE,
ScaleMAE, and SatMAE++, on extensive RS benchmarks BEN-14K (multispectral and
SAR data), FMoW-RGB and FMoW-Sentinel. Through effective generalisation across
sensor modalities, REJEPA establishes itself as a sensor-agnostic benchmark for
efficient, scalable, and precise RS-CBIR, addressing challenges like varying
resolutions, high object density, and complex backgrounds with computational
efficiency.

</details>


### [420] [SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement](https://arxiv.org/pdf/2504.07934)
*Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang*

Main category: cs.CV

TL;DR: ThinkLite-VL models achieve SoTA performance with fewer training samples using reinforcement fine-tuning (RFT) and MCTS-based difficulty filtering.


<details>
  <summary>Details</summary>
Motivation: To improve visual reasoning models efficiently by focusing on challenging samples without knowledge distillation.

Method: Uses MCTS to measure sample difficulty and selects high-quality subsets for RFT, training on 11k and 7.5k samples for 7B and 72B models, respectively.

Result: ThinkLite-VL-7B and ThinkLite-VL-72B outperform base models and larger competitors, achieving new SoTA scores (e.g., 75.1 on MathVista).

Conclusion: MCTS-guided difficulty filtering enables scalable, data-efficient self-improvement for multimodal reasoning.

Abstract: We introduce ThinkLite-VL, a family of visual reasoning models that achieve
state-of-the-art (SoTA) performance using an order of magnitude fewer training
samples, relying purely on reinforcement fine-tuning (RFT) self-improvement
without any knowledge distillation. Our central insight is that sample
difficulty critically influences RFT effectiveness: appropriately challenging
examples can drive substantial reasoning improvements, even in low-data
regimes. However, quantifying sample difficulty in a reliable and scalable
manner remains non-trivial. To address this, we repurpose Monte Carlo Tree
Search (MCTS) to measure sample difficulty via the number of reasoning
iterations a vision-language model (VLM) requires to solve each instance. This
MCTS-based selection procedure identifies samples that induce deeper reasoning
while remaining solvable, allowing us to filter a high-quality subset from 70k
open-source examples spanning math, natural image understanding, and chart
comprehension. Using this approach, we select just 11k challenging samples for
RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The
resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly
outperform their respective base models across eight visual reasoning
benchmarks. In particular, ThinkLite-VL-7B improves the average performance of
Qwen2.5-VL-7B-Instruct by 7\% and surpasses all existing 7B-level models, as
well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a
new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA
frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark
improvement of 4.42 over the open-source SOTA. These results demonstrate that
MCTS-guided difficulty filtering provides a scalable and effective path toward
data-efficient self-improvement in multimodal reasoning.

</details>


### [421] [A Decade of Wheat Mapping for Lebanon](https://arxiv.org/pdf/2504.11366)
*Hasan Wehbi, Hasan Nasrallah, Mohamad Hasan Zahweh, Zeinab Takach, Veera Ganesh Yalla, Ali J. Ghandour*

Main category: cs.CV

TL;DR: The paper introduces an improved pipeline for wheat field mapping using a TSViT with PEFT and a novel post-processing method, validated by a decade-long case study in Lebanon.


<details>
  <summary>Details</summary>
Motivation: Accurate wheat field mapping is crucial for global food security, supply chain management, and resource allocation.

Method: The proposed method integrates a Temporal Spatial Vision Transformer (TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing pipeline based on the FTW framework.

Result: The pipeline improves boundary delineation and field-level precision, enabling detailed analysis like crop rotation tracking.

Conclusion: The framework supports operational agricultural monitoring and historical trend analysis, laying the groundwork for future studies in crop monitoring and yield estimation.

Abstract: Wheat accounts for approximately 20% of the world's caloric intake, making it
a vital component of global food security. Given this importance, mapping wheat
fields plays a crucial role in enabling various stakeholders, including policy
makers, researchers, and agricultural organizations, to make informed decisions
regarding food security, supply chain management, and resource allocation. In
this paper, we tackle the problem of accurately mapping wheat fields out of
satellite images by introducing an improved pipeline for winter wheat
segmentation, as well as presenting a case study on a decade-long analysis of
wheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer
(TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing
pipeline based on the Fields of The World (FTW) framework. Our proposed
pipeline addresses key challenges encountered in existing approaches, such as
the clustering of small agricultural parcels in a single large field. By
merging wheat segmentation with precise field boundary extraction, our method
produces geometrically coherent and semantically rich maps that enable us to
perform in-depth analysis such as tracking crop rotation pattern over years.
Extensive evaluations demonstrate improved boundary delineation and field-level
precision, establishing the potential of the proposed framework in operational
agricultural monitoring and historical trend analysis. By allowing for accurate
mapping of wheat fields, this work lays the foundation for a range of critical
studies and future advances, including crop monitoring and yield estimation.

</details>


### [422] [Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation](https://arxiv.org/pdf/2504.15085)
*Wangyu Wu, Zhenhong Chen, Siqi Song, Xianglin Qiu, Xiaowei Huang, Fei Ma, Jimin Xiao*

Main category: cs.CV

TL;DR: HAF-VT integrates visual and textual data for cross-domain sequential recommendation, outperforming existing methods by mimicking human cognitive processes.


<details>
  <summary>Details</summary>
Motivation: To enhance cognitive modeling in cross-domain sequential recommendation by leveraging multimodal data (visual and textual) and human-like information integration.

Method: Uses frozen CLIP model for image and text embeddings, with a hierarchical attention mechanism to learn single- and cross-domain preferences.

Result: Outperforms existing methods on four e-commerce datasets, better capturing cross-domain user interests.

Conclusion: HAF-VT bridges cognitive principles with computational models, emphasizing the importance of multimodal data in sequential decision-making.

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences through intra- and inter-sequence item
relationships. Inspired by human cognitive processes, we propose Hierarchical
Attention Fusion of Visual and Textual Representations (HAF-VT), a novel
approach integrating visual and textual data to enhance cognitive modeling.
Using the frozen CLIP model, we generate image and text embeddings, enriching
item representations with multimodal data. A hierarchical attention mechanism
jointly learns single-domain and cross-domain preferences, mimicking human
information integration. Evaluated on four e-commerce datasets, HAF-VT
outperforms existing methods in capturing cross-domain user interests, bridging
cognitive principles with computational models and highlighting the role of
multimodal data in sequential decision-making.

</details>


### [423] [CLIP-IT: CLIP-based Pairing for Histology Images Classification](https://arxiv.org/pdf/2504.16181)
*Banafsheh Karimian, Giulia Avanzato, Soufian Belharbi, Luke McCaffrey, Mohammadhadi Shateri, Eric Granger*

Main category: cs.CV

TL;DR: CLIP-IT trains a vision backbone model for histology image classification by leveraging external text reports, avoiding the need for large paired datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of requiring large paired datasets for training vision-language models in medical image analysis, which raises privacy and cost concerns.

Method: Uses CLIP-based modality pairing to match images with external text, followed by knowledge distillation to enhance unimodal image classifiers without text during inference.

Result: Outperforms unimodal classifiers on PCAM, CRC, and BACH datasets, offering a cost-effective solution.

Conclusion: CLIP-IT provides a practical way to leverage privileged textual information for improved histology image analysis without paired data constraints.

Abstract: Multimodal learning has shown significant promise for improving medical image
analysis by integrating information from complementary data sources. This is
widely employed for training vision-language models (VLMs) for cancer detection
based on histology images and text reports. However, one of the main
limitations in training these VLMs is the requirement for large paired
datasets, raising concerns over privacy, and data collection, annotation, and
maintenance costs. To address this challenge, we introduce CLIP-IT method to
train a vision backbone model to classify histology images by pairing them with
privileged textual information from an external source. At first, the modality
pairing step relies on a CLIP-based model to match histology images with
semantically relevant textual report data from external sources, creating an
augmented multimodal dataset without the need for manually paired samples.
Then, we propose a multimodal training procedure that distills the knowledge
from the paired text modality to the unimodal image classifier for enhanced
performance without the need for the textual data during inference. A
parameter-efficient fine-tuning method is used to efficiently address the
misalignment between the main (image) and paired (text) modalities. During
inference, the improved unimodal histology classifier is used, with only
minimal additional computational complexity. Our experiments on challenging
PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a
cost-effective approach to leverage privileged textual information and
outperform unimodal classifiers for histology.

</details>


### [424] [SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity](https://arxiv.org/pdf/2504.19581)
*Chengzhi Wu, Yuxin Wan, Hao Fu, Julius Pfrommer, Zeyun Zhong, Junwei Zheng, Jiaming Zhang, Jürgen Beyerer*

Main category: cs.CV

TL;DR: SAMBLE introduces a shape-specific sampling method for point clouds, balancing edge details and global uniformity, outperforming prior methods in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of prior learning-based sampling methods, which either produce unrecognizable patterns or biased results by ignoring shape-specific variations.

Method: Proposes SAMBLE, using sparse attention maps and bin-based learning to adapt sampling strategies to individual point cloud shapes.

Result: SAMBLE achieves better balance between edge details and global uniformity, excelling in downstream tasks, even with few-point sampling.

Conclusion: SAMBLE offers a robust, shape-aware solution for point cloud sampling, enhancing performance in practical applications.

Abstract: Driven by the increasing demand for accurate and efficient representation of
3D data in various domains, point cloud sampling has emerged as a pivotal
research topic in 3D computer vision. Recently, learning-to-sample methods have
garnered growing interest from the community, particularly for their ability to
be jointly trained with downstream tasks. However, previous learning-based
sampling methods either lead to unrecognizable sampling patterns by generating
a new point cloud or biased sampled results by focusing excessively on sharp
edge details. Moreover, they all overlook the natural variations in point
distribution across different shapes, applying a similar sampling strategy to
all point clouds. In this paper, we propose a Sparse Attention Map and
Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling
strategies for point cloud shapes. SAMBLE effectively achieves an improved
balance between sampling edge points for local details and preserving
uniformity in the global shape, resulting in superior performance across
multiple common point cloud downstream tasks, even in scenarios with few-point
sampling.

</details>


### [425] [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/pdf/2505.02746)
*Simon Ging, Sebastian Walter, Jelena Bratulić, Johannes Dienert, Hannah Bast, Thomas Brox*

Main category: cs.CV

TL;DR: Training CLIP models with less data using smart web search and knowledge graphs, achieving robust results with 10M images for domain-specific tasks and introducing EntityNet for faster generic model training.


<details>
  <summary>Details</summary>
Motivation: High-quality CLIP models require large datasets, limiting domain-specific applications and increasing costs. This work addresses the need for fine-grained control and reduced data requirements.

Method: Employ smart web search strategies enhanced with knowledge graphs to train CLIP models with less data (10M images for domain-specific tasks). Introduce EntityNet (33M images, 46M text pairs) for faster generic model training.

Result: Demonstrated ability to train robust CLIP models from scratch with significantly less data (10M images) and reduced training time using EntityNet.

Conclusion: Smart data collection strategies and EntityNet enable efficient CLIP model training, reducing data needs and costs while maintaining quality.

Abstract: Training high-quality CLIP models typically requires enormous datasets, which
limits the development of domain-specific models -- especially in areas that
even the largest CLIP models do not cover well -- and drives up training costs.
This poses challenges for scientific research that needs fine-grained control
over the training procedure of CLIP models. In this work, we show that by
employing smart web search strategies enhanced with knowledge graphs, a robust
CLIP model can be trained from scratch with considerably less data.
Specifically, we demonstrate that an expert foundation model for living
organisms can be built using just 10M images. Moreover, we introduce EntityNet,
a dataset comprising 33M images paired with 46M text descriptions, which
enables the training of a generic CLIP model in significantly reduced time.

</details>


### [426] [MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection](https://arxiv.org/pdf/2505.04594)
*Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu*

Main category: cs.CV

TL;DR: MonoCoP improves monocular 3D object detection by sequentially predicting 3D attributes using a Chain-of-Prediction (CoP) approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook the inter-correlation of 3D attributes, limiting accuracy. MonoCoP addresses this by conditioning predictions on prior attributes.

Method: MonoCoP uses AttributeNets for feature learning, constructs a feature propagation chain, and employs residual connections to aggregate features.

Result: Achieves SoTA on KITTI and outperforms methods on Waymo and nuScenes datasets.

Conclusion: Sequential and conditional prediction of 3D attributes enhances accuracy and stability in monocular 3D object detection.

Abstract: Accurately predicting 3D attributes is crucial for monocular 3D object
detection (Mono3D), with depth estimation posing the greatest challenge due to
the inherent ambiguity in mapping 2D images to 3D space. While existing methods
leverage multiple depth cues (e.g., estimating depth uncertainty, modeling
depth error) to improve depth accuracy, they overlook that accurate depth
prediction requires conditioning on other 3D attributes, as these attributes
are intrinsically inter-correlated through the 3D to 2D projection, which
ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought
(CoT) in large language models (LLMs), this paper proposes MonoCoP, which
leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and
conditionally via three key designs. First, it employs a lightweight
AttributeNet (AN) for each 3D attribute to learn attribute-specific features.
Next, MonoCoP constructs an explicit chain to propagate these learned features
from one attribute to the next. Finally, MonoCoP uses a residual connection to
aggregate features for each attribute along the chain, ensuring that later
attribute predictions are conditioned on all previously processed attributes
without forgetting the features of earlier ones. Experimental results show that
our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI
leaderboard without requiring additional data and further surpasses existing
methods on the Waymo and nuScenes frontal datasets.

</details>


### [427] [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/pdf/2505.05528)
*Hanxun Huang, Sarah Erfani, Yige Li, Xingjun Ma, James Bailey*

Main category: cs.CV

TL;DR: X-Transfer is a novel attack method exposing universal adversarial vulnerability in CLIP models, achieving super transferability across data, domains, models, and tasks via surrogate scaling.


<details>
  <summary>Details</summary>
Motivation: CLIP models' susceptibility to adversarial perturbations is a critical concern as they are widely adopted in downstream tasks and VLMs.

Method: X-Transfer generates Universal Adversarial Perturbations (UAPs) using surrogate scaling, dynamically selecting suitable surrogates for efficiency.

Result: X-Transfer outperforms existing UAP methods, setting a new benchmark for adversarial transferability in CLIP models.

Conclusion: X-Transfer demonstrates superior adversarial transferability, highlighting vulnerabilities in CLIP models and advancing attack methodologies.

Abstract: As Contrastive Language-Image Pre-training (CLIP) models are increasingly
adopted for diverse downstream tasks and integrated into large vision-language
models (VLMs), their susceptibility to adversarial perturbations has emerged as
a critical concern. In this work, we introduce \textbf{X-Transfer}, a novel
attack method that exposes a universal adversarial vulnerability in CLIP.
X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of
deceiving various CLIP encoders and downstream VLMs across different samples,
tasks, and domains. We refer to this property as \textbf{super
transferability}--a single perturbation achieving cross-data, cross-domain,
cross-model, and cross-task adversarial transferability simultaneously. This is
achieved through \textbf{surrogate scaling}, a key innovation of our approach.
Unlike existing methods that rely on fixed surrogate models, which are
computationally intensive to scale, X-Transfer employs an efficient surrogate
scaling strategy that dynamically selects a small subset of suitable surrogates
from a large search space. Extensive evaluations demonstrate that X-Transfer
significantly outperforms previous state-of-the-art UAP methods, establishing a
new benchmark for adversarial transferability across CLIP models. The code is
publicly available in our
\href{https://github.com/HanxunH/XTransferBench}{GitHub repository}.

</details>


### [428] [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](https://arxiv.org/pdf/2505.10238)
*Yanbo Ding, Xirui Hu, Zhizhi Guo, Chi Zhang, Yali Wang*

Main category: cs.CV

TL;DR: MTVCrafter introduces 4D motion tokenization for human image animation, outperforming 2D methods with better generalization and control.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on 2D-rendered pose images, limiting generalization and discarding 3D information. MTVCrafter aims to address this by modeling raw 3D motion sequences.

Method: Proposes 4DMoT for quantizing 3D motion into tokens and MV-DiT, a motion-aware video model with 4D positional encodings.

Result: Achieves state-of-the-art results (FID-VID 6.98), surpassing the second-best by 65%, and generalizes well to diverse characters and scenarios.

Conclusion: MTVCrafter advances human image animation by leveraging 4D motion tokens, opening new directions for pose-guided video generation.

Abstract: Human image animation has gained increasing attention and developed rapidly
due to its broad applications in digital humans. However, existing methods rely
largely on 2D-rendered pose images for motion guidance, which limits
generalization and discards essential 3D information for open-world animation.
To tackle this problem, we propose MTVCrafter (Motion Tokenization Video
Crafter), the first framework that directly models raw 3D motion sequences
(i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT
(4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens.
Compared to 2D-rendered pose images, 4D motion tokens offer more robust
spatio-temporal cues and avoid strict pixel-level alignment between pose image
and character, enabling more flexible and disentangled control. Then, we
introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention
with 4D positional encodings, MV-DiT can effectively leverage motion tokens as
4D compact yet expressive context for human image animation in the complex 3D
world. Hence, it marks a significant step forward in this field and opens a new
direction for pose-guided human video generation. Experiments show that our
MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98,
surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter
also generalizes well to diverse open-world characters (single/multiple,
full/half-body) across various styles and scenarios. Our video demos and code
are on: https://github.com/DINGYANB/MTVCrafter.

</details>


### [429] [Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner](https://arxiv.org/pdf/2505.11404)
*Wenchuan Zhang, Penghao Zhang, Jingru Guo, Tao Cheng, Jie Chen, Shuwan Zhang, Zhang Zhang, Yuhao Yi, Hong Bu*

Main category: cs.CV

TL;DR: The paper introduces Patho-R1, a multimodal RL-based pathology Reasoner, and Patho-CLIP, addressing limitations in current pathology VLMs by leveraging high-quality datasets and a three-stage training pipeline for improved diagnostic accuracy and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current pathology-specific VLMs lack diagnostic accuracy and reasoning plausibility due to shallow datasets. The study aims to improve this by using pathology textbooks and expert input to create reasoning-oriented datasets.

Method: A three-stage pipeline: (1) continued pretraining on 3.5M image-text pairs, (2) supervised fine-tuning on 500k Chain-of-Thought samples, (3) reinforcement learning with advanced strategies for multimodal reasoning refinement.

Result: Patho-R1 and Patho-CLIP achieve robust performance in tasks like zero-shot classification, cross-modal retrieval, VQA, and MCQs.

Conclusion: The study successfully addresses limitations in pathology VLMs by introducing high-quality datasets and a refined training approach, demonstrating strong performance across multiple tasks.

Abstract: Recent advances in vision language models (VLMs) have enabled broad progress
in the general medical field. However, pathology still remains a more
challenging subdomain, with current pathology specific VLMs exhibiting
limitations in both diagnostic accuracy and reasoning plausibility. Such
shortcomings are largely attributable to the nature of current pathology
datasets, which are primarily composed of image description pairs that lack the
depth and structured diagnostic paradigms employed by real world pathologists.
In this study, we leverage pathology textbooks and real world pathology experts
to construct high-quality, reasoning-oriented datasets. Building on this, we
introduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a
three-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs
for knowledge infusion; (2) supervised fine-tuning on 500k high-quality
Chain-of-Thought samples for reasoning incentivizing; (3) reinforcement
learning using Group Relative Policy Optimization and Decoupled Clip and
Dynamic sAmpling Policy Optimization strategies for multimodal reasoning
quality refinement. To further assess the alignment quality of our dataset, we
propose Patho-CLIP, trained on the same figure-caption corpus used for
continued pretraining. Comprehensive experimental results demonstrate that both
Patho-CLIP and Patho-R1 achieve robust performance across a wide range of
pathology-related tasks, including zero-shot classification, cross-modal
retrieval, Visual Question Answering, and Multiple Choice Question. Our project
is available at the Patho-R1 repository:
https://github.com/Wenchuan-Zhang/Patho-R1.

</details>


### [430] [VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought](https://arxiv.org/pdf/2505.16192)
*Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang*

Main category: cs.CV

TL;DR: VLM-R$^3$ enhances MLLMs by enabling dynamic visual region selection and integration into reasoning chains, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with complex tasks requiring iterative visual grounding, prompting the need for a framework like VLM-R$^3$.

Method: VLM-R$^3$ uses Region-Conditioned Reinforcement Policy Optimization (R-GRPO) and a curated VLIR corpus for training.

Result: Achieves state-of-the-art performance on MathVista and ScienceQA, especially in tasks needing spatial reasoning.

Conclusion: VLM-R$^3$ effectively bridges visual and textual reasoning, setting a new benchmark for MLLMs.

Abstract: Recently, reasoning-based MLLMs have achieved a degree of success in
generating long-form textual reasoning chains. However, they still struggle
with complex tasks that necessitate dynamic and iterative focusing on and
revisiting of visual regions to achieve precise grounding of textual reasoning
in visual evidence. We introduce \textbf{VLM-R$^3$} (\textbf{V}isual
\textbf{L}anguage \textbf{M}odel with \textbf{R}egion \textbf{R}ecognition and
\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)
decide \emph{when} additional visual evidence is needed, (ii) determine
\emph{where} to ground within the image, and (iii) seamlessly weave the
relevant sub-image content back into an interleaved chain-of-thought. The core
of our method is \textbf{Region-Conditioned Reinforcement Policy Optimization
(R-GRPO)}, a training paradigm that rewards the model for selecting informative
regions, formulating appropriate transformations (e.g.\ crop, zoom), and
integrating the resulting visual context into subsequent reasoning steps. To
bootstrap this policy, we compile a modest but carefully curated Visuo-Lingual
Interleaved Rationale (VLIR) corpus that provides step-level supervision on
region selection and textual justification. Extensive experiments on MathVista,
ScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art
in zero-shot and few-shot settings, with the largest gains appearing on
questions demanding subtle spatial reasoning or fine-grained visual cue
extraction.

</details>


### [431] [T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models](https://arxiv.org/pdf/2505.17550)
*Xiaoyu Ye, Songjie Cheng, Yongtao Wang, Yajiao Xiong, Yishen Li*

Main category: cs.CV

TL;DR: A method for unlearning harmful or explicit concepts in text-to-video (T2V) models is proposed, combining negatively-guided velocity prediction and prompt augmentation for robustness and precision.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse and rights violations due to T2V models generating harmful content, inspired by unlearning techniques in text-to-image models.

Method: Uses negatively-guided velocity prediction fine-tuning with prompt augmentation, plus localization and preservation regularization for precision.

Result: Effectively erases specific harmful concepts while preserving other generation capabilities, outperforming existing methods.

Conclusion: The proposed unlearning method is robust and precise, offering a solution to mitigate misuse of T2V models.

Abstract: Recent advances in text-to-video (T2V) diffusion models have significantly
enhanced the quality of generated videos. However, their ability to produce
explicit or harmful content raises concerns about misuse and potential rights
violations. Inspired by the success of unlearning techniques in erasing
undesirable concepts from text-to-image (T2I) models, we extend unlearning to
T2V models and propose a robust and precise unlearning method. Specifically, we
adopt negatively-guided velocity prediction fine-tuning and enhance it with
prompt augmentation to ensure robustness against LLM-refined prompts. To
achieve precise unlearning, we incorporate a localization and a preservation
regularization to preserve the model's ability to generate non-target concepts.
Extensive experiments demonstrate that our method effectively erases a specific
concept while preserving the model's generation capability for all other
concepts, outperforming existing methods. We provide the unlearned models in
\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.

</details>


### [432] [Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery](https://arxiv.org/pdf/2505.17677)
*Ming Hu, Zhengdi Yu, Feilong Tang, Kaiwen Chen, Yulong Li, Imran Razzak, Junjun He, Tolga Birdal, Kaijing Zhou, Zongyuan Ge*

Main category: cs.CV

TL;DR: OphNet-3D is a large-scale RGB-D dataset for 3D reconstruction in ophthalmic surgery, featuring fine-grained annotations and an automatic annotation pipeline. It introduces benchmarks and models (H-Net and OH-Net) that outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of realistic datasets and reliable annotation tools for 3D reconstruction in ophthalmic surgery motivated the creation of OphNet-3D.

Method: A multi-stage automatic annotation pipeline integrates multi-view data, motion priors, biomechanical constraints, and collision-aware interaction constraints. H-Net and OH-Net models are proposed for hand and instrument reconstruction.

Result: OphNet-3D includes 7.1 million frames with detailed annotations. H-Net and OH-Net achieve over 2mm improvement in MPJPE and up to 23% in ADD-S metrics.

Conclusion: OphNet-3D and the proposed models significantly advance 3D reconstruction in ophthalmic surgery, setting new benchmarks for accuracy.

Abstract: Accurate 3D reconstruction of hands and instruments is critical for
vision-based analysis of ophthalmic microsurgery, yet progress has been
hampered by the lack of realistic, large-scale datasets and reliable annotation
tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic
3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from
40 surgeons and totaling 7.1 million frames, with fine-grained annotations of
12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full
6-DoF instrument poses. To scalably produce high-fidelity labels, we design a
multi-stage automatic annotation pipeline that integrates multi-view data
observation, data-driven motion prior with cross-view geometric consistency and
biomechanical constraints, along with a combination of collision-aware
interaction constraints for instrument interactions. Building upon OphNet-3D,
we establish two challenging benchmarks-bimanual hand pose estimation and
hand-instrument interaction reconstruction-and propose two dedicated
architectures: H-Net for dual-hand mesh recovery and OH-Net for joint
reconstruction of two-hand-two-instrument interactions. These models leverage a
novel spatial reasoning module with weak-perspective camera modeling and
collision-aware center-based representation. Both architectures outperform
existing methods by substantial margins, achieving improvements of over 2mm in
Mean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand
and instrument reconstruction, respectively.

</details>


### [433] [U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding](https://arxiv.org/pdf/2505.17779)
*Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo*

Main category: cs.CV

TL;DR: U2-BENCH is the first benchmark to evaluate large vision-language models (LVLMs) on ultrasound tasks, revealing strengths in classification but weaknesses in spatial reasoning and clinical language generation.


<details>
  <summary>Details</summary>
Motivation: Ultrasound interpretation is challenging due to variable image quality and noise. LVLMs' performance in this domain is unexplored.

Method: U2-BENCH evaluates 20 LVLMs on 7,241 cases across 15 anatomical regions and 8 clinical tasks.

Result: LVLMs perform well in image-level classification but struggle with spatial reasoning and clinical language generation.

Conclusion: U2-BENCH provides a unified testbed to advance LVLM research in medical ultrasound.

Abstract: Ultrasound is a widely-used imaging modality critical to global healthcare,
yet its interpretation remains challenging due to its varying image quality on
operators, noises, and anatomical structures. Although large vision-language
models (LVLMs) have demonstrated impressive multimodal capabilities across
natural and medical domains, their performance on ultrasound remains largely
unexplored. We introduce U2-BENCH, the first comprehensive benchmark to
evaluate LVLMs on ultrasound understanding across classification, detection,
regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning
15 anatomical regions and defines 8 clinically inspired tasks, such as
diagnosis, view recognition, lesion localization, clinical value estimation,
and report generation, across 50 ultrasound application scenarios. We evaluate
20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and
medical-specific. Our results reveal strong performance on image-level
classification, but persistent challenges in spatial reasoning and clinical
language generation. U2-BENCH establishes a rigorous and unified testbed to
assess and accelerate LVLM research in the uniquely multimodal domain of
medical ultrasound imaging.

</details>


### [434] [JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models](https://arxiv.org/pdf/2505.19610)
*Jiaxin Song, Yixu Wang, Jie Li, Rui Yu, Yan Teng, Xingjun Ma, Yingchun Wang*

Main category: cs.CV

TL;DR: JailBound is a novel latent space jailbreak framework for Vision-Language Models (VLMs) that exploits an implicit safety decision boundary in the latent space to achieve high attack success rates.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods for VLMs lack precise directional guidance and neglect cross-modal interactions, limiting their effectiveness.

Method: JailBound involves two stages: (1) Safety Boundary Probing to approximate the decision boundary and identify optimal perturbation directions, and (2) Safety Boundary Crossing to jointly optimize perturbations across image and text inputs.

Result: JailBound achieves 94.32% white-box and 67.28% black-box attack success rates, outperforming state-of-the-art methods by 6.17% and 21.13%, respectively.

Conclusion: The study reveals a significant safety risk in VLMs and underscores the need for more robust defenses.

Abstract: Vision-Language Models (VLMs) exhibit impressive performance, yet the
integration of powerful vision encoders has significantly broadened their
attack surface, rendering them increasingly susceptible to jailbreak attacks.
However, lacking well-defined attack objectives, existing jailbreak methods
often struggle with gradient-based strategies prone to local optima and lacking
precise directional guidance, and typically decouple visual and textual
modalities, thereby limiting their effectiveness by neglecting crucial
cross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)
framework, we posit that VLMs encode safety-relevant information within their
internal fusion-layer representations, revealing an implicit safety decision
boundary in the latent space. This motivates exploiting boundary to steer model
behavior. Accordingly, we propose JailBound, a novel latent space jailbreak
framework comprising two stages: (1) Safety Boundary Probing, which addresses
the guidance issue by approximating decision boundary within fusion layer's
latent space, thereby identifying optimal perturbation directions towards the
target region; and (2) Safety Boundary Crossing, which overcomes the
limitations of decoupled approaches by jointly optimizing adversarial
perturbations across both image and text inputs. This latter stage employs an
innovative mechanism to steer the model's internal state towards
policy-violating outputs while maintaining cross-modal semantic consistency.
Extensive experiments on six diverse VLMs demonstrate JailBound's efficacy,
achieves 94.32% white-box and 67.28% black-box attack success averagely, which
are 6.17% and 21.13% higher than SOTA methods, respectively. Our findings
expose a overlooked safety risk in VLMs and highlight the urgent need for more
robust defenses. Warning: This paper contains potentially sensitive, harmful
and offensive content.

</details>


### [435] [ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization](https://arxiv.org/pdf/2505.19883)
*Shintaro Ito, Natsuki Takama, Koichi Ito, Hwann-Tzong Chen, Takafumi Aoki*

Main category: cs.CV

TL;DR: ErpGS improves 3D reconstruction from 360-degree camera images by addressing distortion issues in 3D Gaussian Splatting (3DGS), enhancing rendering accuracy with techniques like geometric and scale regularization.


<details>
  <summary>Details</summary>
Motivation: Overcoming the large distortion in equirectangular images from 360-degree cameras, which causes poor rendering accuracy in 3DGS-based methods.

Method: Proposes ErpGS, an omnidirectional GS based on 3DGS, with techniques like geometric regularization, scale regularization, distortion-aware weights, and obstacle suppression masks.

Result: ErpGS outperforms conventional methods in rendering novel view images, as validated on public datasets.

Conclusion: ErpGS effectively addresses distortion challenges in 360-degree camera images, improving rendering accuracy for novel view synthesis.

Abstract: The use of multi-view images acquired by a 360-degree camera can reconstruct
a 3D space with a wide area. There are 3D reconstruction methods from
equirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis
(NVS) methods. On the other hand, it is necessary to overcome the large
distortion caused by the projection model of a 360-degree camera when
equirectangular images are used. In 3DGS-based methods, the large distortion of
the 360-degree camera model generates extremely large 3D Gaussians, resulting
in poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based
on 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering
accuracy improvement techniques: geometric regularization, scale
regularization, and distortion-aware weights and a mask to suppress the effects
of obstacles in equirectangular images. Through experiments on public datasets,
we demonstrate that ErpGS can render novel view images more accurately than
conventional methods.

</details>


### [436] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/pdf/2505.21541)
*Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, Yiren Song*

Main category: cs.CV

TL;DR: The paper introduces DiffDecompose, a diffusion Transformer-based framework for decomposing semi-transparent/transparent layers in images, supported by the new AlphaBlend dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with semi-transparent/transparent layer decomposition due to mask dependencies, static assumptions, and lack of datasets.

Method: Proposes DiffDecompose, a framework using diffusion Transformers to learn layer decompositions conditioned on input images, semantic prompts, and blending types. Introduces In-Context Decomposition and Layer Position Encoding Cloning.

Result: Extensive experiments on AlphaBlend and LOGO datasets show DiffDecompose's effectiveness.

Conclusion: DiffDecompose addresses layer ambiguity and data scarcity, offering a robust solution for transparent layer decomposition.

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [437] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/pdf/2505.21649)
*Keanu Nichols, Nazia Tasnim, Yuting Yan, Nicholas Ikechukwu, Elva Zou, Deepti Ghadiyaram, Bryan A. Plummer*

Main category: cs.CV

TL;DR: DORI is a benchmark for evaluating object orientation understanding in vision-language models, revealing significant limitations in current systems.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks conflate object orientation with other tasks, failing to isolate this critical capability for applications like robotics and AR.

Method: DORI assesses orientation comprehension across four dimensions using curated tasks from 11 datasets, evaluating 15 state-of-the-art models.

Result: Best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular ones, showing poor performance on reference frame shifts and compound rotations.

Conclusion: DORI highlights the need for dedicated orientation mechanisms in models, with implications for robotics, 3D reconstruction, and human-AI interaction.

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [438] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/pdf/2505.21850)
*Yanbei Jiang, Yihao Ding, Chao Lei, Jiayang Ao, Jey Han Lau, Krista A. Ehinger*

Main category: cs.CV

TL;DR: The paper introduces MultiStAR, a benchmark for Multi-Stage Abstract Visual Reasoning (AVR), and MSEval, a metric evaluating intermediate reasoning steps, revealing MLLMs' limitations in complex rule detection.


<details>
  <summary>Details</summary>
Motivation: Existing AVR benchmarks and metrics overlook the multi-stage reasoning process, leaving gaps in understanding MLLMs' failures in abstract reasoning.

Method: Developed MultiStAR based on RAVEN to assess reasoning complexity and proposed MSEval to evaluate intermediate steps. Tested 17 MLLMs.

Result: MLLMs perform well on basic perception but struggle with complex rule detection, highlighting their limitations in AVR.

Conclusion: The study underscores the need for benchmarks and metrics that account for multi-stage reasoning to better evaluate MLLMs in AVR.

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [439] [Autoregression-free video prediction using diffusion model for mitigating error propagation](https://arxiv.org/pdf/2505.22111)
*Woonho Ko, Jin Bok Park, Il Yong Chun*

Main category: cs.CV

TL;DR: Proposes an AutoRegression-Free (ARFree) video prediction framework using diffusion models to avoid error propagation in long-term predictions.


<details>
  <summary>Details</summary>
Motivation: Autoregressive methods suffer from error propagation in distant future frames, limiting long-term video prediction accuracy.

Method: ARFree directly predicts future frames from context frames using a motion prediction module and a training method for motion continuity and contextual consistency.

Result: Outperforms state-of-the-art methods on two benchmark datasets.

Conclusion: ARFree offers a promising alternative to autoregressive methods for accurate long-term video prediction.

Abstract: Existing long-term video prediction methods often rely on an autoregressive
video prediction mechanism. However, this approach suffers from error
propagation, particularly in distant future frames. To address this limitation,
this paper proposes the first AutoRegression-Free (ARFree) video prediction
framework using diffusion models. Different from an autoregressive video
prediction mechanism, ARFree directly predicts any future frame tuples from the
context frame tuple. The proposed ARFree consists of two key components: 1) a
motion prediction module that predicts a future motion using motion feature
extracted from the context frame tuple; 2) a training method that improves
motion continuity and contextual consistency between adjacent future frame
tuples. Our experiments with two benchmark datasets show that the proposed
ARFree video prediction framework outperforms several state-of-the-art video
prediction methods.

</details>


### [440] [Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning](https://arxiv.org/pdf/2505.22465)
*Zobia Batool, Huseyin Ozkan, Erchan Aptoula*

Main category: cs.CV

TL;DR: The paper proposes a method using pseudo-morphological modules and contrastive learning to improve Alzheimer's disease detection in MRIs, addressing class imbalance and domain generalization challenges.


<details>
  <summary>Details</summary>
Motivation: Challenges like class imbalance, protocol variations, and limited dataset diversity hinder the generalization of deep learning models for Alzheimer's detection in MRIs.

Method: The approach combines learnable pseudo-morphological modules for shape-aware augmentations with supervised contrastive learning to extract robust class-specific representations.

Result: Experiments on three datasets demonstrate improved performance and generalization, especially under class imbalance and protocol variations.

Conclusion: The proposed method enhances Alzheimer's detection by addressing domain generalization issues, with promising results across diverse datasets.

Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly
thanks to contemporary deep learning models, challenges such as class
imbalance, protocol variations, and limited dataset diversity often hinder
their generalization capacity. To address this issue, this article focuses on
the single domain generalization setting, where given the data of one domain, a
model is designed and developed with maximal performance w.r.t. an unseen
domain of distinct distribution. Since brain morphology is known to play a
crucial role in Alzheimer's diagnosis, we propose the use of learnable
pseudo-morphological modules aimed at producing shape-aware, anatomically
meaningful class-specific augmentations in combination with a supervised
contrastive learning module to extract robust class-specific representations.
Experiments conducted across three datasets show improved performance and
generalization capacity, especially under class imbalance and imaging protocol
variations. The source code will be made available upon acceptance at
https://github.com/zobia111/SDG-Alzheimer.

</details>


### [441] [Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective](https://arxiv.org/pdf/2505.22604)
*Ruixuan Zhang, He Wang, Zhengyu Zhao, Zhiqing Guo, Xun Yang, Yunfeng Diao, Meng Wang*

Main category: cs.CV

TL;DR: The paper identifies performance collapse in adversarial training for AIGI detection, proposes TRIM, a training-free defense using information-theoretic measures, and validates its superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated images (AIGI) has led to misuse, necessitating robust detection methods. Existing detectors are vulnerable to adversarial attacks, and defenses are limited.

Method: The authors propose TRIM, a training-free adversarial defense for AIGI detection, leveraging prediction entropy and KL divergence to quantify feature shifts.

Result: TRIM outperforms state-of-the-art defenses by significant margins (e.g., 33.88% on ProGAN) while maintaining original accuracy.

Conclusion: TRIM offers an effective, training-free solution for robust AIGI detection, addressing the limitations of adversarial training.

Abstract: Rapid advances in Artificial Intelligence Generated Images (AIGI) have
facilitated malicious use, such as forgery and misinformation. Therefore,
numerous methods have been proposed to detect fake images. Although such
detectors have been proven to be universally vulnerable to adversarial attacks,
defenses in this field are scarce. In this paper, we first identify that
adversarial training (AT), widely regarded as the most effective defense,
suffers from performance collapse in AIGI detection. Through an
information-theoretic lens, we further attribute the cause of collapse to
feature entanglement, which disrupts the preservation of feature-label mutual
information. Instead, standard detectors show clear feature separation.
Motivated by this difference, we propose Training-free Robust Detection via
Information-theoretic Measures (TRIM), the first training-free adversarial
defense for AIGI detection. TRIM builds on standard detectors and quantifies
feature shifts using prediction entropy and KL divergence. Extensive
experiments across multiple datasets and attacks validate the superiority of
our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)
on ProGAN (GenImage), while well maintaining original accuracy.

</details>


### [442] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/pdf/2505.22815)
*Zhangyi Hu, Jiemin Wu, Hua Xu, Mingqian Liao, Ninghui Feng, Bo Gao, Songning Lai, Yutao Yue*

Main category: cs.CV

TL;DR: VIMTS adapts Visual MAE for Irregular Multivariate Time Series (IMTS) forecasting, handling missing data and unaligned signals via patch processing and cross-channel dependencies, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with IMTS due to missing data and unaligned signals, while pre-trained models are designed for regular time series. Visual MAE's success in sparse data motivates its adaptation.

Method: VIMTS processes IMTS into feature patches, uses cross-channel dependencies to fill gaps, leverages Visual MAE for reconstruction, and employs coarse-to-fine prediction. Self-supervised learning is integrated.

Result: VIMTS outperforms existing methods, demonstrating superior performance and few-shot capability in IMTS forecasting.

Conclusion: VIMTS advances visual foundation models' application in general time series tasks, offering a robust solution for IMTS forecasting.

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [443] [Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape](https://arxiv.org/pdf/2505.22918)
*Ruichen Chen, Keith G. Mills, Liyao Jiang, Chao Gao, Di Niu*

Main category: cs.CV

TL;DR: Re-ttention introduces high sparsity in attention for Diffusion Transformers, preserving quality while reducing compute overhead.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic complexity bottleneck in attention mechanisms for visual generation models.

Method: Leverages temporal redundancy in Diffusion Models to reshape attention scores based on prior softmax distributions.

Result: Achieves 3.1% token usage, outperforming other methods, with 45% end-to-end and 92% self-attention latency reduction.

Conclusion: Re-ttention effectively reduces compute overhead without compromising visual quality in high sparsity scenarios.

Abstract: Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. % To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. % Experimental
results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate
that Re-ttention requires as few as 3.1\% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference. Further, we measure latency to show that our method can attain over
45\% end-to-end % and over 92\% self-attention latency reduction on an H100 GPU
at negligible overhead cost.
  Code available online here:
\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}

</details>


### [444] [WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver Assistance Systems](https://arxiv.org/pdf/2505.23201)
*Hao Wu, Junzhou Chen, Ronghui Zhang, Nengchao Lyu, Hongyu Hu, Yanyong Guo, Tony Z. Qiu*

Main category: cs.CV

TL;DR: WTEFNet is a real-time object detection framework for low-light conditions, combining low-light enhancement, wavelet-based feature extraction, and adaptive fusion for robust performance.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-based object detection methods perform poorly in low-light conditions, limiting ADAS effectiveness.

Method: WTEFNet uses three modules: Low-Light Enhancement (LLE), Wavelet-based Feature Extraction (WFE), and Adaptive Fusion Detection (AFFD).

Result: Achieves state-of-the-art accuracy on datasets like BDD100K and GSN, and runs in real-time on embedded platforms.

Conclusion: WTEFNet is effective for low-light object detection in ADAS, with practical deployment potential.

Abstract: Object detection is a cornerstone of environmental perception in advanced
driver assistance systems(ADAS). However, most existing methods rely on RGB
cameras, which suffer from significant performance degradation under low-light
conditions due to poor image quality. To address this challenge, we proposes
WTEFNet, a real-time object detection framework specifically designed for
low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet
comprises three core modules: a Low-Light Enhancement (LLE) module, a
Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection
(AFFD) module. The LLE enhances dark regions while suppressing overexposed
areas; the WFE applies multi-level discrete wavelet transforms to isolate high-
and low-frequency components, enabling effective denoising and structural
feature retention; the AFFD fuses semantic and illumination features for robust
detection. To support training and evaluation, we introduce GSN, a manually
annotated dataset covering both clear and rainy night-time scenes. Extensive
experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet
achieves state-of-the-art accuracy under low-light conditions. Furthermore,
deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the
framework's suitability for real-time ADAS applications.

</details>


### [445] [Are MLMs Trapped in the Visual Room?](https://arxiv.org/pdf/2505.23272)
*Yazhou Zhang, Chunwang Zou, Qimeng Liu, Lu Rong, Ben Yao, Zheng Lian, Qiuchi Li, Peng Zhang, Jing Qin*

Main category: cs.CV

TL;DR: The paper questions whether MLMs truly understand images, proposing the Visual Room argument. It introduces a two-tier evaluation framework (perception and cognition) and a sarcasm dataset, finding MLMs excel in perception but struggle with sarcasm understanding.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that perceptual mastery in MLMs equates to genuine understanding, inspired by Searle's Chinese Room.

Method: A two-tier evaluation framework (perception and cognition) tested on 8 SoTA MLMs using a new multi-modal sarcasm dataset (924 images, 100 videos).

Result: MLMs perform well in perception (~high accuracy) but poorly in sarcasm understanding (~17.1% error rate), revealing a gap due to weak context integration, emotional reasoning, and pragmatic inference.

Conclusion: The Visual Room argument is empirically supported, highlighting the need for improved cognitive evaluation in MLMs.

Abstract: Can multi-modal large models (MLMs) that can ``see'' an image be said to
``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose
the \textbf{Visual Room} argument: a system may process and describe every
detail of visual inputs by following algorithmic rules, without genuinely
comprehending the underlying intention. This dilemma challenges the prevailing
assumption that perceptual mastery implies genuine understanding. In
implementation, we introduce a two-tier evaluation framework spanning
perception and cognition. The perception component evaluates whether MLMs can
accurately capture the surface-level details of visual contents, where the
cognitive component examines their ability to infer sarcasm polarity. To
support this framework, We further introduce a high-quality multi-modal sarcasm
dataset comprising both 924 static images and 100 dynamic videos. All sarcasm
labels are annotated by the original authors and verified by independent
reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art
(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate
high accuracy in visual perception; (2) even with correct perception, MLMs
exhibit an average error rate of ~17.1\% in sarcasm understanding, revealing a
significant gap between seeing and understanding; (3) this gap stems from
weaknesses in context integration, emotional reasoning, and pragmatic
inference. This work provides empirical grounding for the proposed Visual Room
argument and offers a new evaluation paradigm for MLMs.

</details>


### [446] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/pdf/2505.23558)
*Xu Chu, Xinrong Chen, Guanyu Wang, Zhijie Tan, Kui Huang, Wenyu Lv, Tong Mo, Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LA introduces a vision-text reflection process to mitigate hallucinations in Vision-Language Reasoning Models (VLRMs), enhancing visual attention during reasoning.


<details>
  <summary>Details</summary>
Motivation: Long reasoning in VLMs dilutes visual tokens, reducing attention to visual information and triggering hallucinations. Text-only reflection is insufficient to address this.

Method: Qwen-LA uses Balanced Reflective Policy Optimization (BRPO) to guide vision-text reflection and introduces Visual Token COPY and ROUTE to reinforce visual attention.

Result: Qwen-LA achieves leading accuracy on visual QA datasets and reduces hallucinations.

Conclusion: Incorporating vision-text reflection and visual token reinforcement effectively mitigates hallucinations in VLRMs.

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at: https://github.com/Liar406/Look_Again

</details>


### [447] [OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation](https://arxiv.org/pdf/2505.23661)
*Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, Chen Change Loy*

Main category: cs.CV

TL;DR: OpenUni is a lightweight, open-source baseline for multimodal understanding and generation, using a simple architecture to unify LLMs and diffusion models, achieving high performance with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To create a simple, efficient, and open-source solution for unifying multimodal tasks, reducing training complexity while maintaining high performance.

Method: Uses learnable queries and a lightweight transformer-based connector to bridge multimodal LLMs and diffusion models, with a minimalist architecture.

Result: Generates high-quality, instruction-aligned images and excels on benchmarks (GenEval, DPG-Bench, WISE) with only 1.1B and 3.1B parameters.

Conclusion: OpenUni is a promising, efficient, and open solution for multimodal tasks, with released resources to support community research.

Abstract: In this report, we present OpenUni, a simple, lightweight, and fully
open-source baseline for unifying multimodal understanding and generation.
Inspired by prevailing practices in unified model learning, we adopt an
efficient training strategy that minimizes the training complexity and overhead
by bridging the off-the-shelf multimodal large language models (LLMs) and
diffusion models through a set of learnable queries and a light-weight
transformer-based connector. With a minimalist choice of architecture, we
demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned
images, and 2) achieve exceptional performance on standard benchmarks such as
GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To
support open research and community advancement, we release all model weights,
training code, and our curated training datasets (including 23M image-text
pairs) at https://github.com/wusize/OpenUni.

</details>


### [448] [ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS](https://arxiv.org/pdf/2505.23734)
*Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang*

Main category: cs.CV

TL;DR: ZPressor improves feed-forward 3D Gaussian Splatting (3DGS) models by compressing multi-view inputs into a compact latent state, enabling scalability to over 100 views on an 80GB GPU.


<details>
  <summary>Details</summary>
Motivation: Feed-forward 3DGS models face scalability issues due to encoder limitations, degrading performance or consuming excessive memory with more input views.

Method: ZPressor partitions views into anchor and support sets, using cross attention to compress support views into anchor views, forming a compact latent state.

Result: ZPressor enhances performance under moderate views and robustness in dense view settings on DL3DV-10K and RealEstate10K benchmarks.

Conclusion: ZPressor is a lightweight, architecture-agnostic solution for efficient multi-view compression in 3DGS models.

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a
promising solution for novel view synthesis, enabling one-pass inference
without the need for per-scene 3DGS optimization. However, their scalability is
fundamentally constrained by the limited capacity of their encoders, leading to
degraded performance or excessive memory consumption as the number of input
views increases. In this work, we analyze feed-forward 3DGS frameworks through
the lens of the Information Bottleneck principle and introduce ZPressor, a
lightweight architecture-agnostic module that enables efficient compression of
multi-view inputs into a compact latent state $Z$ that retains essential scene
information while discarding redundancy. Concretely, ZPressor enables existing
feed-forward 3DGS models to scale to over 100 input views at 480P resolution on
an 80GB GPU, by partitioning the views into anchor and support sets and using
cross attention to compress the information from the support views into anchor
views, forming the compressed latent state $Z$. We show that integrating
ZPressor into several state-of-the-art feed-forward 3DGS models consistently
improves performance under moderate input views and enhances robustness under
dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.
The video results, code and trained models are available on our project page:
https://lhmd.top/zpressor.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [449] [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/pdf/2505.23881)
*Christopher D. Rosin*

Main category: cs.AI

TL;DR: Reasoning LLMs improve combinatorial design problem-solving, solving long-standing open instances and outperforming non-reasoning LLMs.


<details>
  <summary>Details</summary>
Motivation: To leverage reasoning LLMs for solving open combinatorial design problems, which traditional methods and non-reasoning LLMs struggle with.

Method: The Constructive Protocol CPro1 uses reasoning LLMs to generate search heuristics, guided by textual definitions and validity verifiers, with hyperparameter tuning and feedback.

Result: Solved 7 of 16 selected problems from the 2006 Handbook, including 3 new instances, and addressed recent 2025 literature problems.

Conclusion: Reasoning LLMs enhance combinatorial design problem-solving, demonstrating broader applicability and effectiveness.

Abstract: Large Language Models (LLMs) with reasoning are trained to iteratively
generate and refine their answers before finalizing them, which can help with
applications to mathematics and code generation. We apply code generation with
reasoning LLMs to a specific task in the mathematical field of combinatorial
design. This field studies diverse types of combinatorial designs, many of
which have lists of open instances for which existence has not yet been
determined. The Constructive Protocol CPro1 uses LLMs to generate search
heuristics that have the potential to construct solutions to small open
instances. Starting with a textual definition and a validity verifier for a
particular type of design, CPro1 guides LLMs to select and implement
strategies, while providing automated hyperparameter tuning and execution
feedback. CPro1 with reasoning LLMs successfully solves long-standing open
instances for 7 of 16 combinatorial design problems selected from the 2006
Handbook of Combinatorial Designs, including new solved instances for 3 of
these (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary
Designs) that were unsolved by CPro1 with non-reasoning LLMs. It also solves
open instances for several problems from recent (2025) literature, generating
new Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform
Nested Steiner Quadruple System.

</details>


### [450] [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/pdf/2505.23885)
*Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, Guohao Li*

Main category: cs.AI

TL;DR: Workforce introduces a hierarchical multi-agent framework for cross-domain task automation, achieving state-of-the-art performance with modular adaptability and optimized training.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based multi-agent systems struggle with domain transfer due to rigid architectures and retraining needs. Workforce aims to address this by decoupling planning and execution.

Method: Workforce uses a modular architecture with a domain-agnostic Planner, Coordinator, and specialized Workers. It employs Optimized Workforce Learning (OWL) for training.

Result: Workforce achieves 69.70% accuracy on GAIA, outperforming OpenAI's Deep Research by 2.34%. The OWL-trained 32B model shows a 16.37% improvement and matches GPT-4o on challenging tasks.

Conclusion: Workforce enables scalable generalization and modular domain transfer, paving the way for advanced general-purpose AI assistants.

Abstract: Large Language Model (LLM)-based multi-agent systems show promise for
automating real-world tasks but struggle to transfer across domains due to
their domain-specific nature. Current approaches face two critical
shortcomings: they require complete architectural redesign and full retraining
of all components when applied to new domains. We introduce Workforce, a
hierarchical multi-agent framework that decouples strategic planning from
specialized execution through a modular architecture comprising: (i) a
domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask
management, and (iii) specialized Workers with domain-specific tool-calling
capabilities. This decoupling enables cross-domain transferability during both
inference and training phases: During inference, Workforce seamlessly adapts to
new domains by adding or modifying worker agents; For training, we introduce
Optimized Workforce Learning (OWL), which improves generalization across
domains by optimizing a domain-agnostic planner with reinforcement learning
from real-world feedback. To validate our approach, we evaluate Workforce on
the GAIA benchmark, covering various realistic, multi-domain agentic tasks.
Experimental results demonstrate Workforce achieves open-source
state-of-the-art performance (69.70%), outperforming commercial systems like
OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model
achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to
GPT-4o on challenging tasks. To summarize, by enabling scalable generalization
and modular domain transfer, our work establishes a foundation for the next
generation of general-purpose AI assistants.

</details>


### [451] [Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve](https://arxiv.org/pdf/2505.23946)
*Yuanzhe Liu, Ryan Deng, Tim Kaler, Xuhao Chen, Charles E. Leiserson, Yao Ma, Jie Chen*

Main category: cs.AI

TL;DR: A framework for LLM collaboration uses lessons learned from agents' successes and failures to improve performance, outperforming larger LLMs and other methods.


<details>
  <summary>Details</summary>
Motivation: LLMs specialize in different tasks, but leveraging their complementary strengths without prior knowledge is challenging.

Method: Proposes a lesson-based collaboration framework with solicitation, banking, and selection mechanisms.

Result: A team of small LLMs with lessons outperforms larger LLMs and other collaboration methods.

Conclusion: Lesson-based collaboration enhances LLM performance by sharing knowledge among agents.

Abstract: Recent studies show that LLMs possess different skills and specialize in
different tasks. In fact, we observe that their varied performance occur in
several levels of granularity. For example, in the code optimization task, code
LLMs excel at different optimization categories and no one dominates others.
This observation prompts the question of how one leverages multiple LLM agents
to solve a coding problem without knowing their complementary strengths a
priori. We argue that a team of agents can learn from each other's successes
and failures so as to improve their own performance. Thus, a lesson is the
knowledge produced by an agent and passed on to other agents in the collective
solution process. We propose a lesson-based collaboration framework, design the
lesson solicitation--banking--selection mechanism, and demonstrate that a team
of small LLMs with lessons learned can outperform a much larger LLM and other
multi-LLM collaboration methods.

</details>


### [452] [InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback](https://arxiv.org/pdf/2505.23950)
*Boyuan Chen, Donghai Hong, Jiaming Ji, Jiacheng Zheng, Bowen Dong, Jiayi Zhou, Kaile Wang, Juntao Dai, Xuyao Wang, Wenqi Chen, Qirui Zheng, Wenxin Li, Sirui Han, Yike Guo, Yaodong Yang*

Main category: cs.AI

TL;DR: The paper introduces InterMT, a preference dataset for multi-turn multimodal interaction, emphasizing human oversight and expert annotations to address gaps in MLLMs' interactive capabilities.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack complex interactive capabilities akin to human learning, which involves continuous multimodal interaction. The work aims to bridge this gap.

Method: The authors create InterMT, a dataset with human-labeled preferences, and introduce an agentic workflow using tool-augmented MLLMs. They also develop InterMT-Bench for assessment.

Result: InterMT includes 15.6k prompts, 52.6k dialogue instances, and 32.4k preference pairs. Applications like judge moderation demonstrate its utility.

Conclusion: The open-source dataset aims to advance research in aligning MLLMs with human-like multimodal interaction capabilities.

Abstract: As multimodal large models (MLLMs) continue to advance across challenging
tasks, a key question emerges: What essential capabilities are still missing? A
critical aspect of human learning is continuous interaction with the
environment -- not limited to language, but also involving multimodal
understanding and generation. To move closer to human-level intelligence,
models must similarly support multi-turn, multimodal interaction. In
particular, they should comprehend interleaved multimodal contexts and respond
coherently in ongoing exchanges. In this work, we present an initial
exploration through the InterMT -- the first preference dataset for multi-turn
multimodal interaction, grounded in real human feedback. In this exploration,
we particularly emphasize the importance of human oversight, introducing expert
annotations to guide the process, motivated by the fact that current MLLMs lack
such complex interactive capabilities. InterMT captures human preferences at
both global and local levels into nine sub-dimensions, consists of 15.6k
prompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled
preference pairs. To compensate for the lack of capability for multi-modal
understanding and generation, we introduce an agentic workflow that leverages
tool-augmented MLLMs to construct multi-turn QA instances. To further this
goal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting
judges with multi-turn, multimodal tasks. We demonstrate the utility of
\InterMT through applications such as judge moderation and further reveal the
multi-turn scaling law of judge model. We hope the open-source of our data can
help facilitate further research on aligning current MLLMs to the next step.
Our project website can be found at https://pku-intermt.github.io .

</details>


### [453] [MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge](https://arxiv.org/pdf/2505.23982)
*Jerry Junyang Cheung, Shiyao Shen, Yuchen Zhuang, Yinghao Li, Rampi Ramprasad, Chao Zhang*

Main category: cs.AI

TL;DR: MSQA is a benchmark for evaluating LLMs in materials science, testing factual knowledge and reasoning across 1,757 questions. Proprietary LLMs perform best (84.5% accuracy), while open-source and domain-specific models lag.


<details>
  <summary>Details</summary>
Motivation: There's no existing benchmark to assess LLMs' domain-specific knowledge and reasoning in materials science.

Method: MSQA includes 1,757 graduate-level questions in two formats (explanatory and True/False) across seven sub-fields.

Result: Proprietary LLMs achieve 84.5% accuracy, open-source models 60.5%, and domain-specific models underperform due to overfitting.

Conclusion: MSQA fills a critical gap by evaluating LLMs' factual and reasoning abilities in advanced materials science.

Abstract: Despite recent advances in large language models (LLMs) for materials
science, there is a lack of benchmarks for evaluating their domain-specific
knowledge and complex reasoning abilities. To bridge this gap, we introduce
MSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials
science questions in two formats: detailed explanatory responses and binary
True/False assessments. MSQA distinctively challenges LLMs by requiring both
precise factual knowledge and multi-step reasoning across seven materials
science sub-fields, such as structure-property relationships, synthesis
processes, and computational modeling. Through experiments with 10
state-of-the-art LLMs, we identify significant gaps in current LLM performance.
While API-based proprietary LLMs achieve up to 84.5% accuracy, open-source
(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform
significantly due to overfitting and distributional shifts. MSQA represents the
first benchmark to jointly evaluate the factual and reasoning capabilities of
LLMs crucial for LLMs in advanced materials science.

</details>


### [454] [Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding](https://arxiv.org/pdf/2505.23990)
*Mingyang Mao, Mariela M. Perez-Cabarcas, Utteja Kallakuri, Nicholas R. Waytowich, Xiaomin Lin, Tinoosh Mohsenin*

Main category: cs.AI

TL;DR: Multi-RAG is a multimodal retrieval-augmented generation system designed to reduce human cognitive load by integrating and reasoning over video, audio, and text. It outperforms existing models on the MMBench-Video dataset with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To offload cognitive burden from humans to intelligent systems in dynamic, information-rich scenarios, enabling adaptive robotic assistance.

Method: Developed Multi-RAG, integrating multimodal information (video, audio, text) for reasoning and situational understanding. Evaluated on MMBench-Video dataset.

Result: Superior performance compared to Video-LLMs and LVLMs, using fewer resources and less input data.

Conclusion: Multi-RAG shows promise as a practical foundation for adaptive human-robot assistance in dynamic real-world contexts.

Abstract: To effectively engage in human society, the ability to adapt, filter
information, and make informed decisions in ever-changing situations is
critical. As robots and intelligent agents become more integrated into human
life, there is a growing opportunity-and need-to offload the cognitive burden
on humans to these systems, particularly in dynamic, information-rich
scenarios.
  To fill this critical need, we present Multi-RAG, a multimodal
retrieval-augmented generation system designed to provide adaptive assistance
to humans in information-intensive circumstances. Our system aims to improve
situational understanding and reduce cognitive load by integrating and
reasoning over multi-source information streams, including video, audio, and
text. As an enabling step toward long-term human-robot partnerships, Multi-RAG
explores how multimodal information understanding can serve as a foundation for
adaptive robotic assistance in dynamic, human-centered situations. To evaluate
its capability in a realistic human-assistance proxy task, we benchmarked
Multi-RAG on the MMBench-Video dataset, a challenging multimodal video
understanding benchmark. Our system achieves superior performance compared to
existing open-source video large language models (Video-LLMs) and large
vision-language models (LVLMs), while utilizing fewer resources and less input
data. The results demonstrate Multi- RAG's potential as a practical and
efficient foundation for future human-robot adaptive assistance systems in
dynamic, real-world contexts.

</details>


### [455] [GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs](https://arxiv.org/pdf/2505.24036)
*Amel Gader, Alsayed Algergawy*

Main category: cs.AI

TL;DR: GenIC is a two-step LLM-powered framework for knowledge graph instance completion, outperforming baselines by leveraging textual descriptions and graph patterns.


<details>
  <summary>Details</summary>
Motivation: Address gaps in knowledge bases by inferring missing facts using entity descriptions and types, aided by large language models.

Method: Two-step approach: property prediction (multi-label classification) and link prediction (sequence-to-sequence generation).

Result: Outperforms existing baselines on three datasets.

Conclusion: GenIC effectively leverages LLMs and textual context for knowledge graph completion, demonstrating superior performance.

Abstract: Knowledge graph completion aims to address the gaps of knowledge bases by
adding new triples that represent facts. The complexity of this task depends on
how many parts of a triple are already known. Instance completion involves
predicting the relation-tail pair when only the head is given (h, ?, ?).
Notably, modern knowledge bases often contain entity descriptions and types,
which can provide valuable context for inferring missing facts. By leveraging
these textual descriptions and the ability of large language models to extract
facts from them and recognize patterns within the knowledge graph schema, we
propose an LLM-powered, end-to-end instance completion approach. Specifically,
we introduce GenIC: a two-step Generative Instance Completion framework. The
first step focuses on property prediction, treated as a multi-label
classification task. The second step is link prediction, framed as a generative
sequence-to-sequence task. Experimental results on three datasets show that our
method outperforms existing baselines. Our code is available at
https://github.com/amal-gader/genic.

</details>


### [456] [Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution](https://arxiv.org/pdf/2505.24037)
*Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu*

Main category: cs.AI

TL;DR: SEFT is a novel fine-tuning method for sparse LLMs that dynamically evolves sparse topology while maintaining sparsity, outperforming existing methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of maintaining performance in sparse LLMs during fine-tuning, as existing methods fail to preserve sparsity or adapt effectively.

Method: SEFT uses a weight drop-and-grow strategy and sensitivity-driven pruning to dynamically evolve sparse connectivity during fine-tuning.

Result: SEFT achieves stronger performance and superior memory/time efficiency on various LLMs and benchmarks.

Conclusion: SEFT is an effective solution for fine-tuning sparse LLMs, balancing performance and efficiency.

Abstract: Large language models (LLMs) have achieved remarkable success across various
tasks but face deployment challenges due to their massive computational
demands. While post-training pruning methods like SparseGPT and Wanda can
effectively reduce the model size, but struggle to maintain model performance
at high sparsity levels, limiting their utility for downstream tasks. Existing
fine-tuning methods, such as full fine-tuning and LoRA, fail to preserve
sparsity as they require updating the whole dense metrics, not well-suited for
sparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a
novel method designed specifically for sparse LLMs. SEFT dynamically evolves
the sparse topology of pruned models during fine-tuning, while preserving the
overall sparsity throughout the process. The strengths of SEFT lie in its
ability to perform task-specific adaptation through a weight drop-and-grow
strategy, enabling the pruned model to self-adapt its sparse connectivity
pattern based on the target dataset. Furthermore, a sensitivity-driven pruning
criterion is employed to ensure that the desired sparsity level is consistently
maintained throughout fine-tuning. Our experiments on various LLMs, including
LLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks
demonstrate that SEFT achieves stronger performance while offering superior
memory and time efficiency compared to existing baselines. Our code is publicly
available at: https://github.com/QiaoXiao7282/SEFT.

</details>


### [457] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/pdf/2505.24073)
*Chan-Wei Hu, Yueqi Wang, Shuo Xing, Chia-Ju Chen, Zhengzhong Tu*

Main category: cs.AI

TL;DR: The paper explores Retrieval-Augmented Generation (RAG) for Large Vision-Language Models (LVLMs) to address limitations like static data and hallucinations, improving performance by 5% without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs face challenges like static training data, hallucinations, and lack of real-time verification, limiting real-world applicability. RAG offers a solution by grounding outputs in factual, dynamic knowledge.

Method: The study systematically examines the multimodal RAG pipeline, focusing on retrieval (modality configurations, strategies), re-ranking (mitigating biases, improving relevance), and generation (integrating retrieved candidates). It also explores a unified agentic framework for dynamic evidence selection.

Result: The approach yields a 5% average performance boost for LVLMs without fine-tuning.

Conclusion: RAG significantly enhances LVLMs by integrating dynamic knowledge retrieval, re-ranking, and generation, offering a robust solution for real-world applications.

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


### [458] [SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought](https://arxiv.org/pdf/2505.24181)
*Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan*

Main category: cs.AI

TL;DR: Flow CoT introduces a recursive reasoning paradigm for LLMs, avoiding explicit CoT supervision. SCOUT, a lightweight fine-tuning framework, enhances reasoning accuracy and quality without pretraining.


<details>
  <summary>Details</summary>
Motivation: To address scalability and generalization limitations of CoT-based methods by modeling recursive inference as progressive cognitive states.

Method: Proposes Flow CoT and SCOUT, using progressive distillation and cross-attention for iterative reasoning refinement.

Result: Achieves up to 1.8% accuracy gains and improves explanation quality across benchmarks.

Conclusion: Flow CoT and SCOUT offer a scalable, effective framework for enhancing LLM reasoning without costly pretraining.

Abstract: Chain of Thought (CoT) prompting improves the reasoning performance of large
language models (LLMs) by encouraging step by step thinking. However, CoT-based
methods depend on intermediate reasoning steps, which limits scalability and
generalization. Recent work explores recursive reasoning, where LLMs reuse
internal layers across iterations to refine latent representations without
explicit CoT supervision. While promising, these approaches often require
costly pretraining and lack a principled framework for how reasoning should
evolve across iterations. We address this gap by introducing Flow Chain of
Thought (Flow CoT), a reasoning paradigm that models recursive inference as a
progressive trajectory of latent cognitive states. Flow CoT frames each
iteration as a distinct cognitive stage deepening reasoning across iterations
without relying on manual supervision. To realize this, we propose SCOUT
(Stepwise Cognitive Optimization Using Teachers), a lightweight fine tuning
framework that enables Flow CoT style reasoning without the need for
pretraining. SCOUT uses progressive distillation to align each iteration with a
teacher of appropriate capacity, and a cross attention based retrospective
module that integrates outputs from previous iterations while preserving the
models original computation flow. Experiments across eight reasoning benchmarks
show that SCOUT consistently improves both accuracy and explanation quality,
achieving up to 1.8% gains under fine tuning. Qualitative analyses further
reveal that SCOUT enables progressively deeper reasoning across iterations
refining both belief formation and explanation granularity. These results not
only validate the effectiveness of SCOUT, but also demonstrate the practical
viability of Flow CoT as a scalable framework for enhancing reasoning in LLMs.

</details>


### [459] [Learning API Functionality from Demonstrations for Tool-based Agents](https://arxiv.org/pdf/2505.24197)
*Bhrij Patel, Ashish Jagmohan, Aditya Vempaty*

Main category: cs.AI

TL;DR: The paper proposes learning API functionality from demonstrations instead of relying on documentation, addressing issues like missing or outdated docs. It evaluates the impact of demonstration quantity and LLM-generated summaries on task success, finding explicit function calls and critiques improve performance.


<details>
  <summary>Details</summary>
Motivation: Current API documentation is often unreliable (missing, outdated, privatized, or inconsistent), hindering the development of general-purpose agents. Demonstrations offer a viable alternative.

Method: Collect demonstrations from expert agents and self-exploration. Study the effect of demonstration count and LLM-generated summaries/evaluations on task success. Test across 3 datasets and 5 models.

Result: Learning from demonstrations is challenging but improves with explicit function calls and natural language critiques, enhancing parameter accuracy.

Conclusion: Demonstrations can replace documentation, but challenges remain. Future work should focus on documentation-free, self-improving agents.

Abstract: Digital tool-based agents that invoke external Application Programming
Interfaces (APIs) often rely on documentation to understand API functionality.
However, such documentation is frequently missing, outdated, privatized, or
inconsistent-hindering the development of reliable, general-purpose agents. In
this work, we propose learning API functionality directly from demonstrations
as a new paradigm applicable in scenarios without documentation. Using existing
API benchmarks, we collect demonstrations from both expert API-based agents and
from self-exploration. To understand what information demonstrations must
convey for successful task completion, we extensively study how the number of
demonstrations and the use of LLM-generated summaries and evaluations affect
the task success rate of the API-based agent. Our experiments across 3 datasets
and 5 models show that learning functionality from demonstrations remains a
non-trivial challenge, even for state-of-the-art LLMs. We find that providing
explicit function calls and natural language critiques significantly improves
the agent's task success rate due to more accurate parameter filling. We
analyze failure modes, identify sources of error, and highlight key open
challenges for future work in documentation-free, self-improving, API-based
agents.

</details>


### [460] [SentinelAgent: Graph-based Anomaly Detection in Multi-Agent Systems](https://arxiv.org/pdf/2505.24201)
*Xu He, Di Wu, Yan Zhai, Kun Sun*

Main category: cs.AI

TL;DR: A system-level anomaly detection framework for LLM-based multi-agent systems (MAS) is proposed, combining structural modeling and runtime oversight to address security and reliability challenges.


<details>
  <summary>Details</summary>
Motivation: Existing guardrail mechanisms for MAS are insufficient, focusing only on input-output levels and failing to address systemic or multi-point failures.

Method: The framework includes a graph-based model for semantic anomaly detection and a SentinelAgent for runtime oversight and intervention.

Result: The framework detects single-point faults, prompt injections, multi-agent collusion, and latent exploit paths, validated through case studies.

Conclusion: The work advances trustworthy, monitorable, and secure agent-based AI ecosystems.

Abstract: The rise of large language model (LLM)-based multi-agent systems (MAS)
introduces new security and reliability challenges. While these systems show
great promise in decomposing and coordinating complex tasks, they also face
multi-faceted risks across prompt manipulation, unsafe tool usage, and emergent
agent miscoordination. Existing guardrail mechanisms offer only partial
protection, primarily at the input-output level, and fall short in addressing
systemic or multi-point failures in MAS. In this work, we present a
system-level anomaly detection framework tailored for MAS, integrating
structural modeling with runtime behavioral oversight. Our approach consists of
two components. First, we propose a graph-based framework that models agent
interactions as dynamic execution graphs, enabling semantic anomaly detection
at node, edge, and path levels. Second, we introduce a pluggable SentinelAgent,
an LLM-powered oversight agent that observes, analyzes, and intervenes in MAS
execution based on security policies and contextual reasoning. By bridging
abstract detection logic with actionable enforcement, our method detects not
only single-point faults and prompt injections but also multi-agent collusion
and latent exploit paths. We validate our framework through two case studies,
including an email assistant and Microsoft's Magentic-One system, demonstrating
its ability to detect covert risks and provide explainable root-cause
attribution. Our work lays the foundation for more trustworthy, monitorable,
and secure agent-based AI ecosystems.

</details>


### [461] [Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap](https://arxiv.org/pdf/2505.24208)
*Wenhan Yang, Spencer Stice, Ali Payani, Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: The paper investigates the safety degradation in Vision-Language Models (VLMs) due to the modality gap between image and text representations, proposing a regularization method to reduce this gap and improve safety.


<details>
  <summary>Details</summary>
Motivation: VLMs exhibit significant safety degradation compared to their LLM counterparts, especially when irrelevant images trigger harmful responses. The modality gap is hypothesized as a cause, but its impact on safety is unexplored.

Method: The study correlates modality gap with safety, identifies its persistence through pretraining and fine-tuning, and introduces a regularization technique to reduce the gap during pretraining.

Result: Experiments on LLaVA v1.5, ShareGPT4V, and MiniGPT-4 show a 16.3% reduction in unsafe responses without performance loss, and up to 18.2% improvement with existing defenses.

Conclusion: Reducing the modality gap during pretraining enhances VLMs' safety alignment, offering a practical solution to mitigate harmful outputs.

Abstract: Ensuring Vision-Language Models (VLMs) generate safe outputs is crucial for
their reliable deployment. However, LVLMs suffer from drastic safety
degradation compared to their LLM backbone. Even blank or irrelevant images can
trigger LVLMs to generate harmful responses to prompts that would otherwise be
refused in text-only contexts. The modality gap between image and text
representations has been recently hypothesized to contribute to safety
degradation of LVLMs. However, if and how the amount of modality gap affects
LVLMs' safety is not studied. In this work, we show that the amount of modality
gap is highly inversely correlated with VLMs' safety. Then, we show that this
modality gap is introduced during pretraining LVLMs and persists through
fine-tuning. Inspired by this observation, we propose a regularization to
reduce the modality gap during pretraining. Our extensive experiments on LLaVA
v1.5, ShareGPT4V, and MiniGPT-4 show that our method substantially improves
safety alignment of LVLMs, reducing unsafe rate by up to 16.3% without
compromising performance, and can further boost existing defenses by up to
18.2%.

</details>


### [462] [E^2GraphRAG: Streamlining Graph-based RAG for High Efficiency and Effectiveness](https://arxiv.org/pdf/2505.24226)
*Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, Xiang Li*

Main category: cs.AI

TL;DR: E^2GraphRAG improves efficiency and effectiveness in graph-based RAG by using a summary tree, entity graph, and adaptive retrieval strategy, achieving significant speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based RAG methods like GraphRAG are inefficient and rely on manual query modes, limiting practicality.

Method: E^2GraphRAG constructs a summary tree with LLMs and an entity graph with SpaCy, then uses bidirectional indexes and an adaptive retrieval strategy.

Result: Achieves 10x faster indexing than GraphRAG and 100x speedup over LightRAG in retrieval while maintaining QA performance.

Conclusion: E^2GraphRAG offers a more efficient and effective solution for graph-based RAG, addressing limitations of prior methods.

Abstract: Graph-based RAG methods like GraphRAG have shown promising global
understanding of the knowledge base by constructing hierarchical entity graphs.
However, they often suffer from inefficiency and rely on manually pre-defined
query modes, limiting practical use. In this paper, we propose E^2GraphRAG, a
streamlined graph-based RAG framework that improves both Efficiency and
Effectiveness. During the indexing stage, E^2GraphRAG constructs a summary tree
with large language models and an entity graph with SpaCy based on document
chunks. We then construct bidirectional indexes between entities and chunks to
capture their many-to-many relationships, enabling fast lookup during both
local and global retrieval. For the retrieval stage, we design an adaptive
retrieval strategy that leverages the graph structure to retrieve and select
between local and global modes. Experiments show that E^2GraphRAG achieves up
to 10 times faster indexing than GraphRAG and 100 times speedup over LightRAG
in retrieval while maintaining competitive QA performance.

</details>


### [463] [MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge](https://arxiv.org/pdf/2505.24493)
*Xin Jing, Jiadong Wang, Iosif Tsangko, Andreas Triantafyllopoulos, Björn W. Schuller*

Main category: cs.AI

TL;DR: The paper explores using GPT-4o to annotate speech emotion data without human supervision, creating the MELT dataset, and shows improved SER performance.


<details>
  <summary>Details</summary>
Motivation: Human annotation for speech emotion recognition is costly and inconsistent, while LLMs offer a scalable alternative. The potential of LLMs for annotating emotional speech data is underexplored.

Method: GPT-4o is used to annotate a multimodal dataset from Friends using text prompts, creating MELT. Four SSL backbones are fine-tuned to evaluate SER performance.

Result: MELT annotations are accurate and contextually relevant. Fine-tuning with MELT improves SER performance across datasets.

Conclusion: GPT-4o can effectively annotate speech emotion data, and MELT enhances SER performance, offering a scalable alternative to human annotation.

Abstract: Although speech emotion recognition (SER) has advanced significantly with
deep learning, annotation remains a major hurdle. Human annotation is not only
costly but also subject to inconsistencies annotators often have different
preferences and may lack the necessary contextual knowledge, which can lead to
varied and inaccurate labels. Meanwhile, Large Language Models (LLMs) have
emerged as a scalable alternative for annotating text data. However, the
potential of LLMs to perform emotional speech data annotation without human
supervision has yet to be thoroughly investigated. To address these problems,
we apply GPT-4o to annotate a multimodal dataset collected from the sitcom
Friends, using only textual cues as inputs. By crafting structured text
prompts, our methodology capitalizes on the knowledge GPT-4o has accumulated
during its training, showcasing that it can generate accurate and contextually
relevant annotations without direct access to multimodal inputs. Therefore, we
propose MELT, a multimodal emotion dataset fully annotated by GPT-4o. We
demonstrate the effectiveness of MELT by fine-tuning four self-supervised
learning (SSL) backbones and assessing speech emotion recognition performance
across emotion datasets. Additionally, our subjective experiments\' results
demonstrate a consistence performance improvement on SER.

</details>


### [464] [ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction](https://arxiv.org/pdf/2505.24230)
*Murari Ambati*

Main category: cs.AI

TL;DR: ProofNet++ is a neuro-symbolic framework combining LLMs with formal proof verification and self-correction to improve theorem proving accuracy and verifiability.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based systems often produce unverifiable or incorrect logical steps, limiting their reliability in theorem proving.

Method: Integrates symbolic proof tree supervision, a reinforcement learning loop with verifiers as rewards, and an iterative self-correction module.

Result: Significantly improves proof accuracy, correctness, and verifiability on benchmarks like miniF2F, Lean's mathlib, and HOL Light.

Conclusion: ProofNet++ advances automated theorem proving by addressing LLM limitations, with theoretical analysis and released resources for future work.

Abstract: We propose ProofNet++, a neuro-symbolic framework that enhances automated
theorem proving by combining large language models (LLMs) with formal proof
verification and self-correction mechanisms. Current LLM-based systems suffer
from hallucinated logical steps and unverifiable reasoning. ProofNet++
mitigates these limitations by integrating symbolic proof tree supervision, a
reinforcement learning loop using verifiers as reward functions, and an
iterative self-correction module. Our experiments on miniF2F, Lean's mathlib,
and HOL Light show that ProofNet++ significantly improves proof accuracy,
correctness, and formal verifiability over prior models. We provide theoretical
analysis of the convergence and stability of the verifier-guided RL framework
and release our datasets and codebase for future research.

</details>


### [465] [FABLE: A Novel Data-Flow Analysis Benchmark on Procedural Text for Large Language Model Evaluation](https://arxiv.org/pdf/2505.24258)
*Vishal Pallagani, Nitin Gupta, John Aydin, Biplav Srivastava*

Main category: cs.AI

TL;DR: FABLE is a benchmark for evaluating LLMs' data-flow reasoning, testing three models across eight analyses and three domains. The reasoning-focused model performs best but is slower, while others perform poorly.


<details>
  <summary>Details</summary>
Motivation: To systematically assess LLMs' ability to reason about data flow, a critical skill for procedural tasks, which current models lack evaluation for.

Method: FABLE adapts eight software engineering data-flow analyses into a benchmark with 2,400 question-answer pairs across cooking, travel, and planning domains. Three LLM types are evaluated using majority voting.

Result: The reasoning-focused model (DeepSeek-R1 8B) achieves higher accuracy but is 20x slower. General-purpose and code-specific models (LLaMA 3.1 8B, Granite Code 8B) perform near random chance.

Conclusion: FABLE is the first benchmark for data-flow reasoning, highlighting the need for improved procedural understanding in LLMs.

Abstract: Understanding how data moves, transforms, and persists, known as data flow,
is fundamental to reasoning in procedural tasks. Despite their fluency in
natural and programming languages, large language models (LLMs), although
increasingly being applied to decisions with procedural tasks, have not been
systematically evaluated for their ability to perform data-flow reasoning. We
introduce FABLE, an extensible benchmark designed to assess LLMs' understanding
of data flow using structured, procedural text. FABLE adapts eight classical
data-flow analyses from software engineering: reaching definitions, very busy
expressions, available expressions, live variable analysis, interval analysis,
type-state analysis, taint analysis, and concurrency analysis. These analyses
are instantiated across three real-world domains: cooking recipes, travel
routes, and automated plans. The benchmark includes 2,400 question-answer
pairs, with 100 examples for each domain-analysis combination. We evaluate
three types of LLMs: a reasoning-focused model (DeepSeek-R1 8B), a
general-purpose model (LLaMA 3.1 8B), and a code-specific model (Granite Code
8B). Each model is tested using majority voting over five sampled completions
per prompt. Results show that the reasoning model achieves higher accuracy, but
at the cost of over 20 times slower inference compared to the other models. In
contrast, the general-purpose and code-specific models perform close to random
chance. FABLE provides the first diagnostic benchmark to systematically
evaluate data-flow reasoning and offers insights for developing models with
stronger procedural understanding.

</details>


### [466] [Generative AI for Urban Design: A Stepwise Approach Integrating Human Expertise with Multimodal Diffusion Models](https://arxiv.org/pdf/2505.24260)
*Mingyi He, Yuebing Liang, Shenhao Wang, Yunhan Zheng, Qingyi Wang, Dingyi Zhuang, Li Tian, Jinhua Zhao*

Main category: cs.AI

TL;DR: A stepwise generative urban design framework using multimodal diffusion models integrates human expertise for adaptive, controllable design processes, outperforming end-to-end approaches.


<details>
  <summary>Details</summary>
Motivation: Existing GenAI approaches lack integration with human workflows and iterative design processes, limiting control and adaptability.

Method: Proposes a three-stage framework (road/land use, building layout, detailed planning) using multimodal diffusion models with human refinement.

Result: Outperforms baselines in fidelity, compliance, and diversity in experiments with Chicago and NYC data.

Conclusion: Highlights the benefits of stepwise generation and human-AI interaction for iterative urban design.

Abstract: Urban design is a multifaceted process that demands careful consideration of
site-specific constraints and collaboration among diverse professionals and
stakeholders. The advent of generative artificial intelligence (GenAI) offers
transformative potential by improving the efficiency of design generation and
facilitating the communication of design ideas. However, most existing
approaches are not well integrated with human design workflows. They often
follow end-to-end pipelines with limited control, overlooking the iterative
nature of real-world design. This study proposes a stepwise generative urban
design framework that integrates multimodal diffusion models with human
expertise to enable more adaptive and controllable design processes. Instead of
generating design outcomes in a single end-to-end process, the framework
divides the process into three key stages aligned with established urban design
workflows: (1) road network and land use planning, (2) building layout
planning, and (3) detailed planning and rendering. At each stage, multimodal
diffusion models generate preliminary designs based on textual prompts and
image-based constraints, which can then be reviewed and refined by human
designers. We design an evaluation framework to assess the fidelity,
compliance, and diversity of the generated designs. Experiments using data from
Chicago and New York City demonstrate that our framework outperforms baseline
models and end-to-end approaches across all three dimensions. This study
underscores the benefits of multimodal diffusion models and stepwise generation
in preserving human control and facilitating iterative refinements, laying the
groundwork for human-AI interaction in urban design solutions.

</details>


### [467] [How Much Backtracking is Enough? Exploring the Interplay of SFT and RL in Enhancing LLM Reasoning](https://arxiv.org/pdf/2505.24273)
*Hongyi James Cai, Junlin Wang, Xiaoyin Chen, Bhuwan Dhingra*

Main category: cs.AI

TL;DR: The paper explores the impact of backtracking in reasoning tasks for LLMs, comparing SFT and RL, and finds that longer CoT with backtracks improves RL training, especially for harder tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the precise benefits of backtracking in reasoning tasks and how it contributes to improvements in LLMs' reasoning abilities.

Method: Systematic investigation of SFT and RL on eight reasoning tasks, with controlled experiments on synthetic datasets varying backtracking steps.

Result: Longer CoT with backtracks improves RL training; harder tasks need more backtracks. RL prioritizes structural patterns over content correctness.

Conclusion: Optimal training strategies for scaling reasoning in LLMs should leverage longer CoT with backtracks, especially for challenging tasks.

Abstract: Recent breakthroughs in large language models (LLMs) have effectively
improved their reasoning abilities, particularly on mathematical and logical
problems that have verifiable answers, through techniques such as supervised
finetuning (SFT) and reinforcement learning (RL). Prior research indicates that
RL effectively internalizes search strategies, enabling long chain-of-thought
(CoT) reasoning, with backtracking emerging naturally as a learned capability.
However, the precise benefits of backtracking, specifically, how significantly
it contributes to reasoning improvements and the optimal extent of its use,
remain poorly understood. In this work, we systematically investigate the
dynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc
1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self
Reference. Our findings highlight that short CoT sequences used in SFT as a
warm-up do have moderate contribution to RL training, compared with cold-start
RL; however such contribution diminishes when tasks become increasingly
difficult. Motivated by this observation, we construct synthetic datasets
varying systematically in the number of backtracking steps and conduct
controlled experiments to isolate the influence of either the correctness
(content) or the structure (i.e., backtrack frequency). We find that (1) longer
CoT with backtracks generally induce better and more stable RL training, (2)
more challenging problems with larger search space tend to need higher numbers
of backtracks during the SFT stage. Additionally, we demonstrate through
experiments on distilled data that RL training is largely unaffected by the
correctness of long CoT sequences, suggesting that RL prioritizes structural
patterns over content correctness. Collectively, our results offer practical
insights into designing optimal training strategies to effectively scale
reasoning in LLMs.

</details>


### [468] [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/pdf/2505.24292)
*Yueqi Zhang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

Main category: cs.AI

TL;DR: The paper introduces span-conditioned generation for human-AI conversations, proposing QuAda, a lightweight method to enhance quotation-aware dialogue without significant overhead.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack explicit mechanisms to locate and utilize quoted text spans in conversations, limiting their effectiveness in dialogue tasks.

Method: The paper formalizes span-conditioned generation, creates a quotation-centric data pipeline, and introduces QuAda, a method with bottleneck projections to dynamically adjust attention to quoted spans.

Result: QuAda performs well across five scenarios, generalizes to unseen topics, and updates <2.8% of backbone weights, proving efficient and effective.

Conclusion: QuAda offers a plug-and-play solution for quotation-aware dialogue, addressing the gap in current LLMs with minimal overhead.

Abstract: Human-AI conversation frequently relies on quoting earlier text-"check it
with the formula I just highlighted"-yet today's large language models (LLMs)
lack an explicit mechanism for locating and exploiting such spans. We formalise
the challenge as span-conditioned generation, decomposing each turn into the
dialogue history, a set of token-offset quotation spans, and an intent
utterance. Building on this abstraction, we introduce a quotation-centric data
pipeline that automatically synthesises task-specific dialogues, verifies
answer correctness through multi-stage consistency checks, and yields both a
heterogeneous training corpus and the first benchmark covering five
representative scenarios. To meet the benchmark's zero-overhead and
parameter-efficiency requirements, we propose QuAda, a lightweight
training-based method that attaches two bottleneck projections to every
attention head, dynamically amplifying or suppressing attention to quoted spans
at inference time while leaving the prompt unchanged and updating < 2.8% of
backbone weights. Experiments across models show that QuAda is suitable for all
scenarios and generalises to unseen topics, offering an effective,
plug-and-play solution for quotation-aware dialogue.

</details>


### [469] [GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments](https://arxiv.org/pdf/2505.24306)
*Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji*

Main category: cs.AI

TL;DR: The paper introduces GridRoute, a benchmark to evaluate LLMs' synergy with traditional algorithms, and proposes Algorithm of Thought (AoT), a hybrid prompting technique that enhances LLM performance in path planning.


<details>
  <summary>Details</summary>
Motivation: To explore the untapped synergy between LLMs and traditional algorithms in planning tasks, addressing the gap in existing research.

Method: Developed GridRoute benchmark and AoT, a hybrid prompting technique integrating traditional algorithms into LLM reasoning.

Result: AoT significantly improves LLM performance in correctness, optimality, and efficiency, especially in complex environments.

Conclusion: The study highlights the potential of combining LLMs with traditional algorithms for better path planning solutions.

Abstract: Recent advancements in Large Language Models (LLMs) have demonstrated their
potential in planning and reasoning tasks, offering a flexible alternative to
classical pathfinding algorithms. However, most existing studies focus on LLMs'
independent reasoning capabilities and overlook the potential synergy between
LLMs and traditional algorithms. To fill this gap, we propose a comprehensive
evaluation benchmark GridRoute to assess how LLMs can take advantage of
traditional algorithms. We also propose a novel hybrid prompting technique
called Algorithm of Thought (AoT), which introduces traditional algorithms'
guidance into prompting. Our benchmark evaluates six LLMs ranging from 7B to
72B parameters across various map sizes, assessing their performance in
correctness, optimality, and efficiency in grid environments with varying
sizes. Our results show that AoT significantly boosts performance across all
model sizes, particularly in larger or more complex environments, suggesting a
promising approach to addressing path planning challenges. Our code is
open-sourced at https://github.com/LinChance/GridRoute.

</details>


### [470] [Three Kinds of Negation in Knowledge and Their Mathematical Foundations](https://arxiv.org/pdf/2505.24422)
*Zhenghua Pan, Yong Wang*

Main category: cs.AI

TL;DR: The paper explores negation in knowledge, categorizing it into three types (contradictory, opposite, intermediary) and introduces mathematical frameworks (SCOI, LCOI) to analyze their properties and inference relations.


<details>
  <summary>Details</summary>
Motivation: Understanding and formalizing negation in knowledge is crucial for AI and knowledge processing, bridging gaps across philosophy, logic, and linguistics.

Method: The study distinguishes negation types (contradictory, opposite, intermediary) and develops mathematical frameworks (SCOI, LCOI) to model their properties and inference rules.

Result: Key operational properties of SCOI and formal inference relations in LCOI are proven, providing a foundation for negation analysis.

Conclusion: The proposed frameworks (SCOI, LCOI) offer a structured approach to studying negation, enhancing knowledge representation and reasoning in AI.

Abstract: In the field of artificial intelligence, understanding, distinguishing,
expressing, and computing the negation in knowledge is a fundamental issue in
knowledge processing and research. In this paper, we examine and analyze the
understanding and characteristics of negation in various fields such as
philosophy, logic, and linguistics etc. Based on the distinction between the
concepts of contradiction and opposition, we propose that there are three
different types of negation in knowledge from a conceptual perspective:
contradictory negation, opposite negation, and intermediary negation. To
establish a mathematical foundation that fully reflects the intrinsic
connections, properties, and laws of these different forms of negation, we
introduce SCOI: sets with contradictory negation, opposite negation and
intermediary negation, and LCOI: logic with contradictory negation, opposite
negation and intermediary negation, and we proved the main operational
properties of SCOI as well as the formal inference relations in LCOI.

</details>


### [471] [P: A Universal Measure of Predictive Intelligence](https://arxiv.org/pdf/2505.24426)
*David Gamez*

Main category: cs.AI

TL;DR: The paper proposes a universal measure of intelligence based on prediction accuracy and Kolmogorov complexity, tested in virtual maze and time-series experiments.


<details>
  <summary>Details</summary>
Motivation: Current AI systems mimic intelligence without a clear definition or measure, prompting the need for a universal intelligence metric.

Method: The measure sums prediction accuracy and accounts for complexity using Kolmogorov complexity, tested in virtual maze and time-series experiments.

Result: The algorithm successfully measured intelligence in virtual maze and time-series prediction scenarios.

Conclusion: This measure could enable comparative intelligence science across humans, animals, and AIs.

Abstract: Over the last thirty years, considerable progress has been made with the
development of systems that can drive cars, play games, predict protein folding
and generate natural language. These systems are described as intelligent and
there has been a great deal of talk about the rapid increase in artificial
intelligence and its potential dangers. However, our theoretical understanding
of intelligence and ability to measure it lag far behind our capacity for
building systems that mimic intelligent human behaviour. There is no commonly
agreed definition of the intelligence that AI systems are said to possess.
No-one has developed a practical measure that would enable us to compare the
intelligence of humans, animals and AIs on a single ratio scale.
  This paper sets out a new universal measure of intelligence that is based on
the hypothesis that prediction is the most important component of intelligence.
As an agent interacts with its normal environment, the accuracy of its
predictions is summed up and the complexity of its predictions and perceived
environment is accounted for using Kolmogorov complexity. Two experiments were
carried out to evaluate the practical feasibility of the algorithm. These
demonstrated that it could measure the intelligence of an agent embodied in a
virtual maze and an agent that makes predictions about time-series data. This
universal measure could be the starting point for a new comparative science of
intelligence that ranks humans, animals and AIs on a single ratio scale.

</details>


### [472] [RMoA: Optimizing Mixture-of-Agents through Diversity Maximization and Residual Compensation](https://arxiv.org/pdf/2505.24442)
*Zhentao Xie, Chengcheng Han, Jinxin Shi, Wenjun Cui, Xin Zhao, Xingjiao Wu, Jiabao Zhao*

Main category: cs.AI

TL;DR: RMoA improves multi-agent systems by using residual connections, diversity selection, and adaptive termination to enhance efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: Address limitations like high computational overhead, information loss, and robustness in multi-agent systems based on large language models.

Method: Proposes Residual Mixture-of-Agents (RMoA) with residual connections, embedding-based diversity selection, Residual Extraction Agent, Residual Aggregation Agent, and adaptive termination.

Result: Achieves state-of-the-art performance in alignment, reasoning, code generation, and multitasking while reducing computational costs.

Conclusion: RMoA effectively optimizes efficiency and reliability in multi-agent systems, demonstrating superior performance across benchmarks.

Abstract: Although multi-agent systems based on large language models show strong
capabilities on multiple tasks, they are still limited by high computational
overhead, information loss, and robustness. Inspired by ResNet's residual
learning, we propose Residual Mixture-of-Agents (RMoA), integrating residual
connections to optimize efficiency and reliability. To maximize information
utilization from model responses while minimizing computational costs, we
innovatively design an embedding-based diversity selection mechanism that
greedily selects responses via vector similarity. Furthermore, to mitigate
iterative information degradation, we introduce a Residual Extraction Agent to
preserve cross-layer incremental information by capturing inter-layer response
differences, coupled with a Residual Aggregation Agent for hierarchical
information integration. Additionally, we propose an adaptive termination
mechanism that dynamically halts processing based on residual convergence,
further improving inference efficiency. RMoA achieves state-of-the-art
performance on the benchmarks of across alignment, mathematical reasoning, code
generation, and multitasking understanding, while significantly reducing
computational overhead. Code is available at
https://github.com/mindhunter01/RMoA.

</details>


### [473] [SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors](https://arxiv.org/pdf/2505.24458)
*Tianlong Yu, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Zui Tao, Jun Zhang, Kailong Wang, Liting Zhou, Yang Yang, Ting Bi*

Main category: cs.AI

TL;DR: The SEAR Dataset is a multimodal resource for studying social engineering attacks via AR and LLMs, featuring annotated conversations, visual/audio cues, and trust metrics. It shows high attack efficacy and supports defensive research.


<details>
  <summary>Details</summary>
Motivation: To address the emerging threat of social engineering attacks using AR and LLMs by providing a comprehensive dataset for research.

Method: The dataset includes 180 annotated conversations from 60 participants in simulated adversarial scenarios, capturing multimodal data (visual/audio cues, environmental context) and subjective trust metrics.

Result: High efficacy of SEAR attacks: 93.3% phishing link clicks, 85% call acceptance, and 76.7% post-interaction trust surge.

Conclusion: The SEAR Dataset aids in detecting AR-driven SE attacks, designing defenses, and understanding adversarial manipulation, with ethical safeguards for responsible use.

Abstract: The SEAR Dataset is a novel multimodal resource designed to study the
emerging threat of social engineering (SE) attacks orchestrated through
augmented reality (AR) and multimodal large language models (LLMs). This
dataset captures 180 annotated conversations across 60 participants in
simulated adversarial scenarios, including meetings, classes and networking
events. It comprises synchronized AR-captured visual/audio cues (e.g., facial
expressions, vocal tones), environmental context, and curated social media
profiles, alongside subjective metrics such as trust ratings and susceptibility
assessments. Key findings reveal SEAR's alarming efficacy in eliciting
compliance (e.g., 93.3% phishing link clicks, 85% call acceptance) and
hijacking trust (76.7% post-interaction trust surge). The dataset supports
research in detecting AR-driven SE attacks, designing defensive frameworks, and
understanding multimodal adversarial manipulation. Rigorous ethical safeguards,
including anonymization and IRB compliance, ensure responsible use. The SEAR
dataset is available at https://github.com/INSLabCN/SEAR-Dataset.

</details>


### [474] [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/pdf/2505.24478)
*Vasilije Markovic, Lazar Obradovic, Laszlo Hajdu, Jovan Pavlovic*

Main category: cs.AI

TL;DR: The paper explores hyperparameter optimization in LLM-KG integration, showing performance gains but variability across datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored role of systematic hyperparameter optimization in LLM-KG systems, particularly in retrieval-augmented generation.

Method: Uses Cognee framework and three QA benchmarks to optimize parameters (chunking, graph construction, retrieval, prompting), scoring with exact match, F1, and DeepEval metrics.

Result: Targeted tuning yields meaningful but inconsistent gains, varying by dataset and metric.

Conclusion: Hyperparameter tuning is valuable but future progress requires clearer optimization/evaluation frameworks alongside architectural advances.

Abstract: Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) results
in complex systems with numerous hyperparameters that directly affect
performance. While such systems are increasingly common in retrieval-augmented
generation, the role of systematic hyperparameter optimization remains
underexplored. In this paper, we study this problem in the context of Cognee, a
modular framework for end-to-end KG construction and retrieval. Using three
multi-hop QA benchmarks (HotPotQA, TwoWikiMultiHop, and MuSiQue) we optimize
parameters related to chunking, graph construction, retrieval, and prompting.
Each configuration is scored using established metrics (exact match, F1, and
DeepEval's LLM-based correctness metric). Our results demonstrate that
meaningful gains can be achieved through targeted tuning. While the gains are
consistent, they are not uniform, with performance varying across datasets and
metrics. This variability highlights both the value of tuning and the
limitations of standard evaluation measures. While demonstrating the immediate
potential of hyperparameter tuning, we argue that future progress will depend
not only on architectural advances but also on clearer frameworks for
optimization and evaluation in complex, modular systems.

</details>


### [475] [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/pdf/2505.24479)
*Sania Nayab, Marco Simoni, Giulio Rossolini*

Main category: cs.AI

TL;DR: The paper proposes a method using knowledge graphs to systematically generate misinformation, evaluates LLMs' ability to detect it, and highlights detection limitations.


<details>
  <summary>Details</summary>
Motivation: The spread of misinformation, exacerbated by generative AI, threatens society, requiring scalable methods to assess and understand these threats.

Method: Leverages knowledge graphs to generate fake triplets, guiding LLMs to produce misinformation with varying credibility.

Result: Generates misinformation hard to detect, reveals LLMs' limitations in distinguishing fake from genuine content.

Conclusion: Emphasizes the need for better detection strategies and deeper analysis of generative model biases.

Abstract: The rapid spread of misinformation, further amplified by recent advances in
generative AI, poses significant threats to society, impacting public opinion,
democratic stability, and national security. Understanding and proactively
assessing these threats requires exploring methodologies that enable structured
and scalable misinformation generation. In this paper, we propose a novel
approach that leverages knowledge graphs (KGs) as structured semantic resources
to systematically generate fake triplets. By analyzing the structural
properties of KGs, such as the distance between entities and their predicates,
we identify plausibly false relationships. These triplets are then used to
guide large language models (LLMs) in generating misinformation statements with
varying degrees of credibility. By utilizing structured semantic relationships,
our deterministic approach produces misinformation inherently challenging for
humans to detect, drawing exclusively upon publicly available KGs (e.g.,
WikiGraphs).
  Additionally, we investigate the effectiveness of LLMs in distinguishing
between genuine and artificially generated misinformation. Our analysis
highlights significant limitations in current LLM-based detection methods,
underscoring the necessity for enhanced detection strategies and a deeper
exploration of inherent biases in generative models.

</details>


### [476] [Mixture-of-Experts for Personalized and Semantic-Aware Next Location Prediction](https://arxiv.org/pdf/2505.24597)
*Shuai Liu, Ning Cao, Yile Chen, Yue Jiang, Gao Cong*

Main category: cs.AI

TL;DR: NextLocMoE is a novel framework using LLMs and a dual-level Mixture-of-Experts design to improve next location prediction by capturing location semantics and user behavior dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model complex location semantics and diverse user behaviors, limiting prediction accuracy and generalization.

Method: NextLocMoE uses a dual-level MoE design: Location Semantics MoE for location embeddings and Personalized MoE for user-specific patterns, with a history-aware routing mechanism.

Result: Empirical tests show NextLocMoE outperforms others in accuracy, generalization, and interpretability.

Conclusion: NextLocMoE effectively addresses limitations in next location prediction, offering a robust and adaptable solution.

Abstract: Next location prediction plays a critical role in understanding human
mobility patterns. However, existing approaches face two core limitations: (1)
they fall short in capturing the complex, multi-functional semantics of
real-world locations; and (2) they lack the capacity to model heterogeneous
behavioral dynamics across diverse user groups. To tackle these challenges, we
introduce NextLocMoE, a novel framework built upon large language models (LLMs)
and structured around a dual-level Mixture-of-Experts (MoE) design. Our
architecture comprises two specialized modules: a Location Semantics MoE that
operates at the embedding level to encode rich functional semantics of
locations, and a Personalized MoE embedded within the Transformer backbone to
dynamically adapt to individual user mobility patterns. In addition, we
incorporate a history-aware routing mechanism that leverages long-term
trajectory data to enhance expert selection and ensure prediction stability.
Empirical evaluations across several real-world urban datasets show that
NextLocMoE achieves superior performance in terms of predictive accuracy,
cross-domain generalization, and interpretability

</details>


### [477] [Taxonomic Networks: A Representation for Neuro-Symbolic Pairing](https://arxiv.org/pdf/2505.24601)
*Zekun Wang, Ethan L. Haarer, Nicki Barari, Christopher J. MacLellan*

Main category: cs.AI

TL;DR: The paper introduces a neuro-symbolic pair linked by a common knowledge representation, using taxonomic networks for efficient learning and high-accuracy results.


<details>
  <summary>Details</summary>
Motivation: To bridge neural and symbolic approaches for more adaptable and efficient systems.

Method: Constructs a neuro-symbolic pair using taxonomic networks, evaluating performance under varying resource conditions.

Result: Symbolic method learns efficiently with less data, while neural method achieves higher accuracy with more resources.

Conclusion: The neuro-symbolic pair enables flexible integration of neural and symbolic computation, paving the way for future hybrid systems.

Abstract: We introduce the concept of a \textbf{neuro-symbolic pair} -- neural and
symbolic approaches that are linked through a common knowledge representation.
Next, we present \textbf{taxonomic networks}, a type of discrimination network
in which nodes represent hierarchically organized taxonomic concepts. Using
this representation, we construct a novel neuro-symbolic pair and evaluate its
performance. We show that our symbolic method learns taxonomic nets more
efficiently with less data and compute, while the neural method finds
higher-accuracy taxonomic nets when provided with greater resources. As a
neuro-symbolic pair, these approaches can be used interchangeably based on
situational needs, with seamless translation between them when necessary. This
work lays the foundation for future systems that more fundamentally integrate
neural and symbolic computation.

</details>


### [478] [Random Rule Forest (RRF): Interpretable Ensembles of LLM-Generated Questions for Predicting Startup Success](https://arxiv.org/pdf/2505.24622)
*Ben Griffin, Joseph Ternasky, Fuat Alican, Yigit Ihlamur*

Main category: cs.AI

TL;DR: A lightweight ensemble framework uses LLM-generated YES/NO questions to predict startup success, achieving 50% precision (5x better than random) and 54% with expert input, while maintaining transparency.


<details>
  <summary>Details</summary>
Motivation: To create an accurate and interpretable model for predicting startup success, crucial for high-stakes decisions like venture capital.

Method: Combines LLM-generated YES/NO questions as weak heuristics, filtered, ranked, and aggregated via threshold-based voting to form a strong predictor.

Result: 50% precision (5x improvement over random) and 54% with expert-guided heuristics.

Conclusion: LLM reasoning combined with human insight yields simple, interpretable ensembles effective for high-stakes decisions.

Abstract: Predicting startup success requires models that are both accurate and
interpretable. We present a lightweight ensemble framework that combines YES/NO
questions generated by large language models (LLMs), forming a transparent
decision-making system. Each question acts as a weak heuristic, and by
filtering, ranking, and aggregating them through a threshold-based voting
mechanism, we construct a strong ensemble predictor. On a test set where 10% of
startups are classified as successful, our approach achieves a precision rate
of 50%, representing a 5x improvement over random selection, while remaining
fully transparent. When we incorporate expert-guided heuristics into the
generation process, performance improves further to 54% precision. These
results highlight the value of combining LLM reasoning with human insight and
demonstrate that simple, interpretable ensembles can support high-stakes
decisions in domains such as venture capital (VC).

</details>


### [479] [Adaptable Cardiovascular Disease Risk Prediction from Heterogeneous Data using Large Language Models](https://arxiv.org/pdf/2505.24655)
*Frederike Lübeck, Jonas Wildberger, Frederik Träuble, Maximilian Mordig, Sergios Gatidis, Andreas Krause, Bernhard Schölkopf*

Main category: cs.AI

TL;DR: AdaCVD, a CVD risk prediction framework using large language models, outperforms existing methods by flexibly handling diverse patient data and adapting to new populations.


<details>
  <summary>Details</summary>
Motivation: Existing CVD risk models oversimplify patient profiles and struggle with real-world clinical challenges like distribution shifts and rigid input requirements.

Method: AdaCVD is built on large language models fine-tuned on UK Biobank data (over 500,000 participants), integrating structured and unstructured data for adaptable predictions.

Result: AdaCVD achieves state-of-the-art performance, outperforming traditional risk scores and machine learning methods, with robust results across diverse subgroups.

Conclusion: AdaCVD provides a flexible, AI-driven solution for CVD risk prediction, addressing clinical challenges and improving adaptability in heterogeneous healthcare settings.

Abstract: Cardiovascular disease (CVD) risk prediction models are essential for
identifying high-risk individuals and guiding preventive actions. However,
existing models struggle with the challenges of real-world clinical practice as
they oversimplify patient profiles, rely on rigid input schemas, and are
sensitive to distribution shifts. We developed AdaCVD, an adaptable CVD risk
prediction framework built on large language models extensively fine-tuned on
over half a million participants from the UK Biobank. In benchmark comparisons,
AdaCVD surpasses established risk scores and standard machine learning
approaches, achieving state-of-the-art performance. Crucially, for the first
time, it addresses key clinical challenges across three dimensions: it flexibly
incorporates comprehensive yet variable patient information; it seamlessly
integrates both structured data and unstructured text; and it rapidly adapts to
new patient populations using minimal additional data. In stratified analyses,
it demonstrates robust performance across demographic, socioeconomic, and
clinical subgroups, including underrepresented cohorts. AdaCVD offers a
promising path toward more flexible, AI-driven clinical decision support tools
suited to the realities of heterogeneous and dynamic healthcare environments.

</details>


### [480] [AXIOM: Learning to Play Games in Minutes with Expanding Object-Centric Models](https://arxiv.org/pdf/2505.24784)
*Conor Heins, Toon Van de Maele, Alexander Tschantz, Hampus Linander, Dimitrije Markovic, Tommaso Salvatori, Corrado Pezzato, Ozan Catal, Ran Wei, Magnus Koudahl, Marco Perin, Karl Friston, Tim Verbelen, Christopher Buckley*

Main category: cs.AI

TL;DR: AXIOM integrates core priors about object dynamics into active inference, combining Bayesian efficiency with DRL-like generalization, achieving mastery in games with minimal data and parameters.


<details>
  <summary>Details</summary>
Motivation: Current DRL lacks data efficiency, while active inference lacks domain flexibility. AXIOM bridges this gap by leveraging core priors for better learning.

Method: AXIOM uses object-centric dynamics and interactions, modeling scenes as piecewise linear trajectories with sparse interactions, refined online via Bayesian model reduction.

Result: AXIOM masters games in 10,000 steps, with fewer parameters than DRL and no gradient-based optimization.

Conclusion: AXIOM successfully merges Bayesian interpretability and DRL generalization, offering efficient learning in low-data regimes.

Abstract: Current deep reinforcement learning (DRL) approaches achieve state-of-the-art
performance in various domains, but struggle with data efficiency compared to
human learning, which leverages core priors about objects and their
interactions. Active inference offers a principled framework for integrating
sensory information with prior knowledge to learn a world model and quantify
the uncertainty of its own beliefs and predictions. However, active inference
models are usually crafted for a single task with bespoke knowledge, so they
lack the domain flexibility typical of DRL approaches. To bridge this gap, we
propose a novel architecture that integrates a minimal yet expressive set of
core priors about object-centric dynamics and interactions to accelerate
learning in low-data regimes. The resulting approach, which we call AXIOM,
combines the usual data efficiency and interpretability of Bayesian approaches
with the across-task generalization usually associated with DRL. AXIOM
represents scenes as compositions of objects, whose dynamics are modeled as
piecewise linear trajectories that capture sparse object-object interactions.
The structure of the generative model is expanded online by growing and
learning mixture models from single events and periodically refined through
Bayesian model reduction to induce generalization. AXIOM masters various games
within only 10,000 interaction steps, with both a small number of parameters
compared to DRL, and without the computational expense of gradient-based
optimization.

</details>


### [481] [EXP-Bench: Can AI Conduct AI Research Experiments?](https://arxiv.org/pdf/2505.24785)
*Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, Ang Chen*

Main category: cs.AI

TL;DR: EXP-Bench is a benchmark for evaluating AI agents on end-to-end research experiments, revealing their current limitations in executing complete experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of AI agents struggling with rigorous, end-to-end experimentation in AI research.

Method: Developed EXP-Bench, a semi-autonomous pipeline to extract and structure experimental details from AI papers and code, curating 461 tasks.

Result: Leading LLM-based agents showed partial capabilities (20-35% on aspects like design), but only 0.5% success rate for complete experiments.

Conclusion: EXP-Bench identifies bottlenecks and provides a tool for improving AI agents' research experiment capabilities.

Abstract: Automating AI research holds immense potential for accelerating scientific
progress, yet current AI agents struggle with the complexities of rigorous,
end-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed
to systematically evaluate AI agents on complete research experiments sourced
from influential AI publications. Given a research question and incomplete
starter code, EXP-Bench challenges AI agents to formulate hypotheses, design
and implement experimental procedures, execute them, and analyze results. To
enable the creation of such intricate and authentic tasks with high-fidelity,
we design a semi-autonomous pipeline to extract and structure crucial
experimental details from these research papers and their associated
open-source code. With the pipeline, EXP-Bench curated 461 AI research tasks
from 51 top-tier AI research papers. Evaluations of leading LLM-based agents,
such as OpenHands and IterativeAgent on EXP-Bench demonstrate partial
capabilities: while scores on individual experimental aspects such as design or
implementation correctness occasionally reach 20-35%, the success rate for
complete, executable experiments was a mere 0.5%. By identifying these
bottlenecks and providing realistic step-by-step experiment procedures,
EXP-Bench serves as a vital tool for future AI agents to improve their ability
to conduct AI research experiments. EXP-Bench is open-sourced at
https://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.

</details>


### [482] [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/pdf/2505.24846)
*Jingyan Shen, Jiarui Yao, Rui Yang, Yifan Sun, Feng Luo, Rui Pan, Tong Zhang, Han Zhao*

Main category: cs.AI

TL;DR: MiCRo is a two-stage framework for personalized preference learning in LLMs, addressing the limitations of the Bradley-Terry model by capturing diverse human preferences without fine-grained annotations.


<details>
  <summary>Details</summary>
Motivation: The Bradley-Terry model oversimplifies human preferences, limiting personalization in LLMs. MiCRo aims to address this by modeling diverse preferences more effectively.

Method: MiCRo uses a two-stage approach: (1) context-aware mixture modeling to capture diverse preferences, and (2) online routing to dynamically adapt mixture weights based on context.

Result: Experiments show MiCRo effectively captures diverse preferences and improves personalization in LLMs.

Conclusion: MiCRo offers a scalable and efficient solution for personalized preference learning in LLMs, enhancing alignment with human values.

Abstract: Reward modeling is a key step in building safe foundation models when
applying reinforcement learning from human feedback (RLHF) to align Large
Language Models (LLMs). However, reward modeling based on the Bradley-Terry
(BT) model assumes a global reward function, failing to capture the inherently
diverse and heterogeneous human preferences. Hence, such oversimplification
limits LLMs from supporting personalization and pluralistic alignment.
Theoretically, we show that when human preferences follow a mixture
distribution of diverse subgroups, a single BT model has an irreducible error.
While existing solutions, such as multi-objective learning with fine-grained
annotations, help address this issue, they are costly and constrained by
predefined attributes, failing to fully capture the richness of human values.
In this work, we introduce MiCRo, a two-stage framework that enhances
personalized preference learning by leveraging large-scale binary preference
datasets without requiring explicit fine-grained annotations. In the first
stage, MiCRo introduces context-aware mixture modeling approach to capture
diverse human preferences. In the second stage, MiCRo integrates an online
routing strategy that dynamically adapts mixture weights based on specific
context to resolve ambiguity, allowing for efficient and scalable preference
adaptation with minimal additional supervision. Experiments on multiple
preference datasets demonstrate that MiCRo effectively captures diverse human
preferences and significantly improves downstream personalization.

</details>


### [483] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/pdf/2505.24878)
*Yaxin Luo, Zhaoyi Li, Jiacheng Liu, Jiacheng Cui, Xiaohan Zhao, Zhiqiang Shen*

Main category: cs.AI

TL;DR: Open CaptchaWorld is a new benchmark for evaluating MLLM agents' ability to solve diverse CAPTCHAs, showing they lag far behind humans.


<details>
  <summary>Details</summary>
Motivation: CAPTCHAs block web agents; MLLMs' performance in interactive tasks like CAPTCHAs is untested.

Method: Introduces Open CaptchaWorld with 20 CAPTCHA types and a new metric, CAPTCHA Reasoning Depth, to evaluate MLLM agents.

Result: Humans score 93.3%; top MLLM agents score only 40.0%.

Conclusion: Open CaptchaWorld is crucial for diagnosing MLLM limits and improving multimodal reasoning.

Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>


### [484] [Why is plausibility surprisingly problematic as an XAI criterion?](https://arxiv.org/pdf/2303.17707)
*Weina Jin, Xiaoxiao Li, Ghassan Hamarneh*

Main category: cs.AI

TL;DR: The paper critiques the use of plausibility as an XAI criterion, arguing it is invalid and unethical, and suggests alternative research approaches for trustworthy and understandable AI.


<details>
  <summary>Details</summary>
Motivation: To address the lack of scientific rigor in XAI by critically examining the plausibility criterion, which is commonly used but flawed.

Method: A critical examination of plausibility in XAI, analyzing its assumptions, consequences, and ethical implications.

Result: Plausibility is invalid for measuring explainability, leads to misleading explanations, and harms trust and human autonomy.

Conclusion: The paper advocates abandoning plausibility as an XAI criterion and proposes new research directions for trustworthy and user-centric XAI.

Abstract: Explainable artificial intelligence (XAI) is motivated by the problem of
making AI predictions understandable, transparent, and responsible, as AI
becomes increasingly impactful in society and high-stakes domains. The
evaluation and optimization criteria of XAI are gatekeepers for XAI algorithms
to achieve their expected goals and should withstand rigorous inspection. To
improve the scientific rigor of XAI, we conduct a critical examination of a
common XAI criterion: plausibility. Plausibility assesses how convincing the AI
explanation is to humans, and is usually quantified by metrics of feature
localization or feature correlation. Our examination shows that plausibility is
invalid to measure explainability, and human explanations are not the ground
truth for XAI, because doing so ignores the necessary assumptions underpinning
an explanation. Our examination further reveals the consequences of using
plausibility as an XAI criterion, including increasing misleading explanations
that manipulate users, deteriorating users' trust in the AI system, undermining
human autonomy, being unable to achieve complementary human-AI task
performance, and abandoning other possible approaches of enhancing
understandability. Due to the invalidity of measurements and the unethical
issues, this position paper argues that the community should stop using
plausibility as a criterion for the evaluation and optimization of XAI
algorithms. We also delineate new research approaches to improve XAI in
trustworthiness, understandability, and utility to users, including
complementary human-AI task performance.

</details>


### [485] [From the Pursuit of Universal AGI Architecture to Systematic Approach to Heterogenous AGI: Addressing Alignment, Energy, & AGI Grand Challenges](https://arxiv.org/pdf/2310.15274)
*Eren Kurshan*

Main category: cs.AI

TL;DR: The paper proposes SAGI, a Systematic Approach to AGI, addressing energy, alignment, and narrow-to-AGI leap challenges through system design principles, emphasizing architecture learning and moral integration.


<details>
  <summary>Details</summary>
Motivation: To overcome the grand challenges of AI—Energy Wall, Alignment Problem, and the leap to AGI—by advocating for systematic design over singular architectures.

Method: Utilizes system design principles, including feedback loops and energy optimization, on learning substrates to allow AGI to learn its own architecture, embedding moral decision-making deeply.

Result: AGI systems with diverse architectures tailored to use cases, achieving alignment and efficiency through self-learning system design, surpassing traditional approaches.

Conclusion: AGI can be systematically engineered with integrated moral architecture, ensuring alignment and performance reliability without mimicking human brain structures.

Abstract: Artificial intelligence (AI) faces a trifecta of grand challenges: the Energy
Wall, the Alignment Problem and the Leap from Narrow AI to AGI. We present
SAGI, a Systematic Approach to AGI that utilizes system design principles to
overcome the energy wall and alignment challenges. This paper asserts that AGI
can be realized through multiplicity of design specific pathways and customized
through system design rather than a singular overarching architecture. AGI
systems may exhibit diver architectural configurations and capabilities,
contingent upon their intended use cases. Alignment, a challenge broadly
recognized as AIs most formidable, is the one that depends most critically on
system design and serves as its primary driving force as a foundational
criterion for AGI. Capturing the complexities of human morality for alignment
requires architectural support to represent the intricacies of moral
decision-making and the pervasive ethical processing at every level, with
performance reliability exceeding that of human moral judgment. Hence,
requiring a more robust architecture towards safety and alignment goals,
without replicating or resembling the human brain.
  We argue that system design (such as feedback loops, energy and performance
optimization) on learning substrates (capable of learning its system
architecture) is more fundamental to achieving AGI goals and guarantees,
superseding classical symbolic, emergentist and hybrid approaches. Through
learning of the system architecture itself, the resulting AGI is not a product
of spontaneous emergence but of systematic design and deliberate engineering,
with core features, including an integrated moral architecture, deeply embedded
within its architecture. The approach aims to guarantee design goals such as
alignment, efficiency by self-learning system architecture.

</details>


### [486] [GUICourse: From General Vision Language Models to Versatile GUI Agents](https://arxiv.org/pdf/2406.11317)
*Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, Yuan Yao, Yankai Lin, Zhiyuan Liu, Maosong Sun*

Main category: cs.AI

TL;DR: The paper introduces GUICourse, a suite of datasets to train GUI agents from Vision Language Models (VLMs), addressing their limitations in OCR, grounding, and GUI knowledge.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack fundamental abilities (OCR, grounding) and GUI knowledge, limiting their practicality as GUI agents.

Method: GUICourse includes three datasets: GUIEnv (for OCR and grounding), GUIAct, and GUIChat (for GUI component knowledge).

Result: Trained GUI agents outperform baseline VLMs, with even small models (3.1B parameters) performing well on single- and multi-step tasks.

Conclusion: GUICourse effectively enhances VLMs for GUI tasks, with potential for broader applications.

Abstract: Utilizing Graphic User Interface (GUI) for human-computer interaction is
essential for accessing a wide range of digital tools. Recent advancements in
Vision Language Models (VLMs) highlight the compelling potential to develop
versatile agents to help humans finish GUI navigation tasks. However, current
VLMs are challenged in terms of fundamental abilities (OCR and grounding) and
GUI knowledge (the functions and control methods of GUI elements), preventing
them from becoming practical GUI agents. To solve these challenges, we
contribute GUICourse, a suite of datasets to train visual-based GUI agents from
general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and
grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat
datasets to enrich their knowledge of GUI components and interactions.
Experiments demonstrate that our GUI agents have better performance on common
GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B
parameters) can still work well on single-step and multi-step GUI tasks.
Finally, we analyze the different varieties in the training stage of this agent
by ablation study. Our source codes and datasets are released at
https://github.com/yiye3/GUICourse.

</details>


### [487] [QPO: Query-dependent Prompt Optimization via Multi-Loop Offline Reinforcement Learning](https://arxiv.org/pdf/2408.10504)
*Yilun Kong, Hangyu Mao, Qi Zhao, Bin Zhang, Jingqing Ruan, Li Shen, Yongzhe Chang, Xueqian Wang, Rui Zhao, Dacheng Tao*

Main category: cs.AI

TL;DR: QPO introduces query-dependent prompt optimization using multi-loop offline reinforcement learning to improve LLM performance without costly online interactions.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods focus on task-level performance and require frequent LLM interactions, leading to inefficiencies.

Method: QPO fine-tunes a small pretrained model to generate query-specific prompts using offline data, iteratively improving the dataset.

Result: Experiments show QPO's effectiveness and cost-efficiency across diverse tasks and LLM scales.

Conclusion: QPO provides a scalable, low-cost solution for optimizing prompts tailored to input queries.

Abstract: Prompt engineering has demonstrated remarkable success in enhancing the
performance of large language models (LLMs) across diverse tasks. However, most
existing prompt optimization methods only focus on the task-level performance,
overlooking the importance of query-preferred prompts, which leads to
suboptimal performances. Additionally, these methods rely heavily on frequent
interactions with LLMs to obtain feedback for guiding the optimization process,
incurring substantial redundant interaction costs. In this paper, we introduce
Query-dependent Prompt Optimization (QPO), which leverages multi-loop offline
reinforcement learning to iteratively fine-tune a small pretrained language
model to generate optimal prompts tailored to the input queries, thus
significantly improving the prompting effect on the large target LLM. We derive
insights from offline prompting demonstration data, which already exists in
large quantities as a by-product of benchmarking diverse prompts on
open-sourced tasks, thereby circumventing the expenses of online interactions.
Furthermore, we continuously augment the offline dataset with the generated
prompts in each loop, as the prompts from the fine-tuned model are supposed to
outperform the source prompts in the original dataset. These iterative loops
bootstrap the model towards generating optimal prompts. Experiments on various
LLM scales and diverse NLP and math tasks demonstrate the efficacy and
cost-efficiency of our method in both zero-shot and few-shot scenarios.

</details>


### [488] [Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams](https://arxiv.org/pdf/2505.05880)
*Bettina Fazzinga, Sergio Flesca, Filippo Furfaro, Luigi Pontieri, Francesco Scala*

Main category: cs.AI

TL;DR: A neuro-symbolic approach combines sequence tagging and argumentation frameworks to efficiently interpret uncertain event-to-activity mappings in process traces, reducing data and computation costs.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of interpreting uncertain event-to-activity mappings in process traces while minimizing labor, computational costs, and environmental impact.

Method: Proposes a hybrid approach: a sequence-tagging model suggests probable event interpretations, refined by an Abstract Argumentation Framework (AAF) to leverage prior knowledge.

Result: The approach compensates for scarce annotated data and reduces computational costs, as confirmed by experiments.

Conclusion: The neuro-symbolic method is efficient and effective, especially in resource-constrained settings.

Abstract: Monitoring and analyzing process traces is a critical task for modern
companies and organizations. In scenarios where there is a gap between trace
events and reference business activities, this entails an interpretation
problem, amounting to translating each event of any ongoing trace into the
corresponding step of the activity instance. Building on a recent approach that
frames the interpretation problem as an acceptance problem within an Abstract
Argumentation Framework (AAF), one can elegantly analyze plausible event
interpretations (possibly in an aggregated form), as well as offer explanations
for those that conflict with prior process knowledge. Since, in settings where
event-to-activity mapping is highly uncertain (or simply under-specified) this
reasoning-based approach may yield lowly-informative results and heavy
computation, one can think of discovering a sequencetagging model, trained to
suggest highly-probable candidate event interpretations in a context-aware way.
However, training such a model optimally may require using a large amount of
manually-annotated example traces. Considering the urgent need of developing
Green AI solutions enabling environmental and societal sustainability (with
reduced labor/computational costs and carbon footprint), we propose a
data/computation-efficient neuro-symbolic approach to the problem, where the
candidate interpretations returned by the example-driven sequence tagger is
refined by the AAF-based reasoner. This allows us to also leverage prior
knowledge to compensate for the scarcity of example data, as confirmed by
experimental results; clearly, this property is particularly useful in settings
where data annotation and model optimization costs are subject to stringent
constraints.

</details>


### [489] [Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment](https://arxiv.org/pdf/2409.14106)
*Yibo Li, Yuan Fang, Mengmei Zhang, Chuan Shi*

Main category: cs.AI

TL;DR: FineMolTex is a framework for fine-grained molecular graph-text pre-training, combining coarse and fine-grained knowledge to improve molecular representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing models neglect motif-level insights, limiting generalization. FineMolTex addresses this by integrating motif and word-level knowledge.

Method: Uses two pre-training tasks: contrastive alignment for coarse-grained matching and masked multi-modal modeling for fine-grained matching.

Result: Achieves up to 230% improvement in text-based molecule editing and captures fine-grained knowledge for drug discovery.

Conclusion: FineMolTex bridges the gap in molecular representation learning, offering valuable insights for scientific applications.

Abstract: Understanding molecular structure and related knowledge is crucial for
scientific research. Recent studies integrate molecular graphs with their
textual descriptions to enhance molecular representation learning. However,
they focus on the whole molecular graph and neglect frequently occurring
subgraphs, known as motifs, which are essential for determining molecular
properties. Without such fine-grained knowledge, these models struggle to
generalize to unseen molecules and tasks that require motif-level insights. To
bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular
graph-Text pre-training framework to jointly learn coarse-grained
molecule-level knowledge and fine-grained motif-level knowledge. Specifically,
FineMolTex consists of two pre-training tasks: a contrastive alignment task for
coarse-grained matching and a masked multi-modal modeling task for fine-grained
matching. In particular, the latter predicts the labels of masked motifs and
words, which are selected based on their importance. By leveraging insights
from both modalities, FineMolTex is able to understand the fine-grained
matching between motifs and words. Finally, we conduct extensive experiments
across three downstream tasks, achieving up to 230% improvement in the
text-based molecule editing task. Additionally, our case studies reveal that
FineMolTex successfully captures fine-grained knowledge, potentially offering
valuable insights for drug discovery and catalyst design.

</details>


### [490] [Reflection-Bench: Evaluating Epistemic Agency in Large Language Models](https://arxiv.org/pdf/2410.16270)
*Lingyu Li, Yixu Wang, Haiquan Zhao, Shuqi Kong, Yan Teng, Chunbo Li, Yingchun Wang*

Main category: cs.AI

TL;DR: The paper studies epistemic agency in LLMs, proposing a benchmark (Reflection-Bench) to evaluate seven cognitive dimensions, revealing limitations in current models, especially in meta-reflection.


<details>
  <summary>Details</summary>
Motivation: To address the understudied intrinsic epistemic agency of LLMs, which is crucial for their reliability and effectiveness as AI agents.

Method: Proposed Reflection-Bench, a cognitive-psychology-inspired benchmark with seven tasks, evaluated 16 models using three prompting strategies.

Result: Identified a three-tier performance hierarchy and significant limitations in current LLMs, particularly in meta-reflection.

Conclusion: While LLMs show rudimentary epistemic agency, research should focus on enhancing cognitive functions, coordination, and adaptive mechanisms.

Abstract: With large language models (LLMs) increasingly deployed as cognitive engines
for AI agents, the reliability and effectiveness critically hinge on their
intrinsic epistemic agency, which remains understudied. Epistemic agency, the
ability to flexibly construct, adapt, and monitor beliefs about dynamic
environments, represents a base-model-level capacity independent of specific
tools, modules, or applications. We characterize the holistic process
underlying epistemic agency, which unfolds in seven interrelated dimensions:
prediction, decision-making, perception, memory, counterfactual thinking,
belief updating, and meta-reflection. Correspondingly, we propose
Reflection-Bench, a cognitive-psychology-inspired benchmark consisting of seven
tasks with long-term relevance and minimization of data leakage. Through a
comprehensive evaluation of 16 models using three prompting strategies, we
identify a clear three-tier performance hierarchy and significant limitations
of current LLMs, particularly in meta-reflection capabilities. While
state-of-the-art LLMs demonstrate rudimentary signs of epistemic agency, our
findings suggest several promising research directions, including enhancing
core cognitive functions, improving cross-functional coordination, and
developing adaptive processing mechanisms. Our code and data are available at
https://github.com/AI45Lab/ReflectionBench.

</details>


### [491] [Theorem-Validated Reverse Chain-of-Thought Problem Generation for Geometric Reasoning](https://arxiv.org/pdf/2410.17885)
*Linger Deng, Linghao Zhu, Yuliang Liu, Yu Wang, Qunyi Xie, Jingjing Wu, Gang Zhang, Yingying Zhu, Xiang Bai*

Main category: cs.AI

TL;DR: TR-CoT framework improves geometric reasoning in LMMs by synthesizing theorem-grounded diagrams and refining QA pairs, outperforming baselines and GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Address limitations in geometric reasoning for LMMs due to insufficient CoT training data and challenges in diversity and precision.

Method: Two-stage TR-CoT framework: TR-Engine synthesizes diagrams with descriptions, and TR-Reasoner refines QA pairs via reverse reasoning.

Result: 24.5% increase in logical consistency; outperforms baselines in MathVista (10.1%) and GeoQA (4.7%), surpassing GPT-4o.

Conclusion: TR-CoT enhances geometric reasoning, expands theorem coverage, and corrects misunderstandings, proving effective for LMMs.

Abstract: Large Multimodal Models (LMMs) face limitations in geometric reasoning due to
insufficient Chain of Thought (CoT) image-text training data. While existing
approaches leverage template-based or LLM-assisted methods for geometric CoT
data creation, they often face challenges in achieving both diversity and
precision. To bridge this gap, we introduce a two-stage Theorem-Validated
Reverse Chain-of-Thought Reasoning Synthesis (TR-CoT) framework. The first
stage, TR-Engine, synthesizes theorem-grounded geometric diagrams with
structured descriptions and properties. The second stage, TR-Reasoner, employs
reverse reasoning to iteratively refine question-answer pairs by
cross-validating geometric properties and description fragments. Our approach
expands theorem-type coverage, corrects long-standing misunderstandings, and
enhances geometric reasoning. Fine-grained CoT improves theorem understanding
and increases logical consistency by 24.5%. Our best models surpass the
baselines in MathVista and GeoQA by 10.1% and 4.7%, outperforming advanced
closed-source models like GPT-4o.

</details>


### [492] [Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs](https://arxiv.org/pdf/2411.06824)
*Megh Thakkar, Quentin Fournier, Matthew Riemer, Pin-Yu Chen, Amal Zouaq, Payel Das, Sarath Chandar*

Main category: cs.AI

TL;DR: A method called MergeAlign is introduced to align domain-expert LLMs safely without losing utility, tested on Llama3 variants in medicine and finance.


<details>
  <summary>Details</summary>
Motivation: Domain-expert LLMs often lose safety abilities, risking harmful content generation.

Method: MergeAlign interpolates domain and alignment vectors to create safer models.

Result: Substantial alignment improvements with minimal domain-specific performance loss.

Conclusion: MergeAlign offers a promising approach for developing safe expert LLMs efficiently.

Abstract: There is a growing interest in training domain-expert LLMs that excel in
specific technical fields compared to their general-purpose instruction-tuned
counterparts. However, these expert models often experience a loss in their
safety abilities in the process, making them capable of generating harmful
content. As a solution, we introduce an efficient and effective merging-based
alignment method called \textsc{MergeAlign} that interpolates the domain and
alignment vectors, creating safer domain-specific models while preserving their
utility. We apply \textsc{MergeAlign} on Llama3 variants that are experts in
medicine and finance, obtaining substantial alignment improvements with minimal
to no degradation on domain-specific benchmarks. We study the impact of model
merging through model similarity metrics and contributions of individual models
being merged. We hope our findings open new research avenues and inspire more
efficient development of safe expert LLMs.

</details>


### [493] [AMXFP4: Taming Activation Outliers with Asymmetric Microscaling Floating-Point for 4-bit LLM Inference](https://arxiv.org/pdf/2411.09909)
*Janghwan Lee, Jiwoong Park, Jinseok Kim, Yongjik Kim, Jungju Oh, Jinwook Oh, Jungwook Choi*

Main category: cs.AI

TL;DR: The paper introduces AMXFP4, a 4-bit asymmetric floating-point format, to address accuracy degradation in low-precision LLMs by balancing outlier suppression and group-wise asymmetry without calibration.


<details>
  <summary>Details</summary>
Motivation: Reducing computation precision in LLMs (e.g., to 4-bit) improves efficiency but degrades accuracy due to activation outliers. Existing methods like rotation-based INT4 or MXFP4 have limitations in overhead or robustness.

Method: Proposes AMXFP4, a 4-bit asymmetric FP format, using asymmetric shared scales to handle outliers and asymmetry without calibration. Includes a custom MAC engine for efficient implementation.

Result: AMXFP4 outperforms MXFP4 by 3% on VQA and exceeds rotation-based methods by 1.6% on CSQA, also surpassing commercial MXFP4 variants.

Conclusion: AMXFP4 effectively balances efficiency and accuracy in low-precision LLMs, offering a hardware-friendly solution without calibration overhead.

Abstract: As large language models (LLMs) grow in parameter size and context length,
computation precision has been reduced from 16-bit to 4-bit to improve
inference efficiency. However, this reduction causes accuracy degradation due
to activation outliers. Rotation-based INT4 methods address this via matrix
calibration, but they introduce multi-hour overheads and leave key computations
in full precision. Microscaling (MX) floating-point (FP) formats offer
fine-grained representation with a shared scale, enabling fully quantized
matrix multiplications through direct casting without calibration. However,
existing research shows unsatisfactory empirical results for MXFP4 inference,
and the robustness of MX formats remains largely unexplored. In this work, we
uncover the fundamental tradeoffs of the MX format: while it effectively
suppresses activation outliers, it does so at the cost of increased group-wise
asymmetry. To address this, we propose AMXFP4, a 4-bit asymmetric FP format
that handles both issues using asymmetric shared scales, without requiring
calibration. Our custom MAC engine adds negligible hardware cost while
improving accuracy: AMXFP4 outperforms MXFP4 by 3% on VQA and exceeds
rotation-based methods by 1.6% on CSQA. It also surpasses recently deployed
commercial MXFP4 variants. Code: https://github.com/aiha-lab/MX-QLLM

</details>


### [494] [ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning](https://arxiv.org/pdf/2412.13682)
*Jie-Jing Shao, Bo-Wen Zhang, Xiao-Wen Yang, Baizhi Chen, Si-Yu Han, Wen-Da Wei, Guohao Cai, Zhenhua Dong, Lan-Zhe Guo, Yu-feng Li*

Main category: cs.AI

TL;DR: The paper introduces ChinaTravel, a benchmark for evaluating language agents in multi-day, multi-POI travel planning, addressing gaps in existing benchmarks by using real-world data and a domain-specific language for scalable evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks oversimplify real-world travel planning by using synthetic queries and limited constraints. The paper aims to evaluate language agents in more complex, open-ended scenarios.

Method: The authors introduce ChinaTravel, a benchmark based on authentic Chinese travel requirements from 1,154 participants, and design a domain-specific language (DSL) for evaluation.

Result: Neuro-symbolic agents achieve a 37.0% constraint satisfaction rate on human queries, a 10x improvement over purely neural models.

Conclusion: ChinaTravel is a significant step forward for advancing language agents in complex, real-world planning tasks.

Abstract: Recent advances in LLMs, particularly in language reasoning and tool
integration, have rapidly sparked the \emph{Language Agents} for real-world
development. Among these, travel planning represents a prominent domain,
combining complex multi-objective planning challenges with practical deployment
demands. However, existing benchmarks often oversimplify real-world
requirements by focusing on synthetic queries and limited constraints. We
address the gap of evaluating language agents in multi-day, multi-POI travel
planning scenarios with diverse and open human needs. Specifically, we
introduce \emph{ChinaTravel}, the first open-ended benchmark grounded in
authentic Chinese travel requirements collected from 1,154 human participants.
We design a compositionally generalizable domain-specific language (DSL) for
scalable evaluation, covering feasibility, constraint satisfaction, and
preference comparison. Empirical studies reveal the potential of neuro-symbolic
agents in travel planning, achieving a 37.0\% constraint satisfaction rate on
human queries, a 10\times improvement over purely neural models. These findings
highlight ChinaTravel as a pivotal milestone for advancing language agents in
complex, real-world planning scenarios.

</details>


### [495] [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/pdf/2502.04675)
*Xueru Wen, Jie Lou, Xinyu Lu, Junjie Yang, Yanjiang Liu, Yaojie Lu, Debing Zhang, Xing Yu*

Main category: cs.AI

TL;DR: Recursive self-critiquing offers a scalable solution for AI oversight when direct human evaluation is infeasible due to AI surpassing human cognitive limits.


<details>
  <summary>Details</summary>
Motivation: Current alignment techniques (SFT, RLHF) fail when AI outputs exceed human cognitive thresholds, necessitating alternative oversight methods.

Method: Explores two hypotheses: critique of critique is easier than critique itself, and this relationship holds recursively. Conducts Human-AI and AI-AI experiments to test recursive self-critiquing.

Result: Recursive critique proves effective for scalable AI oversight.

Conclusion: Recursive self-critiquing is a promising approach for reliable AI supervision in scenarios where direct human evaluation is impractical.

Abstract: As AI capabilities increasingly surpass human proficiency in complex tasks,
current alignment techniques including SFT and RLHF face fundamental challenges
in ensuring reliable oversight. These methods rely on direct human assessment
and become untenable when AI outputs exceed human cognitive thresholds. In
response to this challenge, we explore two hypotheses: (1) \textit{Critique of
critique can be easier than critique itself}, extending the widely-accepted
observation that verification is easier than generation to the critique domain,
as critique itself is a specialized form of generation; (2) \textit{This
difficulty relationship is recursively held}, suggesting that when direct
evaluation is infeasible, performing high-order critiques (e.g., critique of
critique of critique) offers a more tractable supervision pathway. We further
conduct Human-AI and AI-AI experiments to investigate the potential of
utilizing recursive self-critiquing for AI supervision. Our results highlight
recursive critique as a promising approach for scalable AI oversight.

</details>


### [496] [Explorer: Scaling Exploration-driven Web Trajectory Synthesis for Multimodal Web Agents](https://arxiv.org/pdf/2502.11357)
*Vardaan Pahuja, Yadong Lu, Corby Rosset, Boyu Gou, Arindam Mitra, Spencer Whitehead, Yu Su, Ahmed Awadallah*

Main category: cs.AI

TL;DR: The paper introduces a scalable method to create a large, diverse trajectory-level dataset for training multimodal web agents, achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The lack of diverse, large-scale trajectory datasets hinders the development of human-level multimodal web agents.

Method: A scalable recipe synthesizes a dataset with 94K trajectories, leveraging web exploration and refinement.

Result: The dataset enables training Explorer, a web agent, which performs well on benchmarks like Mind2Web-Live.

Conclusion: Data scaling is key for improving web agents, making advanced LMM-based research more accessible.

Abstract: Recent success in large multimodal models (LMMs) has sparked promising
applications of agents capable of autonomously completing complex web tasks.
While open-source LMM agents have made significant advances in offline
evaluation benchmarks, their performance still falls substantially short of
human-level capabilities in more realistic online settings. A key bottleneck is
the lack of diverse and large-scale trajectory-level datasets across various
domains, which are expensive to collect. In this paper, we address this
challenge by developing a scalable recipe to synthesize the largest and most
diverse trajectory-level dataset to date, containing over 94K successful
multimodal web trajectories, spanning 49K unique URLs, 720K screenshots, and
33M web elements. In particular, we leverage extensive web exploration and
refinement to obtain diverse task intents. The average cost is 28 cents per
successful trajectory, making it affordable to a wide range of users in the
community. Leveraging this dataset, we train Explorer, a multimodal web agent,
and demonstrate strong performance on both offline and online web agent
benchmarks such as Mind2Web-Live, Multimodal-Mind2Web, and MiniWob++.
Additionally, our experiments highlight data scaling as a key driver for
improving web agent capabilities. We hope this study makes state-of-the-art
LMM-based agent research at a larger scale more accessible.

</details>


### [497] [You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations](https://arxiv.org/pdf/2502.13001)
*Frederic Kirstein, Muneeb Khan, Jan Philip Wahle, Terry Ruas, Bela Gipp*

Main category: cs.AI

TL;DR: FAME introduces a scalable dataset of synthetic meetings (500 English, 300 German) using MIMIC, a multi-agent framework with LLM debate and post-processing, validated as a realistic proxy for real meetings.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of high-quality meeting data due to privacy and collection challenges.

Method: MIMIC generates meeting transcripts via psychologically grounded participant profiles, LLM debate, and post-processing for coherence.

Result: FAME achieves high naturalness (4.5/5) and introduces realistic challenges (3/5 in spoken language, 4/5 in difficulty).

Conclusion: FAME is a scalable, realistic dataset for meeting summarization and conversation-centric research.

Abstract: Meeting summarization suffers from limited high-quality data, mainly due to
privacy restrictions and expensive collection processes. We address this gap
with FAME, a dataset of 500 meetings in English and 300 in German produced by
MIMIC, our new multi-agent meeting synthesis framework that generates meeting
transcripts on a given knowledge source by defining psychologically grounded
participant profiles, outlining the conversation, and orchestrating a large
language model (LLM) debate. A modular post-processing step refines these
outputs, mitigating potential repetitiveness and overly formal tones, ensuring
coherent, credible dialogues at scale. We also propose a psychologically
grounded evaluation framework assessing naturalness, social behavior
authenticity, and transcript difficulties. Human assessments show that FAME
approximates real-meeting spontaneity (4.5/5 in naturalness), preserves
speaker-centric challenges (3/5 in spoken language), and introduces richer
information-oriented difficulty (4/5 in difficulty). These findings highlight
that FAME is a good and scalable proxy for real-world meeting conditions. It
enables new test scenarios for meeting summarization research and other
conversation-centric applications in tasks requiring conversation data or
simulating social scenarios under behavioral constraints.

</details>


### [498] [The Relativity of Causal Knowledge](https://arxiv.org/pdf/2503.11718)
*Gabriele D'Acunto, Claudio Battiloro*

Main category: cs.AI

TL;DR: The paper proposes a shift from predictive to causal reasoning, introducing relativity of causal knowledge using category and sheaf theory to model imperfect, subjective causal structures.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of purely predictive AI systems by emphasizing causal and collaborative reasoning.

Method: Leveraging category theory to arrange structural causal models (SCMs) into functor categories and using sheaf theory to construct network sheaf and cosheaf for transferring causal knowledge.

Result: Demonstrates that observational and interventional probability measures form convex structures, enabling encoding of non-intervened SCMs and formalizing relative causal knowledge.

Conclusion: Introduces a mathematical framework for relative causal knowledge, enhancing causal reasoning in AI.

Abstract: Recent advances in artificial intelligence reveal the limits of purely
predictive systems and call for a shift toward causal and collaborative
reasoning. Drawing inspiration from the revolution of Grothendieck in
mathematics, we introduce the relativity of causal knowledge, which posits
structural causal models (SCMs) are inherently imperfect, subjective
representations embedded within networks of relationships. By leveraging
category theory, we arrange SCMs into a functor category and show that their
observational and interventional probability measures naturally form convex
structures. This result allows us to encode non-intervened SCMs with convex
spaces of probability measures. Next, using sheaf theory, we construct the
network sheaf and cosheaf of causal knowledge. These structures enable the
transfer of causal knowledge across the network while incorporating
interventional consistency and the perspective of the subjects, ultimately
leading to the formal, mathematical definition of relative causal knowledge.

</details>


### [499] [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/pdf/2505.10844)
*Simeng Han, Stephen Xia, Grant Zhang, Howard Dai, Chen Liu, Lichang Chen, Hoang Huy Nguyen, Hongyuan Mei, Jiayuan Mao, R. Thomas McCoy*

Main category: cs.AI

TL;DR: The paper introduces a brainteaser-based benchmark to evaluate LLMs' reasoning strategies, focusing on correctness, quality, and creativity of solutions.


<details>
  <summary>Details</summary>
Motivation: Accuracy alone doesn't reveal how models solve problems. Brainteasers, solvable via multiple approaches, offer deeper insights into reasoning strategies.

Method: The study evaluates LLMs across five reasoning layers: semantic parsing, solution generation, self-correction, step-by-step solution sketching, and hint utilization.

Result: LLMs often produce creative solutions but sometimes rely on brute force, indicating room for improvement in reasoning efficiency.

Conclusion: LLMs show potential for creative problem-solving but need refinement to reduce reliance on brute-force methods.

Abstract: Accuracy remains a standard metric for evaluating AI systems, but it offers
limited insight into how models arrive at their solutions. In this work, we
introduce a benchmark based on brainteasers written in long narrative form to
probe more deeply into the types of reasoning strategies that models use.
Brainteasers are well-suited for this goal because they can be solved with
multiple approaches, such as a few-step solution that uses a creative insight
or a longer solution that uses more brute force. We investigate large language
models (LLMs) across multiple layers of reasoning, focusing not only on
correctness but also on the quality and creativity of their solutions. We
investigate many aspects of the reasoning process: (1) semantic parsing of the
brainteasers into precise mathematical competition style formats; (2)
generating solutions from these mathematical forms; (3) self-correcting
solutions based on gold solutions; (4) producing step-by-step sketches of
solutions; and (5) making use of hints. We find that LLMs are in many cases
able to find creative, insightful solutions to brainteasers, suggesting that
they capture some of the capacities needed to solve novel problems in creative
ways. Nonetheless, there also remain situations where they rely on brute force
despite the availability of more efficient, creative solutions, highlighting a
potential direction for improvement in the reasoning abilities of LLMs.

</details>


### [500] [LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners](https://arxiv.org/pdf/2505.11942)
*Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, ZhongZhi Li, Yingying Zhang, Le Song, Qianli Ma*

Main category: cs.AI

TL;DR: LifelongAgentBench is a new benchmark to evaluate lifelong learning in LLM agents, revealing limitations of current methods and proposing a group self-consistency mechanism for improvement.


<details>
  <summary>Details</summary>
Motivation: Current LLM agents lack lifelong learning capabilities, and existing benchmarks fail to assess this.

Method: Introduces LifelongAgentBench with skill-grounded tasks across three environments, testing lifelong learning. Proposes group self-consistency for better performance.

Result: Shows conventional experience replay is ineffective; group self-consistency improves learning.

Conclusion: LifelongAgentBench aims to advance adaptive, memory-capable LLM agents.

Abstract: Lifelong learning is essential for intelligent agents operating in dynamic
environments. Current large language model (LLM)-based agents, however, remain
stateless and unable to accumulate or transfer knowledge over time. Existing
benchmarks treat agents as static systems and fail to evaluate lifelong
learning capabilities. We present LifelongAgentBench, the first unified
benchmark designed to systematically assess the lifelong learning ability of
LLM agents. It provides skill-grounded, interdependent tasks across three
interactive environments, Database, Operating System, and Knowledge Graph, with
automatic label verification, reproducibility, and modular extensibility.
Extensive experiments reveal that conventional experience replay has limited
effectiveness for LLM agents due to irrelevant information and context length
constraints. We further introduce a group self-consistency mechanism that
significantly improves lifelong learning performance. We hope
LifelongAgentBench will advance the development of adaptive, memory-capable LLM
agents.

</details>


### [501] [Self-Evolving Curriculum for LLM Reasoning](https://arxiv.org/pdf/2505.14970)
*Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, Ehsan Kamalloo*

Main category: cs.AI

TL;DR: SEC, an automatic curriculum learning method, improves RL fine-tuning of LLMs by dynamically selecting training problems, outperforming random and heuristic-based curricula.


<details>
  <summary>Details</summary>
Motivation: Current RL fine-tuning curricula (random, heuristic-based, or online filtering) are suboptimal or computationally expensive, necessitating a more efficient and effective approach.

Method: SEC formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, using policy gradient advantages as rewards and updating the curriculum policy with TD(0).

Result: SEC enhances reasoning abilities in planning, inductive reasoning, and mathematics, improving generalization to harder problems and balancing skills across domains.

Conclusion: SEC is a promising automatic curriculum learning method for RL fine-tuning of LLMs, offering better performance and efficiency.

Abstract: Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.

</details>


### [502] [MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network](https://arxiv.org/pdf/2505.16223)
*Sangyong Lee, Subo Hwang, Dohoon Kim*

Main category: cs.AI

TL;DR: MADCluster is a model-agnostic anomaly detection framework using self-supervised clustering to address the 'hypersphere collapse' problem in deep learning-based methods. It clusters normal data into a single cluster and introduces a new loss function for optimization.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing deep learning-based anomaly detection methods, particularly the 'hypersphere collapse' issue, and provide a versatile solution applicable to various architectures.

Method: MADCluster uses self-supervised clustering with three components: Base Embedder, Cluster Distance Mapping, and Sequence-wise Clustering. It introduces a 'One-directed Adaptive loss' for optimization.

Result: Experiments on four time series datasets show improved performance of comparative models when using MADCluster.

Conclusion: MADCluster is compatible with various architectures and has potential to enhance model performance broadly.

Abstract: In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.

</details>


### [503] [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/pdf/2505.18134)
*Alex L. Zhang, Thomas L. Griffiths, Karthik R. Narasimhan, Ofir Press*

Main category: cs.AI

TL;DR: VideoGameBench evaluates VLMs on 10 classic video games to test human-like skills, revealing poor performance even with top models like Gemini 2.5 Pro.


<details>
  <summary>Details</summary>
Motivation: Assess VLMs' ability to perform human-intuitive tasks like perception and spatial navigation, which are understudied despite their coding and math prowess.

Method: Introduce VideoGameBench, a benchmark with 10 games from the 1990s, requiring VLMs to play using only raw visuals and high-level instructions. Three games are hidden to test generalization.

Result: Frontier VLMs struggle, with Gemini 2.5 Pro completing only 0.48% of the benchmark. Latency issues led to the creation of VideoGameBench Lite, where performance slightly improved to 1.6%.

Conclusion: VideoGameBench highlights VLMs' limitations in human-like tasks and aims to drive research in these areas.

Abstract: Vision-language models (VLMs) have achieved strong results on coding and math
benchmarks that are challenging for humans, yet their ability to perform tasks
that come naturally to humans--such as perception, spatial navigation, and
memory management--remains understudied. Real video games are crafted to be
intuitive for humans to learn and master by leveraging innate inductive biases,
making them an ideal testbed for evaluating such capabilities in VLMs. To this
end, we introduce VideoGameBench, a benchmark consisting of 10 popular video
games from the 1990s that VLMs directly interact with in real-time.
VideoGameBench challenges models to complete entire games with access to only
raw visual inputs and a high-level description of objectives and controls, a
significant departure from existing setups that rely on game-specific
scaffolding and auxiliary information. We keep three of the games secret to
encourage solutions that generalize to unseen environments. Our experiments
show that frontier vision-language models struggle to progress beyond the
beginning of each game. We find inference latency to be a major limitation of
frontier models in the real-time setting; therefore, we introduce
VideoGameBench Lite, a setting where the game pauses while waiting for the LM's
next action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of
VideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization
of the human skills mentioned above into this benchmark motivates progress in
these research directions.

</details>


### [504] [Turing Test 2.0: The General Intelligence Threshold](https://arxiv.org/pdf/2505.19550)
*Georgios Mappouras*

Main category: cs.AI

TL;DR: The paper critiques traditional A.G.I. detection methods like the Turing test and introduces a new framework, Turing test 2.0, with a clear G.I. threshold and practical testing approach.


<details>
  <summary>Details</summary>
Motivation: The lack of consensus on detecting A.G.I. in A.I. models, despite advancements, motivates the need for a clearer, more practical method.

Method: The paper defines general intelligence (G.I.) and sets a G.I. threshold (G.I.T.), then introduces the Turing test 2.0 framework for fail/pass A.G.I. detection.

Result: The framework is demonstrated with real-life tests on modern A.I. models, showing its applicability.

Conclusion: The proposed Turing test 2.0 offers a clearer, more practical method for detecting A.G.I. compared to traditional approaches.

Abstract: With the rise of artificial intelligence (A.I.) and large language models
like Chat-GPT, a new race for achieving artificial general intelligence (A.G.I)
has started. While many speculate how and when A.I. will achieve A.G.I., there
is no clear agreement on how A.G.I. can be detected in A.I. models, even when
popular tools like the Turing test (and its modern variations) are used to
measure their intelligence. In this work, we discuss why traditional methods
like the Turing test do not suffice for measuring or detecting A.G.I. and
provide a new, practical method that can be used to decide if a system
(computer or any other) has reached or surpassed A.G.I. To achieve this, we
make two new contributions. First, we present a clear definition for general
intelligence (G.I.) and set a G.I. threshold (G.I.T.) that can be used to
distinguish between systems that achieve A.G.I. and systems that do not.
Second, we present a new framework on how to construct tests that can detect if
a system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass
way. We call this novel framework the Turing test 2.0. We then demonstrate
real-life examples of applying tests that follow our Turing test 2.0 framework
on modern A.I. models.

</details>


### [505] [FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks](https://arxiv.org/pdf/2505.19662)
*Atsunori Moteki, Shoichi Masui, Fan Yang, Yueqi Song, Yonatan Bisk, Graham Neubig, Ikuo Kusajima, Yasuto Watanabe, Hiroyuki Ishida, Jun Takahashi, Shan Jiang*

Main category: cs.AI

TL;DR: FieldWorkArena is a new benchmark for agentic AI in real-world field work, addressing gaps in existing benchmarks by introducing a tailored action space and improved evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for agentic AI focus on web tasks, lacking suitability for complex real-world work environments like factories and warehouses.

Method: The paper defines a new action space for agentic AI, improves evaluation functions, and uses on-site videos, documents, and worker interviews to create tasks.

Result: Performance evaluation for Multimodal LLM (MLLM) like GPT-4o is feasible, and the effectiveness and limitations of the new method are identified.

Conclusion: FieldWorkArena provides a practical benchmark for agentic AI in real-world field work, with datasets and tools publicly available.

Abstract: This paper proposes FieldWorkArena, a benchmark for agentic AI targeting
real-world field work. With the recent increase in demand for agentic AI, they
are required to monitor and report safety and health incidents, as well as
manufacturing-related incidents, that may occur in real-world work
environments. Existing agentic AI benchmarks have been limited to evaluating
web tasks and are insufficient for evaluating agents in real-world work
environments, where complexity increases significantly. In this paper, we
define a new action space that agentic AI should possess for real world work
environment benchmarks and improve the evaluation function from previous
methods to assess the performance of agentic AI in diverse real-world tasks.
The dataset consists of videos captured on-site and documents actually used in
factories and warehouses, and tasks were created based on interviews with
on-site workers and managers. Evaluation results confirmed that performance
evaluation considering the characteristics of Multimodal LLM (MLLM) such as
GPT-4o is feasible. Additionally, the effectiveness and limitations of the
proposed new evaluation method were identified. The complete dataset
(HuggingFace) and evaluation program (GitHub) can be downloaded from the
following website:
https://en-documents.research.global.fujitsu.com/fieldworkarena/.

</details>


### [506] [AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage](https://arxiv.org/pdf/2505.20662)
*Xuanle Zhao, Zilin Sang, Yuxuan Li, Qi Shi, Weilun Zhao, Shuo Wang, Duzhen Zhang, Xu Han, Zhiyuan Liu, Maosong Sun*

Main category: cs.AI

TL;DR: AutoReproduce is a multi-agent framework for automating AI experiment reproduction, outperforming baselines by up to 70% and achieving 22.1% performance gap on 89.74% of runs.


<details>
  <summary>Details</summary>
Motivation: Efficient experiment reproduction is hindered by implicit domain knowledge and lack of automation.

Method: Introduces the paper lineage algorithm and AutoReproduce framework, generating unit tests and using ReproduceBench for evaluation.

Result: AutoReproduce outperforms baselines by up to 70% and achieves 22.1% performance gap on 89.74% of runs.

Conclusion: AutoReproduce effectively automates experiment reproduction, addressing implicit knowledge gaps and improving fidelity.

Abstract: Efficient experiment reproduction is critical to accelerating progress in
artificial intelligence. However, the inherent complexity of method design and
training procedures presents substantial challenges for automation. Notably,
reproducing experiments often requires implicit domain-specific knowledge not
explicitly documented in the original papers. To address this, we introduce the
paper lineage algorithm, which identifies and extracts implicit knowledge from
the relevant references cited by the target paper. Building on this idea, we
propose AutoReproduce, a multi-agent framework capable of automatically
reproducing experiments described in research papers in an end-to-end manner.
AutoReproduce enhances code executability by generating unit tests alongside
the reproduction process. To evaluate the reproduction capability, we construct
ReproduceBench, a benchmark annotated with verified implementations, and
introduce novel evaluation metrics to assess both the reproduction and
execution fidelity. Experimental results demonstrate that AutoReproduce
outperforms the existing strong agent baselines on all five evaluation metrics
by a peak margin of over $70\%$. In particular, compared to the official
implementations, AutoReproduce achieves an average performance gap of $22.1\%$
on $89.74\%$ of the executable experiment runs. The code will be available at
https://github.com/AI9Stars/AutoReproduce.

</details>


### [507] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/pdf/2505.22092)
*Valentin Cuzin-Rambaud, Emilien Komlenovic, Alexandre Faure, Bruno Yun*

Main category: cs.AI

TL;DR: VIRAL is a pipeline using multi-modal LLMs to autonomously generate and refine reward functions, improving alignment with human intent in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning humans and machines in AI, especially the risks of poorly designed reward functions in reinforcement learning.

Method: VIRAL employs multi-modal LLMs to create and refine reward functions, incorporating human feedback or video LLM descriptions for policy explanation.

Result: Tested in five Gymnasium environments, VIRAL accelerates behavior learning and enhances alignment with user intent.

Conclusion: VIRAL demonstrates effective reward function generation and refinement, advancing AI alignment in reinforcement learning.

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/Hqo82CxVT38.

</details>


### [508] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/pdf/2505.23432)
*L. Elisa Celis, Lingxiao Huang, Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: A mathematical framework models jobs and worker-GenAI fit, showing how complementary subskills enhance job success and explaining productivity compression.


<details>
  <summary>Details</summary>
Motivation: To understand how GenAI complements or replaces human workers by analyzing skill decomposition and job success.

Method: Developed a framework decomposing skills into decision-level and action-level subskills, validated with O*NET and Big-Bench Lite data.

Result: Identified conditions for job success transitions and showed combining human and GenAI subskills outperforms single workers.

Conclusion: GenAI complements human skills under specific conditions, rather than replacing them, as demonstrated by real-world data.

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [509] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/pdf/2505.23473)
*Xiaorui Wu, Xiaofeng Mao, Xin Zhang, Fei Li, Chong Teng, Yuxiang Peng, Li Zheng, Donghong Ji, Zhuang Li*

Main category: cs.AI

TL;DR: EVOREFUSE is a prompt optimization method using evolutionary algorithms to generate diverse pseudo-malicious instructions, improving LLM refusal evaluation and alignment training.


<details>
  <summary>Details</summary>
Motivation: Addressing the issue of LLMs over-refusing harmless queries due to conservative safety alignment, which harms user experience.

Method: EVOREFUSE employs evolutionary algorithms with mutation and recombination to optimize prompts, maximizing refusal probability.

Result: Created EVOREFUSE-TEST (582 instructions) and EVOREFUSE-ALIGN (3,000 instructions) datasets, showing higher refusal rates, diversity, and alignment performance.

Conclusion: EVOREFUSE effectively reduces over-refusals and improves LLM alignment, revealing models' over-reliance on sensitive keywords.

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [510] [Unified AI for Accurate Audio Anomaly Detection](https://arxiv.org/pdf/2505.23781)
*Hamideh Khaleghpour, Brett McKinney*

Main category: cs.SD

TL;DR: A unified AI framework for high-accuracy audio anomaly detection integrates noise reduction, feature extraction, and machine learning, outperforming benchmarks in noisy environments.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in noisy environments and real-time applications for scalable audio anomaly detection.

Method: Combines spectral subtraction, adaptive filtering, MFCCs, deep embeddings (OpenL3), and hybrid modeling (SVM, Random Forest, CNNs, ensembles).

Result: Superior performance in precision, recall, and classification on TORGO and LibriSpeech datasets.

Conclusion: Provides a robust, scalable solution for audio anomaly detection, especially in noisy settings.

Abstract: This paper presents a unified AI framework for high-accuracy audio anomaly
detection by integrating advanced noise reduction, feature extraction, and
machine learning modeling techniques. The approach combines spectral
subtraction and adaptive filtering to enhance audio quality, followed by
feature extraction using traditional methods like MFCCs and deep embeddings
from pre-trained models such as OpenL3. The modeling pipeline incorporates
classical models (SVM, Random Forest), deep learning architectures (CNNs), and
ensemble methods to boost robustness and accuracy. Evaluated on benchmark
datasets including TORGO and LibriSpeech, the proposed framework demonstrates
superior performance in precision, recall, and classification of slurred vs.
normal speech. This work addresses challenges in noisy environments and
real-time applications and provides a scalable solution for audio-based anomaly
detection.

</details>


### [511] [4,500 Seconds: Small Data Training Approaches for Deep UAV Audio Classification](https://arxiv.org/pdf/2505.23782)
*Andrew P. Berg, Qian Zhang, Mia Y. Wang*

Main category: cs.SD

TL;DR: The paper explores deep learning for UAV classification, addressing data scarcity with PEFT and data augmentation. CNNs slightly outperform transformers in accuracy and efficiency, but transformers show potential with more data.


<details>
  <summary>Details</summary>
Motivation: The surge in UAV usage necessitates improved security measures, particularly for classification tasks, but data scarcity poses a challenge.

Method: The study uses 4,500 seconds of audio data across 9 classes, applying PEFT and data augmentation. It compares CNNs and transformers.

Result: CNNs achieve 1-2% higher accuracy than transformers and are more efficient, though transformers show promise with further optimization.

Conclusion: Future work will expand the dataset to better evaluate CNN and transformer trade-offs, as transformers may outperform with more data.

Abstract: Unmanned aerial vehicle (UAV) usage is expected to surge in the coming
decade, raising the need for heightened security measures to prevent airspace
violations and security threats. This study investigates deep learning
approaches to UAV classification focusing on the key issue of data scarcity. To
investigate this we opted to train the models using a total of 4,500 seconds of
audio samples, evenly distributed across a 9-class dataset. We leveraged
parameter efficient fine-tuning (PEFT) and data augmentations to mitigate the
data scarcity. This paper implements and compares the use of convolutional
neural networks (CNNs) and attention-based transformers. Our results show that,
CNNs outperform transformers by 1-2\% accuracy, while still being more
computationally efficient. These early findings, however, point to potential in
using transformers models; suggesting that with more data and further
optimizations they could outperform CNNs. Future works aims to upscale the
dataset to better understand the trade-offs between these approaches.

</details>


### [512] [Learning Normal Patterns in Musical Loops](https://arxiv.org/pdf/2505.23784)
*Shayan Dadman, Bernt Arild Bremdal, Børre Bang, Rune Dalmo*

Main category: cs.SD

TL;DR: An unsupervised framework using anomaly detection for audio pattern identification in music loops, leveraging deep learning and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in music information retrieval (MIR) like reliance on handcrafted features, domain constraints, and user interaction.

Method: Combines HTS-AT and FFM for feature extraction, then uses Deep SVDD for anomaly detection in a latent hypersphere.

Result: Deep SVDD models, especially the residual autoencoder variant, show superior anomaly separation, particularly for larger variations.

Conclusion: Provides a flexible, unsupervised solution for diverse audio samples, overcoming prior limitations and enabling effective pattern identification.

Abstract: This paper introduces an unsupervised framework for detecting audio patterns
in musical samples (loops) through anomaly detection techniques, addressing
challenges in music information retrieval (MIR). Existing methods are often
constrained by reliance on handcrafted features, domain-specific limitations,
or dependence on iterative user interaction. We address these limitations
through an architecture combining deep feature extraction with unsupervised
anomaly detection. Our approach leverages a pre-trained Hierarchical
Token-semantic Audio Transformer (HTS-AT), paired with a Feature Fusion
Mechanism (FFM), to generate representations from variable-length audio loops.
These embeddings are processed using one-class Deep Support Vector Data
Description (Deep SVDD), which learns normative audio patterns by mapping them
to a compact latent hypersphere. Evaluations on curated bass and guitar
datasets compare standard and residual autoencoder variants against baselines
like Isolation Forest (IF) and and principle component analysis (PCA) methods.
Results show our Deep SVDD models, especially the residual autoencoder variant,
deliver improved anomaly separation, particularly for larger variations. This
research contributes a flexible, fully unsupervised solution for processing
diverse audio samples, overcoming previous structural and input limitations
while enabling effective pattern identification through distance-based latent
space scoring.

</details>


### [513] [Patient-Aware Feature Alignment for Robust Lung Sound Classification:Cohesion-Separation and Global Alignment Losses](https://arxiv.org/pdf/2505.23834)
*Seung Gyu Jeong, Seong Eun Kim*

Main category: cs.SD

TL;DR: The paper proposes a Patient-Aware Feature Alignment (PAFA) framework with novel losses (PCSL and GPAL) to address inter-patient variability in lung sound classification, achieving high accuracy on the ICBHI dataset.


<details>
  <summary>Details</summary>
Motivation: Biomedical signals like lung sounds vary even among patients with the same symptoms, necessitating a method that accounts for individual differences.

Method: The PAFA framework uses PCSL to cluster features by patient and GPAL to align patient centroids globally, preventing feature space fragmentation.

Result: Achieved 64.84% for four-class and 72.08% for two-class classification on the ICBHI dataset.

Conclusion: PAFA effectively captures individualized patterns, improving performance in patient clusters and enabling patient-centered healthcare applications.

Abstract: Lung sound classification is vital for early diagnosis of respiratory
diseases. However, biomedical signals often exhibit inter-patient variability
even among patients with the same symptoms, requiring a learning approach that
considers individual differences. We propose a Patient-Aware Feature Alignment
(PAFA) framework with two novel losses, Patient Cohesion-Separation Loss (PCSL)
and Global Patient Alignment Loss (GPAL). PCSL clusters features of the same
patient while separating those from other patients to capture patient
variability, whereas GPAL draws each patient's centroid toward a global center,
preventing feature space fragmentation. Our method achieves outstanding results
on the ICBHI dataset with a score of 64.84\% for four-class and 72.08\% for
two-class classification. These findings highlight PAFA's ability to capture
individualized patterns and demonstrate performance gains in distinct patient
clusters, offering broader applications for patient-centered healthcare.

</details>


### [514] [Acoustic Classification of Maritime Vessels using Learnable Filterbanks](https://arxiv.org/pdf/2505.23964)
*Jonas Elsborg, Tejs Vegge, Arghya Bhowmik*

Main category: cs.SD

TL;DR: A deep learning model, CATFISH, achieves 96.63% accuracy in maritime vessel classification by dynamically adapting to acoustic variability.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of reliable vessel monitoring despite diverse acoustic environments and variable source-sensor distances.

Method: Uses a trainable spectral front-end and temporal feature encoder with a Gabor filterbank to dynamically emphasize frequencies.

Result: Achieves 96.63% test accuracy, surpassing previous benchmarks by over 12 percentage points.

Conclusion: CATFISH demonstrates robust performance across varying scenarios, with insights from ablation studies on data fusion and pooling.

Abstract: Reliably monitoring and recognizing maritime vessels based on acoustic
signatures is complicated by the variability of different recording scenarios.
A robust classification framework must be able to generalize across diverse
acoustic environments and variable source-sensor distances. To this end, we
present a deep learning model with robust performance across different
recording scenarios. Using a trainable spectral front-end and temporal feature
encoder to learn a Gabor filterbank, the model can dynamically emphasize
different frequency components. Trained on the VTUAD hydrophone recordings from
the Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %
percent test accuracy across varying source-sensor distances, surpassing the
previous benchmark by over 12 percentage points. We present the model, justify
our architectural choices, analyze the learned Gabor filters, and perform
ablation studies on sensor data fusion and attention-based pooling.

</details>


### [515] [FeatureSense: Protecting Speaker Attributes in Always-On Audio Sensing System](https://arxiv.org/pdf/2505.24115)
*Bhawana Chhaglani, Sarmistha Sarna Gomasta, Yuvraj Agarwal, Jeremy Gummeson, Prashant Shenoy*

Main category: cs.SD

TL;DR: The paper introduces FeatureSense, a privacy-aware audio feature library, addressing privacy concerns in audio sensing while maintaining utility. It evaluates speaker attribute leakage and optimizes privacy-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns in audio sensing due to ubiquitous microphones in devices, with prior work failing to fully protect sensitive speaker attributes like age and gender.

Method: Develops FeatureSense, an open-source library with privacy-aware audio features, and an adaptive feature selection algorithm for optimizing privacy-utility trade-offs.

Result: FeatureSense outperforms existing privacy techniques by 60.6% in preserving user-specific privacy while maintaining high utility across sensing tasks.

Conclusion: Provides a foundational framework for privacy-aware audio classification, ensuring trust in audio sensing applications.

Abstract: Audio is a rich sensing modality that is useful for a variety of human
activity recognition tasks. However, the ubiquitous nature of smartphones and
smart speakers with always-on microphones has led to numerous privacy concerns
and a lack of trust in deploying these audio-based sensing systems. This paper
addresses this critical challenge of preserving user privacy when using audio
for sensing applications while maintaining utility. While prior work focuses
primarily on protecting recoverable speech content, we show that sensitive
speaker-specific attributes such as age and gender can still be inferred after
masking speech and propose a comprehensive privacy evaluation framework to
assess this speaker attribute leakage. We design and implement FeatureSense, an
open-source library that provides a set of generalizable privacy-aware audio
features that can be used for wide range of sensing applications. We present an
adaptive task-specific feature selection algorithm that optimizes the
privacy-utility-cost trade-off based on the application requirements. Through
our extensive evaluation, we demonstrate the high utility of FeatureSense
across a diverse set of sensing tasks. Our system outperforms existing privacy
techniques by 60.6% in preserving user-specific privacy. This work provides a
foundational framework for ensuring trust in audio sensing by enabling
effective privacy-aware audio classification systems.

</details>


### [516] [ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation](https://arxiv.org/pdf/2505.24518)
*Jiatong Shi, Yifan Cheng, Bo-Hao Su, Hye-jin Shim, Jinchuan Tian, Samuele Cornell, Yiwen Zhao, Siddhant Arora, Shinji Watanabe*

Main category: cs.SD

TL;DR: ARECHO is a novel autoregressive system for speech assessment, improving joint estimation of diverse speech metrics through tokenization, dynamic classifier chains, and confidence-oriented decoding.


<details>
  <summary>Details</summary>
Motivation: Speech quality evaluation involves multiple metrics with different scales and dependencies, making joint estimation challenging.

Method: ARECHO uses a tokenization pipeline, dynamic classifier chains, and confidence-oriented decoding to model dependencies between metrics.

Result: ARECHO outperforms baselines in tasks like enhanced speech analysis and noisy speech evaluation, while improving interpretability.

Conclusion: ARECHO effectively addresses joint speech metric estimation, offering versatility and reliability in diverse evaluation scenarios.

Abstract: Speech signal analysis poses significant challenges, particularly in tasks
such as speech quality evaluation and profiling, where the goal is to predict
multiple perceptual and objective metrics. For instance, metrics like PESQ
(Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective
Intelligibility), and MOS (Mean Opinion Score) each capture different aspects
of speech quality. However, these metrics often have different scales,
assumptions, and dependencies, making joint estimation non-trivial. To address
these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based
Hypothesis Optimization), a chain-based, versatile evaluation system for speech
assessment grounded in autoregressive dependency modeling. ARECHO is
distinguished by three key innovations: (1) a comprehensive speech information
tokenization pipeline; (2) a dynamic classifier chain that explicitly captures
inter-metric dependencies; and (3) a two-step confidence-oriented decoding
algorithm that enhances inference reliability. Experiments demonstrate that
ARECHO significantly outperforms the baseline framework across diverse
evaluation scenarios, including enhanced speech analysis, speech generation
evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency
modeling improves interpretability by capturing inter-metric relationships.

</details>


### [517] [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/pdf/2505.24200)
*Qingzheng Wang, Jiancheng Sun, Yifan Peng, Shinji Watanabe*

Main category: cs.SD

TL;DR: The paper improves multilingual LID and ASR performance on ML-SUPERB 2.0 by adapting SFMs with strategies like frozen training, partial fine-tuning, and low-rank adaptation, plus data augmentation and LID CTC loss. Results show 14% LID accuracy boost and 30% ASR CER reduction.


<details>
  <summary>Details</summary>
Motivation: Existing SFMs struggle with limited resources during fine-tuning, prompting the need for better adaptation strategies.

Method: Adapts SFMs via frozen upstream training, partial fine-tuning, low-rank adaptation, data augmentation, and LID CTC loss.

Result: Achieves 14% relative LID accuracy improvement and 30% relative ASR CER reduction.

Conclusion: The proposed methods significantly enhance multilingual LID and ASR, securing second place in the ML-SUPERB 2.0 Challenge.

Abstract: Multilingual speech processing with self-supervised or supervised pre-trained
Speech Foundation Models (SFM) has achieved strong performance on tasks like
Language Identification (LID) and Automatic Speech Recognition (ASR). However,
these models struggle with limited resources during fine-tuning. This paper
enhances multilingual LID and ASR on ML-SUPERB 2.0 by exploring multiple
strategies for adapting SFMs, including frozen upstream training, partial
fine-tuning, and low-rank adaptation. Furthermore, we employ data augmentation
to mitigate performance gaps in few-shot settings and introduce LID
Connectionist Temporal Classification (CTC) loss for regularization. Our
approach achieves a 14% relative improvement in LID accuracy and a 30% relative
reduction in ASR CER over the baseline on ML-SUPERB 2.0, securing second place
in the Interspeech 2025 ML-SUPERB 2.0 Challenge.

</details>


### [518] [Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion](https://arxiv.org/pdf/2505.24291)
*Kaidi Wang, Wenhao Guan, Ziyue Jiang, Hukai Huang, Peijie Chen, Weijie Wu, Qingyang Hong, Lin Li*

Main category: cs.SD

TL;DR: Discl-VC is a zero-shot voice conversion framework that disentangles content and prosody, using a flow matching transformer and mask generative transformer for precise prosody control.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot voice conversion systems struggle to replicate or mimic speaking styles accurately, limiting controllability.

Method: Disentangles content and prosody from self-supervised speech representations, uses in-context learning with a flow matching transformer, and predicts prosody tokens non-autoregressively.

Result: Superior performance in zero-shot voice conversion and accurate prosody control.

Conclusion: Discl-VC effectively addresses the limitations of existing systems, offering precise prosody control and high-quality voice conversion.

Abstract: Currently, zero-shot voice conversion systems are capable of synthesizing the
voice of unseen speakers. However, most existing approaches struggle to
accurately replicate the speaking style of the source speaker or mimic the
distinctive speaking style of the target speaker, thereby limiting the
controllability of voice conversion. In this work, we propose Discl-VC, a novel
voice conversion framework that disentangles content and prosody information
from self-supervised speech representations and synthesizes the target
speaker's voice through in-context learning with a flow matching transformer.
To enable precise control over the prosody of generated speech, we introduce a
mask generative transformer that predicts discrete prosody tokens in a
non-autoregressive manner based on prompts. Experimental results demonstrate
the superior performance of Discl-VC in zero-shot voice conversion and its
remarkable accuracy in prosody control for synthesized speech.

</details>


### [519] [DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec](https://arxiv.org/pdf/2505.24314)
*Peijie Chen, Wenhao Guan, Kaidi Wang, Weijie Wu, Hukai Huang, Qingyang Hong, Lin Li*

Main category: cs.SD

TL;DR: DS-Codec, a dual-stage neural speech codec with mirror and non-mirror architectures, improves speech reconstruction in TTS systems.


<details>
  <summary>Details</summary>
Motivation: Advancing TTS systems by developing high-quality speech tokenizers, inspired by the success of large language models in text generation.

Method: Introduces DS-Codec with a dual-stage training framework and mirror/non-mirror architecture switching.

Result: Mirrored structure enhances codebook robustness; training strategy balances advantages of both architectures for better speech reconstruction.

Conclusion: DS-Codec achieves superior high-fidelity speech reconstruction through its innovative dual-stage framework and architecture design.

Abstract: Neural speech codecs are essential for advancing text-to-speech (TTS)
systems. With the recent success of large language models in text generation,
developing high-quality speech tokenizers has become increasingly important.
This paper introduces DS-Codec, a novel neural speech codec featuring a
dual-stage training framework with mirror and non-mirror architectures
switching, designed to achieve superior speech reconstruction. We conduct
extensive experiments and ablation studies to evaluate the effectiveness of our
training strategy and compare the performance of the two architectures. Our
results show that the mirrored structure significantly enhances the robustness
of the learned codebooks, and the training strategy balances the advantages
between mirrored and non-mirrored structures, leading to improved high-fidelity
speech reconstruction.

</details>


### [520] [SwitchCodec: A High-Fidelity Nerual Audio Codec With Sparse Quantization](https://arxiv.org/pdf/2505.24437)
*Jin Wang, Wenbin Jiang, Xiangbo Wang*

Main category: cs.SD

TL;DR: A high-fidelity neural audio compression algorithm (REVQ) is proposed, achieving compression below 3 kbps for speech, music, and general audio, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current audio codecs struggle with high compression (low bandwidth), leading to degraded performance.

Method: Proposes Residual Experts Vector Quantization (REVQ) to expand embedding space and a STFT-based discriminator for better spectrogram generation.

Result: REVQ improves performance without sacrificing bandwidth, outperforming baseline methods.

Conclusion: The proposed approach effectively addresses high-compression challenges in audio codecs.

Abstract: We present a universal high-fidelity neural audio compression algorithm that
can compress speech, music, and general audio below 3 kbps bandwidth. Although
current state-of-the-art audio codecs excel in audio compression, their
effectiveness significantly declines when embedding space is sharply reduced,
which corresponds to higher compression. To address this problem, we propose
Residual Experts Vector Quantization (REVQ), which significantly expands the
available embedding space and improves the performance while hardly sacrificing
the bandwidth. Furthermore, we introduce a strategy to ensure that the vast
embedding space can be fully utilized. Additionally, we propose a STFT-based
discriminator to guide the generator in producing indistinguishable
spectrograms. We demonstrate that the proposed approach outperforms baseline
methods through detailed ablations.

</details>


### [521] [Pseudo Labels-based Neural Speech Enhancement for the AVSR Task in the MISP-Meeting Challenge](https://arxiv.org/pdf/2505.24446)
*Longjie Luo, Shenghui Lu, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: The paper introduces a system for the MISP-Meeting Challenge Track 2, addressing challenges like noise and overlapping speech with G-SpatialNet, TLS framework, and ASR enhancements, achieving significant CER improvements.


<details>
  <summary>Details</summary>
Motivation: The dataset's strong background noise, reverberation, overlapping speech, and diverse topics posed challenges, motivating the development of robust solutions.

Method: The authors designed G-SpatialNet for speech enhancement, proposed the TLS framework for pseudo-label generation, and fine-tuned ASR models with data augmentation and multimodal info.

Result: The system achieved CERs of 5.44% (Dev) and 9.52% (Eval), with 64.8% and 52.6% relative improvements over the baseline, securing second place.

Conclusion: The proposed methods effectively addressed the dataset's challenges, demonstrating significant performance gains in meeting scenarios.

Abstract: This paper presents our system for the MISP-Meeting Challenge Track 2. The
primary difficulty lies in the dataset, which contains strong background noise,
reverberation, overlapping speech, and diverse meeting topics. To address these
issues, we (a) designed G-SpatialNet, a speech enhancement (SE) model to
improve Guided Source Separation (GSS) signals; (b) proposed TLS, a framework
comprising time alignment, level alignment, and signal-to-noise ratio
filtering, to generate signal-level pseudo labels for real-recorded far-field
audio data, thereby facilitating SE models' training; and (c) explored
fine-tuning strategies, data augmentation, and multimodal information to
enhance the performance of pre-trained Automatic Speech Recognition (ASR)
models in meeting scenarios. Finally, our system achieved character error rates
(CERs) of 5.44% and 9.52% on the Dev and Eval sets, respectively, with relative
improvements of 64.8% and 52.6% over the baseline, securing second place.

</details>


### [522] [SuPseudo: A Pseudo-supervised Learning Method for Neural Speech Enhancement in Far-field Speech Recognition](https://arxiv.org/pdf/2505.24450)
*Longjie Luo, Lin Li, Qingyang Hong*

Main category: cs.SD

TL;DR: The paper introduces SuPseudo, a pseudo-supervised learning method, and DSE for improving speech enhancement models' performance on real-recorded data, validated on the MISP2023 corpus.


<details>
  <summary>Details</summary>
Motivation: Existing speech enhancement models trained on simulated data perform poorly in real-world conditions, limiting their practical use in far-field speech recognition.

Method: Proposes Direct Sound Estimation (DSE) for oracle direct sound estimation and SuPseudo, a pseudo-supervised learning method using DSE-estimates as pseudo-labels. Also designs FARNET to leverage SuPseudo.

Result: Experiments on MISP2023 show SuPseudo's effectiveness, with the system outperforming prior state-of-the-art.

Conclusion: SuPseudo and DSE enable SE models to adapt to real-recorded data, enhancing generalization and performance.

Abstract: Due to the lack of target speech annotations in real-recorded far-field
conversational datasets, speech enhancement (SE) models are typically trained
on simulated data. However, the trained models often perform poorly in
real-world conditions, hindering their application in far-field speech
recognition. To address the issue, we (a) propose direct sound estimation (DSE)
to estimate the oracle direct sound of real-recorded data for SE; and (b)
present a novel pseudo-supervised learning method, SuPseudo, which leverages
DSE-estimates as pseudo-labels and enables SE models to directly learn from and
adapt to real-recorded data, thereby improving their generalization capability.
Furthermore, an SE model called FARNET is designed to fully utilize SuPseudo.
Experiments on the MISP2023 corpus demonstrate the effectiveness of SuPseudo,
and our system significantly outperforms the previous state-of-the-art. A demo
of our method can be found at https://EeLLJ.github.io/SuPseudo/.

</details>


### [523] [Rehearsal with Auxiliary-Informed Sampling for Audio Deepfake Detection](https://arxiv.org/pdf/2505.24486)
*Falih Gozi Febrinanto, Kristen Moore, Chandra Thapa, Jiangang Ma, Vidya Saikrishna, Feng Xia*

Main category: cs.SD

TL;DR: RAIS improves audio deepfake detection by using auxiliary labels to guide diverse sample selection in rehearsal-based continual learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing rehearsal-based CL techniques fail to capture audio diversity, leading to bias and forgetting.

Method: RAIS uses a label generation network to create auxiliary labels for selecting diverse samples in the memory buffer.

Result: RAIS achieves an average EER of 1.953% across five experiences, outperforming state-of-the-art methods.

Conclusion: RAIS effectively addresses diversity and bias in rehearsal-based CL for audio deepfake detection.

Abstract: The performance of existing audio deepfake detection frameworks degrades when
confronted with new deepfake attacks. Rehearsal-based continual learning (CL),
which updates models using a limited set of old data samples, helps preserve
prior knowledge while incorporating new information. However, existing
rehearsal techniques don't effectively capture the diversity of audio
characteristics, introducing bias and increasing the risk of forgetting. To
address this challenge, we propose Rehearsal with Auxiliary-Informed Sampling
(RAIS), a rehearsal-based CL approach for audio deepfake detection. RAIS
employs a label generation network to produce auxiliary labels, guiding diverse
sample selection for the memory buffer. Extensive experiments show RAIS
outperforms state-of-the-art methods, achieving an average Equal Error Rate
(EER) of 1.953 % across five experiences. The code is available at:
https://github.com/falihgoz/RAIS.

</details>


### [524] [Masked Self-distilled Transducer-based Keyword Spotting with Semi-autoregressive Decoding](https://arxiv.org/pdf/2505.24820)
*Yu Xi, Xiaoyu Gu, Haoyu Li, Jun Song, Bo Zheng, Kai Yu*

Main category: cs.SD

TL;DR: Proposes masked self-distillation (MSD) training and semi-autoregressive (SAR) decoding to address overfitting in RNN-T-based keyword spotting (KWS).


<details>
  <summary>Details</summary>
Motivation: Overfitting in RNN-T's prediction network degrades performance in challenging scenarios.

Method: Introduces MSD training to reduce reliance on prediction networks and SAR decoding to combine AR and NAR decoding benefits.

Result: MSD alleviates overfitting; SAR preserves AR performance while suppressing overfitting.

Conclusion: MSD and SAR improve KWS performance by addressing overfitting and combining decoding strengths.

Abstract: RNN-T-based keyword spotting (KWS) with autoregressive decoding~(AR) has
gained attention due to its streaming architecture and superior performance.
However, the simplicity of the prediction network in RNN-T poses an overfitting
issue, especially under challenging scenarios, resulting in degraded
performance. In this paper, we propose a masked self-distillation (MSD)
training strategy that avoids RNN-Ts overly relying on prediction networks to
alleviate overfitting. Such training enables masked non-autoregressive (NAR)
decoding, which fully masks the RNN-T predictor output during KWS decoding. In
addition, we propose a semi-autoregressive (SAR) decoding approach to integrate
the advantages of AR and NAR decoding. Our experiments across multiple KWS
datasets demonstrate that MSD training effectively alleviates overfitting. The
SAR decoding method preserves the superior performance of AR decoding while
benefits from the overfitting suppression of NAR decoding, achieving excellent
results.

</details>


### [525] [Breaking Resource Barriers in Speech Emotion Recognition via Data Distillation](https://arxiv.org/pdf/2406.15119)
*Yi Chang, Zhao Ren, Zhonghao Zhao, Thanh Tam Nguyen, Kun Qian, Tanja Schultz, Björn W. Schuller*

Main category: cs.SD

TL;DR: A data distillation framework is proposed for efficient SER model development in IoT, using a smaller, synthesized dataset to match original dataset performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in SER for IoT include resource constraints and privacy concerns with emotional speech data.

Method: Proposes a data distillation framework to create a smaller, synthesized dataset for training SER models.

Result: Distilled dataset trains SER models effectively, achieving performance comparable to the original dataset.

Conclusion: The framework addresses IoT constraints and privacy issues while maintaining SER model performance.

Abstract: Speech emotion recognition (SER) plays a crucial role in human-computer
interaction. The emergence of edge devices in the Internet of Things (IoT)
presents challenges in constructing intricate deep learning models due to
constraints in memory and computational resources. Moreover, emotional speech
data often contains private information, raising concerns about privacy leakage
during the deployment of SER models. To address these challenges, we propose a
data distillation framework to facilitate efficient development of SER models
in IoT applications using a synthesised, smaller, and distilled dataset. Our
experiments demonstrate that the distilled dataset can be effectively utilised
to train SER models with fixed initialisation, achieving performances
comparable to those developed using the original full emotional speech dataset.

</details>


### [526] [SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation](https://arxiv.org/pdf/2502.13128)
*Zihan Liu, Shuangrui Ding, Zhixiong Zhang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang*

Main category: cs.SD

TL;DR: SongGen is a single-stage auto-regressive transformer for controllable text-to-song generation, offering fine-grained control over musical attributes and two output modes, with open-source release.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in text-to-song generation like domain complexity, data scarcity, and suboptimal quality from multi-stage approaches.

Method: Proposes SongGen, a single-stage auto-regressive transformer with two output modes (mixed and dual-track) and diverse token patterns. Includes a data preprocessing pipeline.

Result: Notable improvements in generation quality and flexibility, with insights from token pattern strategies.

Conclusion: SongGen advances text-to-song generation with unified control and open-source resources, fostering future research.

Abstract: Text-to-song generation, the task of creating vocals and accompaniment from
textual inputs, poses significant challenges due to domain complexity and data
scarcity. Existing approaches often employ multi-stage generation procedures,
leading to cumbersome training and inference pipelines, as well as suboptimal
overall generation quality due to error accumulation across stages. In this
paper, we propose SongGen, a fully open-source, single-stage auto-regressive
transformer designed for controllable song generation. The proposed model
facilitates fine-grained control over diverse musical attributes, including
lyrics and textual descriptions of instrumentation, genre, mood, and timbre,
while also offering an optional three-second reference clip for voice cloning.
Within a unified auto-regressive framework, SongGen supports two output modes:
mixed mode, which generates a mixture of vocals and accompaniment directly, and
dual-track mode, which synthesizes them separately for greater flexibility in
downstream applications. We explore diverse token pattern strategies for each
mode, leading to notable improvements and valuable insights. Furthermore, we
design an automated data preprocessing pipeline with effective quality control.
To foster community engagement and future research, we will release our model
weights, training code, annotated data, and preprocessing pipeline. The code is
available at https://github.com/LiuZH-19/SongGen.

</details>


### [527] [ReelWave: Multi-Agentic Movie Sound Generation through Multimodal LLM Conversation](https://arxiv.org/pdf/2503.07217)
*Zixuan Wang, Chi-Keung Tang, Yu-Wing Tai*

Main category: cs.SD

TL;DR: The paper proposes a multi-agent framework for audio generation in movies, supervised by a Sound Director agent, to handle on-screen and off-screen sounds via multimodal LLM.


<details>
  <summary>Details</summary>
Motivation: Current audio generation methods align well with text/video but fail to address the complexity of movie storytelling, which requires temporally-aligned on-screen sounds and complementary off-screen sounds.

Method: A multi-agent system (Sound Director, Foley Artist, Composer, Voice Actor) generates audio. On-screen sounds are created using a prediction model for audio control signals, while off-screen sounds are collaboratively produced.

Result: The framework generates rich, relevant audio for movie clips by decomposing conditions into specific sound instructions and grounding them temporally.

Conclusion: The proposed system effectively addresses the challenges of movie audio production, offering a scalable and autonomous solution.

Abstract: Current audio generation conditioned by text or video focuses on aligning
audio with text/video modalities. Despite excellent alignment results, these
multimodal frameworks still cannot be directly applied to compelling movie
storytelling involving multiple scenes, where "on-screen" sounds require
temporally-aligned audio generation, while "off-screen" sounds contribute to
appropriate environment sounds accompanied by background music when applicable.
Inspired by professional movie production, this paper proposes a multi-agentic
framework for audio generation supervised by an autonomous Sound Director
agent, engaging multi-turn conversations with other agents for on-screen and
off-screen sound generation through multimodal LLM. To address on-screen sound
generation, after detecting any talking humans in videos, we capture
semantically and temporally synchronized sound by training a prediction model
that forecasts interpretable, time-varying audio control signals: loudness,
pitch, and timbre, which are used by a Foley Artist agent to condition a
cross-attention module in the sound generation. The Foley Artist works
cooperatively with the Composer and Voice Actor agents, and together they
autonomously generate off-screen sound to complement the overall production.
Each agent takes on specific roles similar to those of a movie production team.
To temporally ground audio language models, in ReelWave, text/video conditions
are decomposed into atomic, specific sound generation instructions synchronized
with visuals when applicable. Consequently, our framework can generate rich and
relevant audio content conditioned on video clips extracted from movies.

</details>


### [528] [Towards Robust Assessment of Pathological Voices via Combined Low-Level Descriptors and Foundation Model Representations](https://arxiv.org/pdf/2505.21356)
*Whenty Ariyanti, Kuan-Yu Chen, Sabato Marco Siniscalchi, Hsin-Min Wang, Yu Tsao*

Main category: cs.SD

TL;DR: The paper introduces VOQANet and VOQANet+, deep learning frameworks for automated, objective voice quality assessment, outperforming traditional subjective methods and baseline models.


<details>
  <summary>Details</summary>
Motivation: Traditional voice quality assessments like CAPE-V and GRBAS are subjective and variable, necessitating automated, objective methods.

Method: VOQANet uses a deep learning framework with an attention mechanism and Speech Foundation Model (SFM). VOQANet+ integrates low-level descriptors (jitter, shimmer, HNR) with SFM embeddings for a hybrid approach.

Result: VOQANet+ outperforms baselines in RMSE and PCC, with sentence-level input showing better results than vowel-based input. It remains robust under noisy conditions.

Conclusion: VOQANet+ is effective for real-world and telehealth applications, offering improved accuracy and generalizability over traditional methods.

Abstract: Perceptual voice quality assessment is essential for diagnosing and
monitoring voice disorders by providing standardized evaluations of vocal
function. Traditionally, expert raters use standard scales such as the
Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade,
Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics
are subjective and prone to inter-rater variability, motivating the need for
automated, objective assessment methods. This study proposes Voice Quality
Assessment Network (VOQANet), a deep learning-based framework with an attention
mechanism that leverages a Speech Foundation Model (SFM) to extract high-level
acoustic and prosodic information from raw speech. To enhance robustness and
interpretability, we also introduce VOQANet+, which integrates low-level speech
descriptors such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with
SFM embeddings into a hybrid representation. Unlike prior studies focused only
on vowel-based phonation (PVQD-A subset) of the Perceptual Voice Quality
Dataset (PVQD), we evaluate our models on both vowel-based and sentence-level
speech (PVQD-S subset) to improve generalizability. Results show that
sentence-based input outperforms vowel-based input, especially at the patient
level, underscoring the value of longer utterances for capturing perceptual
voice attributes. VOQANet consistently surpasses baseline methods in root mean
squared error (RMSE) and Pearson correlation coefficient (PCC) across CAPE-V
and GRBAS dimensions, with VOQANet+ achieving even better performance.
Additional experiments under noisy conditions show that VOQANet+ maintains high
prediction accuracy and robustness, supporting its potential for real-world and
telehealth deployment.

</details>


### [529] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/pdf/2505.21568)
*Haiyun Li, Zhiyong Wu, Xiaofeng Xie, Jingran Xie, Yaoxun Xu, Hanyang Peng*

Main category: cs.SD

TL;DR: VoiceMark is a zero-shot VC-resistant watermarking method using speaker-specific latents as watermarks, achieving 95% detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods fail in zero-shot VC scenarios, where models synthesize audio without training.

Method: VoiceMark leverages speaker-specific latents as watermark carriers and uses VC-simulated augmentations and VAD-based loss for robustness.

Result: VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, outperforming existing methods (50%).

Conclusion: VoiceMark is effective for zero-shot VC-resistant watermarking, significantly improving detection accuracy.

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [530] [HARP: A Large-Scale Higher-Order Ambisonic Room Impulse Response Dataset](https://arxiv.org/pdf/2411.14207)
*Shivam Saini, Jürgen Peissig*

Main category: cs.SD

TL;DR: A 7th-order Ambisonic Room Impulse Responses (HOA-RIRs) dataset is introduced, created using the Image Source Method, enabling precise spatial audio reproduction for immersive applications.


<details>
  <summary>Details</summary>
Motivation: To address the need for realistic immersive audio by providing a high-resolution spatial audio dataset, overcoming limitations of traditional microphone arrays.

Method: The dataset is generated via virtual simulation using a unique 64-microphone configuration based on the superposition principle, capturing RIRs directly in the Spherical Harmonics domain.

Result: The dataset includes diverse room configurations (geometry, materials, distances) and offers high spatial resolution, aiding tasks like source localization and reverberation prediction.

Conclusion: This dataset is a valuable resource for spatial audio research, particularly for machine learning in room acoustics and sound field synthesis, enhancing realism in immersive audio applications.

Abstract: This contribution introduces a dataset of 7th-order Ambisonic Room Impulse
Responses (HOA-RIRs), created using the Image Source Method. By employing
higher-order Ambisonics, our dataset enables precise spatial audio
reproduction, a critical requirement for realistic immersive audio
applications. Leveraging the virtual simulation, we present a unique microphone
configuration, based on the superposition principle, designed to optimize sound
field coverage while addressing the limitations of traditional microphone
arrays. The presented 64-microphone configuration allows us to capture RIRs
directly in the Spherical Harmonics domain. The dataset features a wide range
of room configurations, encompassing variations in room geometry, acoustic
absorption materials, and source-receiver distances. A detailed description of
the simulation setup is provided alongside for an accurate reproduction. The
dataset serves as a vital resource for researchers working on spatial audio,
particularly in applications involving machine learning to improve room
acoustics modeling and sound field synthesis. It further provides a very high
level of spatial resolution and realism crucial for tasks such as source
localization, reverberation prediction, and immersive sound reproduction.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [531] [DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control](https://arxiv.org/pdf/2505.23857)
*Wuhao Wang, Zhiyong Chen*

Main category: cs.LG

TL;DR: Proposes DATD3, a novel RL algorithm for output-feedback settings, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges in RL with partial state information by extending MDPs to observation histories.

Method: Introduces DATD3, combining depthwise convolution and multi-head attention for encoding history.

Result: DATD3 surpasses memory-based and recurrent baselines in continuous control tasks.

Conclusion: DATD3 is effective for RL in partial and full observability settings.

Abstract: Reinforcement learning in real-world applications often involves
output-feedback settings, where the agent receives only partial state
information. To address this challenge, we propose the Output-Feedback Markov
Decision Process (OPMDP), which extends the standard MDP formulation to
accommodate decision-making based on observation histories. Building on this
framework, we introduce Depthwise Attention Twin Delayed Deep Deterministic
Policy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise
separable convolution and multi-head attention to encode historical
observations. DATD3 maintains policy expressiveness while avoiding the
instability of recurrent models. Extensive experiments on continuous control
tasks demonstrate that DATD3 outperforms existing memory-based and recurrent
baselines under both partial and full observability.

</details>


### [532] [Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration](https://arxiv.org/pdf/2505.23859)
*Wenju Sun, Qingyong Li, Wen Wang, Yang Liu, Yangli-ao Geng, Boyang Li*

Main category: cs.LG

TL;DR: LOT Merging minimizes feature drift in multi-task model merging, outperforming baselines by up to 4.4%.


<details>
  <summary>Details</summary>
Motivation: Performance degradation in model merging correlates with feature drift, prompting a need for efficient solutions.

Method: Layer-wise Optimal Task Vector Merging (LOT Merging) minimizes feature drift via convex quadratic optimization.

Result: LOT Merging improves performance by up to 4.4% over state-of-the-art methods.

Conclusion: LOT Merging efficiently consolidates models with superior performance and minimal degradation.

Abstract: Multi-task model merging aims to consolidate knowledge from multiple
fine-tuned task-specific experts into a unified model while minimizing
performance degradation. Existing methods primarily approach this by minimizing
differences between task-specific experts and the unified model, either from a
parameter-level or a task-loss perspective. However, parameter-level methods
exhibit a significant performance gap compared to the upper bound, while
task-loss approaches entail costly secondary training procedures. In contrast,
we observe that performance degradation closely correlates with feature drift,
i.e., differences in feature representations of the same sample caused by model
merging. Motivated by this observation, we propose Layer-wise Optimal Task
Vector Merging (LOT Merging), a technique that explicitly minimizes feature
drift between task-specific experts and the unified model in a layer-by-layer
manner. LOT Merging can be formulated as a convex quadratic optimization
problem, enabling us to analytically derive closed-form solutions for the
parameters of linear and normalization layers. Consequently, LOT Merging
achieves efficient model consolidation through basic matrix operations.
Extensive experiments across vision and vision-language benchmarks demonstrate
that LOT Merging significantly outperforms baseline methods, achieving
improvements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.

</details>


### [533] [BiBLDR: Bidirectional Behavior Learning for Drug Repositioning](https://arxiv.org/pdf/2505.23861)
*Renye Zhang, Mengyun Yang, Qichang Zhao, Jianxin Wang*

Main category: cs.LG

TL;DR: BiBLDR introduces a bidirectional behavior learning strategy for drug repositioning, outperforming graph-based methods, especially in cold-start scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of graph-based drug repositioning methods in cold-start scenarios involving novel drugs due to lack of association information.

Method: Proposes BiBLDR, a two-stage strategy: (1) constructs prototype spaces for drugs and diseases, (2) leverages bidirectional behavior sequences to predict associations.

Result: Achieves state-of-the-art performance on benchmarks and excels in cold-start scenarios.

Conclusion: BiBLDR provides a robust and precise framework for drug repositioning, particularly effective for novel drugs.

Abstract: Drug repositioning aims to identify potential new indications for existing
drugs to reduce the time and financial costs associated with developing new
drugs. Most existing deep learning-based drug repositioning methods
predominantly utilize graph-based representations. However, graph-based drug
repositioning methods struggle to perform effective inference in cold-start
scenarios involving novel drugs because of the lack of association information
with the diseases. Unlike traditional graph-based approaches, we propose a
bidirectional behavior learning strategy for drug repositioning, known as
BiBLDR. This innovative framework redefines drug repositioning as a behavior
sequential learning task to capture drug-disease interaction patterns. First,
we construct bidirectional behavioral sequences based on drug and disease
sides. The consideration of bidirectional information ensures a more meticulous
and rigorous characterization of the behavioral sequences. Subsequently, we
propose a two-stage strategy for drug repositioning. In the first stage, we
construct prototype spaces to characterize the representational attributes of
drugs and diseases. In the second stage, these refined prototypes and
bidirectional behavior sequence data are leveraged to predict potential
drug-disease associations. Based on this learning approach, the model can more
robustly and precisely capture the interactive relationships between drug and
disease features from bidirectional behavioral sequences. Extensive experiments
demonstrate that our method achieves state-of-the-art performance on benchmark
datasets. Meanwhile, BiBLDR demonstrates significantly superior performance
compared to previous methods in cold-start scenarios. Our code is published in
https://github.com/Renyeeah/BiBLDR.

</details>


### [534] [Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting](https://arxiv.org/pdf/2505.23863)
*Chang Liu, Bohao Zhao, Jingtao Ding, Huandong Wang, Yong Li*

Main category: cs.LG

TL;DR: PhyxMamba is a novel framework combining Mamba-based state-space models with physics-informed principles to improve long-term forecasting of chaotic systems from short-term observations.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with long-term predictive stability and dynamical coherence in chaotic systems due to their sensitivity to initial conditions and complex attractor geometry.

Method: PhyxMamba reconstructs the attractor manifold using time-delay embeddings, employs generative training, multi-token prediction, and attractor geometry regularization to capture dynamics.

Result: The framework outperforms existing approaches in long-term forecasting and preserves dynamical invariants, validated on diverse chaotic systems.

Conclusion: PhyxMamba enables reliable chaotic system prediction under observation-scarce conditions, with broad applications in climate science, neuroscience, and epidemiology.

Abstract: Long-term forecasting of chaotic systems from short-term observations remains
a fundamental and underexplored challenge due to the intrinsic sensitivity to
initial conditions and the complex geometry of strange attractors. Existing
approaches often rely on long-term training data or focus on short-term
sequence correlations, struggling to maintain predictive stability and
dynamical coherence over extended horizons. We propose PhyxMamba, a novel
framework that integrates a Mamba-based state-space model with physics-informed
principles to capture the underlying dynamics of chaotic systems. By
reconstructing the attractor manifold from brief observations using time-delay
embeddings, PhyxMamba extracts global dynamical features essential for accurate
forecasting. Our generative training scheme enables Mamba to replicate the
physical process, augmented by multi-token prediction and attractor geometry
regularization for physical constraints, enhancing prediction accuracy and
preserving key statistical invariants. Extensive evaluations on diverse
simulated and real-world chaotic systems demonstrate that PhyxMamba delivers
superior long-term forecasting and faithfully captures essential dynamical
invariants from short-term data. This framework opens new avenues for reliably
predicting chaotic systems under observation-scarce conditions, with broad
implications across climate science, neuroscience, epidemiology, and beyond.
Our code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.

</details>


### [535] [Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections](https://arxiv.org/pdf/2505.23864)
*Wei Zhuo, Zhaohuan Zhan, Ziduo Yang, Han Yu*

Main category: cs.LG

TL;DR: FedAux is a federated learning framework for graph data that uses auxiliary projections to align and aggregate local models without sharing raw data, improving accuracy and personalization.


<details>
  <summary>Details</summary>
Motivation: Addressing non-IID challenges in federated learning on graph-structured data where clients hold distinct subgraphs.

Method: Clients train local GNNs and auxiliary projection vectors (APVs) to project embeddings into a 1D space, refined via soft-sorting and 1D convolution. APVs enable similarity-weighted parameter mixing.

Result: FedAux outperforms baselines in accuracy and personalization, supported by theoretical convergence analysis.

Conclusion: FedAux effectively handles heterogeneous graph data in FL, enhancing model performance and knowledge transfer.

Abstract: Federated learning (FL) on graph-structured data typically faces non-IID
challenges, particularly in scenarios where each client holds a distinct
subgraph sampled from a global graph. In this paper, we introduce Federated
learning with Auxiliary projections (FedAux), a personalized subgraph FL
framework that learns to align, compare, and aggregate heterogeneously
distributed local models without sharing raw data or node embeddings. In
FedAux, each client jointly trains (i) a local GNN and (ii) a learnable
auxiliary projection vector (APV) that differentiably projects node embeddings
onto a 1D space. A soft-sorting operation followed by a lightweight 1D
convolution refines these embeddings in the ordered space, enabling the APV to
effectively capture client-specific information. After local training, these
APVs serve as compact signatures that the server uses to compute inter-client
similarities and perform similarity-weighted parameter mixing, yielding
personalized models while preserving cross-client knowledge transfer. Moreover,
we provide rigorous theoretical analysis to establish the convergence and
rationality of our design. Empirical evaluations across diverse graph
benchmarks demonstrate that FedAux substantially outperforms existing baselines
in both accuracy and personalization performance.

</details>


### [536] [Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration](https://arxiv.org/pdf/2505.23865)
*Emanuele Masiero, Vito Trianni, Giuseppe Vizzari, Dimitri Ognibene*

Main category: cs.LG

TL;DR: A two-stage deep learning framework for active exploration in precision agriculture, using LSTM for belief modeling and DQN agents for efficient exploration, with visibility masks to avoid redundant observations.


<details>
  <summary>Details</summary>
Motivation: To develop autonomous systems for crop monitoring that minimize resource consumption while efficiently exploring large-scale agricultural fields.

Method: A pre-trained LSTM updates a probabilistic map and entropy, guiding agents to prioritize informative regions. Three agent architectures (untrained IG-based, DQN, and Double-CNN DQN) are compared, with visibility masks to avoid revisits.

Result: The Double-CNN DQN agent outperforms others in exploration efficiency, especially in larger environments, demonstrating the effectiveness of uncertainty-aware policies.

Conclusion: Uncertainty-aware policies with visibility tracking are robust and scalable. Future work includes curriculum learning, multi-agent cooperation, and transformer-based models for further improvements.

Abstract: Precision agriculture requires efficient autonomous systems for crop
monitoring, where agents must explore large-scale environments while minimizing
resource consumption. This work addresses the problem as an active exploration
task in a grid environment representing an agricultural field. Each cell may
contain targets (e.g., damaged crops) observable from nine predefined points of
view (POVs). Agents must infer the number of targets per cell using partial,
sequential observations.
  We propose a two-stage deep learning framework. A pre-trained LSTM serves as
a belief model, updating a probabilistic map of the environment and its
associated entropy, which defines the expected information gain (IG). This
allows agents to prioritize informative regions. A key contribution is the
inclusion of a POV visibility mask in the input, preserving the Markov property
under partial observability and avoiding revisits to already explored views.
  Three agent architectures were compared: an untrained IG-based agent
selecting actions to maximize entropy reduction; a DQN agent using CNNs over
local 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent
with wider spatial context. Simulations on 20x20 maps showed that the untrained
agent performs well despite its simplicity. The DQN agent matches this
performance when the POV mask is included, while the Double-CNN agent
consistently achieves superior exploration efficiency, especially in larger
environments.
  Results show that uncertainty-aware policies leveraging entropy, belief
states, and visibility tracking lead to robust and scalable exploration. Future
work includes curriculum learning, multi-agent cooperation with shared rewards,
transformer-based models, and intrinsic motivation mechanisms to further
enhance learning efficiency and policy generalization.

</details>


### [537] [Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization](https://arxiv.org/pdf/2505.23866)
*Chengli Tan, Yubo Zhou, Haishan Ye, Guang Dai, Junmin Liu, Zengjie Song, Jiangshe Zhang, Zixiang Zhao, Yunda Hao, Yong Xu*

Main category: cs.LG

TL;DR: SAM and CSAM improve neural network calibration by reducing overconfidence, with CSAM outperforming SAM and other methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks in safety-critical applications often suffer from poor calibration and overconfidence, risking disastrous outcomes.

Method: Proposes SAM to counteract overconfidence by maximizing predictive entropy, and introduces CSAM as an improved variant.

Result: SAM reduces calibration error; CSAM outperforms SAM and other methods consistently.

Conclusion: SAM and CSAM effectively address calibration issues, with CSAM showing superior performance.

Abstract: Deep neural networks have been increasingly used in safety-critical
applications such as medical diagnosis and autonomous driving. However, many
studies suggest that they are prone to being poorly calibrated and have a
propensity for overconfidence, which may have disastrous consequences. In this
paper, unlike standard training such as stochastic gradient descent, we show
that the recently proposed sharpness-aware minimization (SAM) counteracts this
tendency towards overconfidence. The theoretical analysis suggests that SAM
allows us to learn models that are already well-calibrated by implicitly
maximizing the entropy of the predictive distribution. Inspired by this
finding, we further propose a variant of SAM, coined as CSAM, to ameliorate
model calibration. Extensive experiments on various datasets, including
ImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.
Meanwhile, CSAM performs even better than SAM and consistently achieves lower
calibration error than other approaches

</details>


### [538] [Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert](https://arxiv.org/pdf/2505.23868)
*Zhaokun Wang, Jinyu Guo, Jingwen Pu, Lingfeng Chen, Hongli Pu, Jie Ou. Libo Qin, Wenhong Tian*

Main category: cs.LG

TL;DR: Proposes LoPE, a noise-robust adaptation method using asymmetric LoRA poisoning experts, eliminating data cleaning needs.


<details>
  <summary>Details</summary>
Motivation: Address interference from noisy data in fine-tuning without laborious pre-processing or error-prone architecture changes.

Method: Integrates a poisoning expert in asymmetric LoRA, injects noise during fine-tuning, and masks it during inference.

Result: Achieves strong performance and robustness via low-cost noise injection.

Conclusion: LoPE offers a cost-effective, noise-robust solution for fine-tuning without data cleaning.

Abstract: Current parameter-efficient fine-tuning methods for adapting pre-trained
language models to downstream tasks are susceptible to interference from noisy
data. Conventional noise-handling approaches either rely on laborious data
pre-processing or employ model architecture modifications prone to error
accumulation. In contrast to existing noise-process paradigms, we propose a
noise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a
novel framework that enhances model robustness to noise only with generated
noisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE
strategically integrates a dedicated poisoning expert in an asymmetric LoRA
configuration. Through a two-stage paradigm, LoPE performs noise injection on
the poisoning expert during fine-tuning to enhance its noise discrimination and
processing ability. During inference, we selectively mask the dedicated
poisoning expert to leverage purified knowledge acquired by normal experts for
noise-robust output. Extensive experiments demonstrate that LoPE achieves
strong performance and robustness purely through the low-cost noise injection,
which completely eliminates the requirement of data cleaning.

</details>


### [539] [MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection](https://arxiv.org/pdf/2505.23870)
*Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania*

Main category: cs.LG

TL;DR: MaCP is a lightweight adaptation method using cosine projection to enhance efficiency and accuracy in fine-tuning large models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve model efficiency and accuracy with minimal parameters and memory usage by leveraging cosine projection's properties.

Method: Projects weight changes into discrete cosine space, partitions them, and selects critical frequency components.

Result: Achieves superior accuracy, reduced computational complexity, and lower memory usage across various tasks.

Conclusion: MaCP is a highly effective, efficient adaptation method for fine-tuning large models.

Abstract: We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine
Projection, that achieves exceptional performance while requiring minimal
parameters and memory for fine-tuning large foundation models. Its general idea
is to exploit the superior energy compaction and decorrelation properties of
cosine projection to improve both model efficiency and accuracy. Specifically,
it projects the weight change from the low-rank adaptation into the discrete
cosine space. Then, the weight change is partitioned over different levels of
the discrete cosine spectrum, and each partition's most critical frequency
components are selected. Extensive experiments demonstrate the effectiveness of
MaCP across a wide range of single-modality tasks, including natural language
understanding, natural language generation, text summarization, as well as
multi-modality tasks such as image classification and video understanding. MaCP
consistently delivers superior accuracy, significantly reduced computational
complexity, and lower memory requirements compared to existing alternatives.

</details>


### [540] [ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning](https://arxiv.org/pdf/2505.23871)
*Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, Xiu Li*

Main category: cs.LG

TL;DR: ADG uses diffusion models to clean corrupted offline RL datasets, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Handling noise and errors in real-world datasets for offline RL is challenging, especially in high-dimensional state spaces or with multiple corrupted elements.

Method: ADG introduces Ambient DDPM for learning on corrupted data, distinguishes clean data, refines corrupted data, and integrates with any offline RL algorithm.

Result: ADG effectively mitigates data corruption impact, improving offline RL robustness and achieving state-of-the-art results on benchmarks.

Conclusion: ADG is a versatile and effective solution for enhancing offline RL performance by addressing data corruption.

Abstract: Real-world datasets collected from sensors or human inputs are prone to noise
and errors, posing significant challenges for applying offline reinforcement
learning (RL). While existing methods have made progress in addressing
corrupted actions and rewards, they remain insufficient for handling corruption
in high-dimensional state spaces and for cases where multiple elements in the
dataset are corrupted simultaneously. Diffusion models, known for their strong
denoising capabilities, offer a promising direction for this problem-but their
tendency to overfit noisy samples limits their direct applicability. To
overcome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a
novel approach that pioneers the use of diffusion models to tackle data
corruption in offline RL. First, we introduce Ambient Denoising Diffusion
Probabilistic Models (DDPM) from approximated distributions, which enable
learning on partially corrupted datasets with theoretical guarantees. Second,
we use the noise-prediction property of Ambient DDPM to distinguish between
clean and corrupted data, and then use the clean subset to train a standard
DDPM. Third, we employ the trained standard DDPM to refine the previously
identified corrupted data, enhancing data quality for subsequent offline RL
training. A notable strength of ADG is its versatility-it can be seamlessly
integrated with any offline RL algorithm. Experiments on a range of benchmarks,
including MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively
mitigates the impact of corrupted data and improves the robustness of offline
RL under various noise settings, achieving state-of-the-art results.

</details>


### [541] [A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants](https://arxiv.org/pdf/2505.23875)
*Peter Samoaa, Marcus Vukojevic, Morteza Haghir Chehreghani, Antonio Longa*

Main category: cs.LG

TL;DR: The paper introduces RelSC, a new graph-regression dataset from program graphs, addressing the lack of diversity in existing benchmarks. It evaluates GNNs on homogeneous and multi-relational variants, highlighting performance differences.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack diversity, focusing mainly on molecular graphs and citation networks, limiting progress in models for heterogeneous graph structures.

Method: RelSC dataset is created from program graphs with execution-time labels. It has two variants: RelSC-H (homogeneous) and RelSC-M (multi-relational). Various GNN architectures are evaluated on these.

Result: Performance differences are observed between homogeneous and multi-relational settings, underscoring the impact of structural representation.

Conclusion: RelSC serves as a versatile benchmark for advancing graph regression methods, emphasizing the importance of structural diversity.

Abstract: Graph-level regression underpins many real-world applications, yet public
benchmarks remain heavily skewed toward molecular graphs and citation networks.
This limited diversity hinders progress on models that must generalize across
both homogeneous and heterogeneous graph structures. We introduce RelSC, a new
graph-regression dataset built from program graphs that combine syntactic and
semantic information extracted from source code. Each graph is labelled with
the execution-time cost of the corresponding program, providing a continuous
target variable that differs markedly from those found in existing benchmarks.
RelSC is released in two complementary variants. RelSC-H supplies rich node
features under a single (homogeneous) edge type, while RelSC-M preserves the
original multi-relational structure, connecting nodes through multiple edge
types that encode distinct semantic relationships. Together, these variants let
researchers probe how representation choice influences model behaviour. We
evaluate a diverse set of graph neural network architectures on both variants
of RelSC. The results reveal consistent performance differences between the
homogeneous and multi-relational settings, emphasising the importance of
structural representation. These findings demonstrate RelSC's value as a
challenging and versatile benchmark for advancing graph regression methods.

</details>


### [542] [A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size](https://arxiv.org/pdf/2505.23876)
*Polad Geidarov*

Main category: cs.LG

TL;DR: Pre-calculated weights in multilayer perceptrons improve training speed and robustness compared to random initialization, especially with smaller datasets.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency and robustness of neural networks with analytically pre-calculated weights versus random initialization.

Method: Comparative experiments on MNIST dataset with varying training sizes, using pre-calculated and randomly initialized weights.

Result: Networks with pre-calculated weights train faster and perform better with reduced datasets.

Conclusion: Analytically determined weights enhance neural network training efficiency and robustness.

Abstract: The paper discusses the capabilities of multilayer perceptron neural networks
implementing metric recognition methods, for which the values of the weights
are calculated analytically by formulas. Comparative experiments in training a
neural network with pre-calculated weights and with random initialization of
weights on different sizes of the MNIST training dataset are carried out. The
results of the experiments show that a multilayer perceptron with
pre-calculated weights can be trained much faster and is much more robust to
the reduction of the training dataset.

</details>


### [543] [Actor-Critic based Online Data Mixing For Language Model Pre-Training](https://arxiv.org/pdf/2505.23878)
*Jing Ma, Chenhao Dang, Mingjie Liao*

Main category: cs.LG

TL;DR: AC-ODM improves online data mixing for LLM pretraining by using actor-critic networks to dynamically adjust domain weights and consider intra-domain interactions, achieving faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: Existing data mixing methods for LLMs lack adaptability to training dynamics and ignore intra-domain interactions, limiting efficiency and performance.

Method: Developed an actor-critic based online data mixing (AC-ODM) method, using auxiliary networks to dynamically adjust domain weights and a reward function for intra-domain interactions.

Result: AC-ODM-410M reached optimal validation perplexity 71% faster than ODM, improved MMLU accuracy by 27.5%, and HumanEval pass@1 by 2.23x.

Conclusion: AC-ODM efficiently transfers sampling strategies from proxy to target LLMs, enhancing pretraining convergence and downstream task performance.

Abstract: The coverage and composition of pretraining data significantly impacts the
generalization ability of Large Language Models (LLMs). To reduce the carbon
footprint and financial costs of training, some data mixing methods, which
applied the optimized domain weights of a small proxy model to train a larger
one, were proposed. However, these methods did not evolute with the training
dynamics. The existing online data mixing (ODM) method addressed this
limitation by applying the multi-armed bandit algorithm as data sampling
strategy. Yet, it did not consider the intra-domain interactions. In this
paper, we develop an actor-critic based online data mixing (AC-ODM) method,
which captures the varying domain weights by auxiliary actor-critic networks
and consider the intra-domain interactions with the reward function. While
constructing the dataset to pretrain a large target LLM, we directly apply the
actor, which is trained with a small proxy LLM as the environment, as the
sampling strategy. The transfer of sampling strategy can not only ensure the
efficiency of dynamical data mixing, but also expedite the convergence of
pretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,
which invokes the sampling strategy obtained by a proxy LLM with 410M
parameters, reaching the optimal validation perplexity of ODM 71% faster, and
improves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,
about 2.23x better on pass@1 of HumanEval benchmark.

</details>


### [544] [CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data](https://arxiv.org/pdf/2505.23879)
*Caio Cheohen, Vinnícius M. S. Gomes, Manuela L. da Silva*

Main category: cs.LG

TL;DR: A hybrid CNN-LSTM model predicts COVID-19 severity using spike protein sequences and clinical data, achieving high accuracy and highlighting AI's role in genomic surveillance.


<details>
  <summary>Details</summary>
Motivation: The COVID-19 pandemic underscored the need for accurate severity prediction to optimize healthcare resources and patient management, leveraging AI and genomic data.

Method: A hybrid CNN-LSTM model was trained on 3,467 spike protein sequences and clinical metadata, combining CNN for local patterns and LSTM for long-term dependencies.

Result: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, and 85% accuracy, with prevalent lineages aligning with regional trends.

Conclusion: The CNN-LSTM model effectively predicts COVID-19 severity, offering a framework for future outbreaks and precision public health.

Abstract: The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need
for accurate prediction of disease severity to optimize healthcare resource
allocation and patient management. The spike protein, which facilitates viral
entry into host cells, exhibits high mutation rates, particularly in the
receptor-binding domain, influencing viral pathogenicity. Artificial
intelligence approaches, such as deep learning, offer promising solutions for
leveraging genomic and clinical data to predict disease outcomes. Objective:
This study aimed to develop a hybrid CNN-LSTM deep learning model to predict
COVID-19 severity using spike protein sequences and associated clinical
metadata from South American patients. Methods: We retrieved 9,570 spike
protein sequences from the GISAID database, of which 3,467 met inclusion
criteria after standardization. The dataset included 2,313 severe and 1,154
mild cases. A feature engineering pipeline extracted features from sequences,
while demographic and clinical variables were one-hot encoded. A hybrid
CNN-LSTM architecture was trained, combining CNN layers for local pattern
extraction and an LSTM layer for long-term dependency modeling. Results: The
model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,
and recall of 82.85%, demonstrating robust classification performance. Training
stabilized at 85% accuracy with minimal overfitting. The most prevalent
lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional
epidemiological trends, suggesting potential associations between viral
genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model
effectively predicted COVID-19 severity using spike protein sequences and
clinical data, highlighting the utility of AI in genomic surveillance and
precision public health. Despite limitations, this approach provides a
framework for early severity prediction in future outbreaks.

</details>


### [545] [Test-Time Training Done Right](https://arxiv.org/pdf/2505.23884)
*Tianyuan Zhang, Sai Bi, Yicong Hong, Kai Zhang, Fujun Luan, Songlin Yang, Kalyan Sunkavalli, William T. Freeman, Hao Tan*

Main category: cs.LG

TL;DR: LaCT improves Test-Time Training by using large chunk updates (2K-1M tokens), enhancing hardware efficiency and state capacity without complex kernels.


<details>
  <summary>Details</summary>
Motivation: Existing TTT methods are inefficient for long-context data due to small minibatch sizes and low FLOPs utilization.

Method: Proposes Large Chunk Test-Time Training (LaCT) with large chunk updates, improving hardware utilization and state capacity.

Result: Validated across diverse tasks, scaling to 14B-parameter models and 1M-token sequences.

Conclusion: LaCT advances long-context modeling and TTT, enabling efficient large-scale applications.

Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [546] [Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling](https://arxiv.org/pdf/2505.23913)
*Gustavo Sutter Pessurno de Carvalho, Mohammed Abdulrahman, Hao Wang, Sriram Ganapathi Subramanian, Marc St-Aubin, Sharon O'Sullivan, Lawrence Wan, Luis Ricardez-Sandoval, Pascal Poupart, Agustinus Kristiadi*

Main category: cs.LG

TL;DR: A zero-shot Bayesian optimization (BO) method using a pre-trained deep generative model eliminates surrogate fitting and acquisition function optimization, achieving 35x efficiency gain over traditional BO.


<details>
  <summary>Details</summary>
Motivation: Traditional BO methods require costly surrogate model re-training and acquisition function optimization, which introduces tuning complexities.

Method: Proposes a zero-shot BO solution using a pre-trained deep generative model to directly sample from the posterior over the optimum point, equivalent to Thompson sampling.

Result: Achieves a 35x efficiency gain in wall-clock time compared to Gaussian process-based BO, enabling scalable parallel and distributed optimization.

Conclusion: The method offers a cost-effective, efficient alternative to traditional BO, suitable for high-throughput optimization tasks.

Abstract: The optimization of expensive black-box functions is ubiquitous in science
and engineering. A common solution to this problem is Bayesian optimization
(BO), which is generally comprised of two components: (i) a surrogate model and
(ii) an acquisition function, which generally require expensive re-training and
optimization steps at each iteration, respectively. Although recent work
enabled in-context surrogate models that do not require re-training, virtually
all existing BO methods still require acquisition function maximization to
select the next observation, which introduces many knobs to tune, such as Monte
Carlo samplers and multi-start optimizers. In this work, we propose a
completely in-context, zero-shot solution for BO that does not require
surrogate fitting or acquisition function optimization. This is done by using a
pre-trained deep generative model to directly sample from the posterior over
the optimum point. We show that this process is equivalent to Thompson sampling
and demonstrate the capabilities and cost-effectiveness of our foundation model
on a suite of real-world benchmarks. We achieve an efficiency gain of more than
35x in terms of wall-clock time when compared with Gaussian process-based BO,
enabling efficient parallel and distributed BO, e.g., for high-throughput
optimization.

</details>


### [547] [Thompson Sampling in Online RLHF with General Function Approximation](https://arxiv.org/pdf/2505.23927)
*Songtao Feng, Jie Fu*

Main category: cs.LG

TL;DR: The paper studies the statistical efficiency of RLHF algorithms, proposing a model-free posterior sampling algorithm with a theoretical $O(\sqrt{T})$ regret bound.


<details>
  <summary>Details</summary>
Motivation: To align large language models with human preferences, understanding the statistical efficiency of RLHF algorithms is crucial.

Method: A model-free posterior sampling algorithm inspired by Thompson sampling is designed, using Bellman eluder (BE) dimension for complexity analysis.

Result: The algorithm achieves an $O(\sqrt{T})$ regret bound, with factors depending on horizon, BE dimension, and $log$-bracketing number.

Conclusion: The theoretical guarantees and concentration inequalities derived contribute to understanding RLHF efficiency and may have broader applications.

Abstract: Reinforcement learning from human feedback (RLHF) has achieved great
empirical success in aligning large language models (LLMs) with human
preference, and it is of great importance to study the statistical efficiency
of RLHF algorithms from a theoretical perspective. In this work, we consider
the online RLHF setting where the preference data is revealed during the
learning process and study action value function approximation. We design a
model-free posterior sampling algorithm for online RLHF inspired by Thompson
sampling and provide its theoretical guarantee. Specifically, we adopt Bellman
eluder (BE) dimension as the complexity measure of the function class and
establish $O(\sqrt{T})$ regret bound for the proposed algorithm with other
multiplicative factor depending on the horizon, BE dimension and the
$log$-bracketing number of the function class. Further, in the analysis, we
first establish the concentration-type inequality of the squared Bellman error
bound based on the maximum likelihood estimator (MLE) generalization bound,
which plays the crucial rules in obtaining the eluder-type regret bound and may
be of independent interest.

</details>


### [548] [BIRD: Behavior Induction via Representation-structure Distillation](https://arxiv.org/pdf/2505.23933)
*Galen Pogoncheff, Michael Beyeler*

Main category: cs.LG

TL;DR: BIRD (Behavior Induction via Representation-structure Distillation) transfers aligned behavior from teacher to student models by matching internal representations, improving robustness and scalability.


<details>
  <summary>Details</summary>
Motivation: Aligned behavior in models is often lost during fine-tuning, and collecting task-specific aligned data is costly.

Method: BIRD matches the internal representation structure of a student model to a teacher model to transfer aligned behavior.

Result: BIRD outperforms baselines by up to 16% in robust accuracy and works even with simpler, smaller teachers.

Conclusion: BIRD enables scalable alignment of AI systems, with teacher representation properties explaining 85% of transfer success.

Abstract: Human-aligned deep learning models exhibit behaviors consistent with human
values, such as robustness, fairness, and honesty. Transferring these
behavioral properties to models trained on different tasks or data
distributions remains challenging: aligned behavior is easily forgotten during
fine-tuning, and collecting task-specific data that preserves this behavior can
be prohibitively costly. We introduce BIRD (Behavior Induction via
Representation-structure Distillation), a flexible framework for transferring
aligned behavior by matching the internal representation structure of a student
model to that of a teacher. Applied to out-of-distribution robustness in image
classification, BIRD outperforms fine-tuning, transfer learning, and continual
learning methods, improving robust accuracy by up to 16% over the next
strongest baseline. It remains effective even when the teacher is trained on a
much simpler dataset and is $25 \times$ smaller than the student. In a
large-scale study of over 400 teacher-student pairs, we show that three
interpretable and computable properties of the teacher's representations (i.e.,
task relevance, behavioral relevance, and complementary knowledge) explain up
to 85% of the variance in transfer success. These insights offer practical
guidance for teacher selection and design. BIRD turns small, well-aligned
models into scalable alignment seeds, removing a key bottleneck in deploying
safe AI systems in the wild.

</details>


### [549] [Searching Neural Architectures for Sensor Nodes on IoT Gateways](https://arxiv.org/pdf/2505.23939)
*Andrea Mattia Garavagno, Edoardo Ragusa, Antonio Frisoli, Paolo Gastaldo*

Main category: cs.LG

TL;DR: An automatic method for designing Neural Networks (NNs) at the edge, enabling privacy-sensitive ML in IoT applications without data sharing outside the local network.


<details>
  <summary>Details</summary>
Motivation: To enable ML in privacy-sensitive IoT applications (e.g., healthcare and industrial IoT) while keeping data local and secure.

Method: Runs on IoT gateways to design hardware-friendly NNs for sensor nodes without external data sharing.

Result: Achieves state-of-the-art results on the Visual Wake Words dataset, with a search procedure completing in under 10 hours on Raspberry Pi Zero 2.

Conclusion: The method successfully enables edge-based ML for sensitive applications, ensuring data privacy and achieving high performance.

Abstract: This paper presents an automatic method for the design of Neural Networks
(NNs) at the edge, enabling Machine Learning (ML) access even in
privacy-sensitive Internet of Things (IoT) applications. The proposed method
runs on IoT gateways and designs NNs for connected sensor nodes without sharing
the collected data outside the local network, keeping the data in the site of
collection. This approach has the potential to enable ML for Healthcare
Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing
hardware-friendly and custom NNs at the edge for personalized healthcare and
advanced industrial services such as quality control, predictive maintenance,
or fault diagnosis. By preventing data from being disclosed to cloud services,
this method safeguards sensitive information, including industrial secrets and
personal data. The outcomes of a thorough experimental session confirm that --
on the Visual Wake Words dataset -- the proposed approach can achieve
state-of-the-art results by exploiting a search procedure that runs in less
than 10 hours on the Raspberry Pi Zero 2.

</details>


### [550] [Vision Language Models are Biased](https://arxiv.org/pdf/2505.23941)
*An Vo, Khai-Nguyen Nguyen, Mohammad Reza Taesiri, Vy Tuong Dang, Anh Totti Nguyen, Daeyoung Kim*

Main category: cs.LG

TL;DR: VLMs are biased by prior knowledge, performing poorly on objective visual tasks like counting and identification, with minimal improvement from corrective instructions.


<details>
  <summary>Details</summary>
Motivation: To investigate how prior knowledge biases VLMs, reducing accuracy on standard visual tasks.

Method: Tested VLMs on counting and identification tasks across diverse domains, including logos and optical illusions, and introduced counterfactual images with text descriptions.

Result: VLMs scored only 17.05% accuracy on counting tasks, with further accuracy drops when text descriptions were added. Corrective instructions improved accuracy by only +2 points.

Conclusion: VLMs exhibit strong biases due to prior knowledge, highlighting a failure mode and providing a framework for testing such biases.

Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [551] [SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations](https://arxiv.org/pdf/2505.23942)
*Gaurav Sarkar, Jay Gala, Subarna Tripathi*

Main category: cs.LG

TL;DR: SG-Blend is a new activation function combining SSwish and GELU for better performance in deep neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing activation functions like Swish and GELU have domain-specific optima, limiting their universality.

Method: SG-Blend dynamically interpolates SSwish (a symmetric Swish variant) and GELU using learnable parameters.

Result: SG-Blend improves performance across NLP and computer vision tasks with minimal computational cost.

Conclusion: SG-Blend is a versatile, drop-in replacement that outperforms current baselines.

Abstract: The design of activation functions remains a pivotal component in optimizing
deep neural networks. While prevailing choices like Swish and GELU demonstrate
considerable efficacy, they often exhibit domain-specific optima. This work
introduces SG-Blend, a novel activation function that blends our proposed
SSwish, a first-order symmetric variant of Swish and the established GELU
through dynamic interpolation. By adaptively blending these constituent
functions via learnable parameters, SG-Blend aims to harness their
complementary strengths: SSwish's controlled non-monotonicity and symmetry, and
GELU's smooth, probabilistic profile, to achieve a more universally robust
balance between model expressivity and gradient stability. We conduct
comprehensive empirical evaluations across diverse modalities and
architectures, showing performance improvements across all considered natural
language and computer vision tasks and models. These results, achieved with
negligible computational overhead, underscore SG-Blend's potential as a
versatile, drop-in replacement that consistently outperforms strong
contemporary baselines. The code is available at
https://anonymous.4open.science/r/SGBlend-6CBC.

</details>


### [552] [Position: The Future of Bayesian Prediction Is Prior-Fitted](https://arxiv.org/pdf/2505.23947)
*Samuel Müller, Arik Reuter, Noah Hollmann, David Rügamer, Frank Hutter*

Main category: cs.LG

TL;DR: PFNs leverage pre-training on artificial datasets to address low-data scenarios, expanding from small tasks to complex domains, and are argued to be the future of Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address data scarcity by using pre-trained models on artificial datasets, leveraging computational resources for efficient Bayesian inference.

Method: The method involves training neural networks on randomly generated artificial datasets to capture the prior defined by the dataset-generating distribution, as in Prior-data Fitted Networks (PFNs).

Result: PFNs have expanded from small Bayesian tasks to complex domains, showing promise for efficient pre-training in low-data scenarios.

Conclusion: PFNs and amortized inference are seen as the future of Bayesian inference, with potential to overcome current limitations and address data-scarce problems.

Abstract: Training neural networks on randomly generated artificial datasets yields
Bayesian models that capture the prior defined by the dataset-generating
distribution. Prior-data Fitted Networks (PFNs) are a class of methods designed
to leverage this insight. In an era of rapidly increasing computational
resources for pre-training and a near stagnation in the generation of new
real-world data in many applications, PFNs are poised to play a more important
role across a wide range of applications. They enable the efficient allocation
of pre-training compute to low-data scenarios. Originally applied to small
Bayesian modeling tasks, the field of PFNs has significantly expanded to
address more complex domains and larger datasets. This position paper argues
that PFNs and other amortized inference approaches represent the future of
Bayesian inference, leveraging amortized learning to tackle data-scarce
problems. We thus believe they are a fruitful area of research. In this
position paper, we explore their potential and directions to address their
current limitations.

</details>


### [553] [TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks](https://arxiv.org/pdf/2505.23949)
*Xiang Meng, Mehdi Makni, Rahul Mazumder*

Main category: cs.LG

TL;DR: The paper introduces an efficient solver for transposable N:M sparsity masks, scaling to billion-parameter models, improving training efficiency while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: N:M sparsity accelerates forward-pass computations but lacks efficiency during training due to non-preserved patterns during matrix transposition. Existing methods for transposable N:M masks are limited in scalability or flexibility.

Method: The authors formulate mask generation as optimal transport problems, solving them via entropy regularization and Dykstra's algorithm, followed by rounding. A tensor-based GPU implementation ensures scalability and speed.

Result: The method achieves up to 100x speedup with minimal error, integrates with existing pruning frameworks, and maintains performance in large models like LLaMA3.2-8B.

Conclusion: The proposed solver enables efficient transposable N:M sparsity, offering practical benefits for large-scale model training and outperforming traditional sparse models.

Abstract: Network pruning reduces the computational requirements of large neural
networks, with N:M sparsity -- retaining only N out of every M consecutive
weights -- offering a compelling balance between compressed model quality and
hardware acceleration. However, N:M sparsity only accelerates forward-pass
computations, as N:M patterns are not preserved during matrix transposition,
limiting efficiency during training where both passes are computationally
intensive. While transposable N:M sparsity has been proposed to address this
limitation, existing methods for finding transposable N:M sparse masks either
fail to scale to large models or are restricted to M=4 which results in
suboptimal compression-accuracy trade-off. We introduce an efficient solver for
transposable N:M masks that scales to billion-parameter models. We formulate
mask generation as optimal transport problems and solve through entropy
regularization and Dykstra's algorithm, followed by a rounding procedure. Our
tensor-based implementation exploits GPU parallelism, achieving up to 100x
speedup with only 1-10% error compared to existing methods. Our approach can be
integrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT
and ALPS to produce transposable N:M sparse models with arbitrary N:M values.
Experiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains
performance close to its standard N:M counterpart and outperforms standard 2:4
sparse model, showing the practical value of our approach.

</details>


### [554] [Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective](https://arxiv.org/pdf/2505.23954)
*Dylan Zapzalka, Trenton Chang, Lindsay Warrenburg, Sae-Hwan Park, Daniel K. Shenfeld, Ravi B. Parikh, Jenna Wiens, Maggie Makar*

Main category: cs.LG

TL;DR: A causally-motivated method to distinguish and quantify misreporting from genuine feature changes in ML-driven resource allocation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of disentangling misreporting from genuine feature modifications in strategic settings.

Method: Compare causal effects of misreported features on downstream variables between manipulated and unmanipulated datasets.

Result: Proves identifiability of misreporting rate and validates approach with semi-synthetic and real Medicare data.

Conclusion: The method effectively identifies misreporting in real-world scenarios.

Abstract: In settings where ML models are used to inform the allocation of resources,
agents affected by the allocation decisions might have an incentive to
strategically change their features to secure better outcomes. While prior work
has studied strategic responses broadly, disentangling misreporting from
genuine modification remains a fundamental challenge. In this paper, we propose
a causally-motivated approach to identify and quantify how much an agent
misreports on average by distinguishing deceptive changes in their features
from genuine modification. Our key insight is that, unlike genuine
modification, misreported features do not causally affect downstream variables
(i.e., causal descendants). We exploit this asymmetry by comparing the causal
effect of misreported features on their causal descendants as derived from
manipulated datasets against those from unmanipulated datasets. We formally
prove identifiability of the misreporting rate and characterize the variance of
our estimator. We empirically validate our theoretical results using a
semi-synthetic and real Medicare dataset with misreported data, demonstrating
that our approach can be employed to identify misreporting in real-world
scenarios.

</details>


### [555] [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/pdf/2505.23960)
*Henry Conklin*

Main category: cs.LG

TL;DR: The paper introduces quantitative methods to analyze the representational structures in large-scale neural networks, focusing on learning, generalization, and design impacts. It includes a novel entropy estimation technique for models up to 12B parameters.


<details>
  <summary>Details</summary>
Motivation: The lack of unified notation and methods to describe representational spaces in large neural networks motivates the development of tools to analyze their structure and learning processes.

Method: The author identifies structural primitives and uses information-theoretic quantifications to analyze learning and generalization in models like multi-agent RL, sequence-to-sequence models, and LLMs. A novel entropy estimation method is introduced for scalability.

Result: The study reveals parallels between neural network structures and human cognition, showing how language-like constraints drive performance in large models.

Conclusion: The methods provide insights into how large neural networks learn and generalize, bridging gaps between artificial and human cognition.

Abstract: Despite the remarkable success of large large-scale neural networks, we still
lack unified notation for thinking about and describing their representational
spaces. We lack methods to reliably describe how their representations are
structured, how that structure emerges over training, and what kinds of
structures are desirable. This thesis introduces quantitative methods for
identifying systematic structure in a mapping between spaces, and leverages
them to understand how deep-learning models learn to represent information,
what representational structures drive generalisation, and how design decisions
condition the structures that emerge. To do this I identify structural
primitives present in a mapping, along with information theoretic
quantifications of each. These allow us to analyse learning, structure, and
generalisation across multi-agent reinforcement learning models,
sequence-to-sequence models trained on a single task, and Large Language
Models. I also introduce a novel, performant, approach to estimating the
entropy of vector space, that allows this analysis to be applied to models
ranging in size from 1 million to 12 billion parameters.
  The experiments here work to shed light on how large-scale distributed models
of cognition learn, while allowing us to draw parallels between those systems
and their human analogs. They show how the structures of language and the
constraints that give rise to them in many ways parallel the kinds of
structures that drive performance of contemporary neural networks.

</details>


### [556] [Improved Approximations for Hard Graph Problems using Predictions](https://arxiv.org/pdf/2505.23967)
*Anders Aamand, Justin Y. Chen, Siddharth Gollapudi, Sandeep Silwal, Hao Wu*

Main category: cs.LG

TL;DR: Improved approximation algorithms for NP-hard graph problems using edge-based predictions to break standard approximation barriers.


<details>
  <summary>Details</summary>
Motivation: To leverage predictions (e.g., from past data) to enhance approximation algorithms for NP-hard graph problems, overcoming limitations in standard settings.

Method: Edge-based prediction model with two bits per edge, indicating predictions about endpoints in an optimal solution. Algorithms separately handle high-degree (using predictions) and low-degree vertices (without predictions).

Result: Improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and Maximum Independent Set problems, even with weak predictions.

Conclusion: Predictions enable breaking approximation barriers, with a unifying algorithmic approach for diverse graph problems.

Abstract: We design improved approximation algorithms for NP-hard graph problems by
incorporating predictions (e.g., learned from past data). Our prediction model
builds upon and extends the $\varepsilon$-prediction framework by Cohen-Addad,
d'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based
version of this model, where each edge provides two bits of information,
corresponding to predictions about whether each of its endpoints belong to an
optimal solution. Even with weak predictions where each bit is only
$\varepsilon$-correlated with the true solution, this information allows us to
break approximation barriers in the standard setting. We develop algorithms
with improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and
Maximum Independent Set problems (among others). Across these problems, our
algorithms share a unifying theme, where we separately satisfy constraints
related to high degree vertices (using predictions) and low-degree vertices
(without using predictions) and carefully combine the answers.

</details>


### [557] [Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training](https://arxiv.org/pdf/2505.23971)
*William Merrill, Shane Arora, Dirk Groeneveld, Hannaneh Hajishirzi*

Main category: cs.LG

TL;DR: The paper introduces an empirical method to measure the critical batch size (CBS) in language model training, showing its evolution and proposing batch size warmup for efficient training.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating CBS rely on strong assumptions, limiting their practical applicability. The paper aims to provide a direct, empirical approach to measure CBS and understand its behavior during training.

Method: The authors propose a simple, empirical approach to directly measure CBS and track its evolution over training. They apply this to OLMo models of varying sizes (1B and 7B).

Result: CBS starts near 0, increases rapidly early in training, and plateaus later. This trend is consistent across model sizes, suggesting small-scale runs can inform larger ones. Batch size warmup improves efficiency, reducing gradient steps by 43% for OLMo 1B.

Conclusion: Batch size warmup, guided by empirical CBS measurement, enables efficient large-batch training without performance loss, enhancing data parallelism.

Abstract: The right batch size is important when training language models at scale: a
large batch size is necessary for fast training, but a batch size that is too
large will harm token efficiency. To navigate this tradeoff, McCandlish et al.
(2018) suggest that a critical batch size (CBS), below which training will not
substantially degrade loss, can be estimated based on the gradient noise scale
during training. While their method has been adopted in practice, e.g., when
training GPT-3, strong assumptions are required to justify gradient noise as a
proxy for the CBS, which makes it unclear whether their approach should be
trusted in practice, limiting its applicability. In this paper, we introduce a
simple, empirical approach to directly measure the CBS and show how the CBS
evolves over training. Applying our approach to the OLMo models, we find that
CBS is near 0 at initialization, increases rapidly at first, and then plateaus
as training progresses. Furthermore, we find that this trend holds across
different model sizes (1B and 7B), suggesting CBS from small training runs can
inform larger-scale training runs. Our findings about how the CBS changes over
training motivate batch size warmup as a natural way to reliably train language
models at large batch size: start the batch size small and increase it as the
CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to
slightly better loss than the original training run with 43% fewer gradient
steps. This shows how our framework can be applied to reliably train language
models at larger batch sizes, increasing data parallelism without compromising
performance.

</details>


### [558] [Adaptive Deadline and Batch Layered Synchronized Federated Learning](https://arxiv.org/pdf/2505.23973)
*Asaf Goren, Natalie Lang, Nir Shlezinger, Alejandro Cohen*

Main category: cs.LG

TL;DR: ADEL-FL optimizes deadlines and batch sizes in federated learning to address stragglers, improving convergence and accuracy.


<details>
  <summary>Details</summary>
Motivation: Synchronous FL suffers from latency due to device heterogeneity; existing solutions treat timing and workload as static, limiting effectiveness.

Method: ADEL-FL jointly optimizes per-round deadlines and batch sizes for layer-wise aggregation via constrained optimization.

Result: ADEL-FL provides unbiased updates with bounded variance and outperforms alternatives in convergence and accuracy.

Conclusion: ADEL-FL effectively mitigates straggler effects in FL, enhancing performance under heterogeneous conditions.

Abstract: Federated learning (FL) enables collaborative model training across
distributed edge devices while preserving data privacy, and typically operates
in a round-based synchronous manner. However, synchronous FL suffers from
latency bottlenecks due to device heterogeneity, where slower clients
(stragglers) delay or degrade global updates. Prior solutions, such as fixed
deadlines, client selection, and layer-wise partial aggregation, alleviate the
effect of stragglers, but treat round timing and local workload as static
parameters, limiting their effectiveness under strict time constraints. We
propose ADEL-FL, a novel framework that jointly optimizes per-round deadlines
and user-specific batch sizes for layer-wise aggregation. Our approach
formulates a constrained optimization problem minimizing the expected L2
distance to the global optimum under total training time and global rounds. We
provide a convergence analysis under exponential compute models and prove that
ADEL-FL yields unbiased updates with bounded variance. Extensive experiments
demonstrate that ADEL-FL outperforms alternative methods in both convergence
rate and final accuracy under heterogeneous conditions.

</details>


### [559] [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/pdf/2505.23987)
*Vishal Dey, Xiao Hu, Xia Ning*

Main category: cs.LG

TL;DR: C-MuMOInstruct is a new dataset for multi-property molecule optimization, used to train GeLLMO-Cs, an LLM that outperforms baselines by up to 126% in success rate.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to address nuanced, property-specific optimization in drug design, limiting practical use.

Method: Developed C-MuMOInstruct dataset and trained GeLLMO-Cs, an instruction-tuned LLM, for targeted property optimization.

Result: GeLLMO-Cs outperformed baselines, achieving higher success rates and showing strong 0-shot generalization.

Conclusion: GeLLMO-Cs advances realistic, diverse molecule optimization with property-specific objectives.

Abstract: In real-world drug design, molecule optimization requires selectively
improving multiple molecular properties up to pharmaceutically relevant levels,
while maintaining others that already meet such criteria. However, existing
computational approaches and instruction-tuned LLMs fail to capture such
nuanced property-specific objectives, limiting their practical applicability.
To address this, we introduce C-MuMOInstruct, the first instruction-tuning
dataset focused on multi-property optimization with explicit, property-specific
objectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of
instruction-tuned LLMs that can perform targeted property-specific
optimization. Our experiments across 5 in-distribution and 5
out-of-distribution tasks show that GeLLMO-Cs consistently outperform strong
baselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit
impressive 0-shot generalization to novel optimization tasks and unseen
instructions. This offers a step toward a foundational LLM to support
realistic, diverse optimizations with property-specific objectives.
C-MuMOInstruct and code are accessible through
https://github.com/ninglab/GeLLMO-C.

</details>


### [560] [Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting](https://arxiv.org/pdf/2505.24003)
*ChengAo Shen, Wenchao Yu, Ziming Zhao, Dongjin Song, Wei Cheng, Haifeng Chen, Jingchao Ni*

Main category: cs.LG

TL;DR: DMMV, a decomposition-based multi-modal view framework, improves long-term time series forecasting by integrating trend-seasonal decomposition and adaptive decomposition, outperforming 14 SOTA models.


<details>
  <summary>Details</summary>
Motivation: Time series can be represented as images and texts (multi-modal views), revealing complementary patterns. However, applying large vision models to forecasting introduces a bias towards forecasting periods, which DMMV aims to harness.

Method: DMMV uses trend-seasonal decomposition and a novel backcast residual based adaptive decomposition to integrate multi-modal views for forecasting.

Result: DMMV achieves the best MSE on 6 out of 8 benchmark datasets, outperforming single-view and existing multi-modal baselines.

Conclusion: DMMV effectively leverages multi-modal views and decomposition techniques to enhance long-term time series forecasting performance.

Abstract: Time series, typically represented as numerical sequences, can also be
transformed into images and texts, offering multi-modal views (MMVs) of the
same underlying signal. These MMVs can reveal complementary patterns and enable
the use of powerful pre-trained large models, such as large vision models
(LVMs), for long-term time series forecasting (LTSF). However, as we identified
in this work, applying LVMs to LTSF poses an inductive bias towards
"forecasting periods". To harness this bias, we propose DMMV, a novel
decomposition-based multi-modal view framework that leverages trend-seasonal
decomposition and a novel backcast residual based adaptive decomposition to
integrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art
(SOTA) models across diverse datasets show that DMMV outperforms single-view
and existing multi-modal baselines, achieving the best mean squared error (MSE)
on 6 out of 8 benchmark datasets.

</details>


### [561] [How far away are truly hyperparameter-free learning algorithms?](https://arxiv.org/pdf/2505.24005)
*Priya Kasimbeg, Vincent Roulet, Naman Agarwal, Sourabh Medapati, Fabian Pedregosa, Atish Agarwala, George E. Dahl*

Main category: cs.LG

TL;DR: The paper evaluates learning-rate-free methods for hyperparameter-free neural network training, finding they improve but still lag behind traditional methods like NadamW.


<details>
  <summary>Details</summary>
Motivation: To reduce the need for workload-specific hyperparameter tuning in neural networks by exploring learning-rate-free methods.

Method: Evaluated learning-rate-free methods using the AlgoPerf benchmark, freezing non-learning-rate hyperparameters and searching for optimal configurations.

Result: Learning-rate-free methods improved but were slightly outperformed by a calibrated NadamW baseline.

Conclusion: Learning-rate-free methods need further improvement, and strong baselines are crucial for advancing hyperparameter reduction techniques.

Abstract: Despite major advances in methodology, hyperparameter tuning remains a
crucial (and expensive) part of the development of machine learning systems.
Even ignoring architectural choices, deep neural networks have a large number
of optimization and regularization hyperparameters that need to be tuned
carefully per workload in order to obtain the best results. In a perfect world,
training algorithms would not require workload-specific hyperparameter tuning,
but would instead have default settings that performed well across many
workloads. Recently, there has been a growing literature on optimization
methods which attempt to reduce the number of hyperparameters -- particularly
the learning rate and its accompanying schedule. Given these developments, how
far away is the dream of neural network training algorithms that completely
obviate the need for painful tuning?
  In this paper, we evaluate the potential of learning-rate-free methods as
components of hyperparameter-free methods. We freeze their (non-learning rate)
hyperparameters to default values, and score their performance using the
recently-proposed AlgoPerf: Training Algorithms benchmark. We found that
literature-supplied default settings performed poorly on the benchmark, so we
performed a search for hyperparameter configurations that performed well across
all workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free
methods had much improved performance but still lagged slightly behind a
similarly calibrated NadamW baseline in overall benchmark score. Our results
suggest that there is still much room for improvement for learning-rate-free
methods, and that testing against a strong, workload-agnostic baseline is
important to improve hyperparameter reduction techniques.

</details>


### [562] [The Rich and the Simple: On the Implicit Bias of Adam and SGD](https://arxiv.org/pdf/2505.24022)
*Bhavya Vasudeva, Jung Whan Lee, Vatsal Sharan, Mahdi Soltanolkotabi*

Main category: cs.LG

TL;DR: Adam resists simplicity bias, unlike SGD, leading to richer features and better generalization in neural networks.


<details>
  <summary>Details</summary>
Motivation: To understand the implicit biases of Adam and how they differ from SGD, particularly in terms of simplicity bias and generalization.

Method: Analyze Adam and GD on a binary classification task with synthetic Gaussian data, focusing on decision boundaries and feature diversity.

Result: Adam produces nonlinear, Bayes-optimal boundaries with higher test accuracy, while SGD shows simplicity bias with suboptimal margins.

Conclusion: Adam's resistance to simplicity bias enhances generalization, especially in datasets with spurious correlations.

Abstract: Adam is the de facto optimization algorithm for several deep learning
applications, but an understanding of its implicit bias and how it differs from
other algorithms, particularly standard first-order methods such as
(stochastic) gradient descent (GD), remains limited. In practice, neural
networks trained with SGD are known to exhibit simplicity bias -- a tendency to
find simple solutions. In contrast, we show that Adam is more resistant to such
simplicity bias. To demystify this phenomenon, in this paper, we investigate
the differences in the implicit biases of Adam and GD when training two-layer
ReLU neural networks on a binary classification task involving synthetic data
with Gaussian clusters. We find that GD exhibits a simplicity bias, resulting
in a linear decision boundary with a suboptimal margin, whereas Adam leads to
much richer and more diverse features, producing a nonlinear boundary that is
closer to the Bayes' optimal predictor. This richer decision boundary also
allows Adam to achieve higher test accuracy both in-distribution and under
certain distribution shifts. We theoretically prove these results by analyzing
the population gradients. To corroborate our theoretical findings, we present
empirical results showing that this property of Adam leads to superior
generalization across datasets with spurious correlations where neural networks
trained with SGD are known to show simplicity bias and don't generalize well
under certain distributional shifts.

</details>


### [563] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/pdf/2505.24030)
*Ziming Zhao, ChengAo Shen, Hanghang Tong, Dongjin Song, Zhigang Deng, Qingsong Wen, Jingchao Ni*

Main category: cs.LG

TL;DR: The paper investigates the utility of Large Vision Models (LVMs) in time series analysis, finding they are effective for classification but face challenges in forecasting.


<details>
  <summary>Details</summary>
Motivation: To determine whether LVMs are truly useful for time series analysis, given the debate around Transformer and LLMs in this domain.

Method: A principled study involving 4 LVMs, 8 imaging methods, 18 datasets, and 26 baselines across classification and forecasting tasks, with extensive ablation analysis.

Result: LVMs are useful for time series classification but struggle with forecasting, showing biases and limitations in handling long look-back windows.

Conclusion: The study serves as a foundation for future research on LVMs and multimodal solutions for time series tasks.

Abstract: Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [564] [LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Trainin](https://arxiv.org/pdf/2505.24034)
*Bo Wu, Sid Wang, Yunhao Tang, Jia Ding, Eryk Helenowski, Liang Tan, Tengyu Xu, Tushar Gowda, Zhengxing Chen, Chen Zhu, Xiaocheng Tang, Yundi Qian, Beibei Zhu, Rui Hou*

Main category: cs.LG

TL;DR: LlamaRL is a distributed, asynchronous RL framework for efficient training of large-scale LLMs, achieving up to 10.7x speed-up over existing systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high latency and memory demands in RL for large-scale LLMs.

Method: Introduces a single-controller architecture on PyTorch, with colocated model offloading, asynchronous off-policy training, and distributed memory access.

Result: Achieves significant efficiency gains, especially with larger models (e.g., 405B parameters).

Conclusion: LlamaRL is scalable and efficient, making it suitable for future large-scale RL training.

Abstract: Reinforcement Learning (RL) has become the most effective post-training
approach for improving the capabilities of Large Language Models (LLMs). In
practice, because of the high demands on latency and memory, it is particularly
challenging to develop an efficient RL framework that reliably manages policy
models with hundreds to thousands of billions of parameters.
  In this paper, we present LlamaRL, a fully distributed, asynchronous RL
framework optimized for efficient training of large-scale LLMs with various
model sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a
handful to thousands of devices. LlamaRL introduces a streamlined,
single-controller architecture built entirely on native PyTorch, enabling
modularity, ease of use, and seamless scalability to thousands of GPUs. We also
provide a theoretical analysis of LlamaRL's efficiency, including a formal
proof that its asynchronous design leads to strict RL speed-up. Empirically, by
leveraging best practices such as colocated model offloading, asynchronous
off-policy training, and distributed direct memory access for weight
synchronization, LlamaRL achieves significant efficiency gains -- up to 10.7x
speed-up compared to DeepSpeed-Chat-like systems on a 405B-parameter policy
model. Furthermore, the efficiency advantage continues to grow with increasing
model scale, demonstrating the framework's suitability for future large-scale
RL training.

</details>


### [565] [NeuronTune: Towards Self-Guided Spurious Bias Mitigation](https://arxiv.org/pdf/2505.24048)
*Guangtao Zheng, Wenqian Ye, Aidong Zhang*

Main category: cs.LG

TL;DR: NeuronTune is a post hoc method to mitigate spurious bias in deep neural networks by regulating neurons causing spurious predictions, without needing external annotations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often rely on spurious correlations (e.g., background features) for predictions, degrading performance on unbiased data. Existing methods require external annotations, which are impractical.

Method: NeuronTune probes a model's latent space to identify and regulate neurons linked to spurious predictions, theoretically justifying its approach.

Result: Experiments show NeuronTune effectively reduces spurious bias across architectures and data modalities without external annotations.

Conclusion: NeuronTune is a practical, self-guided tool for improving model robustness by mitigating spurious bias.

Abstract: Deep neural networks often develop spurious bias, reliance on correlations
between non-essential features and classes for predictions. For example, a
model may identify objects based on frequently co-occurring backgrounds rather
than intrinsic features, resulting in degraded performance on data lacking
these correlations. Existing mitigation approaches typically depend on external
annotations of spurious correlations, which may be difficult to obtain and are
not relevant to the spurious bias in a model. In this paper, we take a step
towards self-guided mitigation of spurious bias by proposing NeuronTune, a post
hoc method that directly intervenes in a model's internal decision process. Our
method probes in a model's latent embedding space to identify and regulate
neurons that lead to spurious prediction behaviors. We theoretically justify
our approach and show that it brings the model closer to an unbiased one.
Unlike previous methods, NeuronTune operates without requiring spurious
correlation annotations, making it a practical and effective tool for improving
model robustness. Experiments across different architectures and data
modalities demonstrate that our method significantly mitigates spurious bias in
a self-guided way.

</details>


### [566] [Differential Gated Self-Attention](https://arxiv.org/pdf/2505.24054)
*Elpiniki Maria Lygizou, Mónika Farsang, Radu Grosu*

Main category: cs.LG

TL;DR: Proposes Multihead Differential Gated Self-Attention (M-DGSA) for dynamic noise suppression in Transformers, improving robustness with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Transformers are vulnerable to corrupted inputs due to uniform self-attention; biological lateral inhibition inspires a solution.

Method: M-DGSA uses per-head gating with excitatory/inhibitory branches and sigmoid fusion for context-aware contrast enhancement.

Result: Outperforms vanilla Transformer, Vision Transformer, and Differential Transformer in vision and language tasks.

Conclusion: M-DGSA enhances noise resilience and cross-domain applicability, integrating biological principles into self-attention.

Abstract: Transformers excel across a large variety of tasks but remain susceptible to
corrupted inputs, since standard self-attention treats all query-key
interactions uniformly. Inspired by lateral inhibition in biological neural
circuits and building on the recent use by the Differential Transformer's use
of two parallel softmax subtraction for noise cancellation, we propose
Multihead Differential Gated Self-Attention (M-DGSA) that learns per-head
input-dependent gating to dynamically suppress attention noise. Each head
splits into excitatory and inhibitory branches whose dual softmax maps are
fused by a sigmoid gate predicted from the token embedding, yielding a
context-aware contrast enhancement. M-DGSA integrates seamlessly into existing
Transformer stacks with minimal computational overhead. We evaluate on both
vision and language benchmarks, demonstrating consistent robustness gains over
vanilla Transformer, Vision Transformer, and Differential Transformer
baselines. Our contributions are (i) a novel input-dependent gating mechanism
for self-attention grounded in lateral inhibition, (ii) a principled synthesis
of biological contrast-enhancement and self-attention theory, and (iii)
comprehensive experiments demonstrating noise resilience and cross-domain
applicability.

</details>


### [567] [Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs](https://arxiv.org/pdf/2505.24055)
*Yilong Wang, Tianxiang Zhao, Zongyu Wu, Suhang Wang*

Main category: cs.LG

TL;DR: A novel framework for unsupervised domain adaptation in GNNs uses link prediction to connect source and target graphs, improving adaptation by aligning neighborhoods and preserving discriminative information.


<details>
  <summary>Details</summary>
Motivation: GNNs require abundant labeled data, which is costly. Existing UDA techniques for graphs often ignore graph structure and message-passing, failing under label distribution shifts.

Method: Proposes a framework using link prediction to connect source and target graphs, aligning neighborhoods and employing an identity-preserving learning objective.

Result: Experiments on real-world datasets show the framework's effectiveness in adapting GNNs across domains.

Conclusion: The method successfully addresses domain adaptation in GNNs by leveraging graph structure and preserving target discriminative information.

Abstract: Graph neural networks (GNNs) have shown great ability for node classification
on graphs. However, the success of GNNs relies on abundant labeled data, while
obtaining high-quality labels is costly and challenging, especially for newly
emerging domains. Hence, unsupervised domain adaptation (UDA), which trains a
classifier on the labeled source graph and adapts it to the unlabeled target
graph, is attracting increasing attention. Various approaches have been
proposed to alleviate the distribution shift between the source and target
graphs to facilitate the classifier adaptation. However, most of them simply
adopt existing UDA techniques developed for independent and identically
distributed data to gain domain-invariant node embeddings for graphs, which do
not fully consider the graph structure and message-passing mechanism of GNNs
during the adaptation and will fail when label distribution shift exists among
domains. In this paper, we proposed a novel framework that adopts link
prediction to connect nodes between source and target graphs, which can
facilitate message-passing between the source and target graphs and augment the
target nodes to have ``in-distribution'' neighborhoods with the source domain.
This strategy modified the target graph on the input level to reduce its
deviation from the source domain in the embedding space and is insensitive to
disproportional label distributions across domains. To prevent the loss of
discriminative information in the target graph, we further design a novel
identity-preserving learning objective, which guides the learning of the edge
insertion module together with reconstruction and adaptation losses.
Experimental results on real-world datasets demonstrate the effectiveness of
our framework.

</details>


### [568] [Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition](https://arxiv.org/pdf/2505.24059)
*Sean Foley, Hong Nguyen, Jihwan Lee, Sudarsana Reddy Kadiri, Dani Byrd, Louis Goldstein, Shrikanth Narayanan*

Main category: cs.LG

TL;DR: The study focuses on improving phoneme recognition by using a single-speaker MRI corpus to reduce cross-speaker variability, comparing unimodal and multimodal models.


<details>
  <summary>Details</summary>
Motivation: Previous studies relied on multi-speaker corpora, which introduced variability, hindering detailed learning of acoustic-articulatory relationships.

Method: Developed unimodal (audio, video) and multimodal models for phoneme recognition using a long-form single-speaker MRI corpus.

Result: Audio and multimodal models performed similarly on phonetic manner classes but differed on places of articulation. Latent space analysis revealed similar phonetic encoding, while attention weights highlighted timing differences.

Conclusion: Single-speaker data reduces variability, enabling better interpretation of modality contributions in phoneme recognition.

Abstract: Although many previous studies have carried out multimodal learning with
real-time MRI data that captures the audio-visual kinematics of the vocal tract
during speech, these studies have been limited by their reliance on
multi-speaker corpora. This prevents such models from learning a detailed
relationship between acoustics and articulation due to considerable
cross-speaker variability. In this study, we develop unimodal audio and video
models as well as multimodal models for phoneme recognition using a long-form
single-speaker MRI corpus, with the goal of disentangling and interpreting the
contributions of each modality. Audio and multimodal models show similar
performance on different phonetic manner classes but diverge on places of
articulation. Interpretation of the models' latent space shows similar encoding
of the phonetic space across audio and multimodal models, while the models'
attention weights highlight differences in acoustic and articulatory timing for
certain phonemes.

</details>


### [569] [Characterising the Inductive Biases of Neural Networks on Boolean Data](https://arxiv.org/pdf/2505.24060)
*Chris Mingard, Lukas Seier, Niclas Göring, Andrei-Vlad Badelita, Charles London, Ard Louis*

Main category: cs.LG

TL;DR: The paper studies how inductive bias and feature learning in deep neural networks drive generalization, using a depth-2 network trained on Boolean functions as a tractable case study.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive explanation of generalization in deep neural networks, linking inductive prior, training dynamics, and feature learning, which existing partial explanations (e.g., NTK-based) overlook.

Method: A depth-2 discrete fully connected network is trained on Boolean functions, exploiting their correspondence with DNF formulas, under a Monte Carlo learning algorithm.

Result: The model shows predictable training dynamics, interpretable feature emergence, and detailed insights into how inductive bias and feature formation drive generalization.

Conclusion: The framework successfully traces the interplay of inductive bias and feature learning in generalization, offering an end-to-end, analytically tractable explanation.

Abstract: Deep neural networks are renowned for their ability to generalise well across
diverse tasks, even when heavily overparameterized. Existing works offer only
partial explanations (for example, the NTK-based task-model alignment
explanation neglects feature learning). Here, we provide an end-to-end,
analytically tractable case study that links a network's inductive prior, its
training dynamics including feature learning, and its eventual generalisation.
Specifically, we exploit the one-to-one correspondence between depth-2 discrete
fully connected networks and disjunctive normal form (DNF) formulas by training
on Boolean functions. Under a Monte Carlo learning algorithm, our model
exhibits predictable training dynamics and the emergence of interpretable
features. This framework allows us to trace, in detail, how inductive bias and
feature formation drive generalisation.

</details>


### [570] [Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning](https://arxiv.org/pdf/2505.24061)
*Jiashun Liu, Zihao Wu, Johan Obando-Ceron, Pablo Samuel Castro, Aaron Courville, Ling Pan*

Main category: cs.LG

TL;DR: GraMa (Gradient Magnitude Neural Activity Metric) is introduced to measure neuron-level learning capacity in deep RL, outperforming activation-based metrics and improving performance when used for neuron resetting (ReGraMa).


<details>
  <summary>Details</summary>
Motivation: Deep RL agents face neuronal activity loss, hindering adaptation. Existing activation-based metrics (e.g., tau-dormant neuron ratio) fail in complex architectures.

Method: GraMa shifts focus from activations to gradients, offering a lightweight, architecture-agnostic metric. ReGraMa resets neurons based on GraMa.

Result: GraMa effectively identifies inactive neurons across diverse architectures. ReGraMa boosts performance in benchmarks like MuJoCo and DeepMind Control Suite.

Conclusion: GraMa and ReGraMa provide a scalable solution for maintaining learning capacity in deep RL, applicable to varied architectures and tasks.

Abstract: Deep reinforcement learning (RL) agents frequently suffer from neuronal
activity loss, which impairs their ability to adapt to new data and learn
continually. A common method to quantify and address this issue is the
tau-dormant neuron ratio, which uses activation statistics to measure the
expressive ability of neurons. While effective for simple MLP-based agents,
this approach loses statistical power in more complex architectures. To address
this, we argue that in advanced RL agents, maintaining a neuron's learning
capacity, its ability to adapt via gradient updates, is more critical than
preserving its expressive ability. Based on this insight, we shift the
statistical objective from activations to gradients, and introduce GraMa
(Gradient Magnitude Neural Activity Metric), a lightweight,
architecture-agnostic metric for quantifying neuron-level learning capacity. We
show that GraMa effectively reveals persistent neuron inactivity across diverse
architectures, including residual networks, diffusion models, and agents with
varied activation functions. Moreover, resetting neurons guided by GraMa
(ReGraMa) consistently improves learning performance across multiple deep RL
algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.

</details>


### [571] [Primal-Dual Neural Algorithmic Reasoning](https://arxiv.org/pdf/2505.24067)
*Yu He, Ellen Vitercik*

Main category: cs.LG

TL;DR: A Neural Algorithmic Reasoning (NAR) framework is introduced, leveraging the primal-dual paradigm to simulate and outperform approximation algorithms, with robust generalization and practical applications.


<details>
  <summary>Details</summary>
Motivation: Extending NAR to harder problems beyond polynomial-time-solvable ones remains a challenge, addressed here using the primal-dual paradigm.

Method: The framework uses a bipartite representation aligning primal-dual algorithms with Graph Neural Networks, incorporating optimal solutions from small instances.

Result: The model outperforms approximation algorithms, generalizes well to larger and out-of-distribution graphs, and integrates with commercial solvers for real-world use.

Conclusion: The proposed NAR framework advances algorithmic reasoning, offering practical utility and improved performance over classical methods.

Abstract: Neural Algorithmic Reasoning (NAR) trains neural networks to simulate
classical algorithms, enabling structured and interpretable reasoning over
complex data. While prior research has predominantly focused on learning exact
algorithms for polynomial-time-solvable problems, extending NAR to harder
problems remains an open challenge. In this work, we introduce a general NAR
framework grounded in the primal-dual paradigm, a classical method for
designing efficient approximation algorithms. By leveraging a bipartite
representation between primal and dual variables, we establish an alignment
between primal-dual algorithms and Graph Neural Networks. Furthermore, we
incorporate optimal solutions from small instances to greatly enhance the
model's reasoning capabilities. Our empirical results demonstrate that our
model not only simulates but also outperforms approximation algorithms for
multiple tasks, exhibiting robust generalization to larger and
out-of-distribution graphs. Moreover, we highlight the framework's practical
utility by integrating it with commercial solvers and applying it to real-world
datasets.

</details>


### [572] [DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures](https://arxiv.org/pdf/2505.24069)
*Yu He, Yingxi Li, Colin White, Ellen Vitercik*

Main category: cs.LG

TL;DR: DSR-Bench is a benchmark for evaluating LLMs' structural reasoning using data structures, revealing gaps in multi-attribute and multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack focus on structural reasoning, a core requirement for real-world LLM tasks.

Method: DSR-Bench includes 20 data structures, 35 operations, and 4,140 problem instances, with an automated, deterministic evaluation pipeline.

Result: Instruction-tuned models struggle with basic reasoning, and even reasoning-oriented models score only 47% on complex tasks.

Conclusion: LLMs have significant limitations in structural reasoning, especially for multi-dimensional data and natural language tasks.

Abstract: Large language models (LLMs) are increasingly deployed for real-world tasks
that fundamentally involve data manipulation. A core requirement across these
tasks is the ability to perform structural reasoning--that is, to understand
and reason about data relationships. For example, customer requests require a
temporal ordering, which can be represented by data structures such as queues.
However, existing benchmarks primarily focus on high-level, application-driven
evaluations without isolating this fundamental capability. To address this gap,
we introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning
capabilities through data structures, which provide interpretable
representations of data relationships. DSR-Bench includes 20 data structures,
35 operations, and 4,140 problem instances, organized hierarchically for
fine-grained analysis of reasoning limitations. Our evaluation pipeline is
fully automated and deterministic, eliminating subjective human or model-based
judgments. Its synthetic nature also ensures scalability and minimizes data
contamination risks. We benchmark nine state-of-the-art LLMs. Our analysis
shows that instruction-tuned models struggle with basic multi-attribute and
multi-hop reasoning. Furthermore, while reasoning-oriented models perform
better, they remain fragile on complex and hybrid structures, with the best
model achieving an average score of only 47% on the challenge subset.
Crucially, models often perform poorly on multi-dimensional data and natural
language task descriptions, highlighting a critical gap for real-world
deployment.

</details>


### [573] [DeepBoost-AF: A Novel Unsupervised Feature Learning and Gradient Boosting Fusion for Robust Atrial Fibrillation Detection in Raw ECG Signals](https://arxiv.org/pdf/2505.24085)
*Alireza Jafari, Fereshteh Yousefirizi, Vahid Seydi*

Main category: cs.LG

TL;DR: A hybrid method combining deep learning and gradient boosting improves atrial fibrillation (AF) detection with high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: AF is a serious cardiac condition requiring timely detection to reduce stroke risks. Current methods lack efficiency or accuracy.

Method: A 19-layer deep convolutional autoencoder (DCAE) is integrated with gradient boosting models (AdaBoost, XGBoost, LightGBM) for automated AF detection without manual feature extraction.

Result: The DCAE-LGBM model achieves an F1-score of 95.20%, sensitivity of 99.99%, and inference latency of four seconds, outperforming existing methods.

Conclusion: The hybrid system is a reliable, automated solution for AF detection in clinical settings, enhancing accuracy and efficiency.

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia associated with
elevated health risks, where timely detection is pivotal for mitigating
stroke-related morbidity. This study introduces an innovative hybrid
methodology integrating unsupervised deep learning and gradient boosting models
to improve AF detection. A 19-layer deep convolutional autoencoder (DCAE) is
coupled with three boosting classifiers-AdaBoost, XGBoost, and LightGBM
(LGBM)-to harness their complementary advantages while addressing individual
limitations. The proposed framework uniquely combines DCAE with gradient
boosting, enabling end-to-end AF identification devoid of manual feature
extraction. The DCAE-LGBM model attains an F1-score of 95.20%, sensitivity of
99.99%, and inference latency of four seconds, outperforming existing methods
and aligning with clinical deployment requirements. The DCAE integration
significantly enhances boosting models, positioning this hybrid system as a
reliable tool for automated AF detection in clinical settings.

</details>


### [574] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/pdf/2505.24088)
*Chen Huang, Skyler Seto, Hadi Pouransari, Mehrdad Farajtabar, Raviteja Vemulapalli, Fartash Faghri, Oncel Tuzel, Barry-John Theobald, Josh Susskind*

Main category: cs.LG

TL;DR: Proxy-FDA is a novel regularization method that preserves structural knowledge in feature space during fine-tuning of vision foundation models, reducing concept forgetting.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning foundation models on one task often leads to forgetting of prior knowledge. Existing methods focus on point-wise matching, which lacks awareness of feature neighborhood structures.

Method: Proxy-FDA aligns feature distributions using nearest neighbor graphs and dynamically generated informative proxies to preserve structural knowledge.

Result: Proxy-FDA significantly reduces concept forgetting and shows a strong correlation between forgetting and a distributional distance metric. It performs well in various fine-tuning settings and tasks.

Conclusion: Proxy-FDA effectively mitigates concept forgetting by preserving feature space structures, outperforming traditional methods in diverse fine-tuning scenarios.

Abstract: Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [575] [Practical Bayes-Optimal Membership Inference Attacks](https://arxiv.org/pdf/2505.24089)
*Marcus Lassila, Johan Östman, Khac-Hoang Ngo, Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: The paper introduces practical and theoretically grounded membership inference attacks (MIAs) for i.i.d. and graph data, proposing efficient approximations (BASE and G-BASE) that outperform prior methods.


<details>
  <summary>Details</summary>
Motivation: To address key questions about optimal query strategies in graph settings and improve the efficiency and performance of MIAs.

Method: Derives the Bayes-optimal membership inference rule for node-level MIAs, introduces BASE and G-BASE as efficient approximations.

Result: G-BASE outperforms prior classifier-based node-level MIA attacks; BASE matches or exceeds state-of-the-art MIAs like LiRA and RMIA at lower cost.

Conclusion: BASE and RMIA are equivalent under specific settings, providing a principled justification for RMIA.

Abstract: We develop practical and theoretically grounded membership inference attacks
(MIAs) against both independent and identically distributed (i.i.d.) data and
graph-structured data. Building on the Bayesian decision-theoretic framework of
Sablayrolles et al., we derive the Bayes-optimal membership inference rule for
node-level MIAs against graph neural networks, addressing key open questions
about optimal query strategies in the graph setting. We introduce BASE and
G-BASE, computationally efficient approximations of the Bayes-optimal attack.
G-BASE achieves superior performance compared to previously proposed
classifier-based node-level MIA attacks. BASE, which is also applicable to
non-graph data, matches or exceeds the performance of prior state-of-the-art
MIAs, such as LiRA and RMIA, at a significantly lower computational cost.
Finally, we show that BASE and RMIA are equivalent under a specific
hyperparameter setting, providing a principled, Bayes-optimal justification for
the RMIA attack.

</details>


### [576] [A SHAP-based explainable multi-level stacking ensemble learning method for predicting the length of stay in acute stroke](https://arxiv.org/pdf/2505.24101)
*Zhenran Xu*

Main category: cs.LG

TL;DR: An interpretable multi-level stacking ensemble model was developed to predict prolonged length of stay (LOS) in acute stroke, outperforming logistic regression for ischaemic stroke but not for haemorrhagic stroke.


<details>
  <summary>Details</summary>
Motivation: Improving care planning by enhancing the efficiency, performance, and interpretability of LOS prediction models for acute stroke.

Method: Used data from the Stroke Foundation Acute Audit in Australia, refined predictors via feature selection, and developed separate models for ischaemic and haemorrhagic stroke. Evaluated models using AUC, calibration curves, and SHAP plots.

Result: The ensemble model achieved superior performance for ischaemic stroke (AUC: 0.824) but not for haemorrhagic stroke (AUC: 0.843 vs. 0.828). Key predictors included rehabilitation assessment and stroke unit care.

Conclusion: The model effectively predicts prolonged LOS in ischaemic stroke, but further validation is needed for haemorrhagic stroke.

Abstract: Length of stay (LOS) prediction in acute stroke is critical for improving
care planning. Existing machine learning models have shown suboptimal
predictive performance, limited generalisability, and have overlooked
system-level factors. We aimed to enhance model efficiency, performance, and
interpretability by refining predictors and developing an interpretable
multi-level stacking ensemble model. Data were accessed from the biennial
Stroke Foundation Acute Audit (2015, 2017, 2019, 2021) in Australia. Models
were developed for ischaemic and haemorrhagic stroke separately. The outcome
was prolonged LOS (the LOS above the 75th percentile). Candidate predictors
(ischaemic: n=89; haemorrhagic: n=83) were categorised into patient, clinical,
and system domains. Feature selection with correlation-based approaches was
used to refine key predictors. The evaluation of models included discrimination
(AUC), calibration curves, and interpretability (SHAP plots). In ischaemic
stroke (N=12,575), prolonged LOS was >=9 days, compared to >=11 days in
haemorrhagic stroke (N=1,970). The ensemble model achieved superior performance
[AUC: 0.824 (95% CI: 0.801-0.846)] and statistically outperformed logistic
regression [AUC: 0.805 (95% CI: 0.782-0.829); P=0.0004] for ischaemic. However,
the model [AUC: 0.843 (95% CI: 0.790-0.895)] did not statistically outperform
logistic regression [AUC: 0.828 (95% CI: 0.774-0.882); P=0.136] for
haemorrhagic. SHAP analysis identified shared predictors for both types of
stroke: rehabilitation assessment, urinary incontinence, stroke unit care,
inability to walk independently, physiotherapy, and stroke care coordinators
involvement. An explainable ensemble model effectively predicted the prolonged
LOS in ischaemic stroke. Further validation in larger cohorts is needed for
haemorrhagic stroke.

</details>


### [577] [Neural Networks as Universal Finite-State Machines: A Constructive ReLU Simulation Framework for NFAs](https://arxiv.org/pdf/2505.24110)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: The paper proves that ReLU neural networks can exactly simulate nondeterministic finite automata (NFAs) using binary state encoding and sparse linear layers, achieving perfect alignment with automata theory.


<details>
  <summary>Details</summary>
Motivation: To bridge automata theory and neural computation by demonstrating how ReLU networks can precisely simulate NFAs without approximation or recurrence.

Method: Encodes NFA states as binary vectors and transitions as sparse linear layers, using ReLU activations to simulate nondeterministic branching and other automata operations.

Result: A three-layer ReLU network of width O(n) can exactly recognize any regular language accepted by an n-state NFA, with perfect empirical alignment in experiments.

Conclusion: The work unites automata theory and neural computation, providing a provably complete symbolic simulation of NFAs in standard deep learning architectures.

Abstract: We present a formal and constructive framework establishing the equivalence
between nondeterministic finite automata (NFAs) and standard feedforward ReLU
neural networks. By encoding automaton states as binary vectors and transitions
as sparse linear layers, we show that ReLU activations simulate
nondeterministic branching, subset construction, and $\epsilon$-closures in a
mathematically precise manner. Our core theoretical results prove that a
three-layer ReLU network of width $\mathcal{O}(n)$ can exactly recognize any
regular language accepted by an $n$-state NFA-without recurrence, memory, or
approximation. Furthermore, we show that gradient descent over
structure-preserving networks preserves symbolic semantics and acceptance
behavior. Extensive experiments across multiple validation tasks-including
parallel path tracking, symbolic subset construction, $\epsilon$-closure
convergence, acceptance classification, structural training invariants, and
functional equivalence-achieve perfect or near-perfect empirical alignment with
ground-truth automata. This work provides the first provably complete symbolic
simulation of NFAs within standard deep learning architectures, uniting
automata theory with neural computation through ReLU dynamics.

</details>


### [578] [AMSbench: A Comprehensive Benchmark for Evaluating MLLM Capabilities in AMS Circuits](https://arxiv.org/pdf/2505.24138)
*Yichen Shi, Ze Zhang, Hongyang Wang, Zhuofu Tao, Zhongyi Li, Bingyu Chen, Yaxin Wang, Zhiping Yu, Ting-Jung Lin, Lei He*

Main category: cs.LG

TL;DR: AMSbench is introduced as a benchmark suite to evaluate Multi-modal Large Language Models (MLLMs) on diverse Analog/Mixed-Signal (AMS) circuit tasks, revealing their limitations in complex reasoning and design.


<details>
  <summary>Details</summary>
Motivation: Automating AMS circuit design is challenging, and current MLLM evaluations lack comprehensive benchmarks.

Method: AMSbench includes ~8000 test questions across schematic perception, analysis, and design, evaluating eight models.

Result: Current MLLMs show limitations in multi-modal reasoning and complex circuit design.

Conclusion: Advancing MLLMs' circuit-specific knowledge is crucial for automating AMS design workflows.

Abstract: Analog/Mixed-Signal (AMS) circuits play a critical role in the integrated
circuit (IC) industry. However, automating Analog/Mixed-Signal (AMS) circuit
design has remained a longstanding challenge due to its difficulty and
complexity. Recent advances in Multi-modal Large Language Models (MLLMs) offer
promising potential for supporting AMS circuit analysis and design. However,
current research typically evaluates MLLMs on isolated tasks within the domain,
lacking a comprehensive benchmark that systematically assesses model
capabilities across diverse AMS-related challenges. To address this gap, we
introduce AMSbench, a benchmark suite designed to evaluate MLLM performance
across critical tasks including circuit schematic perception, circuit analysis,
and circuit design. AMSbench comprises approximately 8000 test questions
spanning multiple difficulty levels and assesses eight prominent models,
encompassing both open-source and proprietary solutions such as Qwen 2.5-VL and
Gemini 2.5 Pro. Our evaluation highlights significant limitations in current
MLLMs, particularly in complex multi-modal reasoning and sophisticated circuit
design tasks. These results underscore the necessity of advancing MLLMs'
understanding and effective application of circuit-specific knowledge, thereby
narrowing the existing performance gap relative to human expertise and moving
toward fully automated AMS circuit design workflows. Our data is released at
https://huggingface.co/datasets/wwhhyy/AMSBench

</details>


### [579] [Autoregressive regularized score-based diffusion models for multi-scenarios fluid flow prediction](https://arxiv.org/pdf/2505.24145)
*Wilfried Genuist, Éric Savin, Filippo Gatti, Didier Clouteau*

Main category: cs.LG

TL;DR: A conditional score-based diffusion model for multi-scenarios fluid flow prediction, integrating energy constraints for improved quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance fluid flow prediction by leveraging scientific machine learning and generative modeling, addressing challenges like turbulence and multi-scenario adaptability.

Method: Uses a conditional score-based diffusion model with energy constraints, simple architecture, and plug-and-play enhancements. Explores stochastic differential equations for performance.

Result: Achieves stable, robust, and physically faithful predictions across diverse flow regimes, even in turbulent conditions.

Conclusion: The model is effective for multi-scenario fluid dynamics tasks, offering flexibility, efficiency, and physical accuracy.

Abstract: Building on recent advances in scientific machine learning and generative
modeling for computational fluid dynamics, we propose a conditional score-based
diffusion model designed for multi-scenarios fluid flow prediction. Our model
integrates an energy constraint rooted in the statistical properties of
turbulent flows, improving prediction quality with minimal training, while
enabling efficient sampling at low cost. The method features a simple and
general architecture that requires no problem-specific design, supports
plug-and-play enhancements, and enables fast and flexible solution generation.
It also demonstrates an efficient conditioning mechanism that simplifies
training across different scenarios without demanding a redesign of existing
models. We further explore various stochastic differential equation
formulations to demonstrate how thoughtful design choices enhance performance.
We validate the proposed methodology through extensive experiments on complex
fluid dynamics datasets encompassing a variety of flow regimes and
configurations. Results demonstrate that our model consistently achieves
stable, robust, and physically faithful predictions, even under challenging
turbulent conditions. With properly tuned parameters, it achieves accurate
results across multiple scenarios while preserving key physical and statistical
properties. We present a comprehensive analysis of stochastic differential
equation impact and discuss our approach across diverse fluid mechanics tasks.

</details>


### [580] [RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget](https://arxiv.org/pdf/2505.24149)
*Adam Piaseczny, Md Kamran Chowdhury Shisher, Shiqiang Wang, Christopher G. Brinton*

Main category: cs.LG

TL;DR: Proposes RCCDA, a dynamic model update policy for ML under concept drift, ensuring resource constraints and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adapting ML models to concept drift under strict resource constraints, where existing solutions lack guarantees.

Method: Uses a Lyapunov drift-plus-penalty framework to optimize training dynamics with past loss info and a tunable drift threshold.

Result: Outperforms baselines in accuracy while adhering to resource constraints across various drift schedules.

Conclusion: RCCDA is effective for real-time ML deployments, balancing performance and resource limits.

Abstract: Machine learning (ML) algorithms deployed in real-world environments are
often faced with the challenge of adapting models to concept drift, where the
task data distributions are shifting over time. The problem becomes even more
difficult when model performance must be maintained under adherence to strict
resource constraints. Existing solutions often depend on drift-detection
methods that produce high computational overhead for resource-constrained
environments, and fail to provide strict guarantees on resource usage or
theoretical performance assurances. To address these shortcomings, we propose
RCCDA: a dynamic model update policy that optimizes ML training dynamics while
ensuring strict compliance to predefined resource constraints, utilizing only
past loss information and a tunable drift threshold. In developing our policy,
we analytically characterize the evolution of model loss under concept drift
with arbitrary training update decisions. Integrating these results into a
Lyapunov drift-plus-penalty framework produces a lightweight policy based on a
measurable accumulated loss threshold that provably limits update frequency and
cost. Experimental results on three domain generalization datasets demonstrate
that our policy outperforms baseline methods in inference accuracy while
adhering to strict resource constraints under several schedules of concept
drift, making our solution uniquely suited for real-time ML deployments.

</details>


### [581] [Biological Pathway Guided Gene Selection Through Collaborative Reinforcement Learning](https://arxiv.org/pdf/2505.24155)
*Ehtesamul Azim, Dongjie Wang, Tae Hyun Hwang, Yanjie Fu, Wei Zhang*

Main category: cs.LG

TL;DR: A novel two-stage framework integrates statistical gene selection with biological pathway knowledge using multi-agent reinforcement learning (MARL) to improve prediction accuracy and biological interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional gene selection methods ignore complex biological pathways, leading to unstable and irrelevant signatures. The goal is to integrate pathway knowledge while maintaining statistical rigor.

Method: A pathway-guided pre-filtering strategy reduces dimensionality, followed by MARL modeling of genes as collaborative agents optimizing predictive power and biological relevance. Graph Neural Networks and a reward mechanism incorporating pathway coverage are used.

Result: The framework outperforms traditional methods in prediction accuracy and biological interpretability across multiple gene expression datasets.

Conclusion: The proposed approach effectively combines statistical and biological knowledge for robust and interpretable gene selection.

Abstract: Gene selection in high-dimensional genomic data is essential for
understanding disease mechanisms and improving therapeutic outcomes.
Traditional feature selection methods effectively identify predictive genes but
often ignore complex biological pathways and regulatory networks, leading to
unstable and biologically irrelevant signatures. Prior approaches, such as
Lasso-based methods and statistical filtering, either focus solely on
individual gene-outcome associations or fail to capture pathway-level
interactions, presenting a key challenge: how to integrate biological pathway
knowledge while maintaining statistical rigor in gene selection? To address
this gap, we propose a novel two-stage framework that integrates statistical
selection with biological pathway knowledge using multi-agent reinforcement
learning (MARL). First, we introduce a pathway-guided pre-filtering strategy
that leverages multiple statistical methods alongside KEGG pathway information
for initial dimensionality reduction. Next, for refined selection, we model
genes as collaborative agents in a MARL framework, where each agent optimizes
both predictive power and biological relevance. Our framework incorporates
pathway knowledge through Graph Neural Network-based state representations, a
reward mechanism combining prediction performance with gene centrality and
pathway coverage, and collaborative learning strategies using shared memory and
a centralized critic component. Extensive experiments on multiple gene
expression datasets demonstrate that our approach significantly improves both
prediction accuracy and biological interpretability compared to traditional
methods.

</details>


### [582] [Don't Just Follow MLLM Plans: Robust and Efficient Planning for Open-world Agents](https://arxiv.org/pdf/2505.24157)
*Seungjoon Lee, Suhwan Kim, Minhyeon Oh, Youngsik Yoon, Jungseul Ok*

Main category: cs.LG

TL;DR: REPOA is a framework for robust and efficient planning in open-world agents, addressing challenges like inaccurate LLM knowledge and exploration inefficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for autonomous agents rely on problematic internal knowledge or unrealistic assumptions, lacking the ability to acquire planning knowledge from scratch in realistic settings.

Method: REPOA introduces adaptive dependency learning, fine-grained failure-aware operation memory, and difficulty-based exploration to enhance robustness and efficiency.

Result: Evaluations show REPOA successfully obtains challenging late-game items, outperforming prior methods.

Conclusion: REPOA effectively addresses robustness and efficiency in open-world agent planning, demonstrating superior performance in realistic settings.

Abstract: Developing autonomous agents capable of mastering complex, multi-step tasks
in unpredictable, interactive environments presents a significant challenge.
While Large Language Models (LLMs) offer promise for planning, existing
approaches often rely on problematic internal knowledge or make unrealistic
environmental assumptions. Although recent work explores learning planning
knowledge, they still retain limitations due to partial reliance on external
knowledge or impractical setups. Indeed, prior research has largely overlooked
developing agents capable of acquiring planning knowledge from scratch,
directly in realistic settings. While realizing this capability is necessary,
it presents significant challenges, primarily achieving robustness given the
substantial risk of incorporating LLMs' inaccurate knowledge. Moreover,
efficiency is crucial for practicality as learning can demand prohibitive
exploration. In response, we introduce Robust and Efficient Planning for
Open-world Agents (REPOA), a novel framework designed to tackle these issues.
REPOA features three key components: adaptive dependency learning and
fine-grained failure-aware operation memory to enhance robustness to knowledge
inaccuracies, and difficulty-based exploration to improve learning efficiency.
Our evaluation in two established open-world testbeds demonstrates REPOA's
robust and efficient planning, showcasing its capability to successfully obtain
challenging late-game items that were beyond the reach of prior approaches.

</details>


### [583] [Invariant Link Selector for Spatial-Temporal Out-of-Distribution Problem](https://arxiv.org/pdf/2505.24178)
*Katherine Tieu, Dongqi Fu, Jun Wu, Jingrui He*

Main category: cs.LG

TL;DR: The paper addresses Out-of-Distribution (OOD) problems in temporal graphs, proposing an Invariant Link Selector using the Information Bottleneck method to improve generalization.


<details>
  <summary>Details</summary>
Motivation: The challenge of OOD problems in temporal graphs, especially with non-IID data, motivates the need for robust invariant learning.

Method: An error-bounded Invariant Link Selector is introduced to distinguish invariant components, combined with task-specific loss functions for training.

Result: The method achieves generalization across testing scenarios and performs well in tasks like citation and merchandise recommendation, outperforming SOTA methods.

Conclusion: The proposed approach effectively addresses OOD challenges in temporal graphs, enhancing model generalizability for real-world applications.

Abstract: In the era of foundation models, Out-of- Distribution (OOD) problems, i.e.,
the data discrepancy between the training environments and testing
environments, hinder AI generalization. Further, relational data like graphs
disobeying the Independent and Identically Distributed (IID) condition makes
the problem more challenging, especially much harder when it is associated with
time. Motivated by this, to realize the robust invariant learning over temporal
graphs, we want to investigate what components in temporal graphs are most
invariant and representative with respect to labels. With the Information
Bottleneck (IB) method, we propose an error-bounded Invariant Link Selector
that can distinguish invariant components and variant components during the
training process to make the deep learning model generalizable for different
testing scenarios. Besides deriving a series of rigorous generalizable
optimization functions, we also equip the training with task-specific loss
functions, e.g., temporal link prediction, to make pretrained models solve
real-world application tasks like citation recommendation and merchandise
recommendation, as demonstrated in our experiments with state-of-the-art (SOTA)
methods. Our code is available at https://github.com/kthrn22/OOD-Linker.

</details>


### [584] [SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling](https://arxiv.org/pdf/2505.24179)
*Xiaodong Ji, Hailin Zhang, Fangcheng Fu, Bin Cui*

Main category: cs.LG

TL;DR: SALE is a fine-grained sparse attention method for LLMs, accelerating long-context prefilling with minimal accuracy loss using 4-bit quantization and block-sparse attention.


<details>
  <summary>Details</summary>
Motivation: Self-attention in LLMs is slow for long contexts due to quadratic complexity. Existing sparse methods are coarse and lose accuracy.

Method: SALE uses 4-bit quantized query-key products and block-sparse attention with a Relative Attention Score metric, optimized via a custom CUDA kernel.

Result: Achieves 3.36x speedup on Llama-3.1-8B for sequences >64K with negligible accuracy loss.

Conclusion: SALE offers efficient, accurate long-context processing without training, easily integrable into existing systems.

Abstract: Many advanced Large Language Model (LLM) applications require long-context
processing, but the self-attention module becomes a bottleneck during the
prefilling stage of inference due to its quadratic time complexity with respect
to sequence length. Existing sparse attention methods accelerate attention
computation by skipping less significant regions of the attention map. However,
these approaches typically perform coarse-grained inspection of the attention
map, rendering considerable loss in model accuracy. In this paper, we propose
SALE, a fine-grained sparse attention method that accelerates the long-context
prefilling stage of LLM with negligible loss in model accuracy. SALE achieves
fast and accurate fine-grained attention weight estimation through 4-bit
quantized query-key products, followed by block-sparse attention to accelerate
prefilling computations. For importance evaluation for query-key pairs, we
adopt our Relative Attention Score metric, which offers significantly higher
efficiency within our framework. We implement a custom CUDA kernel optimized
for our approach for hardware efficiency, reducing the additional overhead to
approximately 11% of the full attention latency. Notably, SALE requires no
parameter training and can be seamlessly integrated into existing systems with
trivial code modifications. Experiments on long-context benchmarks demonstrate
that our method outperforms existing approaches in accuracy-efficiency
trade-offs, achieving at least 3.36x speedups on Llama-3.1-8B for sequences
longer than 64K while maintaining model quality.

</details>


### [585] [CodeV-R1: Reasoning-Enhanced Verilog Generation](https://arxiv.org/pdf/2505.24183)
*Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen*

Main category: cs.LG

TL;DR: CodeV-R1 is an RLVR framework for training Verilog generation LLMs, addressing challenges like verification, data scarcity, and computation cost. It achieves state-of-the-art results on VerilogEval and RTLLM benchmarks.


<details>
  <summary>Details</summary>
Motivation: Extend RLVR to EDA by generating Verilog from NL specs, overcoming challenges like lack of verification environments, data scarcity, and high computation costs.

Method: Develops a testbench generator, round-trip data synthesis, and a two-stage training pipeline (distillation + adaptive DAPO).

Result: CodeV-R1-7B achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, outperforming prior SOTA by 12~20%.

Conclusion: CodeV-R1 advances Verilog generation, offering a scalable solution for EDA and LLM research, with plans to release the model and dataset.

Abstract: Large language models (LLMs) trained via reinforcement learning with
verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,
automatable verification, such as software programming and mathematical
problems. Extending RLVR to electronic design automation (EDA), especially
automatically generating hardware description languages (HDLs) like Verilog
from natural-language (NL) specifications, however, poses three key challenges:
the lack of automated and accurate verification environments, the scarcity of
high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To
this end, we introduce CodeV-R1, an RLVR framework for training Verilog
generation LLMs. First, we develop a rule-based testbench generator that
performs robust equivalence checking against golden references. Second, we
propose a round-trip data synthesis method that pairs open-source Verilog
snippets with LLM-generated NL descriptions, verifies code-NL-code consistency
via the generated testbench, and filters out inequivalent examples to yield a
high-quality dataset. Third, we employ a two-stage "distill-then-RL" training
pipeline: distillation for the cold start of reasoning abilities, followed by
adaptive DAPO, our novel RLVR algorithm that can reduce training cost by
adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves
68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,
surpassing prior state-of-the-art by 12~20%, while matching or even exceeding
the performance of 671B DeepSeek-R1. We will release our model, training
pipeline, and dataset to facilitate research in EDA and LLM communities.

</details>


### [586] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/pdf/2505.24185)
*Yipan Wei, Yuchen Zou, Yapeng Li, Bo Du*

Main category: cs.LG

TL;DR: FedDEA is a federated learning method for multi-task collaboration, enabling unified model aggregation without data sharing or task labels.


<details>
  <summary>Details</summary>
Motivation: Existing FMTL methods struggle with heterogeneous tasks due to varied objectives, label spaces, and optimization paths.

Method: FedDEA dynamically identifies task-relevant dimensions and rescales updates to suppress cross-task interference.

Result: FedDEA improves performance on NYUD-V2 and PASCAL-Context datasets and integrates easily with existing federated algorithms.

Conclusion: FedDEA is robust, generalizable, and deployment-friendly for heterogeneous multi-task federated learning.

Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [587] [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/pdf/2505.24189)
*Orlando Marquez Ayala, Patrice Bechard, Emily Chen, Maggie Baird, Jingfei Chen*

Main category: cs.LG

TL;DR: Fine-tuned Small Language Models (SLMs) outperform Large Language Models (LLMs) in domain-specific tasks requiring structured outputs, showing a 10% quality improvement.


<details>
  <summary>Details</summary>
Motivation: To assess whether fine-tuning SLMs remains advantageous over prompting LLMs for structured outputs, given reduced token costs.

Method: Compare fine-tuning an SLM against prompting LLMs on generating low-code workflows in JSON.

Result: Fine-tuning improves quality by 10% on average over prompting.

Conclusion: SLMs retain a quality edge for domain-specific structured tasks despite LLMs' versatility.

Abstract: Large Language Models (LLMs) such as GPT-4o can handle a wide range of
complex tasks with the right prompt. As per token costs are reduced, the
advantages of fine-tuning Small Language Models (SLMs) for real-world
applications -- faster inference, lower costs -- may no longer be clear. In
this work, we present evidence that, for domain-specific tasks that require
structured outputs, SLMs still have a quality advantage. We compare fine-tuning
an SLM against prompting LLMs on the task of generating low-code workflows in
JSON form. We observe that while a good prompt can yield reasonable results,
fine-tuning improves quality by 10% on average. We also perform systematic
error analysis to reveal model limitations.

</details>


### [588] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/pdf/2505.24190)
*Lan-Cuong Nguyen, Quan Nguyen-Tri, Bang Tran Khanh, Dung D. Le, Long Tran-Thanh, Khoat Than*

Main category: cs.LG

TL;DR: A theoretical framework addresses the gap between real and synthetic data in few-shot image classification, proposing a novel algorithm that outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled training examples in few-shot image classification and the performance degradation caused by the gap between real and synthetic data distributions.

Method: Develops a theoretical framework to quantify distribution discrepancies and proposes a prototype-based algorithm for optimizing data partitioning and model training.

Result: Superior performance compared to state-of-the-art methods across multiple datasets.

Conclusion: The framework and algorithm effectively bridge the gap between real and synthetic data, improving few-shot classification performance.

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [589] [Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback](https://arxiv.org/pdf/2505.24193)
*Ofir Schlisselberg, Tal Lancewicki, Peter Auer, Yishay Mansour*

Main category: cs.LG

TL;DR: A new algorithm for the multi-armed bandit problem with adversarial delays achieves near-optimal performance in both stochastic and adversarial settings, matching known lower bounds.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in existing algorithms' performance compared to lower bounds, especially in stochastic settings, and to handle adversarial delays effectively.

Method: Develops a new algorithm that optimizes regret in both stochastic and adversarial environments, considering cumulative delays and sub-optimality gaps.

Result: Achieves optimal regret bounds in adversarial settings ($\widetilde{O}(\sqrt{KT} + \sqrt{D})$) and matches lower bounds in stochastic settings ($\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum \Delta_i \sigma_{max}$).

Conclusion: This is the first Best-of-Both-Worlds algorithm to simultaneously match lower bounds in both regimes under adversarial delays, with improved stochastic regret bounds.

Abstract: We study the multi-armed bandit problem with adversarially chosen delays in
the Best-of-Both-Worlds (BoBW) framework, which aims to achieve near-optimal
performance in both stochastic and adversarial environments. While prior work
has made progress toward this goal, existing algorithms suffer from significant
gaps to the known lower bounds, especially in the stochastic settings. Our main
contribution is a new algorithm that, up to logarithmic factors, matches the
known lower bounds in each setting individually.
  In the adversarial case, our algorithm achieves regret of
$\widetilde{O}(\sqrt{KT} + \sqrt{D})$, which is optimal up to logarithmic
terms, where $T$ is the number of rounds, $K$ is the number of arms, and $D$ is
the cumulative delay. In the stochastic case, we provide a regret bound which
scale as $\sum_{i:\Delta_i>0}\left(\log T/\Delta_i\right) + \frac{1}{K}\sum
\Delta_i \sigma_{max}$, where $\Delta_i$ is the sub-optimality gap of arm $i$
and $\sigma_{\max}$ is the maximum number of missing observations.
  To the best of our knowledge, this is the first BoBW algorithm to
simultaneously match the lower bounds in both stochastic and adversarial
regimes in delayed environment. Moreover, even beyond the BoBW setting, our
stochastic regret bound is the first to match the known lower bound under
adversarial delays, improving the second term over the best known result by a
factor of $K$.

</details>


### [590] [On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks](https://arxiv.org/pdf/2505.24205)
*Mingze Wang, Weinan E*

Main category: cs.LG

TL;DR: The paper analyzes the expressive power of Mixture-of-Experts (MoE) networks, showing their efficiency in modeling tasks with low-dimensionality and sparsity, and provides insights into architectural components.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical foundations of MoEs' ability to model complex tasks, given their empirical success but lack of theoretical clarity.

Method: Systematic study of shallow and deep MoEs, proving their approximation capabilities for low-dimensional manifolds and piecewise functions with compositional sparsity.

Result: Shallow MoEs overcome the curse of dimensionality; deep MoEs with L layers and E experts can approximate E^L piecewise functions.

Conclusion: MoEs' architectural components (gating, experts, layers) are critical for their performance, and the study offers suggestions for MoE variants.

Abstract: Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in
modern deep learning. Despite their empirical success, the theoretical
foundations underlying their ability to model complex tasks remain poorly
understood. In this work, we conduct a systematic study of the expressive power
of MoEs in modeling complex tasks with two common structural priors:
low-dimensionality and sparsity. For shallow MoEs, we prove that they can
efficiently approximate functions supported on low-dimensional manifolds,
overcoming the curse of dimensionality. For deep MoEs, we show that
$\cO(L)$-layer MoEs with $E$ experts per layer can approximate piecewise
functions comprising $E^L$ pieces with compositional sparsity, i.e., they can
exhibit an exponential number of structured tasks. Our analysis reveals the
roles of critical architectural components and hyperparameters in MoEs,
including the gating mechanism, expert networks, the number of experts, and the
number of layers, and offers natural suggestions for MoE variants.

</details>


### [591] [Dynamic Malware Classification of Windows PE Files using CNNs and Greyscale Images Derived from Runtime API Call Argument Conversion](https://arxiv.org/pdf/2505.24231)
*Md Shahnawaz, Bishwajit Prasad Gond, Durga Prasad Mohapatra*

Main category: cs.LG

TL;DR: A dynamic malware categorization framework using API call features and CNN achieves 98.36% accuracy in classifying malware and benign files.


<details>
  <summary>Details</summary>
Motivation: Advanced obfuscation techniques make static analysis ineffective for polymorphic and metamorphic malware, necessitating dynamic behavior-based detection.

Method: Extracts API argument calls during runtime, encodes them into temporal patterns, converts these to grayscale images, and trains a CNN for classification.

Result: Achieves 98.36% accuracy and shows resilience against evasion strategies.

Conclusion: Combining dynamic analysis with deep learning provides reliable and accurate malware classification.

Abstract: Malware detection and classification remains a topic of concern for
cybersecurity, since it is becoming common for attackers to use advanced
obfuscation on their malware to stay undetected. Conventional static analysis
is not effective against polymorphic and metamorphic malware as these change
their appearance without modifying their behavior, thus defying the analysis by
code structure alone. This makes it important to use dynamic detection that
monitors malware behavior at runtime. In this paper, we present a dynamic
malware categorization framework that extracts API argument calls at the
runtime execution of Windows Portable Executable (PE) files. Extracting and
encoding the dynamic features of API names, argument return values, and other
relative features, we convert raw behavioral data to temporal patterns. To
enhance feature portrayal, the generated patterns are subsequently converted
into grayscale pictures using a magma colormap. These improved photos are used
to teach a Convolutional Neural Network (CNN) model discriminative features,
which allows for reliable and accurate malware classification. Results from
experiments indicate that our method, with an average accuracy of 98.36% is
effective in classifying different classes of malware and benign by integrating
dynamic analysis and deep learning. It not only achieves high classification
accuracy but also demonstrates significant resilience against typical evasion
strategies.

</details>


### [592] [Model Informed Flows for Bayesian Inference of Probabilistic Programs](https://arxiv.org/pdf/2505.24243)
*Joohwan Ko, Justin Domke*

Main category: cs.LG

TL;DR: The paper introduces Model-Informed Flow (MIF), a method combining VIP and Gaussian flows for better variational inference in hierarchical models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in variational inference for complex hierarchical Bayesian models by unifying VIP and flow-based methods.

Method: Proves VIP + Gaussian flow equivalence to an autoregressive flow with prior input, then introduces MIF with translation and hierarchical ordering.

Result: MIF improves posterior approximations and matches/exceeds state-of-the-art performance on benchmarks.

Conclusion: MIF effectively combines VIP and flow-based methods, enhancing variational inference for hierarchical models.

Abstract: Variational inference often struggles with the posterior geometry exhibited
by complex hierarchical Bayesian models. Recent advances in flow-based
variational families and Variationally Inferred Parameters (VIP) each address
aspects of this challenge, but their formal relationship is unexplored. Here,
we prove that the combination of VIP and a full-rank Gaussian can be
represented exactly as a forward autoregressive flow augmented with a
translation term and input from the model's prior. Guided by this theoretical
insight, we introduce the Model-Informed Flow (MIF) architecture, which adds
the necessary translation mechanism, prior information, and hierarchical
ordering. Empirically, MIF delivers tighter posterior approximations and
matches or exceeds state-of-the-art performance across a suite of hierarchical
and non-hierarchical benchmarks.

</details>


### [593] [Rethinking Continual Learning with Progressive Neural Collapse](https://arxiv.org/pdf/2505.24254)
*Zheng Wang, Wanhao Yu, Li Yang, Sen Lin*

Main category: cs.LG

TL;DR: Progressive Neural Collapse (ProNC) is introduced to address Catastrophic Forgetting in Continual Learning by dynamically expanding a simplex ETF target, outperforming fixed global ETF methods.


<details>
  <summary>Details</summary>
Motivation: Catastrophic Forgetting in Continual Learning and the limitations of fixed global ETF approaches motivate the need for a dynamic, progressive solution.

Method: ProNC progressively expands an ETF target by adding new class prototypes, ensuring maximal separability and minimal shifts, and integrates distillation for balance.

Result: ProNC significantly outperforms baselines, offering superior flexibility, simplicity, and efficiency.

Conclusion: ProNC effectively mitigates knowledge interference in Continual Learning by dynamically adapting the ETF target, demonstrating strong performance and practicality.

Abstract: Continual Learning (CL) seeks to build an agent that can continuously learn a
sequence of tasks, where a key challenge, namely Catastrophic Forgetting,
persists due to the potential knowledge interference among different tasks. On
the other hand, deep neural networks (DNNs) are shown to converge to a terminal
state termed Neural Collapse during training, where all class prototypes
geometrically form a static simplex equiangular tight frame (ETF). These
maximally and equally separated class prototypes make the ETF an ideal target
for model learning in CL to mitigate knowledge interference. Thus inspired,
several studies have emerged very recently to leverage a fixed global ETF in
CL, which however suffers from key drawbacks, such as impracticability and
limited performance.To address these challenges and fully unlock the potential
of ETF in CL, we propose Progressive Neural Collapse (ProNC), a novel framework
that completely removes the need of a fixed global ETF in CL. Specifically,
ProNC progressively expands the ETF target in a principled way by adding new
class prototypes as vertices for new tasks, ensuring maximal separability
across all encountered classes with minimal shifts from the previous ETF. We
next develop a new CL framework by plugging ProNC into commonly used CL
algorithm designs, where distillation is further leveraged to balance between
target shifting for old classes and target aligning for new classes. Extensive
experiments show that our approach significantly outperforms related baselines
while maintaining superior flexibility, simplicity, and efficiency.

</details>


### [594] [Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining](https://arxiv.org/pdf/2505.24261)
*Weiyi Wang, Junwei Deng, Yuzheng Hu, Shiyuan Zhang, Xirui Jiang, Runting Zhang, Han Zhao, Jiaqi W. Ma*

Main category: cs.LG

TL;DR: The paper examines hyperparameter sensitivity in data attribution methods, revealing their impact and proposing a lightweight tuning solution.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored impact of hyperparameter tuning in data attribution methods, which is critical for their practical application.

Method: Conducts a large-scale empirical study on hyperparameter sensitivity and provides a theoretical analysis of regularization in influence function methods.

Result: Most data attribution methods are sensitive to key hyperparameters, and a lightweight tuning procedure is proposed and validated.

Conclusion: Highlights the overlooked challenge of hyperparameter tuning in data attribution and calls for better theoretical understanding and efficient strategies.

Abstract: Data attribution methods, which quantify the influence of individual training
data points on a machine learning model, have gained increasing popularity in
data-centric applications in modern AI. Despite a recent surge of new methods
developed in this space, the impact of hyperparameter tuning in these methods
remains under-explored. In this work, we present the first large-scale
empirical study to understand the hyperparameter sensitivity of common data
attribution methods. Our results show that most methods are indeed sensitive to
certain key hyperparameters. However, unlike typical machine learning
algorithms -- whose hyperparameters can be tuned using computationally-cheap
validation metrics -- evaluating data attribution performance often requires
retraining models on subsets of training data, making such metrics
prohibitively costly for hyperparameter tuning. This poses a critical open
challenge for the practical application of data attribution methods. To address
this challenge, we advocate for better theoretical understandings of
hyperparameter behavior to inform efficient tuning strategies. As a case study,
we provide a theoretical analysis of the regularization term that is critical
in many variants of influence function methods. Building on this analysis, we
propose a lightweight procedure for selecting the regularization value without
model retraining, and validate its effectiveness across a range of standard
data attribution benchmarks. Overall, our study identifies a fundamental yet
overlooked challenge in the practical application of data attribution, and
highlights the importance of careful discussion on hyperparameter selection in
future method development.

</details>


### [595] [On Fairness of Task Arithmetic: The Role of Task Vectors](https://arxiv.org/pdf/2505.24262)
*Hiroki Naganuma, Kotaro Yoshida, Laura Gomezjurado Gonzalez, Takafumi Horie, Yuji Naraki, Ryotaro Shimizu*

Main category: cs.LG

TL;DR: The paper explores the fairness implications of task arithmetic in model editing, comparing it to full fine-tuning and LoRA, and investigates controlling fairness by adjusting task vectors.


<details>
  <summary>Details</summary>
Motivation: Task arithmetic methods for model editing may unintentionally impact fairness, especially in sensitive applications like hate speech detection, a gap not yet addressed in literature.

Method: Systematically examines fairness metrics (Demographic Parity, Equalized Odds) by benchmarking task arithmetic against full fine-tuning and LoRA, and explores merging task vectors from demographic subgroups.

Result: Provides insights into how task arithmetic affects fairness and suggests potential for controlling fairness by adjusting task vector coefficients.

Conclusion: Establishes a foundation for fairness-aware model editing practices, highlighting the need for responsible approaches in sensitive applications.

Abstract: Model editing techniques, particularly task arithmetic using task vectors,
have shown promise in efficiently modifying pre-trained models through
arithmetic operations like task addition and negation. Despite computational
advantages, these methods may inadvertently affect model fairness, creating
risks in sensitive applications like hate speech detection. However, the
fairness implications of task arithmetic remain largely unexplored, presenting
a critical gap in the existing literature. We systematically examine how
manipulating task vectors affects fairness metrics, including Demographic
Parity and Equalized Odds. To rigorously assess these effects, we benchmark
task arithmetic against full fine-tuning, a costly but widely used baseline,
and Low-Rank Adaptation (LoRA), a prevalent parameter-efficient fine-tuning
method. Additionally, we explore merging task vectors from models fine-tuned on
demographic subgroups vulnerable to hate speech, investigating whether fairness
outcomes can be controlled by adjusting task vector coefficients, potentially
enabling tailored model behavior. Our results offer novel insights into the
fairness implications of model editing and establish a foundation for
fairness-aware and responsible model editing practices.

</details>


### [596] [GradPower: Powering Gradients for Faster Language Model Pre-Training](https://arxiv.org/pdf/2505.24275)
*Mingze Wang, Jinbo Wang, Jiaqi Zhang, Wei Wang, Peng Pei, Xunliang Cai, Weinan E, Lei Wu*

Main category: cs.LG

TL;DR: GradPower is a lightweight gradient-transformation technique for accelerating language model pre-training, requiring minimal code changes and no optimizer modifications. It consistently improves performance across various architectures, scales, datasets, and schedules.


<details>
  <summary>Details</summary>
Motivation: To accelerate language model pre-training with a simple, effective gradient-transformation technique that avoids complex optimizer modifications.

Method: Applies an elementwise sign-power transformation to gradients before feeding them into a base optimizer (e.g., AdamPower for Adam).

Result: Consistently achieves lower terminal loss across diverse settings, with notable gains for mixture-of-experts models and warmup-stable-decay schedules.

Conclusion: GradPower is a versatile and effective method for improving pre-training efficiency, supported by theoretical insights into gradient noise.

Abstract: We propose GradPower, a lightweight gradient-transformation technique for
accelerating language model pre-training. Given a gradient vector $g=(g_i)_i$,
GradPower first applies the elementwise sign-power transformation:
$\varphi_p(g)=({\rm sign}(g_i)|g_i|^p)_{i}$ for a fixed $p>0$, and then feeds
the transformed gradient into a base optimizer. Notably, GradPower requires
only a single-line code change and no modifications to the base optimizer's
internal logic, including the hyperparameters. When applied to Adam (termed
AdamPower), GradPower consistently achieves lower terminal loss across diverse
architectures (LLaMA, Qwen2MoE), parameter scales (66M to 2B), datasets (C4,
OpenWebText), and learning-rate schedules (cosine, warmup-stable-decay). The
most pronounced gains are observed when training modern mixture-of-experts
models with warmup-stable-decay schedules. GradPower also integrates seamlessly
with other state-of-the-art optimizers, such as Muon, yielding further
improvements. Finally, we provide theoretical analyses that reveal the
underlying mechanism of GradPower and highlights the influence of gradient
noise.

</details>


### [597] [Large Language Models are Locally Linear Mappings](https://arxiv.org/pdf/2505.24293)
*James R. Golden*

Main category: cs.LG

TL;DR: The paper shows that large language models (LLMs) can be interpreted as nearly-exact linear systems for input sequences without changing weights or outputs, revealing low-dimensional subspaces and interpretable semantic structures.


<details>
  <summary>Details</summary>
Motivation: To understand the internal representations and operations of LLMs by mapping their inference to linear systems, providing insights into their behavior.

Method: Extends techniques from image diffusion models to alter gradient computation, making the Jacobian of LLMs nearly reproduce forward predictions as linear systems.

Result: Demonstrated across multiple models, showing LLMs operate in low-dimensional subspaces with interpretable semantic concepts in their Jacobian.

Conclusion: Despite their nonlinearity, LLMs can be analyzed through locally linear decompositions, revealing internal semantic structures in next-token prediction.

Abstract: We demonstrate that the inference operations of several open-weight large
language models (LLMs) can be mapped to an exactly equivalent linear system for
an input sequence without modifying the model weights or altering output
predictions. Extending techniques from image diffusion models that exhibit
local or piecewise linearity, we strategically alter the gradient computation
with respect to a given input sequence for a next-token prediction such that
the Jacobian of the model nearly exactly reproduces the forward prediction with
a linear system. We demonstrate this approach across models (Llama 3, Gemma 3,
Qwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show
through the singular value decomposition of the detached Jacobian that these
LLMs operate in extremely low-dimensional subspaces where many of the largest
singular vectors decode to concepts related to the most-likely output token.
This approach also allows us to examine the operation of each successive layer
(and its attention and MLP components) as nearly-exact linear systems and
observe the emergence of semantic concepts. Despite their expressive power and
global nonlinearity, modern LLMs can be interpreted through nearly-exact
locally linear decompositions that provide insights into their internal
representations and reveal interpretable semantic structures in the next-token
prediction process.

</details>


### [598] [AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning](https://arxiv.org/pdf/2505.24298)
*Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, Yi Wu*

Main category: cs.LG

TL;DR: AReaL introduces a fully asynchronous RL system for LLMs, improving GPU utilization and achieving up to 2.57× training speedup over synchronous systems.


<details>
  <summary>Details</summary>
Motivation: Existing synchronous RL systems for LLMs suffer from inefficiency due to GPU underutilization caused by waiting for batch completion.

Method: AReaL decouples generation and training, allowing continuous rollouts and model updates. It includes optimizations like workload balancing and a staleness-enhanced PPO variant.

Result: AReaL achieves up to 2.57× speedup with matched or improved performance on math and code reasoning tasks.

Conclusion: AReaL offers a scalable and efficient solution for RL training in LLMs, addressing system-level inefficiencies.

Abstract: Reinforcement learning (RL) has become a trending paradigm for training large
language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs
requires massive parallelization and poses an urgent need for efficient
training systems. Most existing large-scale RL systems for LLMs are synchronous
by alternating generation and training in a batch setting, where the rollouts
in each training batch are generated by the same (or latest) model. This
stabilizes RL training but suffers from severe system-level inefficiency.
Generation must wait until the longest output in the batch is completed before
model update, resulting in GPU underutilization. We present AReaL, a
\emph{fully asynchronous} RL system that completely decouples generation from
training. Rollout workers in AReaL continuously generate new outputs without
waiting, while training workers update the model whenever a batch of data is
collected. AReaL also incorporates a collection of system-level optimizations,
leading to substantially higher GPU utilization. To stabilize RL training,
AReaL balances the workload of rollout and training workers to control data
staleness, and adopts a staleness-enhanced PPO variant to better handle
outdated training samples. Extensive experiments on math and code reasoning
benchmarks show that AReaL achieves \textbf{up to 2.57$\times$ training
speedup} compared to the best synchronous systems with the same number of GPUs
and matched or even improved final performance. The code of AReaL is available
at https://github.com/inclusionAI/AReaL/.

</details>


### [599] [On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective](https://arxiv.org/pdf/2505.24313)
*Gengze Xu, Wei Yao, Ziqiao Wang, Yong Liu*

Main category: cs.LG

TL;DR: The paper theoretically analyzes weak-to-strong generalization (W2SG), showing it arises from prediction misfit between student and teacher models, with less restrictive assumptions than prior work. It identifies conditions favoring W2SG and proposes using reverse cross-entropy loss for better performance.


<details>
  <summary>Details</summary>
Motivation: To understand why weak-to-strong generalization occurs and to generalize prior theoretical results by removing restrictive assumptions like convexity.

Method: Theoretical analysis using a generalized bias-variance decomposition of Bregman divergence, supported by empirical validation.

Result: W2SG emerges when the student approximates the posterior mean teacher, avoids overfitting, and reduces prediction entropy. Reverse cross-entropy loss improves student performance.

Conclusion: The study provides a broader theoretical foundation for W2SG and practical insights for improving student model training, such as using reverse cross-entropy loss.

Abstract: Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong
student model, trained on a dataset labeled by a weak teacher, ultimately
outperforms the teacher on the target task. Recent studies attribute this
performance gain to the prediction misfit between the student and teacher
models. In this work, we theoretically investigate the emergence of W2SG
through a generalized bias-variance decomposition of Bregman divergence.
Specifically, we show that the expected population risk gap between the student
and teacher is quantified by the expected misfit between the two models. While
this aligns with previous results, our analysis removes several restrictive
assumptions, most notably, the convexity of the student's hypothesis class,
required in earlier works. Moreover, we show that W2SG is more likely to emerge
when the student model approximates its posterior mean teacher, rather than
mimicking an individual teacher. Using a concrete example, we demonstrate that
if the student model has significantly larger capacity than the teacher, it can
indeed converge to this posterior mean. Our analysis also suggests that
avoiding overfitting to the teacher's supervision and reducing the entropy of
student's prediction further facilitate W2SG. In addition, we show that the
reverse cross-entropy loss, unlike the standard forward cross-entropy, is less
sensitive to the predictive uncertainty of the teacher. Finally, we empirically
verify our theoretical insights and demonstrate that incorporating the reverse
cross-entropy loss consistently improves student performance.

</details>


### [600] [ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline Calibration](https://arxiv.org/pdf/2505.24357)
*Xianglong Yan, Zhiteng Li, Tianao Zhang, Linghe Kong, Yulun Zhang, Xiaokang Yang*

Main category: cs.LG

TL;DR: ReCalKV is a post-training KV cache compression method for LLMs, using distinct strategies for Keys and Values to reduce memory usage without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: KV cache compression is needed for efficient long-context reasoning in LLMs, but existing methods either add computation or degrade performance.

Method: ReCalKV uses Head-wise Similarity-aware Reordering (HSR) for Keys and Offline Calibration and Matrix Fusion (OCMF) for Values.

Result: ReCalKV outperforms existing methods, achieving high compression ratios with minimal accuracy loss.

Conclusion: ReCalKV effectively compresses KV cache for LLMs, enabling efficient long-context reasoning.

Abstract: Large language models (LLMs) have achieved remarkable performance, yet their
capability on long-context reasoning is often constrained by the excessive
memory required to store the Key-Value (KV) cache. This makes KV cache
compression an essential step toward enabling efficient long-context reasoning.
Recent methods have explored reducing the hidden dimensions of the KV cache,
but many introduce additional computation through projection layers or suffer
from significant performance degradation under high compression ratios. To
address these challenges, we propose ReCalKV, a post-training KV cache
compression method that reduces the hidden dimensions of the KV cache. We
develop distinct compression strategies for Keys and Values based on their
different roles and varying importance in the attention mechanism. For Keys, we
propose Head-wise Similarity-aware Reordering (HSR), which clusters similar
heads and applies grouped SVD to the key projection matrix, reducing additional
computation while preserving accuracy. For Values, we propose Offline
Calibration and Matrix Fusion (OCMF) to preserve accuracy without extra
computational overhead. Experiments show that ReCalKV outperforms existing
low-rank compression methods, achieving high compression ratios with minimal
performance loss. Code is available at:
https://github.com/XIANGLONGYAN/ReCalKV.

</details>


### [601] [ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving](https://arxiv.org/pdf/2505.24317)
*Yongming Chen, Miner Chen, Liewen Liao, Mingyang Jiang, Xiang Zuo, Hengrui Zhang, Yuchen Xi, Songan Zhang*

Main category: cs.LG

TL;DR: The paper proposes a responsibility-oriented reward function for RL in autonomous driving, integrating traffic regulations via a Knowledge Graph and Vision-Language Models to automate rewards, improving adherence to laws and reducing liability.


<details>
  <summary>Details</summary>
Motivation: Manual reward design in RL for autonomous driving is ineffective in complex scenarios, necessitating an automated, regulation-compliant approach.

Method: Introduced a Traffic Regulation Knowledge Graph and used Vision-Language Models with Retrieval-Augmented Generation to automate reward assignment.

Result: Improved accuracy in assigning accident responsibilities and reduced agent liability in traffic incidents.

Conclusion: The responsibility-oriented reward function enhances RL performance in autonomous driving by ensuring strict compliance with traffic laws.

Abstract: Reinforcement learning (RL) in autonomous driving employs a trial-and-error
mechanism, enhancing robustness in unpredictable environments. However,
crafting effective reward functions remains challenging, as conventional
approaches rely heavily on manual design and demonstrate limited efficacy in
complex scenarios. To address this issue, this study introduces a
responsibility-oriented reward function that explicitly incorporates traffic
regulations into the RL framework. Specifically, we introduced a Traffic
Regulation Knowledge Graph and leveraged Vision-Language Models alongside
Retrieval-Augmented Generation techniques to automate reward assignment. This
integration guides agents to adhere strictly to traffic laws, thus minimizing
rule violations and optimizing decision-making performance in diverse driving
conditions. Experimental validations demonstrate that the proposed methodology
significantly improves the accuracy of assigning accident responsibilities and
effectively reduces the agent's liability in traffic incidents.

</details>


### [602] [Adversarial Preference Learning for Robust LLM Alignment](https://arxiv.org/pdf/2505.24369)
*Yuanfu Wang, Pengyu Wang, Chenyang Xi, Bo Tang, Junyi Zhu, Wenqiang Wei, Chen Chen, Chao Yang, Jingfeng Zhang, Chaochao Lu, Yijun Niu, Keming Mao, Zhiyu Li, Feiyu Xiong, Jie Hu, Mingchuan Yang*

Main category: cs.LG

TL;DR: APL enhances language model robustness against adversarial attacks by using intrinsic metrics, generative attackers, and iterative feedback, achieving high harmlessness and utility.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies, diversity of attacks, and feedback bias in RLHF-based models.

Method: Introduces Adversarial Preference Learning (APL) with intrinsic harmfulness metrics, conditional generative attackers, and closed-loop feedback.

Result: APL improves harmlessness (83.33% win rate), reduces harmful outputs (5.88% to 0.43%), and lowers attack success by 65%, while maintaining utility.

Conclusion: APL effectively mitigates adversarial vulnerabilities without compromising model performance.

Abstract: Modern language models often rely on Reinforcement Learning from Human
Feedback (RLHF) to encourage safe behaviors. However, they remain vulnerable to
adversarial attacks due to three key limitations: (1) the inefficiency and high
cost of human annotation, (2) the vast diversity of potential adversarial
attacks, and (3) the risk of feedback bias and reward hacking. To address these
challenges, we introduce Adversarial Preference Learning (APL), an iterative
adversarial training method incorporating three key innovations. First, a
direct harmfulness metric based on the model's intrinsic preference
probabilities, eliminating reliance on external assessment. Second, a
conditional generative attacker that synthesizes input-specific adversarial
variations. Third, an iterative framework with automated closed-loop feedback,
enabling continuous adaptation through vulnerability discovery and mitigation.
Experiments on Mistral-7B-Instruct-v0.3 demonstrate that APL significantly
enhances robustness, achieving 83.33% harmlessness win rate over the base model
(evaluated by GPT-4o), reducing harmful outputs from 5.88% to 0.43% (measured
by LLaMA-Guard), and lowering attack success rate by up to 65% according to
HarmBench. Notably, APL maintains competitive utility, with an MT-Bench score
of 6.59 (comparable to the baseline 6.78) and an LC-WinRate of 46.52% against
the base model.

</details>


### [603] [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/pdf/2505.24324)
*Ivan Petrukha, Yana Kurliak, Nataliia Stulova*

Main category: cs.LG

TL;DR: SwiftEval is a new Swift-focused benchmark addressing gaps in existing multilingual benchmarks for evaluating LLMs in Swift code generation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like HumanEval-XL and MultiPL-E are insufficient for Swift due to issues in their Swift components, highlighting the need for a dedicated Swift benchmark.

Method: SwiftEval introduces 28 hand-crafted Swift problems, evaluating 44 Code LLMs with a focus on language-specific features.

Result: LLMs, especially smaller ones, perform poorly on Swift-specific problems, revealing limitations in current benchmarks.

Conclusion: SwiftEval provides a reliable benchmark for Swift, emphasizing the importance of language-specific evaluation in LLM code generation.

Abstract: In recent years, large language models (LLMs) have showcased significant
advancements in code generation. However, most evaluation benchmarks are
primarily oriented towards Python, making it difficult to evaluate other
programming languages, such as Swift, with high quality. By examining widely
established multilingual benchmarks like HumanEval-XL and MultiPL-E, we
identified critical issues specific to their Swift components, making them
insufficient or even irrelevant for assessing LLM coding capabilities on Swift.
Unlike these existing approaches, which prioritize rapid scaling and
generalization by automatically translating Python-centric benchmarks with
LLMs, we adopt a quality-over-quantity methodology. We present SwiftEval, the
first Swift-oriented benchmark consisting of 28 carefully hand-crafted
problems, and evaluate 44 popular Code LLMs on it. Our results show significant
LLM scores drop for problems requiring language-specific features, most
noticeable in the models of smaller sizes.

</details>


### [604] [Mastering Massive Multi-Task Reinforcement Learning via Mixture-of-Expert Decision Transformer](https://arxiv.org/pdf/2505.24378)
*Yilun Kong, Guozheng Ma, Qi Zhao, Haoyu Wang, Li Shen, Xueqian Wang, Dacheng Tao*

Main category: cs.LG

TL;DR: M3DT, a MoE-based framework, addresses scalability in offline MTRL by enhancing parameter scalability and introducing a three-stage training mechanism, achieving superior performance on 160 tasks.


<details>
  <summary>Details</summary>
Motivation: Current MTRL methods struggle with scaling to massive tasks, and naive parameter expansion fails to counteract performance degradation.

Method: Proposes M3DT, combining MoE with Decision Transformer to reduce task load and a three-stage training mechanism for efficient optimization.

Result: M3DT improves performance with model expansion and scales successfully to 160 tasks.

Conclusion: M3DT effectively tackles task scalability in offline MTRL, demonstrating robustness and superior performance.

Abstract: Despite recent advancements in offline multi-task reinforcement learning
(MTRL) have harnessed the powerful capabilities of the Transformer
architecture, most approaches focus on a limited number of tasks, with scaling
to extremely massive tasks remaining a formidable challenge. In this paper, we
first revisit the key impact of task numbers on current MTRL method, and
further reveal that naively expanding the parameters proves insufficient to
counteract the performance degradation as the number of tasks escalates.
Building upon these insights, we propose M3DT, a novel mixture-of-experts (MoE)
framework that tackles task scalability by further unlocking the model's
parameter scalability. Specifically, we enhance both the architecture and the
optimization of the agent, where we strengthen the Decision Transformer (DT)
backbone with MoE to reduce task load on parameter subsets, and introduce a
three-stage training mechanism to facilitate efficient training with optimal
performance. Experimental results show that, by increasing the number of
experts, M3DT not only consistently enhances its performance as model expansion
on the fixed task numbers, but also exhibits remarkable task scalability,
successfully extending to 160 tasks with superior performance.

</details>


### [605] [Cartan Networks: Group theoretical Hyperbolic Deep Learning](https://arxiv.org/pdf/2505.24353)
*Federico Milanesio, Matteo Santoro, Pietro G. Fré, Guido Sanguinetti*

Main category: cs.LG

TL;DR: The paper introduces Cartan networks, a new class of hyperbolic deep learning algorithms combining group homomorphisms and metric-preserving diffeomorphisms, showing promising results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To leverage the solvable group structure of hyperbolic spaces for more efficient and informative embeddings of hierarchical data.

Method: Proposes Cartan networks, interleaving group homomorphisms with metric-preserving diffeomorphisms.

Result: Demonstrates promising performance on various benchmark datasets.

Conclusion: Opens the way for a novel class of hyperbolic deep learning architectures.

Abstract: Hyperbolic deep learning leverages the metric properties of hyperbolic spaces
to develop efficient and informative embeddings of hierarchical data. Here, we
focus on the solvable group structure of hyperbolic spaces, which follows
naturally from their construction as symmetric spaces. This dual nature of Lie
group and Riemannian manifold allows us to propose a new class of hyperbolic
deep learning algorithms where group homomorphisms are interleaved with
metric-preserving diffeomorphisms. The resulting algorithms, which we call
Cartan networks, show promising results on various benchmark data sets and open
the way to a novel class of hyperbolic deep learning architectures.

</details>


### [606] [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/pdf/2505.24379)
*Xiaoyu Wu, Yifei Pang, Terrance Liu, Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: The paper challenges the robustness of exact unlearning in large language models by introducing a novel data extraction attack, showing that unlearning may paradoxically increase privacy risks.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in large language models by evaluating the effectiveness of exact unlearning, which is believed to be secure against data extraction attacks.

Method: Proposes a data extraction attack that leverages pre- and post-unlearning models, using model guidance and token filtering to uncover removed data patterns.

Result: The attack significantly improves extraction success rates (doubling in some cases) across benchmarks and demonstrates real-world risks on a medical dataset.

Conclusion: Exact unlearning may not be as secure as assumed; broader threat models, including adversarial access to prior checkpoints, should be considered in evaluations.

Abstract: Large language models are typically trained on datasets collected from the
web, which may inadvertently contain harmful or sensitive personal information.
To address growing privacy concerns, unlearning methods have been proposed to
remove the influence of specific data from trained models. Of these, exact
unlearning -- which retrains the model from scratch without the target data --
is widely regarded the gold standard, believed to be robust against
privacy-related attacks. In this paper, we challenge this assumption by
introducing a novel data extraction attack that compromises even exact
unlearning. Our method leverages both the pre- and post-unlearning models: by
guiding the post-unlearning model using signals from the pre-unlearning model,
we uncover patterns that reflect the removed data distribution. Combining model
guidance with a token filtering strategy, our attack significantly improves
extraction success rates -- doubling performance in some cases -- across common
benchmarks such as MUSE, TOFU, and WMDP. Furthermore, we demonstrate our
attack's effectiveness on a simulated medical diagnosis dataset to highlight
real-world privacy risks associated with exact unlearning. In light of our
findings, which suggest that unlearning may, in a contradictory way, increase
the risk of privacy leakage, we advocate for evaluation of unlearning methods
to consider broader threat models that account not only for post-unlearning
models but also for adversarial access to prior checkpoints.

</details>


### [607] [Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning](https://arxiv.org/pdf/2505.24360)
*Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy*

Main category: cs.LG

TL;DR: Sparse autoencoders (SAEs) and ITDA improve interpretability and control in large text-to-image diffusion models, with SAEs outperforming MLP neurons.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability and control of language model activations in text-to-image diffusion models.

Method: Apply SAEs and ITDA to Flux 1, a large text-to-image diffusion model, using a visual automated interpretation pipeline.

Result: SAEs accurately reconstruct embeddings and outperform MLP neurons in interpretability; ITDA matches SAEs in interpretability.

Conclusion: SAEs and ITDA are effective for decomposing and interpreting activations, enabling better control in image generation.

Abstract: Sparse autoencoders are a promising new approach for decomposing language
model activations for interpretation and control. They have been applied
successfully to vision transformer image encoders and to small-scale diffusion
models. Inference-Time Decomposition of Activations (ITDA) is a recently
proposed variant of dictionary learning that takes the dictionary to be a set
of data points from the activation distribution and reconstructs them with
gradient pursuit. We apply Sparse Autoencoders (SAEs) and ITDA to a large
text-to-image diffusion model, Flux 1, and consider the interpretability of
embeddings of both by introducing a visual automated interpretation pipeline.
We find that SAEs accurately reconstruct residual stream embeddings and beat
MLP neurons on interpretability. We are able to use SAE features to steer image
generation through activation addition. We find that ITDA has comparable
interpretability to SAEs.

</details>


### [608] [Anomaly Detection and Improvement of Clusters using Enhanced K-Means Algorithm](https://arxiv.org/pdf/2505.24365)
*Vardhan Shorewala, Shivam Shorewala*

Main category: cs.LG

TL;DR: A unified approach for cluster refinement and anomaly detection is proposed, outperforming k-means with tighter clusters and better anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To improve clustering accuracy and integrate anomaly detection by minimizing intra-cluster variance.

Method: A novel iterative algorithm reduces intra-cluster variance, evaluated using intrinsic measures (e.g., silhouette coefficient) and extended for anomaly detection.

Result: Achieved variance reductions of 18.7% (synthetic) and 88.1% (Wine Quality), with accuracy/F1 improvements of 22.5%/20.8%.

Conclusion: The method effectively refines clusters and detects anomalies, demonstrating superior performance on benchmark datasets.

Abstract: This paper introduces a unified approach to cluster refinement and anomaly
detection in datasets. We propose a novel algorithm that iteratively reduces
the intra-cluster variance of N clusters until a global minimum is reached,
yielding tighter clusters than the standard k-means algorithm. We evaluate the
method using intrinsic measures for unsupervised learning, including the
silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index, and
extend it to anomaly detection by identifying points whose assignment causes a
significant variance increase. External validation on synthetic data and the
UCI Breast Cancer and UCI Wine Quality datasets employs the Jaccard similarity
score, V-measure, and F1 score. Results show variance reductions of 18.7% and
88.1% on the synthetic and Wine Quality datasets, respectively, along with
accuracy and F1 score improvements of 22.5% and 20.8% on the Wine Quality
dataset.

</details>


### [609] [Boosting Automatic Exercise Evaluation Through Musculoskeletal Simulation-Based IMU Data Augmentation](https://arxiv.org/pdf/2505.24415)
*Andreas Spilz, Heiko Oppel, Michael Munz*

Main category: cs.LG

TL;DR: A novel data augmentation method using musculoskeletal simulations improves deep learning models for movement quality assessment with IMUs, addressing data scarcity and label ambiguity.


<details>
  <summary>Details</summary>
Motivation: Automated movement evaluation can enhance physiotherapy and sports training, but deep learning models face challenges like limited data, class imbalance, and label ambiguity.

Method: The method generates realistic IMU data via musculoskeletal simulations and systematic movement trajectory modifications, ensuring biomechanical plausibility and automatic labeling.

Result: Augmented data closely resembles real-world data, boosting classification accuracy and generalization, especially in patient-specific scenarios with limited training examples.

Conclusion: The approach effectively addresses common challenges in deep learning for physiotherapeutic exercise evaluation, proving practical and impactful.

Abstract: Automated evaluation of movement quality holds significant potential for
enhancing physiotherapeutic treatments and sports training by providing
objective, real-time feedback. However, the effectiveness of deep learning
models in assessing movements captured by inertial measurement units (IMUs) is
often hampered by limited data availability, class imbalance, and label
ambiguity. In this work, we present a novel data augmentation method that
generates realistic IMU data using musculoskeletal simulations integrated with
systematic modifications of movement trajectories. Crucially, our approach
ensures biomechanical plausibility and allows for automatic, reliable labeling
by combining inverse kinematic parameters with a knowledge-based evaluation
strategy. Extensive evaluations demonstrate that augmented variants closely
resembles real-world data, significantly improving the classification accuracy
and generalization capability of neural network models. Additionally, we
highlight the benefits of augmented data for patient-specific fine-tuning
scenarios, particularly when only limited subject-specific training examples
are available. Our findings underline the practicality and efficacy of this
augmentation method in overcoming common challenges faced by deep learning
applications in physiotherapeutic exercise evaluation.

</details>


### [610] [LightSAM: Parameter-Agnostic Sharpness-Aware Minimization](https://arxiv.org/pdf/2505.24399)
*Yifei Cheng, Li Shen, Hao Sun, Nan Yin, Xiaochun Cao, Enhong Chen*

Main category: cs.LG

TL;DR: LightSAM adaptively sets SAM's perturbation radius and learning rate, reducing sensitivity to hyperparameters and extending its applicability.


<details>
  <summary>Details</summary>
Motivation: SAM's sensitivity to hyperparameters and convergence constraints limit its practical use, prompting the need for a parameter-agnostic solution.

Method: LightSAM replaces SAM's SGD optimizer with adaptive optimizers (AdaGrad-Norm, AdaGrad, Adam) for weight perturbation and model updates.

Result: Theoretical and experimental results show LightSAM converges ideally without hyperparameter tuning, validating its effectiveness.

Conclusion: LightSAM successfully addresses SAM's limitations, offering a parameter-agnostic approach with improved generalization.

Abstract: Sharpness-Aware Minimization (SAM) optimizer enhances the generalization
ability of the machine learning model by exploring the flat minima landscape
through weight perturbations. Despite its empirical success, SAM introduces an
additional hyper-parameter, the perturbation radius, which causes the
sensitivity of SAM to it. Moreover, it has been proved that the perturbation
radius and learning rate of SAM are constrained by problem-dependent parameters
to guarantee convergence. These limitations indicate the requirement of
parameter-tuning in practical applications. In this paper, we propose the
algorithm LightSAM which sets the perturbation radius and learning rate of SAM
adaptively, thus extending the application scope of SAM. LightSAM employs three
popular adaptive optimizers, including AdaGrad-Norm, AdaGrad and Adam, to
replace the SGD optimizer for weight perturbation and model updating, reducing
sensitivity to parameters. Theoretical results show that under weak
assumptions, LightSAM could converge ideally with any choices of perturbation
radius and learning rate, thus achieving parameter-agnostic. We conduct
preliminary experiments on several deep learning tasks, which together with the
theoretical findings validate the the effectiveness of LightSAM.

</details>


### [611] [On the Lipschitz Continuity of Set Aggregation Functions and Neural Networks for Sets](https://arxiv.org/pdf/2505.24403)
*Giannis Nikolentzos, Konstantinos Skianis*

Main category: cs.LG

TL;DR: The paper investigates the Lipschitz continuity of aggregation functions in neural networks processing multisets of vectors, derives their Lipschitz constants, and provides upper bounds for such networks. It also studies their robustness and generalization, supported by empirical experiments.


<details>
  <summary>Details</summary>
Motivation: Understanding the Lipschitz constant of neural networks is crucial for assessing robustness and generalization, especially for models handling unordered data like multisets of vectors.

Method: The study analyzes the Lipschitz continuity of aggregation functions (sum, mean, max) with respect to three distance functions for multisets. It derives their Lipschitz constants and extends this to bound the Lipschitz constant of entire networks.

Result: Each aggregation function is Lipschitz continuous with respect to only one of the three distance functions. Upper bounds for network Lipschitz constants are derived, and empirical experiments validate the theoretical findings.

Conclusion: The work provides theoretical and empirical insights into the Lipschitz properties of aggregation functions in multiset-processing neural networks, aiding in understanding their robustness and generalization.

Abstract: The Lipschitz constant of a neural network is connected to several important
properties of the network such as its robustness and generalization. It is thus
useful in many settings to estimate the Lipschitz constant of a model. Prior
work has focused mainly on estimating the Lipschitz constant of multi-layer
perceptrons and convolutional neural networks. Here we focus on data modeled as
sets or multisets of vectors and on neural networks that can handle such data.
These models typically apply some permutation invariant aggregation function,
such as the sum, mean or max operator, to the input multisets to produce a
single vector for each input sample. In this paper, we investigate whether
these aggregation functions are Lipschitz continuous with respect to three
distance functions for unordered multisets, and we compute their Lipschitz
constants. In the general case, we find that each aggregation function is
Lipschitz continuous with respect to only one of the three distance functions.
Then, we build on these results to derive upper bounds on the Lipschitz
constant of neural networks that can process multisets of vectors, while we
also study their stability to perturbations and generalization under
distribution shifts. To empirically verify our theoretical analysis, we conduct
a series of experiments on datasets from different domains.

</details>


### [612] [Learning Safety Constraints for Large Language Models](https://arxiv.org/pdf/2505.24445)
*Xin Chen, Yarden As, Andreas Krause*

Main category: cs.LG

TL;DR: SaP (Safety Polytope) is a geometric method for enforcing safety constraints in LLMs by identifying safe/unsafe regions in representation space, preserving model performance while reducing harmful outputs.


<details>
  <summary>Details</summary>
Motivation: Address safety risks in LLMs, such as harmful outputs and adversarial attacks, without compromising model capabilities.

Method: Uses a geometric polytope framework to identify and enforce safety constraints in the model's representation space, enabling detection and correction of unsafe outputs.

Result: Effectively detects unethical inputs, reduces adversarial attack success rates, and maintains performance on standard tasks.

Conclusion: SaP provides an interpretable, post-hoc geometric approach to LLM safety, highlighting the value of explicit safety modeling in representation space.

Abstract: Large language models (LLMs) have emerged as powerful tools but pose
significant safety risks through harmful outputs and vulnerability to
adversarial attacks. We propose SaP, short for Safety Polytope, a geometric
approach to LLM safety that learns and enforces multiple safety constraints
directly in the model's representation space. We develop a framework that
identifies safe and unsafe regions via the polytope's facets, enabling both
detection and correction of unsafe outputs through geometric steering. Unlike
existing approaches that modify model weights, SaP operates post-hoc in the
representation space, preserving model capabilities while enforcing safety
constraints. Experiments across multiple LLMs demonstrate that our method can
effectively detect unethical inputs, reduce adversarial attack success rates
while maintaining performance on standard tasks, thus highlighting the
importance of having an explicit geometric model for safety. Analysis of the
learned polytope facets reveals emergence of specialization in detecting
different semantic notions of safety, providing interpretable insights into how
safety is captured in LLMs' representation space.

</details>


### [613] [Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data](https://arxiv.org/pdf/2505.24413)
*Yang Sui, Qi Xu, Yang Bai, Annie Qu*

Main category: cs.LG

TL;DR: A two-step learning strategy for multi-task learning (MTL) addresses block-wise, distribution, and posterior heterogeneity by imputing missing blocks and disentangling shared and task-specific components.


<details>
  <summary>Details</summary>
Motivation: Existing MTL methods struggle to handle multiple forms of heterogeneity simultaneously, limiting their effectiveness in diverse applications like healthcare and marketing.

Method: The proposed method involves imputing missing blocks using shared representations and disentangling input-output mappings into shared and task-specific components.

Result: Numerical experiments and ADNI database analysis show superior performance compared to competing methods.

Conclusion: The two-step strategy effectively addresses heterogeneity in MTL, improving performance in real-world applications.

Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool
to solve multiple learning tasks simultaneously and has been successfully
applied to healthcare, marketing, and biomedical fields. However, in order to
borrow information across different tasks effectively, it is essential to
utilize both homogeneous and heterogeneous information. Among the extensive
literature on MTL, various forms of heterogeneity are presented in MTL
problems, such as block-wise, distribution, and posterior heterogeneity.
Existing methods, however, struggle to tackle these forms of heterogeneity
simultaneously in a unified framework. In this paper, we propose a two-step
learning strategy for MTL which addresses the aforementioned heterogeneity.
First, we impute the missing blocks using shared representations extracted from
homogeneous source across different tasks. Next, we disentangle the mappings
between input features and responses into a shared component and a
task-specific component, respectively, thereby enabling information borrowing
through the shared component. Our numerical experiments and real-data analysis
from the ADNI database demonstrate the superior MTL performance of the proposed
method compared to other competing methods.

</details>


### [614] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/pdf/2505.24424)
*Amit Peleg, Naman Deep Singh, Matthias Hein*

Main category: cs.LG

TL;DR: CLIC improves CLIP's compositional reasoning and retrieval performance by fine-tuning with a novel multi-image-caption training technique.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP lack strong compositional reasoning, and prior improvements focused on lexical sensitivity, neglecting semantic understanding and retrieval performance.

Method: CLIC introduces a fine-tuning method using a novel training technique combining multiple images and captions.

Result: CLIC enhances both lexical and semantic understanding, improves retrieval performance, and achieves state-of-the-art results on SugarCrepe++.

Conclusion: CLIC effectively addresses compositional reasoning in CLIP, improving retrieval and semantic understanding without compromising performance.

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [615] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/pdf/2505.24434)
*Md Shahriar Rahim Siddiqui, Moshe Eliasof, Eldad Haber*

Main category: cs.LG

TL;DR: Graph Flow Matching (GFM) enhances flow matching by incorporating neighbor information via a graph neural module, improving generation quality with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing flow matching methods predict velocities independently, ignoring correlations between points, which could improve generation quality.

Method: GFM decomposes velocity into a reaction term (standard flow matching) and a diffusion term (graph neural module for neighbor aggregation).

Result: GFM improves Fréchet Inception Distance (FID) and recall across five image generation benchmarks.

Conclusion: GFM is an effective, modular enhancement for flow matching architectures.

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [616] [Train One Sparse Autoencoder Across Multiple Sparsity Budgets to Preserve Interpretability and Accuracy](https://arxiv.org/pdf/2505.24473)
*Nikita Balagansky, Yaroslav Aksenov, Daniil Laptev, Vadim Kurochkin, Gleb Gerasimov, Nikita Koryagin, Daniil Gavrilov*

Main category: cs.LG

TL;DR: A novel training objective, HierarchicalTopK, enables a single Sparse Autoencoder (SAE) to optimize reconstructions across multiple sparsity levels, outperforming traditional SAEs and maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Conventional SAEs require separate models for different sparsity levels, increasing computational costs. HierarchicalTopK addresses this limitation.

Method: Introduces HierarchicalTopK, a training objective for SAEs to optimize reconstructions across multiple sparsity levels simultaneously.

Result: Achieves Pareto-optimal trade-offs between sparsity and explained variance, outperforming traditional SAEs while preserving interpretability.

Conclusion: HierarchicalTopK bridges the gap between flexibility and interpretability in SAE design.

Abstract: Sparse Autoencoders (SAEs) have proven to be powerful tools for interpreting
neural networks by decomposing hidden representations into disentangled,
interpretable features via sparsity constraints. However, conventional SAEs are
constrained by the fixed sparsity level chosen during training; meeting
different sparsity requirements therefore demands separate models and increases
the computational footprint during both training and evaluation. We introduce a
novel training objective, \emph{HierarchicalTopK}, which trains a single SAE to
optimise reconstructions across multiple sparsity levels simultaneously.
Experiments with Gemma-2 2B demonstrate that our approach achieves
Pareto-optimal trade-offs between sparsity and explained variance,
outperforming traditional SAEs trained at individual sparsity levels. Further
analysis shows that HierarchicalTopK preserves high interpretability scores
even at higher sparsity. The proposed objective thus closes an important gap
between flexibility and interpretability in SAE design.

</details>


### [617] [Weisfeiler and Leman Follow the Arrow of Time: Expressive Power of Message Passing in Temporal Event Graphs](https://arxiv.org/pdf/2505.24438)
*Franziska Heeg, Jonas Sauer, Petra Mutzel, Ingo Scholtes*

Main category: cs.LG

TL;DR: The paper introduces a new concept of consistent event graph isomorphism for temporal graphs to better capture causal topology, proposes a temporal Weisfeiler-Leman algorithm, and develops a novel TGNN message passing scheme, showing strong performance in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing temporal graph neural networks (TGNNs) often overlook the causal topology of temporal graphs, necessitating a formal framework to analyze their expressive power.

Method: The authors define consistent event graph isomorphism, compare it with existing notions, and develop a temporal Weisfeiler-Leman algorithm. They also introduce a new message passing scheme for TGNNs based on event graph representation.

Result: The proposed approach performs well in temporal graph classification experiments.

Conclusion: The work provides a theoretical foundation for analyzing TGNNs and demonstrates practical improvements in classification tasks.

Abstract: An important characteristic of temporal graphs is how the directed arrow of
time influences their causal topology, i.e., which nodes can possibly influence
each other causally via time-respecting paths. The resulting patterns are often
neglected by temporal graph neural networks (TGNNs). To formally analyze the
expressive power of TGNNs, we lack a generalization of graph isomorphism to
temporal graphs that fully captures their causal topology. Addressing this gap,
we introduce the notion of consistent event graph isomorphism, which utilizes a
time-unfolded representation of time-respecting paths in temporal graphs. We
compare this definition with existing notions of temporal graph isomorphisms.
We illustrate and highlight the advantages of our approach and develop a
temporal generalization of the Weisfeiler-Leman algorithm to heuristically
distinguish non-isomorphic temporal graphs. Building on this theoretical
foundation, we derive a novel message passing scheme for temporal graph neural
networks that operates on the event graph representation of temporal graphs. An
experimental evaluation shows that our approach performs well in a temporal
graph classification experiment.

</details>


### [618] [Stepsize anything: A unified learning rate schedule for budgeted-iteration training](https://arxiv.org/pdf/2505.24452)
*Anda Tang, Yiming Dong, Yutao Zeng, zhou Xun, Zhouchen Lin*

Main category: cs.LG

TL;DR: The paper introduces the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule for budgeted-iteration training, outperforming heuristic schedules across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: The need for efficient learning within constrained iteration budgets, coupled with the heuristic and inefficient nature of existing learning rate schedules, drives this work.

Method: Proposes the UBA schedule, derived from a budget-aware optimization framework, controlled by a single hyper-parameter φ, balancing flexibility and simplicity.

Result: UBA consistently outperforms common schedules across diverse tasks and architectures under varying budgets, with theoretical convergence guarantees.

Conclusion: The UBA schedule provides a robust, interpretable, and efficient solution for budgeted-iteration training, supported by theory and empirical results.

Abstract: The expanding computational costs and limited resources underscore the
critical need for budgeted-iteration training, which aims to achieve optimal
learning within predetermined iteration budgets.While learning rate schedules
fundamentally govern the performance of different networks and tasks,
particularly in budgeted-iteration scenarios, their design remains largely
heuristic, lacking theoretical foundations.In addition, the optimal learning
rate schedule requires extensive trial-and-error selection, making the training
process inefficient.In this work, we propose the Unified Budget-Aware (UBA)
schedule, a theoretically grounded learning rate schedule that consistently
outperforms commonly-used schedules among diverse architectures and tasks under
different constrained training budgets.First, we bridge the gap by constructing
a novel training budget-aware optimization framework, which explicitly accounts
for the robustness to landscape curvature variations.From this framework, we
derive the UBA schedule, controlled by a single hyper-parameter $\varphi$ that
provides a trade-off between flexibility and simplicity, eliminating the need
for per-network numerical optimization. Moreover, we establish a theoretical
connection between $\varphi$ and the condition number, adding interpretation
and justification to our approach. Besides, we prove the convergence for
different values of $\varphi$.We offer practical guidelines for its selection
via theoretical analysis and empirical results.xtensive experimental results
show that UBA \textit{consistently surpasses} the commonly-used schedules
across diverse vision and language tasks, spanning network architectures (e.g.,
ResNet, OLMo) and scales, under different training-iteration budgets.

</details>


### [619] [Logits-Based Finetuning](https://arxiv.org/pdf/2505.24461)
*Jingyao Li, Senqiao Yang, Sitong Wu, Han Shi, Chuanyang Zheng, Hong Xu, Jiaya Jia*

Main category: cs.LG

TL;DR: Reconstruction-based methods, like Masked Image Modeling (MOOD), significantly improve OOD detection performance without needing OOD samples.


<details>
  <summary>Details</summary>
Motivation: Previous recognition-based methods for OOD detection learn shortcuts instead of comprehensive representations.

Method: Uses Masked Image Modeling as a pretext task to learn intrinsic data distributions of in-distribution (ID) datasets.

Result: MOOD outperforms SOTA in one-class (5.7%), multi-class (3.0%), and near-distribution OOD detection (2.1%), even beating 10-shot-per-class outlier exposure methods.

Conclusion: Reconstruction-based pretext tasks provide a generally applicable and effective prior for OOD detection.

Abstract: The core of out-of-distribution (OOD) detection is to learn the
in-distribution (ID) representation, which is distinguishable from OOD samples.
Previous work applied recognition-based methods to learn the ID features, which
tend to learn shortcuts instead of comprehensive representations. In this work,
we find surprisingly that simply using reconstruction-based methods could boost
the performance of OOD detection significantly. We deeply explore the main
contributors of OOD detection and find that reconstruction-based pretext tasks
have the potential to provide a generally applicable and efficacious prior,
which benefits the model in learning intrinsic data distributions of the ID
dataset. Specifically, we take Masked Image Modeling as a pretext task for our
OOD detection framework (MOOD). Without bells and whistles, MOOD outperforms
previous SOTA of one-class OOD detection by 5.7%, multi-class OOD detection by
3.0%, and near-distribution OOD detection by 2.1%. It even defeats the
10-shot-per-class outlier exposure OOD detection, although we do not include
any OOD samples for our detection. Codes are available at
https://github.com/JulietLJY/MOOD.

</details>


### [620] [Object Centric Concept Bottlenecks](https://arxiv.org/pdf/2505.24492)
*David Steinmann, Wolfgang Stammer, Antonia Wüst, Kristian Kersting*

Main category: cs.LG

TL;DR: OCB combines CBMs with object-centric models to improve performance and interpretability in complex vision tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of CBMs in handling complex, object-centric tasks while maintaining interpretability.

Method: Introduces Object-Centric Concept Bottlenecks (OCB), leveraging pre-trained object-centric models and concept activations.

Result: OCB outperforms traditional CBMs and enables interpretable decisions for complex visual tasks.

Conclusion: OCB successfully bridges the gap between performance and interpretability in AI models for complex vision tasks.

Abstract: Developing high-performing, yet interpretable models remains a critical
challenge in modern AI. Concept-based models (CBMs) attempt to address this by
extracting human-understandable concepts from a global encoding (e.g., image
encoding) and then applying a linear classifier on the resulting concept
activations, enabling transparent decision-making. However, their reliance on
holistic image encodings limits their expressiveness in object-centric
real-world settings and thus hinders their ability to solve complex vision
tasks beyond single-label classification. To tackle these challenges, we
introduce Object-Centric Concept Bottlenecks (OCB), a framework that combines
the strengths of CBMs and pre-trained object-centric foundation models,
boosting performance and interpretability. We evaluate OCB on complex image
datasets and conduct a comprehensive ablation study to analyze key components
of the framework, such as strategies for aggregating object-concept encodings.
The results show that OCB outperforms traditional CBMs and allows one to make
interpretable decisions for complex visual tasks.

</details>


### [621] [Smooth Model Compression without Fine-Tuning](https://arxiv.org/pdf/2505.24469)
*Christina Runkel, Natacha Kuete Meli, Jovita Lukasik, Ander Biguri, Carola-Bibiane Schönlieb, Michael Moeller*

Main category: cs.LG

TL;DR: The paper explores how smooth regularization improves neural network pruning and compression, achieving high accuracy with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Standard pruning and compression techniques often ignore network weight structure, limiting effectiveness. Smooth regularization can enhance these methods.

Method: Apply nuclear norm and derivative penalties during training to encourage smoothness, then use SVD-based compression on the smooth models.

Result: Achieves 91% accuracy on CIFAR-10 with a smooth ResNet-18, using 70% fewer parameters.

Conclusion: Smooth regularization and structured compression enable state-of-the-art model compression without fine-tuning.

Abstract: Compressing and pruning large machine learning models has become a critical
step towards their deployment in real-world applications. Standard pruning and
compression techniques are typically designed without taking the structure of
the network's weights into account, limiting their effectiveness. We explore
the impact of smooth regularization on neural network training and model
compression. By applying nuclear norm, first- and second-order derivative
penalties of the weights during training, we encourage structured smoothness
while preserving predictive performance on par with non-smooth models. We find
that standard pruning methods often perform better when applied to these smooth
models. Building on this observation, we apply a
Singular-Value-Decomposition-based compression method that exploits the
underlying smooth structure and approximates the model's weight tensors by
smaller low-rank tensors. Our approach enables state-of-the-art compression
without any fine-tuning - reaching up to $91\%$ accuracy on a smooth ResNet-18
on CIFAR-10 with $70\%$ fewer parameters.

</details>


### [622] [Efficient Neural and Numerical Methods for High-Quality Online Speech Spectrogram Inversion via Gradient Theorem](https://arxiv.org/pdf/2505.24498)
*Andres Fernandez, Juan Azcarreta, Cagdas Bilen, Jesus Monge Alvarez*

Main category: cs.LG

TL;DR: The paper introduces three innovations to reduce computational cost in speech spectrogram inversion while maintaining high quality: a compact neural network, increased latency for cost reduction, and a linear-complexity solver for the least squares step.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of speech spectrogram inversion by reducing computational costs without sacrificing reconstruction quality.

Method: 1. A novel neural network with 8k parameters. 2. Increased latency to halve neural inference cost. 3. A linear-complexity solver for the least squares problem leveraging tridiagonality.

Result: Drastically reduced computational cost while maintaining high-quality reconstruction, with a speedup of several orders of magnitude.

Conclusion: The proposed innovations significantly enhance the efficiency of speech spectrogram inversion, making it more practical for real-world applications.

Abstract: Recent work in online speech spectrogram inversion effectively combines Deep
Learning with the Gradient Theorem to predict phase derivatives directly from
magnitudes. Then, phases are estimated from their derivatives via least
squares, resulting in a high quality reconstruction. In this work, we introduce
three innovations that drastically reduce computational cost, while maintaining
high quality: Firstly, we introduce a novel neural network architecture with
just 8k parameters, 30 times smaller than previous state of the art. Secondly,
increasing latency by 1 hop size allows us to further halve the cost of the
neural inference step. Thirdly, we we observe that the least squares problem
features a tridiagonal matrix and propose a linear-complexity solver for the
least squares step that leverages tridiagonality and positive-semidefiniteness,
achieving a speedup of several orders of magnitude. We release samples online.

</details>


### [623] [Learning to Optimally Dispatch Power: Performance on a Nation-Wide Real-World Dataset](https://arxiv.org/pdf/2505.24505)
*Ignacio Boero, Santiago Diaz, Tomás Vázquez, Enzo Coppes, Pablo Belzarena, Federico Larroca*

Main category: cs.LG

TL;DR: The paper introduces a real-world dataset for ORPD and evaluates learning-based solutions, showing higher errors with real data compared to synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating learning-based ORPD solutions under real-world grid conditions, especially with high renewable energy penetration.

Method: Introduces a dataset of Uruguay's grid and evaluates learning-based ORPD models on real vs. synthetic data.

Result: Prediction errors increase significantly with real-world data, highlighting limitations of current models.

Conclusion: More expressive architectures are needed for robust learning-based ORPD solutions, and the dataset aims to support further research.

Abstract: The Optimal Reactive Power Dispatch (ORPD) problem plays a crucial role in
power system operations, ensuring voltage stability and minimizing power
losses. Recent advances in machine learning, particularly within the ``learning
to optimize'' framework, have enabled fast and efficient approximations of ORPD
solutions, typically by training models on precomputed optimization results.
While these approaches have demonstrated promising performance on synthetic
datasets, their effectiveness under real-world grid conditions remains largely
unexplored. This paper makes two key contributions. First, we introduce a
publicly available power system dataset that includes both the structural
characteristics of Uruguay's electrical grid and nearly two years of real-world
operational data, encompassing actual demand and generation profiles. Given
Uruguay's high penetration of renewable energy, the ORPD problem has become the
primary optimization challenge in its power network. Second, we assess the
impact of real-world data on learning-based ORPD solutions, revealing a
significant increase in prediction errors when transitioning from synthetic to
actual demand and generation inputs. Our results highlight the limitations of
existing models in learning under the complex statistical properties of real
grid conditions and emphasize the need for more expressive architectures. By
providing this dataset, we aim to facilitate further research into robust
learning-based optimization techniques for power system management.

</details>


### [624] [Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting](https://arxiv.org/pdf/2505.24511)
*Jiahao Wang, Mingyue Cheng, Qi Liu*

Main category: cs.LG

TL;DR: The paper explores using slow-thinking LLMs for time series forecasting (TSF) by framing it as a reasoning task, proposing TimeReasoner for evaluation. Results show LLMs have promising zero-shot capabilities but with limitations.


<details>
  <summary>Details</summary>
Motivation: Current TSF methods lack explicit reasoning over temporal dynamics. Slow-thinking LLMs show potential for structured reasoning, prompting the question of their effectiveness in TSF.

Method: Proposes TimeReasoner, a study using prompting strategies to evaluate pretrained slow-thinking LLMs on TSF benchmarks.

Result: Slow-thinking LLMs demonstrate non-trivial zero-shot forecasting abilities, particularly in capturing trends and contextual shifts, though with limitations.

Conclusion: The study highlights LLMs' potential and limitations in TSF, encouraging further research into reasoning-based forecasting for more interpretable frameworks.

Abstract: Time series forecasting (TSF) is a fundamental and widely studied task,
spanning methods from classical statistical approaches to modern deep learning
and multimodal language modeling. Despite their effectiveness, these methods
often follow a fast thinking paradigm emphasizing pattern extraction and direct
value mapping, while overlooking explicit reasoning over temporal dynamics and
contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g.,
ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning
capabilities across diverse domains, suggesting a new opportunity for reframing
TSF as a structured reasoning task. This motivates a key question: can
slow-thinking LLMs effectively reason over temporal patterns to support time
series forecasting, even in zero-shot manner? To investigate this, in this
paper, we propose TimeReasoner, an extensive empirical study that formulates
TSF as a conditional reasoning task. We design a series of prompting strategies
to elicit inference-time reasoning from pretrained slow-thinking LLMs and
evaluate their performance across diverse TSF benchmarks. Our findings reveal
that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities,
especially in capturing high-level trends and contextual shifts. While
preliminary, our study surfaces important insights into the reasoning behaviors
of LLMs in temporal domains highlighting both their potential and limitations.
We hope this work catalyzes further research into reasoning-based forecasting
paradigms and paves the way toward more interpretable and generalizable TSF
frameworks.

</details>


### [625] [Airborne Neural Network](https://arxiv.org/pdf/2505.24513)
*Paritosh Ranjan, Surajit Majumder, Prodip Roy*

Main category: cs.LG

TL;DR: The paper introduces the Airborne Neural Network, a distributed architecture for deploying deep learning in Aerospace, enabling real-time learning and inference during flight.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning in Aerospace is challenging due to infrastructure limitations, despite its potential for real-time data processing and low latency.

Method: Proposes a distributed architecture where airborne devices host subsets of neural network neurons, collaborating under airborne network and layer-specific controllers.

Result: Enables real-time learning and inference in flight, with applications in air traffic control, weather prediction, and geospatial data processing.

Conclusion: This work paves the way for AI-powered Aerospace systems by addressing deployment challenges in airborne environments.

Abstract: Deep Learning, driven by neural networks, has led to groundbreaking
advancements in Artificial Intelligence by enabling systems to learn and adapt
like the human brain. These models have achieved remarkable results,
particularly in data-intensive domains, supported by massive computational
infrastructure. However, deploying such systems in Aerospace, where real time
data processing and ultra low latency are critical, remains a challenge due to
infrastructure limitations. This paper proposes a novel concept: the Airborne
Neural Network a distributed architecture where multiple airborne devices each
host a subset of neural network neurons. These devices compute collaboratively,
guided by an airborne network controller and layer specific controllers,
enabling real-time learning and inference during flight. This approach has the
potential to revolutionize Aerospace applications, including airborne air
traffic control, real-time weather and geographical predictions, and dynamic
geospatial data processing. By enabling large-scale neural network operations
in airborne environments, this work lays the foundation for the next generation
of AI powered Aerospace systems.

</details>


### [626] [Transformers Are Universally Consistent](https://arxiv.org/pdf/2505.24531)
*Sagar Ghosh, Kushal Bose, Swagatam Das*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Despite their central role in the success of foundational models and
large-scale language modeling, the theoretical foundations governing the
operation of Transformers remain only partially understood. Contemporary
research has largely focused on their representational capacity for language
comprehension and their prowess in in-context learning, frequently under
idealized assumptions such as linearized attention mechanisms. Initially
conceived to model sequence-to-sequence transformations, a fundamental and
unresolved question is whether Transformers can robustly perform functional
regression over sequences of input tokens. This question assumes heightened
importance given the inherently non-Euclidean geometry underlying real-world
data distributions. In this work, we establish that Transformers equipped with
softmax-based nonlinear attention are uniformly consistent when tasked with
executing Ordinary Least Squares (OLS) regression, provided both the inputs and
outputs are embedded in hyperbolic space. We derive deterministic upper bounds
on the empirical error which, in the asymptotic regime, decay at a provable
rate of $\mathcal{O}(t^{-1/2d})$, where $t$ denotes the number of input tokens
and $d$ the embedding dimensionality. Notably, our analysis subsumes the
Euclidean setting as a special case, recovering analogous convergence
guarantees parameterized by the intrinsic dimensionality of the data manifold.
These theoretical insights are corroborated through empirical evaluations on
real-world datasets involving both continuous and categorical response
variables.

</details>


### [627] [Directional Non-Commutative Monoidal Structures with Interchange Law via Commutative Generators](https://arxiv.org/pdf/2505.24533)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: A novel algebraic framework generalizes monoidal systems to higher dimensions, unifying classical linear transforms like DFT, Walsh, and Hadamard, while enabling learnable transformations.


<details>
  <summary>Details</summary>
Motivation: To generalize one-dimensional monoidal systems into higher dimensions and unify classical linear transforms under a common algebraic structure.

Method: Define per-axis composition operators with non-commutativity and a global interchange law, recursively from vector-matrix pairs.

Result: The framework unifies classical transforms (DFT, Walsh, Hadamard) and allows derivation of learnable transformations for specific tasks.

Conclusion: The framework provides a systematic, coherent way to model and extend classical transforms, enabling tailored solutions for diverse data modalities.

Abstract: We introduce a novel framework consisting of a class of algebraic structures
that generalize one-dimensional monoidal systems into higher dimensions by
defining per-axis composition operators subject to non-commutativity and a
global interchange law. These structures, defined recursively from a base case
of vector-matrix pairs, model directional composition in multiple dimensions
while preserving structural coherence through commutative linear operators.
  We show that the framework that unifies several well-known linear transforms
in signal processing and data analysis. In this framework, data indices are
embedded into a composite structure that decomposes into simpler components. We
show that classic transforms such as the Discrete Fourier Transform (DFT), the
Walsh transform, and the Hadamard transform are special cases of our algebraic
structure. The framework provides a systematic way to derive these transforms
by appropriately choosing vector and matrix pairs. By subsuming classical
transforms within a common structure, the framework also enables the
development of learnable transformations tailored to specific data modalities
and tasks.

</details>


### [628] [HLSAD: Hodge Laplacian-based Simplicial Anomaly Detection](https://arxiv.org/pdf/2505.24534)
*Florian Frantzen, Michael T. Schaub*

Main category: cs.LG

TL;DR: HLSAD is a novel method for anomaly detection in time-evolving simplicial complexes, outperforming traditional graph-based techniques by capturing higher-order interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional graph anomaly detection fails to capture higher-order interactions, which are crucial for identifying complex structural anomalies.

Method: HLSAD leverages spectral properties of Hodge Laplacians in simplicial complexes to model multi-way interactions, enhancing accuracy and efficiency.

Result: Experiments on synthetic and real-world datasets show HLSAD outperforms existing graph methods in detecting anomalies and change points.

Conclusion: HLSAD provides a robust solution for anomaly detection in higher-order interactions, improving upon traditional graph-based approaches.

Abstract: In this paper, we propose HLSAD, a novel method for detecting anomalies in
time-evolving simplicial complexes. While traditional graph anomaly detection
techniques have been extensively studied, they often fail to capture changes in
higher-order interactions that are crucial for identifying complex structural
anomalies. These higher-order interactions can arise either directly from the
underlying data itself or through graph lifting techniques. Our approach
leverages the spectral properties of Hodge Laplacians of simplicial complexes
to effectively model multi-way interactions among data points. By incorporating
higher-dimensional simplicial structures into our method, our method enhances
both detection accuracy and computational efficiency. Through comprehensive
experiments on both synthetic and real-world datasets, we demonstrate that our
approach outperforms existing graph methods in detecting both events and change
points.

</details>


### [629] [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/pdf/2505.24535)
*Narmeen Oozeer, Luke Marks, Fazl Barez, Amirali Abdullah*

Main category: cs.LG

TL;DR: K-Steering is a non-linear method for controlling multiple behavioral attributes in LLMs at inference time, outperforming linear methods by using gradients from a trained multi-label classifier.


<details>
  <summary>Details</summary>
Motivation: Current linear steering methods for LLMs struggle with attribute interference and require per-attribute tuning, limiting flexibility and scalability.

Method: K-Steering trains a single non-linear multi-label classifier on hidden activations and computes intervention directions via gradients at inference time, avoiding linear assumptions.

Result: Empirical results on benchmarks (ToneBank, DebateMix) show K-Steering outperforms baselines in steering multiple behaviors across 3 model families.

Conclusion: K-Steering provides a unified, flexible solution for dynamic compositional control of LLM behaviors without retraining.

Abstract: Controlling multiple behavioral attributes in large language models (LLMs) at
inference time is a challenging problem due to interference between attributes
and the limitations of linear steering methods, which assume additive behavior
in activation space and require per-attribute tuning. We introduce K-Steering,
a unified and flexible approach that trains a single non-linear multi-label
classifier on hidden activations and computes intervention directions via
gradients at inference time. This avoids linearity assumptions, removes the
need for storing and tuning separate attribute vectors, and allows dynamic
composition of behaviors without retraining. To evaluate our method, we propose
two new benchmarks, ToneBank and DebateMix, targeting compositional behavioral
control. Empirical results across 3 model families, validated by both
activation-based classifiers and LLM-based judges, demonstrate that K-Steering
outperforms strong baselines in accurately steering multiple behaviors.

</details>


### [630] [Neuro-Symbolic Operator for Interpretable and Generalizable Characterization of Complex Piezoelectric Systems](https://arxiv.org/pdf/2505.24578)
*Abhishek Chandra, Taniya Kapoor, Mitrofan Curti, Koen Tiels, Elena A. Lomonova*

Main category: cs.LG

TL;DR: The paper proposes a neuro-symbolic operator (NSO) framework to address interpretability and generalizability issues in neural operators for characterizing piezoelectric systems. NSO combines Fourier neural operators with sparse model discovery to create interpretable, accurate models for hysteresis prediction.


<details>
  <summary>Details</summary>
Motivation: Current neural operators for piezoelectric systems lack interpretability and struggle with generalizability, limiting their practical use.

Method: NSO uses a Fourier neural operator to map voltage fields to displacement profiles, followed by library-based sparse model discovery to derive analytical, interpretable models.

Result: NSO accurately predicts hysteresis, including complex butterfly-shaped relationships, and performs well with noisy or low-fidelity data, outperforming state-of-the-art methods.

Conclusion: NSO improves interpretability and generalizability of neural operators, making it valuable for real-world applications in piezoelectric system characterization.

Abstract: Complex piezoelectric systems are foundational in industrial applications.
Their performance, however, is challenged by the nonlinear voltage-displacement
hysteretic relationships. Efficient characterization methods are, therefore,
essential for reliable design, monitoring, and maintenance. Recently proposed
neural operator methods serve as surrogates for system characterization but
face two pressing issues: interpretability and generalizability.
State-of-the-art (SOTA) neural operators are black-boxes, providing little
insight into the learned operator. Additionally, generalizing them to novel
voltages and predicting displacement profiles beyond the training domain is
challenging, limiting their practical use. To address these limitations, this
paper proposes a neuro-symbolic operator (NSO) framework that derives the
analytical operators governing hysteretic relationships. NSO first learns a
Fourier neural operator mapping voltage fields to displacement profiles,
followed by a library-based sparse model discovery method, generating white-box
parsimonious models governing the underlying hysteresis. These models enable
accurate and interpretable prediction of displacement profiles across varying
and out-of-distribution voltage fields, facilitating generalizability. The
potential of NSO is demonstrated by accurately predicting voltage-displacement
hysteresis, including butterfly-shaped relationships. Moreover, NSO predicts
displacement profiles even for noisy and low-fidelity voltage data, emphasizing
its robustness. The results highlight the advantages of NSO compared to SOTA
neural operators and model discovery methods on several evaluation metrics.
Consequently, NSO contributes to characterizing complex piezoelectric systems
while improving the interpretability and generalizability of neural operators,
essential for design, monitoring, maintenance, and other real-world scenarios.

</details>


### [631] [Conservation-preserved Fourier Neural Operator through Adaptive Correction](https://arxiv.org/pdf/2505.24579)
*Chaoyu Liu, Yangming Li, Zhongying Deng, Chris Budd, Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: Proposes an adaptive correction method for Fourier Neural Operators (FNOs) to ensure exact conservation of fundamental quantities, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Standard FNOs fail to preserve key conservation laws (e.g., mass, momentum), which are critical for accurate physical modeling. Existing methods either use loss functions or post-processing, but none ensure exact and adaptive correction.

Method: Introduces a learnable matrix to adaptively adjust solutions during training, ensuring exact conservation while maintaining flexibility.

Result: Theoretically guarantees no worse data loss than the best conservation-satisfying FNO. Experiments show superior performance over existing methods on various PDEs.

Conclusion: The adaptive correction method effectively enforces conservation laws, improving accuracy and flexibility in FNOs for PDE solutions.

Abstract: Fourier Neural Operators (FNOs) have recently emerged as a promising and
efficient approach for learning the numerical solutions to partial differential
equations (PDEs) from data. However, standard FNO often fails to preserve key
conservation laws, such as mass conservation, momentum conservation, norm
conservation, etc., which are crucial for accurately modeling physical systems.
Existing methods for incorporating these conservation laws into Fourier neural
operators are achieved by designing related loss function or incorporating
post-processing method at the training time. None of them can both exactly and
adaptively correct the outputs to satisfy conservation laws, and our
experiments show that these methods can lead to inferior performance while
preserving conservation laws. In this work, we propose a novel adaptive
correction approach to ensure the conservation of fundamental quantities. Our
method introduces a learnable matrix to adaptively adjust the solution to
satisfy the conservation law during training. It ensures that the outputs
exactly satisfy the goal conservation law and allow for more flexibility and
adaptivity for the model to correct the outputs. We theoretically show that
applying our adaptive correction to an unconstrained FNO yields a solution with
data loss no worse than that of the best conservation-satisfying FNO. We
compare our approach with existing methods on a range of representative PDEs.
Experiment results show that our method consistently outperform other methods.

</details>


### [632] [AutoChemSchematic AI: A Closed-Loop, Physics-Aware Agentic Framework for Auto-Generating Chemical Process and Instrumentation Diagrams](https://arxiv.org/pdf/2505.24584)
*Sakhinana Sagar Srinivas, Shivam Gupta, Venkataramana Runkana*

Main category: cs.LG

TL;DR: A physics-aware AI framework automates industrial-scale PFD/PID generation for chemical processes, integrating domain-specialized SLMs, simulation, and advanced optimizations, outperforming baselines and generalizing to new chemicals.


<details>
  <summary>Details</summary>
Motivation: Transitioning AI-discovered chemicals to industrial production is bottlenecked by the lack of automated PFD/PID generation adhering to engineering constraints.

Method: The framework combines hierarchical knowledge graphs, multi-stage SLM training (SFT, DPO, RAIT), and simulator-in-loop validation with advanced optimizations (FlashAttention, pruning).

Result: Generates high-fidelity, simulator-validated process descriptions, outperforming baselines in correctness and generalizing to unseen chemicals.

Conclusion: Bridges AI-driven design with industrial feasibility, reducing R&D timelines from lab to plant deployment.

Abstract: Recent advancements in generative AI have accelerated the discovery of novel
chemicals and materials; however, transitioning these discoveries to
industrial-scale production remains a critical bottleneck, as it requires the
development of entirely new chemical manufacturing processes. Current AI
methods cannot auto-generate PFDs or PIDs, despite their critical role in
scaling chemical processes, while adhering to engineering constraints. We
present a closed loop, physics aware framework for the automated generation of
industrially viable PFDs and PIDs. The framework integrates domain specialized
small scale language models (SLMs) (trained for chemical process QA tasks) with
first principles simulation, leveraging three key components: (1) a
hierarchical knowledge graph of process flow and instrumentation descriptions
for 1,020+ chemicals, (2) a multi-stage training pipeline that fine tunes
domain specialized SLMs on synthetic datasets via Supervised Fine-Tuning (SFT),
Direct Preference Optimization (DPO), and Retrieval-Augmented Instruction
Tuning (RAIT), and (3) DWSIM based simulator in the loop validation to ensure
feasibility. To improve both runtime efficiency and model compactness, the
framework incorporates advanced inference time optimizations including
FlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,
and Test Time Inference Scaling and independently applies structural pruning
techniques (width and depth) guided by importance heuristics to reduce model
size with minimal accuracy loss. Experiments demonstrate that the framework
generates simulator-validated process descriptions with high fidelity,
outperforms baseline methods in correctness, and generalizes to unseen
chemicals. By bridging AI-driven design with industrial-scale feasibility, this
work significantly reduces R&D timelines from lab discovery to plant
deployment.

</details>


### [633] [A Flat Minima Perspective on Understanding Augmentations and Model Robustness](https://arxiv.org/pdf/2505.24592)
*Weebum Yoo, Sung Whan Yoon*

Main category: cs.LG

TL;DR: The paper provides a unified theoretical framework to explain how data augmentations improve model robustness, focusing on loss surface flatness and PAC generalization bounds. It validates the theory with experiments on common benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a general theoretical understanding of how data augmentations enhance model robustness across various distribution shifts.

Method: The study proposes a theoretical framework analyzing augmentations through loss surface flatness and PAC generalization bounds, and validates it with simulations on benchmarks like CIFAR, ImageNet, PACS, and OfficeHome.

Result: The framework broadly explains the efficacy of augmentations in improving robustness, confirmed by experiments on corruption, adversarial, and domain generalization benchmarks.

Conclusion: The work offers a unified theoretical perspective on augmentations' role in robustness, applicable across diverse augmentation methods and distribution shifts.

Abstract: Model robustness indicates a model's capability to generalize well on
unforeseen distributional shifts, including data corruption, adversarial
attacks, and domain shifts. Data augmentation is one of the prevalent and
effective ways to enhance robustness. Despite the great success of
augmentations in different fields, a general theoretical understanding of their
efficacy in improving model robustness is lacking. We offer a unified
theoretical framework to clarify how augmentations can enhance model robustness
through the lens of loss surface flatness and PAC generalization bound. Our
work diverges from prior studies in that our analysis i) broadly encompasses
much of the existing augmentation methods, and ii) is not limited to specific
types of distribution shifts like adversarial attacks. We confirm our theories
through simulations on the existing common corruption and adversarial
robustness benchmarks based on the CIFAR and ImageNet datasets, as well as
domain generalization benchmarks including PACS and OfficeHome.

</details>


### [634] [Binary Cumulative Encoding meets Time Series Forecasting](https://arxiv.org/pdf/2505.24595)
*Andrei Chernov, Vitaliy Pozdnyakov, Ilya Makarov*

Main category: cs.LG

TL;DR: The paper proposes binary cumulative encoding (BCE) for time series forecasting, addressing limitations of one-hot encoding by preserving ordinal structure and improving distance awareness.


<details>
  <summary>Details</summary>
Motivation: Existing methods using one-hot encoding ignore ordinal structure, limiting their ability to model relative distances between predicted and true values.

Method: Introduces BCE to represent scalar targets as monotonic binary vectors, preserving order and magnitude. Uses a CNN with residual and dilated convolutions for efficient temporal modeling.

Result: Outperforms existing methods in point and probabilistic forecasting, with fewer parameters and faster training.

Conclusion: BCE enhances forecasting accuracy and efficiency by leveraging ordinal structure within a classification framework.

Abstract: Recent studies in time series forecasting have explored formulating
regression via classification task. By discretizing the continuous target space
into bins and predicting over a fixed set of classes, these approaches benefit
from stable training, robust uncertainty modeling, and compatibility with
modern deep learning architectures. However, most existing methods rely on
one-hot encoding that ignores the inherent ordinal structure of the underlying
values. As a result, they fail to provide information about the relative
distance between predicted and true values during training. In this paper, we
propose to address this limitation by introducing binary cumulative encoding
(BCE), that represents scalar targets into monotonic binary vectors. This
encoding implicitly preserves order and magnitude information, allowing the
model to learn distance-aware representations while still operating within a
classification framework. We propose a convolutional neural network
architecture specifically designed for BCE, incorporating residual and dilated
convolutions to enable fast and expressive temporal modeling. Through extensive
experiments on benchmark forecasting datasets, we show that our approach
outperforms widely used methods in both point and probabilistic forecasting,
while requiring fewer parameters and enabling faster training.

</details>


### [635] [The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches](https://arxiv.org/pdf/2505.24603)
*Omri Lev, Vishwak Srinivasan, Moshe Shenfeld, Katrina Ligett, Ayush Sekhari, Ashia C. Wilson*

Main category: cs.LG

TL;DR: The paper refines the privacy analysis of Gaussian sketching using Renyi Differential Privacy (RDP), providing tighter bounds and demonstrating improved performance in linear regression.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding and utility of Gaussian sketching in data science by leveraging RDP for tighter privacy bounds.

Method: Revisits Gaussian sketching through RDP, refining privacy analysis and applying it to linear regression.

Result: Tighter privacy bounds and improved performance in linear regression, with empirical validation across datasets.

Conclusion: The refined RDP analysis of Gaussian sketching enhances privacy guarantees and practical performance in data science applications.

Abstract: Gaussian sketching, which consists of pre-multiplying the data with a random
Gaussian matrix, is a widely used technique for multiple problems in data
science and machine learning, with applications spanning computationally
efficient optimization, coded computing, and federated learning. This operation
also provides differential privacy guarantees due to its inherent randomness.
In this work, we revisit this operation through the lens of Renyi Differential
Privacy (RDP), providing a refined privacy analysis that yields significantly
tighter bounds than prior results. We then demonstrate how this improved
analysis leads to performance improvement in different linear regression
settings, establishing theoretical utility guarantees. Empirically, our methods
improve performance across multiple datasets and, in several cases, reduce
runtime.

</details>


### [636] [Multi-criteria Rank-based Aggregation for Explainable AI](https://arxiv.org/pdf/2505.24612)
*Sujoy Chatterjee, Everton Romanzini Colombo, Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: The paper introduces a multi-criteria rank-based weighted aggregation method to balance conflicting XAI metrics, improving explanation robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of multi-criteria decision-making in XAI to reduce variability in explanations.

Method: Proposes a rank-based weighted aggregation method and adapts XAI metrics for ranked feature importance.

Result: Demonstrates robustness on public datasets, identifying TOPSIS and WSUM as top-performing algorithms.

Conclusion: The method effectively balances multiple XAI metrics, enhancing explanation reliability.

Abstract: Explainability is crucial for improving the transparency of black-box machine
learning models. With the advancement of explanation methods such as LIME and
SHAP, various XAI performance metrics have been developed to evaluate the
quality of explanations. However, different explainers can provide contrasting
explanations for the same prediction, introducing trade-offs across conflicting
quality metrics. Although available aggregation approaches improve robustness,
reducing explanations' variability, very limited research employed a
multi-criteria decision-making approach. To address this gap, this paper
introduces a multi-criteria rank-based weighted aggregation method that
balances multiple quality metrics simultaneously to produce an ensemble of
explanation models. Furthermore, we propose rank-based versions of existing XAI
metrics (complexity, faithfulness and stability) to better evaluate ranked
feature importance explanations. Extensive experiments on publicly available
datasets demonstrate the robustness of the proposed model across these metrics.
Comparative analyses of various multi-criteria decision-making and rank
aggregation algorithms showed that TOPSIS and WSUM are the best candidates for
this use case.

</details>


### [637] [Hyperbolic Dataset Distillation](https://arxiv.org/pdf/2505.24623)
*Wenyuan Li, Guang Li, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama*

Main category: cs.LG

TL;DR: The paper introduces Hyperbolic Dataset Distillation (HDD), a method to synthesize compact datasets while preserving hierarchical relationships, improving efficiency and performance over Euclidean-based methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale datasets in deep learning pose computational and storage challenges. Existing distribution matching (DM) methods overlook hierarchical relationships by treating data as independent points in Euclidean space.

Method: HDD embeds features into Lorentz hyperbolic space, measuring discrepancy via hyperbolic distance between centroids. It optimizes this distance to integrate hierarchical structure into distillation.

Result: HDD retains model performance with only 20% of the distilled core set, improves training stability, and is compatible with existing DM methods.

Conclusion: HDD effectively addresses limitations of Euclidean-based DM by leveraging hyperbolic space, enhancing efficiency and preserving hierarchical data relationships.

Abstract: To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


### [638] [Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees](https://arxiv.org/pdf/2505.24627)
*Fu Luo, Yaoxin Wu, Zhi Zheng, Zhenkun Wang*

Main category: cs.LG

TL;DR: The paper analyzes how constraint tightness affects neural combinatorial optimization (NCO) methods, focusing on the CVRP. It finds existing methods overfit to fixed constraints and proposes a new training scheme with a multi-expert module to improve adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing NCO methods lack research on constraint tightness impact, leading to overfitting and poor performance across varying constraints.

Method: Proposes a training scheme considering varying constraint tightness and introduces a multi-expert module for adaptable strategies.

Result: The method outperforms existing NCO techniques on CVRP and CVRPTW with diverse constraint tightness.

Conclusion: The proposed approach effectively addresses overfitting, enhancing NCO performance under varying constraints.

Abstract: Recent neural combinatorial optimization (NCO) methods have shown promising
problem-solving ability without requiring domain-specific expertise. Most
existing NCO methods use training and testing data with a fixed constraint
value and lack research on the effect of constraint tightness on the
performance of NCO methods. This paper takes the capacity-constrained vehicle
routing problem (CVRP) as an example to empirically analyze the NCO performance
under different tightness degrees of the capacity constraint. Our analysis
reveals that existing NCO methods overfit the capacity constraint, and they can
only perform satisfactorily on a small range of the constraint values but
poorly on other values. To tackle this drawback of existing NCO methods, we
develop an efficient training scheme that explicitly considers varying degrees
of constraint tightness and proposes a multi-expert module to learn a generally
adaptable solving strategy. Experimental results show that the proposed method
can effectively overcome the overfitting issue, demonstrating superior
performances on the CVRP and CVRP with time windows (CVRPTW) with various
constraint tightness degrees.

</details>


### [639] [Stop Guessing: Optimizing Goalkeeper Policies for Soccer Penalty Kicks](https://arxiv.org/pdf/2505.24629)
*Lotte Bransen, Tim Janssen, Jesse Davis*

Main category: cs.LG

TL;DR: The paper analyzes soccer penalties, addressing the unrealistic assumption of independent actions by goalkeepers and takers. It introduces a player-agnostic simulation framework to evaluate goalkeeper strategies, using expert-annotated data.


<details>
  <summary>Details</summary>
Motivation: Existing penalty analyses assume independent actions, ignoring interdependencies between goalkeepers and takers. This limits practical advice.

Method: Develops a player-agnostic simulation framework incorporating diverse choices and goalkeeper skills, using expert-annotated penalty data.

Result: The framework effectively evaluates and optimizes goalkeeper strategies, overcoming small sample size limitations.

Conclusion: The proposed framework provides actionable insights for optimizing goalkeeper policies in real-world penalty scenarios.

Abstract: Penalties are fraught and game-changing moments in soccer games that teams
explicitly prepare for. Consequently, there has been substantial interest in
analyzing them in order to provide advice to practitioners. From a data science
perspective, such analyses suffer from a significant limitation: they make the
unrealistic simplifying assumption that goalkeepers and takers select their
action -- where to dive and where to the place the kick -- independently of
each other. In reality, the choices that some goalkeepers make depend on the
taker's movements and vice-versa. This adds substantial complexity to the
problem because not all players have the same action capacities, that is, only
some players are capable of basing their decisions on their opponent's
movements. However, the small sample sizes on the player level mean that one
may have limited insights into a specific opponent's capacities. We address
these challenges by developing a player-agnostic simulation framework that can
evaluate the efficacy of different goalkeeper strategies. It considers a rich
set of choices and incorporates information about a goalkeeper's skills. Our
work is grounded in a large dataset of penalties that were annotated by penalty
experts and include aspects of both kicker and goalkeeper strategies. We show
how our framework can be used to optimize goalkeeper policies in real-world
situations.

</details>


### [640] [WILTing Trees: Interpreting the Distance Between MPNN Embeddings](https://arxiv.org/pdf/2505.24642)
*Masahiro Negishi, Thomas Gärtner, Pascal Welke*

Main category: cs.LG

TL;DR: The paper explores the distance function learned by MPNNs in specific tasks, focusing on functional distances between prediction targets, and introduces an interpretable graph distance method using optimal transport on WILT.


<details>
  <summary>Details</summary>
Motivation: Previous work linked MPNN distances to structural graph distances, ignoring task-specific information. This study aims to bridge this gap by analyzing functional distances learned by MPNNs.

Method: The method distills MPNN embedding distances into an interpretable graph distance using optimal transport on the Weisfeiler Leman Labeling Tree (WILT), revealing influential subgraphs.

Result: The approach generalizes two well-known graph kernels, is computable in linear time, and shows MPNNs focus on functionally important subgraphs for embedding positions.

Conclusion: MPNNs implicitly learn functional distances by emphasizing key subgraphs, providing a task-aware interpretable distance measure.

Abstract: We investigate the distance function learned by message passing neural
networks (MPNNs) in specific tasks, aiming to capture the functional distance
between prediction targets that MPNNs implicitly learn. This contrasts with
previous work, which links MPNN distances on arbitrary tasks to structural
distances on graphs that ignore task-specific information. To address this gap,
we distill the distance between MPNN embeddings into an interpretable graph
distance. Our method uses optimal transport on the Weisfeiler Leman Labeling
Tree (WILT), where the edge weights reveal subgraphs that strongly influence
the distance between embeddings. This approach generalizes two well-known graph
kernels and can be computed in linear time. Through extensive experiments, we
demonstrate that MPNNs define the relative position of embeddings by focusing
on a small set of subgraphs that are known to be functionally important in the
domain.

</details>


### [641] [Learning Distributions over Permutations and Rankings with Factorized Representations](https://arxiv.org/pdf/2505.24664)
*Daniel Severo, Brian Karrer, Niklas Nolte*

Main category: cs.LG

TL;DR: A novel method for learning distributions over permutations using alternative representations like Lehmer codes, enabling unconstrained deep learning and outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing methods for permutation learning, which rely on expensive variational inference or parametric families.

Method: Leverages bijective representations (Lehmer codes, Fisher-Yates draws, Insertion-Vectors) to enable unconstrained learning with deep learning techniques.

Result: Outperforms current methods on benchmarks like jigsaw puzzles and handles non-trivial distributions even in less expressive modes.

Conclusion: Proposes scalable and flexible permutation learning, validated by new benchmarks, showing superiority over traditional models.

Abstract: Learning distributions over permutations is a fundamental problem in machine
learning, with applications in ranking, combinatorial optimization, structured
prediction, and data association. Existing methods rely on mixtures of
parametric families or neural networks with expensive variational inference
procedures. In this work, we propose a novel approach that leverages
alternative representations for permutations, including Lehmer codes,
Fisher-Yates draws, and Insertion-Vectors. These representations form a
bijection with the symmetric group, allowing for unconstrained learning using
conventional deep learning techniques, and can represent any probability
distribution over permutations. Our approach enables a trade-off between
expressivity of the model family and computational requirements. In the least
expressive and most computationally efficient case, our method subsumes
previous families of well established probabilistic models over permutations,
including Mallow's and the Repeated Insertion Model. Experiments indicate our
method significantly outperforms current approaches on the jigsaw puzzle
benchmark, a common task for permutation learning. However, we argue this
benchmark is limited in its ability to assess learning probability
distributions, as the target is a delta distribution (i.e., a single correct
solution exists). We therefore propose two additional benchmarks: learning
cyclic permutations and re-ranking movies based on user preference. We show
that our method learns non-trivial distributions even in the least expressive
mode, while traditional models fail to even generate valid permutations in this
setting.

</details>


### [642] [Learning geometry and topology via multi-chart flows](https://arxiv.org/pdf/2505.24665)
*Hanlin Yu, Søren Hauberg, Marcelo Hartmann, Arto Klami, Georgios Arvanitidis*

Main category: cs.LG

TL;DR: The paper proposes a method to learn degenerate normalizing flows for data on low-dimensional Riemannian manifolds with non-trivial topology, using multiple flows and a new algorithm for geodesic computation.


<details>
  <summary>Details</summary>
Motivation: Real-world data often lie on low-dimensional manifolds with complex topology, which cannot be accurately modeled by a single flow.

Method: The authors introduce a training scheme for multiple flows and develop numerical algorithms to compute geodesics on such manifolds.

Result: Empirical results show significant improvements in topology estimation.

Conclusion: The approach effectively handles non-trivial topology by combining multiple flows and geodesic computation.

Abstract: Real world data often lie on low-dimensional Riemannian manifolds embedded in
high-dimensional spaces. This motivates learning degenerate normalizing flows
that map between the ambient space and a low-dimensional latent space. However,
if the manifold has a non-trivial topology, it can never be correctly learned
using a single flow. Instead multiple flows must be `glued together'. In this
paper, we first propose the general training scheme for learning such a
collection of flows, and secondly we develop the first numerical algorithms for
computing geodesics on such manifolds. Empirically, we demonstrate that this
leads to highly significant improvements in topology estimation.

</details>


### [643] [Predicting the Past: Estimating Historical Appraisals with OCR and Machine Learning](https://arxiv.org/pdf/2505.24676)
*Mihir Bhaskar, Jun Tao Luo, Zihan Geng, Asmita Hajra, Junia Howell, Matthew R. Gormley*

Main category: cs.LG

TL;DR: The paper presents a method to digitize historical housing appraisal records using OCR and regression models, enabling large-scale analysis of racial wealth disparities caused by 1930s U.S. housing policies.


<details>
  <summary>Details</summary>
Motivation: Quantifying the financial effects of 1930s U.S. housing policies on racial wealth disparities is challenging due to inaccessible historical property records stored in physical formats.

Method: Combines manual annotation, OCR, and regression models to digitize and estimate historical property values from scanned documents and building features.

Result: Created a dataset for one county, digitizing 12,000 properties manually and 50,000 using OCR, and tested a regression model for generalizability.

Conclusion: Provides cost-effective tools for scholars and policymakers to analyze historical redlining impacts.

Abstract: Despite well-documented consequences of the U.S. government's 1930s housing
policies on racial wealth disparities, scholars have struggled to quantify its
precise financial effects due to the inaccessibility of historical property
appraisal records. Many counties still store these records in physical formats,
making large-scale quantitative analysis difficult. We present an approach
scholars can use to digitize historical housing assessment data, applying it to
build and release a dataset for one county. Starting from publicly available
scanned documents, we manually annotated property cards for over 12,000
properties to train and validate our methods. We use OCR to label data for an
additional 50,000 properties, based on our two-stage approach combining
classical computer vision techniques with deep learning-based OCR. For cases
where OCR cannot be applied, such as when scanned documents are not available,
we show how a regression model based on building feature data can estimate the
historical values, and test the generalizability of this model to other
counties. With these cost-effective tools, scholars, community activists, and
policy makers can better analyze and understand the historical impacts of
redlining.

</details>


### [644] [Disentangling Granularity: An Implicit Inductive Bias in Factorized VAEs](https://arxiv.org/pdf/2505.24684)
*Zihao Chen, Yu Xiang, Wenyong Wang*

Main category: cs.LG

TL;DR: The paper explores implicit inductive bias in VAEs, identifying "disentangling granularity" as a key factor, validated through experiments with β-TCVAE and β-STCVAE.


<details>
  <summary>Details</summary>
Motivation: To uncover the elusive implicit inductive bias driving disentanglement in VAEs with factorization priors.

Method: Analysis of total correlation in β-TCVAE, leading to the discovery of disentangling granularity and validation via 100K+ experiments.

Result: Fixed granularity in VAEs limits disentanglement to low-complexity features, while tunable granularity (β-STCVAE) enables high-complexity disentanglement.

Conclusion: Disentangling granularity is a critical implicit bias in VAEs, influencing disentanglement performance and ELBO inference, enhancing interpretability.

Abstract: Despite the success in learning semantically meaningful, unsupervised
disentangled representations, variational autoencoders (VAEs) and their
variants face a fundamental theoretical challenge: substantial evidence
indicates that unsupervised disentanglement is unattainable without implicit
inductive bias, yet such bias remains elusive. In this work, we focus on
exploring the implicit inductive bias that drive disentanglement in VAEs with
factorization priors. By analyzing the total correlation in \b{eta}-TCVAE, we
uncover a crucial implicit inductive bias called disentangling granularity,
which leads to the discovery of an interesting "V"-shaped optimal Evidence
Lower Bound (ELBO) trajectory within the parameter space. This finding is
validated through over 100K experiments using factorized VAEs and our newly
proposed model, \b{eta}-STCVAE. Notably, experimental results reveal that
conventional factorized VAEs, constrained by fixed disentangling granularity,
inherently tend to disentangle low-complexity feature. Whereas, appropriately
tuning disentangling granularity, as enabled by \b{eta}-STCVAE, broadens the
range of disentangled representations, allowing for the disentanglement of
high-complexity features. Our findings unveil that disentangling granularity as
an implicit inductive bias in factorized VAEs influence both disentanglement
performance and the inference of the ELBO, offering fresh insights into the
interpretability and inherent biases of VAEs.

</details>


### [645] [Quick-Draw Bandits: Quickly Optimizing in Nonstationary Environments with Extremely Many Arms](https://arxiv.org/pdf/2505.24692)
*Derek Everett, Fred Lu, Edward Raff, Fernando Camacho, James Holt*

Main category: cs.LG

TL;DR: Proposes a novel Gaussian interpolation-based policy for multi-armed bandits, handling continuous reward spaces and non-stationarity, with efficient regret bounds and computational advantages.


<details>
  <summary>Details</summary>
Motivation: Existing bandit methods either handle non-stationarity or large action spaces, but not both. This work aims to bridge this gap.

Method: Uses Gaussian interpolation to learn continuous Lipschitz reward functions, extending to non-stationary problems with a simple modification.

Result: Achieves O*(√T) cumulative regret, is computationally efficient (100-10000x faster), and outperforms sliding Gaussian process policies in non-stationary, large-arm settings.

Conclusion: The proposed method effectively combines handling large action spaces and non-stationarity, offering computational and performance benefits.

Abstract: Canonical algorithms for multi-armed bandits typically assume a stationary
reward environment where the size of the action space (number of arms) is
small. More recently developed methods typically relax only one of these
assumptions: existing non-stationary bandit policies are designed for a small
number of arms, while Lipschitz, linear, and Gaussian process bandit policies
are designed to handle a large (or infinite) number of arms in stationary
reward environments under constraints on the reward function. In this
manuscript, we propose a novel policy to learn reward environments over a
continuous space using Gaussian interpolation. We show that our method
efficiently learns continuous Lipschitz reward functions with
$\mathcal{O}^*(\sqrt{T})$ cumulative regret. Furthermore, our method naturally
extends to non-stationary problems with a simple modification. We finally
demonstrate that our method is computationally favorable (100-10000x faster)
and experimentally outperforms sliding Gaussian process policies on datasets
with non-stationarity and an extremely large number of arms.

</details>


### [646] [On Symmetric Losses for Robust Policy Optimization with Noisy Preferences](https://arxiv.org/pdf/2505.24709)
*Soichiro Nishimori, Yu-Jie Zhang, Thanawat Lodkaew, Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper introduces Symmetric Preference Optimization (SymPO), a robust method for optimizing policies under noisy human preference data by leveraging symmetric losses in reward modeling.


<details>
  <summary>Details</summary>
Motivation: Real-world preference data often contains noise due to human errors or biases, challenging conventional methods that assume accurate annotations.

Method: The proposed SymPO framework treats reward modeling as a classification problem, using symmetric losses known for robustness to label noise.

Result: Experiments on synthetic and real-world tasks show SymPO's effectiveness in maintaining policy improvement despite noisy labels.

Conclusion: SymPO provides a principled solution for robust policy optimization under noisy human preferences, preserving reward ranks for successful policy improvement.

Abstract: Optimizing policies based on human preferences is key to aligning language
models with human intent. This work focuses on reward modeling, a core
component in reinforcement learning from human feedback (RLHF), and offline
preference optimization, such as direct preference optimization. Conventional
approaches typically assume accurate annotations. However, real-world
preference data often contains noise due to human errors or biases. We propose
a principled framework for robust policy optimization under noisy preferences,
viewing reward modeling as a classification problem. This allows us to leverage
symmetric losses, known for their robustness to label noise in classification,
leading to our Symmetric Preference Optimization (SymPO) method. We prove that
symmetric losses enable successful policy optimization even under noisy labels,
as the resulting reward remains rank-preserving -- a property sufficient for
policy improvement. Experiments on synthetic and real-world tasks demonstrate
the effectiveness of SymPO.

</details>


### [647] [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/pdf/2505.24710)
*Wei Chen, Jiahao Zhang, Haipeng Zhu, Boyan Xu, Zhifeng Hao, Keli Zhang, Junjian Ye, Ruichu Cai*

Main category: cs.LG

TL;DR: The paper proposes Causal-aware LLMs, integrating structural causal models (SCM) to enhance decision-making by learning, adapting, and acting iteratively.


<details>
  <summary>Details</summary>
Motivation: Pre-trained LLMs lack reasoning and adaptability for complex tasks, needing a structured approach inspired by human cognition.

Method: A three-stage paradigm: learning (extract causal entities), adapting (update via causal intervention), and acting (use causal knowledge for policy-making).

Result: Validated on 22 tasks in "Crafter," showing improved decision-making.

Conclusion: Causal-aware LLMs enhance understanding and efficiency in dynamic environments.

Abstract: Large language models (LLMs) have shown great potential in decision-making
due to the vast amount of knowledge stored within the models. However, these
pre-trained models are prone to lack reasoning abilities and are difficult to
adapt to new environments, further hindering their application to complex
real-world tasks. To address these challenges, inspired by the human cognitive
process, we propose Causal-aware LLMs, which integrate the structural causal
model (SCM) into the decision-making process to model, update, and utilize
structured knowledge of the environment in a ``learning-adapting-acting"
paradigm. Specifically, in the learning stage, we first utilize an LLM to
extract the environment-specific causal entities and their causal relations to
initialize a structured causal model of the environment. Subsequently,in the
adapting stage, we update the structured causal model through external feedback
about the environment, via an idea of causal intervention. Finally, in the
acting stage, Causal-aware LLMs exploit structured causal knowledge for more
efficient policy-making through the reinforcement learning agent. The above
processes are performed iteratively to learn causal knowledge, ultimately
enabling the causal-aware LLMs to achieve a more accurate understanding of the
environment and make more efficient decisions. Experimental results across 22
diverse tasks within the open-world game ``Crafter" validate the effectiveness
of our proposed method.

</details>


### [648] [CoRet: Improved Retriever for Code Editing](https://arxiv.org/pdf/2505.24715)
*Fabio Fehr, Prabhu Teja Sivaprasad, Luca Franceschi, Giovanni Zappella*

Main category: cs.LG

TL;DR: CoRet is a dense retrieval model for code-editing tasks, improving recall by 15% over existing models by integrating code semantics, repository structure, and call graphs.


<details>
  <summary>Details</summary>
Motivation: To enhance retrieval of relevant code portions for natural language queries in code-editing tasks like feature implementation or bug fixes.

Method: Uses a dense retrieval model with a custom loss function for repository-level retrieval, integrating code semantics, repository structure, and call graph dependencies.

Result: Achieves at least 15% higher retrieval recall on SWE-bench and Long Code Arena datasets compared to existing models.

Conclusion: CoRet's design choices significantly improve retrieval performance for code-editing tasks.

Abstract: In this paper, we introduce CoRet, a dense retrieval model designed for
code-editing tasks that integrates code semantics, repository structure, and
call graph dependencies. The model focuses on retrieving relevant portions of a
code repository based on natural language queries such as requests to implement
new features or fix bugs. These retrieved code chunks can then be presented to
a user or to a second code-editing model or agent. To train CoRet, we propose a
loss function explicitly designed for repository-level retrieval. On SWE-bench
and Long Code Arena's bug localisation datasets, we show that our model
substantially improves retrieval recall by at least 15 percentage points over
existing models, and ablate the design choices to show their importance in
achieving these results.

</details>


### [649] [PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations](https://arxiv.org/pdf/2505.24717)
*Benjamin Holzschuh, Qiang Liu, Georg Kohl, Nils Thuerey*

Main category: cs.LG

TL;DR: PDE-Transformer is a scalable transformer-based architecture for surrogate modeling of physics simulations, outperforming state-of-the-art models on PDE datasets and improving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: To create a more scalable and versatile transformer architecture for large-scale physics simulations, addressing the need for general-purpose foundation models in physical sciences.

Method: Combines diffusion transformer improvements with adjustments for large-scale simulations, embedding physical channels as spatio-temporal tokens with channel-wise self-attention.

Result: Outperforms state-of-the-art transformer architectures on 16 PDE types and shows improved performance on downstream tasks compared to training from scratch.

Conclusion: PDE-Transformer is effective for large-scale physics simulations, offering a scalable backbone for foundation models in physical sciences.

Abstract: We introduce PDE-Transformer, an improved transformer-based architecture for
surrogate modeling of physics simulations on regular grids. We combine recent
architectural improvements of diffusion transformers with adjustments specific
for large-scale simulations to yield a more scalable and versatile
general-purpose transformer architecture, which can be used as the backbone for
building large-scale foundation models in physical sciences. We demonstrate
that our proposed architecture outperforms state-of-the-art transformer
architectures for computer vision on a large dataset of 16 different types of
PDEs. We propose to embed different physical channels individually as
spatio-temporal tokens, which interact via channel-wise self-attention. This
helps to maintain a consistent information density of tokens when learning
multiple types of PDEs simultaneously. We demonstrate that our pre-trained
models achieve improved performance on several challenging downstream tasks
compared to training from scratch and also beat other foundation model
architectures for physics simulations.

</details>


### [650] [HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts](https://arxiv.org/pdf/2505.24722)
*Neil He, Rishabh Anand, Hiren Madhu, Ali Maatouk, Smita Krishnaswamy, Leandros Tassiulas, Menglin Yang, Rex Ying*

Main category: cs.LG

TL;DR: The paper introduces HELM, a family of hyperbolic large language models, addressing limitations of Euclidean-based LLMs by leveraging hyperbolic geometry for better text representation and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs rely on Euclidean operations, which fail to capture the inherent semantic hierarchies and geometric structure of natural language, leading to training instabilities and degraded performance.

Method: The authors propose HELM, a hyperbolic Transformer-based LLM, including variants like HELM-MICE (Mixture-of-Curvature Experts) and HELM-D (dense model). They introduce hyperbolic equivalents of key operations like rotary positional encodings and RMS normalization, along with hyperbolic Multi-Head Latent Attention (HMLA) for efficiency.

Result: HELM models achieve consistent gains (up to 4%) over Euclidean architectures (e.g., LLaMA, DeepSeek) on benchmarks like MMLU and ARC, demonstrating improved reasoning and performance.

Conclusion: Hyperbolic geometry enhances large-scale LM pretraining, offering better alignment with natural language structure and improved reasoning capabilities.

Abstract: Large language models (LLMs) have shown great success in text modeling tasks
across domains. However, natural language exhibits inherent semantic
hierarchies and nuanced geometric structure, which current LLMs do not capture
completely owing to their reliance on Euclidean operations. Recent studies have
also shown that not respecting the geometry of token embeddings leads to
training instabilities and degradation of generative capabilities. These
findings suggest that shifting to non-Euclidean geometries can better align
language models with the underlying geometry of text. We thus propose to
operate fully in Hyperbolic space, known for its expansive, scale-free, and
low-distortion properties. We thus introduce HELM, a family of HypErbolic Large
Language Models, offering a geometric rethinking of the Transformer-based LLM
that addresses the representational inflexibility, missing set of necessary
operations, and poor scalability of existing hyperbolic LMs. We additionally
introduce a Mixture-of-Curvature Experts model, HELM-MICE, where each expert
operates in a distinct curvature space to encode more fine-grained geometric
structure from text, as well as a dense model, HELM-D. For HELM-MICE, we
further develop hyperbolic Multi-Head Latent Attention (HMLA) for efficient,
reduced-KV-cache training and inference. For both models, we develop essential
hyperbolic equivalents of rotary positional encodings and RMS normalization. We
are the first to train fully hyperbolic LLMs at billion-parameter scale, and
evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM
problem-solving, general knowledge, and commonsense reasoning. Our results show
consistent gains from our HELM architectures -- up to 4% -- over popular
Euclidean architectures used in LLaMA and DeepSeek, highlighting the efficacy
and enhanced reasoning afforded by hyperbolic geometry in large-scale LM
pretraining.

</details>


### [651] [Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach](https://arxiv.org/pdf/2505.24721)
*Nick Rossenbach, Benedikt Hilmes, Leon Brackmann, Moritz Gunz, Ralf Schlüter*

Main category: cs.LG

TL;DR: A PyTorch-based library simulates memristor hardware for large neural networks, showing minimal performance degradation with 3-bit precision.


<details>
  <summary>Details</summary>
Motivation: Current memristor hardware prototypes are limited to small ML models, and existing simulations oversimplify hardware properties.

Method: Proposed a PyTorch library (Synaptogen) to simulate neural networks with accurate memristor properties, tested on a Conformer model for speech recognition.

Result: With quantization-aware training, a 3-bit weight precision limited performance degradation to 25% in word error rate.

Conclusion: The library enables realistic simulation of large ML models on memristor hardware, demonstrating feasibility with minimal performance loss.

Abstract: Memristor-based hardware offers new possibilities for energy-efficient
machine learning (ML) by providing analog in-memory matrix multiplication.
Current hardware prototypes cannot fit large neural networks, and related
literature covers only small ML models for tasks like MNIST or single word
recognition. Simulation can be used to explore how hardware properties affect
larger models, but existing software assumes simplified hardware. We propose a
PyTorch-based library based on "Synaptogen" to simulate neural network
execution with accurately captured memristor hardware properties. For the first
time, we show how an ML system with millions of parameters would behave on
memristor hardware, using a Conformer trained on the speech recognition task
TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the
relative degradation in word error rate to 25% when using a 3-bit weight
precision to execute linear operations via simulated analog computation.

</details>


### [652] [Robust Federated Learning against Model Perturbation in Edge Networks](https://arxiv.org/pdf/2505.24728)
*Dongzi Jin, Yong Xiao, Yingyu Li*

Main category: cs.LG

TL;DR: SMRFL improves robustness in Federated Learning by minimizing the maximum loss in a model neighborhood, achieving convergence rates comparable to standard FL.


<details>
  <summary>Details</summary>
Motivation: Existing FL methods assume ideal shared models, but real-world perturbations degrade performance. SMRFL addresses this by enhancing model robustness.

Method: SMRFL uses a min-max optimization to find flat minima, reducing sensitivity to perturbations by minimizing the maximum loss in a parameter neighborhood.

Result: Theoretical and experimental results show SMRFL converges like standard FL and outperforms baselines in robustness under perturbations.

Conclusion: SMRFL effectively enhances model robustness in FL, making it practical for real-world edge intelligence applications.

Abstract: Federated Learning (FL) is a promising paradigm for realizing edge
intelligence, allowing collaborative learning among distributed edge devices by
sharing models instead of raw data. However, the shared models are often
assumed to be ideal, which would be inevitably violated in practice due to
various perturbations, leading to significant performance degradation. To
overcome this challenge, we propose a novel method, termed Sharpness-Aware
Minimization-based Robust Federated Learning (SMRFL), which aims to improve
model robustness against perturbations by exploring the geometrical property of
the model landscape. Specifically, SMRFL solves a min-max optimization problem
that promotes model convergence towards a flat minimum by minimizing the
maximum loss within a neighborhood of the model parameters. In this way, model
sensitivity to perturbations is reduced, and robustness is enhanced since
models in the neighborhood of the flat minimum also enjoy low loss values. The
theoretical result proves that SMRFL can converge at the same rate as FL
without perturbations. Extensive experimental results show that SMRFL
significantly enhances robustness against perturbations compared to three
baseline methods on two real-world datasets under three perturbation scenarios.

</details>


### [653] [Feature Attribution from First Principles](https://arxiv.org/pdf/2505.24729)
*Magamed Taimeskhanov, Damien Garreau*

Main category: cs.LG

TL;DR: The paper critiques restrictive axioms in feature attribution methods and proposes a new framework starting from simple models (indicator functions) to build attributions for complex models, recovering existing methods and deriving closed-form expressions for ReLU networks.


<details>
  <summary>Details</summary>
Motivation: Existing axiomatic frameworks for feature attribution are too restrictive, limiting practical applicability. The paper aims to develop a more flexible and grounded approach.

Method: The framework begins with attributions for simple indicator functions, using them as building blocks for complex models. It recovers existing methods and derives closed-form expressions for deep ReLU networks.

Result: The proposed framework generalizes existing attribution methods and provides closed-form solutions for ReLU networks, advancing the optimization of evaluation metrics.

Conclusion: The new framework offers a more flexible and theoretically grounded alternative to restrictive axioms, with practical implications for feature attribution in complex models.

Abstract: Feature attribution methods are a popular approach to explain the behavior of
machine learning models. They assign importance scores to each input feature,
quantifying their influence on the model's prediction. However, evaluating
these methods empirically remains a significant challenge. To bypass this
shortcoming, several prior works have proposed axiomatic frameworks that any
feature attribution method should satisfy. In this work, we argue that such
axioms are often too restrictive, and propose in response a new feature
attribution framework, built from the ground up. Rather than imposing axioms,
we start by defining attributions for the simplest possible models, i.e.,
indicator functions, and use these as building blocks for more complex models.
We then show that one recovers several existing attribution methods, depending
on the choice of atomic attribution. Subsequently, we derive closed-form
expressions for attribution of deep ReLU networks, and take a step toward the
optimization of evaluation metrics with respect to feature attributions.

</details>


### [654] [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/pdf/2505.24760)
*Zafir Stojanovski, Oliver Stanley, Joe Sharratt, Richard Jones, Abdulhakeem Adefioye, Jean Kaddour, Andreas Köpf*

Main category: cs.LG

TL;DR: Reasoning Gym (RG) is a library for reinforcement learning with verifiable rewards, offering diverse domains and infinite adjustable training data.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of fixed reasoning datasets by providing procedurally generated, scalable training data.

Method: Uses over 100 data generators and verifiers across multiple domains, enabling adjustable complexity.

Result: Demonstrates efficacy in evaluating and training reasoning models.

Conclusion: RG is a versatile tool for reinforcement learning in reasoning tasks.

Abstract: We introduce Reasoning Gym (RG), a library of reasoning environments for
reinforcement learning with verifiable rewards. It provides over 100 data
generators and verifiers spanning multiple domains including algebra,
arithmetic, computation, cognition, geometry, graph theory, logic, and various
common games. Its key innovation is the ability to generate virtually infinite
training data with adjustable complexity, unlike most previous reasoning
datasets, which are typically fixed. This procedural generation approach allows
for continuous evaluation across varying difficulty levels. Our experimental
results demonstrate the efficacy of RG in both evaluating and reinforcement
learning of reasoning models.

</details>


### [655] [Adapting to Linear Separable Subsets with Large-Margin in Differentially Private Learning](https://arxiv.org/pdf/2505.24737)
*Erchi Wang, Yuqing Zhu, Yu-Xiang Wang*

Main category: cs.LG

TL;DR: The paper presents an efficient differentially private algorithm for binary linear classification, improving results when outliers are few, without needing prior knowledge of key parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of differentially private empirical risk minimization (DP-ERM) in binary linear classification, especially when outliers are present.

Method: Develops an adaptive $\varepsilon,\delta$-DP algorithm that doesn't require knowing the margin or outlier subset, and derives bounds for empirical zero-one risk.

Result: Achieves a risk bound of $\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} + \frac{|S_{\mathrm{out}}|}{\gamma n}\right)$, improving existing results for small outlier counts.

Conclusion: The algorithm is efficient and adaptive, offering practical utility for DP-ERM in binary classification, with potential for hyperparameter tuning.

Abstract: This paper studies the problem of differentially private empirical risk
minimization (DP-ERM) for binary linear classification. We obtain an efficient
$(\varepsilon,\delta)$-DP algorithm with an empirical zero-one risk bound of
$\tilde{O}\left(\frac{1}{\gamma^2\varepsilon n} +
\frac{|S_{\mathrm{out}}|}{\gamma n}\right)$ where $n$ is the number of data
points, $S_{\mathrm{out}}$ is an arbitrary subset of data one can remove and
$\gamma$ is the margin of linear separation of the remaining data points (after
$S_{\mathrm{out}}$ is removed). Here, $\tilde{O}(\cdot)$ hides only logarithmic
terms. In the agnostic case, we improve the existing results when the number of
outliers is small. Our algorithm is highly adaptive because it does not require
knowing the margin parameter $\gamma$ or outlier subset $S_{\mathrm{out}}$. We
also derive a utility bound for the advanced private hyperparameter tuning
algorithm.

</details>


### [656] [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/pdf/2505.24749)
*Yehonathan Refael, Guy Smorodinsky, Tom Tirer, Ofir Lindenbaum*

Main category: cs.LG

TL;DR: SUMO, a subspace-aware optimizer using exact SVD for moment orthogonalization, improves convergence and reduces memory in LLM training.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank methods focus on memory efficiency but neglect convergence acceleration due to suboptimal isotropic descent in anisotropic LLM landscapes.

Method: SUMO employs exact SVD for moment orthogonalization in a dynamic low-dimensional subspace, aligning steps with loss landscape spectral characteristics.

Result: SUMO reduces memory by 20%, accelerates convergence, and outperforms state-of-the-art methods in stability and performance.

Conclusion: SUMO's exact orthogonalization via SVD enhances optimization efficiency and effectiveness for LLMs.

Abstract: Low-rank gradient-based optimization methods have significantly improved
memory efficiency during the training of large language models (LLMs), enabling
operations within constrained hardware without sacrificing performance.
However, these methods primarily emphasize memory savings, often overlooking
potential acceleration in convergence due to their reliance on standard
isotropic steepest descent techniques, which can perform suboptimally in the
highly anisotropic landscapes typical of deep networks, particularly LLMs. In
this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an
optimizer that employs exact singular value decomposition (SVD) for moment
orthogonalization within a dynamically adapted low-dimensional subspace,
enabling norm-inducing steepest descent optimization steps. By explicitly
aligning optimization steps with the spectral characteristics of the loss
landscape, SUMO effectively mitigates approximation errors associated with
commonly used methods like Newton-Schulz orthogonalization approximation. We
theoretically establish an upper bound on these approximation errors, proving
their dependence on the condition numbers of moments, conditions we
analytically demonstrate are encountered during LLM training. Furthermore, we
both theoretically and empirically illustrate that exact orthogonalization via
SVD substantially improves convergence rates while reducing overall complexity.
Empirical evaluations confirm that SUMO accelerates convergence, enhances
stability, improves performance, and reduces memory requirements by up to 20%
compared to state-of-the-art methods.

</details>


### [657] [AFLoRA: Adaptive Federated Fine-Tuning of Large Language Models with Resource-Aware Low-Rank Adaption](https://arxiv.org/pdf/2505.24773)
*Yajie Zhou, Xiaoyi Pang, Zhibo Wang*

Main category: cs.LG

TL;DR: AFLoRA is a federated fine-tuning framework for LLMs that reduces overhead, improves aggregation accuracy, and handles data heterogeneity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in federated fine-tuning of LLMs due to high computational/communication demands, client heterogeneity, and non-IID data hinder performance.

Method: AFLoRA decouples shared/client-specific updates, uses diagonal matrix-based rank pruning, and employs rank-aware aggregation with public data refinement.

Result: AFLoRA outperforms state-of-the-art methods in accuracy and efficiency.

Conclusion: AFLoRA provides a practical solution for efficient LLM adaptation in heterogeneous environments.

Abstract: Federated fine-tuning has emerged as a promising approach to adapt foundation
models to downstream tasks using decentralized data. However, real-world
deployment remains challenging due to the high computational and communication
demands of fine-tuning Large Language Models (LLMs) on clients with data and
system resources that are heterogeneous and constrained. In such settings, the
global model's performance is often bottlenecked by the weakest clients and
further degraded by the non-IID nature of local data. Although existing methods
leverage parameter-efficient techniques such as Low-Rank Adaptation (LoRA) to
reduce communication and computation overhead, they often fail to
simultaneously ensure accurate aggregation of low-rank updates and maintain low
system costs, thereby hindering overall performance. To address these
challenges, we propose AFLoRA, an adaptive and lightweight federated
fine-tuning framework for LLMs. AFLoRA decouples shared and client-specific
updates to reduce overhead and improve aggregation accuracy, incorporates
diagonal matrix-based rank pruning to better utilize local resources, and
employs rank-aware aggregation with public data refinement to strengthen
generalization under data heterogeneity. Extensive experiments demonstrate that
AFLoRA outperforms state-of-the-art methods in both accuracy and efficiency,
providing a practical solution for efficient LLM adaptation in heterogeneous
environments in the real world.

</details>


### [658] [Diffusion-Based Symbolic Regression](https://arxiv.org/pdf/2505.24776)
*Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe*

Main category: cs.LG

TL;DR: A diffusion-based approach for symbolic regression using masked diffusion and GRPO for reinforcement learning, enhanced by a risk-seeking policy.


<details>
  <summary>Details</summary>
Motivation: Leveraging diffusion models' success in generative tasks to improve symbolic regression.

Method: Random mask-based diffusion/denoising, token-wise GRPO, and long short-term risk-seeking policy.

Result: Effective generation of diverse, high-quality equations, validated by experiments.

Conclusion: The proposed method advances symbolic regression with diffusion and reinforcement learning.

Abstract: Diffusion has emerged as a powerful framework for generative modeling,
achieving remarkable success in applications such as image and audio synthesis.
Enlightened by this progress, we propose a novel diffusion-based approach for
symbolic regression. We construct a random mask-based diffusion and denoising
process to generate diverse and high-quality equations. We integrate this
generative processes with a token-wise Group Relative Policy Optimization
(GRPO) method to conduct efficient reinforcement learning on the given
measurement dataset. In addition, we introduce a long short-term risk-seeking
policy to expand the pool of top-performing candidates, further enhancing
performance. Extensive experiments and ablation studies have demonstrated the
effectiveness of our approach.

</details>


### [659] [EVA-MILP: Towards Standardized Evaluation of MILP Instance Generation](https://arxiv.org/pdf/2505.24779)
*Yidong Luo, Chenguang Wang, Jiahao Yang, Fanzeng Xia, Tianshu Yu*

Main category: cs.LG

TL;DR: The paper presents a benchmark framework for evaluating Mixed-Integer Linear Programming (MILP) instance generation methods, focusing on mathematical validity, structural similarity, computational hardness, and utility in machine learning.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized evaluation techniques for MILP instance generation methods, driven by the need for diverse optimization datasets and limitations of static benchmarks, necessitates a robust framework.

Method: The framework uses solver-independent and solver-dependent metrics, analyzing solver-internal features like root node gap, heuristic success rates, and cut plane usage to assess instance quality.

Result: The framework effectively compares the fidelity of synthetic MILP instances, demonstrating its utility with contemporary generative models.

Conclusion: The benchmark aims to standardize evaluations, improve instance generator quality, and enhance the reliability of research using synthetic MILP data.

Abstract: Mixed-Integer Linear Programming (MILP) is fundamental to solving complex
decision-making problems. The proliferation of MILP instance generation
methods, driven by machine learning's demand for diverse optimization datasets
and the limitations of static benchmarks, has significantly outpaced
standardized evaluation techniques. Consequently, assessing the fidelity and
utility of synthetic MILP instances remains a critical, multifaceted challenge.
This paper introduces a comprehensive benchmark framework designed for the
systematic and objective evaluation of MILP instance generation methods. Our
framework provides a unified and extensible methodology, assessing instance
quality across crucial dimensions: mathematical validity, structural
similarity, computational hardness, and utility in downstream machine learning
tasks. A key innovation is its in-depth analysis of solver-internal features --
particularly by comparing distributions of key solver outputs including root
node gap, heuristic success rates, and cut plane usage -- leveraging the
solver's dynamic solution behavior as an `expert assessment' to reveal nuanced
computational resemblances. By offering a structured approach with clearly
defined solver-independent and solver-dependent metrics, our benchmark aims to
facilitate robust comparisons among diverse generation techniques, spur the
development of higher-quality instance generators, and ultimately enhance the
reliability of research reliant on synthetic MILP data. The framework's
effectiveness in systematically comparing the fidelity of instance sets is
demonstrated using contemporary generative models.

</details>


### [660] [Inference Acceleration of Autoregressive Normalizing Flows by Selective Jacobi Decoding](https://arxiv.org/pdf/2505.24791)
*Jiaru Zhang, Juanwu Lu, Ziran Wang, Ruqi Zhang*

Main category: cs.LG

TL;DR: The paper proposes a selective Jacobi decoding (SeJD) strategy to accelerate autoregressive inference in normalizing flows, achieving up to 4.7x faster inference without compromising quality.


<details>
  <summary>Details</summary>
Motivation: Autoregressive modeling in normalizing flows enhances expressive power but restricts parallel computation, slowing down inference. The paper aims to address this bottleneck.

Method: The authors identify that strict sequential dependency is unnecessary for high-quality samples. They propose SeJD, a parallel iterative optimization technique, leveraging patch approximation and layer-wise dependency redundancy.

Result: Theoretical analysis shows superlinear convergence, and empirical evaluations confirm up to 4.7x faster inference while maintaining quality.

Conclusion: SeJD effectively accelerates autoregressive inference in normalizing flows, balancing speed and performance.

Abstract: Normalizing flows are promising generative models with advantages such as
theoretical rigor, analytical log-likelihood computation, and end-to-end
training. However, the architectural constraints to ensure invertibility and
tractable Jacobian computation limit their expressive power and practical
usability. Recent advancements utilize autoregressive modeling, significantly
enhancing expressive power and generation quality. However, such sequential
modeling inherently restricts parallel computation during inference, leading to
slow generation that impedes practical deployment. In this paper, we first
identify that strict sequential dependency in inference is unnecessary to
generate high-quality samples. We observe that patches in sequential modeling
can also be approximated without strictly conditioning on all preceding
patches. Moreover, the models tend to exhibit low dependency redundancy in the
initial layer and higher redundancy in subsequent layers. Leveraging these
observations, we propose a selective Jacobi decoding (SeJD) strategy that
accelerates autoregressive inference through parallel iterative optimization.
Theoretical analyses demonstrate the method's superlinear convergence rate and
guarantee that the number of iterations required is no greater than the
original sequential approach. Empirical evaluations across multiple datasets
validate the generality and effectiveness of our acceleration technique.
Experiments demonstrate substantial speed improvements up to 4.7 times faster
inference while keeping the generation quality and fidelity.

</details>


### [661] [QGAN-based data augmentation for hybrid quantum-classical neural networks](https://arxiv.org/pdf/2505.24780)
*Run-Ze He, Jun-Jian Su, Su-Juan Qin, Zheng-Ping Jin, Fei Gao*

Main category: cs.LG

TL;DR: Quantum GANs (QGANs) combined with hybrid quantum-classical neural networks (HQCNNs) outperform classical methods in data augmentation, improving accuracy and efficiency with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Data augmentation in quantum machine learning is underexplored, and QGANs address data scarcity to enhance HQCNN performance.

Method: Integrate QGANs with HQCNNs, proposing general and customized augmentation strategies. Tested on MNIST dataset.

Result: QGANs outperform classical GANs and traditional methods, achieving comparable performance with fewer parameters.

Conclusion: QGANs simplify models and generate high-quality data, advancing quantum data augmentation in machine learning.

Abstract: Quantum neural networks converge faster and achieve higher accuracy than
classical models. However, data augmentation in quantum machine learning
remains underexplored. To tackle data scarcity, we integrate quantum generative
adversarial networks (QGANs) with hybrid quantum-classical neural networks
(HQCNNs) to develop an augmentation framework. We propose two strategies: a
general approach to enhance data processing and classification across HQCNNs,
and a customized strategy that dynamically generates samples tailored to the
HQCNN's performance on specific data categories, improving its ability to learn
from complex datasets. Simulation experiments on the MNIST dataset demonstrate
that QGAN outperforms traditional data augmentation methods and classical GANs.
Compared to baseline DCGAN, QGAN achieves comparable performance with half the
parameters, balancing efficiency and effectiveness. This suggests that QGANs
can simplify models and generate high-quality data, enhancing HQCNN accuracy
and performance. These findings pave the way for applying quantum data
augmentation techniques in machine learning.

</details>


### [662] [ByzFL: Research Framework for Robust Federated Learning](https://arxiv.org/pdf/2505.24802)
*Marc González, Rachid Guerraoui, Rafael Pinot, Geovani Rizk, John Stephan, François Taïani*

Main category: cs.LG

TL;DR: ByzFL is an open-source Python library for developing and benchmarking robust federated learning algorithms, offering a unified framework with state-of-the-art tools and configurations.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized and extensible platform for reproducible research and rapid prototyping in robust federated learning.

Method: ByzFL includes robust aggregators, configurable attacks, and simulation tools for diverse FL scenarios, all configurable via a JSON file.

Result: The library supports PyTorch and NumPy, enabling systematic experimentation and visualization of results.

Conclusion: ByzFL facilitates robust FL research and is available as an open-source tool for the community.

Abstract: We present ByzFL, an open-source Python library for developing and
benchmarking robust federated learning (FL) algorithms. ByzFL provides a
unified and extensible framework that includes implementations of
state-of-the-art robust aggregators, a suite of configurable attacks, and tools
for simulating a variety of FL scenarios, including heterogeneous data
distributions, multiple training algorithms, and adversarial threat models. The
library enables systematic experimentation via a single JSON-based
configuration file and includes built-in utilities for result visualization.
Compatible with PyTorch tensors and NumPy arrays, ByzFL is designed to
facilitate reproducible research and rapid prototyping of robust FL solutions.
ByzFL is available at https://byzfl.epfl.ch/, with source code hosted on
GitHub: https://github.com/LPD-EPFL/byzfl.

</details>


### [663] [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/pdf/2505.24823)
*Yinggan Xu, Yue Liu, Zhiqiang Gao, Changnan Peng, Di Luo*

Main category: cs.LG

TL;DR: LLMs struggle with concise, principle-based reasoning in physics. PhySense benchmark reveals their limitations, guiding future AI development for interpretable scientific reasoning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack the ability to emulate human experts' concise, principle-based reasoning in physics, leading to opaque solutions.

Method: Introduces PhySense, a benchmark to test LLMs' ability to use guiding principles for physics problems.

Result: LLMs consistently fail to align with expert-like reasoning, highlighting their limitations.

Conclusion: PhySense provides insights for developing AI systems with efficient, robust, and interpretable principle-based reasoning.

Abstract: Large language models (LLMs) have rapidly advanced and are increasingly
capable of tackling complex scientific problems, including those in physics.
Despite this progress, current LLMs often fail to emulate the concise,
principle-based reasoning characteristic of human experts, instead generating
lengthy and opaque solutions. This discrepancy highlights a crucial gap in
their ability to apply core physical principles for efficient and interpretable
problem solving. To systematically investigate this limitation, we introduce
PhySense, a novel principle-based physics reasoning benchmark designed to be
easily solvable by experts using guiding principles, yet deceptively difficult
for LLMs without principle-first reasoning. Our evaluation across multiple
state-of-the-art LLMs and prompt types reveals a consistent failure to align
with expert-like reasoning paths, providing insights for developing AI systems
with efficient, robust and interpretable principle-based scientific reasoning.

</details>


### [664] [Timing is important: Risk-aware Fund Allocation based on Time-Series Forecasting](https://arxiv.org/pdf/2505.24835)
*Fuyuan Lyu, Linfeng Du, Yunpeng Weng, Qiufang Ying, Zhiyan Xu, Wen Zou, Haolun Wu, Xiuqiang He, Xing Tang*

Main category: cs.LG

TL;DR: The paper introduces RTS-PnO, a risk-aware framework for fund allocation that addresses goal mismatch and forecasting uncertainty, outperforming baselines in offline and online experiments.


<details>
  <summary>Details</summary>
Motivation: Fund allocation is critical in finance, but existing methods like prediction-only or Predict-then-Optimize suffer from goal mismatch and forecasting uncertainty.

Method: The RTS-PnO framework includes end-to-end training, adaptive uncertainty calibration, and model agnosticism, evaluated on eight financial datasets and a real-world business case.

Result: RTS-PnO outperforms baselines in offline experiments and reduces regret by 8.4% in online tests.

Conclusion: RTS-PnO effectively addresses fund allocation challenges, demonstrating superior performance in both simulated and real-world settings.

Abstract: Fund allocation has been an increasingly important problem in the financial
domain. In reality, we aim to allocate the funds to buy certain assets within a
certain future period. Naive solutions such as prediction-only or
Predict-then-Optimize approaches suffer from goal mismatch. Additionally, the
introduction of the SOTA time series forecasting model inevitably introduces
additional uncertainty in the predicted result. To solve both problems
mentioned above, we introduce a Risk-aware Time-Series Predict-and-Allocate
(RTS-PnO) framework, which holds no prior assumption on the forecasting models.
Such a framework contains three features: (i) end-to-end training with
objective alignment measurement, (ii) adaptive forecasting uncertainty
calibration, and (iii) agnostic towards forecasting models. The evaluation of
RTS-PnO is conducted over both online and offline experiments. For offline
experiments, eight datasets from three categories of financial applications are
used: Currency, Stock, and Cryptos. RTS-PnO consistently outperforms other
competitive baselines. The online experiment is conducted on the Cross-Border
Payment business at FiT, Tencent, and an 8.4\% decrease in regret is witnessed
when compared with the product-line approach. The code for the offline
experiment is available at https://github.com/fuyuanlyu/RTS-PnO.

</details>


### [665] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/pdf/2505.24842)
*Harsh Chaudhari, Jamie Hayes, Matthew Jagielski, Ilia Shumailov, Milad Nasr, Alina Oprea*

Main category: cs.LG

TL;DR: Distilled models are vulnerable to adversarial bias injection during training, with biases amplifying in student models. Current defenses fail against these attacks, necessitating specialized safeguards.


<details>
  <summary>Details</summary>
Motivation: To investigate the vulnerability of distilled models to adversarial bias injection and its amplification in student models.

Method: Demonstrated bias injection via minimal data poisoning in teacher models, leading to amplified bias in student models. Proposed two propagation modes (untargeted and targeted) and validated findings across six bias types and various distillation methods.

Result: With 0.25% poisoning rate, student models showed 76.9% biased responses in targeted scenarios (higher than teacher models). Untargeted bias appeared 6x-29x more frequently in student models. Current defenses were ineffective.

Conclusion: Distilled models have significant security vulnerabilities. Practical design principles are proposed for mitigating adversarial bias.

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [666] [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/pdf/2505.24850)
*Shuyao Xu, Cheng Peng, Jiangxuan Long, Weidi Xu, Wei Chu, Yuan Qi*

Main category: cs.LG

TL;DR: REDI, a two-stage distillation framework, leverages both positive and negative reasoning traces to enhance LLM reasoning, outperforming standard methods like DPO and SimPO.


<details>
  <summary>Details</summary>
Motivation: Standard distillation discards incorrect reasoning examples, wasting valuable data. REDI aims to utilize both positive and negative traces for better performance.

Method: REDI involves Stage 1 (SFT with positive traces) and Stage 2 (refinement with a novel REDI objective using both traces).

Result: Qwen-REDI-1.5B achieves 83.1% on MATH-500, matching or surpassing models trained on proprietary data.

Conclusion: REDI sets a new SOTA for 1.5B models using openly available data, proving the value of leveraging negative reasoning traces.

Abstract: Recent advances in model distillation demonstrate that data from advanced
reasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer
complex reasoning abilities to smaller, efficient student models. However,
standard practices employ rejection sampling, discarding incorrect reasoning
examples -- valuable, yet often underutilized data. This paper addresses the
critical question: How can both positive and negative distilled reasoning
traces be effectively leveraged to maximize LLM reasoning performance in an
offline setting? To this end, We propose Reinforcement Distillation (REDI), a
two-stage framework. Stage 1 learns from positive traces via Supervised
Fine-Tuning (SFT). Stage 2 further refines the model using both positive and
negative traces through our proposed REDI objective. This novel objective is a
simple, reference-free loss function that outperforms established methods like
DPO and SimPO in this distillation context. Our empirical evaluations
demonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT
combined with DPO/SimPO on mathematical reasoning tasks. Notably, the
Qwen-REDI-1.5B model, post-trained on just 131k positive and negative examples
from the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).
Its performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a
model post-trained on 800k proprietary data) across various mathematical
reasoning benchmarks, establishing a new state-of-the-art for 1.5B models
post-trained offline with openly available data.

</details>


### [667] [From Invariant Representations to Invariant Data: Provable Robustness to Spurious Correlations via Noisy Counterfactual Matching](https://arxiv.org/pdf/2505.24843)
*Ruqi Bai, Yao Ji, Zeyu Zhou, David I. Inouye*

Main category: cs.LG

TL;DR: The paper introduces noisy counterfactual matching (NCM) to improve model robustness by leveraging invariant data pairs, addressing spurious correlations without relying on test-time data.


<details>
  <summary>Details</summary>
Motivation: Spurious correlations degrade model performance in new environments, and existing methods like IRM underperform ERM. Test-time data solutions are impractical when such data is unavailable.

Method: Proposes NCM, a constraint-based method using invariant data pairs (counterfactual pairs) to eliminate spurious features, with theoretical guarantees for linear causal models.

Result: Theoretical bounds show test domain error depends on counterfactual diversity and quality. Empirical validation on synthetic and real-world benchmarks confirms improved robustness.

Conclusion: NCM effectively enhances robustness by leveraging invariant pairs, even with noisy or limited data, outperforming prior methods.

Abstract: Spurious correlations can cause model performance to degrade in new
environments. Prior causality-inspired works aim to learn invariant
representations (e.g., IRM) but typically underperform empirical risk
minimization (ERM). Recent alternatives improve robustness by leveraging
test-time data, but such data may be unavailable in practice. To address these
issues, we take a data-centric approach by leveraging invariant data pairs,
pairs of samples that would have the same prediction with the optimally robust
classifier. We prove that certain counterfactual pairs will naturally satisfy
this invariance property and introduce noisy counterfactual matching (NCM), a
simple constraint-based method for leveraging invariant pairs for enhanced
robustness, even with a small set of noisy pairs-in the ideal case, each pair
can eliminate one spurious feature. For linear causal models, we prove that the
test domain error can be upper bounded by the in-domain error and a term that
depends on the counterfactuals' diversity and quality. We validate on a
synthetic dataset and demonstrate on real-world benchmarks that linear probing
on a pretrained backbone improves robustness.

</details>


### [668] [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/pdf/2505.24844)
*Wanyun Xie, Francesco Tonin, Volkan Cevher*

Main category: cs.LG

TL;DR: Chameleon is a flexible data mixing framework using leverage scores to optimize domain importance in embeddings, improving performance and adaptability without costly retraining.


<details>
  <summary>Details</summary>
Motivation: Existing domain reweighting methods are computationally expensive and inflexible for new data.

Method: Uses domain affinity matrices and leverage scores to determine optimal data mixtures in an embedding space.

Result: Improves performance on pretraining domains, adapts to new data without retraining, and enhances finetuning efficiency.

Conclusion: Chameleon offers a scalable and efficient solution for domain reweighting in language models.

Abstract: Training data mixtures greatly impact the generalization performance of large
language models. Existing domain reweighting methods often rely on costly
weight computations and require retraining when new data is introduced. To this
end, we introduce a flexible and efficient data mixing framework, Chameleon,
that employs leverage scores to quantify domain importance within a learned
embedding space. We first construct a domain affinity matrix over domain
embeddings. The induced leverage scores determine a mixture that upweights
domains sharing common representations in embedding space. This formulation
allows direct transfer to new data by computing the new domain embeddings. In
experiments, we demonstrate improvements over three key scenarios: (i) our
computed weights improve performance on pretraining domains with a fraction of
the compute of existing methods; (ii) Chameleon can adapt to data changes
without proxy retraining, boosting few-shot reasoning accuracies when
transferred to new data; (iii) our method enables efficient domain reweighting
in finetuning, consistently improving test perplexity on all finetuning domains
over uniform mixture. Our code is available at
https://github.com/LIONS-EPFL/Chameleon.

</details>


### [669] [Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking](https://arxiv.org/pdf/2505.24857)
*Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, Brian Karrer*

Main category: cs.LG

TL;DR: EB-Sampler accelerates masked diffusion models (MDMs) by 2-3x without performance loss, outperforming autoregressive models (ARMs) in tasks like coding, math reasoning, maze navigation, and Sudoku.


<details>
  <summary>Details</summary>
Motivation: Efficient sampling from MDMs is underexplored, and standard procedures underutilize deterministic token predictions.

Method: Introduces EB-Sampler, an adaptive unmasking procedure with predefined error tolerance, dynamically unmasking multiple tokens per evaluation.

Result: EB-Sampler speeds up MDMs by 2-3x on coding and math benchmarks, maintaining performance, and works well on smaller reasoning tasks.

Conclusion: EB-Sampler is a simple, effective drop-in replacement for existing samplers, enhancing MDM efficiency without compromising accuracy.

Abstract: Recent masked diffusion models (MDMs) have shown competitive performance
compared to autoregressive models (ARMs) for language modeling. While most
literature has focused on performance enhancing sampling procedures, efficient
sampling from MDMs has been scarcely explored. We make the observation that
often a given sequence of partially masked tokens determines the values of
multiple unknown tokens deterministically, meaning that a single prediction of
a masked model holds additional information unused by standard sampling
procedures. Based on this observation, we introduce EB-Sampler, a simple
drop-in replacement for existing samplers, utilizing an Entropy Bounded
unmasking procedure that dynamically unmasks multiple tokens in one function
evaluation with predefined approximate error tolerance. We formulate the
EB-Sampler as part of a broad family of adaptive samplers for which we provide
an error analysis that motivates our algorithmic choices. EB-Sampler
accelerates sampling from current state of the art MDMs by roughly 2-3x on
standard coding and math reasoning benchmarks without loss in performance. We
also validate the same procedure works well on smaller reasoning tasks
including maze navigation and Sudoku, tasks ARMs often struggle with.

</details>


### [670] [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/pdf/2505.24859)
*Joschka Braun, Carsten Eickhoff, Seyed Ali Bahrainian*

Main category: cs.LG

TL;DR: Steering vectors control text properties in free-form generation, but high strengths degrade quality. Combining steering and prompting offers the best trade-off.


<details>
  <summary>Details</summary>
Motivation: To evaluate steering vectors' effectiveness beyond multiple-choice settings, focusing on free-form generation tasks like abstractive summaries.

Method: Assessed steering vectors' control over topical focus, sentiment, toxicity, and readability in summaries from the NEWTS dataset. Compared steering to prompting and their combination.

Result: Steering effectively controls properties but degrades quality at high strengths. Prompting preserves quality but offers weaker control. Combining both yields the best trade-off.

Conclusion: Steering vectors require balancing control strength and text quality, with combined steering and prompting being optimal for free-form generation.

Abstract: Steering vectors are a lightweight method for controlling text properties by
adding a learned bias to language model activations at inference time. So far,
steering vectors have predominantly been evaluated in multiple-choice settings,
while their effectiveness in free-form generation tasks remains understudied.
Moving "Beyond Multiple Choice," we thoroughly evaluate the effectiveness of
steering vectors in adaptively controlling topical focus, sentiment, toxicity,
and readability in abstractive summaries of the NEWTS dataset. We find that
steering effectively controls the targeted summary properties, but high
steering strengths consistently degrade both intrinsic and extrinsic text
quality. Compared to steering, prompting offers weaker control, while
preserving text quality. Combining steering and prompting yields the strongest
control over text properties and offers the most favorable efficacy-quality
trade-off at moderate steering strengths. Our results underscore the practical
trade-off between control strength and text quality preservation when applying
steering vectors to free-form generation tasks.

</details>


### [671] [The Road to Generalizable Neuro-Symbolic Learning Should be Paved with Foundation Models](https://arxiv.org/pdf/2505.24874)
*Adam Stein, Aaditya Naik, Neelay Velingker, Mayur Naik, Eric Wong*

Main category: cs.LG

TL;DR: Neuro-symbolic prompting combines foundation models with symbolic programs to address the limitations of traditional neuro-symbolic learning and purely-neural models.


<details>
  <summary>Details</summary>
Motivation: To explore the role of specialized model training in neuro-symbolic learning in the era of foundation models, addressing pitfalls like compute, data, and program limitations.

Method: Proposes neuro-symbolic prompting, supplementing foundation models with symbolic programs for complex reasoning tasks.

Result: Argues that foundation models enable generalizable neuro-symbolic solutions, avoiding downsides of training from scratch.

Conclusion: Foundation models offer a viable path to achieve neuro-symbolic learning goals without traditional training challenges.

Abstract: Neuro-symbolic learning was proposed to address challenges with training
neural networks for complex reasoning tasks with the added benefits of
interpretability, reliability, and efficiency. Neuro-symbolic learning methods
traditionally train neural models in conjunction with symbolic programs, but
they face significant challenges that limit them to simplistic problems. On the
other hand, purely-neural foundation models now reach state-of-the-art
performance through prompting rather than training, but they are often
unreliable and lack interpretability. Supplementing foundation models with
symbolic programs, which we call neuro-symbolic prompting, provides a way to
use these models for complex reasoning tasks. Doing so raises the question:
What role does specialized model training as part of neuro-symbolic learning
have in the age of foundation models? To explore this question, we highlight
three pitfalls of traditional neuro-symbolic learning with respect to the
compute, data, and programs leading to generalization problems. This position
paper argues that foundation models enable generalizable neuro-symbolic
solutions, offering a path towards achieving the original goals of
neuro-symbolic learning without the downsides of training from scratch.

</details>


### [672] [On the Design Fundamentals of Diffusion Models: A Survey](https://arxiv.org/pdf/2306.04542)
*Ziyi Chang, George Alex Koulieris, Hyung Jin Chang, Hubert P. H. Shum*

Main category: cs.LG

TL;DR: A review of design fundamentals in diffusion models' components (forward, reverse, sampling processes), filling a gap in existing literature.


<details>
  <summary>Details</summary>
Motivation: Existing reviews focus on high-level solutions, lacking detail on component design fundamentals.

Method: Comprehensive review of seminal designable factors in each functional component of diffusion models.

Result: Provides a finer-grained perspective on diffusion models, aiding future component analysis, design, and implementation.

Conclusion: The study enhances understanding of diffusion models' design fundamentals, supporting future research and applications.

Abstract: Diffusion models are learning pattern-learning systems to model and sample
from data distributions with three functional components namely the forward
process, the reverse process, and the sampling process. The components of
diffusion models have gained significant attention with many design factors
being considered in common practice. Existing reviews have primarily focused on
higher-level solutions, covering less on the design fundamentals of components.
This study seeks to address this gap by providing a comprehensive and coherent
review of seminal designable factors within each functional component of
diffusion models. This provides a finer-grained perspective of diffusion
models, benefiting future studies in the analysis of individual components, the
design factors for different purposes, and the implementation of diffusion
models.

</details>


### [673] [RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark](https://arxiv.org/pdf/2306.17100)
*Federico Berto, Chuanbo Hua, Junyoung Park, Laurin Luttmann, Yining Ma, Fanchen Bu, Jiarui Wang, Haoran Ye, Minsu Kim, Sanghyeok Choi, Nayeli Gast Zepeda, André Hottung, Jianan Zhou, Jieyi Bi, Yu Hu, Fei Liu, Hyeonah Kim, Jiwoo Son, Haeyeon Kim, Davide Angioni, Wouter Kool, Zhiguang Cao, Qingfu Zhang, Joungho Kim, Jie Zhang, Kijung Shin, Cathy Wu, Sungsoo Ahn, Guojie Song, Changhyun Kwon, Kevin Tierney, Lin Xie, Jinkyoo Park*

Main category: cs.LG

TL;DR: RL4CO is a unified benchmark for deep reinforcement learning in combinatorial optimization, offering 27 problem environments and 23 baselines to improve reproducibility and reduce engineering overhead.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified benchmarking framework in combinatorial optimization (CO) with deep RL leads to inconsistent evaluations and high barriers for new researchers.

Method: RL4CO provides modularized implementations, flexible configurations, and extensive documentation for environments, policies, RL algorithms, and utilities.

Result: RL4CO facilitates research by decoupling science from engineering, attracting community interest and being open-sourced.

Conclusion: RL4CO aims to inspire future work and insights in CO with deep RL, promoting reproducibility and adoption.

Abstract: Combinatorial optimization (CO) is fundamental to several real-world
applications, from logistics and scheduling to hardware design and resource
allocation. Deep reinforcement learning (RL) has recently shown significant
benefits in solving CO problems, reducing reliance on domain expertise and
improving computational efficiency. However, the absence of a unified
benchmarking framework leads to inconsistent evaluations, limits
reproducibility, and increases engineering overhead, raising barriers to
adoption for new researchers. To address these challenges, we introduce RL4CO,
a unified and extensive benchmark with in-depth library coverage of 27 CO
problem environments and 23 state-of-the-art baselines. Built on efficient
software libraries and best practices in implementation, RL4CO features
modularized implementation and flexible configurations of diverse environments,
policy architectures, RL algorithms, and utilities with extensive
documentation. RL4CO helps researchers build on existing successes while
exploring and developing their own designs, facilitating the entire research
process by decoupling science from heavy engineering. We finally provide
extensive benchmark studies to inspire new insights and future work. RL4CO has
already attracted numerous researchers in the community and is open-sourced at
https://github.com/ai4co/rl4co.

</details>


### [674] [Model Extrapolation Expedites Alignment](https://arxiv.org/pdf/2404.16792)
*Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, Nanyun Peng*

Main category: cs.LG

TL;DR: ExPO (model extrapolation) reduces training overhead for LLM alignment by amplifying parameter changes from a partially-trained model, achieving comparable or better performance with fewer steps.


<details>
  <summary>Details</summary>
Motivation: High computational cost of preference alignment training for LLMs and the observation that alignment involves small parameter changes without new knowledge.

Method: ExPO amplifies parameter changes based on a first-order approximation, using a partially-trained model and its initial SFT checkpoint, without extra training overhead.

Result: ExPO boosts a DPO model trained with 20% steps to outperform the fully-trained one and improves open-source LLMs on AlpacaEval 2.0 and MT-Bench benchmarks.

Conclusion: ExPO is an efficient method for enhancing LLM alignment, demonstrating broader utility across model sizes.

Abstract: Given the high computational cost of preference alignment training of large
language models (LLMs), exploring efficient methods to reduce the training
overhead remains an important and compelling research problem. Motivated by the
observation that alignment training typically involves only small parameter
changes without injecting new knowledge into models, we propose a
straightforward method called ExPO (model extrapolation) to expedite LLMs'
alignment with human preferences. Given a partially-trained model and its
initial SFT checkpoint, ExPO improves the implicit optimization objective of
alignment training by simply amplifying the parameter change based on a
first-order approximation, without any additional training overhead. Through
controlled experiments, we demonstrate that ExPO boosts a DPO model trained
with only 20% steps to outperform the fully-trained one. Moreover, we show that
ExPO notably improves existing open-source LLMs (ranging from 1.8B to 70B
parameters) on the leading AlpacaEval 2.0 and MT-Bench benchmarks, which
highlights ExPO's broader utility in efficiently enhancing LLM alignment.

</details>


### [675] [Complexity-Aware Deep Symbolic Regression with Robust Risk-Seeking Policy Gradients](https://arxiv.org/pdf/2406.06751)
*Zachary Bastiani, Robert M. Kirby, Jacob Hochhalter, Shandian Zhe*

Main category: cs.LG

TL;DR: A novel deep symbolic regression method improves robustness and interpretability by using a decoder-only architecture, BIC-based rewards, and ranking-based updates.


<details>
  <summary>Details</summary>
Motivation: Existing DSR methods face inefficiencies due to tail barriers and lack of balance between expression complexity and data fitness.

Method: Uses a decoder-only architecture with frequency-domain attention, dual-indexed position encoding, BIC-based rewards, and ranking-based policy updates.

Result: Demonstrates improved performance and robustness in benchmarks.

Conclusion: The proposed approach effectively overcomes limitations of existing DSR methods.

Abstract: We propose a novel deep symbolic regression approach to enhance the
robustness and interpretability of data-driven mathematical expression
discovery. Our work is aligned with the popular DSR framework which focuses on
learning a data-specific expression generator, without relying on pretrained
models or additional search or planning procedures. Despite the success of
existing DSR methods, they are built on recurrent neural networks, solely
guided by data fitness, and potentially meet tail barriers that can zero out
the policy gradient, causing inefficient model updates. To overcome these
limitations, we design a decoder-only architecture that performs attention in
the frequency domain and introduce a dual-indexed position encoding to conduct
layer-wise generation. Second, we propose a Bayesian information criterion
(BIC)-based reward function that can automatically adjust the trade-off between
expression complexity and data fitness, without the need for explicit manual
tuning. Third, we develop a ranking-based weighted policy update method that
eliminates the tail barriers and enhances training effectiveness. Extensive
benchmarks and systematic experiments demonstrate the advantages of our
approach.

</details>


### [676] [Kolmogorov-Arnold Graph Neural Networks](https://arxiv.org/pdf/2406.18354)
*Gianluca De Carlo, Andrea Mastropietro, Aris Anagnostopoulos*

Main category: cs.LG

TL;DR: GKAN is a new GNN model using spline-based activation for better accuracy and interpretability, outperforming existing models in node classification, link prediction, and graph classification.


<details>
  <summary>Details</summary>
Motivation: GNNs lack interpretability, limiting their use in domains requiring transparent decision-making.

Method: GKAN employs spline-based activation functions on edges to enhance accuracy and interpretability.

Result: GKAN outperforms state-of-the-art GNN models in benchmark tasks and provides inherent interpretability.

Conclusion: GKAN offers improved accuracy and transparency, making it suitable for interpretability-critical applications.

Abstract: Graph neural networks (GNNs) excel in learning from network-like data but
often lack interpretability, making their application challenging in domains
requiring transparent decision-making. We propose the Graph Kolmogorov-Arnold
Network (GKAN), a novel GNN model leveraging spline-based activation functions
on edges to enhance both accuracy and interpretability. Our experiments on five
benchmark datasets demonstrate that GKAN outperforms state-of-the-art GNN
models in node classification, link prediction, and graph classification tasks.
In addition to the improved accuracy, GKAN's design inherently provides clear
insights into the model's decision-making process, eliminating the need for
post-hoc explainability techniques. This paper discusses the methodology,
performance, and interpretability of GKAN, highlighting its potential for
applications in domains where interpretability is crucial.

</details>


### [677] [Planning in a recurrent neural network that plays Sokoban](https://arxiv.org/pdf/2407.15421)
*Mohammad Taufeeque, Philip Quirke, Maximilian Li, Chris Cundy, Aaron David Tucker, Adam Gleave, Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: An RNN trained on Sokoban exhibits causal plan representations, predicting actions 50 steps ahead, with pacing behavior for extra computation. It generalizes to larger puzzles.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms of planning in neural networks, using Sokoban as a testbed.

Method: Analyze an RNN trained on Sokoban, focusing on its plan representation and pacing behavior.

Result: The RNN predicts actions 50 steps ahead, paces for extra computation, and generalizes to larger puzzles.

Conclusion: The RNN's behavior offers insights into learned planning and serves as a model organism for further study.

Abstract: Planning is essential for solving complex tasks, yet the internal mechanisms
underlying planning in neural networks remain poorly understood. Building on
prior work, we analyze a recurrent neural network (RNN) trained on Sokoban, a
challenging puzzle requiring sequential, irreversible decisions. We find that
the RNN has a causal plan representation which predicts its future actions
about 50 steps in advance. The quality and length of the represented plan
increases over the first few steps. We uncover a surprising behavior: the RNN
"paces" in cycles to give itself extra computation at the start of a level, and
show that this behavior is incentivized by training. Leveraging these insights,
we extend the trained RNN to significantly larger, out-of-distribution Sokoban
puzzles, demonstrating robust representations beyond the training regime. We
open-source our model and code, and believe the neural network's interesting
behavior makes it an excellent model organism to deepen our understanding of
learned planning.

</details>


### [678] [Optimizing Connectivity through Network Gradients for Restricted Boltzmann Machines](https://arxiv.org/pdf/2209.06932)
*A. C. N. de Oliveira, D. R. Figueiredo*

Main category: cs.LG

TL;DR: NCG optimizes connectivity patterns in shallow networks like RBMs using gradients, improving performance without altering the energy-based objective.


<details>
  <summary>Details</summary>
Motivation: Sparse connectivity impacts shallow networks' learning, but current methods rely on penalization or hyperparameters. NCG aims to optimize connections directly.

Method: NCG uses network gradients to update connection strengths, jointly learning RBM parameters and connectivity patterns with different learning rates.

Result: NCG improves RBM performance on MNIST for generation and classification, is robust to initialization, and dynamically adjusts connections.

Conclusion: NCG effectively optimizes network connectivity for RBMs, enhancing performance without modifying the core objective function.

Abstract: Leveraging sparse networks to connect successive layers in deep neural
networks has recently been shown to provide benefits to large-scale
state-of-the-art models. However, network connectivity also plays a significant
role in the learning performance of shallow networks, such as the classic
Restricted Boltzmann Machine (RBM). Efficiently finding sparse connectivity
patterns that improve the learning performance of shallow networks is a
fundamental problem. While recent principled approaches explicitly include
network connections as model parameters that must be optimized, they often rely
on explicit penalization or network sparsity as a hyperparameter. This work
presents the Network Connectivity Gradients (NCG), an optimization method to
find optimal connectivity patterns for RBMs. NCG leverages the idea of network
gradients: given a specific connection pattern, it determines the gradient of
every possible connection and uses the gradient to drive a continuous
connection strength parameter that in turn is used to determine the connection
pattern. Thus, learning RBM parameters and learning network connections is
truly jointly performed, albeit with different learning rates, and without
changes to the model's classic energy-based objective function. The proposed
method is applied to the MNIST and other data sets showing that better RBM
models are found for the benchmark tasks of sample generation and
classification. Results also show that NCG is robust to network initialization
and is capable of both adding and removing network connections while learning.

</details>


### [679] [Principled Understanding of Generalization for Generative Transformer Models in Arithmetic Reasoning Tasks](https://arxiv.org/pdf/2407.17963)
*Xingcheng Xu, Zibo Zhao, Haipeng Zhang, Yanqing Yang*

Main category: cs.LG

TL;DR: The paper investigates transformer generalization in arithmetic tasks, revealing how translation invariance and base mismatch affect performance, validated by experiments on GPT models.


<details>
  <summary>Details</summary>
Motivation: To understand the inconsistent generalization of transformers in arithmetic tasks, such as multiplication and modular addition, by developing a theoretical framework.

Method: Develops a unified theoretical framework analyzing addition, multiplication, and modular operations, focusing on length generalization. Experiments validate the framework using GPT-family models.

Result: Translation invariance in addition aligns with relative positional encoding for robust generalization, while base mismatch in modular operations disrupts it. The framework predicts generalization behaviors accurately.

Conclusion: Task structure and training data distribution are crucial for data-efficient training, offering a systematic approach to understanding length generalization in transformers.

Abstract: Transformer-based models excel in various tasks but their generalization
capabilities, especially in arithmetic reasoning, remain incompletely
understood. Arithmetic tasks provide a controlled framework to explore these
capabilities, yet performance anomalies persist, such as inconsistent
effectiveness in multiplication and erratic generalization in modular addition
(e.g., modulo 100 vs. 101). This paper develops a unified theoretical framework
for understanding the generalization behaviors of transformers in arithmetic
tasks, focusing on length generalization. Through detailed analysis of
addition, multiplication, and modular operations, we reveal that translation
invariance in addition aligns with relative positional encoding for robust
generalization, while base mismatch in modular operations disrupts this
alignment. Experiments across GPT-family models validate our framework,
confirming its ability to predict generalization behaviors. Our work highlights
the importance of task structure and training data distribution for achieving
data-efficient and structure-aware training, providing a systematic approach to
understanding of length generalization in transformers.

</details>


### [680] [Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning](https://arxiv.org/pdf/2303.14537)
*Rickard Brüel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin Solomon*

Main category: cs.LG

TL;DR: Dropout's effectiveness as data augmentation is explored, revealing it works best in deeper layers and can be matched by alternatives like PCA. A stop-gradient operation is critical, and performance trends differ between contrastive and supervised tasks.


<details>
  <summary>Details</summary>
Motivation: To understand when and why dropout is effective as an augmentation strategy and whether it is uniquely beneficial under certain conditions.

Method: Proposes Deep Augmentation, a method applying dropout or PCA to targeted neural network layers, tested on contrastive learning tasks across NLP, computer vision, and graph learning.

Result: Dropout is most effective in deeper layers and can be replaced by alternatives like PCA. A stop-gradient operation is crucial, and performance trends invert between contrastive and supervised tasks.

Conclusion: Deep Augmentation mitigates inter-layer co-adaptation, outperforms traditional input-level augmentations, and is adaptable across architectures and modalities, enhancing performance and generalization.

Abstract: Despite dropout's ubiquity in machine learning, its effectiveness as a form
of data augmentation remains under-explored. We address two key questions: (i)
When is dropout effective as an augmentation strategy? (ii) Is dropout uniquely
effective under these conditions? To explore these questions, we propose Deep
Augmentation, a network- and modality-agnostic method that applies dropout or
PCA transformations to targeted layers in neural networks. Through extensive
experiments on contrastive learning tasks in NLP, computer vision, and graph
learning, we find that uniformly applying dropout across layers does not
consistently improve performance. Instead, dropout proves most beneficial in
deeper layers and can be matched by alternative augmentations (e.g., PCA). We
also show that a stop-gradient operation is critical for ensuring dropout
functions effectively as an augmentation, and that performance trends invert
when moving from contrastive tasks to supervised tasks. Our analysis suggests
that Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable
issue in self-supervised learning due to the absence of labeled data. Drawing
on these insights, we outline a procedure for selecting the optimal
augmentation layer and demonstrate that Deep Augmentation can outperform
traditional input-level augmentations. This simple yet powerful approach can be
seamlessly integrated into a wide range of architectures and modalities,
yielding notable gains in both performance and generalization.

</details>


### [681] [Beyond KAN: Introducing KarSein for Adaptive High-Order Feature Interaction Modeling in CTR Prediction](https://arxiv.org/pdf/2408.08713)
*Yunxiao Shi, Wujiang Xu, Haimin Zhang, Qiang Wu, Min Xu*

Main category: cs.LG

TL;DR: KarSein introduces an efficient, adaptive method for high-order feature interactions in CTR prediction, reducing computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational inefficiency and the need for complex high-order feature interactions in CTR prediction.

Method: Leverages the Kolmogorov-Arnold Network's learnable activation mechanism to adaptively transform low-order features into high-order interactions, with an efficient architecture for reduced costs.

Result: Outperforms baseline methods and vanilla KAN in CTR prediction, achieving high accuracy with compact parameters and low computational overhead.

Conclusion: KarSein is a transformative solution for CTR prediction, efficiently modeling complex feature interactions.

Abstract: Modeling high-order feature interactions is crucial for click-through rate
(CTR) prediction, and traditional approaches often predefine a maximum
interaction order and rely on exhaustive enumeration of feature combinations up
to this predefined order. This framework heavily relies on prior domain
knowledge to define interaction scope and entails high computational costs from
enumeration. Conventional CTR models face a trade-off between improving
representation through complex high-order feature interactions and reducing
computational inefficiencies associated with these processes. To address this
dual challenge, this study introduces the Kolmogorov-Arnold Represented Sparse
Efficient Interaction Network (KarSein). Drawing inspiration from the learnable
activation mechanism in the Kolmogorov-Arnold Network (KAN), KarSein leverages
this mechanism to adaptively transform low-order basic features into high-order
feature interactions, offering a novel approach to feature interaction
modeling. KarSein extends the capabilities of KAN by introducing a more
efficient architecture that significantly reduces computational costs while
accommodating two-dimensional embedding vectors as feature inputs. Furthermore,
it overcomes the limitation of KAN's its inability to spontaneously capture
multiplicative relationships among features.
  Extensive experiments highlight the superiority of KarSein, demonstrating its
ability to surpass not only the vanilla implementation of KAN in CTR predictio
but also other baseline methods. Remarkably, KarSein achieves exceptional
predictive accuracy while maintaining a highly compact parameter size and
minimal computational overhead. As the first attempt to apply KAN in the CTR
domain, this work introduces KarSein as a novel solution for modeling complex
feature interactions, underscoring its transformative potential in advancing
CTR prediction task.

</details>


### [682] [Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers and Gradient Clipping](https://arxiv.org/pdf/2310.00098)
*Martin Pelikan, Sheikh Shams Azam, Vitaly Feldman, Jan "Honza" Silovsky, Kunal Talwar, Christopher G. Brinton, Tatiana Likhomanenko*

Main category: cs.LG

TL;DR: The paper introduces the first benchmark for federated learning (FL) with differential privacy (DP) in end-to-end automatic speech recognition (ASR), addressing challenges like gradient heterogeneity in large transformer models.


<details>
  <summary>Details</summary>
Motivation: The gap in applying FL and DP to ASR due to challenges in training large models, particularly gradient heterogeneity, motivates this work.

Method: The approach uses per-layer clipping and layer-wise gradient normalization to mitigate clipping bias and gradient heterogeneity.

Result: Empirical results show FL with DP is viable under strong privacy guarantees, with minimal performance drop in word error rate.

Conclusion: The principles extend beyond ASR, offering guidance for scalable, privacy-preserving FL algorithms for large models.

Abstract: While federated learning (FL) and differential privacy (DP) have been
extensively studied, their application to automatic speech recognition (ASR)
remains largely unexplored due to the challenges in training large transformer
models. Specifically, large models further exacerbate issues in FL as they are
particularly susceptible to gradient heterogeneity across layers, unlike the
relatively uniform gradient behavior observed in shallow models. As a result,
prior works struggle to converge with standard optimization techniques, even in
the absence of DP mechanisms. To the best of our knowledge, no existing work
establishes a competitive, practical recipe for FL with DP in the context of
ASR. To address this gap, we establish \textbf{the first benchmark for FL with
DP in end-to-end ASR}. Our approach centers on per-layer clipping and
layer-wise gradient normalization: theoretical analysis reveals that these
techniques together mitigate clipping bias and gradient heterogeneity across
layers in deeper models. Consistent with these theoretical insights, our
empirical results show that FL with DP is viable under strong privacy
guarantees, provided a population of at least several million users.
Specifically, we achieve user-level (7.2, $10^{-9}$)-DP (resp. (4.5,
$10^{-9}$)-DP) with only a 1.3% (resp. 4.6%) absolute drop in word error rate
when extrapolating to high (resp. low) population scales for FL with DP in ASR.
Although our experiments focus on ASR, the underlying principles we uncover -
particularly those concerning gradient heterogeneity and layer-wise gradient
normalization - offer broader guidance for designing scalable,
privacy-preserving FL algorithms for large models across domains.

</details>


### [683] [LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs](https://arxiv.org/pdf/2408.13467)
*Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jing Tang*

Main category: cs.LG

TL;DR: LlamaDuo is an LLMOps pipeline for transferring knowledge from large, service-oriented LLMs to smaller, locally manageable models, addressing privacy, connectivity, and dependency issues.


<details>
  <summary>Details</summary>
Motivation: To mitigate challenges like operational dependencies, privacy concerns, and internet connectivity requirements in cloud-based LLMs.

Method: Fine-tuning a small model using synthetic data from the service LLM, with iterative improvements if performance is insufficient.

Result: The smaller model matches or surpasses the service LLM in specific tasks, offering a scalable solution for constrained environments.

Conclusion: LlamaDuo provides an effective, adaptable, and affordable approach for deploying AI in resource-limited settings.

Abstract: The widespread adoption of cloud-based proprietary large language models
(LLMs) has introduced significant challenges, including operational
dependencies, privacy concerns, and the necessity of continuous internet
connectivity. In this work, we introduce an LLMOps pipeline, "LlamaDuo", for
the seamless migration of knowledge and abilities from service-oriented LLMs to
smaller, locally manageable models. This pipeline is crucial for ensuring
service continuity in the presence of operational failures, strict privacy
policies, or offline requirements. Our LlamaDuo involves fine-tuning a small
language model against the service LLM using a synthetic dataset generated by
the latter. If the performance of the fine-tuned model falls short of
expectations, it is automatically improved through additional fine-tuning using
extra similar data generated by the service LLM. This multi-turn process
guarantees that the smaller model can eventually match or even surpass the
service LLM's capabilities in specific downstream tasks, offering a practical
and scalable solution for managing AI deployments in constrained environments.
Extensive experiments with leading-edge LLMs are conducted to demonstrate the
effectiveness, adaptability, and affordability of LlamaDuo across various
downstream tasks. Our pipeline implementation is available at
https://github.com/deep-diver/llamaduo.

</details>


### [684] [CCNETS: A Modular Causal Learning Framework for Pattern Recognition in Imbalanced Datasets](https://arxiv.org/pdf/2401.04139)
*Hanbeot Park, Yunjeong Cho, Hoon-Hee Kim*

Main category: cs.LG

TL;DR: CCNETS is a modular framework integrating generation, inference, and reconstruction to address class imbalance, outperforming baselines in fraud detection.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in tasks like fraud detection limits traditional generative models due to distribution mismatch.

Method: CCNETS uses three cooperative modules (Explainer, Reasoner, Producer) with a causal feedback loop and Zoint mechanism for adaptive feature fusion.

Result: CCNETS achieves higher F1 scores, precision, and recall on a credit card fraud dataset (<0.2% fraud cases).

Conclusion: CCNETS offers a scalable, interpretable solution for imbalanced learning, improving generalization and robustness.

Abstract: Handling class imbalance remains a central challenge in machine learning,
particularly in pattern recognition tasks where rare but critical events-such
as fraudulent transactions or medical anomalies-must be identified accurately.
Traditional generative models offer a potential remedy through data
augmentation but often treat generation and classification as independent
processes, leading to distribution mismatch and limited classifier benefit. To
address these shortcomings, we propose Causal Cooperative Networks (CCNETS), a
modular learning framework that integrates generation, inference, and
reconstruction within a unified causal paradigm. CCNETS comprises three
cooperative modules: an Explainer for latent feature abstraction, a Reasoner
for label prediction, and a Producer for context-aware data generation. These
components interact through a causal feedback loop, where classification
results guide targeted sample synthesis. A key innovation, the Zoint mechanism,
enables adaptive fusion of latent and observable features, enhancing semantic
richness and enabling robust decision-making under uncertainty. We evaluate
CCNETS on a real-world credit card fraud detection dataset with extreme
imbalance (fraud cases < 0.2%). Across three experimental setups-including
synthetic training, amplified generation, and direct classifier
comparison-CCNETS outperforms baseline methods, achieving higher F1 scores,
precision, and recall. Models trained on CCNETS-generated data also demonstrate
superior generalization under limited data conditions. These results establish
CCNETS as a scalable, interpretable, and hybrid soft computing framework. By
causally aligning synthetic data with classifier objectives, CCNETS advances
imbalanced pattern recognition and opens new directions for robust, modular
learning in real-world applications.

</details>


### [685] [Cross Entropy versus Label Smoothing: A Neural Collapse Perspective](https://arxiv.org/pdf/2402.03979)
*Li Guo, George Andriopoulos, Zifan Zhao, Shuyang Ling, Zixuan Dong, Keith Ross*

Main category: cs.LG

TL;DR: Label smoothing loss enhances neural collapse (NC) convergence and performance in deep neural networks, offering faster training and better model calibration.


<details>
  <summary>Details</summary>
Motivation: To understand label smoothing's impact on neural collapse and its benefits over cross-entropy loss.

Method: Empirical analysis of NC convergence and theoretical derivation of global minimizers using the unconstrained feature model.

Result: Label smoothing accelerates NC convergence, intensifies NC2, and improves conditioning for faster theoretical convergence.

Conclusion: Label smoothing provides performance and calibration benefits, demonstrating NC framework's utility in DNN analysis.

Abstract: Label smoothing loss is a widely adopted technique to mitigate overfitting in
deep neural networks. This paper studies label smoothing from the perspective
of Neural Collapse (NC), a powerful empirical and theoretical framework which
characterizes model behavior during the terminal phase of training. We first
show empirically that models trained with label smoothing converge faster to
neural collapse solutions and attain a stronger level of neural collapse.
Additionally, we show that at the same level of NC1, models under label
smoothing loss exhibit intensified NC2. These findings provide valuable
insights into the performance benefits and enhanced model calibration under
label smoothing loss. We then leverage the unconstrained feature model to
derive closed-form solutions for the global minimizers for both loss functions
and further demonstrate that models under label smoothing have a lower
conditioning number and, therefore, theoretically converge faster. Our study,
combining empirical evidence and theoretical results, not only provides nuanced
insights into the differences between label smoothing and cross-entropy losses,
but also serves as an example of how the powerful neural collapse framework can
be used to improve our understanding of DNNs.

</details>


### [686] [A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications](https://arxiv.org/pdf/2403.00485)
*Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, Hongteng Xu, Zhewei Wei, Deli Zhao, Yang Liu, Yu Rong, Wenbing Huang*

Main category: cs.LG

TL;DR: A survey on geometric GNNs, addressing their need for handling geometric graphs with symmetries, summarizing models, applications, datasets, and future directions.


<details>
  <summary>Details</summary>
Motivation: Current GNNs struggle with geometric graphs due to their physical symmetries, prompting the need for specialized geometric GNNs.

Method: Formalizes geometric graphs, provides a unified view of models via geometric message passing, and summarizes applications and datasets.

Result: Comprehensive survey of geometric GNNs, including data structures, models, applications, and challenges.

Conclusion: Highlights the potential of geometric GNNs and outlines future research directions.

Abstract: Geometric graphs are a special kind of graph with geometric features, which
are vital to model many scientific problems. Unlike generic graphs, geometric
graphs often exhibit physical symmetries of translations, rotations, and
reflections, making them ineffectively processed by current Graph Neural
Networks (GNNs). To address this issue, researchers proposed a variety of
geometric GNNs equipped with invariant/equivariant properties to better
characterize the geometry and topology of geometric graphs. Given the current
progress in this field, it is imperative to conduct a comprehensive survey of
data structures, models, and applications related to geometric GNNs. In this
paper, based on the necessary but concise mathematical preliminaries, we
formalize geometric graph as the data structure, on top of which we provide a
unified view of existing models from the geometric message passing perspective.
Additionally, we summarize the applications as well as the related datasets to
facilitate later research for methodology development and experimental
evaluation. We also discuss the challenges and future potential directions of
geometric GNNs at the end of this survey.

</details>


### [687] [Accelerating the Evolution of Personalized Automated Lane Change through Lesson Learning](https://arxiv.org/pdf/2405.07543)
*Jia Hu, Mingyue Lei, Haoran Wang, Zeyu Liu, Fan Yang*

Main category: cs.LG

TL;DR: A lesson learning approach for personalizing driver assistance systems uses takeover interventions to improve evolution efficiency and perceived safety.


<details>
  <summary>Details</summary>
Motivation: Personalization is key for driver assistance systems, but conventional methods are resource-intensive and offline. This paper addresses the need for online evolution.

Method: The approach learns from driver takeovers, uses Gaussian discriminant analysis for safety zones, and employs apprenticeship learning and model predictive control for trajectory planning.

Result: Simulations show successful customization, 24% efficiency improvement, 13.8 average learning iterations, and 0.08s computation time.

Conclusion: The framework is efficient, safe, and computationally feasible, enabling real-time personalization.

Abstract: Personalization is crucial for the widespread adoption of advanced driver
assistance system. To match up with each user's preference, the online
evolution capability is a must. However, conventional evolution methods learn
from naturalistic driving data, which requires a lot computing power and cannot
be applied online. To address this challenge, this paper proposes a lesson
learning approach: learning from driver's takeover interventions. By leveraging
online takeover data, the driving zone is generated to ensure perceived safety
using Gaussian discriminant analysis. Real-time corrections to trajectory
planning rewards are enacted through apprenticeship learning. Guided by the
objective of optimizing rewards within the constraints of the driving zone,
this approach employs model predictive control for trajectory planning. This
lesson learning framework is highlighted for its faster evolution capability,
adeptness at experience accumulating, assurance of perceived safety, and
computational efficiency. Simulation results demonstrate that the proposed
system consistently achieves a successful customization without further
takeover interventions. Accumulated experience yields a 24% enhancement in
evolution efficiency. The average number of learning iterations is only 13.8.
The average computation time is 0.08 seconds.

</details>


### [688] [DispaRisk: Auditing Fairness Through Usable Information](https://arxiv.org/pdf/2405.12372)
*Jonathan Vasquez, Carlotta Domeniconi, Huzefa Rangwala*

Main category: cs.LG

TL;DR: DispaRisk is a framework for early bias detection in ML pipelines using information theory, tested on fairness datasets to identify high-risk biases and improve explainability.


<details>
  <summary>Details</summary>
Motivation: ML algorithms often amplify societal biases, harming minority groups. Early bias detection is crucial to mitigate these effects.

Method: Leverages usable information theory to create DispaRisk, a proactive framework for assessing disparity risks in datasets.

Result: DispaRisk effectively identifies high-risk datasets, bias-prone model families, and enhances bias risk explainability.

Conclusion: DispaRisk contributes to fairer ML systems by enabling early bias detection and mitigation.

Abstract: Machine Learning algorithms (ML) impact virtually every aspect of human lives
and have found use across diverse sectors including healthcare, finance, and
education. Often, ML algorithms have been found to exacerbate societal biases
present in datasets leading to adversarial impacts on subsets/groups of
individuals and in many cases on minority groups. To effectively mitigate these
untoward effects, it is crucial that disparities/biases are identified early in
a ML pipeline. This proactive approach facilitates timely interventions to
prevent bias amplification and reduce complexity at later stages of model
development. In this paper, we leverage recent advancements in usable
information theory to introduce DispaRisk, a novel framework designed to
proactively assess the potential risks of disparities in datasets during the
initial stages of the ML pipeline. We evaluate DispaRisk's effectiveness by
benchmarking it against commonly used datasets in fairness research. Our
findings demonstrate DispaRisk's capabilities to identify datasets with a high
risk of discrimination, detect model families prone to biases within an ML
pipeline, and enhance the explainability of these bias risks. This work
contributes to the development of fairer ML systems by providing a robust tool
for early bias detection and mitigation.

</details>


### [689] [Identifying biological perturbation targets through causal differential networks](https://arxiv.org/pdf/2410.03380)
*Menghua Wu, Umesh Padia, Sean H. Murphy, Regina Barzilay, Tommi Jaakkola*

Main category: cs.LG

TL;DR: A causality-inspired method identifies intervention targets in biological systems by comparing inferred causal graphs from observational and interventional data, outperforming baselines on real and synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying intervention targets in biological systems, where traditional causal discovery methods struggle due to high dimensionality and non-classical causality assumptions.

Method: Proposes a supervised framework that infers noisy causal graphs from data, maps differences between graphs to intervention targets, and jointly trains on simulated and real biological data.

Result: Outperforms baselines on seven single-cell transcriptomics datasets and improves prediction of intervention targets in synthetic data.

Conclusion: The approach effectively identifies intervention targets in complex biological systems, offering practical applications in drug discovery and cell engineering.

Abstract: Identifying variables responsible for changes to a biological system enables
applications in drug target discovery and cell engineering. Given a pair of
observational and interventional datasets, the goal is to isolate the subset of
observed variables that were the targets of the intervention. Directly applying
causal discovery algorithms is challenging: the data may contain thousands of
variables with as few as tens of samples per intervention, and biological
systems do not adhere to classical causality assumptions. We propose a
causality-inspired approach to address this practical setting. First, we infer
noisy causal graphs from the observational and interventional data. Then, we
learn to map the differences between these graphs, along with additional
statistical features, to sets of variables that were intervened upon. Both
modules are jointly trained in a supervised framework, on simulated and real
data that reflect the nature of biological interventions. This approach
consistently outperforms baselines for perturbation modeling on seven
single-cell transcriptomics datasets. We also demonstrate significant
improvements over current causal discovery methods for predicting soft and hard
intervention targets across a variety of synthetic data.

</details>


### [690] [Deep Bayesian Filter for Bayes-faithful Data Assimilation](https://arxiv.org/pdf/2405.18674)
*Yuta Tarumi, Keisuke Fukuda, Shin-ichi Maeda*

Main category: cs.LG

TL;DR: Deep Bayesian Filtering (DBF) is proposed for nonlinear state space models, using latent variables to maintain Gaussian posteriors and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume Gaussian posteriors, which fail for nonlinear models where true posteriors are non-Gaussian.

Method: DBF introduces latent variables, enforces linear transitions in latent space, and learns Gaussian inverse observation operators.

Result: DBF outperforms model-based and latent assimilation methods in non-Gaussian scenarios.

Conclusion: DBF effectively handles nonlinear SSMs by maintaining Gaussian posteriors through structured latent space design.

Abstract: State estimation for nonlinear state space models (SSMs) is a challenging
task. Existing assimilation methodologies predominantly assume Gaussian
posteriors on physical space, where true posteriors become inevitably
non-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on
nonlinear SSMs. DBF constructs new latent variables $h_t$ in addition to the
original physical variables $z_t$ and assimilates observations $o_t$. By (i)
constraining the state transition on the new latent space to be linear and (ii)
learning a Gaussian inverse observation operator $r(h_t|o_t)$, posteriors
remain Gaussian. Notably, the structured design of test distributions enables
an analytical formula for the recursive computation, eliminating the
accumulation of Monte Carlo sampling errors across time steps. DBF trains the
Gaussian inverse observation operators $r(h_t|o_t)$ and other latent SSM
parameters (e.g., dynamics matrix) by maximizing the evidence lower bound.
Experiments demonstrate that DBF outperforms model-based approaches and latent
assimilation methods in tasks where the true posterior distribution on physical
space is significantly non-Gaussian.

</details>


### [691] [Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces](https://arxiv.org/pdf/2410.15625)
*Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken*

Main category: cs.LG

TL;DR: A framework automates mapper development for parallel programs, outperforming manual tuning and traditional methods like OpenTuner with faster performance and reduced tuning time.


<details>
  <summary>Details</summary>
Motivation: High-performance computing relies on efficient task and data mapping, but manual tuning is time-consuming and requires expertise. Automating this process can save time and improve performance.

Method: The framework uses generative optimization, a Domain-Specific Language (DSL) to abstract system code, and AutoGuide for actionable feedback from raw execution output.

Result: The method outperforms OpenTuner (3.8X faster in 10 iterations vs. 1000) and expert-written mappers (up to 1.34X speedup), reducing tuning time from days to minutes.

Conclusion: The framework successfully automates mapper development, significantly improving performance and efficiency in parallel computing.

Abstract: Modern scientific discovery increasingly relies on high-performance computing
for complex modeling and simulation. A key challenge in improving parallel
program performance is efficiently mapping tasks to processors and data to
memory, a process dictated by intricate, low-level system code known as
mappers. Developing high-performance mappers demands days of manual tuning,
posing a significant barrier for domain scientists without systems expertise.
We introduce a framework that automates mapper development with generative
optimization, leveraging richer feedback beyond scalar performance metrics. Our
approach features the Agent-System Interface, which includes a Domain-Specific
Language (DSL) to abstract away the low-level complexity of system code and
define a structured search space, as well as AutoGuide, a mechanism that
interprets raw execution output into actionable feedback. Unlike traditional
reinforcement learning methods such as OpenTuner, which rely solely on scalar
feedback, our method finds superior mappers in far fewer iterations. With just
10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving
3.8X faster performance. Our approach finds mappers that surpass expert-written
mappers by up to 1.34X speedup across nine benchmarks while reducing tuning
time from days to minutes.

</details>


### [692] [A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening](https://arxiv.org/pdf/2406.09291)
*Guy Bar-Shalom, Yam Eitan, Fabrizio Frasca, Haggai Maron*

Main category: cs.LG

TL;DR: A new Subgraph GNN framework improves scalability and performance by associating subgraphs with node clusters, enabling flexible subgraph selection and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing Subgraph GNNs face scalability issues due to processing large subgraph numbers, with prior sampling methods yielding suboptimal results.

Method: The approach links subgraphs to node clusters, creating a new connectivity structure for generalized message passing, with controlled coarsening for subgraph selection.

Result: The method outperforms baselines, handles any number of subgraphs flexibly, and demonstrates superior performance on benchmarks.

Conclusion: The framework offers a scalable, effective solution for Subgraph GNNs, with theoretical and empirical validation.

Abstract: Subgraph GNNs enhance message-passing GNNs expressivity by representing
graphs as sets of subgraphs, demonstrating impressive performance across
various tasks. However, their scalability is hindered by the need to process
large numbers of subgraphs. While previous approaches attempted to generate
smaller subsets of subgraphs through random or learnable sampling, these
methods often yielded suboptimal selections or were limited to small subset
sizes, ultimately compromising their effectiveness. This paper introduces a new
Subgraph GNN framework to address these issues. Our approach diverges from most
previous methods by associating subgraphs with node clusters rather than with
individual nodes. We show that the resulting collection of subgraphs can be
viewed as the product of coarsened and original graphs, unveiling a new
connectivity structure on which we perform generalized message passing.
  Crucially, controlling the coarsening function enables meaningful selection
of any number of subgraphs. In addition, we reveal novel permutation symmetries
in the resulting node feature tensor, characterize associated linear
equivariant layers, and integrate them into our Subgraph GNN. We also introduce
novel node marking strategies and provide a theoretical analysis of their
expressive power and other key aspects of our approach. Extensive experiments
on multiple graph learning benchmarks demonstrate that our method is
significantly more flexible than previous approaches, as it can seamlessly
handle any number of subgraphs, while consistently outperforming baseline
approaches. Our code is available at
https://github.com/BarSGuy/Efficient-Subgraph-GNNs.

</details>


### [693] [SVIP: Towards Verifiable Inference of Open-source Large Language Models](https://arxiv.org/pdf/2410.22307)
*Yifan Sun, Yuhang Li, Yue Zhang, Yuchen Jin, Huan Zhang*

Main category: cs.LG

TL;DR: SVIP is a verifiable LLM inference protocol that ensures computing providers use the correct model by analyzing hidden representations and a proxy task, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Decentralized computing for LLM inference is cost-effective but risks providers substituting models without consent, compromising quality.

Method: SVIP requires providers to return generated text and hidden representations, trains a proxy task on these to verify model identity, and uses a secret mechanism for security.

Result: SVIP achieves <5% false negatives and <3% false positives, with verification taking <0.01 seconds per query.

Conclusion: SVIP is an effective, efficient, and secure solution for verifying LLM inference in decentralized settings.

Abstract: The ever-increasing size of open-source Large Language Models (LLMs) renders
local deployment impractical for individual users. Decentralized computing has
emerged as a cost-effective solution, allowing individuals and small companies
to perform LLM inference for users using surplus computational power. However,
a computing provider may stealthily substitute the requested LLM with a
smaller, less capable model without consent from users, thereby benefiting from
cost savings. We introduce SVIP, a secret-based verifiable LLM inference
protocol. Unlike existing solutions based on cryptographic or game-theoretic
techniques, our method is computationally effective and does not rest on strong
assumptions. Our protocol requires the computing provider to return both the
generated text and processed hidden representations from LLMs. We then train a
proxy task on these representations, effectively transforming them into a
unique model identifier. With our protocol, users can reliably verify whether
the computing provider is acting honestly. A carefully integrated secret
mechanism further strengthens its security. We thoroughly analyze our protocol
under multiple strong and adaptive adversarial scenarios. Our extensive
experiments demonstrate that SVIP is accurate, generalizable, computationally
efficient, and resistant to various attacks. Notably, SVIP achieves false
negative rates below 5% and false positive rates below 3%, while requiring less
than 0.01 seconds per prompt query for verification.

</details>


### [694] [Learning Distances from Data with Normalizing Flows and Score Matching](https://arxiv.org/pdf/2407.09297)
*Peter Sorrenson, Daniel Behrend-Uriarte, Christoph Schnörr, Ullrich Köthe*

Main category: cs.LG

TL;DR: The paper introduces improved methods for estimating density-based distances (DBDs), addressing issues like poor convergence and scalability in high dimensions using normalizing flows and geodesic relaxation.


<details>
  <summary>Details</summary>
Motivation: Existing estimators for Fermat distances (a type of DBD) suffer from inaccurate density estimates and poor scalability in high dimensions, limiting their practical use.

Method: The authors propose learning densities with normalizing flows and refining geodesics through relaxation, guided by a learned score model. They also introduce a dimension-adapted Fermat distance for better scalability and stability.

Result: The new approach improves convergence and scalability, making density-based distances more practical, especially in high-dimensional spaces.

Conclusion: This work advances the practical application of density-based distances by addressing key limitations of existing methods.

Abstract: Density-based distances (DBDs) provide a principled approach to metric
learning by defining distances in terms of the underlying data distribution. By
employing a Riemannian metric that increases in regions of low probability
density, shortest paths naturally follow the data manifold. Fermat distances, a
specific type of DBD, have attractive properties, but existing estimators based
on nearest neighbor graphs suffer from poor convergence due to inaccurate
density estimates. Moreover, graph-based methods scale poorly to high
dimensions, as the proposed geodesics are often insufficiently smooth. We
address these challenges in two key ways. First, we learn densities using
normalizing flows. Second, we refine geodesics through relaxation, guided by a
learned score model. Additionally, we introduce a dimension-adapted Fermat
distance that scales intuitively to high dimensions and improves numerical
stability. Our work paves the way for the practical use of density-based
distances, especially in high-dimensional spaces.

</details>


### [695] [One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models](https://arxiv.org/pdf/2410.22366)
*Viacheslav Surkov, Chris Wendler, Antonio Mari, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre, David Bau*

Main category: cs.LG

TL;DR: SAEs decompose SDXL Turbo's intermediate features into interpretable ones, generalizing across models and enabling causal edits via RIEBench.


<details>
  <summary>Details</summary>
Motivation: Extend SAE-based interpretability from LLMs to text-to-image models like SDXL Turbo.

Method: Train SAEs on SDXL Turbo's transformer block updates, test generalization, and evaluate via RIEBench for feature-based editing.

Result: SAEs generalize to other models without retraining, features are interpretable and causally impactful, revealing block specialization.

Conclusion: SAEs are effective for understanding and manipulating text-to-image models, marking the first such application.

Abstract: For large language models (LLMs), sparse autoencoders (SAEs) have been shown
to decompose intermediate representations that often are not interpretable
directly into sparse sums of interpretable features, facilitating better
control and subsequent analysis. However, similar analyses and approaches have
been lacking for text-to-image models. We investigate the possibility of using
SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image
diffusion model. To this end, we train SAEs on the updates performed by
transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting.
Interestingly, we find that they generalize to 4-step SDXL Turbo and even to
the multi-step SDXL base model (i.e., a different model) without additional
training. In addition, we show that their learned features are interpretable,
causally influence the generation process, and reveal specialization among the
blocks. We do so by creating RIEBench, a representation-based image editing
benchmark, for editing images while they are generated by turning on and off
individual SAE features. This allows us to track which transformer blocks'
features are the most impactful depending on the edit category. Our work is the
first investigation of SAEs for interpretability in text-to-image diffusion
models and our results establish SAEs as a promising approach for understanding
and manipulating the internal mechanisms of text-to-image models.

</details>


### [696] [Targeted Unlearning with Single Layer Unlearning Gradient](https://arxiv.org/pdf/2407.11867)
*Zikui Cai, Yaoteng Tan, M. Salman Asif*

Main category: cs.LG

TL;DR: SLUG is an efficient machine unlearning method that updates a single critical layer to remove targeted information, reducing computational cost while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods are computationally expensive and may degrade model performance. SLUG aims to address these limitations.

Method: SLUG identifies and updates a single critical layer using layer importance and gradient alignment metrics for targeted unlearning.

Result: SLUG achieves comparable unlearning performance to existing methods with significantly lower computational costs, demonstrated on CLIP, Stable Diffusion, and VLMs.

Conclusion: SLUG provides a practical, efficient, and precise solution for targeted unlearning in machine learning models.

Abstract: Machine unlearning methods aim to remove sensitive or unwanted content from
trained models, but typically demand extensive model updates at significant
computational cost while potentially degrading model performance on both
related and unrelated tasks. We propose Single Layer Unlearning Gradient (SLUG)
as an efficient method to unlearn targeted information by updating a single
critical layer using a one-time gradient computation. SLUG uses layer
importance and gradient alignment metrics to identify the optimal layer for
targeted information removal while preserving the model utility. We demonstrate
the effectiveness of SLUG for CLIP, Stable Diffusion, and vision-language
models (VLMs) in removing concrete (e.g., identities and objects) and abstract
concepts (e.g., artistic styles). On the UnlearnCanvas benchmark, SLUG achieves
comparable unlearning performance to existing methods while requiring
significantly less computational resources. Our proposed approach offers a
practical solution for targeted unlearning that is computationally efficient
and precise. Our code is available at https://github.com/CSIPlab/SLUG.

</details>


### [697] [Towards Robust and Efficient Federated Low-Rank Adaptation with Heterogeneous Clients](https://arxiv.org/pdf/2410.22815)
*Jabin Koo, Minwoo Jang, Jungseul Ok*

Main category: cs.LG

TL;DR: LoRA-A² improves federated fine-tuning for LLMs by reducing communication overhead and maintaining performance under low ranks and high data heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of heavy communication overhead and aggregation discordance in federated fine-tuning of LLMs, especially with Low Rank Adaptation (LoRA).

Method: Introduces LoRA-A², which alternates freezing and adaptive rank selection to enhance robustness and efficiency.

Result: LoRA-A² maintains performance under extreme heterogeneity and low ranks, significantly reducing uploaded parameters compared to full fine-tuning.

Conclusion: LoRA-A² enables practical deployment of LLMs in resource-constrained environments by improving communication efficiency and robustness.

Abstract: Federated fine-tuning for Large Language Models (LLMs) faces significant
challenges due to the heavy communication overhead of transmitting large model
updates. Although Low Rank Adaptation (LoRA) has been proposed as a solution,
yet its application in federated learning is complicated by discordance in
aggregation. Existing methods addressing this discordance often suffer from
performance degradation at low ranks in heterogeneous data settings. In
response, we introduce LoRA-A$^2$ (Low Rank Adaptation with Alternating freeze
and Adaptive rank selection), which demonstrates robustness in challenging
settings with low ranks and high data heterogeneity. Our experimental findings
reveal that LoRA-A$^2$ maintains performance even under extreme heterogeneity
and low rank conditions, achieving up to a significant reduction in uploaded
parameters compared to full fine-tuning without compromising performance. This
adaptive mechanism increases robustness and communication efficiency in
federated fine-tuning, enabling the practical deployment of LLMs in
resource-constrained environments.

</details>


### [698] [Friends in Unexpected Places: Enhancing Local Fairness in Federated Learning through Clustering](https://arxiv.org/pdf/2407.19331)
*Yifan Yang, Ali Payani, Parinaz Naghizadeh*

Main category: cs.LG

TL;DR: Proposes new FL algorithms balancing personalization and local fairness in heterogeneous settings, improving accuracy and fairness without explicit intervention.


<details>
  <summary>Details</summary>
Motivation: Single shared FL models often underperform in heterogeneous settings, and global fairness doesn't ensure local fairness, necessitating new approaches.

Method: Builds on clustering-based personalized FL, incorporating a fairness metric for tunable balance between accuracy and fairness.

Result: Matches or exceeds existing locally fair FL methods, showing personalization alone can enhance local fairness.

Conclusion: Personalization and fairness can align, and the proposed methods effectively exploit this alignment for improved performance.

Abstract: Federated Learning (FL) has been a pivotal paradigm for collaborative
training of machine learning models across distributed datasets. In
heterogeneous settings, it has been observed that a single shared FL model can
lead to low local accuracy, motivating personalized FL algorithms. In parallel,
fair FL algorithms have been proposed to enforce group fairness on the global
models. Again, in heterogeneous settings, global and local fairness do not
necessarily align, motivating the recent literature on locally fair FL. In this
paper, we propose new FL algorithms for heterogeneous settings, spanning the
space between personalized and locally fair FL. Building on existing
clustering-based personalized FL methods, we incorporate a new fairness metric
into cluster assignment, enabling a tunable balance between local accuracy and
fairness. Our methods match or exceed the performance of existing locally fair
FL approaches, without explicit fairness intervention. We further demonstrate
(numerically and analytically) that personalization alone can improve local
fairness and that our methods exploit this alignment when present.

</details>


### [699] [ECG-FM: An Open Electrocardiogram Foundation Model](https://arxiv.org/pdf/2408.05178)
*Kaden McKeen, Sameer Masood, Augustin Toma, Barry Rubin, Bo Wang*

Main category: cs.LG

TL;DR: ECG-FM is an open, transformer-based foundation model for ECG analysis, pretrained with self-supervised learning. It outperforms task-specific models, especially with small-to-medium datasets, and shows strong generalizability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of open-weight ECG foundation models and reduce reliance on large annotated datasets for ECG analysis.

Method: Transformer-based model pretrained using hybrid contrastive and generative self-supervised learning on 1.5 million ECGs. Evaluated on tasks like LVEF prediction and ECG interpretation.

Result: ECG-FM outperforms task-specific models, achieving high AUROC for clinically salient labels (e.g., atrial fibrillation: 0.996, LVEF<=40%: 0.929).

Conclusion: ECG-FM is robust, label-efficient, and generalizable, offering a scalable solution for ECG analysis. Code and model weights are publicly available.

Abstract: Conventional task-specific electrocardiogram (ECG) analysis models require
large annotated datasets to train. Foundation models mitigate this burden by
leveraging self-supervised pretraining; however, the scarcity of open-weight
ECG foundation models hinders adoption and cross-study comparability. We
present ECG-FM, an open foundation model for ECG analysis, and conduct a study
using a dataset of 1.5 million ECGs. ECG-FM is a transformer-based model
pretrained using a hybrid contrastive and generative self-supervised learning
approach. Our downstream tasks include predicting reduced left ventricular
ejection fraction (LVEF) and ECG interpretation labels, where we release a
benchmark task on the MIMIC-IV-ECG dataset. We affirm that ECG-FM is robust,
label-efficient, and functionally discriminative by showcasing data scaling
experiments, performing a latent space analysis, and generating saliency maps.
ECG-FM markedly outperforms task-specific models in the small-to-medium-scale
data regime and demonstrates cross-dataset generalizability, achieving high
AUROC on many clinically salient labels such as atrial fibrillation (0.996) and
LVEF<=40% (0.929). We release our code, model weights, and benchmark task at
https://github.com/bowang-lab/ECG-FM/.

</details>


### [700] ["Give Me BF16 or Give Me Death"? Accuracy-Performance Trade-Offs in LLM Quantization](https://arxiv.org/pdf/2411.02355)
*Eldar Kurtic, Alexandre Marques, Shubhra Pandit, Mark Kurtz, Dan Alistarh*

Main category: cs.LG

TL;DR: The paper evaluates FP8, INT8, and INT4 quantization for LLM inference, finding FP8 lossless, INT8 with minimal degradation, and INT4 surprisingly competitive. It provides deployment recommendations based on performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: To clarify accuracy-performance trade-offs of quantization formats (FP8, INT8, INT4) for LLM inference, guiding practical deployments.

Method: Conducted over 500,000 evaluations on the Llama-3.1 model family using academic benchmarks and real-world tasks, analyzing performance via the vLLM framework.

Result: FP8 is lossless; INT8 has 1-3% degradation; INT4 rivals 8-bit. W4A16 is cost-efficient for synchronous setups, W8A8 for asynchronous batching.

Conclusion: Provides data-driven guidelines for deploying quantized LLMs, balancing speed, efficiency, and accuracy.

Abstract: Quantization is a powerful tool for accelerating large language model (LLM)
inference, but the accuracy-performance trade-offs across different formats
remain unclear. In this paper, we conduct the most comprehensive empirical
study to date, evaluating FP8, INT8, and INT4 quantization across academic
benchmarks and real-world tasks on the entire Llama-3.1 model family. Through
over 500,000 evaluations, our investigation yields several key findings: (1)
FP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned
INT8 (W8A8-INT) achieves surprisingly low (1-3\%) accuracy degradation, and (3)
INT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit
quantization. Further, we investigate the optimal quantization format for
different deployments by analyzing inference performance through the popular
vLLM framework. Our analysis provides clear deployment recommendations: W4A16
is the most cost-efficient for synchronous setups, while W8A8 dominates in
asynchronous continuous batching. For mixed workloads, the optimal choice
depends on the specific use case. Our findings offer practical, data-driven
guidelines for deploying quantized LLMs at scale -- ensuring the best balance
between speed, efficiency, and accuracy.

</details>


### [701] [A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs](https://arxiv.org/pdf/2408.08868)
*H. Brendan McMahan, Zheng Xu, Yanxiang Zhang*

Main category: cs.LG

TL;DR: The paper introduces BLT-DP-FTRL, a method combining Buffered Linear Toeplitz with DP-FTRL, improving privacy/utility tradeoffs in federated learning for on-device language models.


<details>
  <summary>Details</summary>
Motivation: Existing methods like tree aggregation and matrix factorization have limitations in privacy/utility tradeoffs, optimization complexity, and runtime costs.

Method: Extends the Buffered Linear Toeplitz (BLT) mechanism to multi-participation scenarios, integrating it with DP-FTRL.

Result: BLT-DP-FTRL matches matrix factorization in utility and privacy while being easier to use, validated on StackOverflow and real-world tasks.

Conclusion: BLT-DP-FTRL enhances practicality and effectiveness of differential privacy in federated learning for real-world applications.

Abstract: The state-of-the-art for training on-device language models for mobile
keyboard applications combines federated learning (FL) with differential
privacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two
variants of DP-FTRL are used in practice, tree aggregation and matrix
factorization. However, tree aggregation suffers from significantly suboptimal
privacy/utility tradeoffs, while matrix mechanisms require expensive
optimization parameterized by hard-to-estimate-in-advance constants, and high
runtime memory costs. This paper extends the recently introduced Buffered
Linear Toeplitz (BLT) mechanism to multi-participation scenarios. Our
BLT-DP-FTRL maintains the ease-of-use advantages of tree aggregation, while
essentially matching matrix factorization in terms of utility and privacy. We
evaluate BLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible
simulation benchmark, and across four on-device language model tasks in a
production FL system. Our empirical results highlight the advantages of the BLT
mechanism and elevate the practicality and effectiveness of DP in real-world
scenarios.

</details>


### [702] [Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models](https://arxiv.org/pdf/2408.09429)
*Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, Xuming Hu*

Main category: cs.LG

TL;DR: The paper introduces Reefknot, a benchmark for relation hallucinations in MLLMs, addressing gaps in current research and datasets. It proposes a confidence-based mitigation strategy, reducing hallucinations by 9.75%.


<details>
  <summary>Details</summary>
Motivation: Existing research on MLLM hallucinations focuses on object or attribute levels, overlooking complex relation hallucinations. Current benchmarks lack detailed evaluation and suffer from biases.

Method: Reefknot is developed with 20,000 real-world samples, using Visual Genome for a relation-based corpus. A confidence-based mitigation strategy is proposed.

Result: Current MLLMs show significant limitations in handling relation hallucinations. The mitigation strategy reduces hallucination rates by 9.75%.

Conclusion: Reefknot provides insights for trustworthy multimodal intelligence, addressing relation hallucinations effectively.

Abstract: Hallucination issues continue to affect multimodal large language models
(MLLMs), with existing research mainly addressing object-level or
attribute-level hallucinations, neglecting the more complex relation
hallucinations that require advanced reasoning. Current benchmarks for relation
hallucinations lack detailed evaluation and effective mitigation, and their
datasets often suffer from biases due to systematic annotation processes. To
address these challenges, we introduce Reefknot, a comprehensive benchmark
targeting relation hallucinations, comprising over 20,000 real-world samples.
We provide a systematic definition of relation hallucinations, integrating
perceptive and cognitive perspectives, and construct a relation-based corpus
using the Visual Genome scene graph dataset. Our comparative evaluation reveals
significant limitations in current MLLMs' ability to handle relation
hallucinations. Additionally, we propose a novel confidence-based mitigation
strategy, which reduces the hallucination rate by an average of 9.75% across
three datasets, including Reefknot. Our work offers valuable insights for
achieving trustworthy multimodal intelligence.

</details>


### [703] [Fast Convergence of Softmax Policy Mirror Ascent](https://arxiv.org/pdf/2411.12042)
*Reza Asad, Reza Babanezhad, Issam Laradji, Nicolas Le Roux, Sharan Vaswani*

Main category: cs.LG

TL;DR: SPMA, a refined policy gradient method, matches NPG's linear convergence for tabular MDPs and outperforms other methods. It extends to log-linear and non-linear FA without needing compatible approximation, achieving strong empirical results.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing policy gradient methods by refining the mirror ascent approach in the dual space of logits, removing normalization needs, and enhancing convergence and practicality.

Method: SPMA refines the mirror ascent method in logit space, removes action normalization, and extends to log-linear and non-linear function approximation. It solves convex softmax classification for linear FA.

Result: SPMA matches NPG's linear convergence for tabular MDPs, outperforms accelerated softmax policy gradient, and achieves linear convergence to optimal value neighborhoods in FA settings.

Conclusion: SPMA is a robust and efficient policy optimization method, performing comparably or better than MDPO, PPO, and TRPO in benchmarks.

Abstract: Natural policy gradient (NPG) is a common policy optimization algorithm and
can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani
et al. [2021] introduced a policy gradient method that corresponds to mirror
ascent in the dual space of logits. We refine this algorithm, removing its need
for a normalization across actions and analyze the resulting method (referred
to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size
matches the linear convergence of NPG and achieves a faster convergence than
constant step-size (accelerated) softmax policy gradient. To handle large
state-action spaces, we extend SPMA to use a log-linear policy
parameterization. Unlike that for NPG, generalizing SPMA to the linear function
approximation (FA) setting does not require compatible function approximation.
Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only
requires solving convex softmax classification problems. We prove that SPMA
achieves linear convergence to the neighbourhood of the optimal value function.
We extend SPMA to handle non-linear FA and evaluate its empirical performance
on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA
consistently achieves similar or better performance compared to MDPO, PPO and
TRPO.

</details>


### [704] [Reinforcement Learning for Causal Discovery without Acyclicity Constraints](https://arxiv.org/pdf/2408.13448)
*Bao Duong, Hung Le, Biwei Huang, Thin Nguyen*

Main category: cs.LG

TL;DR: ALIAS is a novel RL-based method for learning DAGs without explicit acyclicity constraints, achieving efficient exploration and strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for causal discovery struggle with the acyclicity constraint and inefficient exploration of DAG space.

Method: ALIAS uses a novel DAG parametrization and policy gradient methods to generate DAGs in one step with quadratic complexity, bypassing acyclicity constraints.

Result: ALIAS outperforms state-of-the-art methods in causal discovery on synthetic and real datasets under challenging conditions.

Conclusion: ALIAS offers an efficient and effective RL-based solution for causal discovery by eliminating the need for explicit acyclicity constraints.

Abstract: Recently, reinforcement learning (RL) has proved a promising alternative for
conventional local heuristics in score-based approaches to learning directed
acyclic causal graphs (DAGs) from observational data. However, the intricate
acyclicity constraint still challenges the efficient exploration of the vast
space of DAGs in existing methods. In this study, we introduce ALIAS
(reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to
causal discovery powered by the RL machinery. Our method features an efficient
policy for generating DAGs in just a single step with an optimal quadratic
complexity, fueled by a novel parametrization of DAGs that directly translates
a continuous space to the space of all DAGs, bypassing the need for explicitly
enforcing acyclicity constraints. This approach enables us to navigate the
search space more effectively by utilizing policy gradient methods and
established scoring functions. In addition, we provide compelling empirical
evidence for the strong performance of ALIAS in comparison with
state-of-the-arts in causal discovery over increasingly difficult experiment
conditions on both synthetic and real datasets.

</details>


### [705] [Adversarial Pruning: A Survey and Benchmark of Pruning Methods for Adversarial Robustness](https://arxiv.org/pdf/2409.01249)
*Giorgio Piras, Maura Pintor, Ambra Demontis, Battista Biggio, Giorgio Giacinto, Fabio Roli*

Main category: cs.LG

TL;DR: A survey and taxonomy of adversarial pruning methods, proposing a fair evaluation benchmark and re-evaluating current methods.


<details>
  <summary>Details</summary>
Motivation: Address the complexity and lack of fair comparison in adversarial pruning methods.

Method: Survey existing methods, propose a taxonomy (pipeline and specifics), and create a benchmark for fair evaluation.

Result: Identified shared traits of top-performing methods and common issues.

Conclusion: Provides a standardized benchmark for future adversarial pruning research.

Abstract: Recent work has proposed neural network pruning techniques to reduce the size
of a network while preserving robustness against adversarial examples, i.e.,
well-crafted inputs inducing a misclassification. These methods, which we refer
to as adversarial pruning methods, involve complex and articulated designs,
making it difficult to analyze the differences and establish a fair and
accurate comparison. In this work, we overcome these issues by surveying
current adversarial pruning methods and proposing a novel taxonomy to
categorize them based on two main dimensions: the pipeline, defining when to
prune; and the specifics, defining how to prune. We then highlight the
limitations of current empirical analyses and propose a novel, fair evaluation
benchmark to address them. We finally conduct an empirical re-evaluation of
current adversarial pruning methods and discuss the results, highlighting the
shared traits of top-performing adversarial pruning methods, as well as common
issues. We welcome contributions in our publicly-available benchmark at
https://github.com/pralab/AdversarialPruningBenchmark

</details>


### [706] [Unified theoretical guarantees for stability, consistency, and convergence in neural PDE solvers from non-IID data to physics-informed networks](https://arxiv.org/pdf/2409.05030)
*Ronald Katende*

Main category: cs.LG

TL;DR: A unified framework for analyzing neural network stability, consistency, and convergence under non-IID data, geometric constraints, and physical laws, with theoretical guarantees validated across various PDEs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous theoretical foundations for neural networks in realistic training scenarios, including non-IID data, federated learning heterogeneity, and physics-informed settings.

Method: Derives stability bounds using mixing coefficients and dynamic learning rates, quantifies inconsistency via curvature-aware aggregation, and proves stability and convergence for PINNs using variational analysis and Sobolev space theorems.

Result: Establishes uniform stability bounds, model inconsistency measures, and rigorous convergence proofs for PINNs, validated across parabolic, elliptic, and hyperbolic PDEs.

Conclusion: Provides a principled mathematical basis for designing robust and physically coherent neural architectures in diverse learning environments.

Abstract: We establish a unified theoretical framework addressing the stability,
consistency, and convergence of neural networks under realistic training
conditions, specifically, in the presence of non-IID data, geometric
constraints, and embedded physical laws. For standard supervised learning with
dependent data, we derive uniform stability bounds for gradient-based methods
using mixing coefficients and dynamic learning rates. In federated learning
with heterogeneous data and non-Euclidean parameter spaces, we quantify model
inconsistency via curvature-aware aggregation and information-theoretic
divergence. For Physics-Informed Neural Networks (PINNs), we rigorously prove
perturbation stability, residual consistency, Sobolev convergence, energy
stability for conservation laws, and convergence under adaptive multi-domain
refinements. Each result is grounded in variational analysis, compactness
arguments, and universal approximation theorems in Sobolev spaces. Our
theoretical guarantees are validated across parabolic, elliptic, and hyperbolic
PDEs, confirming that residual minimization aligns with physical solution
accuracy. This work offers a mathematically principled basis for designing
robust, generalizable, and physically coherent neural architectures across
diverse learning environments.

</details>


### [707] [DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation](https://arxiv.org/pdf/2410.03782)
*Changdae Oh, Yixuan Li, Kyungwoo Song, Sangdoo Yun, Dongyoon Han*

Main category: cs.LG

TL;DR: DaWin is a training-free dynamic weight interpolation method that improves downstream task robustness by dynamically adjusting interpolation coefficients based on model entropy, reducing computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing static weight interpolation methods limit performance and efficiency. DaWin aims to enhance robustness against distribution shifts without retraining.

Method: DaWin uses entropy to assess model expertise and dynamically computes per-sample interpolation coefficients. It also introduces a mixture modeling approach to reduce inference overhead.

Result: DaWin achieves significant performance gains on 14 tasks, including robust fine-tuning and multi-task learning, with minimal computational cost.

Conclusion: DaWin effectively improves downstream task performance dynamically and efficiently, validated by empirical success and analysis.

Abstract: Adapting a pre-trained foundation model on downstream tasks should ensure
robustness against distribution shifts without the need to retrain the whole
model. Although existing weight interpolation methods are simple yet effective,
we argue that their static nature limits downstream performance while achieving
efficiency. In this work, we propose DaWin, a training-free dynamic weight
interpolation method that leverages the entropy of individual models over each
unlabeled test sample to assess model expertise, and compute per-sample
interpolation coefficients dynamically. Unlike previous works that typically
rely on additional training to learn such coefficients, our approach requires
no training. Then, we propose a mixture modeling approach that greatly reduces
inference overhead raised by dynamic interpolation. We validate DaWin on the
large-scale visual recognition benchmarks, spanning 14 tasks across robust
fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and
multi-task learning with eight classification tasks. Results demonstrate that
DaWin achieves significant performance gain in considered settings, with
minimal computational overhead. We further discuss DaWin's analytic behavior to
explain its empirical success.

</details>


### [708] [A Generalization Result for Convergence in Learning-to-Optimize](https://arxiv.org/pdf/2410.07704)
*Michael Sucker, Peter Ochs*

Main category: cs.LG

TL;DR: The paper introduces a probabilistic framework to ensure convergence of learned optimization algorithms, addressing the lack of theoretical guarantees in learning-to-optimize.


<details>
  <summary>Details</summary>
Motivation: Existing learning-to-optimize methods lack theoretical guarantees, particularly for convergence, due to the difficulty of applying classical geometric arguments to learned algorithms.

Method: The authors develop a probabilistic framework that transfers geometric arguments from classical optimization to learning-to-optimize, enabling convergence analysis for non-smooth, non-convex functions.

Result: The main theorem generalizes convergence guarantees to parametric classes of loss functions, ensuring high-probability convergence to critical points.

Conclusion: This work bridges the gap between empirical success and theoretical assurance in learning-to-optimize, allowing for more flexible algorithm design without safeguards.

Abstract: Learning-to-optimize leverages machine learning to accelerate optimization
algorithms. While empirical results show tremendous improvements compared to
classical optimization algorithms, theoretical guarantees are mostly lacking,
such that the outcome cannot be reliably assured. Especially, convergence is
hardly studied in learning-to-optimize, because conventional convergence
guarantees in optimization are based on geometric arguments, which cannot be
applied easily to learned algorithms. Thus, we develop a probabilistic
framework that resembles classical optimization and allows for transferring
geometric arguments into learning-to-optimize. Based on our new proof-strategy,
our main theorem is a generalization result for parametric classes of
potentially non-smooth, non-convex loss functions and establishes the
convergence of learned optimization algorithms to critical points with high
probability. This effectively generalizes the results of a worst-case analysis
into a probabilistic framework, and frees the design of the learned algorithm
from using safeguards.

</details>


### [709] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/pdf/2410.16222)
*Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping*

Main category: cs.LG

TL;DR: A unified threat model is proposed for comparing jailbreaking attacks on LLMs, using an N-gram language model for evaluation. Findings show lower success rates than previously reported and highlight the superiority of discrete optimization-based attacks.


<details>
  <summary>Details</summary>
Motivation: To provide a principled comparison of jailbreaking attacks on safety-tuned LLMs, addressing variability in fluency and computational effort.

Method: Develop an N-gram language model on 1T tokens for LLM-agnostic, nonparametric, and interpretable evaluation. Adapt and benchmark popular attacks under this model.

Result: Attack success rates are lower than previously reported; discrete optimization-based attacks outperform LLM-based ones. Effective attacks exploit rare or infrequent bigrams.

Conclusion: The proposed threat model offers interpretable and comprehensive analysis, revealing key patterns in successful jailbreak attacks.

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [710] [SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting](https://arxiv.org/pdf/2501.16178)
*Wenxuan Xie, Fanpu Cao*

Main category: cs.LG

TL;DR: SWIFT is a lightweight model for long-term time-series forecasting, combining wavelet transform, learnable filters, and minimal linear layers to achieve SOTA performance with efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the gap in lightweight models for time-series prediction in resource-constrained environments, particularly for non-stationary sequences.

Method: Uses wavelet transform for downsampling, learnable filters for cross-band fusion, and shared linear layers or shallow MLPs for sub-series mapping.

Result: Achieves SOTA performance on multiple datasets with significantly fewer parameters (25% of a single-layer linear model).

Conclusion: SWIFT offers an efficient and powerful solution for edge computing in time-series forecasting.

Abstract: In recent work on time-series prediction, Transformers and even large
language models have garnered significant attention due to their strong
capabilities in sequence modeling. However, in practical deployments,
time-series prediction often requires operation in resource-constrained
environments, such as edge devices, which are unable to handle the
computational overhead of large models. To address such scenarios, some
lightweight models have been proposed, but they exhibit poor performance on
non-stationary sequences. In this paper, we propose $\textit{SWIFT}$, a
lightweight model that is not only powerful, but also efficient in deployment
and inference for Long-term Time Series Forecasting (LTSF). Our model is based
on three key points: (i) Utilizing wavelet transform to perform lossless
downsampling of time series. (ii) Achieving cross-band information fusion with
a learnable filter. (iii) Using only one shared linear layer or one shallow MLP
for sub-series' mapping. We conduct comprehensive experiments, and the results
show that $\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on
multiple datasets, offering a promising method for edge computing and
deployment in this task. Moreover, it is noteworthy that the number of
parameters in $\textit{SWIFT-Linear}$ is only 25\% of what it would be with a
single-layer linear model for time-domain prediction. Our code is available at
https://github.com/LancelotXWX/SWIFT.

</details>


### [711] [ExpProof : Operationalizing Explanations for Confidential Models with ZKPs](https://arxiv.org/pdf/2502.03773)
*Chhavi Yadav, Evan Monroe Laufer, Dan Boneh, Kamalika Chaudhuri*

Main category: cs.LG

TL;DR: The paper addresses the failure of explainability methods in adversarial settings and proposes using Zero-Knowledge Proofs (ZKPs) to operationalize explanations, specifically adapting LIME for ZKPs and testing it on Neural Networks and Random Forests.


<details>
  <summary>Details</summary>
Motivation: Explainability methods often fail in adversarial scenarios where parties manipulate explanations for their benefit, despite regulatory demands for trust in machine learning models.

Method: The authors adapt the LIME explainability algorithm to be ZKP-amenable and evaluate its performance on Neural Networks and Random Forests.

Result: The proposed ZKP-amenable versions of LIME are tested, with code made publicly available for reproducibility.

Conclusion: The study advances the operationalization of explanations in adversarial settings using cryptographic primitives like ZKPs, offering a practical solution to trust issues in machine learning.

Abstract: In principle, explanations are intended as a way to increase trust in machine
learning models and are often obligated by regulations. However, many
circumstances where these are demanded are adversarial in nature, meaning the
involved parties have misaligned interests and are incentivized to manipulate
explanations for their purpose. As a result, explainability methods fail to be
operational in such settings despite the demand \cite{bordt2022post}. In this
paper, we take a step towards operationalizing explanations in adversarial
scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive.
Specifically we explore ZKP-amenable versions of the popular explainability
algorithm LIME and evaluate their performance on Neural Networks and Random
Forests. Our code is publicly available at
https://github.com/emlaufer/ExpProof.

</details>


### [712] [Fixing the Double Penalty in Data-Driven Weather Forecasting Through a Modified Spherical Harmonic Loss Function](https://arxiv.org/pdf/2501.19374)
*Christopher Subich, Syed Zahid Husain, Leo Separovic, Jing Yang*

Main category: cs.LG

TL;DR: A modified loss function for data-driven weather forecasting avoids smoothing fine scales, improving forecast sharpness, resolution, and accuracy for extreme weather events.


<details>
  <summary>Details</summary>
Motivation: Data-driven weather models outperform physics-based ones but suffer from fine-scale smoothing due to mean squared error loss.

Method: Develop a parameter-free loss function modification separating decorrelation and spectral amplitude errors.

Result: Improved forecast sharpness, resolution (1,250km to 160km), ensemble spread, and predictions for tropical cyclones and wind extremes.

Conclusion: The new loss function enhances deterministic weather forecasts by addressing fine-scale smoothing issues.

Abstract: Recent advancements in data-driven weather forecasting models have delivered
deterministic models that outperform the leading operational forecast systems
based on traditional, physics-based models. However, these data-driven models
are typically trained with a mean squared error loss function, which causes
smoothing of fine scales through a "double penalty" effect. We develop a
simple, parameter-free modification to this loss function that avoids this
problem by separating the loss attributable to decorrelation from the loss
attributable to spectral amplitude errors. Fine-tuning the GraphCast model with
this new loss function results in sharp deterministic weather forecasts, an
increase of the model's effective resolution from 1,250km to 160km,
improvements to ensemble spread, and improvements to predictions of tropical
cyclone strength and surface wind extremes.

</details>


### [713] [Safety Reasoning with Guidelines](https://arxiv.org/pdf/2502.04040)
*Haoyu Wang, Zeyu Qin, Li Shen, Xueqian Wang, Dacheng Tao, Minhao Cheng*

Main category: cs.LG

TL;DR: The paper questions the effectiveness of Refusal Training (RT) against OOD jailbreaking attacks and proposes a safety reasoning method to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of RT in generalizing against OOD attacks and improving LLM safety.

Method: Proposes training models to perform safety reasoning for each query, synthesizing reasoning supervision aligned with safety guidelines.

Result: Significant improvements in model generalization against OOD attacks.

Conclusion: Safety reasoning outperforms RT by eliciting latent safety knowledge, enhancing generalization.

Abstract: Training safe LLMs remains a critical challenge. The most widely used method,
Refusal Training (RT), struggles to generalize against various
Out-of-Distribution (OOD) jailbreaking attacks. Although various advanced
methods have been proposed to address this issue, we instead question whether
OOD attacks inherently surpass the capability of vanilla RT. Evaluations using
Best-of-N (BoN) reveal significant safety improvements as N increases,
indicating models possess adequate latent safety knowledge but RT fails to
consistently elicit it under OOD scenarios. Further domain adaptation analysis
reveals that direct RT causes reliance on superficial shortcuts, resulting in
non-generalizable representation mappings. Inspired by our findings, we propose
training model to perform safety reasoning for each query. Specifically, we
synthesize reasoning supervision aligned with specified guidelines that reflect
diverse perspectives on safety knowledge. This encourages model to engage in
deeper reasoning, explicitly eliciting and utilizing latent safety knowledge
for each query. Extensive experiments show that our method significantly
improves model generalization against OOD attacks.

</details>


### [714] [Sundial: A Family of Highly Capable Time Series Foundation Models](https://arxiv.org/pdf/2502.00816)
*Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long*

Main category: cs.LG

TL;DR: Sundial is a family of time series foundation models using TimeFlow Loss for flexible, scalable, and native pre-training of Transformers on continuous-valued time series. It achieves state-of-the-art forecasting performance with fast inference.


<details>
  <summary>Details</summary>
Motivation: To create a flexible and scalable time series foundation model that avoids discrete tokenization and enables native pre-training on continuous-valued data.

Method: Proposes TimeFlow Loss based on flow-matching for pre-training Transformers, uses TimeBench dataset (1 trillion time points), and mitigates mode collapse.

Result: Achieves state-of-the-art performance on forecasting benchmarks with fast, zero-shot predictions.

Conclusion: Sundial's generative forecasting capability enhances model reliability for real-world decision-making.

Abstract: We introduce Sundial, a family of native, flexible, and scalable time series
foundation models. To predict the next-patch's distribution, we propose a
TimeFlow Loss based on flow-matching, which facilitates native pre-training of
Transformers on continuous-valued time series without discrete tokenization.
Conditioned on arbitrary-length time series, our models are pre-trained without
specifying any prior distribution and can generate multiple probable
predictions, achieving more flexibility in representation learning than using
parametric densities. Towards time series foundation models, we leverage
minimal but crucial adaptations of Transformers and curate TimeBench with one
trillion time points, comprising mostly real-world datasets and synthetic data.
By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial
models on TimeBench, which achieve unprecedented model capacity and
generalization performance. In addition to excellent scalability, Sundial
achieves state-of-the-art results on both point and probabilistic forecasting
benchmarks with a just-in-time inference speed, i.e., making zero-shot
predictions within a few milliseconds. We believe that Sundial's pioneering
generative forecasting capability can improve model reliability in real-world
decision-making. Code is available at: https://github.com/thuml/Sundial.

</details>


### [715] [WaferLLM: Large Language Model Inference at Wafer Scale](https://arxiv.org/pdf/2502.04563)
*Congjie He, Yeqi Huang, Pei Mu, Ziming Miao, Jilong Xue, Lingxiao Ma, Fan Yang, Luo Mai*

Main category: cs.LG

TL;DR: WaferLLM is the first wafer-scale LLM inference system, optimizing AI accelerator utilization with novel parallelism and operations, achieving significant speedups and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LLM inference systems underutilize wafer-scale AI accelerators due to their shared memory architecture optimization.

Method: Introduces WaferLLM with a PLMR model, wafer-scale parallelism, and MeshGEMM/MeshGEMV operations for efficient scaling.

Result: Achieves 200× higher utilization, 606× faster GEMV, 16× energy efficiency, and 10-20× speedups over GPU clusters.

Conclusion: WaferLLM demonstrates superior performance and efficiency, with potential for further improvements as wafer-scale technologies mature.

Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing
technologies, integrating hundreds of thousands of AI cores in a mesh
architecture with large distributed on-chip memory (tens of GB in total) and
ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM
inference systems, optimized for shared memory architectures like GPUs, fail to
exploit these accelerators fully.
  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM
is guided by a novel PLMR model (pronounced as "Plummer") that captures the
unique hardware characteristics of wafer-scale architectures. Leveraging this
model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the
utilization of hundreds of thousands of on-chip cores. It also introduces
MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to
scale effectively on wafer-scale accelerators.
  Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator
utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator
(Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and
16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM
inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters
running SGLang and vLLM. These advantages are expected to grow as wafer-scale
AI models, software, and hardware continue to mature. WaferLLM is open-sourced
at https://github.com/MeshInfra/WaferLLM.

</details>


### [716] [No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces](https://arxiv.org/pdf/2502.04959)
*Daniel Marczak, Simone Magistri, Sebastian Cygert, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de Weijer*

Main category: cs.LG

TL;DR: The paper proposes an isotropic merging framework to improve model merging by aligning singular components of task matrices, reducing the performance gap between merged and single-task models.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between merged multi-task models and single-task models by analyzing task matrices.

Method: Propose an isotropic merging framework that flattens singular value spectra, enhances alignment, and incorporates common and task-specific subspaces.

Result: Achieves state-of-the-art performance on vision and language tasks across various tasks and model scales.

Conclusion: The work advances understanding of model merging dynamics and offers an effective merging methodology without additional training.

Abstract: Model merging integrates the weights of multiple task-specific models into a
single multi-task model. Despite recent interest in the problem, a significant
performance gap between the combined and single-task models remains. In this
paper, we investigate the key characteristics of task matrices -- weight update
matrices applied to a pre-trained model -- that enable effective merging. We
show that alignment between singular components of task-specific and merged
matrices strongly correlates with performance improvement over the pre-trained
model. Based on this, we propose an isotropic merging framework that flattens
the singular value spectrum of task matrices, enhances alignment, and reduces
the performance gap. Additionally, we incorporate both common and task-specific
subspaces to further improve alignment and performance. Our proposed approach
achieves state-of-the-art performance on vision and language tasks across
various sets of tasks and model scales. This work advances the understanding of
model merging dynamics, offering an effective methodology to merge models
without requiring additional training. Code is available at
https://github.com/danielm1405/iso-merging .

</details>


### [717] [Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation](https://arxiv.org/pdf/2502.06516)
*Soobin Um, Beomsu Kim, Jong Chul Ye*

Main category: cs.LG

TL;DR: A guidance-free method called Boost-and-Skip enhances minority sample generation in diffusion models with minimal changes: variance-boosted initialization and timestep skipping.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based minority generators are computationally expensive due to reliance on guidance.

Method: Proposes Boost-and-Skip, requiring only variance-boosted initialization and timestep skipping.

Result: Effectively generates minority samples, rivaling guidance-based methods with fewer computations.

Conclusion: Boost-and-Skip is a simple, efficient, and powerful approach for minority sample generation.

Abstract: Minority samples are underrepresented instances located in low-density
regions of a data manifold, and are valuable in many generative AI
applications, such as data augmentation, creative content generation, etc.
Unfortunately, existing diffusion-based minority generators often rely on
computationally expensive guidance dedicated for minority generation. To
address this, here we present a simple yet powerful guidance-free approach
called Boost-and-Skip for generating minority samples using diffusion models.
The key advantage of our framework requires only two minimal changes to
standard generative processes: (i) variance-boosted initialization and (ii)
timestep skipping. We highlight that these seemingly-trivial modifications are
supported by solid theoretical and empirical evidence, thereby effectively
promoting emergence of underrepresented minority features. Our comprehensive
experiments demonstrate that Boost-and-Skip greatly enhances the capability of
generating minority samples, even rivaling guidance-based state-of-the-art
approaches while requiring significantly fewer computations. Code is available
at https://github.com/soobin-um/BnS.

</details>


### [718] [DGenNO: A Novel Physics-aware Neural Operator for Solving Forward and Inverse PDE Problems based on Deep, Generative Probabilistic Modeling](https://arxiv.org/pdf/2502.06250)
*Yaohua Zang, Phaedon-Stelios Koutsourelakis*

Main category: cs.LG

TL;DR: DGenNO is a physics-aware framework combining generative models and latent variables to solve high-dimensional, discontinuous PDE problems with unlabeled data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural operator methods struggle with high-dimensional, discontinuous inputs and require large labeled datasets. DGenNO aims to address these limitations.

Method: DGenNO uses a deep generative probabilistic model with latent variables, incorporates physics constraints via weak-form residuals, and introduces MultiONet for enhanced approximation.

Result: DGenNO achieves higher accuracy, robustness to noise, and strong generalization, especially for multi-phase media and inverse problems.

Conclusion: DGenNO is a powerful, adaptable tool for scientific and engineering applications, handling sparse, noisy data and providing probabilistic estimates.

Abstract: Solving parametric partial differential equations (PDEs) and associated
PDE-based, inverse problems is a central task in engineering and physics, yet
existing neural operator methods struggle with high-dimensional, discontinuous
inputs and require large amounts of {\em labeled} training data. We propose the
Deep Generative Neural Operator (DGenNO), a physics-aware framework that
addresses these challenges by leveraging a deep, generative, probabilistic
model in combination with a set of lower-dimensional, latent variables that
simultaneously encode PDE-inputs and PDE-outputs. This formulation can make use
of unlabeled data and significantly improves inverse problem-solving,
particularly for discontinuous or discrete-valued input functions. DGenNO
enforces physics constraints without labeled data by incorporating as virtual
observables, weak-form residuals based on compactly supported radial basis
functions (CSRBFs). These relax regularity constraints and eliminate
higher-order derivatives from the objective function. We also introduce
MultiONet, a novel neural operator architecture, which is a more expressive
generalization of the popular DeepONet that significantly enhances the
approximating power of the proposed model. These innovations make DGenNO
particularly effective for challenging forward and inverse, PDE-based problems,
such as those involving multi-phase media. Numerical experiments demonstrate
that DGenNO achieves higher accuracy across multiple benchmarks while
exhibiting robustness to noise and strong generalization to out-of-distribution
cases. Its adaptability, and the ability to handle sparse, noisy data while
providing probabilistic estimates, make DGenNO a powerful tool for scientific
and engineering applications.

</details>


### [719] [Neural Force Field: Few-shot Learning of Generalized Physical Reasoning](https://arxiv.org/pdf/2502.08987)
*Shiqian Li, Ruihong Shen, Yaoyu Tao, Chi Zhang, Yixin Zhu*

Main category: cs.LG

TL;DR: Neural Force Field (NFF) extends Neural ODEs to learn physical dynamics via force fields, enabling strong generalization from minimal data.


<details>
  <summary>Details</summary>
Motivation: Current AI models struggle with physical reasoning and OOD generalization due to lack of abstraction of core physical principles.

Method: NFF uses continuous explicit force fields to capture physics concepts (e.g., gravity, collision) and integrates them via ODE solvers for trajectory prediction.

Result: NFF achieves strong generalization on unseen scenarios with few training examples and supports efficient planning and adaptation.

Conclusion: Physics-inspired representations like NFF can narrow the gap between AI and human physical reasoning.

Abstract: Physical reasoning is a remarkable human ability that enables rapid learning
and generalization from limited experience. Current AI models, despite
extensive training, still struggle to achieve similar generalization,
especially in Out-of-distribution (OOD) settings. This limitation stems from
their inability to abstract core physical principles from observations. A key
challenge is developing representations that can efficiently learn and
generalize physical dynamics from minimal data. Here we present Neural Force
Field (NFF), a framework extending Neural Ordinary Differential Equation (NODE)
to learn complex object interactions through force field representations, which
can be efficiently integrated through an Ordinary Differential Equation (ODE)
solver to predict object trajectories. Unlike existing approaches that rely on
discrete latent spaces, NFF captures fundamental physical concepts such as
gravity, support, and collision in continuous explicit force fields.
Experiments on three challenging physical reasoning tasks demonstrate that NFF,
trained with only a few examples, achieves strong generalization to unseen
scenarios. This physics-grounded representation enables efficient
forward-backward planning and rapid adaptation through interactive refinement.
Our work suggests that incorporating physics-inspired representations into
learning systems can help bridge the gap between artificial and human physical
reasoning capabilities.

</details>


### [720] [Generating Samples to Question Trained Models](https://arxiv.org/pdf/2502.06658)
*Eren Mehmet Kıral, Nurşen Aydın, Ş. İlker Birbil*

Main category: cs.LG

TL;DR: A framework is proposed to analyze machine learning models by identifying their preferred data samples in scenarios like risky predictions or model contrasts.


<details>
  <summary>Details</summary>
Motivation: To understand how trained machine learning models operate by examining their data preferences.

Method: A mathematical framework is introduced to probe models and identify preferred samples in specific scenarios.

Result: The framework is tested on various models for classification and regression tasks, generating data-based answers.

Conclusion: The work provides a tool to investigate and interpret machine learning models through their data preferences.

Abstract: There is a growing need for investigating how machine learning models
operate. With this work, we aim to understand trained machine learning models
by questioning their data preferences. We propose a mathematical framework that
allows us to probe trained models and identify their preferred samples in
various scenarios including prediction-risky, parameter-sensitive, or
model-contrastive samples. To showcase our framework, we pose these queries to
a range of models trained on a range of classification and regression tasks,
and receive answers in the form of generated data.

</details>


### [721] [RaaS: Reasoning-Aware Attention Sparsity for Efficient LLM Reasoning](https://arxiv.org/pdf/2502.11147)
*Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, Yizhou Shan*

Main category: cs.LG

TL;DR: The paper proposes RaaS, a new algorithm for LLMs that addresses the 'impossible trinity' of accuracy, time, and memory by identifying and managing milestone tokens during reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Current sparsity-based algorithms for LLMs struggle to balance accuracy, time, and memory efficiency, especially in reasoning tasks requiring long sequences.

Method: The authors identify a new attention pattern involving milestone tokens and develop RaaS, which retains KV vectors of these tokens only until they are no longer needed.

Result: RaaS achieves high accuracy with O(L) time and memory complexities, outperforming existing methods like Quest.

Conclusion: RaaS effectively balances accuracy, time, and memory, offering a scalable solution for LLM reasoning tasks.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities across
various domains, with recent advancements in challenging reasoning tasks such
as mathematics and programming. However, solving reasoning tasks often requires
an LLM to generate long sequences, incurring $O(N)$ time and memory
complexities per token, where $N$ is the current sequence length. To reduce
complexities, existing sparsity-based algorithms propose to retain Key-Value
(KV) vectors, the intermediate representations of only the most critical
tokens. However, these algorithms struggle with the "impossible trinity" of
accuracy, time, and memory. For example, the state-of-the-art algorithm, Quest,
achieves high accuracy with $O(L)$ time but $O(N)$ memory ($L$ is the cache
budget, $L \ll N$). To address the "impossible trinity", in this paper, we
identify a new attention pattern during the decode stage of reasoning tasks,
where milestone tokens (analogous to lemmas in mathematical proofs) emerge, are
utilized, and then become unimportant afterward. Based on this pattern, we
propose a new algorithm RaaS that identifies milestone tokens and retains their
KV vectors until they are no longer needed, achieving high accuracy with $O(L)$
time and $O(L)$ memory complexities.

</details>


### [722] [Active Advantage-Aligned Online Reinforcement Learning with Offline Data](https://arxiv.org/pdf/2502.07937)
*Xuefeng Liu, Hung T. C. Le, Siyu Chen, Rick Stevens, Zhuoran Yang, Matthew R. Walter, Yuxin Chen*

Main category: cs.LG

TL;DR: A3RL integrates online and offline RL with a novel active sampling strategy to optimize policy improvement, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in combining online and offline RL, such as catastrophic forgetting and limited sample efficiency.

Method: Introduces A3RL with a confidence-aware Active Advantage Aligned (A3) sampling strategy to dynamically prioritize data from both online and offline sources.

Result: Outperforms competing online RL techniques leveraging offline data, supported by theoretical insights and empirical experiments.

Conclusion: A3RL effectively combines online and offline RL, offering improved policy learning and robustness.

Abstract: Online reinforcement learning (RL) enhances policies through direct
interactions with the environment, but faces challenges related to sample
efficiency. In contrast, offline RL leverages extensive pre-collected data to
learn policies, but often produces suboptimal results due to limited data
coverage. Recent efforts integrate offline and online RL in order to harness
the advantages of both approaches. However, effectively combining online and
offline RL remains challenging due to issues that include catastrophic
forgetting, lack of robustness to data quality and limited sample efficiency in
data utilization. In an effort to address these challenges, we introduce A3RL,
which incorporates a novel confidence aware Active Advantage Aligned (A3)
sampling strategy that dynamically prioritizes data aligned with the policy's
evolving needs from both online and offline sources, optimizing policy
improvement. Moreover, we provide theoretical insights into the effectiveness
of our active sampling strategy and conduct diverse empirical experiments and
ablation studies, demonstrating that our method outperforms competing online RL
techniques that leverage offline data. Our code will be publicly available
at:https://github.com/xuefeng-cs/A3RL.

</details>


### [723] [Can Classic GNNs Be Strong Baselines for Graph-level Tasks? Simple Architectures Meet Excellence](https://arxiv.org/pdf/2502.09263)
*Yuankai Luo, Lei Shi, Xiao-Ming Wu*

Main category: cs.LG

TL;DR: GNN+ framework enhances classic GNNs with six techniques, outperforming Graph Transformers in graph-level tasks while being more efficient.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of GNNs (expressiveness, over-smoothing, long-range dependencies) and challenge the superiority of Graph Transformers.

Method: Enhanced GNNs (GCN, GIN, GatedGCN) with GNN+ framework (edge features, normalization, dropout, residuals, feed-forward networks, positional encoding) and evaluated on 14 datasets.

Result: Classic GNNs with GNN+ matched or surpassed GTs, ranking top-three in all datasets and first in eight, while being faster.

Conclusion: Simple GNN architectures can outperform complex GTs, challenging the necessity of global attention mechanisms for graph-level tasks.

Abstract: Message-passing Graph Neural Networks (GNNs) are often criticized for their
limited expressiveness, issues like over-smoothing and over-squashing, and
challenges in capturing long-range dependencies. Conversely, Graph Transformers
(GTs) are regarded as superior due to their employment of global attention
mechanisms, which potentially mitigate these challenges. Literature frequently
suggests that GTs outperform GNNs in graph-level tasks, especially for graph
classification and regression on small molecular graphs. In this study, we
explore the untapped potential of GNNs through an enhanced framework, GNN+,
which integrates six widely used techniques: edge feature integration,
normalization, dropout, residual connections, feed-forward networks, and
positional encoding, to effectively tackle graph-level tasks. We conduct a
systematic re-evaluation of three classic GNNs (GCN, GIN, and GatedGCN)
enhanced by the GNN+ framework across 14 well-known graph-level datasets. Our
results reveal that, contrary to prevailing beliefs, these classic GNNs
consistently match or surpass the performance of GTs, securing top-three
rankings across all datasets and achieving first place in eight. Furthermore,
they demonstrate greater efficiency, running several times faster than GTs on
many datasets. This highlights the potential of simple GNN architectures,
challenging the notion that complex mechanisms in GTs are essential for
superior graph-level performance. Our source code is available at
https://github.com/LUOyk1999/GNNPlus.

</details>


### [724] [Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options](https://arxiv.org/pdf/2502.12929)
*Lakshmi Nair, Ian Trase, Mark Kim*

Main category: cs.LG

TL;DR: Flow-of-Options (FoO) is a novel reasoning approach for LLMs to reduce biases and explore diverse solutions, improving performance on ML tasks with low cost.


<details>
  <summary>Details</summary>
Motivation: Address intrinsic biases in LLMs and enable systematic exploration of diverse reasoning paths.

Method: Developed an FoO-based agentic framework using compressed, interpretable task representations to enforce solution diversity.

Result: Improved performance by 38.2%-69.2% on data science tasks and 37.4%-47.9% on therapeutic chemistry tasks, with costs under $1 per task.

Conclusion: FoO is effective, cost-efficient, and broadly applicable beyond tabular tasks, including reinforcement learning and image generation.

Abstract: We present a novel reasoning approach called Flow-of-Options (FoO), designed
to address intrinsic biases in Large Language Models (LLMs). Flow-of-Options
enables LLMs to systematically explore a diverse range of possibilities in
their reasoning, as demonstrated by an FoO-based agentic framework developed
for autonomously solving Machine Learning (ML) tasks. FoO enforces diversity in
LLM solutions through compressed and interpretable task representations,
resulting in improvements of 38.2% - 69.2% on standard data science tasks, and
37.4% - 47.9% on therapeutic chemistry tasks, as compared to state-of-the-art
baselines. With an overall operation cost under $1 per task, our framework is
well-suited for cost-sensitive applications. Going beyond tabular
classification and regression, we show the broader applicability of our
FoO-based agentic system to tasks such as reinforcement learning and image
generation. Our code is open-sourced at:
https://github.com/flagshippioneering/Flow-of-Options.

</details>


### [725] [Hyper-SET: Designing Transformers via Hyperspherical Energy Minimization](https://arxiv.org/pdf/2502.11646)
*Yunzhe Hu, Difan Zou, Dong Xu*

Main category: cs.LG

TL;DR: The paper introduces Hyper-SET, a principled Transformer model based on energy minimization, offering interpretability and competitive performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Current Transformer layers are heuristic-driven; the paper aims to create a prototypical model with high interpretability and practical competence.

Method: Formalizes token dynamics as joint maximum likelihood estimation on a hypersphere, using energy-based interpretation to design symmetric attention and feedforward modules.

Result: Hyper-SET achieves competitive or superior performance in tasks like Sudoku solving, image classification, and masked image modeling, with fewer parameters.

Conclusion: Hyper-SET represents a step toward interpretable and principled Transformer design, scalable and adaptable to various tasks.

Abstract: Transformer-based models have achieved remarkable success, but their core
components, Transformer layers, are largely heuristics-driven and engineered
from the bottom up, calling for a prototypical model with high interpretability
and practical competence. To this end, we conceptualize a principled, top-down
approach grounded in energy-based interpretation. Specifically, we formalize
token dynamics as a joint maximum likelihood estimation on the hypersphere,
featuring two properties: semantic alignment in the high-dimensional space and
distributional uniformity in the low-dimensional space. By quantifying them
with extended Hopfield energy functions, we instantiate this idea as a
constrained energy minimization problem, which enables designs of symmetric
attention and feedforward modules with RMS normalization. We further present
\textit{Hyper-Spherical Energy Transformer} (Hyper-SET), a recurrent-depth
alternative to vanilla Transformers naturally emerging from iterative energy
optimization on the hypersphere. With shared parameters across layers,
Hyper-SET can scale to arbitrary depth with fewer parameters. Theoretically
grounded and compact, it achieves competitive or superior performance across
diverse tasks, including Sudoku solving, image classification, and masked image
modeling. We also design novel variations under the proposed general principle,
such as linear attention and gated feedforward layer. Moreover, we showcase its
scalability with depth-wise LoRA. Our results highlight Hyper-SET as a step
toward interpretable and principled Transformer design.

</details>


### [726] [SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?](https://arxiv.org/pdf/2502.12115)
*Samuel Miserendino, Michele Wang, Tejal Patwardhan, Johannes Heidecke*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce SWE-Lancer, a benchmark of over 1,400 freelance software
engineering tasks from Upwork, valued at \$1 million USD total in real-world
payouts. SWE-Lancer encompasses both independent engineering tasks--ranging
from \$50 bug fixes to \$32,000 feature implementations--and managerial tasks,
where models choose between technical implementation proposals. Independent
tasks are graded with end-to-end tests triple-verified by experienced software
engineers, while managerial decisions are assessed against the choices of the
original hired engineering managers. We evaluate model performance and find
that frontier models are still unable to solve the majority of tasks. To
facilitate future research, we open-source a unified Docker image and a public
evaluation split, SWE-Lancer Diamond
(https://github.com/openai/SWELancer-Benchmark). By mapping model performance
to monetary value, we hope SWE-Lancer enables greater research into the
economic impact of AI model development.

</details>


### [727] [PairBench: Are Vision-Language Models Reliable at Comparing What They See?](https://arxiv.org/pdf/2502.15210)
*Aarash Feizi, Sai Rajeswar, Adriana Romero-Soriano, Reihaneh Rabbany, Valentina Zantedeschi, Spandana Gella, João Monteiro*

Main category: cs.LG

TL;DR: PairBench is a framework to evaluate vision-language models (VLMs) for comparative tasks, introducing four metrics: alignment, consistency, smoothness, and controllability. No model excels in all, and symmetry in similarity scores is often lacking.


<details>
  <summary>Details</summary>
Motivation: Assessing VLMs' comparative capabilities is crucial for applications like evaluation, re-ranking, and retrieval-augmented generation, but lacks systematic evaluation.

Method: PairBench uses image datasets to evaluate VLMs with four metrics: alignment with human annotations, pair ordering consistency, distribution smoothness, and controllability via prompting.

Result: No model performs consistently across all metrics; symmetry in similarity scores is a common weakness. Performance correlates with complex task benchmarks.

Conclusion: PairBench provides a unique, comprehensive framework for evaluating VLMs in comparative tasks, highlighting their strengths and weaknesses.

Abstract: Understanding how effectively large vision language models (VLMs) compare
visual inputs is crucial across numerous applications, yet this fundamental
capability remains insufficiently assessed. While VLMs are increasingly
deployed for tasks requiring comparative judgment, including automated
evaluation, re-ranking, and retrieval-augmented generation, no systematic
framework exists to measure their performance in these scenarios. We present
PairBench, a simple framework that evaluates VLMs as customizable similarity
tools using widely available image datasets. Our approach introduces four key
metrics for reliable comparison: alignment with human annotations, consistency
across pair ordering, distribution smoothness, and controllability through
prompting. Our analysis reveals that no model consistently excels across all
metrics, with each demonstrating distinct strengths and weaknesses. Most
concerning is the widespread inability of VLMs to maintain symmetric similarity
scores. Interestingly, we demonstrate that performance on our benchmark
strongly correlates with popular benchmarks used for more complex tasks, while
providing additional metrics into controllability, smoothness and ordering.
This makes PairBench a unique and comprehensive framework to evaluate the
performance of VLMs for automatic evaluation depending on the task.

</details>


### [728] [Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models](https://arxiv.org/pdf/2502.15678)
*Luca M. Schulze Buschoff, Konstantinos Voudouris, Elif Akata, Matthias Bethge, Joshua B. Tenenbaum, Eric Schulz*

Main category: cs.LG

TL;DR: Fine-tuning pre-trained vision language models with human visual cognition tasks improves domain-specific performance but lacks robust generalization to other tasks or domains.


<details>
  <summary>Details</summary>
Motivation: To enhance visual cognition in models and align them with human behavior by incorporating visual stimuli and human judgments.

Method: Fine-tuning models on ground truth data for intuitive physics and causal reasoning tasks.

Result: Improved performance in fine-tuned domains and better alignment with human behavior, but no robust generalization to other tasks or domains.

Conclusion: Task-specific fine-tuning enhances domain performance but fails to achieve human-like generalization across diverse visual or cognitive tasks.

Abstract: Pre-trained vision language models still fall short of human visual
cognition. In an effort to improve visual cognition and align models with human
behavior, we introduce visual stimuli and human judgments on visual cognition
tasks, allowing us to systematically evaluate performance across cognitive
domains under a consistent environment. We fine-tune models on ground truth
data for intuitive physics and causal reasoning and find that this improves
model performance in the respective fine-tuning domain. Furthermore, it can
improve model alignment with human behavior. However, we find that
task-specific fine-tuning does not contribute to robust human-like
generalization to data with other visual characteristics or to tasks in other
cognitive domains.

</details>


### [729] [Adaptive Conformal Guidance: A Framework for Multi-Domain Learning under Uncertainty](https://arxiv.org/pdf/2502.16736)
*Rui Liu, Peng Gao, Yu Shen, Ming Lin, Pratap Tokekar*

Main category: cs.LG

TL;DR: AdaConG is a framework that dynamically adjusts guidance influence based on uncertainty, improving learning performance under noisy or imperfect guidance.


<details>
  <summary>Details</summary>
Motivation: Guidance signals (e.g., annotations, pseudo-labels) can be noisy or misaligned, degrading model performance. AdaConG addresses this by quantifying and adapting to uncertainty.

Method: AdaConG uses split conformal prediction to quantify guidance uncertainty and dynamically modulates its influence. It is tested in tasks like knowledge distillation and autonomous driving.

Result: AdaConG outperforms baselines, e.g., achieving 6× higher rewards in gridworld navigation and enhancing robustness across domains.

Conclusion: AdaConG is a simple, effective solution for learning under uncertain guidance, validated across diverse tasks.

Abstract: Learning with guidance has proven effective across a wide range of machine
learning domains. Guidance may, for example, come from annotated datasets in
supervised learning, pseudo-labels in semi-supervised learning, and expert
demonstration policies in reinforcement learning. However, guidance signals can
be noisy due to domain shifts and limited data availability and may not
generalize well. Blindly trusting such signals when they are noisy, incomplete,
or misaligned with the target domain can lead to degraded performance. To
address these challenges, we propose $\underline{Ada}$ptive
$\underline{Con}$formal $\underline{G}$uidance (AdaConG), a universal,
plug-and-play framework that dynamically modulates the influence of guidance
signals based on their associated uncertainty, quantified via split conformal
prediction (CP). By adaptively adjusting to guidance uncertainty, AdaConG
enables models to reduce reliance on potentially misleading signals and enhance
learning performance. We validate AdaConG across diverse domains and tasks,
including knowledge distillation, semi-supervised image classification,
gridworld navigation, and autonomous driving. Experimental results demonstrate
that AdaConG improves performance and robustness under imperfect guidance,
e.g., in gridworld navigation, it accelerates convergence and achieves over
$6\times$ higher rewards than the best-performing baseline. These results
highlight AdaConG as a simple yet effective solution for multi-domain learning
under uncertainty.

</details>


### [730] [Model-Based Exploration in Monitored Markov Decision Processes](https://arxiv.org/pdf/2502.16772)
*Alireza Kazemipour, Simone Parisi, Matthew E. Taylor, Michael Bowling*

Main category: cs.LG

TL;DR: The paper introduces a model-based algorithm for Monitored Markov Decision Processes (Mon-MDPs) to address limitations of prior methods, demonstrating faster convergence and providing finite-sample performance guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing Mon-MDP algorithms fail to fully exploit problem structure, lack worst-case guarantees, and only offer asymptotic convergence proofs.

Method: A model-based algorithm using two instances of model-based interval estimation: one for capturing observable rewards and another for learning the minimax-optimal policy.

Result: Faster convergence in benchmarks, especially with known monitoring processes, and finite-sample bounds on performance.

Conclusion: The proposed algorithm improves upon prior work by addressing key limitations and providing robust guarantees, even when some rewards are unobservable.

Abstract: A tenet of reinforcement learning is that the agent always observes rewards.
However, this is not true in many realistic settings, e.g., a human observer
may not always be available to provide rewards, sensors may be limited or
malfunctioning, or rewards may be inaccessible during deployment. Monitored
Markov decision processes (Mon-MDPs) have recently been proposed to model such
settings. However, existing Mon-MDP algorithms have several limitations: they
do not fully exploit the problem structure, cannot leverage a known monitor,
lack worst-case guarantees for 'unsolvable' Mon-MDPs without specific
initialization, and offer only asymptotic convergence proofs. This paper makes
three contributions. First, we introduce a model-based algorithm for Mon-MDPs
that addresses these shortcomings. The algorithm employs two instances of
model-based interval estimation: one to ensure that observable rewards are
reliably captured, and another to learn the minimax-optimal policy. Second, we
empirically demonstrate the advantages. We show faster convergence than prior
algorithms in over four dozen benchmarks, and even more dramatic improvement
when the monitoring process is known. Third, we present the first finite-sample
bound on performance. We show convergence to a minimax-optimal policy even when
some rewards are never observable.

</details>


### [731] [Recurrent Knowledge Identification and Fusion for Language Model Continual Learning](https://arxiv.org/pdf/2502.17510)
*Yujie Feng, Xujia Wang, Zexin Lu, Shenghong Fu, Guangyuan Shi, Yongxin Xu, Yasha Wang, Philip S. Yu, Xu Chu, Xiao-Ming Wu*

Main category: cs.LG

TL;DR: Recurrent-KIF is a novel CL framework for dynamic parameter importance estimation, improving knowledge transfer in LLMs by iterative inner-outer loop fusion.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing knowledge transfer and forgetting in continual learning for LLMs, overcoming static importance estimates.

Method: Uses inner loop for task adaptation and parameter importance identification, and outer loop for global knowledge fusion via pruning and merging.

Result: Effectively mitigates catastrophic forgetting and enhances knowledge transfer across various model sizes (770M to 13B).

Conclusion: Recurrent-KIF offers a dynamic, iterative approach to continual learning, outperforming static methods in real-world LLM deployment.

Abstract: Continual learning (CL) is crucial for deploying large language models (LLMs)
in dynamic real-world environments without costly retraining. While recent
model ensemble and model merging methods guided by parameter importance have
gained popularity, they often struggle to balance knowledge transfer and
forgetting, mainly due to the reliance on static importance estimates during
sequential training. In this paper, we present Recurrent-KIF, a novel CL
framework for Recurrent Knowledge Identification and Fusion, which enables
dynamic estimation of parameter importance distributions to enhance knowledge
transfer. Inspired by human continual learning, Recurrent-KIF employs an inner
loop that rapidly adapts to new tasks while identifying important parameters,
coupled with an outer loop that globally manages the fusion of new and
historical knowledge through redundant knowledge pruning and key knowledge
merging. These inner-outer loops iteratively perform multiple rounds of fusion,
allowing Recurrent-KIF to leverage intermediate training information and
adaptively adjust fusion strategies based on evolving importance distributions.
Extensive experiments on two CL benchmarks with various model sizes (from 770M
to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic
forgetting and enhances knowledge transfer.

</details>


### [732] [On Traceability in $\ell_p$ Stochastic Convex Optimization](https://arxiv.org/pdf/2502.17384)
*Sasha Voitovych, Mahdi Haghifam, Idan Attias, Gintare Karolina Dziugaite, Roi Livni, Daniel M. Roy*

Main category: cs.LG

TL;DR: The paper explores the tradeoff between traceability and excess risk in stochastic convex optimization (SCO) under ℓ_p geometries, revealing thresholds for traceability and connections to differentially private algorithms.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between traceability (identifying training samples from a learning algorithm's output) and excess risk in SCO, and to bridge gaps in understanding DP learning bounds.

Method: The study establishes thresholds for traceability and excess risk, proving a sparse variant of the fingerprinting lemma to support the findings.

Result: For p∈[1,2], a sharp phase transition exists where traceability aligns with DP algorithms' best excess risk. For p∈(2,∞), new lower bounds for DP learning are provided.

Conclusion: The work highlights a fundamental tradeoff in SCO, advances understanding of DP learning, and introduces a useful lemma for the community.

Abstract: In this paper, we investigate the necessity of traceability for accurate
learning in stochastic convex optimization (SCO) under $\ell_p$ geometries.
Informally, we say a learning algorithm is $m$-traceable if, by analyzing its
output, it is possible to identify at least $m$ of its training samples. Our
main results uncover a fundamental tradeoff between traceability and excess
risk in SCO. For every $p\in [1,\infty)$, we establish the existence of an
excess risk threshold below which every sample-efficient learner is traceable
with the number of samples which is a constant fraction of its training sample.
For $p\in [1,2]$, this threshold coincides with the best excess risk of
differentially private (DP) algorithms, i.e., above this threshold, there exist
algorithms that are not traceable, which corresponds to a sharp phase
transition. For $p \in (2,\infty)$, this threshold instead gives novel lower
bounds for DP learning, partially closing an open problem in this setup. En
route to establishing these results, we prove a sparse variant of the
fingerprinting lemma, which is of independent interest to the community.

</details>


### [733] [BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction](https://arxiv.org/pdf/2502.18807)
*Ruifeng Tan, Weixiang Hong, Jiayue Tang, Xibin Lu, Ruijun Ma, Xiang Zheng, Jia Li, Jiaqiang Huang, Tong-Yi Zhang*

Main category: cs.LG

TL;DR: The paper introduces BatteryLife, a comprehensive dataset and benchmark for Battery Life Prediction (BLP), addressing challenges like limited dataset size, lack of diversity, and inconsistent benchmarks. It also proposes CyclePatch, a plug-in technique to improve model performance.


<details>
  <summary>Details</summary>
Motivation: The research addresses three key challenges in BLP: limited dataset size, lack of diversity in existing datasets, and inconsistent benchmarks, which hinder progress in the field.

Method: The authors propose BatteryLife, integrating 16 datasets with diverse battery types and conditions, and introduce CyclePatch, a plug-in technique for neural networks.

Result: BatteryLife offers a 2.5 times larger sample size and diverse data. CyclePatch improves model performance, establishing state-of-the-art benchmarks.

Conclusion: BatteryLife and CyclePatch provide a robust solution for BLP, demonstrating that popular time series models may not suit BLP and highlighting the need for domain-specific approaches.

Abstract: Battery Life Prediction (BLP), which relies on time series data produced by
battery degradation tests, is crucial for battery utilization, optimization,
and production. Despite impressive advancements, this research area faces three
key challenges. Firstly, the limited size of existing datasets impedes insights
into modern battery life data. Secondly, most datasets are restricted to
small-capacity lithium-ion batteries tested under a narrow range of diversity
in labs, raising concerns about the generalizability of findings. Thirdly,
inconsistent and limited benchmarks across studies obscure the effectiveness of
baselines and leave it unclear if models popular in other time series fields
are effective for BLP. To address these challenges, we propose BatteryLife, a
comprehensive dataset and benchmark for BLP. BatteryLife integrates 16
datasets, offering a 2.5 times sample size compared to the previous largest
dataset, and provides the most diverse battery life resource with batteries
from 8 formats, 59 chemical systems, 9 operating temperatures, and 421
charge/discharge protocols, including both laboratory and industrial tests.
Notably, BatteryLife is the first to release battery life datasets of zinc-ion
batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion
batteries. With the comprehensive dataset, we revisit the effectiveness of
baselines popular in this and other time series fields. Furthermore, we propose
CyclePatch, a plug-in technique that can be employed in various neural
networks. Extensive benchmarking of 18 methods reveals that models popular in
other time series fields can be unsuitable for BLP, and CyclePatch consistently
improves model performance establishing state-of-the-art benchmarks. Moreover,
BatteryLife evaluates model performance across aging conditions and domains.
BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.

</details>


### [734] [Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance](https://arxiv.org/pdf/2502.18891)
*Ziyuan Zhong, Junyang Zhou*

Main category: cs.LG

TL;DR: A dynamic classification algorithm achieves zero missed detections and minimal false positives by partitioning data into subareas with self-supervised learning, refining predictions without extra models, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the critical need for high accuracy in safety-critical domains like medical diagnostics, where undetected cases can have severe consequences.

Method: Partitions data into subareas using self-supervised learning, trains models (e.g., linear or random forest) for each subarea, and refines predictions using subarea boundaries.

Result: Achieves zero missed detections and minimal false positives, outperforming models like XGBoost and LGBM, even with larger errors.

Conclusion: The algorithm shows exceptional performance but requires improvements in parameter tuning and efficiency; future work will enhance robustness and adaptability.

Abstract: In this study, we propose an innovative dynamic classification algorithm
aimed at achieving zero missed detections and minimal false positives,acritical
in safety-critical domains (e.g., medical diagnostics) where undetected cases
risk severe outcomes. The algorithm partitions data in a self-supervised
learning-generated way, which allows the model to learn from the training set
to understand the data distribution and thereby divides training set and test
set into N different subareas. The training and test subsets in the same
subarea will have nearly the same boundary. For each subarea, there will be the
same type of model, such as linear or random forest model, to predict the
results of that subareas. In addition, the algorithm uses subareas boundary to
refine predictions results and filter out substandard results without requiring
additional models. This approach allows each model to operate within a smaller
data range and remove the inaccurate prediction results, thereby improving
overall accuracy. Experimental results show that, with minimal data
partitioning errors, the algorithm achieves exceptional performance with zero
missed detections and minimal false positives, outperforming existing ensembles
like XGBoost or LGBM model. Even with larger classification errors, it remains
comparable to that of state-of-the-art models.
  Key innovations include self-supervised classification learning, small-range
subset predictions, and optimizing the prediction results and eliminate the
unqualified ones without the need for additional model support. Although the
algorithm still has room for improvement in automatic parameter tuning and
efficiency, it demonstrates outstanding performance across multiple datasets.
Future work will focus on optimizing the classification components to enhance
robustness and adaptability.

</details>


### [735] [Corporate Fraud Detection in Rich-yet-Noisy Financial Graph](https://arxiv.org/pdf/2502.19305)
*Shiqi Wang, Zhibo Zhang, Libing Fang, Cam-Tu Nguyen, Wenzhong Li*

Main category: cs.LG

TL;DR: The paper introduces a novel graph-based method, KeGCNR, to improve corporate fraud detection by addressing information overload and hidden fraud issues in financial graphs.


<details>
  <summary>Details</summary>
Motivation: Previous methods fail to integrate interactions in company networks effectively, and noisy data/labels hinder fraud detection.

Method: Proposes KeGCNR, a knowledge-enhanced GCN with robust two-stage learning, leveraging knowledge graph embeddings and two-stage training.

Result: Experiments show KeGCNR outperforms baselines in fraud detection effectiveness and robustness.

Conclusion: The method successfully addresses key challenges and improves fraud detection by leveraging interactions and robust learning.

Abstract: Corporate fraud detection aims to automatically recognize companies that
conduct wrongful activities such as fraudulent financial statements or illegal
insider trading. Previous learning-based methods fail to effectively integrate
rich interactions in the company network. To close this gap, we collect 18-year
financial records in China to form three graph datasets with fraud labels. We
analyze the characteristics of the financial graphs, highlighting two
pronounced issues: (1) information overload: the dominance of (noisy)
non-company nodes over company nodes hinders the message-passing process in
Graph Convolution Networks (GCN); and (2) hidden fraud: there exists a large
percentage of possible undetected violations in the collected data. The hidden
fraud problem will introduce noisy labels in the training dataset and
compromise fraud detection results. To handle such challenges, we propose a
novel graph-based method, namely, Knowledge-enhanced GCN with Robust Two-stage
Learning (${\rm KeGCN}_{R}$), which leverages Knowledge Graph Embeddings to
mitigate the information overload and effectively learns rich representations.
The proposed model adopts a two-stage learning method to enhance robustness
against hidden frauds. Extensive experimental results not only confirm the
importance of interactions but also show the superiority of ${\rm KeGCN}_{R}$
over a number of strong baselines in terms of fraud detection effectiveness and
robustness.

</details>


### [736] [IL-SOAR : Imitation Learning with Soft Optimistic Actor cRitic](https://arxiv.org/pdf/2502.19859)
*Stefano Viel, Luca Viano, Volkan Cevher*

Main category: cs.LG

TL;DR: SOAR is a framework for imitation learning using a primal-dual algorithm with actor-critic methods, improving performance and reducing required training episodes by half.


<details>
  <summary>Details</summary>
Motivation: To enhance imitation learning by introducing a framework that efficiently learns policies from expert demonstrations with provable guarantees and practical performance boosts.

Method: Uses a primal-dual algorithm alternating cost and policy updates, incorporating an actor-critic method with multiple critics to estimate uncertainty and drive exploration.

Result: In tabular settings, SOAR matches best-known results in ε. Practically, it improves performance of imitation learning algorithms like f-IRL, ML-IRL, and CSIL in MuJoCo environments.

Conclusion: SOAR significantly reduces the number of episodes needed to achieve performance, cutting it by half, while maintaining theoretical and practical benefits.

Abstract: This paper introduces the SOAR framework for imitation learning. SOAR is an
algorithmic template that learns a policy from expert demonstrations with a
primal dual style algorithm that alternates cost and policy updates. Within the
policy updates, the SOAR framework uses an actor critic method with multiple
critics to estimate the critic uncertainty and build an optimistic critic
fundamental to drive exploration. When instantiated in the tabular setting, we
get a provable algorithm with guarantees that matches the best known results in
$\epsilon$. Practically, the SOAR template is shown to boost consistently the
performance of imitation learning algorithms based on Soft Actor Critic such as
f-IRL, ML-IRL and CSIL in several MuJoCo environments. Overall, thanks to SOAR,
the required number of episodes to achieve the same performance is reduced by
half.

</details>


### [737] [BOSE: A Systematic Evaluation Method Optimized for Base Models](https://arxiv.org/pdf/2503.00812)
*Hongzhi Luan, Changxin Tian, Zhaoxin Huan, Xiaolu Zhang, Kunlong Chen, Zhiqiang Zhang, Jun Zhou*

Main category: cs.LG

TL;DR: The paper addresses unstable evaluation in base models and inconsistency with instruct models, proposing BOSE for improved evaluation stability and consistency.


<details>
  <summary>Details</summary>
Motivation: To tackle unstable evaluation during training and the performance gap between base and instruct models, which hinder reliable training guidance.

Method: Introduces BOSE with In-Context Light-instruction Prompt (ICLiP) for open tasks and Blank-ppl for multi-choice tasks, plus Kendall's rank correlation for stability measurement.

Result: BOSE improves evaluation stability and consistency between base and instruct models, offering better training guidance.

Conclusion: BOSE effectively addresses evaluation challenges, enhancing reliability in LLM training assessments.

Abstract: This paper poses two critical issues in evaluating base models (without
post-training): (1) Unstable evaluation during training: in the early stages of
pre-training, the models lack the capability to answer questions as required,
leading to unstable evaluation results. This instability makes it difficult to
provide solid conclusions to guide the training, especially for key experiments
such as data ablation and scaling law. (2) Inconsistency between base and
instruct models: base models generally exhibit poorer evaluation performance
compared to corresponding instruct models. This gap poses a challenge for
assessing whether a base model with better evaluation can truly lead to a
better instruct model. To address these issues, we propose Base model Oriented
Systematic Evaluation (BOSE), a method specifically designed to optimize the
evaluation of base models. Specifically, BOSE introduces two key innovations:
In-Context Light-instruction Prompt (ICLiP) for open-ended tasks and Blank-ppl
for multi-choice tasks with candidate options, which transforms the standard
perplexity (ppl) metric into a fill-in-the-blank format to mitigate early-stage
evaluation fluctuations. Furthermore, we are the first to propose Kendall's
rank correlation to quantitatively measure the evaluation stability and
consistency. Experimental results demonstrate that BOSE significantly enhances
both the stability of evaluations during pre-training and the consistency
between base and instruct models, thereby providing more reliable guidance for
the LLMs' training.

</details>


### [738] [A Materials Foundation Model via Hybrid Invariant-Equivariant Architectures](https://arxiv.org/pdf/2503.05771)
*Keqiang Yan, Montgomery Bohde, Andrii Kryvenko, Ziyu Xiang, Kaiji Zhao, Siya Zhu, Saagar Kolachina, Doğuhan Sarıtürk, Jianwen Xie, Raymundo Arroyave, Xiaoning Qian, Xiaofeng Qian, Shuiwang Ji*

Main category: cs.LG

TL;DR: HIENet is a hybrid invariant-equivariant MLIP model combining efficiency and performance, outperforming prior models.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between computational efficiency and accuracy in MLIPs by integrating invariant and equivariant architectures.

Method: Proposes HIENet, a hybrid model with invariant and equivariant message passing layers, ensuring physical constraints.

Result: Achieves state-of-the-art performance with significant computational speedups.

Conclusion: HIENet is efficient and effective for materials discovery, validated by benchmarks and tasks.

Abstract: Machine learning interatomic potentials (MLIPs) can predict energy, force,
and stress of materials and enable a wide range of downstream discovery tasks.
A key design choice in MLIPs involves the trade-off between invariant and
equivariant architectures. Invariant models offer computational efficiency but
may not perform as well, especially when predicting high-order outputs. In
contrast, equivariant models can capture high-order symmetries, but are
computationally expensive. In this work, we propose HIENet, a hybrid
invariant-equivariant materials interatomic potential model that integrates
both invariant and equivariant message passing layers, while provably
satisfying key physical constraints. HIENet achieves state-of-the-art
performance with considerable computational speedups over prior models.
Experimental results on both common benchmarks and downstream materials
discovery tasks demonstrate the efficiency and effectiveness of HIENet.

</details>


### [739] [FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs](https://arxiv.org/pdf/2503.17229)
*Albert Sawczyn, Jakub Binkowski, Denis Janiak, Bogdan Gabrys, Tomasz Kajdanowicz*

Main category: cs.LG

TL;DR: FactSelfCheck is a black-box sampling-based method for fine-grained fact-level hallucination detection in LLMs, outperforming sentence-level methods and improving factual correction by 35.5%.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of hallucinated content in LLMs, especially where factuality is critical, by enabling fine-grained fact-level detection.

Method: Represents text as knowledge graphs (triples), analyzes factual consistency across LLM responses, and computes hallucination scores without external resources.

Result: Competes with leading sentence-level methods, improves hallucination correction by 35.5%, and provides more detailed insights.

Conclusion: FactSelfCheck offers precise hallucination detection and correction, with a new dataset (FavaMultiSamples) for evaluating sampling-based methods.

Abstract: Large Language Models (LLMs) frequently generate hallucinated content, posing
significant challenges for applications where factuality is crucial. While
existing hallucination detection methods typically operate at the sentence
level or passage level, we propose FactSelfCheck, a novel black-box
sampling-based method that enables fine-grained fact-level detection. Our
approach represents text as knowledge graphs consisting of facts in the form of
triples. Through analyzing factual consistency across multiple LLM responses,
we compute fine-grained hallucination scores without requiring external
resources or training data. Our evaluation demonstrates that FactSelfCheck
performs competitively with leading sentence-level sampling-based methods while
providing more detailed insights. Most notably, our fact-level approach
significantly improves hallucination correction, achieving a 35.5% increase in
factual content compared to the baseline, while sentence-level SelfCheckGPT
yields only a 10.6% improvement. The granular nature of our detection enables
more precise identification and correction of hallucinated content.
Additionally, we contribute a new dataset for evaluating sampling-based methods
- FavaMultiSamples.

</details>


### [740] [Mirror Online Conformal Prediction with Intermittent Feedback](https://arxiv.org/pdf/2503.10345)
*Bowen Wang, Matteo Zecchin, Osvaldo Simeone*

Main category: cs.LG

TL;DR: IM-OCP is a new runtime calibration framework that integrates prior knowledge while maintaining coverage guarantees and achieving sub-linear regret, with minimal memory complexity.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between incorporating prior knowledge and maintaining coverage guarantees in online conformal prediction.

Method: Introduces intermittent mirror online conformal prediction (IM-OCP), featuring closed-form updates and minimal memory complexity, designed for intermittent feedback.

Result: IM-OCP maintains long-term coverage guarantees and achieves sub-linear regret.

Conclusion: IM-OCP effectively balances prior knowledge integration with coverage guarantees, offering a practical solution for runtime calibration.

Abstract: Online conformal prediction enables the runtime calibration of a pre-trained
artificial intelligence model using feedback on its performance. Calibration is
achieved through set predictions that are updated via online rules so as to
ensure long-term coverage guarantees. While recent research has demonstrated
the benefits of incorporating prior knowledge into the calibration process,
this has come at the cost of replacing coverage guarantees with less tangible
regret guarantees based on the quantile loss. This work introduces intermittent
mirror online conformal prediction (IM-OCP), a novel runtime calibration
framework that integrates prior knowledge, while maintaining long-term coverage
and achieving sub-linear regret. IM-OCP features closed-form updates with
minimal memory complexity, and is designed to operate under potentially
intermittent feedback.

</details>


### [741] [NdLinear: Don't Flatten! Building Superior Neural Architectures by Preserving N-D Structure](https://arxiv.org/pdf/2503.17353)
*Alex Reneau, Jerry Yao-Chieh Hu, Zhongfang Zhuang, Ting-Chun Liu, Xiang He, Judah Goldfeder, Nadav Timor, Allen G Roush, Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: NdLinear is a novel linear transformation for tensors that preserves multi-dimensional structure, enhancing performance while reducing parameters.


<details>
  <summary>Details</summary>
Motivation: To address the loss of cross-dimension information when flattening multi-dimensional data in neural networks.

Method: NdLinear applies transformations separately along each dimension of the input tensor, avoiding flattening.

Result: Achieves significant parameter reductions (e.g., 9x fewer in LoRA) and improved accuracy across diverse tasks.

Conclusion: NdLinear is a versatile, efficient replacement for standard linear layers, promising for next-gen neural architectures.

Abstract: Many high-impact machine learning tasks involve multi-dimensional data such
as images, volumetric medical scans, and multivariate time-series. Yet, most
neural architectures flatten these inputs, discarding critical cross-dimension
information. We introduce $\textbf{NdLinear}$, a novel linear transformation
that circumvents this destructive flattening by operating directly on tensors.
NdLinear applies transformations separately along each data dimension, thereby
preserving the native data structure. Extensive experiments demonstrate
NdLinear's capacity to significantly enhance representational power, achieve
dramatic parameter reductions (often by orders of magnitude), and maintain a
favorable computational profile. For instance, when applied to Large Language
Model finetuning, our $\textbf{NdLinear-LoRA}$ delivers comparable or improved
accuracy on reasoning tasks using up to $9\times$ fewer trainable parameters
than standard LoRA. These broad advantages of NdLinear are consistently
validated across diverse neural architectures (CNNs, RNNs, Transformers, MLPs)
and data domains, including vision, language, time-series, and tabular tasks.
As a versatile, drop-in replacement for standard linear layers, NdLinear
processes data in its original N-dimensional form, offering a foundational
component for developing more efficient and powerful next-generation neural
architectures.

</details>


### [742] [LEMMA: Learning from Errors for MatheMatical Advancement in LLMs](https://arxiv.org/pdf/2503.17439)
*Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, Lijun Wu*

Main category: cs.LG

TL;DR: LEMMA enhances LLMs' reasoning by learning from errors, using incorrect solutions and reflections to correct them, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on correct data, ignoring the value of error data, which may limit reflective ability. LEMMA addresses this gap.

Method: LEMMA constructs datasets with incorrect solutions, error types, and reflections to correct solutions, fine-tuning models to self-correct.

Result: LEMMA significantly improves performance over baselines by enabling autonomous error correction.

Conclusion: Learning from errors (LEMMA) effectively enhances LLMs' reasoning without external critique models.

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capability in solving mathematical problems. However, existing approaches
primarily focus on improving the quality of correct training data, e.g.,
distilling high-quality correct solutions from advanced models, neglecting the
value contained in error data, potentially hindering the model's reflective
ability. Though some studies attempt to leverage error data, they often involve
complex mechanisms, such as Monte Carlo Tree Search (MCTS) to explore error
nodes. In this work, we propose to enhance LLMs' reasoning ability by Learning
from Errors for Mathematical Advancement (LEMMA). LEMMA constructs data
consisting of an incorrect solution with an erroneous step and a reflection
connection to a correct solution for fine-tuning. Specifically, we
systematically analyze the model-generated error types and introduce an
error-type grounded mistake augmentation method to collect diverse and
representative errors. Correct solutions are either from fixing the errors or
generating a fresh start. Through a model-aware smooth reflection connection,
the erroneous solution is transferred to the correct one. By fine-tuning on the
constructed dataset, the model is able to self-correct errors autonomously
within the generation process without relying on external critique models.
Experimental results demonstrate that LEMMA achieves significant performance
improvements over other strong baselines.

</details>


### [743] [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/pdf/2504.02922)
*Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda*

Main category: cs.LG

TL;DR: The paper addresses issues in model diffing with Crosscoders, proposing Latent Scaling and BatchTopK loss to improve accuracy in identifying fine-tuning-introduced concepts.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of model diffing by addressing misattributions in Crosscoders, ensuring correct identification of concepts introduced during fine-tuning.

Method: Develops Latent Scaling to measure latent presence accurately and introduces BatchTopK loss for training Crosscoders, tested on Gemma 2 2B models.

Result: BatchTopK Crosscoder mitigates misattributions, identifies chat-specific latents (e.g., false information, personal question), and reveals nuanced refusal behaviors.

Conclusion: The work advances Crosscoder methodology, offering practical techniques for accurate model diffing and insights into chat-tuning effects.

Abstract: Model diffing is the study of how fine-tuning changes a model's
representations and internal algorithms. Many behaviors of interest are
introduced during fine-tuning, and model diffing offers a promising lens to
interpret such behaviors. Crosscoders are a recent model diffing method that
learns a shared dictionary of interpretable concepts represented as latent
directions in both the base and fine-tuned models, allowing us to track how
concepts shift or emerge during fine-tuning. Notably, prior work has observed
concepts with no direction in the base model, and it was hypothesized that
these model-specific latents were concepts introduced during fine-tuning.
However, we identify two issues which stem from the crosscoders L1 training
loss that can misattribute concepts as unique to the fine-tuned model, when
they really exist in both models. We develop Latent Scaling to flag these
issues by more accurately measuring each latent's presence across models. In
experiments comparing Gemma 2 2B base and chat models, we observe that the
standard crosscoder suffers heavily from these issues. Building on these
insights, we train a crosscoder with BatchTopK loss and show that it
substantially mitigates these issues, finding more genuinely chat-specific and
highly interpretable concepts. We recommend practitioners adopt similar
techniques. Using the BatchTopK crosscoder, we successfully identify a set of
chat-specific latents that are both interpretable and causally effective,
representing concepts such as $\textit{false information}$ and
$\textit{personal question}$, along with multiple refusal-related latents that
show nuanced preferences for different refusal triggers. Overall, our work
advances best practices for the crosscoder-based methodology for model diffing
and demonstrates that it can provide concrete insights into how chat-tuning
modifies model behavior.

</details>


### [744] [Modelling bounded rational decision-making through Wasserstein constraints](https://arxiv.org/pdf/2504.03743)
*Benjamin Patrick Evans, Leo Ardon, Sumitra Ganesh*

Main category: cs.LG

TL;DR: The paper proposes using Wasserstein distances to model bounded rational RL agents, addressing issues with entropy, KL-divergence, and mutual information in ordinal action spaces.


<details>
  <summary>Details</summary>
Motivation: Existing methods (entropy, KL-divergence, mutual information) have limitations in handling ordinal action spaces, such as ignoring biases, lacking action nearness, or being hard to estimate.

Method: The authors introduce an alternative approach using Wasserstein distances, which accounts for action nearness, supports low-probability actions, and is computationally simple.

Result: The proposed method effectively models bounded rationality, including decision stickiness and avoids rapid switches between distant actions.

Conclusion: Wasserstein distances provide a superior framework for bounded rational RL in ordinal action spaces, overcoming key limitations of traditional methods.

Abstract: Modelling bounded rational decision-making through information constrained
processing provides a principled approach for representing departures from
rationality within a reinforcement learning framework, while still treating
decision-making as an optimization process. However, existing approaches are
generally based on Entropy, Kullback-Leibler divergence, or Mutual Information.
In this work, we highlight issues with these approaches when dealing with
ordinal action spaces. Specifically, entropy assumes uniform prior beliefs,
missing the impact of a priori biases on decision-makings. KL-Divergence
addresses this, however, has no notion of "nearness" of actions, and
additionally, has several well known potentially undesirable properties such as
the lack of symmetry, and furthermore, requires the distributions to have the
same support (e.g. positive probability for all actions). Mutual information is
often difficult to estimate. Here, we propose an alternative approach for
modeling bounded rational RL agents utilising Wasserstein distances. This
approach overcomes the aforementioned issues. Crucially, this approach accounts
for the nearness of ordinal actions, modeling "stickiness" in agent decisions
and unlikeliness of rapidly switching to far away actions, while also
supporting low probability actions, zero-support prior distributions, and is
simple to calculate directly.

</details>


### [745] [Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models](https://arxiv.org/pdf/2504.05258)
*Adrián Bazaga, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert*

Main category: cs.LG

TL;DR: TISER enhances LLMs' temporal reasoning via timeline construction and iterative self-reflection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with temporal reasoning, which is crucial for applications like question answering and scheduling.

Method: TISER combines timeline construction with iterative self-reflection and test-time scaling to improve reasoning.

Result: TISER outperforms benchmarks, enabling smaller models to surpass larger ones in temporal reasoning.

Conclusion: TISER effectively boosts LLMs' temporal reasoning and inference traceability.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for generating
coherent text, understanding context, and performing reasoning tasks. However,
they struggle with temporal reasoning, which requires processing time-related
information such as event sequencing, durations, and inter-temporal
relationships. These capabilities are critical for applications including
question answering, scheduling, and historical analysis. In this paper, we
introduce TISER, a novel framework that enhances the temporal reasoning
abilities of LLMs through a multi-stage process that combines timeline
construction with iterative self-reflection. Our approach leverages test-time
scaling to extend the length of reasoning traces, enabling models to capture
complex temporal dependencies more effectively. This strategy not only boosts
reasoning accuracy but also improves the traceability of the inference process.
Experimental results demonstrate state-of-the-art performance across multiple
benchmarks, including out-of-distribution test sets, and reveal that TISER
enables smaller open-source models to surpass larger closed-weight models on
challenging temporal reasoning tasks.

</details>


### [746] [Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation](https://arxiv.org/pdf/2504.11992)
*Pascal Schlachter, Jonathan Fuss, Bin Yang*

Main category: cs.LG

TL;DR: The paper analyzes pseudo-labeling in online source-free universal domain adaptation (SF-UniDA), revealing gaps between current methods and ideal performance, and highlights the importance of pseudo-label accuracy over quantity.


<details>
  <summary>Details</summary>
Motivation: Addressing domain and category shifts in real-world applications where source data is restricted and target data is streamed, focusing on the understudied relationship between pseudo-labeling and adaptation outcomes.

Method: Systematic analysis through controlled experiments with simulated pseudo-labeling, comparing contrastive and cross-entropy losses.

Result: A significant gap exists between state-of-the-art methods and the upper bound of adaptation with perfect pseudo-labeling. Contrastive loss works well with moderate pseudo-label accuracy, while CE loss excels with near-perfect pseudo-labels.

Conclusion: Pseudo-label accuracy is more critical than quantity, and the study provides actionable insights for improving online SF-UniDA.

Abstract: A domain (distribution) shift between training and test data often hinders
the real-world performance of deep neural networks, necessitating unsupervised
domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged
as a solution for practical scenarios where access to source data is restricted
and target data is received as a continuous stream. However, the open-world
nature of many real-world applications additionally introduces category shifts
meaning that the source and target label spaces may differ. Online source-free
universal domain adaptation (SF-UniDA) addresses this challenge. Existing
methods mainly rely on self-training with pseudo-labels, yet the relationship
between pseudo-labeling and adaptation outcomes has not been studied yet. To
bridge this gap, we conduct a systematic analysis through controlled
experiments with simulated pseudo-labeling, offering valuable insights into
pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap
between the current state-of-the-art and the upper bound of adaptation achieved
with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables
effective adaptation even with moderate pseudo-label accuracy, while a
cross-entropy (CE) loss, though less robust to pseudo-label errors, achieves
superior results when pseudo-labeling approaches perfection. Lastly, our
findings indicate that pseudo-label accuracy is in general more crucial than
quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels
is beneficial. Overall, our study highlights the critical role of
pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive
future advancements in the field. Our code is available at
https://github.com/pascalschlachter/PLAnalysis.

</details>


### [747] [A Theoretical Framework for OOD Robustness in Transformers using Gevrey Classes](https://arxiv.org/pdf/2504.12991)
*Yu Wang, Fu-Chieh Chang, Pei-Yuan Wu*

Main category: cs.LG

TL;DR: The paper analyzes Transformer robustness under semantic OOD shifts, deriving error bounds using Wasserstein-1 distance and Gevrey-class smoothness, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: To understand how Transformer language models generalize under semantic OOD shifts where training and test data are in disjoint latent spaces.

Method: Uses Wasserstein-1 distance and Gevrey-class smoothness to derive sub-exponential upper bounds on prediction error, validated via experiments on arithmetic and Chain-of-Thought tasks.

Result: Empirical degradation aligns with theoretical bounds, showing how smoothness governs OOD generalization in Transformers.

Conclusion: The study highlights geometric and functional principles underlying OOD generalization in Transformers, supported by theory and experiments.

Abstract: We study the robustness of Transformer language models under semantic
out-of-distribution (OOD) shifts, where training and test data lie in disjoint
latent spaces. Using Wasserstein-1 distance and Gevrey-class smoothness, we
derive sub-exponential upper bounds on prediction error. Our theoretical
framework explains how smoothness governs generalization under distributional
drift. We validate these findings through controlled experiments on arithmetic
and Chain-of-Thought tasks with latent permutations and scalings. Results show
empirical degradation aligns with our bounds, highlighting the geometric and
functional principles underlying OOD generalization in Transformers.

</details>


### [748] [Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework](https://arxiv.org/pdf/2505.02712)
*Andrzej Mizera, Jakub Zarzycki*

Main category: cs.LG

TL;DR: The paper proposes a deep reinforcement learning (DRL) approach to control Boolean network models for cellular reprogramming, addressing scalability and efficiency challenges.


<details>
  <summary>Details</summary>
Motivation: Classical wet-lab experiments for discovering cellular reprogramming strategies are time-consuming and costly, prompting the need for computational alternatives.

Method: The study uses DRL to control Boolean network models under asynchronous update mode, incorporating graph neural networks for scalability and improved pseudo-attractor identification.

Result: Experiments on real-world biological networks show the approach is scalable and effective.

Conclusion: The DRL-based framework offers a promising computational solution for cellular reprogramming challenges.

Abstract: Cellular reprogramming, the artificial transformation of one cell type into
another, has been attracting increasing research attention due to its
therapeutic potential for complex diseases. However, discovering reprogramming
strategies through classical wet-lab experiments is hindered by lengthy time
commitments and high costs. In this study, we explore the use of deep
reinforcement learning (DRL) to control Boolean network models of complex
biological systems, such as gene regulatory networks and signalling pathway
networks. We formulate a novel control problem for Boolean network models under
the asynchronous update mode in the context of cellular reprogramming. To
facilitate scalability, we consider our previously introduced concept of a
pseudo-attractor and we improve our procedure for effective identification of
pseudo-attractor states. Finally, we devise a computational framework to solve
the control problem. To leverage the structure of biological systems, we
incorporate graph neural networks with graph convolutions into the artificial
neural network approximator for the action-value function learned by the DRL
agent. Experiments on a number of large real-world biological networks from
literature demonstrate the scalability and effectiveness of our approach.

</details>


### [749] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/pdf/2504.20966)
*Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji*

Main category: cs.LG

TL;DR: Softpick replaces softmax in transformers, eliminating attention sink and massive activations, improving performance and sparsity.


<details>
  <summary>Details</summary>
Motivation: Address issues like attention sink and massive activations in transformer attention mechanisms.

Method: Introduces softpick, a rectified, not sum-to-one replacement for softmax, tested on 340M and 1.8B parameter models.

Result: Achieves 0% sink rate, lower kurtosis, sparse attention maps, and better performance in quantized models, especially at lower bit precisions.

Conclusion: Softpick offers potential benefits for quantization, low-precision training, sparsity, pruning, and interpretability.

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M and 1.8B parameter models
demonstrate that softpick achieves 0\% sink rate consistently. The softpick
transformers produce hidden states with significantly lower kurtosis and
creates sparse attention maps. Quantized models using softpick outperform
softmax on standard benchmarks, with a particularly pronounced advantage at
lower bit precisions. Our analysis and discussion shows how softpick has the
potential to open new possibilities for quantization, low-precision training,
sparsity optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention

</details>


### [750] [Nearly Optimal Sample Complexity for Learning with Label Proportions](https://arxiv.org/pdf/2505.05355)
*Robert Busa-Fekete, Travis Dick, Claudio Gentile, Haim Kaplan, Tomer Koren, Uri Stemmer*

Main category: cs.LG

TL;DR: The paper explores Learning from Label Proportions (LLP), focusing on sample complexity and algorithmic solutions for achieving low regret at the individual example level.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of learning from aggregated label data (bags) while still achieving accurate predictions for individual examples, despite partial observability.

Method: The method involves variants of Empirical Risk Minimization and Stochastic Gradient Descent, enhanced with ad hoc variance reduction techniques.

Result: The results show an optimal sample complexity under square loss and improved empirical performance (better accuracy with fewer samples) compared to baselines.

Conclusion: The paper concludes with theoretical advancements in LLP's sample complexity and practical validation of the proposed algorithms.

Abstract: We investigate Learning from Label Proportions (LLP), a partial information
setting where examples in a training set are grouped into bags, and only
aggregate label values in each bag are available. Despite the partial
observability, the goal is still to achieve small regret at the level of
individual examples. We give results on the sample complexity of LLP under
square loss, showing that our sample complexity is essentially optimal. From an
algorithmic viewpoint, we rely on carefully designed variants of Empirical Risk
Minimization, and Stochastic Gradient Descent algorithms, combined with ad hoc
variance reduction techniques. On one hand, our theoretical results improve in
important ways on the existing literature on LLP, specifically in the way the
sample complexity depends on the bag size. On the other hand, we validate our
algorithmic solutions on several datasets, demonstrating improved empirical
performance (better accuracy for less samples) against recent baselines.

</details>


### [751] [Spatiotemporal Field Generation Based on Hybrid Mamba-Transformer with Physics-informed Fine-tuning](https://arxiv.org/pdf/2505.11578)
*Peimian Du, Jiabin Liu, Xiaowei Jin, Wangmeng Zuo, Hui Li*

Main category: cs.LG

TL;DR: The paper introduces HMT-PF, a hybrid Mamba-Transformer model for spatiotemporal physical field generation, addressing physical equation discrepancies via a physics-informed fine-tuning block and self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of physical equation discrepancies in data-driven spatiotemporal physical field generation.

Method: Develops HMT-PF using a hybrid Mamba-Transformer architecture with unstructured grid inputs, introduces a physics-informed fine-tuning block, and employs self-supervised learning for refinement.

Result: The model performs well in generating spatiotemporal fields, with the fine-tuning mechanism effectively reducing physical errors. A new MSE-R evaluation method is introduced.

Conclusion: HMT-PF successfully addresses physical discrepancies and improves accuracy in spatiotemporal field generation.

Abstract: This research confronts the challenge of substantial physical equation
discrepancies encountered in the generation of spatiotemporal physical fields
through data-driven trained models. A spatiotemporal physical field generation
model, named HMT-PF, is developed based on the hybrid Mamba-Transformer
architecture, incorporating unstructured grid information as input. A
fine-tuning block, enhanced with physical information, is introduced to
effectively reduce the physical equation discrepancies. The physical equation
residuals are computed through a point query mechanism for efficient gradient
evaluation, then encoded into latent space for refinement. The fine-tuning
process employs a self-supervised learning approach to achieve physical
consistency while maintaining essential field characteristics. Results show
that the hybrid Mamba-Transformer model achieves good performance in generating
spatiotemporal fields, while the physics-informed fine-tuning mechanism further
reduces significant physical errors effectively. A MSE-R evaluation method is
developed to assess the accuracy and realism of physical field generation.

</details>


### [752] [Do Language Models Use Their Depth Efficiently?](https://arxiv.org/pdf/2505.13898)
*Róbert Csordás, Christopher D. Manning, Christopher Potts*

Main category: cs.LG

TL;DR: Deeper LLMs like Llama 3.1 and Qwen 3 don't efficiently use depth for new computations but spread existing ones over more layers, leading to diminishing returns.


<details>
  <summary>Details</summary>
Motivation: To investigate whether deeper LLMs use their depth for higher-order computations or just spread existing computations over more layers.

Method: Analyzed residual streams of Llama 3.1 and Qwen 3 models, compared sublayer outputs, skipped layers, and trained linear maps between shallow and deep models.

Result: Layers in the second half contribute less; skipping them has minimal impact. No evidence of depth aiding multihop tasks. Linear maps suggest deeper models spread computations.

Conclusion: Deeper models don't leverage depth for new computations, only finer adjustments, explaining diminishing returns in scaling.

Abstract: Modern LLMs are increasingly deep, and depth correlates with performance,
albeit with diminishing returns. However, do these models use their depth
efficiently? Do they compose more features to create higher-order computations
that are impossible in shallow models, or do they merely spread the same kinds
of computation out over more layers? To address these questions, we analyze the
residual stream of the Llama 3.1 and Qwen 3 family of models. We find: First,
comparing the output of the sublayers to the residual stream reveals that
layers in the second half contribute much less than those in the first half,
with a clear phase transition between the two halves. Second, skipping layers
in the second half has a much smaller effect on future computations and output
predictions. Third, for multihop tasks, we are unable to find evidence that
models are using increased depth to compose subresults in examples involving
many hops. Fourth, we seek to directly address whether deeper models are using
their additional layers to perform new kinds of computation. To do this, we
train linear maps from the residual stream of a shallow model to a deeper one.
We find that layers with the same relative depth map best to each other,
suggesting that the larger model simply spreads the same computations out over
its many layers. All this evidence suggests that deeper models are not using
their depth to learn new kinds of computation, but only using the greater depth
to perform more fine-grained adjustments to the residual. This may help explain
why increasing scale leads to diminishing returns for stacked Transformer
architectures.

</details>


### [753] [Quaff: Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](https://arxiv.org/pdf/2505.14742)
*Hong Huang, Dapeng Wu*

Main category: cs.LG

TL;DR: Quaff is a quantized parameter-efficient fine-tuning framework for LLMs that optimizes low-precision activations via targeted momentum scaling, reducing latency and memory usage while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs on resource-constrained devices is challenging due to computational and memory demands of fine-tuning. Existing quantization methods struggle with performance-overhead balance and activation outliers.

Method: Proposes the Outlier Spatial Stability Hypothesis (OSSH) and Quaff, which dynamically suppresses outliers in invariant channels using lightweight operations, avoiding full-precision storage and global rescaling.

Result: Quaff achieves 1.73x latency reduction, 30% memory savings, and 0.6% accuracy improvement on the GPQA benchmark, enabling fine-tuning on consumer-grade GPUs.

Conclusion: Quaff democratizes personalized LLM deployment by balancing efficiency, performance, and deployability, with code available for public use.

Abstract: Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While quantization offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in quantized fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing quantization errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.

</details>


### [754] [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/pdf/2505.15072)
*Xin Zhou, Weiqing Wang, Francisco J. Baldán, Wray Buntine, Christoph Bergmeir*

Main category: cs.LG

TL;DR: MoTime introduces multimodal datasets for time series forecasting, showing external modalities improve performance, especially in cold-start scenarios.


<details>
  <summary>Details</summary>
Motivation: Most research focuses on unimodal time series despite the availability of multimodal data, limiting realistic forecasting benchmarks.

Method: MoTime provides datasets pairing temporal signals with text, metadata, and images, evaluated under common and cold-start forecasting scenarios.

Result: External modalities enhance forecasting, with notable benefits for short series, though effectiveness varies by data characteristics.

Conclusion: MoTime aims to advance multimodal time series research by providing datasets and insights for more comprehensive benchmarks.

Abstract: While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.

</details>


### [755] [When Are Concepts Erased From Diffusion Models?](https://arxiv.org/pdf/2505.17013)
*Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen*

Main category: cs.LG

TL;DR: The paper explores concept erasure in diffusion models, proposing two mechanisms and introducing a comprehensive evaluation framework to assess erasure effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the unclear effectiveness of existing concept erasure methods in diffusion models and ensure thorough evaluation.

Method: Proposes two conceptual models for erasure (reducing likelihood and interfering with guidance) and introduces adversarial attacks, probing techniques, and alternative generation analysis for evaluation.

Result: Reveals the tension between minimizing side effects and robustness to adversarial prompts, emphasizing the need for thorough evaluation.

Conclusion: Highlights the importance of comprehensive evaluation for concept erasure in diffusion models.

Abstract: Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.

</details>


### [756] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/pdf/2505.16400)
*Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping*

Main category: cs.LG

TL;DR: Large-scale RL enhances reasoning in small/mid-sized models, outperforming distillation, with math-only and code-only training phases improving performance on respective benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the gap in effective RL training recipes for reasoning models and demonstrate RL's superiority over distillation for smaller models.

Method: Train models sequentially on math-only and code-only prompts, using a robust data curation pipeline and curriculum learning with progressive response lengths.

Result: Significant improvements on math (+14.6%/17.2%) and code (+6.8%/5.8%) benchmarks, with extended code-only RL further boosting code performance without degrading math results.

Conclusion: RL unlocks foundational reasoning capabilities and enables solving previously unsolvable problems, surpassing distillation-based approaches.

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [757] [Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing](https://arxiv.org/pdf/2505.17552)
*Zijie Qiu, Jiaqi Wei, Xiang Zhang, Sheng Xu, Kai Zou, Zhi Jin, Zhiqiang Gao, Nanqing Dong, Siqi Sun*

Main category: cs.LG

TL;DR: RankNovo is a deep reranking framework for de novo peptide sequencing, improving accuracy by combining multiple models and introducing new metrics (PMD, RMD). It outperforms base models and shows strong generalization.


<details>
  <summary>Details</summary>
Motivation: Current deep learning methods for peptide sequencing are limited by data complexity and noise, causing biases. RankNovo aims to overcome these limitations by leveraging multiple models.

Method: RankNovo uses a list-wise reranking approach with axial attention for feature extraction and introduces PMD and RMD metrics for supervision.

Result: RankNovo surpasses base models, sets a new benchmark, and generalizes well to unseen models.

Conclusion: RankNovo challenges single-model paradigms and advances de novo sequencing accuracy, offering a robust and universal framework.

Abstract: De novo peptide sequencing is a critical task in proteomics. However, the
performance of current deep learning-based methods is limited by the inherent
complexity of mass spectrometry data and the heterogeneous distribution of
noise signals, leading to data-specific biases. We present RankNovo, the first
deep reranking framework that enhances de novo peptide sequencing by leveraging
the complementary strengths of multiple sequencing models. RankNovo employs a
list-wise reranking approach, modeling candidate peptides as multiple sequence
alignments and utilizing axial attention to extract informative features across
candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass
Deviation) and RMD (residual Mass Deviation), which offer delicate supervision
by quantifying mass differences between peptides at both the sequence and
residue levels. Extensive experiments demonstrate that RankNovo not only
surpasses its base models used to generate training candidates for reranking
pre-training, but also sets a new state-of-the-art benchmark. Moreover,
RankNovo exhibits strong zero-shot generalization to unseen models whose
generations were not exposed during training, highlighting its robustness and
potential as a universal reranking framework for peptide sequencing. Our work
presents a novel reranking strategy that fundamentally challenges existing
single-model paradigms and advances the frontier of accurate de novo
sequencing. Our source code is provided on GitHub.

</details>


### [758] [Should We Simultaneously Calibrate Multiple Computer Models?](https://arxiv.org/pdf/2505.18176)
*Jonathan Tammer Eweis-Labolle, Tyler Johnson, Xiangyu Sun, Ramin Bostanabad*

Main category: cs.LG

TL;DR: The paper proposes a probabilistic framework using neural networks to simultaneously calibrate multiple computer models of varying fidelity, addressing multi-response models and non-identifiability issues.


<details>
  <summary>Details</summary>
Motivation: Traditional methods calibrate models one at a time; this work explores the potential of simultaneous calibration for improved predictive accuracy.

Method: Develops a probabilistic framework with customized neural networks, unique parameter distributions, a specialized loss function, and a visualizable latent space.

Result: The approach improves predictive accuracy but faces non-identifiability issues in high-dimensional spaces.

Conclusion: Simultaneous calibration of multiple models is promising but requires careful handling of non-identifiability.

Abstract: In an increasing number of applications designers have access to multiple
computer models which typically have different levels of fidelity and cost.
Traditionally, designers calibrate these models one at a time against some
high-fidelity data (e.g., experiments). In this paper, we question this
tradition and assess the potential of calibrating multiple computer models at
the same time. To this end, we develop a probabilistic framework that is
founded on customized neural networks (NNs) that are designed to calibrate an
arbitrary number of computer models. In our approach, we (1) consider the fact
that most computer models are multi-response and that the number and nature of
calibration parameters may change across the models, and (2) learn a unique
probability distribution for each calibration parameter of each computer model,
(3) develop a loss function that enables our NN to emulate all data sources
while calibrating the computer models, and (4) aim to learn a visualizable
latent space where model-form errors can be identified. We test the performance
of our approach on analytic and engineering problems to understand the
potential advantages and pitfalls in simultaneous calibration of multiple
computer models. Our method can improve predictive accuracy, however, it is
prone to non-identifiability issues in higher-dimensional input spaces that are
normally constrained by underlying physics.

</details>


### [759] [Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data](https://arxiv.org/pdf/2505.20485)
*Abhijit Chunduru, Majid Morafah, Mahdi Morafah, Vishnu Pandi Chellapandi, Ang Li*

Main category: cs.LG

TL;DR: FedProj is a federated learning framework addressing data heterogeneity by preventing forgetting of the global decision boundary and improving ensemble knowledge fusion.


<details>
  <summary>Details</summary>
Motivation: Data heterogeneity in federated learning challenges the learning of a robust global decision boundary, with existing methods lacking deep understanding of its impact.

Method: FedProj introduces server-side ensemble knowledge transfer loss and episodic memory of average ensemble logits to regulate local training.

Result: FedProj significantly outperforms state-of-the-art methods.

Conclusion: FedProj effectively mitigates forgetting of the global decision boundary and enhances federated learning performance.

Abstract: The inevitable presence of data heterogeneity has made federated learning
very challenging. There are numerous methods to deal with this issue, such as
local regularization, better model fusion techniques, and data sharing. Though
effective, they lack a deep understanding of how data heterogeneity can affect
the global decision boundary. In this paper, we bridge this gap by performing
an experimental analysis of the learned decision boundary using a toy example.
Our observations are surprising: (1) we find that the existing methods suffer
from forgetting and clients forget the global decision boundary and only learn
the perfect local one, and (2) this happens regardless of the initial weights,
and clients forget the global decision boundary even starting from pre-trained
optimal weights. In this paper, we present FedProj, a federated learning
framework that robustly learns the global decision boundary and avoids its
forgetting during local training. To achieve better ensemble knowledge fusion,
we design a novel server-side ensemble knowledge transfer loss to further
calibrate the learned global decision boundary. To alleviate the issue of
learned global decision boundary forgetting, we further propose leveraging an
episodic memory of average ensemble logits on a public unlabeled dataset to
regulate the gradient updates at each step of local training. Experimental
results demonstrate that FedProj outperforms state-of-the-art methods by a
large margin.

</details>


### [760] [HD-PiSSA: High-Rank Distributed Orthogonal Adaptation](https://arxiv.org/pdf/2505.18777)
*Yiding Wang, Fauxu Meng, Xuefeng Zhang, Fan Jiang, Pingzhi Tang, Muhan Zhang*

Main category: cs.LG

TL;DR: HD-PiSSA improves PEFT for LLMs by distributing orthogonal adapters across GPUs, achieving higher effective ranks and better performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA and PiSSA limit expressiveness by constraining updates to low-rank subspaces, leading to suboptimal performance.

Method: HD-PiSSA initializes orthogonal adapters on different devices, aggregates delta updates, and assigns unique principal components to each GPU, expanding update directions.

Result: HD-PiSSA achieves 16x higher effective ranks and outperforms LoRA and PiSSA by 10.0 and 4.98 points, respectively, on multi-task benchmarks.

Conclusion: HD-PiSSA enhances optimization flexibility and performance in PEFT for LLMs, especially in complex tasks.

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language
models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank
subspaces, limiting their expressiveness and leading to suboptimal performance
on complex tasks. To address this, we introduce High-rank Distributed PiSSA
(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters
across different devices and aggregates their delta updates collectively on W
for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical
adapters across all devices, HD-PiSSA assigns different principal components of
the pre-trained weights to each GPU, significantly expanding the range of
update directions. This results in over 16x higher effective updated ranks than
data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device
adapter rank. Empirically, we evaluate HD-PiSSA across various challenging
downstream tasks, including mathematics, code generation, and multi-task
learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0
absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12
benchmarks, demonstrating its benefits from the extra optimization flexibility.

</details>


### [761] [Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians](https://arxiv.org/pdf/2505.19458)
*Akiyoshi Tomihari, Ryo Karakida*

Main category: cs.LG

TL;DR: This paper relaxes energy constraints in self-attention (SA) layers and uses dynamical systems analysis to characterize inference dynamics, revealing the role of normalization layers and proposing new regularization methods.


<details>
  <summary>Details</summary>
Motivation: To broaden understanding of SA by moving beyond idealized energy-based assumptions and constraints, enabling analysis of more general SA architectures.

Method: Relaxes symmetry and single-head constraints, analyzes the Jacobian matrix of the state, and investigates normalization layers' effects on dynamics.

Result: Normalization layers normalize Jacobian eigenvalues, enhancing inference performance. A pseudo-energy for monitoring dynamics is introduced.

Conclusion: The work provides an energy-agnostic framework for SA analysis, improving performance and offering tools for training and monitoring.

Abstract: The theoretical understanding of self-attention (SA) has been steadily
progressing. A prominent line of work studies a class of SA layers that admit
an energy function decreased by state updates. While it provides valuable
insights into inherent biases in signal propagation, it often relies on
idealized assumptions or additional constraints not necessarily present in
standard SA. Thus, to broaden our understanding, this work aims to relax these
energy constraints and provide an energy-agnostic characterization of inference
dynamics by dynamical systems analysis. In more detail, we first consider
relaxing the symmetry and single-head constraints traditionally required in
energy-based formulations. Next, to investigate more general SA architectures
capable of oscillatory dynamics without necessarily admitting an energy
function, we analyze the Jacobian matrix of the state. We reveal that
normalization layers effectively normalize the Jacobian's complex eigenvalues,
forcing the dynamics close to a critical state. This significantly enhances
inference performance. Furthermore, we utilize the Jacobian perspective to
develop regularization methods for training and a pseudo-energy for monitoring
inference dynamics.

</details>


### [762] [Binarized Neural Networks Converge Toward Algorithmic Simplicity: Empirical Support for the Learning-as-Compression Hypothesis](https://arxiv.org/pdf/2505.20646)
*Eduardo Y. Sakabe, Felipe S. Abrahão, Alexandre Simões, Esther Colombini, Paula Costa, Ricardo Gudwin, Hector Zenil*

Main category: cs.LG

TL;DR: The paper proposes using algorithmic information theory and Binarized Neural Networks (BNNs) to better understand neural network complexity, showing that algorithmic measures like BDM outperform entropy in tracking training dynamics.


<details>
  <summary>Details</summary>
Motivation: Current entropy-based methods fail to capture deeper algorithmic regularities in neural networks, limiting understanding of learning dynamics.

Method: The approach uses algorithmic probability and the Block Decomposition Method (BDM) to analyze BNNs, providing a causally grounded measure of complexity.

Result: BDM more accurately tracks structural changes during training and correlates better with training loss than entropy, supporting the idea of learning as algorithmic compression.

Conclusion: The work offers a principled framework for complexity-aware learning, grounded in algorithmic information theory, and suggests new directions for regularization and model analysis.

Abstract: Understanding and controlling the informational complexity of neural networks
is a central challenge in machine learning, with implications for
generalization, optimization, and model capacity. While most approaches rely on
entropy-based loss functions and statistical metrics, these measures often fail
to capture deeper, causally relevant algorithmic regularities embedded in
network structure. We propose a shift toward algorithmic information theory,
using Binarized Neural Networks (BNNs) as a first proxy. Grounded in
algorithmic probability (AP) and the universal distribution it defines, our
approach characterizes learning dynamics through a formal, causally grounded
lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation
of algorithmic complexity based on AP -- and demonstrate that it more closely
tracks structural changes during training than entropy, consistently exhibiting
stronger correlations with training loss across varying model sizes and
randomized training runs. These results support the view of training as a
process of algorithmic compression, where learning corresponds to the
progressive internalization of structured regularities. In doing so, our work
offers a principled estimate of learning progression and suggests a framework
for complexity-aware learning and regularization, grounded in first principles
from information theory, complexity, and computability.

</details>


### [763] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/pdf/2505.21680)
*Andrew J. Loza, Jun Yup Kim, Shangzheng Song, Yihang Liu, Joseph J. Y. Sung, R Andrew Taylor, Dennis L. Shung*

Main category: cs.LG

TL;DR: MultivariateGPT is a transformer-based model for mixed categorical and numeric data, addressing limitations of existing methods in handling irregular intervals and informative sampling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with mixed data types (categorical and numeric) and irregular sampling. Discrete token-based approaches lack numeric precision, while neural ODEs are unsuitable for categorical data.

Method: MultivariateGPT uses an autoregressive sequence decomposition, embedding scheme, and loss function to jointly predict next token class and value, extending transformer models to mixed data.

Result: The model effectively learns patterns in physical systems and complex time series like electrocardiograms and electronic health records.

Conclusion: MultivariateGPT broadens transformer applicability to mixed data types, improving handling of irregular and informative sampling.

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [764] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/pdf/2505.22442)
*Mattie Fellows, Clarisse Wibault, Uljad Berdica, Johannes Forkel, Michael A. Osborne, Jakob N. Foerster*

Main category: cs.LG

TL;DR: SOReL and TOReL address offline RL challenges by enabling safe performance estimation and offline hyperparameter tuning, reducing reliance on costly online interactions.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency and safety are critical for real-world RL adoption, but current offline RL methods require extensive online tuning and lack performance guarantees.

Method: SOReL uses Bayesian inference to estimate online performance from offline data, while TOReL extends offline hyperparameter tuning to general RL methods.

Result: SOReL accurately estimates regret, and TOReL matches online tuning performance using only offline data.

Conclusion: SOReL and TOReL advance safe and reliable offline RL, enabling broader real-world applications.

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [765] [Born a Transformer -- Always a Transformer?](https://arxiv.org/pdf/2505.21785)
*Yana Veitsman, Mayank Jobanputra, Yash Sarrof, Aleksandra Bakalova, Vera Demberg, Ellie Pavlick, Michael Hahn*

Main category: cs.LG

TL;DR: The paper investigates whether large-scale pretrained LLMs overcome theoretical limitations of transformers in sequence-to-sequence tasks, focusing on retrieval and copying tasks. It reveals an induction-versus-anti-induction asymmetry in pretrained models, which disappears with fine-tuning, and links this to underlying circuit strengths.


<details>
  <summary>Details</summary>
Motivation: To understand if pretrained LLMs bypass theoretical transformer limitations in practice, given their scale and data.

Method: Study retrieval and copying tasks using a framework for length generalization, analyze induction-versus-anti-induction asymmetry, and validate with mechanistic analysis and real-world experiments.

Result: Pretrained models show an induction bias, which fine-tuning can correct if length-generalization is theoretically guaranteed. Pretraining enhances specific capabilities but not fundamental limits.

Conclusion: Pretraining selectively improves transformer capabilities but doesn't overcome core length-generalization constraints, highlighting reliability risks.

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024a]. We use a recently proposed framework for studying length
generalization [Huang et al., 2025] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [766] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/pdf/2505.22257)
*Youssef Mroueh, Nicolas Dupuis, Brian Belgodere, Apoorva Nitsure, Mattia Rigotti, Kristjan Greenewald, Jiri Navratil, Jerret Ross, Jesus Rios*

Main category: cs.LG

TL;DR: GRPO is adapted for off-policy settings, showing improved rewards and performance compared to on-policy GRPO.


<details>
  <summary>Details</summary>
Motivation: Improving training stability, sampling efficiency, and memory usage in reinforcement learning by leveraging off-policy PPO and GRPO advantages.

Method: Adapt GRPO to off-policy settings, using clipped surrogate objectives, and compare on-policy and off-policy GRPO empirically.

Result: Off-policy GRPO outperforms or matches on-policy GRPO in reward improvement and performance.

Conclusion: Off-policy GRPO is a viable and often superior alternative to on-policy GRPO in reinforcement learning.

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [767] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/pdf/2505.22362)
*Aihu Zhang, Jiaxing Xu, Mengcheng Lan, Shili Xiang, Yiping Ke*

Main category: cs.LG

TL;DR: DHGNN is a novel GNN framework addressing heterophilic neighborhoods and directional graph challenges, outperforming baselines in node classification and link prediction.


<details>
  <summary>Details</summary>
Motivation: Most GNNs struggle with heterophilic neighborhoods and ignore directional graph structures, limiting performance.

Method: DHGNN uses a resettable gating mechanism for homophily-aware message modulation and a noise-tolerant fusion module for directional integration.

Result: DHGNN outperforms state-of-the-art methods, improving link prediction by up to 15.07%.

Conclusion: DHGNN effectively addresses GNN limitations, providing insights into message-passing on complex graphs.

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [768] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/pdf/2505.22904)
*Youngsoo Choi, Siu Wun Cheung, Youngkyu Kim, Ping-Hsuan Tsai, Alejandro N. Diaz, Ivan Zanardi, Seung Whan Chung, Dylan Matthew Copeland, Coleman Kendrick, William Anderson, Traian Iliescu, Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: The paper proposes a formal definition of foundation models in computational science, emphasizing generality, reusability, and scalability, and introduces the Data-Driven Finite Element Method (DD-FEM) as a framework to address key challenges.


<details>
  <summary>Details</summary>
Motivation: The term 'foundation model' lacks a universally accepted definition in computational science, leading to confusion. This paper aims to clarify and formalize the concept.

Method: The authors propose a formal definition and characteristics for foundation models, drawing parallels with traditional methods like FEM. They introduce DD-FEM, combining classical FEM with data-driven learning.

Result: DD-FEM demonstrates scalability, adaptability, and physics consistency, addressing challenges in foundation models for computational science.

Conclusion: The work bridges traditional numerical methods with AI, providing a rigorous foundation for future development of foundation models in computational science.

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [769] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/pdf/2505.22861)
*Carlota Parés-Morlans, Michelle Yi, Claire Chen, Sarah A. Wu, Rika Antonova, Tobias Gerstenberg, Jeannette Bohg*

Main category: cs.LG

TL;DR: Causal-PIK uses Bayesian optimization and a Physics-Informed Kernel to improve action planning in complex tasks, outperforming state-of-the-art methods and even matching human performance in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Planning in tasks with unknown object dynamics is difficult, requiring iterative action improvement through active exploration of causes and effects.

Method: Causal-PIK leverages Bayesian optimization and a Physics-Informed Kernel to efficiently search for optimal actions by reasoning about causal interactions.

Result: Causal-PIK outperforms state-of-the-art methods on Virtual Tools and PHYRE benchmarks, requiring fewer actions to achieve goals, and remains competitive with human performance.

Conclusion: Causal-PIK is effective for complex tasks, demonstrating superior efficiency and competitive performance even in highly challenging scenarios.

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [770] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/pdf/2505.23017)
*Xingjian Wu, Xiangfei Qiu, Hongfan Gao, Jilin Hu, Bin Yang, Chenjuan Guo*

Main category: cs.LG

TL;DR: $K^2$VAE improves long-term probabilistic time series forecasting by combining a KoopmanNet and KalmanNet to linearize dynamics and reduce error accumulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on short-term forecasting, neglecting challenges in long-term probabilistic forecasting due to nonlinear dynamics and inefficiency.

Method: Introduces $K^2$VAE, a VAE-based model using KoopmanNet for linearization and KalmanNet for refining predictions and uncertainty modeling.

Result: Outperforms state-of-the-art methods in both short- and long-term forecasting, offering efficiency and accuracy.

Conclusion: $K^2$VAE provides a superior solution for probabilistic time series forecasting, addressing long-term challenges effectively.

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [771] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/pdf/2505.23593)
*Nikita Agrawal, Simon Mertel, Ruben Mayer*

Main category: cs.LG

TL;DR: The paper critiques the use of black-box models in federated learning (FL) for post-training, arguing it contradicts FL principles like privacy and autonomy.


<details>
  <summary>Details</summary>
Motivation: To address concerns about using black-box foundation language models in FL, which may undermine core federation principles.

Method: Critical analysis of black-box model usage in FL post-training, examining openness aspects and their FL implications.

Result: Identifies contradictions between black-box models and FL principles, highlighting risks to privacy and autonomy.

Conclusion: Advocates for reconsidering black-box model usage in FL post-training to align with federation principles.

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [772] [Distributed Neural Policy Gradient Algorithm for Global Convergence of Networked Multi-Agent Reinforcement Learning](https://arxiv.org/pdf/2505.24113)
*Pengcheng Dai, Yuanqiu Mo, Wenwu Yu, Wei Ren*

Main category: cs.MA

TL;DR: A distributed neural policy gradient algorithm is proposed for networked multi-agent reinforcement learning, featuring neural networks for Q-functions and policy functions, with proven convergence and effectiveness in simulations.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from poor expression due to linear function approximation, motivating the need for a more expressive and collaborative approach.

Method: The algorithm includes a distributed critic step for joint policy evaluation via communication networks and a decentralized actor step for local policy updates.

Result: The algorithm achieves global convergence in both policy evaluation and overall objective, outperforming centralized methods in robot path planning simulations.

Conclusion: The proposed algorithm effectively addresses the NMARL problem with improved performance and scalability.

Abstract: This paper studies the networked multi-agent reinforcement learning (NMARL)
problem, where the objective of agents is to collaboratively maximize the
discounted average cumulative rewards. Different from the existing methods that
suffer from poor expression due to linear function approximation, we propose a
distributed neural policy gradient algorithm that features two innovatively
designed neural networks, specifically for the approximate Q-functions and
policy functions of agents. This distributed neural policy gradient algorithm
consists of two key components: the distributed critic step and the
decentralized actor step. In the distributed critic step, agents receive the
approximate Q-function parameters from their neighboring agents via a
time-varying communication networks to collaboratively evaluate the joint
policy. In contrast, in the decentralized actor step, each agent updates its
local policy parameter solely based on its own approximate Q-function. In the
convergence analysis, we first establish the global convergence of agents for
the joint policy evaluation in the distributed critic step. Subsequently, we
rigorously demonstrate the global convergence of the overall distributed neural
policy gradient algorithm with respect to the objective function. Finally, the
effectiveness of the proposed algorithm is demonstrated by comparing it with a
centralized algorithm through simulation in the robot path planning
environment.

</details>


### [773] [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/pdf/2505.24239)
*Sana Ebrahimi, Mohsen Dehghankar, Abolfazl Asudeh*

Main category: cs.MA

TL;DR: A credibility-based multi-agent LLM framework is introduced to mitigate adversarial and low-performing agents by scoring and aggregating contributions iteratively.


<details>
  <summary>Details</summary>
Motivation: Multi-agent LLM systems are vulnerable to adversarial and low-performing agents, necessitating a robust solution.

Method: The framework models query-answering as an iterative game, using learned credibility scores to aggregate outputs.

Result: Experiments show the system effectively mitigates adversarial influence and enhances resilience, even in adversary-majority settings.

Conclusion: The credibility-based framework improves multi-agent cooperation by addressing adversarial vulnerabilities.

Abstract: While multi-agent LLM systems show strong capabilities in various domains,
they are highly vulnerable to adversarial and low-performing agents. To resolve
this issue, in this paper, we introduce a general and adversary-resistant
multi-agent LLM framework based on credibility scoring. We model the
collaborative query-answering process as an iterative game, where the agents
communicate and contribute to a final system output. Our system associates a
credibility score that is used when aggregating the team outputs. The
credibility scores are learned gradually based on the past contributions of
each agent in query answering. Our experiments across multiple tasks and
settings demonstrate our system's effectiveness in mitigating adversarial
influence and enhancing the resilience of multi-agent cooperation, even in the
adversary-majority settings.

</details>


### [774] [R3DM: Enabling Role Discovery and Diversity Through Dynamics Models in Multi-agent Reinforcement Learning](https://arxiv.org/pdf/2505.24265)
*Harsh Goel, Mohammad Omama, Behdad Chalaki, Vaishnav Tadiparthi, Ehsan Moradi Pari, Sandeep Chinchali*

Main category: cs.MA

TL;DR: R3DM is a novel role-based MARL framework that improves coordination by linking roles to future behaviors, outperforming existing methods by up to 20% in win rates.


<details>
  <summary>Details</summary>
Motivation: Existing role-based MARL methods ignore the influence of roles on future trajectories, limiting coordination. R3DM addresses this gap by integrating roles with future behaviors.

Method: R3DM uses contrastive learning on past trajectories to derive roles and a dynamics model to diversify future behaviors, maximizing mutual information between roles, trajectories, and future actions.

Result: R3DM achieves up to 20% higher win rates in SMAC and SMACv2 environments compared to state-of-the-art MARL methods.

Conclusion: R3DM demonstrates that linking roles to future behaviors enhances coordination, offering a promising direction for role-based MARL.

Abstract: Multi-agent reinforcement learning (MARL) has achieved significant progress
in large-scale traffic control, autonomous vehicles, and robotics. Drawing
inspiration from biological systems where roles naturally emerge to enable
coordination, role-based MARL methods have been proposed to enhance cooperation
learning for complex tasks. However, existing methods exclusively derive roles
from an agent's past experience during training, neglecting their influence on
its future trajectories. This paper introduces a key insight: an agent's role
should shape its future behavior to enable effective coordination. Hence, we
propose Role Discovery and Diversity through Dynamics Models (R3DM), a novel
role-based MARL framework that learns emergent roles by maximizing the mutual
information between agents' roles, observed trajectories, and expected future
behaviors. R3DM optimizes the proposed objective through contrastive learning
on past trajectories to first derive intermediate roles that shape intrinsic
rewards to promote diversity in future behaviors across different roles through
a learned dynamics model. Benchmarking on SMAC and SMACv2 environments
demonstrates that R3DM outperforms state-of-the-art MARL approaches, improving
multi-agent coordination to increase win rates by up to 20%.

</details>


### [775] [ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks](https://arxiv.org/pdf/2503.02390)
*Heng Zhou, Hejia Geng, Xiangyuan Xue, Li Kang, Yiran Qin, Zhiyong Wang, Zhenfei Yin, Lei Bai*

Main category: cs.MA

TL;DR: ReSo enhances multi-agent systems (MAS) with task graph generation and a reward-driven agent selection process, outperforming existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current MAS frameworks lack flexibility, scalability, and optimization strategies, limiting their effectiveness in complex problem-solving.

Method: ReSo integrates task graph generation, a two-stage agent selection process, and a Collaborative Reward Model for fine-grained optimization. It also includes an automated data synthesis framework for benchmark generation.

Result: ReSo achieves 33.7% accuracy on Math-MAS and 32.3% on SciBench-MAS, where other methods fail.

Conclusion: ReSo addresses key MAS limitations and demonstrates superior performance, offering a scalable and flexible solution.

Abstract: Multi-agent systems (MAS) have emerged as a promising approach for enhancing
the reasoning capabilities of large language models in complex problem-solving;
however, current MAS frameworks suffer from poor flexibility and scalability
with underdeveloped optimization strategies. To address these challenges, we
propose ReSo, which integrates task graph generation with a reward-driven
two-stage agent selection process centered on our Collaborative Reward Model
that provides fine-grained reward signals to optimize MAS cooperation. We also
introduce an automated data synthesis framework for generating MAS benchmarks
without any human annotations. Experimental results show that ReSo matches or
outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and
32.3 percent accuracy on SciBench-MAS, where other approaches completely fail.

</details>


### [776] [AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems](https://arxiv.org/pdf/2504.00587)
*Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, Weinan Zhang*

Main category: cs.MA

TL;DR: AgentNet is a decentralized, RAG-based framework for LLM-based agents to collaborate autonomously in a DAG, improving scalability, adaptability, and privacy.


<details>
  <summary>Details</summary>
Motivation: Centralized coordination in multi-agent systems causes scalability issues, reduced adaptability, and privacy concerns, hindering cross-organizational collaboration.

Method: AgentNet uses a decentralized DAG structure with dynamic agent connectivity, task routing based on local expertise, and a retrieval-based memory system for continual skill refinement.

Result: AgentNet outperforms single-agent and centralized multi-agent baselines in task accuracy while ensuring fault tolerance and privacy.

Conclusion: AgentNet provides a scalable, resilient, and privacy-preserving solution for decentralized collaboration among LLM-based agents.

Abstract: The rapid advancement of large language models (LLMs) has enabled the
development of multi-agent systems where multiple LLM-based agents collaborate
on complex tasks. However, existing systems often rely on centralized
coordination, leading to scalability bottlenecks, reduced adaptability, and
single points of failure. Privacy and proprietary knowledge concerns further
hinder cross-organizational collaboration, resulting in siloed expertise. We
propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based
framework that enables LLM-based agents to specialize, evolve, and collaborate
autonomously in a dynamically structured Directed Acyclic Graph (DAG). Unlike
prior approaches with static roles or centralized control, AgentNet allows
agents to adjust connectivity and route tasks based on local expertise and
context. AgentNet introduces three key innovations: (1) a fully decentralized
coordination mechanism that eliminates the need for a central orchestrator,
enhancing robustness and emergent intelligence; (2) dynamic agent graph
topology that adapts in real time to task demands, ensuring scalability and
resilience; and (3) a retrieval-based memory system for agents that supports
continual skill refinement and specialization. By minimizing centralized
control and data exchange, AgentNet enables fault-tolerant, privacy-preserving
collaboration across organizations. Experiments show that AgentNet achieves
higher task accuracy than both single-agent and centralized multi-agent
baselines.

</details>


### [777] [Swarm Intelligence Enhanced Reasoning: A Density-Driven Framework for LLM-Based Multi-Agent Optimization](https://arxiv.org/pdf/2505.17115)
*Ying Zhu, Heng Zhou, Rui Su, Peiqin Zhuang, Lei Bai*

Main category: cs.MA

TL;DR: The paper proposes integrating swarm intelligence into LLM reasoning to enhance problem-solving, introducing the ASI paradigm and SIER framework for optimal and diverse solutions.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CoT and MAD may fail in complex problem-solving due to suboptimal solutions, prompting the need for swarm intelligence integration.

Method: The ASI paradigm formulates LLM reasoning as an optimization problem, using swarm intelligence to guide agents. SIER employs density-driven strategies, kernel density estimation, and non-dominated sorting for solution diversity and quality.

Result: SIER enhances solution space exploration and improves intermediate steps, dynamically controlling exploration for efficient reasoning.

Conclusion: The proposed approach effectively combines swarm intelligence with LLM reasoning, improving solution quality and diversity in complex scenarios.

Abstract: Recently, many approaches, such as Chain-of-Thought (CoT) prompting and
Multi-Agent Debate (MAD), have been proposed to further enrich Large Language
Models' (LLMs) complex problem-solving capacities in reasoning scenarios.
However, these methods may fail to solve complex problems due to the lack of
ability to find optimal solutions. Swarm Intelligence has been serving as a
powerful tool for finding optima in the field of traditional optimization
problems. To this end, we propose integrating swarm intelligence into the
reasoning process by introducing a novel Agent-based Swarm Intelligence (ASI)
paradigm. In this paradigm, we formulate LLM reasoning as an optimization
problem and use a swarm intelligence scheme to guide a group of LLM-based
agents in collaboratively searching for optimal solutions. To avoid swarm
intelligence getting trapped in local optima, we further develop a Swarm
Intelligence Enhancing Reasoning (SIER) framework, which develops a
density-driven strategy to enhance the reasoning ability. To be specific, we
propose to perform kernel density estimation and non-dominated sorting to
optimize both solution quality and diversity simultaneously. In this case, SIER
efficiently enhances solution space exploration through expanding the diversity
of the reasoning path. Besides, a step-level quality evaluation is used to help
agents improve solution quality by correcting low-quality intermediate steps.
Then, we use quality thresholds to dynamically control the termination of
exploration and the selection of candidate steps, enabling a more flexible and
efficient reasoning process. Extensive experiments are ...

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [778] [ISMAF: Intrinsic-Social Modality Alignment and Fusion for Multimodal Rumor Detection](https://arxiv.org/pdf/2505.24176)
*Zihao Yu, Xiang Li, Jing Zhang*

Main category: cs.MM

TL;DR: The paper introduces ISMAF, a framework for multimodal rumor detection that aligns and fuses intrinsic and social modalities, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection models overlook the interplay between intrinsic and social modalities, leading to fragmented interpretations and incomplete understanding.

Method: ISMAF uses cross-modal consistency alignment, mutual learning, and adaptive fusion to integrate intrinsic (text, images) and social modalities.

Result: ISMAF outperforms state-of-the-art models on English and Chinese datasets.

Conclusion: The ISMAF framework effectively addresses the limitations of current methods by better capturing nuanced relationships and dynamically fusing modalities.

Abstract: The rapid dissemination of rumors on social media highlights the urgent need
for automatic detection methods to safeguard societal trust and stability.
While existing multimodal rumor detection models primarily emphasize capturing
consistency between intrinsic modalities (e.g., news text and images), they
often overlook the intricate interplay between intrinsic and social modalities.
This limitation hampers the ability to fully capture nuanced relationships that
are crucial for a comprehensive understanding. Additionally, current methods
struggle with effectively fusing social context with textual and visual
information, resulting in fragmented interpretations. To address these
challenges, this paper proposes a novel Intrinsic-Social Modality Alignment and
Fusion (ISMAF) framework for multimodal rumor detection. ISMAF first employs a
cross-modal consistency alignment strategy to align complex interactions
between intrinsic and social modalities. It then leverages a mutual learning
approach to facilitate collaborative refinement and integration of
complementary information across modalities. Finally, an adaptive fusion
mechanism is incorporated to dynamically adjust the contribution of each
modality, tackling the complexities of three-modality fusion. Extensive
experiments on both English and Chinese real-world multimedia datasets
demonstrate that ISMAF consistently outperforms state-of-the-art models.

</details>


### [779] [Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding](https://arxiv.org/pdf/2505.06685)
*Dawei Huang, Qing Li, Chuan Yan, Zebang Cheng, Yurong Huang, Xiang Li, Bin Li, Xiaohui Wang, Zheng Lian, Xiaojiang Peng*

Main category: cs.MM

TL;DR: Emotion-Qwen is a multimodal framework enhancing emotion understanding in videos while maintaining general vision-language reasoning, using a Hybrid Compressor and pre-trained on large datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Large Multimodal Models (LMMs) underperform in emotion-specific tasks and suffer from catastrophic forgetting when fine-tuned.

Method: Emotion-Qwen uses a Hybrid Compressor (MoE-based) for dynamic input routing and is pre-trained in three stages on general and emotional datasets. The VER dataset (40K bilingual clips) is also introduced.

Result: State-of-the-art performance on emotion recognition benchmarks and competitive results on general VL tasks.

Conclusion: Emotion-Qwen effectively balances emotion-specific and general-purpose processing, advancing multimodal emotion understanding.

Abstract: Emotion understanding in videos aims to accurately recognize and interpret
individuals' emotional states by integrating contextual, visual, textual, and
auditory cues. While Large Multimodal Models (LMMs) have demonstrated
significant progress in general vision-language (VL) tasks, their performance
in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on
emotion-related tasks often leads to catastrophic forgetting, hindering their
ability to generalize across diverse tasks. To address these challenges, we
present Emotion-Qwen, a tailored multimodal framework designed to enhance both
emotion understanding and general VL reasoning. Emotion-Qwen incorporates a
sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm,
which dynamically routes inputs to balance emotion-specific and general-purpose
processing. The model is pre-trained in a three-stage pipeline on large-scale
general and emotional image datasets to support robust multimodal
representations. Furthermore, we construct the Video Emotion Reasoning (VER)
dataset, comprising more than 40K bilingual video clips with fine-grained
descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning
capability. Experimental results demonstrate that Emotion-Qwen achieves
state-of-the-art performance on multiple emotion recognition benchmarks, while
maintaining competitive results on general VL tasks. Code and models are
available at https://github.com/24DavidHuang/Emotion-Qwen.

</details>


### [780] [EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations](https://arxiv.org/pdf/2505.23018)
*Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin*

Main category: cs.MM

TL;DR: The paper introduces EmotionTalk, a high-quality Chinese multimodal emotion dataset with rich annotations, addressing the lack of such resources for Chinese language and culture.


<details>
  <summary>Details</summary>
Motivation: There's a need for comprehensive Chinese emotion datasets due to linguistic and cultural uniqueness, unlike widely available English datasets.

Method: The dataset includes multimodal data (acoustic, visual, textual) from dyadic conversations, annotated for emotions, sentiment, and speech captions.

Result: EmotionTalk offers 23.6 hours of speech, 19,250 utterances, and detailed annotations, supporting research on emotion recognition and multimodal challenges.

Conclusion: EmotionTalk is a pioneering Chinese dataset, valuable for cross-cultural emotion research, and will be open-source for academic use.

Abstract: In recent years, emotion recognition plays a critical role in applications
such as human-computer interaction, mental health monitoring, and sentiment
analysis. While datasets for emotion analysis in languages such as English have
proliferated, there remains a pressing need for high-quality, comprehensive
datasets tailored to the unique linguistic, cultural, and multimodal
characteristics of Chinese. In this work, we propose \textbf{EmotionTalk}, an
interactive Chinese multimodal emotion dataset with rich annotations. This
dataset provides multimodal information from 19 actors participating in dyadic
conversational settings, incorporating acoustic, visual, and textual
modalities. It includes 23.6 hours of speech (19,250 utterances), annotations
for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,
fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,
neutral, weakly positive, and positive) and 4-dimensional speech captions
(speaker, speaking style, emotion and overall). The dataset is well-suited for
research on unimodal and multimodal emotion recognition, missing modality
challenges, and speech captioning tasks. To our knowledge, it represents the
first high-quality and versatile Chinese dialogue multimodal emotion dataset,
which is a valuable contribution to research on cross-cultural emotion analysis
and recognition. Additionally, we conduct experiments on EmotionTalk to
demonstrate the effectiveness and quality of the dataset. It will be
open-source and freely available for all academic purposes. The dataset and
codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.

</details>


### [781] [CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection](https://arxiv.org/pdf/2505.23449)
*Fanxiao Li, Jiaying Wu, Canyuan He, Wei Zhou*

Main category: cs.MM

TL;DR: The paper highlights challenges in using MLLMs for detecting out-of-context misinformation and proposes CMIE, a framework with CRG and AS mechanisms, to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with detecting deeper semantic relationships and are affected by noisy evidence in out-of-context misinformation detection.

Method: Proposes CMIE, incorporating Coexistence Relationship Generation (CRG) and Association Scoring (AS) to identify underlying relationships and selectively use evidence.

Result: CMIE outperforms existing methods in detecting out-of-context misinformation.

Conclusion: The CMIE framework effectively addresses the limitations of MLLMs in misinformation detection by leveraging deeper semantic analysis and selective evidence use.

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive
capabilities in visual reasoning and text generation. While previous studies
have explored the application of MLLM for detecting out-of-context (OOC)
misinformation, our empirical analysis reveals two persisting challenges of
this paradigm. Evaluating the representative GPT-4o model on direct reasoning
and evidence augmented reasoning, results indicate that MLLM struggle to
capture the deeper relationships-specifically, cases in which the image and
text are not directly connected but are associated through underlying semantic
links. Moreover, noise in the evidence further impairs detection accuracy. To
address these challenges, we propose CMIE, a novel OOC misinformation detection
framework that incorporates a Coexistence Relationship Generation (CRG)
strategy and an Association Scoring (AS) mechanism. CMIE identifies the
underlying coexistence relationships between images and text, and selectively
utilizes relevant evidence to enhance misinformation detection. Experimental
results demonstrate that our approach outperforms existing methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [782] [Can Emotion Fool Anti-spoofing?](https://arxiv.org/pdf/2505.23962)
*Aurosweta Mahapatra, Ismail Rasim Ulgen, Abinay Reddy Naini, Carlos Busso, Berrak Sisman*

Main category: eess.AS

TL;DR: The paper introduces EmoSpoof-TTS, a corpus of emotional TTS samples, and GEM, a gated ensemble model, to address the lack of robustness in anti-spoofing against emotional synthetic speech.


<details>
  <summary>Details</summary>
Motivation: Existing anti-spoofing models focus on neutral synthetic speech, neglecting emotional variations, which exposes vulnerabilities to emotion-targeted attacks.

Method: The authors propose GEM, a gated ensemble of emotion-specialized models with a speech emotion recognition gating network, and introduce the EmoSpoof-TTS dataset.

Result: Existing models underperform on emotional synthetic speech, even when trained on emotional data. GEM improves performance across all emotions and neutral states.

Conclusion: The paper highlights the need for emotion-focused anti-spoofing and demonstrates GEM's effectiveness in addressing this gap.

Abstract: Traditional anti-spoofing focuses on models and datasets built on synthetic
speech with mostly neutral state, neglecting diverse emotional variations. As a
result, their robustness against high-quality, emotionally expressive synthetic
speech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of
emotional text-to-speech samples. Our analysis shows existing anti-spoofing
models struggle with emotional synthetic speech, exposing risks of
emotion-targeted attacks. Even trained on emotional data, the models
underperform due to limited focus on emotional aspect and show performance
disparities across emotions. This highlights the need for emotion-focused
anti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated
ensemble of emotion-specialized models with a speech emotion recognition gating
network. GEM performs effectively across all emotions and neutral state,
improving defenses against spoofing attacks. We release the EmoSpoof-TTS
Dataset: https://emospoof-tts.github.io/Dataset/

</details>


### [783] [Fine-tune Before Structured Pruning: Towards Compact and Accurate Self-Supervised Models for Speaker Diarization](https://arxiv.org/pdf/2505.24111)
*Jiangyu Han, Federico Landini, Johan Rohdin, Anna Silnova, Mireia Diez, Jan Cernocky, Lukas Burget*

Main category: eess.AS

TL;DR: Proposes structured pruning with knowledge distillation to compress SSL models like WavLM for speaker diarization, achieving 80% parameter reduction without performance loss and faster inference.


<details>
  <summary>Details</summary>
Motivation: SSL models are large and slow, limiting use in resource-constrained scenarios. Existing compression methods degrade performance at high pruning ratios.

Method: Structured pruning with knowledge distillation, emphasizing fine-tuning SSL models before pruning.

Result: Achieves 80% parameter reduction for WavLM Base+ and Large without performance loss, with 4.0x and 2.6x faster inference speeds, respectively.

Conclusion: The method effectively compresses SSL models for practical use, maintaining performance and improving speed.

Abstract: Self-supervised learning (SSL) models like WavLM can be effectively utilized
when building speaker diarization systems but are often large and slow,
limiting their use in resource constrained scenarios. Previous studies have
explored compression techniques, but usually for the price of degraded
performance at high pruning ratios. In this work, we propose to compress SSL
models through structured pruning by introducing knowledge distillation.
Different from the existing works, we emphasize the importance of fine-tuning
SSL models before pruning. Experiments on far-field single-channel AMI,
AISHELL-4, and AliMeeting datasets show that our method can remove redundant
parameters of WavLM Base+ and WavLM Large by up to 80% without any performance
degradation. After pruning, the inference speeds on a single GPU for the Base+
and Large models are 4.0 and 2.6 times faster, respectively. Our source code is
publicly available.

</details>


### [784] [MOPSA: Mixture of Prompt-Experts Based Speaker Adaptation for Elderly Speech Recognition](https://arxiv.org/pdf/2505.24224)
*Chengxi Deng, Xurong Xie, Shujie Hu, Mengzhe Geng, Yicong Jiang, Jiankun Zhao, Jiajun Deng, Guinan Li, Youjun Chen, Huimeng Wang, Haoning Xu, Mingyu Cui, Xunying Liu*

Main category: eess.AS

TL;DR: MOPSA is a novel speaker adaptation method for elderly speech recognition, using prompt-experts and a router network for zero-shot, real-time adaptation, achieving significant error rate reductions and speed improvements.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting speech recognition models to unseen elderly speakers in real-time, leveraging domain-specific knowledge.

Method: Uses K-means to cluster speaker prompts, trains a router network to dynamically combine them, and employs separate encoder/decoder prompts for Whisper to model variability.

Result: Achieves 0.86% and 1.47% absolute WER/CER reductions (4.21% and 5.40% relative) and up to 16.12x speed-up in real-time adaptation.

Conclusion: MOPSA effectively adapts to elderly speakers in real-time, outperforming speaker-independent models and offline methods.

Abstract: This paper proposes a novel Mixture of Prompt-Experts based Speaker
Adaptation approach (MOPSA) for elderly speech recognition. It allows
zero-shot, real-time adaptation to unseen speakers, and leverages domain
knowledge tailored to elderly speakers. Top-K most distinctive speaker prompt
clusters derived using K-means serve as experts. A router network is trained to
dynamically combine clustered prompt-experts. Acoustic and language level
variability among elderly speakers are modelled using separate encoder and
decoder prompts for Whisper. Experiments on the English DementiaBank Pitt and
Cantonese JCCOCC MoCA elderly speech datasets suggest that online MOPSA
adaptation outperforms the speaker-independent (SI) model by statistically
significant word error rate (WER) or character error rate (CER) reductions of
0.86% and 1.47% absolute (4.21% and 5.40% relative). Real-time factor (RTF)
speed-up ratios of up to 16.12 times are obtained over offline batch-mode
adaptation.

</details>


### [785] [Probing the Robustness Properties of Neural Speech Codecs](https://arxiv.org/pdf/2505.24248)
*Wei-Cheng Tseng, David Harwath*

Main category: eess.AS

TL;DR: Neural speech codecs are evaluated for robustness in noisy environments, revealing non-trivial differences and non-linear distortions affecting audio fidelity.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in neural speech codecs, their robustness in noisy environments is underexplored, raising concerns for real-world applications.

Method: Systematic evaluation of neural speech codecs under various noise conditions, analysis of linearity properties, and frequency response.

Result: Non-trivial differences in robustness and non-linear distortions were found, impacting audio fidelity.

Conclusion: The study highlights the importance of noise robustness for real-world integration and provides insights for future codec design.

Abstract: Neural speech codecs have revolutionized speech coding, achieving higher
compression while preserving audio fidelity. Beyond compression, they have
emerged as tokenization strategies, enabling language modeling on speech and
driving paradigm shifts across various speech processing tasks. Despite these
advancements, their robustness in noisy environments remains underexplored,
raising concerns about their generalization to real-world scenarios. In this
work, we systematically evaluate neural speech codecs under various noise
conditions, revealing non-trivial differences in their robustness. We further
examine their linearity properties, uncovering non-linear distortions which
partly explain observed variations in robustness. Lastly, we analyze their
frequency response to identify factors affecting audio fidelity. Our findings
provide critical insights into codec behavior and future codec design, as well
as emphasizing the importance of noise robustness for their real-world
integration.

</details>


### [786] [A Perception-Based L2 Speech Intelligibility Indicator: Leveraging a Rater's Shadowing and Sequence-to-sequence Voice Conversion](https://arxiv.org/pdf/2505.24304)
*Haopeng Geng, Daisuke Saito, Nobuaki Minematsu*

Main category: eess.AS

TL;DR: A new perception-based method for evaluating L2 speech intelligibility outperforms traditional ASR-based approaches by aligning better with native listener judgments.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR-based methods focus on native-likeness, which may not reflect actual intelligibility as perceived by human listeners.

Method: Uses a seq2seq voice conversion framework with shadowing data from native raters, integrating alignment and acoustic feature reconstruction to simulate native auditory perception.

Result: Objective and subjective evaluations show the method aligns more closely with native judgments than ASR-based metrics.

Conclusion: The approach offers a promising direction for improving CALL systems in multilingual contexts.

Abstract: Evaluating L2 speech intelligibility is crucial for effective
computer-assisted language learning (CALL). Conventional ASR-based methods
often focus on native-likeness, which may fail to capture the actual
intelligibility perceived by human listeners. In contrast, our work introduces
a novel, perception based L2 speech intelligibility indicator that leverages a
native rater's shadowing data within a sequence-to-sequence (seq2seq) voice
conversion framework. By integrating an alignment mechanism and acoustic
feature reconstruction, our approach simulates the auditory perception of
native listeners, identifying segments in L2 speech that are likely to cause
comprehension difficulties. Both objective and subjective evaluations indicate
that our method aligns more closely with native judgments than traditional
ASR-based metrics, offering a promising new direction for CALL systems in a
global, multilingual contexts.

</details>


### [787] [When Humans Growl and Birds Speak: High-Fidelity Voice Conversion from Human to Animal and Designed Sounds](https://arxiv.org/pdf/2505.24336)
*Minsu Kang, Seolhee Lee, Choonghyeon Lee, Namhyun Cho*

Main category: eess.AS

TL;DR: The paper introduces an improved H2NH-VC model for converting human speech to diverse non-human sounds, outperforming baselines in quality and naturalness.


<details>
  <summary>Details</summary>
Motivation: To expand beyond prior limited studies (e.g., dog sounds) and enable high-quality 44.1kHz conversion for a broader range of non-speech sounds (e.g., lion roars, synthetic growls).

Method: Proposes a preprocessing pipeline and an enhanced CVAE-based model optimized for diverse human and non-human voices.

Result: Outperformed baselines in MOS scores for quality, naturalness, and similarity, demonstrating effective conversion across varied non-human timbres.

Conclusion: The improved H2NH-VC model successfully addresses broader non-speech sound conversion with high-quality audio, validated by superior performance metrics.

Abstract: Human to non-human voice conversion (H2NH-VC) transforms human speech into
animal or designed vocalizations. Unlike prior studies focused on dog-sounds
and 16 or 22.05kHz audio transformation, this work addresses a broader range of
non-speech sounds, including natural sounds (lion-roars, birdsongs) and
designed voice (synthetic growls). To accomodate generation of diverse
non-speech sounds and 44.1kHz high-quality audio transformation, we introduce a
preprocessing pipeline and an improved CVAE-based H2NH-VC model, both optimized
for human and non-human voices. Experimental results showed that the proposed
method outperformed baselines in quality, naturalness, and similarity MOS,
achieving effective voice conversion across diverse non-human timbres. Demo
samples are available at
https://nc-ai.github.io/speech/publications/nonhuman-vc/

</details>


### [788] [Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation](https://arxiv.org/pdf/2505.24496)
*Wenrui Liu, Qian Chen, Wen Wang, Yafeng Chen, Jin Xu, Zhifang Guo, Guanrou Yang, Weiqin Li, Xiaoda Yang, Tao Jin, Minghui Fang, Jialong Zuo, Bai Jionghao, Zemin Liu*

Main category: eess.AS

TL;DR: The paper proposes a compressed-to-fine language modeling approach to handle long sequences of speech tokens in neural audio codecs, improving speech generation by focusing on local context and compressing long-range tokens.


<details>
  <summary>Details</summary>
Motivation: Neural audio codecs produce long token sequences for high-fidelity audio, challenging downstream language models. The authors note short-range dependencies in speech tokens and aim to reduce redundancy while preserving essential information.

Method: The approach retains fine-grained initial and short-range tokens for alignment and paralinguistic integrity, while compressing long-range tokens into compact representations to minimize redundancy.

Result: Experiments show the method's effectiveness and generalizability across various neural audio codecs and language models, enhancing speech generation.

Conclusion: Token compression is crucial for improving speech generation in neural codec language models, as demonstrated by the proposed compressed-to-fine approach.

Abstract: Neural audio codecs, used as speech tokenizers, have demonstrated remarkable
potential in the field of speech generation. However, to ensure high-fidelity
audio reconstruction, neural audio codecs typically encode audio into long
sequences of speech tokens, posing a significant challenge for downstream
language models in long-context modeling. We observe that speech token
sequences exhibit short-range dependency: due to the monotonic alignment
between text and speech in text-to-speech (TTS) tasks, the prediction of the
current token primarily relies on its local context, while long-range tokens
contribute less to the current token prediction and often contain redundant
information. Inspired by this observation, we propose a
\textbf{compressed-to-fine language modeling} approach to address the challenge
of long sequence speech tokens within neural codec language models: (1)
\textbf{Fine-grained Initial and Short-range Information}: Our approach retains
the prompt and local tokens during prediction to ensure text alignment and the
integrity of paralinguistic information; (2) \textbf{Compressed Long-range
Context}: Our approach compresses long-range token spans into compact
representations to reduce redundant information while preserving essential
semantics. Extensive experiments on various neural audio codecs and downstream
language models validate the effectiveness and generalizability of the proposed
approach, highlighting the importance of token compression in improving speech
generation within neural codec language models. The demo of audio samples will
be available at
https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.

</details>


### [789] [Pretraining Multi-Speaker Identification for Neural Speaker Diarization](https://arxiv.org/pdf/2505.24545)
*Shota Horiguchi, Atsushi Ando, Marc Delcroix, Naohiro Tawara*

Main category: eess.AS

TL;DR: Proposes pretraining a model to identify multiple speakers from overlapped mixtures, avoiding large-scale simulated data needs.


<details>
  <summary>Details</summary>
Motivation: Addresses the data-hungry nature of end-to-end speaker diarization by leveraging speaker recognition datasets instead of simulated conversational data.

Method: Pretrains a model to identify speakers from fully overlapped mixtures, eliminating the need for large-scale simulated datasets.

Result: Enables a highly accurate and lightweight local diarization model without simulated data.

Conclusion: The method effectively reduces storage and I/O demands while maintaining accuracy.

Abstract: End-to-end speaker diarization enables accurate overlap-aware diarization by
jointly estimating multiple speakers' speech activities in parallel. This
approach is data-hungry, requiring a large amount of labeled conversational
data, which cannot be fully obtained from real datasets alone. To address this
issue, large-scale simulated data is often used for pretraining, but it
requires enormous storage and I/O capacity, and simulating data that closely
resembles real conversations remains challenging. In this paper, we propose
pretraining a model to identify multiple speakers from an input fully
overlapped mixture as an alternative to pretraining a diarization model. This
method eliminates the need to prepare a large-scale simulated dataset while
leveraging large-scale speaker recognition datasets for training. Through
comprehensive experiments, we demonstrate that the proposed method enables a
highly accurate yet lightweight local diarization model without simulated
conversational data.

</details>


### [790] [Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models](https://arxiv.org/pdf/2505.24571)
*Nikola Ljubešić, Ivan Porupski, Peter Rupnik*

Main category: eess.AS

TL;DR: The paper explores fine-tuning a pre-trained transformer model for primary stress identification, outperforming traditional methods on Croatian and Serbian datasets, with slightly lower performance on Chakavian and Slovenian.


<details>
  <summary>Details</summary>
Motivation: Automating stress identification is crucial for meaning encoding and speech comprehension, but prior work focused on English and traditional acoustic features.

Method: Fine-tuning a pre-trained transformer model with an audio frame classification head, tested on Croatian, Serbian, Chakavian, and Slovenian datasets.

Result: The transformer model outperformed an SVM using acoustic features, achieving near-perfect results for Croatian and Serbian, with a 10-point drop for Chakavian and Slovenian.

Conclusion: The transformer approach is highly effective, requiring minimal training data, and the datasets and model are released under permissive licenses.

Abstract: Automating primary stress identification has been an active research field
due to the role of stress in encoding meaning and aiding speech comprehension.
Previous studies relied mainly on traditional acoustic features and English
datasets. In this paper, we investigate the approach of fine-tuning a
pre-trained transformer model with an audio frame classification head. Our
experiments use a new Croatian training dataset, with test sets in Croatian,
Serbian, the Chakavian dialect, and Slovenian. By comparing an SVM classifier
using traditional acoustic features with the fine-tuned speech transformer, we
demonstrate the transformer's superiority across the board, achieving
near-perfect results for Croatian and Serbian, with a 10-point performance drop
for the more distant Chakavian and Slovenian. Finally, we show that only a few
hundred multi-syllabic training words suffice for strong performance. We
release our datasets and model under permissive licenses.

</details>


### [791] [A Composite Predictive-Generative Approach to Monaural Universal Speech Enhancement](https://arxiv.org/pdf/2505.24576)
*Jie Zhang, Haoyin Yan, Xiaofei Li*

Main category: eess.AS

TL;DR: The paper proposes PGUSE, a universal speech enhancement model combining predictive and generative modeling to improve speech quality and suppress distortions.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of both predictive and generative methods while overcoming their limitations (artifacts in generative models and computational burden in diffusion models).

Method: PGUSE uses two branches: a predictive branch for direct clean sample prediction and a generative branch for diffusion-based denoising. Output fusion and truncated diffusion integrate these branches.

Result: Experiments show PGUSE outperforms state-of-the-art baselines, highlighting the benefits of combining predictive and generative approaches.

Conclusion: The hybrid model effectively enhances speech quality by combining predictive and generative modeling, demonstrating their complementarity.

Abstract: It is promising to design a single model that can suppress various
distortions and improve speech quality, i.e., universal speech enhancement
(USE). Compared to supervised learning-based predictive methods,
diffusion-based generative models have shown greater potential due to the
generative capacities from degraded speech with severely damaged information.
However, artifacts may be introduced in highly adverse conditions, and
diffusion models often suffer from a heavy computational burden due to many
steps for inference. In order to jointly leverage the superiority of prediction
and generation and overcome the respective defects, in this work we propose a
universal speech enhancement model called PGUSE by combining predictive and
generative modeling. Our model consists of two branches: the predictive branch
directly predicts clean samples from degraded signals, while the generative
branch optimizes the denoising objective of diffusion models. We utilize the
output fusion and truncated diffusion scheme to effectively integrate
predictive and generative modeling, where the former directly combines results
from both branches and the latter modifies the reverse diffusion process with
initial estimates from the predictive branch. Extensive experiments on several
datasets verify the superiority of the proposed model over state-of-the-art
baselines, demonstrating the complementarity and benefits of combining
predictive and generative modeling.

</details>


### [792] [ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting](https://arxiv.org/pdf/2504.20630)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao*

Main category: eess.AS

TL;DR: The paper introduces ISDrama, a model for generating immersive spatial drama using multimodal prompts, addressing challenges in spatial and prosodic modeling.


<details>
  <summary>Details</summary>
Motivation: The task of generating multi-speaker binaural speech with dramatic prosody for AR/VR applications is complex and data-intensive, with no prior solutions.

Method: The authors create MRSDrama dataset and propose ISDrama, featuring a Multimodal Pose Encoder and Immersive Drama Transformer with Drama-MOE and classifier-free guidance.

Result: ISDrama outperforms baselines in objective and subjective metrics.

Conclusion: The work pioneers immersive spatial drama generation, offering a dataset and model with promising applications.

Abstract: Multimodal immersive spatial drama generation focuses on creating continuous
multi-speaker binaural speech with dramatic prosody based on multimodal
prompts, with potential applications in AR, VR, and others. This task requires
simultaneous modeling of spatial information and dramatic prosody based on
multimodal inputs, with high data collection costs. To the best of our
knowledge, our work is the first attempt to address these challenges. We
construct MRSDrama, the first multimodal recorded spatial drama dataset,
containing binaural drama audios, scripts, videos, geometric poses, and textual
prompts. Then, we propose ISDrama, the first immersive spatial drama generation
model through multimodal prompting. ISDrama comprises these primary components:
1) Multimodal Pose Encoder, based on contrastive learning, considering the
Doppler effect caused by moving speakers to extract unified pose information
from multimodal prompts. 2) Immersive Drama Transformer, a flow-based
mamba-transformer model that generates high-quality drama, incorporating
Drama-MOE to select proper experts for enhanced prosody and pose control. We
also design a context-consistent classifier-free guidance strategy to
coherently generate complete drama. Experimental results show that ISDrama
outperforms baseline models on objective and subjective metrics. The demos and
dataset are available at https://aaronz345.github.io/ISDramaDemo.

</details>


### [793] ["Dyadosyncrasy", Idiosyncrasy and Demographic Factors in Turn-Taking](https://arxiv.org/pdf/2505.24736)
*Julio Cesar Cavalcanti, Gabriel Skantze*

Main category: eess.AS

TL;DR: The study explores how demographic and individual factors influence turn-taking in US English conversations, finding that dyadic relationships and joint activity are more impactful than demographics.


<details>
  <summary>Details</summary>
Motivation: To understand how sex, age, education, and individual differences shape turn-taking behavior in dialogue.

Method: Analysis of Transition Floor Offset (TFO) in a large dataset of US English conversations (Fisher corpus).

Result: Sex and age have small effects (shorter offsets for females and older individuals), while education has none. Dyadic relationships and individual differences dominate TFO variation.

Conclusion: Dyadic relationships and joint activity are the strongest determinants of turn-taking behavior, outweighing demographic factors.

Abstract: Turn-taking in dialogue follows universal constraints but also varies
significantly. This study examines how demographic (sex, age, education) and
individual factors shape turn-taking using a large dataset of US English
conversations (Fisher). We analyze Transition Floor Offset (TFO) and find
notable interspeaker variation. Sex and age have small but significant effects
female speakers and older individuals exhibit slightly shorter offsets - while
education shows no effect. Lighter topics correlate with shorter TFOs. However,
individual differences have a greater impact, driven by a strong idiosyncratic
and an even stronger "dyadosyncratic" component - speakers in a dyad resemble
each other more than they resemble themselves in different dyads. This suggests
that the dyadic relationship and joint activity are the strongest determinants
of TFO, outweighing demographic influences.

</details>


### [794] [StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis](https://arxiv.org/pdf/2312.10741)
*Yu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao*

Main category: eess.AS

TL;DR: StyleSinger is a singing voice synthesis model for zero-shot style transfer of out-of-domain singing voices, outperforming baselines in quality and similarity.


<details>
  <summary>Details</summary>
Motivation: Existing SVS methods struggle with quality in OOD scenarios due to training assumptions. StyleSinger aims to capture diverse singing styles without prior exposure.

Method: StyleSinger uses Residual Style Adaptor (RSA) to capture style nuances and Uncertainty Modeling Layer Normalization (UMLN) to improve generalization.

Result: StyleSinger excels in zero-shot style transfer, surpassing baselines in audio quality and reference similarity.

Conclusion: StyleSinger effectively addresses OOD style transfer challenges, offering high-quality synthesis for unseen singing styles.

Abstract: Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses
on generating high-quality singing voices with unseen styles (such as timbre,
emotion, pronunciation, and articulation skills) derived from reference singing
voice samples. However, the endeavor to model the intricate nuances of singing
voice styles is an arduous task, as singing voices possess a remarkable degree
of expressiveness. Moreover, existing SVS methods encounter a decline in the
quality of synthesized singing voices in OOD scenarios, as they rest upon the
assumption that the target vocal attributes are discernible during the training
phase. To overcome these challenges, we propose StyleSinger, the first singing
voice synthesis model for zero-shot style transfer of out-of-domain reference
singing voice samples. StyleSinger incorporates two critical approaches for
enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a
residual quantization module to capture diverse style characteristics in
singing voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to
perturb the style attributes within the content representation during the
training phase and thus improve the model generalization. Our extensive
evaluations in zero-shot style transfer undeniably establish that StyleSinger
outperforms baseline models in both audio quality and similarity to the
reference singing voice samples. Access to singing voice samples can be found
at https://aaronz345.github.io/StyleSingerDemo/.

</details>


### [795] [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/pdf/2504.19062)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao*

Main category: eess.AS

TL;DR: VersBand is a multi-task song generation framework addressing prompt-based control and alignment in vocal and accompaniment synthesis, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack proper alignment and control in song generation, limiting task versatility.

Method: VersBand includes VocalBand (flow-matching for vocals), AccompBand (flow-based transformer for accompaniments), and additional models for lyrics and melodies.

Result: VersBand outperforms baselines in quality, alignment, and control across tasks.

Conclusion: VersBand provides a comprehensive solution for high-quality, controllable song generation.

Abstract: Song generation focuses on producing controllable high-quality songs based on
various prompts. However, existing methods struggle to generate vocals and
accompaniments with prompt-based control and proper alignment. Additionally,
they fall short in supporting various tasks. To address these challenges, we
introduce VersBand, a multi-task song generation framework for synthesizing
high-quality, aligned songs with prompt-based control. VersBand comprises these
primary models: 1) VocalBand, a decoupled model, leverages the flow-matching
method for generating singing styles, pitches, and mel-spectrograms, allowing
fast, high-quality vocal generation with style control. 2) AccompBand, a
flow-based transformer model, incorporates the Band-MOE, selecting suitable
experts for enhanced quality, alignment, and control. This model allows for
generating controllable, high-quality accompaniments aligned with vocals. 3)
Two generation models, LyricBand for lyrics and MelodyBand for melodies,
contribute to the comprehensive multi-task song generation system, allowing for
extensive control based on multiple prompts. Experimental results demonstrate
that VersBand performs better over baseline models across multiple song
generation tasks using objective and subjective metrics. Audio samples are
available at https://aaronz345.github.io/VersBandDemo.

</details>


### [796] [Spatiotemporal Emotional Synchrony in Dyadic Interactions: The Role of Speech Conditions in Facial and Vocal Affective Alignment](https://arxiv.org/pdf/2505.13455)
*Von Ralph Dane Marquez Herbuela, Yukie Nagai*

Main category: eess.AS

TL;DR: The study explores how speech overlap affects emotional synchrony in facial and vocal expressions, finding non-overlapping speech promotes clearer alignment.


<details>
  <summary>Details</summary>
Motivation: To understand how conversational dynamics (speech overlap) influence emotional coordination across facial and vocal modalities for better emotion recognition and human-computer interaction.

Method: Analyzed dyadic interactions from IEMOCAP using EmoNet (facial) and Wav2Vec2 (speech). Segments categorized by speech overlap; emotional alignment assessed via Pearson correlation, lag analysis, and DTW.

Result: Non-overlapping speech showed more stable emotional synchrony, while overlapping speech had higher variability. Facial expressions often preceded speech in turn-taking, but speech led during overlaps.

Conclusion: Conversational structure significantly impacts emotional communication, revealing distinct coordination strategies in multimodal affective alignment.

Abstract: Understanding how humans express and synchronize emotions across multiple
communication channels particularly facial expressions and speech has
significant implications for emotion recognition systems and human computer
interaction. Motivated by the notion that non-overlapping speech promotes
clearer emotional coordination, while overlapping speech disrupts synchrony,
this study examines how these conversational dynamics shape the spatial and
temporal alignment of arousal and valence across facial and vocal modalities.
Using dyadic interactions from the IEMOCAP dataset, we extracted continuous
emotion estimates via EmoNet (facial video) and a Wav2Vec2-based model (speech
audio). Segments were categorized based on speech overlap, and emotional
alignment was assessed using Pearson correlation, lag adjusted analysis, and
Dynamic Time Warping (DTW). Across analyses, non overlapping speech was
associated with more stable and predictable emotional synchrony than
overlapping speech. While zero-lag correlations were low and not statistically
different, non overlapping speech showed reduced variability, especially for
arousal. Lag adjusted correlations and best-lag distributions revealed clearer,
more consistent temporal alignment in these segments. In contrast, overlapping
speech exhibited higher variability and flatter lag profiles, though DTW
indicated unexpectedly tighter alignment suggesting distinct coordination
strategies. Notably, directionality patterns showed that facial expressions
more often preceded speech during turn-taking, while speech led during
simultaneous vocalizations. These findings underscore the importance of
conversational structure in regulating emotional communication and provide new
insight into the spatial and temporal dynamics of multimodal affective
alignment in real world interaction.

</details>


### [797] [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/pdf/2505.14449)
*Yi-Cheng Lin, Huang-Cheng Chou, Hung-yi Lee*

Main category: eess.AS

TL;DR: The paper introduces an Implicit Demography Inference (IDI) module to address fairness in Speech Emotion Recognition (SER) without explicit demographic labels, improving fairness metrics significantly with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Fairness in categorical SER is underexplored, and existing methods rely on hard-to-obtain demographic labels due to privacy concerns.

Method: The proposed IDI module uses pseudo-labeling from a pre-trained model and unsupervised learning (k-means clustering) to mitigate bias.

Result: Pseudo-labeling IDI improves fairness metrics by over 28% with <2% accuracy drop; unsupervised IDI improves fairness by 4.6% with <3.6% performance drop.

Conclusion: The unsupervised IDI effectively mitigates race and age disparities, proving useful when explicit demographic data is unavailable.

Abstract: While subgroup disparities and performance bias are increasingly studied in
computational research, fairness in categorical Speech Emotion Recognition
(SER) remains underexplored. Existing methods often rely on explicit
demographic labels, which are difficult to obtain due to privacy concerns. To
address this limitation, we introduce an Implicit Demography Inference (IDI)
module that leverages pseudo-labeling from a pre-trained model and unsupervised
learning using k-means clustering to mitigate bias in SER. Our experiments show
that pseudo-labeling IDI reduces subgroup disparities, improving fairness
metrics by over 28% with less than a 2% decrease in SER accuracy. Also, the
unsupervised IDI yields more than a 4.6% improvement in fairness metrics with a
drop of less than 3.6% in SER performance. Further analyses reveal that the
unsupervised IDI consistently mitigates race and age disparities, demonstrating
its potential when explicit demographic information is unavailable.

</details>


### [798] [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/pdf/2505.14910)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Dongyu Yao, Zhiyuan Zhu, Ziyue Jiang, Yuhan Wang, Tao Jin, Zhou Zhao*

Main category: eess.AS

TL;DR: TCSinger 2 is a multilingual zero-shot singing voice synthesis model with style transfer and control, addressing limitations of existing models by introducing novel modules for smoother transitions and better style handling.


<details>
  <summary>Details</summary>
Motivation: Existing SVS models rely heavily on annotations, lack robustness in zero-shot scenarios, and have poor phoneme-note transitions. They also lack multi-level style control.

Method: TCSinger 2 includes three modules: Blurred Boundary Content Encoder for smooth transitions, Custom Audio Encoder for aligned representations, and Flow-based Custom Transformer for quality and style enhancement.

Result: TCSinger 2 outperforms baselines in subjective and objective metrics across tasks.

Conclusion: The model effectively addresses prior limitations, offering improved synthesis quality and style control.

Abstract: Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.
Singing voice samples are available at
https://aaronz345.github.io/TCSinger2Demo/.

</details>


### [799] [EASY: Emotion-aware Speaker Anonymization via Factorized Distillation](https://arxiv.org/pdf/2505.15004)
*Jixun Yao, Hexin Liu, Eng Siong Chng, Lei Xie*

Main category: eess.AS

TL;DR: EASY is an emotion-aware speaker anonymization framework that disentangles speaker identity, linguistic content, and emotional representation, outperforming baselines in privacy protection while preserving emotion and content.


<details>
  <summary>Details</summary>
Motivation: Existing speaker anonymization systems often neglect emotional state preservation, focusing only on linguistic content and speaker identity.

Method: EASY uses sequential disentanglement to model speaker identity, linguistic content, and emotional representation in distinct subspaces via factorized distillation.

Result: EASY outperforms baseline systems on the VoicePrivacy Challenge datasets, effectively protecting privacy while maintaining linguistic content and emotional state.

Conclusion: EASY successfully addresses the gap in emotion preservation in speaker anonymization, offering enhanced privacy without compromising emotional or linguistic fidelity.

Abstract: Emotion plays a significant role in speech interaction, conveyed through
tone, pitch, and rhythm, enabling the expression of feelings and intentions
beyond words to create a more personalized experience. However, most existing
speaker anonymization systems employ parallel disentanglement methods, which
only separate speech into linguistic content and speaker identity, often
neglecting the preservation of the original emotional state. In this study, we
introduce EASY, an emotion-aware speaker anonymization framework. EASY employs
a novel sequential disentanglement process to disentangle speaker identity,
linguistic content, and emotional representation, modeling each speech
attribute in distinct subspaces through a factorized distillation approach. By
independently constraining speaker identity and emotional representation, EASY
minimizes information leakage, enhancing privacy protection while preserving
original linguistic content and emotional state. Experimental results on the
VoicePrivacy Challenge official datasets demonstrate that our proposed approach
outperforms all baseline systems, effectively protecting speaker privacy while
maintaining linguistic content and emotional state.

</details>


### [800] [MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous Decoding](https://arxiv.org/pdf/2505.19577)
*Yu Xi, Haoyu Li, Xiaoyu Gu, Yidi Jiang, Kai Yu*

Main category: eess.AS

TL;DR: The paper proposes MFA-KWS, a keyword-specific KWS framework combining CTC-Transducer with multi-head frame-asynchronous decoding, outperforming traditional methods in accuracy, efficiency, and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR-based KWS methods lack explicit keyword prioritization, leading to suboptimal performance. The paper aims to improve accuracy and efficiency for voice-driven applications.

Method: Introduces MFA-KWS, using keyword-specific phone-synchronous decoding for CTC and replacing RNN-T with Token-and-Duration Transducer. Explores score fusion strategies like single-frame-based and consistency-based methods.

Result: MFA-KWS achieves state-of-the-art results on datasets like Snips and LibriKWS-20, with 47-63% speed-up over baselines. Consistency-based CDC-Last fusion performs best.

Conclusion: MFA-KWS is an effective, efficient KWS framework suitable for on-device deployment, offering superior performance and robustness.

Abstract: Keyword spotting (KWS) is essential for voice-driven applications, demanding
both accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy
and beam search, explore the entire search space without explicitly
prioritizing keyword detection, often leading to suboptimal performance. In
this paper, we propose an effective keyword-specific KWS framework by
introducing a streaming-oriented CTC-Transducer-combined frame-asynchronous
system with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,
MFA-KWS employs keyword-specific phone-synchronous decoding for CTC and
replaces conventional RNN-T with Token-and-Duration Transducer to enhance both
performance and efficiency. Furthermore, we explore various score fusion
strategies, including single-frame-based and consistency-based methods.
Extensive experiments demonstrate the superior performance of MFA-KWS, which
achieves state-of-the-art results on both fixed keyword and arbitrary keywords
datasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting
strong robustness in noisy environments. Among fusion strategies, the
consistency-based CDC-Last method delivers the best performance. Additionally,
MFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines
across various datasets. Extensive experimental results confirm that MFA-KWS is
an effective and efficient KWS framework, making it well-suited for on-device
deployment.

</details>


### [801] [Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment](https://arxiv.org/pdf/2505.19595)
*Jeongsoo Choi, Zhikang Niu, Ji-Hoon Kim, Chunhui Wang, Joon Son Chung, Xie Chen*

Main category: eess.AS

TL;DR: A-DMA accelerates diffusion-based text-to-speech training by aligning text and speech modalities, doubling convergence speed while outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Optimize the slow and costly training of diffusion-based text-to-speech models by reducing reliance on complex intermediate representations.

Method: Introduces A-DMA, a dual modality alignment strategy with text-guided and speech-guided alignment to refine representations.

Result: Doubles convergence speed and achieves superior performance compared to baselines.

Conclusion: A-DMA is an effective solution for faster and better-performing text-to-speech model training.

Abstract: The goal of this paper is to optimize the training process of diffusion-based
text-to-speech models. While recent studies have achieved remarkable
advancements, their training demands substantial time and computational costs,
largely due to the implicit guidance of diffusion models in learning complex
intermediate representations. To address this, we propose A-DMA, an effective
strategy for Accelerating training with Dual Modality Alignment. Our method
introduces a novel alignment pipeline leveraging both text and speech
modalities: text-guided alignment, which incorporates contextual
representations, and speech-guided alignment, which refines semantic
representations. By aligning hidden states with discriminative features, our
training scheme reduces the reliance on diffusion models for learning complex
representations. Extensive experiments demonstrate that A-DMA doubles the
convergence speed while achieving superior performance over baselines. Code and
demo samples are available at: https://github.com/ZhikangNiu/A-DMA

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [802] [Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction](https://arxiv.org/pdf/2505.23872)
*Anam Hashmi, Julia Dietlmeier, Kathleen M. Curran, Noel E. O'Connor*

Main category: eess.IV

TL;DR: Proposes a non-linear attention architecture for cardiac MRI reconstruction, inspired by ecological principles, to improve model performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing attention modules lack robust theoretical underpinning. The study aims to bridge this gap by leveraging ecological principles for better attention mechanisms.

Method: Investigates a non-linear ecological difference equation to devise a parameter-free attention module.

Result: The proposed module surpasses current state-of-the-art parameter-free methods.

Conclusion: Ecological principles can guide the development of effective and efficient attention mechanisms, enhancing performance in medical imaging tasks.

Abstract: Attention is a fundamental component of the human visual recognition system.
The inclusion of attention in a convolutional neural network amplifies relevant
visual features and suppresses the less important ones. Integrating attention
mechanisms into convolutional neural networks enhances model performance and
interpretability. Spatial and channel attention mechanisms have shown
significant advantages across many downstream tasks in medical imaging. While
existing attention modules have proven to be effective, their design often
lacks a robust theoretical underpinning. In this study, we address this gap by
proposing a non-linear attention architecture for cardiac MRI reconstruction
and hypothesize that insights from ecological principles can guide the
development of effective and efficient attention mechanisms. Specifically, we
investigate a non-linear ecological difference equation that describes
single-species population growth to devise a parameter-free attention module
surpassing current state-of-the-art parameter-free methods.

</details>


### [803] [Estimating Head Motion in Structural MRI Using a Deep Neural Network Trained on Synthetic Artifacts](https://arxiv.org/pdf/2505.23916)
*Charles Bricout, Samira Ebrahimi Kahou, Sylvain Bouix*

Main category: eess.IV

TL;DR: A 3D CNN is trained to estimate motion severity in MRI scans using synthetic data, achieving good correlation with manual labels and generalizing across scanners.


<details>
  <summary>Details</summary>
Motivation: Motion artifacts in MRI bias neuroanatomical metrics, and existing automated methods have limitations like hardware requirements or noisy data.

Method: A 3D convolutional neural network is trained on synthetically corrupted MRI volumes to estimate motion severity.

Result: The method achieves $R^2 = 0.65$ against manual labels and shows significant thickness-motion correlations in 12/15 datasets. It also correlates with subject age.

Conclusion: The approach generalizes across scanner brands and protocols, enabling scalable, objective motion assessment in structural MRI without specialized hardware.

Abstract: Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)
and can bias automated neuroanatomical metrics such as cortical thickness.
Manual review cannot objectively quantify motion in anatomical scans, and
existing automated approaches often require specialized hardware or rely on
unbalanced noisy training data. Here, we train a 3D convolutional neural
network to estimate motion severity using only synthetically corrupted volumes.
We validate our method with one held-out site from our training cohort and with
14 fully independent datasets, including one with manual ratings, achieving a
representative $R^2 = 0.65$ versus manual labels and significant
thickness-motion correlations in 12/15 datasets. Furthermore, our predicted
motion correlates with subject age in line with prior studies. Our approach
generalizes across scanner brands and protocols, enabling objective, scalable
motion assessment in structural MRI studies without prospective motion
correction.

</details>


### [804] [Improved Accuracy in Pelvic Tumor Resections Using a Real-Time Vision-Guided Surgical System](https://arxiv.org/pdf/2505.23984)
*Vahid Danesh, Paul Arauz, Maede Boroji, Andrew Zhu, Mia Cottone, Elaine Gould, Fazel A. Khan, Imin Kao*

Main category: eess.IV

TL;DR: A vision-guided surgical system with modular jigs improves accuracy in pelvic bone tumor resections, reducing deviations significantly compared to freehand methods.


<details>
  <summary>Details</summary>
Motivation: Current navigation systems and patient-specific instruments for pelvic bone tumor resections have limitations like high costs, radiation exposure, and workflow disruption.

Method: Developed a vision-guided system with modular jigs and real-time optical tracking, tested on sawbones with 20 resections per method.

Result: Reduced mean distance deviation from 2.07 mm to 1.01 mm and improved angular deviations (roll: 15.36° to 4.21°; pitch: 6.17° to 1.84°).

Conclusion: The system enhances accuracy and workflow efficiency, offering a cost-effective solution for complex pelvic tumor resections.

Abstract: Pelvic bone tumor resections remain significantly challenging due to complex
three-dimensional anatomy and limited surgical visualization. Current
navigation systems and patient-specific instruments, while accurate, present
limitations including high costs, radiation exposure, workflow disruption, long
production time, and lack of reusability. This study evaluates a real-time
vision-guided surgical system combined with modular jigs to improve accuracy in
pelvic bone tumor resections. A vision-guided surgical system combined with
modular cutting jigs and real-time optical tracking was developed and
validated. Five female pelvis sawbones were used, with each hemipelvis randomly
assigned to either the vision-guided and modular jig system or traditional
freehand method. A total of twenty resection planes were analyzed for each
method. Accuracy was assessed by measuring distance and angular deviations from
the planned resection planes. The vision-guided and modular jig system
significantly improved resection accuracy compared to the freehand method,
reducing the mean distance deviation from 2.07 $\pm$ 1.71 mm to 1.01 $\pm$ 0.78
mm (p=0.0193). In particular, all specimens resected using the vision-guided
system exhibited errors of less than 3 mm. Angular deviations also showed
significant improvements with roll angle deviation reduced from 15.36 $\pm$
17.57$^\circ$ to 4.21 $\pm$ 3.46$^\circ$ (p=0.0275), and pitch angle deviation
decreased from 6.17 $\pm$ 4.58$^\circ$ to 1.84 $\pm$ 1.48$^\circ$ (p<0.001).
The proposed vision-guided and modular jig system significantly improves the
accuracy of pelvic bone tumor resections while maintaining workflow efficiency.
This cost-effective solution provides real-time guidance without the need for
referencing external monitors, potentially improving surgical outcomes in
complex pelvic bone tumor cases.

</details>


### [805] [Semantics-Guided Generative Image Compression](https://arxiv.org/pdf/2505.24015)
*Cheng-Lin Wu, Hyomin Choi, Ivan V. Bajić*

Main category: eess.IV

TL;DR: The paper introduces enhancements to multimodal image semantic compression (MISC) using AI, improving image quality and reducing complexity.


<details>
  <summary>Details</summary>
Motivation: To advance text-to-image generative AI in image compression, achieving high-quality results at low bit rates.

Method: Adds semantic segmentation guidance and content-adaptive diffusion to the MISC framework.

Result: Improves PSNR and perceptual metrics, reduces encoding/decoding time by over 36%, and outperforms mainstream codecs.

Conclusion: The proposed methods enhance MISC, offering better quality and efficiency in image compression.

Abstract: Advancements in text-to-image generative AI with large multimodal models are
spreading into the field of image compression, creating high-quality
representation of images at extremely low bit rates. This work introduces novel
components to the existing multimodal image semantic compression (MISC)
approach, enhancing the quality of the generated images in terms of PSNR and
perceptual metrics. The new components include semantic segmentation guidance
for the generative decoder, as well as content-adaptive diffusion, which
controls the number of diffusion steps based on image characteristics. The
results show that our newly introduced methods significantly improve the
baseline MISC model while also decreasing the complexity. As a result, both the
encoding and decoding time are reduced by more than 36%. Moreover, the proposed
compression framework outperforms mainstream codecs in terms of perceptual
similarity and quality. The code and visual examples are available.

</details>


### [806] [Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction](https://arxiv.org/pdf/2505.24136)
*Yaşar Utku Alçalar, Mehmet Akçakaya*

Main category: eess.IV

TL;DR: A novel self-supervised training method for physics-driven deep learning in MRI reconstruction reduces artifacts and noise at high acceleration rates.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of self-supervised learning in MRI reconstruction, particularly artifacts at high acceleration rates.

Method: Enhances k-space masking with a consistency term to predict perturbations in a sparse domain.

Result: Reduces aliasing artifacts and noise amplification, outperforming state-of-the-art methods.

Conclusion: The proposed training strategy improves image fidelity in rapid MRI scans.

Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful
approach for improved reconstruction of rapid MRI scans. In order to train
these models in scenarios where fully-sampled reference data is unavailable,
self-supervised learning has gained prominence. However, its application at
high acceleration rates frequently introduces artifacts, compromising image
fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL
networks via carefully-designed perturbations. In particular, we enhance the
k-space masking idea of conventional self-supervised learning with a novel
consistency term that assesses the model's ability to accurately predict the
added perturbations in a sparse domain, leading to more reliable and
artifact-free reconstructions. The results obtained from the fastMRI knee and
brain datasets show that the proposed training strategy effectively reduces
aliasing artifacts and mitigates noise amplification at high acceleration
rates, outperforming state-of-the-art self-supervised methods both visually and
quantitatively.

</details>


### [807] [Beyond the LUMIR challenge: The pathway to foundational registration models](https://arxiv.org/pdf/2505.24160)
*Junyu Chen, Shuwen Wei, Joel Honkamaa, Pekka Marttinen, Hang Zhang, Min Liu, Yichao Zhou, Zuopeng Tan, Zhuoyuan Wang, Yi Wang, Hongchao Zhou, Shunbo Hu, Yi Zhang, Qian Tao, Lukas Förner, Thomas Wendler, Bailiang Jian, Benedikt Wiestler, Tim Hable, Jin Kim, Dan Ruan, Frederic Madesta, Thilo Sentker, Wiebke Heyer, Lianrui Zuo, Yuwei Dai, Jing Wu, Jerry L. Prince, Harrison Bai, Yong Du, Yihao Liu, Alessa Hering, Reuben Dorent, Lasse Hansen, Mattias P. Heinrich, Aaron Carass*

Main category: eess.IV

TL;DR: The LUMIR challenge introduces a large-scale unsupervised benchmark for brain MRI registration, removing dependency on label maps and evaluating performance on diverse tasks, with deep learning methods achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To advance unsupervised brain MRI registration by eliminating reliance on anatomical label maps and assessing generalization across diverse imaging scenarios.

Method: LUMIR provides 4,000 preprocessed T1-weighted brain MRIs for unsupervised training, evaluating performance on 590 test subjects and zero-shot tasks across modalities, diseases, protocols, and species.

Result: Deep learning methods achieved top accuracy with biologically plausible deformations, outperforming optimization-based methods and showing robustness to domain shifts, except for out-of-domain contrasts.

Conclusion: LUMIR successfully advances unsupervised brain MRI registration, demonstrating the effectiveness of deep learning in handling diverse and challenging scenarios.

Abstract: Medical image challenges have played a transformative role in advancing the
field, catalyzing algorithmic innovation and establishing new performance
standards across diverse clinical applications. Image registration, a
foundational task in neuroimaging pipelines, has similarly benefited from the
Learn2Reg initiative. Building on this foundation, we introduce the Large-scale
Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation
benchmark designed to assess and advance unsupervised brain MRI registration.
Distinct from prior challenges that leveraged anatomical label maps for
supervision, LUMIR removes this dependency by providing over 4,000 preprocessed
T1-weighted brain MRIs for training without any label maps, encouraging
biologically plausible deformation modeling through self-supervision. In
addition to evaluating performance on 590 held-out test subjects, LUMIR
introduces a rigorous suite of zero-shot generalization tasks, spanning
out-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),
disease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,
9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and
over 4,000 image pairs were included for evaluation. Performance was assessed
using both segmentation-based metrics (Dice coefficient, 95th percentile
Hausdorff distance) and landmark-based registration accuracy (target
registration error). Across both in-domain and zero-shot tasks, deep
learning-based methods consistently achieved state-of-the-art accuracy while
producing anatomically plausible deformation fields. The top-performing deep
learning-based models demonstrated diffeomorphic properties and inverse
consistency, outperforming several leading optimization-based methods, and
showing strong robustness to most domain shifts, the exception being a drop in
performance on out-of-domain contrasts.

</details>


### [808] [Deep learning-derived arterial input function](https://arxiv.org/pdf/2505.24166)
*Junyu Chen, Zirui Jiang, Jennifer M. Coughlin, Martin G. Pomper, Yong Du*

Main category: eess.IV

TL;DR: A deep learning framework (DLIF) estimates metabolite-corrected arterial input function (AIF) from PET images without blood sampling, offering a non-invasive alternative to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional AIF measurement requires invasive blood sampling, which is labor-intensive. Non-invasive alternatives often lack accuracy or still need some blood sampling.

Method: DLIF uses deep learning to estimate AIF directly from dynamic PET images, leveraging temporal dynamics and prior knowledge of AIF shapes.

Result: DLIF achieves accurate and robust AIF estimation, validated against ground truth measurements.

Conclusion: DLIF provides a rapid, accurate, and entirely non-invasive alternative to traditional AIF methods, enhancing PET imaging for brain studies.

Abstract: Dynamic positron emission tomography (PET) imaging combined with radiotracer
kinetic modeling is a powerful technique for visualizing biological processes
in the brain, offering valuable insights into brain functions and neurological
disorders such as Alzheimer's and Parkinson's diseases. Accurate kinetic
modeling relies heavily on the use of a metabolite-corrected arterial input
function (AIF), which typically requires invasive and labor-intensive arterial
blood sampling. While alternative non-invasive approaches have been proposed,
they often compromise accuracy or still necessitate at least one invasive blood
sampling. In this study, we present the deep learning-derived arterial input
function (DLIF), a deep learning framework capable of estimating a
metabolite-corrected AIF directly from dynamic PET image sequences without any
blood sampling. We validated DLIF using existing dynamic PET patient data. We
compared DLIF and resulting parametric maps against ground truth measurements.
Our evaluation shows that DLIF achieves accurate and robust AIF estimation. By
leveraging deep learning's ability to capture complex temporal dynamics and
incorporating prior knowledge of typical AIF shapes through basis functions,
DLIF provides a rapid, accurate, and entirely non-invasive alternative to
traditional AIF measurement methods.

</details>


### [809] [A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization](https://arxiv.org/pdf/2505.24351)
*Peng Qi, Wenxi Qu, Tianliang Yao, Haonan Ma, Dylan Wintle, Yinyi Lai, Giorgos Papanastasiou, Chengjia Wang*

Main category: eess.IV

TL;DR: A novel multimodal coronary artery image registration method using swarm optimization improves PCI by combining real-time XRA and detailed 3D CTA, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance PCI by accurately registering 2D XRA and 3D CTA images despite challenges like modality differences and noise.

Method: Preprocessing of XRA and CTA images, followed by feature extraction using Steger and Superpixel Particle Swarm Optimization algorithms.

Result: Outperformed four SOTA methods in accuracy, robustness, and efficiency on a dataset of 28 image pairs.

Conclusion: The proposed method offers a promising clinical approach to improve PCI outcomes for coronary artery disease.

Abstract: Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure
that improves coronary blood flow and treats coronary artery disease. Although
PCI typically requires 2D X-ray angiography (XRA) to guide catheter placement
at real-time, computed tomography angiography (CTA) may substantially improve
PCI by providing precise information of 3D vascular anatomy and status. To
leverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal
image registration of XRA and CTA is required, to guide the procedure and avoid
complications. This is a challenging process as it requires registration of
images from different geometrical modalities (2D -> 3D and vice versa), with
variations in contrast and noise levels. In this paper, we propose a novel
multimodal coronary artery image registration method based on a swarm
optimization algorithm, which effectively addresses challenges such as large
deformations, low contrast, and noise across these imaging modalities. Our
algorithm consists of two main modules: 1) preprocessing of XRA and CTA images
separately, and 2) a registration module based on feature extraction using the
Steger and Superpixel Particle Swarm Optimization algorithms. Our technique was
evaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients
who underwent PCI. The algorithm was compared with four state-of-the-art (SOTA)
methods in terms of registration accuracy, robustness, and efficiency. Our
method outperformed the selected SOTA baselines in all aspects. Experimental
results demonstrate the significant effectiveness of our algorithm, surpassing
the previous benchmarks and proposes a novel clinical approach that can
potentially have merit for improving patient outcomes in coronary artery
disease.

</details>


### [810] [Efficient RAW Image Deblurring with Adaptive Frequency Modulation](https://arxiv.org/pdf/2505.24407)
*Wenlong Jiao, Binglong Li, Wei Shang, Ping Wang, Dongwei Ren*

Main category: eess.IV

TL;DR: FrENet is a frequency-domain framework for RAW-to-RAW deblurring, outperforming state-of-the-art methods with adaptive frequency modulation and efficient computation.


<details>
  <summary>Details</summary>
Motivation: RAW images offer superior restoration potential over sRGB images but are underexplored in deblurring due to challenges like frequency-dependent blur and computational efficiency.

Method: Proposes FrENet with an Adaptive Frequency Positional Modulation module and frequency domain skip connections for precise deblurring.

Result: FrENet achieves better restoration quality and efficiency, also extending effectively to sRGB images.

Conclusion: FrENet is a versatile and efficient solution for image deblurring, particularly for RAW images, with potential for broader applications.

Abstract: Image deblurring plays a crucial role in enhancing visual clarity across
various applications. Although most deep learning approaches primarily focus on
sRGB images, which inherently lose critical information during the image signal
processing pipeline, RAW images, being unprocessed and linear, possess superior
restoration potential but remain underexplored. Deblurring RAW images presents
unique challenges, particularly in handling frequency-dependent blur while
maintaining computational efficiency. To address these issues, we propose
Frequency Enhanced Network (FrENet), a framework specifically designed for
RAW-to-RAW deblurring that operates directly in the frequency domain. We
introduce a novel Adaptive Frequency Positional Modulation module, which
dynamically adjusts frequency components according to their spectral positions,
thereby enabling precise control over the deblurring process. Additionally,
frequency domain skip connections are adopted to further preserve
high-frequency details. Experimental results demonstrate that FrENet surpasses
state-of-the-art deblurring methods in RAW image deblurring, achieving
significantly better restoration quality while maintaining high efficiency in
terms of reduced MACs. Furthermore, FrENet's adaptability enables it to be
extended to sRGB images, where it delivers comparable or superior performance
compared to methods specifically designed for sRGB data. The code will be
available at https://github.com/WenlongJiao/FrENet .

</details>


### [811] [pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation](https://arxiv.org/pdf/2505.24421)
*Abdul-mojeed Olabisi Ilyas, Adeleke Maradesa, Jamal Banzi, Jianpan Huang, Henry K. F. Mak, Kannie W. Y. Chan*

Main category: eess.IV

TL;DR: The paper introduces MEAL, a Multi-encoder Augmentation-Aware Learning framework, to address challenges in 3D medical imaging by leveraging distinct augmentation variants and fusion strategies, achieving superior performance in CT-to-MRI translation.


<details>
  <summary>Details</summary>
Motivation: Challenges in AI-driven medical imaging include patient variability, image artifacts, and limited model generalization, exacerbated by data scarcity and inconsistencies in 3D imaging. Traditional augmentation methods are inefficient for large data volumes.

Method: MEAL uses four distinct augmentation variants processed through dedicated encoders, integrated via three fusion strategies (CC, FL, BD). MEAL-BD preserves augmentation-aware representations for robust feature learning.

Result: MEAL-BD outperformed other methods in CT-to-MRI translation, achieving higher PSNR and SSIM scores on both augmented and non-augmented inputs, demonstrating protocol-invariant generalization.

Conclusion: MEAL reframes augmentation as a source of diverse features, enabling robust, clinically reliable medical imaging solutions by preserving structural fidelity and generalizing across variability.

Abstract: Medical imaging is critical for diagnostics, but clinical adoption of
advanced AI-driven imaging faces challenges due to patient variability, image
artifacts, and limited model generalization. While deep learning has
transformed image analysis, 3D medical imaging still suffers from data scarcity
and inconsistencies due to acquisition protocols, scanner differences, and
patient motion. Traditional augmentation uses a single pipeline for all
transformations, disregarding the unique traits of each augmentation and
struggling with large data volumes.
  To address these challenges, we propose a Multi-encoder Augmentation-Aware
Learning (MEAL) framework that leverages four distinct augmentation variants
processed through dedicated encoders. Three fusion strategies such as
concatenation (CC), fusion layer (FL), and adaptive controller block (BD) are
integrated to build multi-encoder models that combine augmentation-specific
features before decoding. MEAL-BD uniquely preserves augmentation-aware
representations, enabling robust, protocol-invariant feature learning.
  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic
Resonance Imaging (MRI) translation study, MEAL-BD consistently achieved the
best performance on both unseen- and predefined-test data. On both geometric
transformations (like rotations and flips) and non-augmented inputs, MEAL-BD
outperformed other competing methods, achieving higher mean peak
signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)
scores. These results establish MEAL as a reliable framework for preserving
structural fidelity and generalizing across clinically relevant variability. By
reframing augmentation as a source of diverse, generalizable features, MEAL
supports robust, protocol-invariant learning, advancing clinically reliable
medical imaging solutions.

</details>


### [812] [Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution](https://arxiv.org/pdf/2505.24605)
*Ivan Pereira-Sánchez, Julia Navarro, Ana Belén Petro, Joan Duran*

Main category: eess.IV

TL;DR: The paper proposes an end-to-end model-driven framework for joint spatio-spectral super-resolution of hyperspectral images, decomposing the problem into spatial, spectral, and fusion tasks with learnable modules.


<details>
  <summary>Details</summary>
Motivation: Joint spatio-spectral super-resolution is understudied compared to separate spatial or spectral super-resolution.

Method: The framework decomposes the problem into spatial super-resolution, spectral super-resolution, and fusion tasks, using learnable modules and a variational-based approach.

Result: Extensive evaluations show the approach's effectiveness across datasets and scaling factors.

Conclusion: The proposed method successfully addresses joint spatio-spectral super-resolution with promising results.

Abstract: This paper addresses the problem of reconstructing a high-resolution
hyperspectral image from a low-resolution multispectral observation. While
spatial super-resolution and spectral super-resolution have been extensively
studied, joint spatio-spectral super-resolution remains relatively explored. We
propose an end-to-end model-driven framework that explicitly decomposes the
joint spatio-spectral super-resolution problem into spatial super-resolution,
spectral super-resolution and fusion tasks. Each sub-task is addressed by
unfolding a variational-based approach, where the operators involved in the
proximal gradient iterative scheme are replaced with tailored learnable
modules. In particular, we design an upsampling operator for spatial
super-resolution based on classical back-projection algorithms, adapted to
handle arbitrary scaling factors. Spectral reconstruction is performed using
learnable cluster-based upsampling and downsampling operators. For image
fusion, we integrate low-frequency estimation and high-frequency injection
modules to combine the spatial and spectral information from spatial
super-resolution and spectral super-resolution outputs. Additionally, we
introduce an efficient nonlocal post-processing step that leverages image
self-similarity by combining a multi-head attention mechanism with residual
connections. Extensive evaluations on several datasets and sampling factors
demonstrate the effectiveness of our approach. The source code will be
available at https://github.com/TAMI-UIB/JSSUNet

</details>


### [813] [TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching](https://arxiv.org/pdf/2505.24687)
*Shengyuan Liu, Wenting Chen, Boyun Zheng, Wentao Pan, Xiang Li, Yixuan Yuan*

Main category: eess.IV

TL;DR: TumorGen introduces a boundary-aware tumor-mask synthesis method using rectified flow matching for efficient 3D tumor synthesis, improving computational efficiency and realism.


<details>
  <summary>Details</summary>
Motivation: Addressing the shortage of annotated medical datasets and limitations of current tumor synthesis methods, such as restricted tumor diversity, computational inefficiency, and unrealistic binary masks.

Method: TumorGen combines a Boundary-Aware Pseudo Mask Generation module, Spatial-Constraint Vector Field Estimator, and VAE-guided mask refiner to synthesize tumors efficiently with realistic boundaries.

Result: TumorGen outperforms existing methods in computational efficiency and realism, requiring fewer sampling steps while maintaining pathological accuracy.

Conclusion: TumorGen provides an efficient and realistic solution for tumor synthesis, advancing AI-driven cancer diagnostics.

Abstract: Tumor data synthesis offers a promising solution to the shortage of annotated
medical datasets. However, current approaches either limit tumor diversity by
using predefined masks or employ computationally expensive two-stage processes
with multiple denoising steps, causing computational inefficiency.
Additionally, these methods typically rely on binary masks that fail to capture
the gradual transitions characteristic of tumor boundaries. We present
TumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow
Matching for efficient 3D tumor synthesis with three key components: a
Boundary-Aware Pseudo Mask Generation module that replaces strict binary masks
with flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that
simultaneously synthesizes tumor latents and masks using rectified flow
matching to ensure computational efficiency; and a VAE-guided mask refiner that
enhances boundary realism. TumorGen significantly improves computational
efficiency by requiring fewer sampling steps while maintaining pathological
accuracy through coarse and fine-grained spatial constraints. Experimental
results demonstrate TumorGen's superior performance over existing tumor
synthesis methods in both efficiency and realism, offering a valuable
contribution to AI-driven cancer diagnostics.

</details>


### [814] [Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI](https://arxiv.org/pdf/2505.24739)
*Xinliu Zhong, Ruiying Liu, Emily S. Nichols, Xuzhe Zhang, Andrew F. Laine, Emma G. Duerden, Yun Wang*

Main category: eess.IV

TL;DR: A framework for placental segmentation in T2*-weighted MRI using multi-echo data, addressing challenges like weak boundaries and motion artifacts.


<details>
  <summary>Details</summary>
Motivation: Accurate placental segmentation is crucial but difficult due to weak boundary contrast, lack of ground truth for all echoes, and motion artifacts.

Method: Combines masked autoencoding (MAE), masked pseudo-labeling (MPL), and global-local collaboration with a semantic matching loss for robust segmentation.

Result: Outperforms single-echo and naive fusion baselines, generalizing well across echo times.

Conclusion: First systematic use of multi-echo T2*-weighted MRI for placental segmentation, offering a robust solution.

Abstract: Accurate placental segmentation is essential for quantitative analysis of the
placenta. However, this task is particularly challenging in T2*-weighted
placental imaging due to: (1) weak and inconsistent boundary contrast across
individual echoes; (2) the absence of manual ground truth annotations for all
echo times; and (3) motion artifacts across echoes caused by fetal and maternal
movement. In this work, we propose a contrast-augmented segmentation framework
that leverages complementary information across multi-echo T2*-weighted MRI to
learn robust, contrast-invariant representations. Our method integrates: (i)
masked autoencoding (MAE) for self-supervised pretraining on unlabeled
multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain
adaptation across echo times; and (iii) global-local collaboration to align
fine-grained features with global anatomical context. We further introduce a
semantic matching loss to encourage representation consistency across echoes of
the same subject. Experiments on a clinical multi-echo placental MRI dataset
demonstrate that our approach generalizes effectively across echo times and
outperforms both single-echo and naive fusion baselines. To our knowledge, this
is the first work to systematically exploit multi-echo T2*-weighted MRI for
placental segmentation.

</details>


### [815] [Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images](https://arxiv.org/pdf/2505.24799)
*Aditya Retnanto, Son Le, Sebastian Mueller, Armin Leitner, Konrad Schindler, Yohan Iddawela, Michael Riffler*

Main category: eess.IV

TL;DR: SEN4X is a hybrid super-resolution method for Sentinel-2 satellite images, combining temporal oversampling and learned priors to achieve 2.5 m resolution, improving urban land-cover classification.


<details>
  <summary>Details</summary>
Motivation: Sentinel-2's coarse resolution limits its ability to capture small features; SEN4X aims to enhance resolution for better Earth observation.

Method: Hybrid architecture combining single-image and multi-image techniques, using temporal oversampling from Sentinel-2 and high-resolution Pléiades Neo data.

Result: Achieves 2.5 m resolution and outperforms state-of-the-art super-resolution methods in urban land-cover classification.

Conclusion: SEN4X effectively enhances Sentinel-2 imagery, enabling better feature detection and classification.

Abstract: Super-resolution aims to increase the resolution of satellite images by
reconstructing high-frequency details, which go beyond na\"ive upsampling. This
has particular relevance for Earth observation missions like Sentinel-2, which
offer frequent, regular coverage at no cost; but at coarse resolution. Its
pixel footprint is too large to capture small features like houses, streets, or
hedge rows. To address this, we present SEN4X, a hybrid super-resolution
architecture that combines the advantages of single-image and multi-image
techniques. It combines temporal oversampling from repeated Sentinel-2
acquisitions with a learned prior from high-resolution Pl\'eiades Neo data. In
doing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.
We test the super-resolved images on urban land-cover classification in Hanoi,
Vietnam. We find that they lead to a significant performance improvement over
state-of-the-art super-resolution baselines.

</details>


### [816] [LesionDiffusion: Towards Text-controlled General Lesion Synthesis](https://arxiv.org/pdf/2503.00741)
*Henrui Tian, Wenhui Lei, Linrui Dai, Hanyu Chen, Xiaofan Zhang*

Main category: eess.IV

TL;DR: LesionDiffusion is a text-controllable framework for generating synthetic lesions and masks in 3D CT scans, improving segmentation performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of fully-supervised lesion recognition methods, which require expensive annotated datasets, by enabling scalable and fine-grained synthetic lesion generation.

Method: Proposes LesionDiffusion with two components: LMNet for lesion mask synthesis and LINet for lesion inpainting, guided by lesion attributes and image features. Uses a structured lesion report template for control.

Result: Outperforms state-of-the-art models, enhances segmentation, and generalizes well to unseen lesion types and organs.

Conclusion: LesionDiffusion offers a scalable, controllable solution for synthetic lesion generation, advancing medical imaging research.

Abstract: Fully-supervised lesion recognition methods in medical imaging face
challenges due to the reliance on large annotated datasets, which are expensive
and difficult to collect. To address this, synthetic lesion generation has
become a promising approach. However, existing models struggle with
scalability, fine-grained control over lesion attributes, and the generation of
complex structures. We propose LesionDiffusion, a text-controllable lesion
synthesis framework for 3D CT imaging that generates both lesions and
corresponding masks. By utilizing a structured lesion report template, our
model provides greater control over lesion attributes and supports a wider
variety of lesion types. We introduce a dataset of 1,505 annotated CT scans
with paired lesion masks and structured reports, covering 14 lesion types
across 8 organs. LesionDiffusion consists of two components: a lesion mask
synthesis network (LMNet) and a lesion inpainting network (LINet), both guided
by lesion attributes and image features. Extensive experiments demonstrate that
LesionDiffusion significantly improves segmentation performance, with strong
generalization to unseen lesion types and organs, outperforming current
state-of-the-art models. Code is available at
https://github.com/HengruiTianSJTU/LesionDiffusion.

</details>
