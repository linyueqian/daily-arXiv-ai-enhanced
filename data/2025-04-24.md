<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 101]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 84]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 7]
- [eess.AS](#eess.AS) [Total: 4]
- [eess.IV](#eess.IV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
*Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Charese H. Smiley*

Main category: cs.CL

TL;DR: FinNLI is a benchmark dataset for Financial NLI, featuring diverse financial texts and expert-annotated test sets. It highlights domain shift challenges and poor performance of financial LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a specialized dataset for financial natural language inference and evaluate model performance in this domain.

Method: Created FinNLI with 21,304 premise-hypothesis pairs, including expert-annotated test sets, and evaluated PLMs and LLMs.

Result: Domain shift degrades general NLI performance; best Macro F1 scores were 74.57% (PLMs) and 78.62% (LLMs). Financial LLMs performed poorly.

Conclusion: FinNLI reveals weaknesses in LLMs for financial reasoning, indicating need for improvement.

Abstract: We introduce FinNLI, a benchmark dataset for Financial Natural Language
Inference (FinNLI) across diverse financial texts like SEC Filings, Annual
Reports, and Earnings Call transcripts. Our dataset framework ensures diverse
premise-hypothesis pairs while minimizing spurious correlations. FinNLI
comprises 21,304 pairs, including a high-quality test set of 3,304 instances
annotated by finance experts. Evaluations show that domain shift significantly
degrades general-domain NLI performance. The highest Macro F1 scores for
pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and
78.62%, respectively, highlighting the dataset's difficulty. Surprisingly,
instruction-tuned financial LLMs perform poorly, suggesting limited
generalizability. FinNLI exposes weaknesses in current LLMs for financial
reasoning, indicating room for improvement.

</details>


### [2] [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
*Frederik Bredgaard, Martin Lund Trinhammer, Elisa Bassignana*

Main category: cs.CL

TL;DR: The paper explores using NLP to automatically assess patient attachment styles from psychotherapy transcripts, aiming to replace the manual PACS method for scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current manual assessment of attachment style (PACS) is complex and resource-heavy, limiting scalability. NLP can automate this, enabling wider adoption of attachment-informed treatment and research.

Method: The study uses NLP classification models to analyze psychotherapy transcripts and automatically identify patient attachment styles.

Result: The work identifies challenges, such as mislabeling (e.g., confusing 'preoccupied' with 'avoidant'), and discusses their impact on therapy outcomes.

Conclusion: This research paves the way for personalized psychotherapy and targeted studies on psychotherapy mechanisms through NLP advancements.

Abstract: The delivery of mental healthcare through psychotherapy stands to benefit
immensely from developments within Natural Language Processing (NLP), in
particular through the automatic identification of patient specific qualities,
such as attachment style. Currently, the assessment of attachment style is
performed manually using the Patient Attachment Coding System (PACS; Talia et
al., 2017), which is complex, resource-consuming and requires extensive
training. To enable wide and scalable adoption of attachment informed treatment
and research, we propose the first exploratory analysis into automatically
assessing patient attachment style from psychotherapy transcripts using NLP
classification models. We further analyze the results and discuss the
implications of using automated tools for this purpose -- e.g., confusing
`preoccupied' patients with `avoidant' likely has a more negative impact on
therapy outcomes with respect to other mislabeling. Our work opens an avenue of
research enabling more personalized psychotherapy and more targeted research
into the mechanisms of psychotherapy through advancements in NLP.

</details>


### [3] [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
*Li Weigang, Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: The study evaluates LLMs and traditional tools for Chinese-English translation, highlighting challenges in poetic intent, cultural retention, and specialized terminology. It proposes a new BLEU variant and identifies key performance gaps.


<details>
  <summary>Details</summary>
Motivation: To address challenges in preserving poetic intent, cultural heritage, and specialized terminology in Chinese-English translation using LLMs.

Method: Constructs a diverse corpus and uses a BT-Fried evaluation system (back-translation and Friedman test) to assess BLEU, CHRF, TER, and semantic similarity across six LLMs and three traditional tools.

Result: (1) Back-translation aids scientific abstracts; traditional tools excel in distinct texts. (2) LLMs struggle with cultural/literary retention. (3) Some models show verbatim back-translation. (4) A new BLEU variant is proposed.

Conclusion: The study advances empirical evaluation of Chinese NLP performance and enhances understanding of cultural fidelity in AI translation.

Abstract: The rapid advancement of large language models (LLMs) has reshaped the
landscape of machine translation, yet challenges persist in preserving poetic
intent, cultural heritage, and handling specialized terminology in
Chinese-English translation. This study constructs a diverse corpus
encompassing Chinese scientific terminology, historical translation paradoxes,
and literary metaphors. Utilizing a back-translation and Friedman test-based
evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic
similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three
traditional translation tools. Key findings include: (1) Scientific abstracts
often benefit from back-translation, while traditional tools outperform LLMs in
linguistically distinct texts; (2) LLMs struggle with cultural and literary
retention, exemplifying the "paradox of poetic intent"; (3) Some models exhibit
"verbatim back-translation", reflecting emergent memory behavior; (4) A novel
BLEU variant using Jieba segmentation and n-gram weighting is proposed. The
study contributes to the empirical evaluation of Chinese NLP performance and
advances understanding of cultural fidelity in AI-mediated translation.

</details>


### [4] [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
*Zhangdie Yuan, Andreas Vlachos*

Main category: cs.CL

TL;DR: A new Wikidata-derived dataset evaluates LLMs on symmetric/antisymmetric relations, showing poor performance. Retraining encoders with contrastive learning improves results.


<details>
  <summary>Details</summary>
Motivation: To address the gap in relational understanding (symmetric/antisymmetric) in LLMs.

Method: Introduce a Wikidata-derived dataset; retrain encoders using contrastive learning with k-nearest neighbors.

Result: LLMs perform near random chance initially; retrained encoders match fine-tuned heads with added efficiency and reduced forgetting.

Conclusion: Retraining encoders via contrastive learning effectively improves relational understanding in LLMs.

Abstract: Capturing symmetric (e.g., country borders another country) and antisymmetric
(e.g., parent_of) relations is crucial for a variety of applications. This
paper tackles this challenge by introducing a novel Wikidata-derived natural
language inference dataset designed to evaluate large language models (LLMs).
Our findings reveal that LLMs perform comparably to random chance on this
benchmark, highlighting a gap in relational understanding. To address this, we
explore encoder retraining via contrastive learning with k-nearest neighbors.
The retrained encoder matches the performance of fine-tuned classification
heads while offering additional benefits, including greater efficiency in
few-shot learning and improved mitigation of catastrophic forgetting.

</details>


### [5] [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
*Arpana Hosabettu, Harsh Shah*

Main category: cs.CL

TL;DR: An advanced NLP system using transformer-based architectures (Legal-BERT) to extract definitions, terms, and scope from the U.S. Code, achieving high accuracy (96.8% precision, 98.9% recall).


<details>
  <summary>Details</summary>
Motivation: Enhancing comprehension and clarity of complex legal corpora like the U.S. Code by automating definition extraction.

Method: Multi-stage pipeline combining document structure analysis, fine-tuned Legal-BERT for paragraph classification, and attention mechanisms with rule-based patterns for term and scope extraction.

Result: Significant improvement over previous methods, with 96.8% precision and 98.9% recall (98.2% F1-score).

Conclusion: The system improves legal information accessibility and supports downstream legal reasoning tasks.

Abstract: Automatic extraction of definitions from legal texts is critical for
enhancing the comprehension and clarity of complex legal corpora such as the
United States Code (U.S.C.). We present an advanced NLP system leveraging
transformer-based architectures to automatically extract defined terms, their
definitions, and their scope from the U.S.C. We address the challenges of
automatically identifying legal definitions, extracting defined terms, and
determining their scope within this complex corpus of over 200,000 pages of
federal statutory law. Building upon previous feature-based machine learning
methods, our updated model employs domain-specific transformers (Legal-BERT)
fine-tuned specifically for statutory texts, significantly improving extraction
accuracy. Our work implements a multi-stage pipeline that combines document
structure analysis with state-of-the-art language models to process legal text
from the XML version of the U.S. Code. Each paragraph is first classified using
a fine-tuned legal domain BERT model to determine if it contains a definition.
Our system then aggregates related paragraphs into coherent definitional units
and applies a combination of attention mechanisms and rule-based patterns to
extract defined terms and their jurisdictional scope. The definition extraction
system is evaluated on multiple titles of the U.S. Code containing thousands of
definitions, demonstrating significant improvements over previous approaches.
Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score),
substantially outperforming traditional machine learning classifiers. This work
contributes to improving accessibility and understanding of legal information
while establishing a foundation for downstream legal reasoning tasks.

</details>


### [6] [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
*Tian Bai, Huiyan Ying, Kailong Suo, Junqiu Wei, Tao Fan, Yuanfeng Song*

Main category: cs.CL

TL;DR: The paper introduces Text-to-TrajVis, a task converting natural language questions into trajectory visualizations, and presents TrajVL, a dataset of 18,140 (question, TVL) pairs, created using LLMs and human efforts.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in natural language interfaces for trajectory visualization systems by addressing the lack of relevant datasets.

Method: Proposed Trajectory Visualization Language (TVL) for querying trajectory data, and used LLMs combined with human efforts to construct the TrajVL dataset.

Result: Created the first large-scale Text-to-TrajVis dataset and evaluated LLMs, showing the task is feasible but challenging.

Conclusion: The Text-to-TrajVis task is promising yet difficult, warranting further research.

Abstract: This paper introduces the Text-to-TrajVis task, which aims to transform
natural language questions into trajectory data visualizations, facilitating
the development of natural language interfaces for trajectory visualization
systems. As this is a novel task, there is currently no relevant dataset
available in the community. To address this gap, we first devised a new
visualization language called Trajectory Visualization Language (TVL) to
facilitate querying trajectory data and generating visualizations. Building on
this foundation, we further proposed a dataset construction method that
integrates Large Language Models (LLMs) with human efforts to create
high-quality data. Specifically, we first generate TVLs using a comprehensive
and systematic process, and then label each TVL with corresponding natural
language questions using LLMs. This process results in the creation of the
first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140
(question, TVL) pairs. Based on this dataset, we systematically evaluated the
performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The
experimental results demonstrate that this task is both feasible and highly
challenging and merits further exploration within the research community.

</details>


### [7] [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
*Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed F. AbouElhamayed, Yueying Li, Mohamed S. Abdelfattah*

Main category: cs.CL

TL;DR: The paper proposes offloading challenging parts of reasoning in LLMs to a larger model while using a smaller model for most generation, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Longer token generation in reasoning tasks is inefficient, but not all parts are equally difficult. Leveraging this can optimize performance.

Method: Annotate difficult reasoning segments, then use SFT and RLFT to train a smaller model to offload challenging parts to a larger model.

Result: Improves reasoning accuracy by 24% and 28.3% while offloading only 1.35% and 5% of tokens.

Conclusion: The approach efficiently balances accuracy and computational cost, with open-sourced resources for further research.

Abstract: Reasoning in large language models (LLMs) tends to produce substantially
longer token generation sequences than simpler language modeling tasks. This
extended generation length reflects the multi-step, compositional nature of
reasoning and is often correlated with higher solution accuracy. From an
efficiency perspective, longer token generation exacerbates the inherently
sequential and memory-bound decoding phase of LLMs. However, not all parts of
this expensive reasoning process are equally difficult to generate. We leverage
this observation by offloading only the most challenging parts of the reasoning
process to a larger, more capable model, while performing most of the
generation with a smaller, more efficient model; furthermore, we teach the
smaller model to identify these difficult segments and independently trigger
offloading when needed. To enable this behavior, we annotate difficult segments
across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT)
dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning
fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to
offload the most challenging parts of its own reasoning process to a larger
model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while
offloading 1.35% and 5% of the generated tokens respectively. We open-source
our SplitReason model, data, code and logs.

</details>


### [8] [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
*Fahmida Liza Piya, Rahmatollah Beheshti*

Main category: cs.CL

TL;DR: ConTextual is a novel framework combining token filtering and knowledge graphs for clinical text summarization, outperforming baselines in linguistic and clinical fidelity.


<details>
  <summary>Details</summary>
Motivation: Unstructured clinical data holds rich information but lacks efficient methods to prioritize critical details for decision-making.

Method: Integrates Context-Preserving Token Filtering with a Domain-Specific Knowledge Graph for contextual augmentation.

Result: Outperforms baselines on benchmark datasets, improving linguistic coherence and clinical fidelity.

Conclusion: Token-level filtering and structured retrieval enhance clinical text summarization, offering scalable precision.

Abstract: Unstructured clinical data can serve as a unique and rich source of
information that can meaningfully inform clinical practice. Extracting the most
pertinent context from such data is critical for exploiting its true potential
toward optimal and timely decision-making in patient care. While prior research
has explored various methods for clinical text summarization, most prior
studies either process all input tokens uniformly or rely on heuristic-based
filters, which can overlook nuanced clinical cues and fail to prioritize
information critical for decision-making. In this study, we propose Contextual,
a novel framework that integrates a Context-Preserving Token Filtering method
with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By
preserving context-specific important tokens and enriching them with structured
knowledge, ConTextual improves both linguistic coherence and clinical fidelity.
Our extensive empirical evaluations on two public benchmark datasets
demonstrate that ConTextual consistently outperforms other baselines. Our
proposed approach highlights the complementary role of token-level filtering
and structured retrieval in enhancing both linguistic and clinical integrity,
as well as offering a scalable solution for improving precision in clinical
text generation.

</details>


### [9] [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
*Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di*

Main category: cs.CL

TL;DR: The paper presents 'Less is More,' a third-place winning approach for the XLLM@ACL2025 Shared Task-III, which improves structured reasoning in LLMs using minimal labeled data (24 examples).


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating interpretable, step-by-step rationales with limited labeled data in low-resource settings.

Method: Uses a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis (via GPT-4o), and dual-stage reward-guided filtering. Modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup.

Result: The pipeline consistently enhances structured reasoning quality by combining structure validation with reward filtering across few-shot and zero-shot prompts.

Conclusion: Controllable data distillation is valuable for improving structured inference under low-resource constraints.

Abstract: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural
reasoning task that challenges LLMs to generate interpretable, step-by-step
rationales with minimal labeled data. We present Less is More, the third-place
winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on
structured reasoning from only 24 labeled examples. Our approach leverages a
multi-agent framework with reverse-prompt induction, retrieval-augmented
reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to
distill high-quality supervision across three subtasks: question parsing, CoT
parsing, and step-level verification. All modules are fine-tuned from
Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure
validation with reward filtering across few-shot and zero-shot prompts, our
pipeline consistently improves structure reasoning quality. These results
underscore the value of controllable data distillation in enhancing structured
inference under low-resource constraints. Our code is available at
https://github.com/Jiahao-Yuan/Less-is-More.

</details>


### [10] [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
*Kosuke Yamada, Peinan Zhang*

Main category: cs.CL

TL;DR: PonTE is an unsupervised method for conditional text embeddings using a causal LLM and conditional prompts, matching supervised methods without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Addresses the high labor and resource costs of fine-tuning models for conditional text embeddings.

Method: Uses a causal large language model (LLM) and conditional prompts to generate embeddings without fine-tuning.

Result: Achieves performance comparable to supervised methods in conditional semantic text similarity and clustering.

Conclusion: PonTE offers a cost-effective, interpretable solution for conditional text embeddings.

Abstract: Conditional text embedding is a proposed representation that captures the
shift in perspective on texts when conditioned on a specific aspect. Previous
methods have relied on extensive training data for fine-tuning models, leading
to challenges in terms of labor and resource costs. We propose PonTE, a novel
unsupervised conditional text embedding method that leverages a causal large
language model and a conditional prompt. Through experiments on conditional
semantic text similarity and text clustering, we demonstrate that PonTE can
generate useful conditional text embeddings and achieve performance comparable
to supervised methods without fine-tuning. We also show the interpretability of
text embeddings with PonTE by analyzing word generation following prompts and
embedding visualization.

</details>


### [11] [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
*Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Jinchao Zhang, Jie Zhou, Haige Zhu*

Main category: cs.CL

TL;DR: The paper introduces MMLA, a benchmark for evaluating multimodal large language models (MLLMs) on cognitive-level semantics, revealing their limitations despite extensive testing.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on MLLMs' ability to understand cognitive-level semantics in multimodal language analysis.

Method: MMLA benchmark with 61K multimodal utterances across six dimensions, evaluated using zero-shot inference, supervised fine-tuning, and instruction tuning on eight LLM/MLLM branches.

Result: Fine-tuned models achieve only 60%~70% accuracy, highlighting current MLLMs' limitations in complex language understanding.

Conclusion: MMLA provides a foundation for advancing MLLMs in multimodal language analysis, with open-sourced datasets and code.

Abstract: Multimodal language analysis is a rapidly evolving field that leverages
multiple modalities to enhance the understanding of high-level semantics
underlying human conversational utterances. Despite its significance, little
research has investigated the capability of multimodal large language models
(MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce
MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA
comprises over 61K multimodal utterances drawn from both staged and real-world
scenarios, covering six core dimensions of multimodal semantics: intent,
emotion, dialogue act, sentiment, speaking style, and communication behavior.
We evaluate eight mainstream branches of LLMs and MLLMs using three methods:
zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive
experiments reveal that even fine-tuned models achieve only about 60%~70%
accuracy, underscoring the limitations of current MLLMs in understanding
complex human language. We believe that MMLA will serve as a solid foundation
for exploring the potential of large language models in multimodal language
analysis and provide valuable resources to advance this field. The datasets and
code are open-sourced at https://github.com/thuiar/MMLA.

</details>


### [12] [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
*Mohammad Khodadad, Ali Shiraee Kasmaee, Mahdi Astaraki, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee*

Main category: cs.CL

TL;DR: A new benchmark for evaluating compositional reasoning in LLMs for chemistry, using automated pipelines and knowledge graphs, reveals challenges even with context augmentation.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the compositional reasoning capabilities of large language models (LLMs) in the chemistry domain, highlighting their limitations.

Method: Developed an automated pipeline integrating OpenAI models and NER systems to create a knowledge graph, then tested LLMs with multi-hop questions in context-augmented and non-context settings.

Result: State-of-the-art LLMs struggle with multi-hop reasoning; document retrieval improves performance but doesn't eliminate errors.

Conclusion: The study benchmarks LLM limitations, introduces a novel data generation pipeline, and advances understanding of reasoning in computational linguistics.

Abstract: In this study, we introduced a new benchmark consisting of a curated dataset
and a defined evaluation process to assess the compositional reasoning
capabilities of large language models within the chemistry domain. We designed
and validated a fully automated pipeline, verified by subject matter experts,
to facilitate this task. Our approach integrates OpenAI reasoning models with
named entity recognition (NER) systems to extract chemical entities from recent
literature, which are then augmented with external knowledge bases to form a
comprehensive knowledge graph. By generating multi-hop questions across these
graphs, we assess LLM performance in both context-augmented and non-context
augmented settings. Our experiments reveal that even state-of-the-art models
face significant challenges in multi-hop compositional reasoning. The results
reflect the importance of augmenting LLMs with document retrieval, which can
have a substantial impact on improving their performance. However, even perfect
retrieval accuracy with full context does not eliminate reasoning errors,
underscoring the complexity of compositional reasoning. This work not only
benchmarks and highlights the limitations of current LLMs but also presents a
novel data generation pipeline capable of producing challenging reasoning
datasets across various domains. Overall, this research advances our
understanding of reasoning in computational linguistics.

</details>


### [13] [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
*Shuguang Zhao, Qiangzhong Feng, Zhiyang He, Peipei Sun, Yingying Wang, Xiaodong Tao, Xiaoliang Lu, Mei Cheng, Xinyue Wu, Yanyan Wang, Wei Liang*

Main category: cs.CL

TL;DR: EMRModel integrates LoRA-based fine-tuning with code-style prompts to convert medical dialogues into structured EMRs, achieving an 88.1% F1 score, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Unstructured medical dialogues hinder effective clinical use; existing methods fail to capture deep semantics.

Method: Combines LoRA fine-tuning with code-style prompt design and constructs a high-quality annotated dataset.

Result: Achieves 88.1% F1 score, a 49.5% improvement over standard models, and outperforms traditional LoRA methods.

Conclusion: EMRModel effectively structures medical records, advancing medical NLP with its novel approach and robust evaluation.

Abstract: Medical consultation dialogues contain critical clinical information, yet
their unstructured nature hinders effective utilization in diagnosis and
treatment. Traditional methods, relying on rule-based or shallow machine
learning techniques, struggle to capture deep and implicit semantics. Recently,
large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight
fine-tuning method, have shown promise for structured information extraction.
We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning
with code-style prompt design, aiming to efficiently convert medical
consultation dialogues into structured electronic medical records (EMRs).
Additionally, we construct a high-quality, realistically grounded dataset of
medical consultation dialogues with detailed annotations. Furthermore, we
introduce a fine-grained evaluation benchmark for medical consultation
information extraction and provide a systematic evaluation methodology,
advancing the optimization of medical natural language processing (NLP) models.
Experimental results show EMRModel achieves an F1 score of 88.1%, improving
by49.5% over standard pre-trained models. Compared to traditional LoRA
fine-tuning methods, our model shows superior performance, highlighting its
effectiveness in structured medical record extraction tasks.

</details>


### [14] [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
*Vignesh Ethiraj, Sidhanth Menon, Divya Vijay*

Main category: cs.CL

TL;DR: T-VEC is a telecom-specific embedding model fine-tuned from gte-Qwen2-1.5B-instruct, outperforming generic models with deep domain adaptation and a custom tokenizer.


<details>
  <summary>Details</summary>
Motivation: Generic NLP models struggle with telecom jargon and concepts, limiting performance in telecom tasks.

Method: Fine-tuning gte-Qwen2-1.5B-instruct using triplet loss on telecom data, modifying 338 layers, and creating a domain-specific tokenizer.

Result: T-VEC achieves top MTEB score (0.825) and excels in telecom-specific benchmarks (0.9380 vs. <0.07).

Conclusion: T-VEC advances telecom AI with deep adaptation and open-source tools, setting a new standard for domain-specific NLP.

Abstract: The specialized vocabulary and complex concepts of the telecommunications
industry present significant challenges for standard Natural Language
Processing models. Generic text embeddings often fail to capture
telecom-specific semantics, hindering downstream task performance. We introduce
T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the
telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created
by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet
loss objective on a meticulously curated, large-scale dataset of
telecom-specific data. Crucially, this process involved substantial
modification of weights across 338 layers of the base model, ensuring deep
integration of domain knowledge, far exceeding superficial adaptation
techniques. We quantify this deep change via weight difference analysis. A key
contribution is the development and open-sourcing (MIT License) of the first
dedicated telecom-specific tokenizer, enhancing the handling of industry
jargon. T-VEC achieves a leading average MTEB score (0.825) compared to
established models and demonstrates vastly superior performance (0.9380 vs.
less than 0.07) on our internal telecom-specific triplet evaluation benchmark,
indicating an exceptional grasp of domain-specific nuances, visually confirmed
by improved embedding separation. This work positions NetoAI at the forefront
of telecom AI innovation, providing the community with a powerful, deeply
adapted, open-source tool.

</details>


### [15] [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
*Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao*

Main category: cs.CL

TL;DR: QuaDMix is a unified framework for optimizing LLM training data by balancing quality and diversity, outperforming separate optimization methods with a 7.2% average performance gain.


<details>
  <summary>Details</summary>
Motivation: Existing methods optimize quality and diversity separately, ignoring their trade-off, which limits LLM performance.

Method: QuaDMix measures data quality and diversity, then uses a parameterized sampling function to balance them, optimized via simulated experiments and LightGBM.

Result: QuaDMix improves performance by 7.2% on average across benchmarks, surpassing independent quality or diversity strategies.

Conclusion: Jointly optimizing quality and diversity is essential for LLM training, and QuaDMix effectively achieves this balance.

Abstract: Quality and diversity are two critical metrics for the training data of large
language models (LLMs), positively impacting performance. Existing studies
often optimize these metrics separately, typically by first applying quality
filtering and then adjusting data proportions. However, these approaches
overlook the inherent trade-off between quality and diversity, necessitating
their joint consideration. Given a fixed training quota, it is essential to
evaluate both the quality of each data point and its complementary effect on
the overall dataset. In this paper, we introduce a unified data selection
framework called QuaDMix, which automatically optimizes the data distribution
for LLM pretraining while balancing both quality and diversity. Specifically,
we first propose multiple criteria to measure data quality and employ domain
classification to distinguish data points, thereby measuring overall diversity.
QuaDMix then employs a unified parameterized data sampling function that
determines the sampling probability of each data point based on these quality
and diversity related labels. To accelerate the search for the optimal
parameters involved in the QuaDMix framework, we conduct simulated experiments
on smaller models and use LightGBM for parameters searching, inspired by the
RegMix method. Our experiments across diverse models and datasets demonstrate
that QuaDMix achieves an average performance improvement of 7.2% across
multiple benchmarks. These results outperform the independent strategies for
quality and diversity, highlighting the necessity and ability to balance data
quality and diversity.

</details>


### [16] [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
*Hong Ting Tsang, Zihao Wang, Yangqiu Song*

Main category: cs.CL

TL;DR: The paper introduces LKHGT, a two-stage transformer model for answering complex queries on knowledge hypergraphs, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge graphs (KGs) like triple KGs lack the ability to represent real-world data's complexity, especially relationships of varying arity. Hyper-relational graphs also have limitations.

Method: Proposed LKHGT, a transformer model with Projection and Logical Encoders, using Type Aware Bias (TAB) for token interactions. Tested on new datasets JF17k-HCQA and M-FB15k-HCQA.

Result: LKHGT achieves state-of-the-art performance on CQA tasks and generalizes well to out-of-distribution query types.

Conclusion: LKHGT effectively addresses the limitations of existing KGs and hyper-relational graphs, providing a robust solution for complex query answering.

Abstract: Complex Query Answering (CQA) has been extensively studied in recent years.
In order to model data that is closer to real-world distribution, knowledge
graphs with different modalities have been introduced. Triple KGs, as the
classic KGs composed of entities and relations of arity 2, have limited
representation of real-world facts. Real-world data is more sophisticated.
While hyper-relational graphs have been introduced, there are limitations in
representing relationships of varying arity that contain entities with equal
contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and
M-FB15k-HCQA. Each dataset contains various query types that include logical
operations such as projection, negation, conjunction, and disjunction. In order
to answer knowledge hypergraph (KHG) existential first-order queries, we
propose a two-stage transformer model, the Logical Knowledge Hypergraph
Transformer (LKHGT), which consists of a Projection Encoder for atomic
projection and a Logical Encoder for complex logical operations. Both encoders
are equipped with Type Aware Bias (TAB) for capturing token interactions.
Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA
method over KHG and is able to generalize to out-of-distribution query types.

</details>


### [17] [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
*Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI*

Main category: cs.CL

TL;DR: The paper introduces Prompt Importance Sampling (PIS), a novel framework for compressing prompts in large language models (LLMs) by dynamically sampling important tokens using attention scores and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: High costs of LLMs limit their adoption, and existing prompt compression methods overlook intrinsic mechanisms and lack systematic token importance evaluation.

Method: PIS uses attention scores for token-level saliency and a lightweight RL network for adaptive compression, plus Russian roulette sampling for semantic-level importance.

Result: PIS achieves state-of-the-art compression performance and enhances reasoning efficiency.

Conclusion: The work advances prompt engineering with theoretical grounding and practical efficiency for LLMs.

Abstract: Large language models (LLMs) have achieved remarkable progress, demonstrating
unprecedented capabilities across various natural language processing tasks.
However, the high costs associated with such exceptional performance limit the
widespread adoption of LLMs, highlighting the need for prompt compression.
Existing prompt compression methods primarily rely on heuristic truncation or
abstractive summarization techniques, which fundamentally overlook the
intrinsic mechanisms of LLMs and lack a systematic evaluation of token
importance for generation. In this work, we introduce Prompt Importance
Sampling (PIS), a novel compression framework that dynamically compresses
prompts by sampling important tokens based on the analysis of attention scores
of hidden states. PIS employs a dual-level compression mechanism: 1) at the
token level, we quantify saliency using LLM-native attention scores and
implement adaptive compression through a lightweight 9-layer reinforcement
learning (RL) network; 2) at the semantic level, we propose a Russian roulette
sampling strategy for sentence-level importance sampling. Comprehensive
evaluations across multiple domain benchmarks demonstrate that our method
achieves state-of-the-art compression performance. Notably, our framework
serendipitously enhances reasoning efficiency through optimized context
structuring. This work advances prompt engineering by offering both theoretical
grounding and practical efficiency in context management for LLMs.

</details>


### [18] [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
*Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon*

Main category: cs.CL

TL;DR: The study compares LLMs and traditional MT tools for translating medical consultation summaries into Arabic, Chinese, and Vietnamese, finding traditional MT generally better, especially for complex texts, while LLMs show promise for simpler summaries.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs versus traditional MT tools in translating medical texts, addressing the need for accurate and clinically relevant translations in healthcare.

Method: Assessed translations of patient-friendly and clinician-focused texts using standard automated metrics.

Result: Traditional MT outperformed LLMs for complex texts, while LLMs showed potential for simpler summaries in Vietnamese and Chinese. Arabic translations improved with text complexity.

Conclusion: LLMs lack consistency, and current metrics miss clinical relevance. Domain-specific training, better evaluation methods, and human oversight are needed for medical translation.

Abstract: This study evaluates how well large language models (LLMs) and traditional
machine translation (MT) tools translate medical consultation summaries from
English into Arabic, Chinese, and Vietnamese. It assesses both patient,
friendly and clinician, focused texts using standard automated metrics. Results
showed that traditional MT tools generally performed better, especially for
complex texts, while LLMs showed promise, particularly in Vietnamese and
Chinese, when translating simpler summaries. Arabic translations improved with
complexity due to the language's morphology. Overall, while LLMs offer
contextual flexibility, they remain inconsistent, and current evaluation
metrics fail to capture clinical relevance. The study highlights the need for
domain-specific training, improved evaluation methods, and human oversight in
medical translation.

</details>


### [19] [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
*Mareike Lisker, Christina Gottschalk, Helena MihaljeviÄ‡*

Main category: cs.CL

TL;DR: LLMs like GPT-4, Llama 3, and Mistral struggle to generate effective counterspeech against conspiracy theories, often producing generic, repetitive, or factually incorrect responses.


<details>
  <summary>Details</summary>
Motivation: The lack of datasets pairing conspiracy theories with expert-crafted counterspeech and the under-researched use of LLMs in this context motivated the study.

Method: The study evaluated GPT-4, Llama 3, and Mistral using structured prompts derived from psychological research to generate counterspeech.

Result: The models frequently produced generic, repetitive, or superficial counterspeech, over-acknowledged fear, and hallucinated facts or sources.

Conclusion: Prompt-based use of LLMs for counterspeech against conspiracy theories is currently problematic due to their limitations in generating accurate and nuanced responses.

Abstract: Counterspeech is a key strategy against harmful online content, but scaling
expert-driven efforts is challenging. Large Language Models (LLMs) present a
potential solution, though their use in countering conspiracy theories is
under-researched. Unlike for hate speech, no datasets exist that pair
conspiracy theory comments with expert-crafted counterspeech. We address this
gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively
apply counterspeech strategies derived from psychological research provided
through structured prompts. Our results show that the models often generate
generic, repetitive, or superficial results. Additionally, they
over-acknowledge fear and frequently hallucinate facts, sources, or figures,
making their prompt-based use in practical applications problematic.

</details>


### [20] [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
*Prasanna Devadiga, Arya Suneesh, Pawan Kumar Rajpoot, Bharatdeep Hazarika, Aditya U Baliga*

Main category: cs.CL

TL;DR: A two-stage system for retrieving fact-checked claims in monolingual and crosslingual settings, combining a fine-tuned embedding model with an LLM-based reranker, achieving high success rates.


<details>
  <summary>Details</summary>
Motivation: Addressing the global challenge of disinformation by improving retrieval of fact-checked claims across languages.

Method: Two-stage strategy: baseline retrieval using a fine-tuned embedding model, followed by LLM-based reranking and translation for multilingual retrieval.

Result: Achieved success@10 scores of 0.938 (monolingual) and 0.81025 (crosslingual).

Conclusion: LLM-based translation enhances multilingual retrieval, and the pipeline is feasible on consumer GPUs.

Abstract: We address the challenge of retrieving previously fact-checked claims in
monolingual and crosslingual settings - a critical task given the global
prevalence of disinformation. Our approach follows a two-stage strategy: a
reliable baseline retrieval system using a fine-tuned embedding model and an
LLM-based reranker. Our key contribution is demonstrating how LLM-based
translation can overcome the hurdles of multilingual information retrieval.
Additionally, we focus on ensuring that the bulk of the pipeline can be
replicated on a consumer GPU. Our final integrated system achieved a success@10
score of 0.938 and 0.81025 on the monolingual and crosslingual test sets,
respectively.

</details>


### [21] [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
*Luisa Shimabucoro, Ahmet Ustun, Marzieh Fadaee, Sebastian Ruder*

Main category: cs.CL

TL;DR: The study explores cross-lingual transfer (CLT) dynamics in multilingual large language models, analyzing how post-training settings affect performance across tasks like summarization, instruction following, and mathematical reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics enabling cross-lingual transfer in multilingual models, as current understanding is limited despite widespread use.

Method: Analyzed two model families (up to 35B parameters) trained on controlled multilingual data mixtures, tested on three generative tasks in single-task and multi-task settings.

Result: CLT dynamics and multilingual performance depend on post-training settings, with no single variable explaining outcomes. Effective CLT conditions were identified.

Conclusion: Effective cross-lingual transfer requires specific post-training conditions, varying by task and setting, highlighting the complexity of multilingual model tuning.

Abstract: In order for large language models to be useful across the globe, they are
fine-tuned to follow instructions on multilingual data. Despite the ubiquity of
such post-training, a clear understanding of the dynamics that enable
cross-lingual transfer remains elusive. This study examines cross-lingual
transfer (CLT) dynamics in realistic post-training settings. We study two model
families of up to 35B parameters in size trained on carefully controlled
mixtures of multilingual data on three generative tasks with varying levels of
complexity (summarization, instruction following, and mathematical reasoning)
in both single-task and multi-task instruction tuning settings. Overall, we
find that the dynamics of cross-lingual transfer and multilingual performance
cannot be explained by isolated variables, varying depending on the combination
of post-training settings. Finally, we identify the conditions that lead to
effective cross-lingual transfer in practice.

</details>


### [22] [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
*Kwangseob Ahn*

Main category: cs.CL

TL;DR: HEMA, a dual-memory system inspired by human cognition, enhances LLM coherence in long conversations by combining Compact Memory (global summaries) and Vector Memory (episodic embeddings), improving factual recall and human-rated coherence.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with coherence in extended conversations beyond their context windows, necessitating a solution for maintaining long-term dialogue coherence.

Method: HEMA integrates Compact Memory (one-sentence summaries) and Vector Memory (chunk embeddings queried via cosine similarity) with a 6B-parameter transformer.

Result: HEMA improves factual recall from 41% to 87%, coherence ratings from 2.7 to 4.3, and retrieval precision/recall, while reducing latency by 34%.

Conclusion: HEMA offers a practical, privacy-aware solution for long-term conversational AI without retraining, balancing verbatim recall and semantic continuity.

Abstract: Large language models (LLMs) struggle with maintaining coherence in extended
conversations spanning hundreds of turns, despite performing well within their
context windows. This paper introduces HEMA (Hippocampus-Inspired Extended
Memory Architecture), a dual-memory system inspired by human cognitive
processes. HEMA combines Compact Memory - a continuously updated one-sentence
summary preserving global narrative coherence, and Vector Memory - an episodic
store of chunk embeddings queried via cosine similarity. When integrated with a
6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns
while keeping prompt length under 3,500 tokens. Experimental results show
substantial improvements: factual recall accuracy increases from 41% to 87%,
and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K
indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling
the area under the precision-recall curve compared to summarization-only
approaches. Ablation studies reveal two key insights: semantic forgetting
through age-weighted pruning reduces retrieval latency by 34% with minimal
recall loss, and a two-level summary hierarchy prevents cascade errors in
ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that
combining verbatim recall with semantic continuity provides a practical
solution for privacy-aware conversational AI capable of month-long dialogues
without model retraining.

</details>


### [23] [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
*Waad Alhoshan, Alessio Ferrari, Liping Zhao*

Main category: cs.CL

TL;DR: The paper explores the effectiveness of generative LLMs (Bloom, Gemma, Llama) in requirements classification, finding that prompt design and model architecture are universally important, while dataset impact varies with task complexity.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of generative LLMs in requirements classification and evaluate their performance compared to non-generative models like BERT.

Method: Conducted over 400 experiments using three generative LLMs on binary and multi-class classification tasks across three datasets (PROMISE NFR, Functional-Quality, SecReq).

Result: Prompt design and LLM architecture are universally critical, while dataset variations impact performance situationally based on task complexity.

Conclusion: Future model development should focus on optimizing prompts and aligning architectures with task-specific needs for better performance.

Abstract: In recent years, transformer-based large language models (LLMs) have
revolutionised natural language processing (NLP), with generative models
opening new possibilities for tasks that require context-aware text generation.
Requirements engineering (RE) has also seen a surge in the experimentation of
LLMs for different tasks, including trace-link detection, regulatory
compliance, and others. Requirements classification is a common task in RE.
While non-generative LLMs like BERT have been successfully applied to this
task, there has been limited exploration of generative LLMs. This gap raises an
important question: how well can generative LLMs, which produce context-aware
outputs, perform in requirements classification? In this study, we explore the
effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing
both binary and multi-class requirements classification. We design an extensive
experimental study involving over 400 experiments across three widely used
datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes
that while factors like prompt design and LLM architecture are universally
important, others-such as dataset variations-have a more situational impact,
depending on the complexity of the classification task. This insight can guide
future model development and deployment strategies, focusing on optimising
prompt structures and aligning model architectures with task-specific needs for
improved performance.

</details>


### [24] [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
*Sarah Jabbour, Trenton Chang, Anindya Das Antar, Joseph Peper, Insu Jang, Jiachen Liu, Jae-Won Chung, Shiqi He, Michael Wellman, Bryan Goodman, Elizabeth Bondi-Kelly, Kevin Samy, Rada Mihalcea, Mosharaf Chowhury, David Jurgens, Lu Wang*

Main category: cs.CL

TL;DR: The paper proposes a dynamic, holistic framework for evaluating GenAI systems in real-world contexts, emphasizing ethics, fairness, and continuous assessment.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for GenAI models are outdated, relying on fixed benchmarks that don't reflect real-world performance, creating a gap between lab results and practical applications.

Method: The paper suggests a comprehensive framework integrating diverse inputs, dynamic assessments, and human-automated collaboration for ongoing evaluation.

Result: The proposed framework ensures GenAI models are technically proficient, ethically responsible, and impactful, bridging the gap between lab and real-world performance.

Conclusion: The paper advocates for continuous, outcome-oriented evaluations and transparent policies to foster trust and societal impact in GenAI applications.

Abstract: Generative AI (GenAI) models have become vital across industries, yet current
evaluation methods have not adapted to their widespread use. Traditional
evaluations often rely on benchmarks and fixed datasets, frequently failing to
reflect real-world performance, which creates a gap between lab-tested outcomes
and practical applications. This white paper proposes a comprehensive framework
for how we should evaluate real-world GenAI systems, emphasizing diverse,
evolving inputs and holistic, dynamic, and ongoing assessment approaches. The
paper offers guidance for practitioners on how to design evaluation methods
that accurately reflect real-time capabilities, and provides policymakers with
recommendations for crafting GenAI policies focused on societal impacts, rather
than fixed performance numbers or parameter sizes. We advocate for holistic
frameworks that integrate performance, fairness, and ethics and the use of
continuous, outcome-oriented methods that combine human and automated
assessments while also being transparent to foster trust among stakeholders.
Implementing these strategies ensures GenAI models are not only technically
proficient but also ethically responsible and impactful.

</details>


### [25] [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
*Fengwei Zhou, Jiafei Song, Wenjin Jason Li, Gengjian Xue, Zhikang Zhao, Yichao Lu, Bailin Na*

Main category: cs.CL

TL;DR: MOOSComp is a token-classification-based method for compressing long-context input, improving efficiency and performance in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of increased inference time and resource consumption in large language models when processing long-context input.

Method: Proposes MOOSComp, which mitigates over-smoothing and incorporates outlier scores during training and compression phases.

Result: Achieves superior performance at various compression ratios and a 3.3x speedup at 4x compression on mobile devices.

Conclusion: MOOSComp effectively enhances long-context compression, balancing performance and resource efficiency.

Abstract: Recent advances in large language models have significantly improved their
ability to process long-context input, but practical applications are
challenged by increased inference time and resource consumption, particularly
in resource-constrained environments. To address these challenges, we propose
MOOSComp, a token-classification-based long-context compression method that
enhances the performance of a BERT-based compressor by mitigating the
over-smoothing problem and incorporating outlier scores. In the training phase,
we add an inter-class cosine similarity loss term to penalize excessively
similar token representations, thereby improving the token classification
accuracy. During the compression phase, we introduce outlier scores to preserve
rare but critical tokens that are prone to be discarded in task-agnostic
compression. These scores are integrated with the classifier's output, making
the compressor more generalizable to various tasks. Superior performance is
achieved at various compression ratios on long-context understanding and
reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x
compression ratio on a resource-constrained mobile device.

</details>


### [26] [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
*Ningning Zhang, Chi Zhang, Zhizhong Tan, Xingxing Yang, Weiping Deng, Wenyong Wang*

Main category: cs.CL

TL;DR: PAR RAG framework improves multi-hop QA by decomposing queries, verifying intermediate results, and mitigating error propagation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle with error propagation in multi-hop QA, leading to inaccurate answers.

Method: PAR RAG uses a three-stage approach: planning, acting, and reviewing, with multi-granularity verification for accuracy.

Result: PAR RAG achieves higher EM and F1 scores than state-of-the-art methods on multi-hop QA datasets.

Conclusion: The PAR RAG framework provides a reliable and interpretable solution for multi-hop QA by addressing error propagation.

Abstract: Multi-hop question answering (QA) presents a considerable challenge for
Retrieval-Augmented Generation (RAG), requiring the structured decomposition of
complex queries into logical reasoning paths and the generation of dependable
intermediate results. However, deviations in reasoning paths or errors in
intermediate results, which are common in current RAG methods, may propagate
and accumulate throughout the reasoning process, diminishing the accuracy of
the answer to complex queries. To address this challenge, we propose the
Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key
stages: planning, act, and review, and aims to offer an interpretable and
incremental reasoning paradigm for accurate and reliable multi-hop question
answering by mitigating error propagation.PAR RAG initially applies a top-down
problem decomposition strategy, formulating a comprehensive plan that
integrates multiple executable steps from a holistic viewpoint. This approach
avoids the pitfalls of local optima common in traditional RAG methods, ensuring
the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a
plan execution mechanism based on multi-granularity verification. By utilizing
both coarse-grained similarity information and fine-grained relevant data, the
framework thoroughly checks and adjusts intermediate results, ensuring process
accuracy while effectively managing error propagation and amplification.
Experimental results on multi-hop QA datasets demonstrate that the PAR RAG
framework substantially outperforms existing state-of-the-art methods in key
metrics, including EM and F1 scores.

</details>


### [27] [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
*Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu*

Main category: cs.CL

TL;DR: HSA enhances RNNs with hierarchical sparse attention, enabling long-range random access while maintaining efficiency. RAMba, combining HSA with Mamba, excels in long-context tasks with minimal memory overhead.


<details>
  <summary>Details</summary>
Motivation: RNNs lack random access to historical context, and adding attention may reduce efficiency. HSA aims to preserve RNNs' efficiency while enabling flexible long-range access.

Method: HSA divides inputs into chunks, selects top-k chunks, and hierarchically aggregates information using token-to-chunk relevance. A hardware-aligned kernel ensures efficiency.

Result: RAMba achieves perfect accuracy in 64M-length passkey retrieval (trained on 4K) and improves downstream tasks with near-constant memory.

Conclusion: HSA and RAMba demonstrate strong potential for efficient long-context modeling, balancing performance and resource use.

Abstract: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is
their linear computational and space complexity enables faster training and
inference for long sequences. However, RNNs are fundamentally unable to
randomly access historical context, and simply integrating attention mechanisms
may undermine their efficiency advantages. To overcome this limitation, we
propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel
attention mechanism that enhances RNNs with long-range random access
flexibility while preserving their merits in efficiency and length
generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks
and hierarchically aggregates information. The core innovation lies in learning
token-to-chunk relevance based on fine-grained token-level information inside
each chunk. This approach enhances the precision of chunk selection across both
in-domain and out-of-domain context lengths. To make HSA efficient, we further
introduce a hardware-aligned kernel design. By combining HSA with Mamba, we
introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64
million contexts despite pre-training on only 4K-length contexts, and
significant improvements on various downstream tasks, with nearly constant
memory footprint. These results show RAMba's huge potential in long-context
modeling.

</details>


### [28] [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
*Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj*

Main category: cs.CL

TL;DR: The paper explores using LLMs with Graph-RAG to parse complex IFC data, improving natural language query responses without complex pipelines.


<details>
  <summary>Details</summary>
Motivation: IFC data is complex and allows multiple representations of the same product information, posing challenges for collaborative work in construction.

Method: Utilizes Graph Retrieval-Augmented Generation (Graph-RAG) to parse IFC data, retrieving building object properties and relations.

Result: Graph-RAG enhances generative LLMs like GPT-4o with graph-based knowledge, enabling efficient natural language query-response retrieval.

Conclusion: Despite IFC data's complexity, Graph-RAG improves LLM capabilities for construction industry applications.

Abstract: IFC data has become the general building information standard for
collaborative work in the construction industry. However, IFC data can be very
complicated because it allows for multiple ways to represent the same product
information. In this research, we utilise the capabilities of LLMs to parse the
IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to
retrieve building object properties and their relations. We will show that,
despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG
parsing enhances generative LLMs like GPT-4o with graph-based knowledge,
enabling natural language query-response retrieval without the need for a
complex pipeline.

</details>


### [29] [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
*Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu*

Main category: cs.CL

TL;DR: GreenMind-Medium-14B-R1 is a Vietnamese reasoning model using Group Relative Policy Optimization, addressing language mixing and factual correctness, outperforming prior works on VLSP 2023 and SeaExam datasets.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning in Vietnamese LLM tasks by addressing language mixing and ensuring factual correctness in generated content.

Method: Finetuning with Group Relative Policy Optimization, using a Vietnamese synthesized reasoning dataset, and designing reward functions for language mixing and factual correctness.

Result: Outperforms prior works on VLSP 2023 and SeaExam datasets, enhancing linguistic consistency and reasoning effectiveness.

Conclusion: The model successfully addresses key limitations and demonstrates superior performance in Vietnamese reasoning tasks.

Abstract: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that
require intermediate reasoning steps prior to generating a final answer. In
this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model
inspired by the finetuning strategy based on Group Relative Policy
Optimization. We also leverage a high-quality Vietnamese synthesized reasoning
dataset and design two reward functions to tackle the main limitations of this
technique: (i) language mixing, where we explicitly detect the presence of
biased language characters during the process of sampling tokens, and (ii) we
leverage Sentence Transformer-based models to ensure that the generated
reasoning content maintains factual correctness and does not distort the final
output. Experimental results on the Vietnamese dataset from the VLSP 2023
Challenge demonstrate that our model outperforms prior works and enhances
linguistic consistency in its responses. Furthermore, we extend our evaluation
to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of
our reasoning method compared to few-shot prompting techniques.

</details>


### [30] [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
*Zijing Shi, Meng Fang, Ling Chen*

Main category: cs.CL

TL;DR: MC-DML combines LLMs with tree search for efficient planning in text-based games, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional planning-then-learning methods are slow and lack language understanding; MC-DML addresses this by integrating LLMs with tree search.

Method: MC-DML uses LLMs with dynamic memory mechanisms (in-trial and cross-trial) to enhance planning and action evaluation.

Result: MC-DML outperforms contemporary methods in initial planning phases on Jericho benchmark games.

Conclusion: MC-DML offers efficient language-grounded planning, improving performance in complex text-based environments.

Abstract: Text-based games provide valuable environments for language-based autonomous
agents. However, planning-then-learning paradigms, such as those combining
Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably
time-consuming due to extensive iterations. Additionally, these algorithms
perform uncertainty-driven exploration but lack language understanding and
reasoning abilities. In this paper, we introduce the Monte Carlo planning with
Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages
the language understanding and reasoning capabilities of Large Language Models
(LLMs) alongside the exploratory advantages of tree search algorithms.
Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms,
enabling them to learn from past experiences and dynamically adjust action
evaluations during planning. We conduct experiments on a series of text-based
games from the Jericho benchmark. Our results demonstrate that the MC-DML
algorithm significantly enhances performance across various games at the
initial planning phase, outperforming strong contemporary methods that require
multiple iterations. This demonstrates the effectiveness of our algorithm,
paving the way for more efficient language-grounded planning in complex
environments.

</details>


### [31] [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
*Alexander Shvets*

Main category: cs.CL

TL;DR: The paper introduces an LLM-based data synthesis pipeline using Mistral-7b to generate diverse, context-rich training examples for lightweight BERT-type models, achieving SOTA performance on multiple emotion datasets.


<details>
  <summary>Details</summary>
Motivation: Existing sentiment analysis datasets lack contextual information and diversity in emotion categories, while large models like GPT-4 are resource-heavy and prone to over-prediction.

Method: A pipeline leveraging Mistral-7b generates 100K contextual and 300K context-less examples over 28 emotion classes, fine-tuning pre-trained encoders (Emo Pillars models).

Result: Emo Pillars models achieve SOTA performance on GoEmotions, ISEAR, and IEMOCAP, with validated dataset diversity and context personalization.

Conclusion: The approach successfully diversifies training data and improves model adaptability, though further improvements are needed for neutral class and out-of-taxonomy labels.

Abstract: Most datasets for sentiment analysis lack context in which an opinion was
expressed, often crucial for emotion understanding, and are mainly limited by a
few emotion categories. Foundation large language models (LLMs) like GPT-4
suffer from over-predicting emotions and are too resource-intensive. We design
an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b,
for the generation of training examples for more accessible, lightweight
BERT-type encoder models. We focus on enlarging the semantic diversity of
examples and propose grounding the generation into a corpus of narratives to
produce non-repetitive story-character-centered utterances with unique contexts
over 28 emotion classes. By running 700K inferences in 450 GPU hours, we
contribute with the dataset of 100K contextual and also 300K context-less
examples to cover both scenarios. We use it for fine-tuning pre-trained
encoders, which results in several Emo Pillars models. We show that Emo Pillars
models are highly adaptive to new domains when tuned to specific tasks such as
GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on
the first three. We also validate our dataset, conducting statistical analysis
and human evaluation, and confirm the success of our measures in utterance
diversification (although less for the neutral class) and context
personalization, while pointing out the need for improved handling of
out-of-taxonomy labels within the pipeline.

</details>


### [32] [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
*Hanwen Du, Bo Peng, Xia Ning*

Main category: cs.CL

TL;DR: DiffTOD introduces a diffusion model-based framework for non-sequential dialogue planning in Target-Oriented Dialogue (TOD), addressing compounding errors and myopic actions in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue planning methods suffer from sequential generation, leading to compounding errors and short-sighted actions, limiting their effectiveness in TOD.

Method: DiffTOD uses diffusion models to formulate dialogue planning as trajectory generation with conditional guidance, employing tailored mechanisms for diverse targets.

Result: Experiments show DiffTOD effectively performs non-myopic exploration and optimizes long-horizon action strategies, demonstrating flexibility across diverse TOD scenarios.

Conclusion: DiffTOD offers a robust and flexible solution for TOD by leveraging non-sequential planning and diffusion models, outperforming traditional sequential methods.

Abstract: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM
era, where strategic dialogue planning is crucial for directing conversations
toward specific targets. However, existing dialogue planning methods generate
dialogue plans in a step-by-step sequential manner, and may suffer from
compounding errors and myopic actions. To address these limitations, we
introduce a novel dialogue planning framework, DiffTOD, which leverages
diffusion models to enable non-sequential dialogue planning. DiffTOD formulates
dialogue planning as a trajectory generation problem with conditional guidance,
and leverages a diffusion language model to estimate the likelihood of the
dialogue trajectory. To optimize the dialogue action strategies, DiffTOD
introduces three tailored guidance mechanisms for different target types,
offering flexible guidance towards diverse TOD targets at test time. Extensive
experiments across three diverse TOD settings show that DiffTOD can effectively
perform non-myopic lookahead exploration and optimize action strategies over a
long horizon through non-sequential dialogue planning, and demonstrates strong
flexibility across complex and diverse dialogue scenarios. Our code and data
are accessible through https://anonymous.4open.science/r/DiffTOD.

</details>


### [33] [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
*Joseph M. Denning, Xiaohan, Guo, Bryor Snefjella, Idan A. Blank*

Main category: cs.CL

TL;DR: The paper investigates whether LLMs capture thematic roles (who did what to whom) in sentences, finding that their representations prioritize syntax over thematic roles, though some attention heads do encode this information.


<details>
  <summary>Details</summary>
Motivation: To address critiques about LLMs' lack of language understanding by focusing on thematic roles, a core aspect of language processing, rather than broader cognitive abilities.

Method: Two experiments analyzed sentence representations in four LLMs, comparing them to human judgments and examining hidden units and attention heads.

Result: LLMs' representations emphasized syntactic similarity over thematic roles, with thematic role information weakly encoded in some attention heads but not in hidden units.

Conclusion: LLMs can extract thematic roles, but this information has a weaker influence on their representations compared to humans.

Abstract: Large Language Models (LLMs) are commonly criticized for not understanding
language. However, many critiques focus on cognitive abilities that, in humans,
are distinct from language processing. Here, we instead study a kind of
understanding tightly linked to language: inferring who did what to whom
(thematic roles) in a sentence. Does the central training objective of
LLMs-word prediction-result in sentence representations that capture thematic
roles? In two experiments, we characterized sentence representations in four
LLMs. In contrast to human similarity judgments, in LLMs the overall
representational similarity of sentence pairs reflected syntactic similarity
but not whether their agent and patient assignments were identical vs.
reversed. Furthermore, we found little evidence that thematic role information
was available in any subset of hidden units. However, some attention heads
robustly captured thematic roles, independently of syntax. Therefore, LLMs can
extract thematic roles but, relative to humans, this information influences
their representations more weakly.

</details>


### [34] [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
*Shifali Agrahari, Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: The paper introduces COT Fine-tuned, a framework for detecting AI-generated text and identifying the source language model, using Chain-of-Thought reasoning for transparency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about academic integrity, misinformation, and ethical AI deployment by improving detection of AI-generated text.

Method: A dual-task approach: Task A classifies text as AI-generated or human-written, while Task B identifies the specific LLM. Uses Chain-of-Thought reasoning for explainable predictions.

Result: COT Fine-tuned achieves high accuracy in both tasks, excelling in LLM identification and human-AI classification, with CoT reasoning enhancing effectiveness.

Conclusion: The framework is effective and interpretable, demonstrating the value of Chain-of-Thought reasoning in AI text detection.

Abstract: In recent years, the detection of AI-generated text has become a critical
area of research due to concerns about academic integrity, misinformation, and
ethical AI deployment. This paper presents COT Fine-tuned, a novel framework
for detecting AI-generated text and identifying the specific language model.
responsible for generating the text. We propose a dual-task approach, where
Task A involves classifying text as AI-generated or human-written, and Task B
identifies the specific LLM behind the text. The key innovation of our method
lies in the use of Chain-of-Thought reasoning, which enables the model to
generate explanations for its predictions, enhancing transparency and
interpretability. Our experiments demonstrate that COT Fine-tuned achieves high
accuracy in both tasks, with strong performance in LLM identification and
human-AI classification. We also show that the CoT reasoning process
contributes significantly to the models effectiveness and interpretability.

</details>


### [35] [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
*Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang*

Main category: cs.CL

TL;DR: OptimAI is a framework using LLM-powered AI agents to translate natural language optimization problems into mathematical formulations, outperforming state-of-the-art methods with 88.1% accuracy on NLP4LP and 71.2% on Optibench.


<details>
  <summary>Details</summary>
Motivation: Formulating optimization problems from natural language requires domain expertise. OptimAI aims to automate this process using AI agents.

Method: The framework includes a formulator, planner, coder, and code critic, with UCB-based debug scheduling for dynamic plan switching.

Result: Ablation studies show productivity drops without key roles. OptimAI achieves 88.1% accuracy on NLP4LP and 71.2% on Optibench, reducing errors by 58% and 50%.

Conclusion: OptimAI demonstrates the effectiveness of multi-agent collaboration in solving optimization problems described in natural language.

Abstract: Optimization plays a vital role in scientific research and practical
applications, but formulating a concrete optimization problem described in
natural language into a mathematical form and selecting a suitable solver to
solve the problem requires substantial domain expertise. We introduce
\textbf{OptimAI}, a framework for solving \underline{Optim}ization problems
described in natural language by leveraging LLM-powered \underline{AI} agents,
achieving superior performance over current state-of-the-art methods. Our
framework is built upon four key roles: (1) a \emph{formulator} that translates
natural language problem descriptions into precise mathematical formulations;
(2) a \emph{planner} that constructs a high-level solution strategy prior to
execution; and (3) a \emph{coder} and a \emph{code critic} capable of
interacting with the environment and reflecting on outcomes to refine future
actions. Ablation studies confirm that all roles are essential; removing the
planner or code critic results in $5.8\times$ and $3.1\times$ drops in
productivity, respectively. Furthermore, we introduce UCB-based debug
scheduling to dynamically switch between alternative plans, yielding an
additional $3.3\times$ productivity gain. Our design emphasizes multi-agent
collaboration, allowing us to conveniently explore the synergistic effect of
combining diverse models within a unified system. Our approach attains 88.1\%
accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o
table) subset, reducing error rates by 58\% and 50\% respectively over prior
best results.

</details>


### [36] [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
*JosÃ© Ãngel GonzÃ¡lez, Ian Borrego Obrador, Ãlvaro Romo Herrero, Areg Mikael Sarvazyan, Mara Chinea-RÃ­os, Angelo Basile, Marc Franco-Salvador*

Main category: cs.CL

TL;DR: IberBench is a comprehensive, extensible benchmark for evaluating LLMs in Iberian and Ibero-American languages, addressing linguistic diversity and industry-relevant tasks. It evaluates 23 LLMs, revealing performance gaps and offering open-source tools.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are English-centric and lack diversity, failing to address industry-relevant tasks and linguistic variety in non-English languages.

Method: IberBench integrates 101 datasets across 22 task categories, enabling continual updates and community contributions. It evaluates 23 LLMs of varying sizes.

Result: LLMs perform worse on industry-relevant tasks, struggle with Galician and Basque, and show inconsistent results across tasks.

Conclusion: IberBench fills gaps in LLM evaluation by focusing on linguistic diversity and industry relevance, providing open-source tools for ongoing assessment.

Abstract: Large Language Models (LLMs) remain difficult to evaluate comprehensively,
particularly for languages other than English, where high-quality data is often
limited. Existing benchmarks and leaderboards are predominantly
English-centric, with only a few addressing other languages. These benchmarks
fall short in several key areas: they overlook the diversity of language
varieties, prioritize fundamental Natural Language Processing (NLP)
capabilities over tasks of industrial relevance, and are static. With these
aspects in mind, we present IberBench, a comprehensive and extensible benchmark
designed to assess LLM performance on both fundamental and industry-relevant
NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America.
IberBench integrates 101 datasets from evaluation campaigns and recent
benchmarks, covering 22 task categories such as sentiment and emotion analysis,
toxicity detection, and summarization. The benchmark addresses key limitations
in current evaluation practices, such as the lack of linguistic diversity and
static evaluation setups by enabling continual updates and community-driven
model and dataset submissions moderated by a committee of experts. We evaluate
23 LLMs ranging from 100 million to 14 billion parameters and provide empirical
insights into their strengths and limitations. Our findings indicate that (i)
LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii)
performance is on average lower for Galician and Basque, (iii) some tasks show
results close to random, and (iv) in other tasks LLMs perform above random but
below shared task systems. IberBench offers open-source implementations for the
entire evaluation pipeline, including dataset normalization and hosting,
incremental evaluation of LLMs, and a publicly accessible leaderboard.

</details>


### [37] [Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion](https://arxiv.org/abs/2111.15473)
*Andrew Kiruluta, Andreas Lemos, Eric Lundy*

Main category: cs.CL

TL;DR: The paper proposes replacing attention in Transformers with Fourier Transform-based token mixing, introducing MDFWA for efficient global and local dependency capture, validated on summarization tasks.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and expressive power in Transformer models by replacing attention mechanisms with spectral techniques.

Method: Introduces Multi-Domain Fourier Wavelet Attention (MDFWA), combining frequency and time-localized transforms, with complexity bounds and gradient formulas.

Result: MDFWA achieves sub-quadratic time/memory costs while enhancing expressive power, validated on PubMed summarization.

Conclusion: Spectral techniques like MDFWA offer efficient alternatives to attention, improving Transformer performance.

Abstract: We revisit the use of spectral techniques to replaces the attention mechanism
in Transformers through Fourier Transform based token mixing, and present a
comprehensive and novel reformulation of this technique in next generation
transformer models. We provide expanded literature context, detailed
mathematical formulations of Fourier mixing and causal masking, and introduce a
novel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency
and time localized transforms to capture both global and local dependencies
efficiently. We derive the complexity bounds, gradient formulas, and show that
MDFWA achieves sub quadratic time and memory cost while improving expressive
power. We validate our design on an abstractive summarization task using PubMed
dataset, by enhancing the proposed approach with learned frequency bases,
adaptive scale selection, and multi-modal extensions.

</details>


### [38] [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge](https://arxiv.org/abs/2307.08813)
*Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa LÃ³pez-Marrero, Shinjae Yoo, Shantenu Jha*

Main category: cs.CL

TL;DR: The study explores using large language models (LLMs) to automate the extraction of biomolecular interactions, showing their potential in tasks like protein interaction recognition and gene regulatory relationship delineation, though challenges remain for diverse functional groups.


<details>
  <summary>Details</summary>
Motivation: Traditional expert curation of molecular interactions is labor-intensive; LLMs offer a scalable, automated alternative for genome-scale knowledge extraction.

Method: The study evaluates various LLMs on biological tasks, comparing their performance in recognizing protein interactions, gene-pathway links, and regulatory relationships.

Result: Larger LLMs performed better, excelling in specific tasks but struggling with diverse functional groups and highly correlated regulatory relationships.

Conclusion: LLMs show promise for AI-assisted biological knowledge discovery, though further refinement is needed for broader applicability.

Abstract: Background: Identification of the interactions and regulatory relations
between biomolecules play pivotal roles in understanding complex biological
systems and the mechanisms underlying diverse biological functions. However,
the collection of such molecular interactions has heavily relied on expert
curation in the past, making it labor-intensive and time-consuming. To mitigate
these challenges, we propose leveraging the capabilities of large language
models (LLMs) to automate genome-scale extraction of this crucial knowledge.
  Results: In this study, we investigate the efficacy of various LLMs in
addressing biological tasks, such as the recognition of protein interactions,
identification of genes linked to pathways affected by low-dose radiation, and
the delineation of gene regulatory relationships. Overall, the larger models
exhibited superior performance, indicating their potential for specific tasks
that involve the extraction of complex interactions among genes and proteins.
Although these models possessed detailed information for distinct gene and
protein groups, they faced challenges in identifying groups with diverse
functions and in recognizing highly correlated gene regulatory relationships.
  Conclusions: By conducting a comprehensive assessment of the state-of-the-art
models using well-established molecular interaction and pathway databases, our
study reveals that LLMs can identify genes/proteins associated with pathways of
interest and predict their interactions to a certain extent. Furthermore, these
models can provide important insights, marking a noteworthy stride toward
advancing our understanding of biological systems through AI-assisted knowledge
discovery.

</details>


### [39] [A dataset and benchmark for hospital course summarization with adapted large language models](https://arxiv.org/abs/2403.05720)
*Asad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Jason Hom, Christian Bluethgen, Eduardo Pontes Reis, Sergios Gatidis, Namuun Clifford, Joseph Daws, Arash S. Tehrani, Jangwon Kim, Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: The paper introduces MIMIC-IV-BHC, a dataset for adapting LLMs to synthesize Brief Hospital Course (BHC) summaries from clinical notes. It benchmarks five LLMs, finding Llama2-13B fine-tuned best quantitatively, while GPT-4 with in-context learning is preferred clinically.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in automating BHC synthesis from clinical notes, a task not yet demonstrated in healthcare.

Method: Uses MIMIC-IV-BHC dataset to adapt LLMs via prompting and fine-tuning. Evaluates five LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2, GPT-3.5, GPT-4) using similarity metrics and a clinician study.

Result: Llama2-13B fine-tuned performs best quantitatively (BLEU, BERT-Score), but GPT-4 with in-context learning is clinically preferred and handles longer inputs better.

Conclusion: Qualitative clinical evaluation is crucial, as GPT-4's in-context learning outperforms fine-tuned models in clinician preference despite similar metrics.

Abstract: Brief hospital course (BHC) summaries are clinical documents that summarize a
patient's hospital stay. While large language models (LLMs) depict remarkable
capabilities in automating real-world tasks, their capabilities for healthcare
applications such as synthesizing BHCs from clinical notes have not been shown.
We introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating
clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC
synthesis. Furthermore, we introduce a benchmark of the summarization
performance of two general-purpose LLMs and three healthcare-adapted LLMs.
Using clinical notes as input, we apply prompting-based (using in-context
learning) and fine-tuning-based adaptation strategies to three open-source LLMs
(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,
GPT-4). We evaluate these LLMs across multiple context-length inputs using
natural language similarity metrics. We further conduct a clinical study with
five clinicians, comparing clinician-written and LLM-generated BHCs across 30
samples, focusing on their potential to enhance clinical decision-making
through improved summary quality. We observe that the Llama2-13B fine-tuned LLM
outperforms other domain-adapted models given quantitative evaluation metrics
of BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to
increasing context lengths of clinical note inputs than fine-tuned Llama2-13B.
Despite comparable quantitative metrics, the reader study depicts a significant
preference for summaries generated by GPT-4 with in-context learning compared
to both Llama2-13B fine-tuned summaries and the original summaries,
highlighting the need for qualitative clinical evaluation.

</details>


### [40] [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://arxiv.org/abs/2403.12766)
*Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, Yue Zhang*

Main category: cs.CL

TL;DR: The paper introduces NovelQA, a benchmark for evaluating long-context understanding in LLMs, revealing their limitations in multi-hop reasoning and handling lengthy inputs.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks fall short in evaluating LLMs' long-context abilities, necessitating a tailored benchmark like NovelQA.

Method: NovelQA is constructed from English novels, featuring manual annotation and diverse question types to assess nuanced comprehension.

Result: LLMs struggle with multi-hop reasoning, detail-oriented questions, and inputs exceeding 200,000 tokens.

Conclusion: Substantial advancements are needed in LLMs to improve long-context comprehension and computational literary analysis.

Abstract: Recent advancements in Large Language Models (LLMs) have pushed the
boundaries of natural language processing, especially in long-context
understanding. However, the evaluation of these models' long-context abilities
remains a challenge due to the limitations of current benchmarks. To address
this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with
complex, extended narratives. Constructed from English novels, NovelQA offers a
unique blend of complexity, length, and narrative coherence, making it an ideal
tool for assessing deep textual understanding in LLMs. This paper details the
design and construction of NovelQA, focusing on its comprehensive manual
annotation process and the variety of question types aimed at evaluating
nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals
significant insights into their strengths and weaknesses. Notably, the models
struggle with multi-hop reasoning, detail-oriented questions, and handling
extremely long inputs, with average lengths exceeding 200,000 tokens. Results
highlight the need for substantial advancements in LLMs to enhance their
long-context comprehension and contribute effectively to computational literary
analysis.

</details>


### [41] [Large Language Model Sentinel: LLM Agent for Adversarial Purification](https://arxiv.org/abs/2405.20770)
*Guang Lin, Toshihisa Tanaka, Qibin Zhao*

Main category: cs.CL

TL;DR: LLAMOS is a novel defense technique for LLMs that purifies adversarial textual inputs using agent instruction and defense guidance, enhancing robustness without needing adversarial training.


<details>
  <summary>Details</summary>
Motivation: Address security concerns in LLMs by defending against adversarial attacks through textual perturbations.

Method: LLAMOS uses agent instruction to simulate defense agents and defense guidance to modify inputs, ensuring accurate outputs.

Result: Effective defense against adversarial attacks on both open-source and closed-source LLMs, with robust performance even without adversarial training.

Conclusion: LLAMOS enhances LLM adversarial robustness, demonstrating resilience in adversarial experiments.

Abstract: Over the past two years, the use of large language models (LLMs) has advanced
rapidly. While these LLMs offer considerable convenience, they also raise
security concerns, as LLMs are vulnerable to adversarial attacks by some
well-designed textual perturbations. In this paper, we introduce a novel
defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is
designed to enhance the adversarial robustness of LLMs by purifying the
adversarial textual examples before feeding them into the target LLM. Our
method comprises two main components: a) Agent instruction, which can simulate
a new agent for adversarial defense, altering minimal characters to maintain
the original meaning of the sentence while defending against attacks; b)
Defense guidance, which provides strategies for modifying clean or adversarial
examples to ensure effective defense and accurate outputs from the target LLMs.
Remarkably, the defense agent demonstrates robust defensive capabilities even
without learning from adversarial examples. Additionally, we conduct an
intriguing adversarial experiment where we develop two agents, one for defense
and one for attack, and engage them in mutual confrontation. During the
adversarial interactions, neither agent completely beat the other. Extensive
experiments on both open-source and closed-source LLMs demonstrate that our
method effectively defends against adversarial attacks, thereby enhancing
adversarial robustness.

</details>


### [42] [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)
*Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure*

Main category: cs.CL

TL;DR: The paper explores detecting synthetic lyrics generated by LLMs, addressing gaps in existing research by evaluating detection methods and domain adaptation on a diverse dataset.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated music content, especially lyrics, raises concerns about copyright, consumer satisfaction, and spam, necessitating detection methods for this unexplored text modality.

Method: A diverse dataset of real and synthetic lyrics was curated, validated by humans and automated methods. Existing synthetic text detection approaches were evaluated and adapted to lyrics using unsupervised domain adaptation.

Result: The study shows promising results for detecting synthetic lyrics, with insights on generalization across languages, scalability, multilingual handling, and few-shot performance.

Conclusion: The findings can inform AI-generated music policies and improve transparency for users, highlighting the need for further research in this domain.

Abstract: In recent years, the use of large language models (LLMs) to generate music
content, particularly lyrics, has gained in popularity. These advances provide
valuable tools for artists and enhance their creative processes, but they also
raise concerns about copyright violations, consumer satisfaction, and content
spamming. Previous research has explored content detection in various domains.
However, no work has focused on the text modality, lyrics, in music. To address
this gap, we curated a diverse dataset of real and synthetic lyrics from
multiple languages, music genres, and artists. The generation pipeline was
validated using both humans and automated methods. We performed a thorough
evaluation of existing synthetic text detection approaches on lyrics, a
previously unexplored data type. We also investigated methods to adapt the
best-performing features to lyrics through unsupervised domain adaptation.
Following both music and industrial constraints, we examined how well these
approaches generalize across languages, scale with data availability, handle
multilingual language content, and perform on novel genres in few-shot
settings. Our findings show promising results that could inform policy
decisions around AI-generated music and enhance transparency for users.

</details>


### [43] [SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy](https://arxiv.org/abs/2407.03004)
*Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata, Stefanie Liebe*

Main category: cs.CL

TL;DR: SemioLLM evaluates 6 LLMs on epilepsy diagnosis using unstructured clinical narratives, showing clinician-level performance with improvements from expert-guided reasoning, but highlights hallucination and citation issues.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLMs on unstructured clinical narratives, focusing on real-world diagnostic challenges in epilepsy.

Method: SemioLLM framework benchmarks 6 LLMs (e.g., GPT-4, LlaMa3) using 1,269 seizure descriptions, assessing performance on seizure onset zone prediction with prompt engineering and expert-guided reasoning.

Result: Most LLMs achieve clinician-level accuracy after prompt engineering, with performance influenced by clinical impersonation, narrative length, and language context. However, hallucination and poor citation accuracy are noted.

Conclusion: SemioLLM offers a scalable framework for clinical LLM evaluation, identifying strengths (accuracy) and limitations (interpretability), aiding development of robust healthcare AI.

Abstract: Large Language Models (LLMs) have been shown to encode clinical knowledge.
Many evaluations, however, rely on structured question-answer benchmarks,
overlooking critical challenges of interpreting and reasoning about
unstructured clinical narratives in real-world settings. Using free-text
clinical descriptions, we present SemioLLM, an evaluation framework that
benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B,
LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of
1,269 seizure descriptions, we show that most LLMs are able to accurately and
confidently generate probabilistic predictions of seizure onset zones in the
brain. Most models approach clinician-level performance after prompt
engineering, with expert-guided chain-of-thought reasoning leading to the most
consistent improvements. Performance was further strongly modulated by clinical
in-context impersonation, narrative length and language context (13.7%, 32.7%
and 14.2% performance variation, respectively). However, expert analysis of
reasoning outputs revealed that correct prediction can be based on hallucinated
knowledge and deficient source citation accuracy, underscoring the need to
improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a
scalable, domain-adaptable framework for evaluating LLMs in clinical
disciplines where unstructured verbal descriptions encode diagnostic
information. By identifying both the strengths and limitations of
state-of-the-art models, our work supports the development of clinically robust
and globally applicable AI systems for healthcare.

</details>


### [44] [ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation](https://arxiv.org/abs/2407.12022)
*Peiyang Wu, Nan Guo, Xiao Xiao, Wenming Li, Xiaochun Ye, Dongrui Fan*

Main category: cs.CL

TL;DR: The paper introduces ITERTL, an iterative training paradigm for LLMs to improve RTL code generation, outperforming GPT4 and SOTA models with a 53.8% pass@1 rate.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for LLMs in RTL generation rely on fixed datasets, limiting their potential and requiring costly reference data.

Method: ITERTL uses iterative training with samples drawn from previous cycles and a plug-and-play data filtering strategy to enhance code quality.

Result: The model achieves a 53.8% pass@1 rate on VerilogEval-human, surpassing GPT4 and SOTA models under similar data conditions.

Conclusion: The proposed method is validated as effective, significantly improving RTL generation efficiency and quality.

Abstract: Recently, large language models (LLMs) have demonstrated excellent
performance, inspiring researchers to explore their use in automating register
transfer level (RTL) code generation and improving hardware design efficiency.
However, the existing approaches to fine-tune LLMs for RTL generation typically
are conducted on fixed datasets, which do not fully stimulate the capability of
LLMs and require large amounts of reference data, which are costly to acquire.
To mitigate these issues, we innovatively introduce an iterative training
paradigm named ITERTL. During each iteration, samples are drawn from the model
trained in the previous cycle. Then these new samples are employed for training
in current loop. Furthermore, we introduce a plug-and-play data filtering
strategy, thereby encouraging the model to generate high-quality,
self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA)
open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human
benchmark. Under similar conditions of data quantity and quality, our approach
significantly outperforms the baseline. Extensive experiments validate the
effectiveness of the proposed method.

</details>


### [45] [Lawma: The Power of Specialization for Legal Annotation](https://arxiv.org/abs/2407.16615)
*Ricardo Dominguez-Olmedo, Vedant Nanda, Rediet Abebe, Stefan Bechtold, Christoph Engel, Jens Frankenreiter, Krishna Gummadi, Moritz Hardt, Michael Livermore*

Main category: cs.CL

TL;DR: The paper introduces CaselawQA, a benchmark for legal annotation tasks, and compares the performance of commercial LLMs (like GPT-4.5) with fine-tuned smaller models, finding the latter more effective.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding in using large language models (LLMs) for legal annotation and reduce reliance on costly human annotation.

Method: Creation of CaselawQA benchmark (260 tasks) and evaluation of commercial LLMs versus fine-tuned smaller models.

Result: Commercial LLMs show variable but insufficient accuracy; fine-tuned smaller models outperform them with fewer labeled examples.

Conclusion: Fine-tuned open-source models are a better alternative to commercial LLMs for legal annotation tasks with labeled data.

Abstract: Annotation and classification of legal text are central components of
empirical legal research. Traditionally, these tasks are often delegated to
trained research assistants. Motivated by the advances in language modeling,
empirical legal scholars are increasingly turning to prompting commercial
models, hoping that it will alleviate the significant cost of human annotation.
Despite growing use, our understanding of how to best utilize large language
models for legal annotation remains limited. To bridge this gap, we introduce
CaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to
the machine learning community. We demonstrate that commercial models, such as
GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable
accuracy, generally falling short of the performance required for legal work.
We then demonstrate that small, lightly fine-tuned models outperform commercial
models. A few hundred to a thousand labeled examples are usually enough to
achieve higher accuracy. Our work points to a viable alternative to the
predominant practice of prompting commercial models. For concrete legal
annotation tasks with some available labeled data, researchers are likely
better off using a fine-tuned open-source model.

</details>


### [46] [Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models](https://arxiv.org/abs/2407.17914)
*Anna Bavaresco, Marianne de Heer Kloots, Sandro Pezzelle, Raquel FernÃ¡ndez*

Main category: cs.CL

TL;DR: Vision-language models (VLMs) outperform language-only models in predicting human brain activity during language processing, with transformer-based VLMs showing the highest alignment.


<details>
  <summary>Details</summary>
Motivation: To determine if integrating visual and linguistic information in models improves alignment with human brain activity compared to language-only models.

Method: Used pre-trained VLMs to analyze fMRI responses from participants reading concept words in sentences or with pictures, comparing results to language-only models.

Result: VLMs, especially transformer-based ones like LXMERT and VisualBERT, correlated more strongly with brain activity in language-related areas than language-only models.

Conclusion: Multimodal models better capture the multimodal nature of human concept representations, highlighting the advantage of integrating vision and language.

Abstract: Text representations from language models have proven remarkably predictive
of human neural activity involved in language processing, with the recent
transformer-based models outperforming previous architectures in downstream
tasks and prediction of brain responses. However, the word representations
learnt by language-only models may be limited in that they lack sensory
information from other modalities, which several cognitive and neuroscience
studies showed to be reflected in human meaning representations. Here, we
leverage current pre-trained vision-language models (VLMs) to investigate
whether the integration of visuo-linguistic information they operate leads to
representations that are more aligned with human brain activity than those
obtained by models trained with language-only input. We focus on fMRI responses
recorded while participants read concept words in the context of either a full
sentence or a picture. Our results reveal that VLM representations correlate
more strongly than those by language-only models with activations in brain
areas functionally related to language processing. Additionally, we find that
transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT --
yield more brain-aligned representations than generative VLMs, whose
autoregressive abilities do not seem to provide an advantage when modelling
single words. Finally, our ablation analyses suggest that the high brain
alignment achieved by some of the VLMs we evaluate results from semantic
information acquired specifically during multimodal pretraining as opposed to
being already encoded in their unimodal modules. Altogether, our findings
indicate an advantage of multimodal models in predicting human brain
activations, which reveals that modelling language and vision integration has
the potential to capture the multimodal nature of human concept
representations.

</details>


### [47] [The advantages of context specific language models: the case of the Erasmian Language Model](https://arxiv.org/abs/2408.06931)
*JoÃ£o GonÃ§alves, Nick Jelicic, Michele Murgia, Evert Stamhuis*

Main category: cs.CL

TL;DR: The paper introduces the Erasmian Language Model (ELM), a smaller, context-specific model, as a sustainable alternative to large-scale language models like GPT-4, demonstrating its effectiveness in educational settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational and energy costs, privacy risks, and misuse concerns associated with large-scale language models, the paper advocates for context-specific, smaller models.

Method: Developed the 900M-parameter ELM, pre-trained and fine-tuned for Erasmus University Rotterdam, focusing on classroom applications like essay writing.

Result: ELM performs adequately in classroom tasks and excels in context-specific subjects, proving viable for resource-constrained, privacy-sensitive scenarios.

Conclusion: Context-specific language models like ELM offer a sustainable and practical alternative to large-scale models for institutions prioritizing efficiency and privacy.

Abstract: The current trend to improve language model performance seems to be based on
scaling up with the number of parameters (e.g. the state of the art GPT4 model
has approximately 1.7 trillion parameters) or the amount of training data fed
into the model. However this comes at significant costs in terms of
computational resources and energy costs that compromise the sustainability of
AI solutions, as well as risk relating to privacy and misuse. In this paper we
present the Erasmian Language Model (ELM) a small context specific, 900 million
parameter model, pre-trained and fine-tuned by and for Erasmus University
Rotterdam. We show how the model performs adequately in a classroom context for
essay writing, and how it achieves superior performance in subjects that are
part of its context. This has implications for a wide range of institutions and
organizations, showing that context specific language models may be a viable
alternative for resource constrained, privacy sensitive use cases.

</details>


### [48] [lamss: when large language models meet self-skepticism](https://arxiv.org/abs/2409.06601)
*Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji*

Main category: cs.CL

TL;DR: The paper proposes LaMsS, a method combining LLMs with self-skepticism to reduce hallucinations by introducing skepticism tokens and thresholds for response confidence.


<details>
  <summary>Details</summary>
Motivation: Addressing LLM hallucinations by leveraging human-like skeptical thinking for self-correction.

Method: Introduces skepticism tokens, augments vocabulary, and uses pre-training/fine-tuning to decode responses with skepticism levels.

Result: LaMsS outperforms baselines in accuracy, AUC, and AP on multi-choice and open-domain QA tasks, generalizing well.

Conclusion: Self-skepticism modeling improves LLM reliability, with potential for broader AI applications.

Abstract: Hallucination is a major challenge for large language models (LLMs), prevent
ing their further application in some fields. The skeptical thinking of
humankind
  could be useful for LLMs to self-cognition, self-reflection and alleviate
their hal lucinations. Inspired by this consideration, we propose a novel
approach called
  LaMsS, which combines the semantic understanding capability of LLMs with
  self-skepticism. By introducing a series of skepticism tokens and augmenting
  them into the vocabulary, we conduct both pertaining and finetuning, which
allow
  the LLM to decode each normal token followed by a skeptical token, represent
ing different skepticism levels. By calculating the response skepticism given a
  query, one can define a new self-aware LLM which is only willing to answer
  with relative lower skepticism level than the threshold. By examining the
accu racy, AUC and AP of willingly answering questions, we demonstrate that
LaMsS
  achieves better performance than baselines on both multi-choice questions and
  open-domain question-answering benchmarks, and can generalize to multi-task
  and out-of-domain settings. Our study sheds some lights on the
self-skepticism
  modeling on further artificial intelligence. Project code and model
checkpoints
  can be found in https://anonymous.4open.science/r/SM-1E76.

</details>


### [49] [ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems](https://arxiv.org/abs/2410.19572)
*Ishneet Sukhvinder Singh, Ritvik Aggarwal, Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin Zhu, Sean O'Brien*

Main category: cs.CL

TL;DR: ChunkRAG improves RAG systems by filtering irrelevant document chunks using LLM-based relevance scoring, reducing inaccuracies and enhancing factual accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems retrieve irrelevant or loosely related information at the document level, leading to inaccurate responses.

Method: Proposes ChunkRAG, which uses semantic chunking and LLM-based relevance scoring to filter chunks before generation.

Result: Outperforms existing RAG models, achieving higher accuracy in tasks requiring precise retrieval.

Conclusion: ChunkRAG enhances RAG reliability, benefiting applications like fact-checking and multi-hop reasoning.

Abstract: Retrieval-Augmented Generation (RAG) systems using large language models
(LLMs) often generate inaccurate responses due to the retrieval of irrelevant
or loosely related information. Existing methods, which operate at the document
level, fail to effectively filter out such content. We propose LLM-driven chunk
filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and
filtering retrieved information at the chunk level. Our approach employs
semantic chunking to divide documents into coherent sections and utilizes
LLM-based relevance scoring to assess each chunk's alignment with the user's
query. By filtering out less pertinent chunks before the generation phase, we
significantly reduce hallucinations and improve factual accuracy. Experiments
show that our method outperforms existing RAG models, achieving higher accuracy
on tasks requiring precise information retrieval. This advancement enhances the
reliability of RAG systems, making them particularly beneficial for
applications like fact-checking and multi-hop reasoning.

</details>


### [50] [MEG: Medical Knowledge-Augmented Large Language Models for Question Answering](https://arxiv.org/abs/2411.03883)
*Laura Cabello, Carmen Martin-Turrero, Uchenna Akujuobi, Anders SÃ¸gaard, Carlos Bobed*

Main category: cs.CL

TL;DR: MEG enhances LLMs for medical QA by integrating knowledge graph embeddings via a lightweight network, improving accuracy over specialized models.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with nuanced reasoning in specialized fields like medicine despite high training costs.

Method: MEG uses a lightweight mapping network to incorporate knowledge graph embeddings into LLMs.

Result: MEG improves accuracy by +6.7% and +9.9% over BioMistral-7B and MediTron-7B, respectively.

Conclusion: MEG effectively leverages external knowledge, maintaining robustness across graph encoders.

Abstract: Question answering is a natural language understanding task that involves
reasoning over both explicit context, and unstated relevant domain knowledge.
Despite the high cost of training, large language models (LLMs) -- the backbone
of most modern question-answering systems -- still struggle to reliably capture
the nuanced relationships between concepts that are crucial for reasoning in
specialized fields like medicine. In this work, we present MEG, a
parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a
lightweight mapping network to incorporate knowledge graph embeddings into the
LLM, enabling it to leverage external knowledge in a cost-effective way. We
evaluate our method on four popular medical multiple-choice datasets and show
that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain
significant advantages from the factual grounding these embeddings provide. MEG
attains an average of +6.7% and +9.9% accuracy over specialized models like
BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's
performance remains robust to the choice of graph encoder.

</details>


### [51] [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000)
*Jonathan Roberts, Kai Han, Samuel Albanie*

Main category: cs.CL

TL;DR: The paper evaluates how effectively 17 leading LLMs use their extended context windows for complex information retrieval and reasoning, finding some models perform well with multiple threads but often have shorter effective context limits than advertised.


<details>
  <summary>Details</summary>
Motivation: To understand the gap between the increasing context limits of LLMs and their actual effectiveness in tasks requiring complex information retrieval and reasoning.

Method: Conducted retrieval experiments on 17 leading LLMs to assess their ability to follow information threads through long contexts.

Result: Many models are threadsafe but have shorter effective context limits than their maximum supported length, with performance declining as context grows. Tokenizer differences also affect comparisons.

Conclusion: LLMs' effective context use is often overestimated; tokenizer differences must be considered. The study provides code and data for further research.

Abstract: As the context limits of Large Language Models (LLMs) increase, the range of
possible applications and downstream functions broadens. In many real-world
tasks, decisions depend on details scattered across collections of often
disparate documents containing mostly irrelevant information. Long-context LLMs
appear well-suited to this form of complex information retrieval and reasoning,
which has traditionally proven costly and time-consuming. However, although the
development of longer context models has seen rapid gains in recent years, our
understanding of how effectively LLMs use their context has not kept pace. To
address this, we conduct a set of retrieval experiments designed to evaluate
the capabilities of 17 leading LLMs, such as their ability to follow threads of
information through the context window. Strikingly, we find that many models
are remarkably threadsafe: capable of simultaneously following multiple threads
without significant loss in performance. Still, for many models, we find the
effective context limit is significantly shorter than the supported context
length, with accuracy decreasing as the context window grows. Our study also
highlights the important point that token counts from different tokenizers
should not be directly compared -- they often correspond to substantially
different numbers of written characters. We release our code and long-context
experimental data.

</details>


### [52] [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://arxiv.org/abs/2411.06037)
*Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, Cyrus Rashtchian*

Main category: cs.CL

TL;DR: The paper explores whether errors in RAG systems stem from LLMs failing to use retrieved context or insufficient context. It introduces 'sufficient context' to classify instances and analyzes model performance, finding larger models excel with sufficient context but struggle otherwise, while smaller models hallucinate or abstain. A selective generation method improves accuracy by 2-10%.


<details>
  <summary>Details</summary>
Motivation: To determine if errors in RAG systems are due to LLMs not utilizing context or insufficient context, and to improve model performance by addressing these issues.

Method: Develops a notion of 'sufficient context,' classifies instances, and analyzes models and datasets. Introduces a selective generation method for guided abstention.

Result: Larger models perform well with sufficient context but fail otherwise; smaller models hallucinate or abstain. The selective generation method improves accuracy by 2-10%.

Conclusion: Context sufficiency is key to RAG performance. Selective generation leveraging sufficient context reduces errors, improving model responses.

Abstract: Augmenting LLMs with context leads to improved performance across many
applications. Despite much research on Retrieval Augmented Generation (RAG)
systems, an open question is whether errors arise because LLMs fail to utilize
the context from retrieval or the context itself is insufficient to answer the
query. To shed light on this, we develop a new notion of sufficient context,
along with a method to classify instances that have enough information to
answer the query. We then use sufficient context to analyze several models and
datasets. By stratifying errors based on context sufficiency, we find that
larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude
3.5) excel at answering queries when the context is sufficient, but often
output incorrect answers instead of abstaining when the context is not. On the
other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2)
hallucinate or abstain often, even with sufficient context. We further
categorize cases when the context is useful, and improves accuracy, even though
it does not fully answer the query and the model errs without the context.
Building on our findings, we explore ways to reduce hallucinations in RAG
systems, including a new selective generation method that leverages sufficient
context information for guided abstention. Our method improves the fraction of
correct answers among times where the model responds by 2--10\% for Gemini,
GPT, and Gemma. Key findings and the prompts used in our autorater analysis are
available on our github.

</details>


### [53] [7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
*Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang*

Main category: cs.CL

TL;DR: Moxin 7B is a fully open-source LLM addressing transparency issues in LLMs by releasing all components, achieving superior performance through advanced training methods.


<details>
  <summary>Details</summary>
Motivation: To mitigate concerns about transparency, reproducibility, and safety in LLMs by providing a fully open-source model with released training code, data, and checkpoints.

Method: Developed Moxin 7B with open science principles, fine-tuned using SOTA frameworks, chain-of-thought data, and GRPO for reasoning.

Result: Achieved superior performance in zero-shot, few-shot, and chain-of-thought evaluations.

Conclusion: Moxin 7B demonstrates the feasibility and benefits of fully open-source LLMs, promoting innovation and transparency in the field.

Abstract: Recently, Large Language Models (LLMs) have undergone a significant
transformation, marked by a rapid rise in both their popularity and
capabilities. Leading this evolution are proprietary LLMs like GPT-4 and
GPT-o1, which have captured widespread attention in the AI community due to
their remarkable performance and versatility. Simultaneously, open-source LLMs,
such as LLaMA, have made great contributions to the ever-increasing popularity
of LLMs due to the ease to customize and deploy the models across diverse
applications. Although open-source LLMs present unprecedented opportunities for
innovation and research, the commercialization of LLMs has raised concerns
about transparency, reproducibility, and safety. Many open-source LLMs fail to
meet fundamental transparency requirements by withholding essential components
like training code and data, which may hinder further innovations on LLMs. To
mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed,
adhering to principles of open science, open source, open data, and open
access. We release the pre-training code and configurations, training and
fine-tuning datasets, and intermediate and final checkpoints, aiming to make
continuous commitments to fully open-source LLMs. After pre-training and
obtaining the base model, we finetune the Moxin Base model with SOTA
post-training framework and instruction data to obtain Moxin Instruct model. To
improve the reasoning capability, we further finetune our Instruct model with
chain-of-thought data distilled from DeepSeek R1, and then use Group Relative
Policy Optimization (GRPO), an efficient and effective reinforcement learning
algorithm following DeepSeek R1, to finetune our model, leading to the Moxin
Reasoning model. Experiments show that our models achieve superior performance
in various evaluations such as zero-shot evaluation, few-shot evaluation, and
CoT evaluation.

</details>


### [54] [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
*Xuemei Tang, Xufeng Duan, Zhenguang G. Cai*

Main category: cs.CL

TL;DR: The study evaluates LLMs' ability to automate literature reviews, revealing issues like hallucinated references and varying performance across disciplines.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability of LLMs in automating literature review tasks (reference generation, summary, and composition) due to unclear effectiveness.

Method: A framework with multidimensional metrics evaluates hallucination rates, semantic coverage, and factual consistency in LLM outputs compared to human-written reviews.

Result: Advanced LLMs still produce hallucinated references, and performance varies by discipline.

Conclusion: Further research is needed to enhance LLMs' reliability for academic literature reviews.

Abstract: Large language models (LLMs) have emerged as a potential solution to automate
the complex processes involved in writing literature reviews, such as
literature collection, organization, and summarization. However, it is yet
unclear how good LLMs are at automating comprehensive and reliable literature
reviews. This study introduces a framework to automatically evaluate the
performance of LLMs in three key tasks of literature writing: reference
generation, literature summary, and literature review composition. We introduce
multidimensional evaluation metrics that assess the hallucination rates in
generated references and measure the semantic coverage and factual consistency
of the literature summaries and compositions against human-written
counterparts. The experimental results reveal that even the most advanced
models still generate hallucinated references, despite recent progress.
Moreover, we observe that the performance of different models varies across
disciplines when it comes to writing literature reviews. These findings
highlight the need for further research and development to improve the
reliability of LLMs in automating academic literature reviews.

</details>


### [55] [Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?](https://arxiv.org/abs/2502.04718)
*Sourabrata Mukherjee, Atul Kr. Ojha, John P. McCrae, Ondrej Dusek*

Main category: cs.CL

TL;DR: The paper explores automatic metrics for evaluating text style transfer (TST), comparing existing and novel NLP metrics, including LLM-based evaluations, and demonstrates their effectiveness through correlation with human judgments.


<details>
  <summary>Details</summary>
Motivation: Evaluating TST outputs is challenging due to the need for assessing style transfer accuracy, content preservation, and naturalness. Human evaluation is costly, and automatic metrics for TST are understudied compared to other NLP tasks.

Method: The study examines existing and novel NLP metrics for TST evaluation, focusing on sentiment transfer and detoxification in English, Hindi, and Bengali. It includes meta-evaluation via human judgment correlation and explores LLMs for evaluation.

Result: Advanced NLP metrics and LLM-based evaluations outperform existing TST metrics. Oracle ensemble approaches show further potential.

Conclusion: The paper highlights the effectiveness of advanced NLP metrics and LLMs for TST evaluation, with ensemble methods offering promising improvements.

Abstract: Text style transfer (TST) is the task of transforming a text to reflect a
particular style while preserving its original content. Evaluating TST outputs
is a multidimensional challenge, requiring the assessment of style transfer
accuracy, content preservation, and naturalness. Using human evaluation is
ideal but costly, as is common in other natural language processing (NLP)
tasks, however, automatic metrics for TST have not received as much attention
as metrics for, e.g., machine translation or summarization. In this paper, we
examine both set of existing and novel metrics from broader NLP tasks for TST
evaluation, focusing on two popular subtasks, sentiment transfer and
detoxification, in a multilingual context comprising English, Hindi, and
Bengali. By conducting meta-evaluation through correlation with human
judgments, we demonstrate the effectiveness of these metrics when used
individually and in ensembles. Additionally, we investigate the potential of
large language models (LLMs) as tools for TST evaluation. Our findings
highlight newly applied advanced NLP metrics and LLM-based evaluations provide
better insights than existing TST metrics. Our oracle ensemble approaches show
even more potential.

</details>


### [56] [Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization](https://arxiv.org/abs/2502.13108)
*Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal, Bhargava Kumar, Srikant Panda, Tejaswini Kumar*

Main category: cs.CL

TL;DR: The paper introduces a Multi-Task Learning (MTL) framework for Clinical Question Answering (CQA) that combines answer extraction and medical categorization, improving performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based CQA models lack answer categorization, which is crucial for structured retrieval and medical decision support.

Method: A Multi-Task Learning (MTL) framework is proposed, jointly training models for answer extraction and classifying answers into five medical categories.

Result: MTL improves F1-score by 2.2% and achieves 90.7% accuracy in categorization on the emrQA dataset.

Conclusion: MTL enhances CQA performance and enables structured medical information retrieval, making models more practical for healthcare.

Abstract: Clinical Question Answering (CQA) plays a crucial role in medical
decision-making, enabling physicians to extract relevant information from
Electronic Medical Records (EMRs). While transformer-based models such as BERT,
BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in
CQA, existing models lack the ability to categorize extracted answers, which is
critical for structured retrieval, content filtering, and medical decision
support.
  To address this limitation, we introduce a Multi-Task Learning (MTL)
framework that jointly trains CQA models for both answer extraction and medical
categorization. In addition to predicting answer spans, our model classifies
responses into five standardized medical categories: Diagnosis, Medication,
Symptoms, Procedure, and Lab Reports. This categorization enables more
structured and interpretable outputs, making clinical QA models more useful in
real-world healthcare settings.
  We evaluate our approach on emrQA, a large-scale dataset for medical question
answering. Results show that MTL improves F1-score by 2.2% compared to standard
fine-tuning, while achieving 90.7% accuracy in answer categorization. These
findings suggest that MTL not only enhances CQA performance but also introduces
an effective mechanism for categorization and structured medical information
retrieval.

</details>


### [57] [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)
*Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang*

Main category: cs.CL

TL;DR: EAGLE-3 improves speculative sampling by switching from feature prediction to direct token prediction and multi-layer feature fusion, achieving up to 6.5x speedup over vanilla methods.


<details>
  <summary>Details</summary>
Motivation: Modern LLMs are slow and expensive due to sequential processing. EAGLE-3 addresses limitations of EAGLE's feature prediction by enabling better scalability with training data.

Method: EAGLE-3 replaces feature prediction with direct token prediction and uses multi-layer feature fusion via training-time test.

Result: Achieves 6.5x speedup, 1.4x improvement over EAGLE-2, and 1.38x throughput gain in SGLang.

Conclusion: EAGLE-3 enhances performance and scalability, making it a superior solution for speculative sampling in LLMs.

Abstract: The sequential nature of modern LLMs makes them expensive and slow, and
speculative sampling has proven to be an effective solution to this problem.
Methods like EAGLE perform autoregression at the feature level, reusing
top-layer features from the target model to achieve better results than vanilla
speculative sampling. A growing trend in the LLM community is scaling up
training data to improve model intelligence without increasing inference costs.
However, we observe that scaling up data provides limited improvements for
EAGLE. We identify that this limitation arises from EAGLE's feature prediction
constraints. In this paper, we introduce EAGLE-3, which abandons feature
prediction in favor of direct token prediction and replaces reliance on
top-layer features with multi-layer feature fusion via a technique named
training-time test. These improvements significantly enhance performance and
enable the draft model to fully benefit from scaling up training data. Our
experiments include both chat models and reasoning models, evaluated on five
tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with
about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves
a 1.38x throughput improvement at a batch size of 64. The code is available at
https://github.com/SafeAILab/EAGLE.

</details>


### [58] [Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations](https://arxiv.org/abs/2503.14477)
*Ziwei Ji, Lei Yu, Yeskendir Koishekenov, Yejin Bang, Anthony Hartshorn, Alan Schelten, Cheng Zhang, Pascale Fung, Nicola Cancedda*

Main category: cs.CL

TL;DR: LLMs often overconfidently assert false claims, misleading users. The paper identifies a linear feature governing verbal uncertainty in LLMs, showing it poorly correlates with actual semantic uncertainty. This mismatch predicts hallucinations better, and interventions reduce confident hallucinations by ~30%.


<details>
  <summary>Details</summary>
Motivation: LLMs' overconfident false claims mislead users, undermining trust. Addressing this by aligning verbal and semantic uncertainty is crucial.

Method: Identified a linear feature for verbal uncertainty in LLMs, compared it with semantic uncertainty, and used the mismatch to predict and reduce hallucinations.

Result: The mismatch between verbal and semantic uncertainty predicts hallucinations better. Interventions reduced confident hallucinations by ~30%.

Conclusion: Aligning verbal and semantic uncertainty can mitigate LLM hallucinations, improving trust and reliability.

Abstract: LLMs often adopt an assertive language style also when making false claims.
Such ``overconfident hallucinations'' mislead users and erode trust. Achieving
the ability to express in language the actual degree of uncertainty around a
claim is therefore of great importance. We find that ``verbal uncertainty'' is
governed by a single linear feature in the representation space of LLMs, and
show that this has only moderate correlation with the actual ``semantic
uncertainty'' of the model. We apply this insight and show that (1) the
mismatch between semantic and verbal uncertainty is a better predictor of
hallucinations than semantic uncertainty alone and (2) we can intervene on
verbal uncertainty at inference time and reduce confident hallucinations on
short-form answers, achieving an average relative reduction of ~30%.

</details>


### [59] [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.16419)
*Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu*

Main category: cs.CL

TL;DR: A survey on improving reasoning efficiency in Large Language Models (LLMs) by addressing the 'overthinking phenomenon' through model-based, output-based, and input prompts-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency caused by verbose and redundant reasoning sequences (overthinking) in LLMs, despite their improved performance in complex tasks.

Method: Systematically categorizes existing approaches into model-based, reasoning output-based, and input prompts-based methods, and explores training data, small models, and evaluation.

Result: Identifies key directions for enhancing reasoning efficiency, including optimizing models, reducing reasoning steps, and leveraging input prompts.

Conclusion: The survey provides a structured overview of current progress and future directions for achieving efficient reasoning in LLMs.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as
OpenAI o1 and DeepSeek-R1, have further improved performance in System-2
reasoning domains like mathematics and programming by harnessing supervised
fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the
Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences
improve performance, they also introduce significant computational overhead due
to verbose and redundant outputs, known as the "overthinking phenomenon". In
this paper, we provide the first structured survey to systematically
investigate and explore the current progress toward achieving efficient
reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we
categorize existing works into several key directions: (1) model-based
efficient reasoning, which considers optimizing full-length reasoning models
into more concise reasoning models or directly training efficient reasoning
models; (2) reasoning output-based efficient reasoning, which aims to
dynamically reduce reasoning steps and length during inference; (3) input
prompts-based efficient reasoning, which seeks to enhance reasoning efficiency
based on input prompt properties such as difficulty or length control.
Additionally, we introduce the use of efficient data for training reasoning
models, explore the reasoning capabilities of small language models, and
discuss evaluation methods and benchmarking.

</details>


### [60] [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)
*Yijiong Yu*

Main category: cs.CL

TL;DR: A method to speed up reasoning models by parallelizing tasks, achieving over 100% speedup without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models are slow due to lengthy sequences; parallelization can improve efficiency.

Method: Use specialized attention masks to decode multiple tokens per step in parallel branches.

Result: Over 100% speedup in decoding time while maintaining answer quality.

Conclusion: Parallelizing reasoning tasks is effective for accelerating models without losing accuracy.

Abstract: Recent advances in reasoning models have demonstrated significant
improvements in accuracy, particularly for complex tasks such as mathematical
reasoning, by employing detailed and comprehensive reasoning processes.
However, generating these lengthy reasoning sequences is computationally
expensive and time-consuming. To address this inefficiency, we leverage the
inherent parallelizability of certain tasks to accelerate the reasoning
process. Specifically, when multiple parallel reasoning branches exist, we
decode multiple tokens per step using a specialized attention mask, processing
them within a single sequence, avoiding additional memory usage. Experimental
results show that our method achieves over 100% speedup in decoding time while
maintaining the answer quality.

</details>


### [61] [OnRL-RAG: Real-Time Personalized Mental Health Dialogue System](https://arxiv.org/abs/2504.02894)
*Ahsan Bilal, Beiyu Lin*

Main category: cs.CL

TL;DR: The paper proposes OnRL-RAG, a system combining retrieval-augmented generation and online reinforcement learning to personalize responses for mental health issues, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs (outdated knowledge) and improve personalization for diverse user needs, especially in mental health applications.

Method: Uses an open-source dataset from 2028 college students and integrates retrieval-augmented generation with online reinforcement learning (RLHF) for dynamic adaptation.

Result: OnRL-RAG outperforms standard RAG and simple LLMs (GPT-4o, GPT-4o-mini, Gemini-1.5, GPT-3.5) in personalizing responses for mental health issues.

Conclusion: The system enables real-life LLM applications for personalized services and aids researchers in aligning theories with daily human environments.

Abstract: Large language models (LLMs) have been widely used for various tasks and
applications. However, LLMs and fine-tuning are limited to the pre-trained
data. For example, ChatGPT's world knowledge until 2021 can be outdated or
inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation
(RAG), is proposed to augment LLMs with additional, new, latest details and
information to LLMs. While RAG offers the correct information, it may not best
present it, especially to different population groups with personalizations.
Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by
aligning model responses with human preference through feedback loops. In
real-life applications, such as mental health problems, a dynamic and
feedback-based model would continuously adapt to new information and offer
personalized assistance due to complex factors fluctuating in a daily
environment. Thus, we propose an Online Reinforcement Learning-based
Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the
responding systems to mental health problems, such as stress, anxiety, and
depression. We use an open-source dataset collected from 2028 College Students
with 28 survey questions for each student to demonstrate the performance of our
proposed system with the existing systems. Our system achieves superior
performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini,
Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life
applications of LLMs for personalized services in the everyday environment. The
results will also help researchers in the fields of sociology, psychology, and
neuroscience to align their theories more closely with the actual human daily
environment.

</details>


### [62] [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)
*Xinnong Zhang, Jiayu Lin, Xinyi Mou, Shiyue Yang, Xiawei Liu, Libo Sun, Hanjia Lyu, Yihang Yang, Weihong Qi, Yue Chen, Guanying Li, Ling Yan, Yao Hu, Siming Chen, Yu Wang, Xuanjing Huang, Jiebo Luo, Shiping Tang, Libo Wu, Baohua Zhou, Zhongyu Wei*

Main category: cs.CL

TL;DR: SocioVerse is an LLM-agent-driven world model for social simulation, addressing alignment challenges and demonstrating effectiveness in large-scale simulations across politics, news, and economics.


<details>
  <summary>Details</summary>
Motivation: To overcome alignment challenges in social simulation (environment, users, interactions, behaviors) and leverage LLMs for capturing individual differences and group behaviors.

Method: Introduces SocioVerse with four alignment components and a user pool of 10 million real individuals, validated through large-scale simulations in politics, news, and economics.

Result: SocioVerse reflects large-scale population dynamics with diversity, credibility, and representativeness, achieved via standardized procedures and minimal manual adjustments.

Conclusion: SocioVerse effectively addresses alignment challenges in social simulation, showcasing its potential for modeling human behavior and group dynamics.

Abstract: Social simulation is transforming traditional social science research by
modeling human behavior through interactions between virtual individuals and
their environments. With recent advances in large language models (LLMs), this
approach has shown growing potential in capturing individual differences and
predicting group behaviors. However, existing methods face alignment challenges
related to the environment, target users, interaction mechanisms, and
behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven
world model for social simulation. Our framework features four powerful
alignment components and a user pool of 10 million real individuals. To
validate its effectiveness, we conducted large-scale simulation experiments
across three distinct domains: politics, news, and economics. Results
demonstrate that SocioVerse can reflect large-scale population dynamics while
ensuring diversity, credibility, and representativeness through standardized
procedures and minimal manual adjustments.

</details>


### [63] [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
*Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li*

Main category: cs.CL

TL;DR: The paper explores a knowledge graph-based RAG framework for Japanese medical QA using small-scale open-source LLMs, finding limited impact due to sensitivity to retrieved content quality.


<details>
  <summary>Details</summary>
Motivation: Privacy constraints limit commercial LLMs in Japanese clinical settings, prompting exploration of open-source alternatives combined with RAG.

Method: The study employs a knowledge graph-based RAG framework for Japanese medical QA with small-scale open-source LLMs.

Result: KG-based RAG shows limited impact, with effectiveness highly dependent on the quality and relevance of retrieved content.

Conclusion: The findings highlight challenges and potential for RAG in Japanese medical QA, offering insights for low-resource languages.

Abstract: Large language models (LLMs) perform well in medical QA, but their
effectiveness in Japanese contexts is limited due to privacy constraints that
prevent the use of commercial models like GPT-4 in clinical settings. As a
result, recent efforts focus on instruction-tuning open-source LLMs, though the
potential of combining them with retrieval-augmented generation (RAG) remains
underexplored. To bridge this gap, we are the first to explore a knowledge
graph-based (KG) RAG framework for Japanese medical QA small-scale open-source
LLMs. Experimental results show that KG-based RAG has only a limited impact on
Japanese medical QA using small-scale open-source LLMs. Further case studies
reveal that the effectiveness of the RAG is sensitive to the quality and
relevance of the external retrieved content. These findings offer valuable
insights into the challenges and potential of applying RAG in Japanese medical
QA, while also serving as a reference for other low-resource languages.

</details>


### [64] [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
*Jennifer Haase, Finn Klessascheck, Jan Mendling, Sebastian Pokutta*

Main category: cs.CL

TL;DR: The study evaluates 11 LLMs across 10 occupational tasks, finding GPT-4o superior but costly, while smaller models like Gemma-3 and Phi-4 offer viable alternatives for efficiency and privacy. It advocates for context-aware sufficiency assessments over performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: Concerns over LLMs' energy consumption, costs, and data sovereignty drive the need to identify when smaller, locally deployable models are sufficient for real-world tasks.

Method: A dual-LLM-based evaluation framework automated task execution and standardized assessment across 10 criteria (output quality, factual accuracy, ethics).

Result: GPT-4o performed best but had high costs; smaller models like Gemma-3 and Phi-4 were reliable for many tasks. Task type influenced effectiveness, with conceptual tasks being more challenging.

Conclusion: The study calls for task- and context-aware sufficiency assessments over performance benchmarks, providing scalable methods for sustainable and responsible LLM deployment.

Abstract: Large language models (LLMs) have become increasingly embedded in
organizational workflows. This has raised concerns over their energy
consumption, financial costs, and data sovereignty. While performance
benchmarks often celebrate cutting-edge models, real-world deployment decisions
require a broader perspective: when is a smaller, locally deployable model
"good enough"? This study offers an empirical answer by evaluating eleven
proprietary and open-weight LLMs across ten everyday occupational tasks,
including summarizing texts, generating schedules, and drafting emails and
proposals. Using a dual-LLM-based evaluation framework, we automated task
execution and standardized evaluation across ten criteria related to output
quality, factual accuracy, and ethical responsibility. Results show that GPT-4o
delivers consistently superior performance but at a significantly higher cost
and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4
achieved strong and reliable results on most tasks, suggesting their viability
in contexts requiring cost-efficiency, local deployment, or privacy. A cluster
analysis revealed three model groups -- premium all-rounders, competent
generalists, and limited but safe performers -- highlighting trade-offs between
quality, control, and sustainability. Significantly, task type influenced model
effectiveness: conceptual tasks challenged most models, while aggregation and
transformation tasks yielded better performances. We argue for a shift from
performance-maximizing benchmarks to task- and context-aware sufficiency
assessments that better reflect organizational priorities. Our approach
contributes a scalable method to evaluate AI models through a sustainability
lens and offers actionable guidance for responsible LLM deployment in practice.

</details>


### [65] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
*Tom Zehle, Moritz Schlager, Timo HeiÃŸ, Matthias Feurer*

Main category: cs.CL

TL;DR: CAPO is a cost-aware prompt optimization algorithm that improves efficiency by integrating AutoML techniques, outperforming state-of-the-art methods while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: LLMs' performance is highly sensitive to prompt formulation, and current prompt optimization methods are expensive due to high computational costs.

Method: CAPO uses an evolutionary approach with LLMs as operators, incorporating racing for evaluation savings and multi-objective optimization to balance performance and prompt length.

Result: CAPO outperforms state-of-the-art methods in 11/15 cases, improves efficiency, and reduces prompt length.

Conclusion: CAPO advances prompt optimization by enhancing cost-efficiency and robustness, making it more accessible.

Abstract: Large language models (LLMs) have revolutionized natural language processing
by solving a wide range of tasks simply guided by a prompt. Yet their
performance is highly sensitive to prompt formulation. While automated prompt
optimization addresses this challenge by finding optimal prompts, current
methods require a substantial number of LLM calls and input tokens, making
prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt
Optimization), an algorithm that enhances prompt optimization efficiency by
integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as
operators, incorporating racing to save evaluations and multi-objective
optimization to balance performance with prompt length. It jointly optimizes
instructions and few-shot examples while leveraging task descriptions for
improved robustness. Our extensive experiments across diverse datasets and LLMs
demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization
methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves
better performances already with smaller budgets, saves evaluations through
racing, and decreases average prompt length via a length penalty, making it
both cost-efficient and cost-aware. Even without few-shot examples, CAPO
outperforms its competitors and generally remains robust to initial prompts.
CAPO represents an important step toward making prompt optimization more
powerful and accessible by improving cost-efficiency.

</details>


### [66] [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
*Jingyu Zhang, Jiacan Yu, Marc Marone, Benjamin Van Durme, Daniel Khashabi*

Main category: cs.CL

TL;DR: BloomScrub is a simple, inference-time method for certified copyright takedown in LLMs, using quote detection and rewriting to reduce infringement risk while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Addressing overlooked worst-case copyright risks in LLMs, particularly long verbatim quotes from copyrighted sources, despite existing mitigation methods.

Method: Proposes BloomScrub, which interleaves quote detection with rewriting techniques, using Bloom filters for scalable screening and adaptive abstention for risk reduction.

Result: BloomScrub effectively reduces infringement risk, maintains utility, and adapts to enforcement stringency, proving lightweight inference-time methods can be effective.

Conclusion: Lightweight, inference-time approaches like BloomScrub are surprisingly effective for certified copyright prevention in LLMs.

Abstract: The exposure of large language models (LLMs) to copyrighted material during
pre-training raises concerns about unintentional copyright infringement post
deployment. This has driven the development of "copyright takedown" methods,
post-training approaches aimed at preventing models from generating content
substantially similar to copyrighted ones. While current mitigation approaches
are somewhat effective for average-case risks, we demonstrate that they
overlook worst-case copyright risks exhibits by the existence of long, verbatim
quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet
highly effective inference-time approach that provides certified copyright
takedown. Our method repeatedly interleaves quote detection with rewriting
techniques to transform potentially infringing segments. By leveraging
efficient data sketches (Bloom filters), our approach enables scalable
copyright screening even for large-scale real-world corpora. When quotes beyond
a length threshold cannot be removed, the system can abstain from responding,
offering certified risk reduction. Experimental results show that BloomScrub
reduces infringement risk, preserves utility, and accommodates different levels
of enforcement stringency with adaptive abstention. Our results suggest that
lightweight, inference-time methods can be surprisingly effective for copyright
prevention.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection](https://arxiv.org/abs/2504.16102)
*Xiwen Li, Ross Whitaker, Tolga Tasdizen*

Main category: cs.CV

TL;DR: AVIVDNetv2, a transformer-based model, improves idling vehicle detection by better aligning audio-visual modalities, outperforming baselines and setting a new benchmark.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning audio and visual modalities for accurate idling vehicle detection to reduce pollution and emissions.

Method: Proposes AVIVDNetv2 with a cross-modal transformer, multiscale visual feature fusion, and decoupled detection heads.

Result: Achieves mAP improvements of 7.66 and 9.42 over baselines, with consistent gains across vehicle categories.

Conclusion: AVIVDNetv2 sets a new benchmark for idling vehicle detection, demonstrating superior performance in cross-modal tasks.

Abstract: Idling vehicle detection (IVD) supports real-time systems that reduce
pollution and emissions by dynamically messaging drivers to curb excess idling
behavior. In computer vision, IVD has become an emerging task that leverages
video from surveillance cameras and audio from remote microphones to localize
and classify vehicles in each frame as moving, idling, or engine-off. As with
other cross-modal tasks, the key challenge lies in modeling the correspondence
between audio and visual modalities, which differ in representation but provide
complementary cues -- video offers spatial and motion context, while audio
conveys engine activity beyond the visual field. The previous end-to-end model,
which uses a basic attention mechanism, struggles to align these modalities
effectively, often missing vehicle detections. To address this issue, we
propose AVIVDNetv2, a transformer-based end-to-end detection network. It
incorporates a cross-modal transformer with global patch-level learning, a
multiscale visual feature fusion module, and decoupled detection heads.
Extensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the
disjoint baseline and 9.42 over the E2E baseline, with consistent AP gains
across all vehicle categories. Furthermore, AVIVDNetv2 outperforms the
state-of-the-art method for sounding object localization, establishing a new
performance benchmark on the AVIVD dataset.

</details>


### [68] [Shape Your Ground: Refining Road Surfaces Beyond Planar Representations](https://arxiv.org/abs/2504.16103)
*Oussema Dhaouadi, Johannes Meier, Jacques Kaiser, Daniel Cremers*

Main category: cs.CV

TL;DR: FlexRoad introduces a framework for high-quality road surface reconstruction using NURBS surfaces and anomaly correction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing road reconstruction methods often produce artifacts and inaccuracies, limiting usability. FlexRoad aims to improve smoothness, compactness, and accuracy.

Method: FlexRoad fits NURBS surfaces to 3D road points and uses the ECSRC algorithm for anomaly correction. It also introduces the GeRoD dataset for evaluation.

Result: FlexRoad outperforms common road surface representations in metrics and is robust to input sources, terrains, and noise.

Conclusion: FlexRoad is a generic, high-quality method for realistic road surface modeling, validated by ablation studies.

Abstract: Road surface reconstruction from aerial images is fundamental for autonomous
driving, urban planning, and virtual simulation, where smoothness, compactness,
and accuracy are critical quality factors. Existing reconstruction methods
often produce artifacts and inconsistencies that limit usability, while
downstream tasks have a tendency to represent roads as planes for simplicity
but at the cost of accuracy. We introduce FlexRoad, the first framework to
directly address road surface smoothing by fitting Non-Uniform Rational
B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric
reconstructions or geodata providers. Our method at its core utilizes the
Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust
anomaly correction, significantly reducing surface roughness and fitting
errors. To facilitate quantitative comparison between road surface
reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse
collection of road surface and terrain profiles derived from openly accessible
geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D
Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used
road surface representations across various metrics while being insensitive to
various input sources, terrains, and noise types. By performing ablation
studies, we identify the key role of each component towards high-quality
reconstruction performance, making FlexRoad a generic method for realistic road
surface modeling.

</details>


### [69] [Persistence-based Hough Transform for Line Detection](https://arxiv.org/abs/2504.16114)
*Johannes Ferner, Stefan Huber, Saverio Messineo, Angel Pop, Martin Uray*

Main category: cs.CV

TL;DR: The paper proposes a persistent homology-based voting technique for the Hough transform, improving robustness and performance over traditional thresholding methods.


<details>
  <summary>Details</summary>
Motivation: The Hough transform's susceptibility to noise and artifacts due to thresholding motivates the search for a more robust alternative.

Method: The authors introduce a voting technique using persistent homology to detect peaks in Hough space, addressing the limitations of thresholding.

Result: Experiments on synthetic data show the method outperforms the original Hough transform and enhances robustness.

Conclusion: The work advocates for broader use of Topological Data Analysis and encourages mathematically grounded improvements to the Hough transform.

Abstract: The Hough transform is a popular and classical technique in computer vision
for the detection of lines (or more general objects). It maps a pixel into a
dual space -- the Hough space: each pixel is mapped to the set of lines through
this pixel, which forms a curve in Hough space. The detection of lines then
becomes a voting process to find those lines that received many votes by
pixels. However, this voting is done by thresholding, which is susceptible to
noise and other artifacts.
  In this work, we present an alternative voting technique to detect peaks in
the Hough space based on persistent homology, which very naturally addresses
limitations of simple thresholding. Experiments on synthetic data show that our
method significantly outperforms the original method, while also demonstrating
enhanced robustness.
  This work seeks to inspire future research in two key directions. First, we
highlight the untapped potential of Topological Data Analysis techniques and
advocate for their broader integration into existing methods, including
well-established ones. Secondly, we initiate a discussion on the mathematical
stability of the Hough transform, encouraging exploration of mathematically
grounded improvements to enhance its robustness.

</details>


### [70] [Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes](https://arxiv.org/abs/2504.16117)
*Sridevi Polavaram, Xin Zhou, Meenu Ravi, Mohammad Zarei, Anmol Srivastava*

Main category: cs.CV

TL;DR: CAIRO is an ontology-based framework for detecting and formalizing rare failures in vision systems, leveraging human-in-the-loop for critical scenario testing.


<details>
  <summary>Details</summary>
Motivation: Vision systems in critical domains are vulnerable to rare or unforeseen scenarios, posing safety risks.

Method: CAIRO uses an ontology-based approach to detect and formalize failure cases (Critical Phenomena) in AI models, involving human-in-the-loop testing.

Result: The framework successfully analyzes object detection failures in automated driving systems, formalizing gaps as knowledge graphs for sharing and analysis.

Conclusion: CAIRO provides a scalable, interpretable method for addressing vision system vulnerabilities, enhancing accountability and safety.

Abstract: Vision systems are increasingly deployed in critical domains such as
surveillance, law enforcement, and transportation. However, their
vulnerabilities to rare or unforeseen scenarios pose significant safety risks.
To address these challenges, we introduce Context-Awareness and
Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive
discovery framework for failure cases (or CP - Critical Phenomena) detection
and formalization. CAIRO by design incentivizes human-in-the-loop for testing
and evaluation of criticality that arises from misdetections, adversarial
attacks, and hallucinations in AI black-box models. Our robust analysis of
object detection model(s) failures in automated driving systems (ADS) showcases
scalable and interpretable ways of formalizing the observed gaps between camera
perception and real-world contexts, resulting in test cases stored as explicit
knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis,
logical reasoning, and accountability.

</details>


### [71] [Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection](https://arxiv.org/abs/2504.16404)
*Md Fahimuzzman Sohan*

Main category: cs.CV

TL;DR: A deep learning model (3D CNN and ConvLSTM2D) detects cattle lameness from video data with high accuracy, simplifying traditional multi-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Cattle lameness causes pain and disrupts essential activities; automated detection can improve welfare and management.

Method: Used 50 videos of 40 cattle (half normal, half lame), applied data augmentation, and classified using 3D CNN and ConvLSTM2D models.

Result: 3D CNN achieved 90% accuracy, 90.9% precision/recall/f1-score; ConvLSTM2D had 85% accuracy.

Conclusion: Deep learning models, especially 3D CNN, effectively classify lameness, offering a streamlined alternative to traditional methods.

Abstract: Cattle lameness is often caused by hoof injuries or interdigital dermatitis,
leads to pain and significantly impacts essential physiological activities such
as walking, feeding, and drinking. This study presents a deep learning-based
model for detecting cattle lameness, sickness, or gait abnormalities using
publicly available video data. The dataset consists of 50 unique videos from 40
individual cattle, recorded from various angles in both indoor and outdoor
environments. Half of the dataset represents naturally walking
(normal/non-lame) cattle, while the other half consists of cattle exhibiting
gait abnormalities (lame). To enhance model robustness and generalizability,
data augmentation was applied to the training data. The pre-processed videos
were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A
comparative analysis of the results demonstrates strong classification
performance. Specifically, the 3D CNN model achieved a video-level
classification accuracy of 90%, with precision, recall, and f1-score of 90.9%,
90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower
accuracy of 85%. This study highlights the effectiveness of directly applying
classification models to learn spatiotemporal features from video data,
offering an alternative to traditional multi-stage approaches that typically
involve object detection, pose estimation, and feature extraction. Besides, the
findings demonstrate that the proposed deep learning models, particularly the
3D CNN, effectively classify and detect lameness in cattle while simplifying
the processing pipeline.

</details>


### [72] [MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation](https://arxiv.org/abs/2504.16127)
*Xingxing Zuo, Nikhil Ranganathan, Connor Lee, Georgia Gkioxari, Soon-Jo Chung*

Main category: cs.CV

TL;DR: A novel pipeline enhances monocular depth estimation (MDE) from thermal images using knowledge distillation from an RGB MDE model, improving accuracy by 22.88% in new scenarios.


<details>
  <summary>Details</summary>
Motivation: Thermal MDE is limited by scarce labeled data, unlike RGB MDE with vast datasets. The goal is to leverage RGB MDE's strengths to improve thermal MDE.

Method: A confidence-aware distillation method selectively transfers knowledge from RGB MDE to thermal MDE, using RGB's predicted confidence to guide the process.

Result: The method reduces absolute relative error by 22.88% in new scenarios without labeled depth data.

Conclusion: The pipeline effectively enhances thermal MDE's accuracy and generalizability, even without labeled supervision.

Abstract: Monocular depth estimation (MDE) from thermal images is a crucial technology
for robotic systems operating in challenging conditions such as fog, smoke, and
low light. The limited availability of labeled thermal data constrains the
generalization capabilities of thermal MDE models compared to foundational RGB
MDE models, which benefit from datasets of millions of images across diverse
scenarios. To address this challenge, we introduce a novel pipeline that
enhances thermal MDE through knowledge distillation from a versatile RGB MDE
model. Our approach features a confidence-aware distillation method that
utilizes the predicted confidence of the RGB MDE to selectively strengthen the
thermal MDE model, capitalizing on the strengths of the RGB model while
mitigating its weaknesses. Our method significantly improves the accuracy of
the thermal MDE, independent of the availability of labeled depth supervision,
and greatly expands its applicability to new scenarios. In our experiments on
new scenarios without labeled depth, the proposed confidence-aware distillation
method reduces the absolute relative error of thermal MDE by 22.88\% compared
to the baseline without distillation.

</details>


### [73] [Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT](https://arxiv.org/abs/2504.16128)
*Stanley Mugisha, Rashid Kisitu, Florence Tushabe*

Main category: cs.CV

TL;DR: A hybrid knowledge distillation framework transfers ViT-level accuracy to lightweight models for efficient plant disease detection on edge devices.


<details>
  <summary>Details</summary>
Motivation: Balancing high accuracy of Vision Transformers (ViTs) with efficiency demands of resource-constrained edge devices in agricultural IoT systems.

Method: Proposes a hybrid knowledge distillation framework transferring logit and attention knowledge from Swin Transformer to MobileNetV3, with adaptive attention alignment and dual-loss optimization.

Result: Distilled MobileNetV3 achieves 92.4% accuracy (vs. 95.9% for Swin-L) with 95% reduction in computational cost and 82% lower latency on IoT devices.

Conclusion: Enables real-time, energy-efficient crop monitoring with ViT-level precision on edge devices, advancing precision agriculture.

Abstract: Integrating deep learning applications into agricultural IoT systems faces a
serious challenge of balancing the high accuracy of Vision Transformers (ViTs)
with the efficiency demands of resource-constrained edge devices. Large
transformer models like the Swin Transformers excel in plant disease
classification by capturing global-local dependencies. However, their
computational complexity (34.1 GFLOPs) limits applications and renders them
impractical for real-time on-device inference. Lightweight models such as
MobileNetV3 and TinyML would be suitable for on-device inference but lack the
required spatial reasoning for fine-grained disease detection. To bridge this
gap, we propose a hybrid knowledge distillation framework that synergistically
transfers logit and attention knowledge from a Swin Transformer teacher to a
MobileNetV3 student model. Our method includes the introduction of adaptive
attention alignment to resolve cross-architecture mismatch (resolution,
channels) and a dual-loss function optimizing both class probabilities and
spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled
MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95%
reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU
and 86ms/image on smartphone CPUs). Key innovations include IoT-centric
validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching
attention maps. Comparative experiments show significant improvements over
standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over
MobileNetV3 baselines. Significantly, this work advances real-time,
energy-efficient crop monitoring in precision agriculture and demonstrates how
we can attain ViT-level diagnostic precision on edge devices. Code and models
will be made available for replication after acceptance.

</details>


### [74] [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
*Mohammad Abu Tami, Mohammed Elhenawy, Huthaifa I. Ashqar*

Main category: cs.CV

TL;DR: The paper explores how Multimodal Large Language Models (MLLMs) can enhance traffic safety by integrating diverse data inputs for better scene understanding, decision-making, and robustness against adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional ADAS systems face limitations in dynamic scenarios due to fragmented sensor processing and vulnerability to adversarial conditions. MLLMs offer a promising solution by unifying cross-modal data.

Method: The paper reviews MLLM-based approaches, analyzing their integration of visual, spatial, and environmental data for holistic scene understanding. Key datasets like KITTI and DRAMA are examined.

Result: MLLMs demonstrate improved perception, decision-making, and adversarial robustness, positioning them as transformative tools for traffic safety.

Conclusion: MLLMs hold significant potential to revolutionize traffic safety by enabling scalable, context-aware solutions. Future directions include real-time edge deployment and human-AI collaboration.

Abstract: Traffic safety remains a critical global challenge, with traditional Advanced
Driver-Assistance Systems (ADAS) often struggling in dynamic real-world
scenarios due to fragmented sensor processing and susceptibility to adversarial
conditions. This paper reviews the transformative potential of Multimodal Large
Language Models (MLLMs) in addressing these limitations by integrating
cross-modal data such as visual, spatial, and environmental inputs to enable
holistic scene understanding. Through a comprehensive analysis of MLLM-based
approaches, we highlight their capabilities in enhancing perception,
decision-making, and adversarial robustness, while also examining the role of
key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research.
Furthermore, we outline future directions, including real-time edge deployment,
causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as
a cornerstone for next-generation traffic safety systems, this review
underscores their potential to revolutionize the field, offering scalable,
context-aware solutions that proactively mitigate risks and improve overall
road safety.

</details>


### [75] [TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance](https://arxiv.org/abs/2504.16505)
*Meng Chu, Yukang Chen, Haokun Gui, Shaozuo Yu, Yi Wang, Jiaya Jia*

Main category: cs.CV

TL;DR: TraveLLaMA is a specialized multimodal AI model for urban travel assistance, outperforming general-purpose models with a 6.5%-9.4% improvement in travel-specific tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems lack specialized knowledge for urban travel planning, necessitating a tailored solution.

Method: Developed using a novel 220k QA dataset (130k text QA pairs from travel forums and 90k vision-language QA pairs) and fine-tuned on state-of-the-art vision-language models.

Result: TraveLLaMA shows significant improvements in travel understanding and visual question answering, excelling in contextual recommendations and map interpretation.

Conclusion: TraveLLaMA sets a new benchmark for multimodal travel assistance, demonstrating superior performance in travel-specific tasks.

Abstract: Tourism and travel planning increasingly rely on digital assistance, yet
existing multimodal AI systems often lack specialized knowledge and contextual
understanding of urban environments. We present TraveLLaMA, a specialized
multimodal language model designed for urban scene understanding and travel
assistance. Our work addresses the fundamental challenge of developing
practical AI travel assistants through a novel large-scale dataset of 220k
question-answer pairs. This comprehensive dataset uniquely combines 130k text
QA pairs meticulously curated from authentic travel forums with GPT-enhanced
responses, alongside 90k vision-language QA pairs specifically focused on map
understanding and scene comprehension. Through extensive fine-tuning
experiments on state-of-the-art vision-language models (LLaVA, Qwen-VL,
Shikra), we demonstrate significant performance improvements ranging from
6.5\%-9.4\% in both pure text travel understanding and visual question
answering tasks. Our model exhibits exceptional capabilities in providing
contextual travel recommendations, interpreting map locations, and
understanding place-specific imagery while offering practical information such
as operating hours and visitor reviews. Comparative evaluations show TraveLLaMA
significantly outperforms general-purpose models in travel-specific tasks,
establishing a new benchmark for multi-modal travel assistance systems.

</details>


### [76] [Progressive Language-guided Visual Learning for Multi-Task Visual Grounding](https://arxiv.org/abs/2504.16145)
*Jingchao Wang, Hong Wang, Wenlong Zhang, Kunhua Ji, Dingjiang Huang, Yefeng Zheng*

Main category: cs.CV

TL;DR: The paper proposes PLVL, a Progressive Language-guided Visual Learning framework for multi-task visual grounding, addressing limitations in existing methods by integrating language into visual feature extraction and exploiting task relationships for collaborative prediction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-task visual grounding (MTVG) have limitations: incomplete linguistic injection into visual features and ineffective exploitation of task relationships.

Method: PLVL progressively injects language information into visual feature learning and uses a multi-task head for collaborative prediction of REC and RES tasks.

Result: PLVL outperforms existing methods on benchmark datasets for both REC and RES tasks.

Conclusion: PLVL effectively addresses the limitations of current MTVG approaches by integrating language guidance and collaborative task prediction, achieving superior performance.

Abstract: Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring
Expression Comprehension (REC) and Referring Expression Segmentation (RES). The
existing representative approaches generally follow the research pipeline which
mainly consists of three core procedures, including independent feature
extraction for visual and linguistic modalities, respectively, cross-modal
interaction module, and independent prediction heads for different sub-tasks.
Albeit achieving remarkable performance, this research line has two
limitations: 1) The linguistic content has not been fully injected into the
entire visual backbone for boosting more effective visual feature extraction
and it needs an extra cross-modal interaction module; 2) The relationship
between REC and RES tasks is not effectively exploited to help the
collaborative prediction for more accurate output. To deal with these problems,
in this paper, we propose a Progressive Language-guided Visual Learning
framework for multi-task visual grounding, called PLVL, which not only finely
mine the inherent feature expression of the visual modality itself but also
progressively inject the language information to help learn linguistic-related
visual features. In this manner, our PLVL does not need additional cross-modal
fusion module while fully introducing the language guidance. Furthermore, we
analyze that the localization center for REC would help identify the
to-be-segmented object region for RES to some extent. Inspired by this
investigation, we design a multi-task head to accomplish collaborative
predictions for these two sub-tasks. Extensive experiments conducted on several
benchmark datasets comprehensively substantiate that our PLVL obviously
outperforms the representative methods in both REC and RES tasks.
https://github.com/jcwang0602/PLVL

</details>


### [77] [Classification of Firn Data via Topological Features](https://arxiv.org/abs/2504.16150)
*Sarah Day, Jesse Dimino, Matt Jester, Kaitlin Keegan, Thomas Weighill*

Main category: cs.CV

TL;DR: The paper evaluates topological features for classifying firn image data, highlighting trade-offs between accuracy, interpretability, and generalizability.


<details>
  <summary>Details</summary>
Motivation: To understand the advantages and limitations of topological featurization in analyzing firn layers, which exhibit distinct topological and geometric structures with depth.

Method: Uses sublevel set features, distance transform features, and persistence curves to predict sample depth from microCT images.

Result: No single method outperforms others in all scenarios, revealing trade-offs between accuracy, interpretability, and generalizability.

Conclusion: Topological features are useful but require careful consideration of trade-offs for robust classification of firn data.

Abstract: In this paper we evaluate the performance of topological features for
generalizable and robust classification of firn image data, with the broader
goal of understanding the advantages, pitfalls, and trade-offs in topological
featurization. Firn refers to layers of granular snow within glaciers that
haven't been compressed into ice. This compactification process imposes
distinct topological and geometric structure on firn that varies with depth
within the firn column, making topological data analysis (TDA) a natural choice
for understanding the connection between depth and structure. We use two
classes of topological features, sublevel set features and distance transform
features, together with persistence curves, to predict sample depth from
microCT images. A range of challenging training-test scenarios reveals that no
one choice of method dominates in all categories, and uncoveres a web of
trade-offs between accuracy, interpretability, and generalizability.

</details>


### [78] [A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images](https://arxiv.org/abs/2504.16171)
*Zezhang Yang, Zitong Yu, Nuri Choi, Abhinav K. Jha*

Main category: cs.CV

TL;DR: A deep-learning method is proposed to improve sparse-view MPI SPECT imaging, reducing scan time while maintaining defect-detection accuracy and image quality.


<details>
  <summary>Details</summary>
Motivation: Lengthy SPECT scans cause discomfort and artifacts; reducing projection angles shortens time but degrades image quality.

Method: Detection-task-specific deep learning with an observer loss term to preserve anthropomorphic features for defect detection.

Result: The method achieved higher AUC for defect detection and restored left ventricle structure, outperforming sparse-view protocols.

Conclusion: Preliminary results show promise, warranting further evaluation of the method.

Abstract: Myocardial perfusion imaging (MPI) with single-photon emission computed
tomography (SPECT) is a widely used and cost-effective diagnostic tool for
coronary artery disease. However, the lengthy scanning time in this imaging
procedure can cause patient discomfort, motion artifacts, and potentially
inaccurate diagnoses due to misalignment between the SPECT scans and the
CT-scans which are acquired for attenuation compensation. Reducing projection
angles is a potential way to shorten scanning time, but this can adversely
impact the quality of the reconstructed images. To address this issue, we
propose a detection-task-specific deep-learning method for sparse-view MPI
SPECT images. This method integrates an observer loss term that penalizes the
loss of anthropomorphic channel features with the goal of improving performance
in perfusion defect-detection task. We observed that, on the task of detecting
myocardial perfusion defects, the proposed method yielded an area under the
receiver operating characteristic (ROC) curve (AUC) significantly larger than
the sparse-view protocol. Further, the proposed method was observed to be able
to restore the structure of the left ventricle wall, demonstrating ability to
overcome sparse-sampling artifacts. Our preliminary results motivate further
evaluations of the method.

</details>


### [79] [CLIP-IT: CLIP-based Pairing for Histology Images Classification](https://arxiv.org/abs/2504.16181)
*Banafsheh Karimian, Giulia Avanzato, Soufian Belharbi, Luke McCaffrey, Mohammadhadi Shateri, Eric Granger*

Main category: cs.CV

TL;DR: CLIP-IT method trains a vision backbone model for histology image classification using privileged text from external sources, avoiding the need for large paired datasets.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning improves medical image analysis but faces challenges like privacy concerns and high costs for paired datasets.

Method: CLIP-IT pairs histology images with external text using CLIP, then distills text knowledge to an image classifier via parameter-efficient fine-tuning.

Result: Outperforms unimodal classifiers on PCAM, CRC, and BACH datasets with minimal added computational cost.

Conclusion: CLIP-IT offers a cost-effective way to leverage text data for histology image analysis without needing paired datasets during inference.

Abstract: Multimodal learning has shown significant promise for improving medical image
analysis by integrating information from complementary data sources. This is
widely employed for training vision-language models (VLMs) for cancer detection
based on histology images and text reports. However, one of the main
limitations in training these VLMs is the requirement for large paired
datasets, raising concerns over privacy, and data collection, annotation, and
maintenance costs. To address this challenge, we introduce CLIP-IT method to
train a vision backbone model to classify histology images by pairing them with
privileged textual information from an external source. At first, the modality
pairing step relies on a CLIP-based model to match histology images with
semantically relevant textual report data from external sources, creating an
augmented multimodal dataset without the need for manually paired samples.
Then, we propose a multimodal training procedure that distills the knowledge
from the paired text modality to the unimodal image classifier for enhanced
performance without the need for the textual data during inference. A
parameter-efficient fine-tuning method is used to efficiently address the
misalignment between the main (image) and paired (text) modalities. During
inference, the improved unimodal histology classifier is used, with only
minimal additional computational complexity. Our experiments on challenging
PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a
cost-effective approach to leverage privileged textual information and
outperform unimodal classifiers for histology.

</details>


### [80] [DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector](https://arxiv.org/abs/2504.16242)
*Henry Marichal, VerÃ³nica Casaravilla, Candice Power, Karolain Mello, JoaquÃ­n Mazarino, Christine Lucas, Ludmila Profumo, Diego Passarella, Gregory Randall*

Main category: cs.CV

TL;DR: Deep CS-TRD is a deep-learning-based algorithm for detecting tree rings in cross-sections, outperforming state-of-the-art methods in macro images but slightly underperforming in microscopy images.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve tree ring detection by replacing traditional edge detection with a deep-learning approach, applicable across diverse image domains and species.

Method: The method uses a U-Net architecture for deep learning, tested on images from microscopy, scanners, and smartphones, and validated on three species.

Result: The algorithm excels in macro images (Pinus taeda, Gleditsia triacanthos) but performs slightly worse in microscopy images (Salix glauca).

Conclusion: Deep CS-TRD is a versatile and effective tool for tree ring detection, with publicly available datasets and code to support further research.

Abstract: Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree
rings in whole cross-sections. It substitutes the edge detection step of CS-TRD
by a deep-learning-based approach (U-Net), which allows the application of the
method to different image domains: microscopy, scanner or smartphone acquired,
and species (Pinus taeda, Gleditsia triachantos and Salix glauca).
Additionally, we introduce two publicly available datasets of annotated images
to the community. The proposed method outperforms state-of-the-art approaches
in macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly
lower performance in microscopy images of Salix glauca. To our knowledge, this
is the first paper that studies automatic tree ring detection for such
different species and acquisition conditions. The dataset and source code are
available in https://github.com/hmarichal93/deepcstrd

</details>


### [81] [Naturally Computed Scale Invariance in the Residual Stream of ResNet18](https://arxiv.org/abs/2504.16290)
*AndrÃ© Longon*

Main category: cs.CV

TL;DR: The paper explores how ResNet18 achieves scale invariance in object recognition, focusing on its residual stream, unlike InceptionV1. It identifies scale-invariant properties in convolutional channels and links them to recognition behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding how neural networks achieve invariance to image-altering variables (e.g., scale) is crucial for improving object recognition. Prior work on InceptionV1 was limited, leaving other architectures like ResNet18 unexplored.

Method: The study investigates ResNet18's residual stream, analyzing convolutional channels for scale-invariant properties. Ablation experiments are used to causally link these properties to scale-robust recognition.

Result: Intermediate blocks in ResNet18 exhibit scale invariance through residual summation of scale-equivariant representations. Ablation experiments tentatively link these properties to recognition behavior.

Conclusion: The residual stream in ResNet18 likely computes scale invariance, contributing to robust object recognition. Further research is needed to confirm these findings.

Abstract: An important capacity in visual object recognition is invariance to
image-altering variables which leave the identity of objects unchanged, such as
lighting, rotation, and scale. How do neural networks achieve this? Prior
mechanistic interpretability research has illuminated some invariance-building
circuitry in InceptionV1, but the results are limited and networks with
different architectures have remained largely unexplored. This work
investigates ResNet18 with a particular focus on its residual stream, an
architectural component which InceptionV1 lacks. We observe that many
convolutional channels in intermediate blocks exhibit scale invariant
properties, computed by the element-wise residual summation of scale
equivariant representations: the block input's smaller-scale copy with the
block pre-sum output's larger-scale copy. Through subsequent ablation
experiments, we attempt to causally link these neural properties with
scale-robust object recognition behavior. Our tentative findings suggest how
the residual stream computes scale invariance and its possible role in
behavior. Code is available at:
https://github.com/cest-andre/residual-stream-interp

</details>


### [82] [MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers](https://arxiv.org/abs/2504.16304)
*Wonjeong Jo, Magdalena Wojcieszak*

Main category: cs.CV

TL;DR: The paper introduces two datasets for measuring online harm on short video platforms, including annotated videos labeled by experts, GPT-4-Turbo, and crowdworkers.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive understanding and measurement of online harm on platforms like YouTube, TikTok, and Instagram.

Method: Creation of two datasets: (1) 60,906 potentially harmful YouTube videos and (2) 19,422 videos annotated by experts, GPT-4-Turbo, and crowdworkers for binary and multi-label harm classification.

Result: Datasets include ground truth annotations and subsets labeled by individual actors, covering six harm categories.

Conclusion: These datasets aim to support future research on online harm, improve classification, and aid in mitigating harmful content.

Abstract: Short video platforms, such as YouTube, Instagram, or TikTok, are used by
billions of users. These platforms expose users to harmful content, ranging
from clickbait or physical harms to hate or misinformation. Yet, we lack a
comprehensive understanding and measurement of online harm on short video
platforms. Toward this end, we present two large-scale datasets of multi-modal
and multi-categorical online harm: (1) 60,906 systematically selected
potentially harmful YouTube videos and (2) 19,422 videos annotated by three
labeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1
thumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master
workers). The annotated dataset includes both (a) binary classification
(harmful vs. harmless) and (b) multi-label categorizations of six harm
categories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and
Physical harms. Furthermore, the annotated dataset provides (1) ground truth
data with videos annotated consistently across (a) all three actors and (b) the
majority of the labeling actors, and (2) three data subsets labeled by
individual actors. These datasets are expected to facilitate future work on
online harm, aid in (multi-modal) classification efforts, and advance the
identification and potential mitigation of harmful content on video platforms.

</details>


### [83] [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
*Sen Fang, Chunyu Sui, Hongwei Yi, Carol Neidle, Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: SignX is a foundation model framework for sign recognition, using a 2-stage approach (Pose2Gloss and Video2Pose) to improve accuracy in translating sign language videos to glosses.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized glossing conventions for ASL signs and the complexity of sign language data processing necessitate a robust framework for accurate sign recognition.

Method: SignX combines a Pose2Gloss component (inverse diffusion model with multi-track pose fusion) and a Video2Pose module (ViT-based) to convert videos into pose representations and then to glosses.

Result: SignX outperforms prior work in accuracy for predicting gloss representations from sign language videos.

Conclusion: SignX provides a scalable and accurate framework for sign language recognition, compatible with existing pose formats.

Abstract: The complexity of sign language data processing brings many challenges. The
current approach to recognition of ASL signs aims to translate RGB sign
language videos through pose information into English-based ID glosses, which
serve to uniquely identify ASL signs. Note that there is no shared convention
for assigning such glosses to ASL signs, so it is essential that the same
glossing conventions are used for all of the data in the datasets that are
employed. This paper proposes SignX, a foundation model framework for sign
recognition. It is a concise yet powerful framework applicable to multiple
human activity recognition scenarios. First, we developed a Pose2Gloss
component based on an inverse diffusion model, which contains a multi-track
pose fusion layer that unifies five of the most powerful pose information
sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens
Segmentation--into a single latent pose representation. Second, we trained a
Video2Pose module based on ViT that can directly convert raw video into signer
pose representation. Through this 2-stage training framework, we enable sign
language recognition models to be compatible with existing pose formats, laying
the foundation for the common pose estimation necessary for sign recognition.
Experimental results show that SignX can recognize signs from sign language
video, producing predicted gloss representations with greater accuracy than has
been reported in prior work.

</details>


### [84] [Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization](https://arxiv.org/abs/2504.16362)
*Colton R. Crum, Adam Czajka*

Main category: cs.CV

TL;DR: The paper proposes a loss component to regularize the first convolutional layer's kernels for near-orthogonality, improving model generalization without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Improving model generalization by mimicking human perceptual intelligence, particularly early vision processes in neuroscience.

Method: Introduces a flexible loss component to enforce near-orthogonality in filtering kernels of the first convolutional layer, allowing the network to choose which pairs to orthogonalize.

Result: Significant gains in generalization performance across three architectures (ResNet-50, DenseNet-121, ViT-b-16) and two challenging tasks: iris presentation attack detection and chest X-ray anomaly detection.

Conclusion: The proposed method outperforms previous orthogonalization- and saliency-based regularization techniques, demonstrating its effectiveness in enhancing generalization.

Abstract: An ongoing research challenge within several domains in computer vision is
how to increase model generalization capabilities. Several attempts to improve
model generalization performance are heavily inspired by human perceptual
intelligence, which is remarkable in both its performance and efficiency to
generalize to unknown samples. Many of these methods attempt to force portions
of the network to be orthogonal, following some observation within neuroscience
related to early vision processes. In this paper, we propose a loss component
that regularizes the filtering kernels in the first convolutional layer of a
network to make them nearly orthogonal. Deviating from previous works, we give
the network flexibility in which pairs of kernels it makes orthogonal, allowing
the network to navigate to a better solution space, imposing harsh penalties.
Without architectural modifications, we report substantial gains in
generalization performance using the proposed loss against previous works
(including orthogonalization- and saliency-based regularization methods) across
three different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two
difficult open-set recognition tasks: presentation attack detection in iris
biometrics, and anomaly detection in chest X-ray images.

</details>


### [85] [CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning](https://arxiv.org/abs/2504.16364)
*Fengchun Liu, Tong Zhang, Chunying Zhang*

Main category: cs.CV

TL;DR: The paper introduces CLPSTNet, a CNN-based steganography method using progressive multi-scale modules for improved invisibility and security in image steganography.


<details>
  <summary>Details</summary>
Motivation: Traditional steganography methods rely on hand-crafted features, but CNNs can autonomously learn embedding. However, issues of invisibility and security persist due to image complexity.

Method: CLPSTNet uses progressive multi-scale convolutional modules with Inception structures and dilated convolutions, refining features from shallow to deep layers.

Result: CLPSTNet achieves high PSNR, SSIM, and decoding accuracy on ALASKA2, VOC2012, and ImageNet datasets, with low steganalysis scores.

Conclusion: CLPSTNet effectively addresses invisibility and security challenges in CNN-based steganography, outperforming traditional methods.

Abstract: In recent years, a large number of works have introduced Convolutional Neural
Networks (CNNs) into image steganography, which transform traditional
steganography methods such as hand-crafted features and prior knowledge design
into steganography methods that neural networks autonomically learn information
embedding. However, due to the inherent complexity of digital images, issues of
invisibility and security persist when using CNN models for information
embedding. In this paper, we propose Curriculum Learning Progressive Steganophy
Network (CLPSTNet). The network consists of multiple progressive multi-scale
convolutional modules that integrate Inception structures and dilated
convolutions. The module contains multiple branching pathways, starting from a
smaller convolutional kernel and dilatation rate, extracting the basic, local
feature information from the feature map, and gradually expanding to the
convolution with a larger convolutional kernel and dilatation rate for
perceiving the feature information of a larger receptive field, so as to
realize the multi-scale feature extraction from shallow to deep, and from fine
to coarse, allowing the shallow secret information features to be refined in
different fusion stages. The experimental results show that the proposed
CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three
large public datasets, ALASKA2, VOC2012 and ImageNet, but also the
steganographic images generated by CLPSTNet have low steganalysis scores.You
can find our code at
\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.

</details>


### [86] [Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection](https://arxiv.org/abs/2504.16368)
*Linhua Kong, Dongxia Chang, Lian Liu, Zisen Kong, Pengyuan Li, Yao Zhao*

Main category: cs.CV

TL;DR: RCAlign, a new alignment model for radar-camera fusion in 3D object detection, addresses feature misalignment and sparsity issues with a Dual-Route Alignment module and Radar Feature Enhancement, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for radar-camera fusion in 3D object detection neglect inter-modal feature interaction or fail to align features spatially, limiting performance.

Method: Proposes RCAlign with a Dual-Route Alignment module for feature alignment and fusion, and a Radar Feature Enhancement module to densify radar BEV features using knowledge distillation.

Result: RCAlign sets a new state-of-the-art on the nuScenes benchmark, with significant performance gains (4.3% NDS and 8.4% mAP) over RCBEVDet.

Conclusion: RCAlign effectively addresses feature alignment and sparsity in radar-camera fusion, improving 3D object detection performance.

Abstract: Recently, 3D object detection algorithms based on radar and camera fusion
have shown excellent performance, setting the stage for their application in
autonomous driving perception tasks. Existing methods have focused on dealing
with feature misalignment caused by the domain gap between radar and camera.
However, existing methods either neglect inter-modal features interaction
during alignment or fail to effectively align features at the same spatial
location across modalities. To alleviate the above problems, we propose a new
alignment model called Radar Camera Alignment (RCAlign). Specifically, we
design a Dual-Route Alignment (DRA) module based on contrastive learning to
align and fuse the features between radar and camera. Moreover, considering the
sparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is
proposed to improve the densification of radar BEV features with the knowledge
distillation loss. Experiments show RCAlign achieves a new state-of-the-art on
the public nuScenes benchmark in radar camera fusion for 3D Object Detection.
Furthermore, the RCAlign achieves a significant performance gain (4.3\% NDS and
8.4\% mAP) in real-time 3D detection compared to the latest state-of-the-art
method (RCBEVDet).

</details>


### [87] [SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields](https://arxiv.org/abs/2504.16389)
*Yuanjian Wang, Yufei Deng, Rong Xiao, Jiahao Fan, Chenwei Tang, Deng Xiong, Jiancheng Lv*

Main category: cs.CV

TL;DR: SaENeRF is a self-supervised framework for reconstructing high-quality 3D scenes from event camera data, addressing artifacts and noise issues in existing methods.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages like low latency and high dynamic range, but reconstructing accurate 3D representations from their data is challenging due to noise and early-stage learning artifacts.

Method: SaENeRF normalizes radiance variations using event polarities and employs regularization losses to suppress artifacts and enhance visual fidelity.

Result: The method significantly reduces artifacts and achieves superior reconstruction quality compared to existing techniques.

Conclusion: SaENeRF provides a robust solution for event-based 3D scene reconstruction, improving both consistency and photorealism.

Abstract: Event cameras are neuromorphic vision sensors that asynchronously capture
changes in logarithmic brightness changes, offering significant advantages such
as low latency, low power consumption, low bandwidth, and high dynamic range.
While these characteristics make them ideal for high-speed scenarios,
reconstructing geometrically consistent and photometrically accurate 3D
representations from event data remains fundamentally challenging. Current
event-based Neural Radiance Fields (NeRF) methods partially address these
challenges but suffer from persistent artifacts caused by aggressive network
learning in early stages and the inherent noise of event cameras. To overcome
these limitations, we present SaENeRF, a novel self-supervised framework that
effectively suppresses artifacts and enables 3D-consistent, dense, and
photorealistic NeRF reconstruction of static scenes solely from event streams.
Our approach normalizes predicted radiance variations based on accumulated
event polarities, facilitating progressive and rapid learning for scene
representation construction. Additionally, we introduce regularization losses
specifically designed to suppress artifacts in regions where photometric
changes fall below the event threshold and simultaneously enhance the light
intensity difference of non-zero events, thereby improving the visual fidelity
of the reconstructed scene. Extensive qualitative and quantitative experiments
demonstrate that our method significantly reduces artifacts and achieves
superior reconstruction quality compared to existing methods. The code is
available at https://github.com/Mr-firework/SaENeRF.

</details>


### [88] [PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels](https://arxiv.org/abs/2504.16419)
*Qi Yang, Weichen Bi, Haiyang Shen, Yaoqi Guo, Yun Ma*

Main category: cs.CV

TL;DR: PixelWeb is a large-scale GUI dataset with over 100,000 annotated web pages, addressing inaccuracies in existing datasets through a novel automatic annotation method combining visual and DOM analysis. It outperforms others in GUI element detection tasks.


<details>
  <summary>Details</summary>
Motivation: Existing GUI datasets suffer from inaccurate BBox annotations and lack comprehensive metadata, limiting model performance and downstream task development.

Method: PixelWeb uses channel derivation (BGRA four-channel bitmap) for precise element localization and layer analysis (DOM-based) for visibility and stacking order. It includes metadata like images, contours, and masks.

Result: PixelWeb achieves 3-7 times better performance on mAP95 in GUI element detection compared to existing datasets.

Conclusion: PixelWeb improves annotation accuracy and supports downstream tasks like GUI generation and automated interaction, demonstrating significant potential.

Abstract: Graphical User Interface (GUI) datasets are crucial for various downstream
tasks. However, GUI datasets often generate annotation information through
automatic labeling, which commonly results in inaccurate GUI element BBox
annotations, including missing, duplicate, or meaningless BBoxes. These issues
can degrade the performance of models trained on these datasets, limiting their
effectiveness in real-world applications. Additionally, existing GUI datasets
only provide BBox annotations visually, which restricts the development of
visually related GUI downstream tasks. To address these issues, we introduce
PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web
pages. PixelWeb is constructed using a novel automatic annotation approach that
integrates visual feature extraction and Document Object Model (DOM) structure
analysis through two core modules: channel derivation and layer analysis.
Channel derivation ensures accurate localization of GUI elements in cases of
occlusion and overlapping elements by extracting BGRA four-channel bitmap
annotations. Layer analysis uses the DOM to determine the visibility and
stacking order of elements, providing precise BBox annotations. Additionally,
PixelWeb includes comprehensive metadata such as element images, contours, and
mask annotations. Manual verification by three independent annotators confirms
the high quality and accuracy of PixelWeb annotations. Experimental results on
GUI element detection tasks show that PixelWeb achieves performance on the
mAP95 metric that is 3-7 times better than existing datasets. We believe that
PixelWeb has great potential for performance improvement in downstream tasks
such as GUI generation and automated user interaction.

</details>


### [89] [FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing](https://arxiv.org/abs/2504.16433)
*Hariseetharam Gunduboina, Muhammad Haris Khan, Biplab Banerjee*

Main category: cs.CV

TL;DR: FrogDogNet improves domain generalization in remote sensing by integrating Fourier frequency filtering and self-attention to reduce noise and retain invariant features.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs like CLIP rely on full-image features, introducing noise and misclassification in remote sensing. FrogDogNet addresses this by focusing on invariant low-frequency components.

Method: FrogDogNet uses projection, self-attention, and Fourier frequency filtering to extract and retain essential structural information for robust prompt learning.

Result: Experiments on four RS datasets show FrogDogNet outperforms state-of-the-art methods in domain generalization tasks.

Conclusion: Frequency-based invariant feature retention enhances generalization, making FrogDogNet adaptable across domain shifts.

Abstract: In recent years, large-scale vision-language models (VLMs) like CLIP have
gained attention for their zero-shot inference using instructional text
prompts. While these models excel in general computer vision, their potential
for domain generalization in remote sensing (RS) remains underexplored.
Existing approaches enhance prompt learning by generating visual prompt tokens
but rely on full-image features, introducing noise and background artifacts
that vary within a class, causing misclassification. To address this, we
propose FrogDogNet, a novel prompt learning framework integrating Fourier
frequency filtering and self-attention to improve RS scene classification and
domain generalization. FrogDogNet selectively retains invariant low-frequency
components while eliminating noise and irrelevant backgrounds, ensuring robust
feature representation across domains. The model first extracts significant
features via projection and self-attention, then applies frequency-based
filtering to preserve essential structural information for prompt learning.
Extensive experiments on four RS datasets and three domain generalization tasks
show that FrogDogNet consistently outperforms state-of-the-art prompt learning
methods, demonstrating superior adaptability across domain shifts. Our findings
highlight the effectiveness of frequency-based invariant feature retention in
generalization, paving the way for broader applications. Our code is available
at https://github.com/HariseetharamG/FrogDogNet

</details>


### [90] [Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes](https://arxiv.org/abs/2504.16443)
*Duy-Tho Le, Trung Pham, Jianfei Cai, Hamid Rezatofighi*

Main category: cs.CV

TL;DR: The paper introduces Marginalized Generalized IoU (MGIoU) and its extensions (MGIoU+, MGIoU-) to unify parametric shape optimization, addressing limitations of existing methods like instability, inefficiency, and lack of generalizability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing parametric shapes suffer from instability, inefficiency, and domain-specific limitations, leading to fragmented solutions. The goal is to unify and improve these methods.

Method: The authors propose MGIoU, a loss function that projects structured convex shapes onto their normals for efficient, differentiable IoU approximation. MGIoU+ extends this to unstructured shapes, and MGIoU- minimizes overlaps.

Result: Experiments show MGIoU and MGIoU+ outperform existing losses, reduce computation latency by 10-40x, and satisfy metric properties and scale-invariance.

Conclusion: MGIoU and its variants provide a unified, efficient, and robust solution for parametric shape optimization across diverse applications.

Abstract: Optimizing the similarity between parametric shapes is crucial for numerous
computer vision tasks, where Intersection over Union (IoU) stands as the
canonical measure. However, existing optimization methods exhibit significant
shortcomings: regression-based losses like L1/L2 lack correlation with IoU,
IoU-based losses are unstable and limited to simple shapes, and task-specific
methods are computationally intensive and not generalizable accross domains. As
a result, the current landscape of parametric shape objective functions has
become scattered, with each domain proposing distinct IoU approximations. To
address this, we unify the parametric shape optimization objective functions by
introducing Marginalized Generalized IoU (MGIoU), a novel loss function that
overcomes these challenges by projecting structured convex shapes onto their
unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a
simple, efficient, fully differentiable approximation strongly correlated with
IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured
convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization
across diverse applications. Experiments on standard benchmarks demonstrate
that MGIoU and MGIoU+ consistently outperform existing losses while reducing
loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy
metric properties and scale-invariance, ensuring robustness as an objective
function. We further propose MGIoU- for minimizing overlaps in tasks like
collision-free trajectory prediction. Code is available at
https://ldtho.github.io/MGIoU

</details>


### [91] [Cross Paradigm Representation and Alignment Transformer for Image Deraining](https://arxiv.org/abs/2504.16455)
*Shun Zou, Yi Zou, Juncheng Li, Guangwei Gao, Guojun Qi*

Main category: cs.CV

TL;DR: The paper proposes CPRAformer, a Transformer-based model for image deraining, integrating global-local and spatial-channel representations via hierarchical alignment and two self-attention mechanisms (SPC-SA and SPR-SA). It introduces AAFM for feature alignment, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing single-paradigm architectures struggle with irregular rain patterns and complex overlaps, necessitating a unified framework for complementary representations.

Method: CPRAformer uses SPC-SA for global channel dependencies and SPR-SA for spatial rain distribution. AAFM aligns features progressively.

Result: State-of-the-art performance on eight benchmark datasets, with robustness in other restoration tasks.

Conclusion: CPRAformer effectively bridges representation gaps, enabling deep feature fusion and superior deraining performance.

Abstract: Transformer-based networks have achieved strong performance in low-level
vision tasks like image deraining by utilizing spatial or channel-wise
self-attention. However, irregular rain patterns and complex geometric overlaps
challenge single-paradigm architectures, necessitating a unified framework to
integrate complementary global-local and spatial-channel representations. To
address this, we propose a novel Cross Paradigm Representation and Alignment
Transformer (CPRAformer). Its core idea is the hierarchical representation and
alignment, leveraging the strengths of both paradigms (spatial-channel and
global-local) to aid image reconstruction. It bridges the gap within and
between paradigms, aligning and coordinating them to enable deep interaction
and fusion of features. Specifically, we use two types of self-attention in the
Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial
pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel
dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain
distribution and fine-grained texture recovery. To address the feature
misalignment and knowledge differences between them, we introduce the Adaptive
Alignment Frequency Module (AAFM), which aligns and interacts with features in
a two-stage progressive manner, enabling adaptive guidance and complementarity.
This reduces the information gap within and between paradigms. Through this
unified cross-paradigm dynamic interaction framework, we achieve the extraction
of the most valuable interactive fusion information from the two paradigms.
Extensive experiments demonstrate that our model achieves state-of-the-art
performance on eight benchmark datasets and further validates CPRAformer's
robustness in other image restoration tasks and downstream applications.

</details>


### [92] [MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition](https://arxiv.org/abs/2504.16467)
*Qishan He, Lingjun Zhao, Ru Luo, Siqian Zhang, Lin Lei, Kefeng Ji, Gangyao Kuang*

Main category: cs.CV

TL;DR: The paper introduces a multi-task structure-guided learning (MTSGL) network for SAR aircraft recognition, combining classification with structural semantic awareness and geometric consistency to improve robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current SAR aircraft recognition lacks structural understanding. The paper aims to integrate aircraft structural knowledge for better performance and human-like comprehension.

Method: Proposes MTSGL with a structural semantic awareness (SSA) module and structural consistency regularization (SCR) module, leveraging fine aircraft annotations for SAR imagery.

Result: Experiments on MT-SARD show MTSGL's superior robustness and interpretability.

Conclusion: MTSGL effectively integrates structural knowledge, mimicking human cognition for SAR aircraft recognition.

Abstract: Aircraft recognition in synthetic aperture radar (SAR) imagery is a
fundamental mission in both military and civilian applications. Recently deep
learning (DL) has emerged a dominant paradigm for its explosive performance on
extracting discriminative features. However, current classification algorithms
focus primarily on learning decision hyperplane without enough comprehension on
aircraft structural knowledge. Inspired by the fined aircraft annotation
methods for optical remote sensing images (RSI), we first introduce a
structure-based SAR aircraft annotations approach to provide structural and
compositional supplement information. On this basis, we propose a multi-task
structure guided learning (MTSGL) network for robust and interpretable SAR
aircraft recognition. Besides the classification task, MTSGL includes a
structural semantic awareness (SSA) module and a structural consistency
regularization (SCR) module. The SSA is designed to capture structure semantic
information, which is conducive to gain human-like comprehension of aircraft
knowledge. The SCR helps maintain the geometric consistency between the
aircraft structure in SAR imagery and the proposed annotation. In this process,
the structural attribute can be disentangled in a geometrically meaningful
manner. In conclusion, the MTSGL is presented with the expert-level aircraft
prior knowledge and structure guided learning paradigm, aiming to comprehend
the aircraft concept in a way analogous to the human cognitive process.
Extensive experiments are conducted on a self-constructed multi-task SAR
aircraft recognition dataset (MT-SARD) and the effective results illustrate the
superiority of robustness and interpretation ability of the proposed MTSGL.

</details>


### [93] [RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory](https://arxiv.org/abs/2504.16471)
*Boyue Xu, Ruichao Hou, Tongwei Ren, Gangshan Wu*

Main category: cs.CV

TL;DR: A novel RGB-D VOS method using multi-store feature memory and SAM for robust segmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D segmentation methods inadequately explore cross-modal information and suffer from object drift in long-term predictions.

Method: Hierarchical modality selection and fusion, segmentation refinement with SAM, and spatio-temporal and modality embeddings for mixed prompts.

Result: State-of-the-art performance on the latest RGB-D VOS benchmark.

Conclusion: The proposed method effectively integrates RGB and depth data, leveraging SAM for refined and reliable segmentation.

Abstract: The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the
fine-grained texture information of RGB with the spatial geometric clues of
depth modality, boosting the performance of segmentation. However,
off-the-shelf RGB-D segmentation methods fail to fully explore cross-modal
information and suffer from object drift during long-term prediction. In this
paper, we propose a novel RGB-D VOS method via multi-store feature memory for
robust segmentation. Specifically, we design the hierarchical modality
selection and fusion, which adaptively combines features from both modalities.
Additionally, we develop a segmentation refinement module that effectively
utilizes the Segmentation Anything Model (SAM) to refine the segmentation mask,
ensuring more reliable results as memory to guide subsequent segmentation
tasks. By leveraging spatio-temporal embedding and modality embedding, mixed
prompts and fused images are fed into SAM to unleash its potential in RGB-D
VOS. Experimental results show that the proposed method achieves
state-of-the-art performance on the latest RGB-D VOS benchmark.

</details>


### [94] [Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning](https://arxiv.org/abs/2504.16487)
*Yahao Lu, Yuehui Li, Xingyuan Guo, Shuai Yuan, Yukai Shi, Liang Lin*

Main category: cs.CV

TL;DR: The paper introduces a domain adaptation-enhanced framework for infrared small target detection (ISTD), addressing domain shift issues with Cross-view Channel Alignment, Cross-view Top-K Fusion, and Noise-guided Representation learning. It also presents a new dataset, RealScene-ISTD, showing superior performance in detection metrics.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in infrared image data hinder ISTD model generalization. The paper aims to improve cross-scenario adaptability by addressing distribution discrepancies and noise impact.

Method: Proposes Cross-view Channel Alignment (CCA) for cross-sample alignment, Cross-view Top-K Fusion for integrating target and background features, and Noise-guided Representation learning for noise resistance. Introduces the RealScene-ISTD dataset.

Result: Outperforms state-of-the-art methods in detection probability (Pd), false alarm rate (Fa), and intersection over union (IoU).

Conclusion: The framework effectively addresses domain shift and noise challenges in ISTD, demonstrating improved generalization and performance.

Abstract: Infrared small target detection (ISTD) is highly sensitive to sensor type,
observation conditions, and the intrinsic properties of the target. These
factors can introduce substantial variations in the distribution of acquired
infrared image data, a phenomenon known as domain shift. Such distribution
discrepancies significantly hinder the generalization capability of ISTD models
across diverse scenarios. To tackle this challenge, this paper introduces an
ISTD framework enhanced by domain adaptation. To alleviate distribution shift
between datasets and achieve cross-sample alignment, we introduce Cross-view
Channel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion
strategy, which integrates target information with diverse background features,
enhancing the model' s ability to extract critical data characteristics. To
further mitigate the impact of noise on ISTD, we develop a Noise-guided
Representation learning strategy. This approach enables the model to learn more
noise-resistant feature representations, to improve its generalization
capability across diverse noisy domains. Finally, we develop a dedicated
infrared small target dataset, RealScene-ISTD. Compared to state-of-the-art
methods, our approach demonstrates superior performance in terms of detection
probability (Pd), false alarm rate (Fa), and intersection over union (IoU). The
code is available at: https://github.com/luy0222/RealScene-ISTD.

</details>


### [95] [PRaDA: Projective Radial Distortion Averaging](https://arxiv.org/abs/2504.16499)
*Daniil Sinitsyn, Linus HÃ¤renstam-Nielsen, Daniel Cremers*

Main category: cs.CV

TL;DR: The paper introduces a method for automatic camera calibration in challenging conditions by decoupling distortion calibration from 3D reconstruction, avoiding complexities of traditional SfM or learning-based approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for camera distortion calibration either require solving the full SfM problem (complex and data-intensive) or rely on less accurate learning-based approaches. The goal is to simplify the process while maintaining accuracy.

Method: The proposed method, Projective Radial Distortion Averaging, works in Projective Space, where geometry is unique up to a homography. It averages distortion estimates without full 3D reconstruction or bundle adjustment, using pairwise projective relations.

Result: The method achieves accuracy comparable to SfM-based approaches while avoiding their complexities, supporting any feature-matching technique without multi-image point tracks.

Conclusion: Decoupling distortion calibration from 3D reconstruction in Projective Space offers a simpler, accurate alternative to traditional methods, suitable for challenging conditions.

Abstract: We tackle the problem of automatic calibration of radially distorted cameras
in challenging conditions. Accurately determining distortion parameters
typically requires either 1) solving the full Structure from Motion (SfM)
problem involving camera poses, 3D points, and the distortion parameters, which
is only possible if many images with sufficient overlap are provided, or 2)
relying heavily on learning-based methods that are comparatively less accurate.
In this work, we demonstrate that distortion calibration can be decoupled from
3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding
many of the associated complexities. This is achieved by working in Projective
Space, where the geometry is unique up to a homography, which encapsulates all
camera parameters except for distortion. Our proposed method, Projective Radial
Distortion Averaging, averages multiple distortion estimates in a fully
projective framework without creating 3d points and full bundle adjustment. By
relying on pairwise projective relations, our methods support any
feature-matching approaches without constructing point tracks across multiple
images.

</details>


### [96] [Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity](https://arxiv.org/abs/2504.16515)
*Abdul Hannaan, Zubair Shah, Aiman Erbad, Amr Mohamed, Ali Safa*

Main category: cs.CV

TL;DR: LoRa-FL is a federated learning framework for low-rank one-shot image detection on edge devices, reducing computational and communication costs without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient training and deployment of lightweight image detection models on resource-constrained edge devices.

Method: Incorporates low-rank adaptation into one-shot detection architectures and uses federated learning for collaborative training.

Result: Achieves competitive performance on MNIST and CIFAR10 (IID and non-IID) with reduced bandwidth and compute complexity.

Conclusion: LoRa-FL is a promising solution for efficient, scalable, and accurate image detection on edge devices.

Abstract: This paper introduces a novel federated learning framework termed LoRa-FL
designed for training low-rank one-shot image detection models deployed on edge
devices. By incorporating low-rank adaptation techniques into one-shot
detection architectures, our method significantly reduces both computational
and communication overhead while maintaining scalable accuracy. The proposed
framework leverages federated learning to collaboratively train lightweight
image recognition models, enabling rapid adaptation and efficient deployment
across heterogeneous, resource-constrained devices. Experimental evaluations on
the MNIST and CIFAR10 benchmark datasets, both in an
independent-and-identically-distributed (IID) and non-IID setting, demonstrate
that our approach achieves competitive detection performance while
significantly reducing communication bandwidth and compute complexity. This
makes it a promising solution for adaptively reducing the communication and
compute power overheads, while not sacrificing model accuracy.

</details>


### [97] [Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation](https://arxiv.org/abs/2504.16516)
*Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao*

Main category: cs.CV

TL;DR: MFRA enhances VLN by fusing multi-level features and reasoning over modalities, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods lack the ability to capture complex cross-modal interactions needed for accurate navigation.

Method: MFRA uses hierarchical fusion of multi-level features and a reasoning module for instruction-guided attention and dynamic context integration.

Result: MFRA achieves superior performance on VLN benchmarks like REVERIE, R2R, and SOON.

Conclusion: Multi-level modal fusion is effective for improving embodied navigation.

Abstract: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow
natural language instructions and reach target locations in real-world
environments. While prior methods often rely on either global scene
representations or object-level features, these approaches are insufficient for
capturing the complex interactions across modalities required for accurate
navigation. In this paper, we propose a Multi-level Fusion and Reasoning
Architecture (MFRA) to enhance the agent's ability to reason over visual
observations, language instructions and navigation history. Specifically, MFRA
introduces a hierarchical fusion mechanism that aggregates multi-level
features-ranging from low-level visual cues to high-level semantic
concepts-across multiple modalities. We further design a reasoning module that
leverages fused representations to infer navigation actions through
instruction-guided attention and dynamic context integration. By selectively
capturing and combining relevant visual, linguistic, and temporal signals, MFRA
improves decision-making accuracy in complex navigation scenarios. Extensive
experiments on benchmark VLN datasets including REVERIE, R2R, and SOON
demonstrate that MFRA achieves superior performance compared to
state-of-the-art methods, validating the effectiveness of multi-level modal
fusion for embodied navigation.

</details>


### [98] [A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification](https://arxiv.org/abs/2504.16520)
*Wenwei Li, Liyi Cai, Wu Chen, Anan Li*

Main category: cs.CV

TL;DR: A few-shot metric learning method with dual-channel attention and a pretrained vision transformer is proposed for robust cross-modal neuron identification, outperforming existing methods in accuracy and recall.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in single-neuron matching across imaging modalities due to modality gaps and limited annotations.

Method: Uses a dual-channel attention mechanism (local and global) and a pretrained vision transformer, enhanced by hard sample mining and Circle Loss.

Result: Superior Top-K accuracy and recall on two-photon and fMOST datasets, validated by ablation studies and t-SNE visualizations.

Conclusion: The method provides an accurate and efficient solution for single-cell level matching and multimodal neuroimaging integration.

Abstract: In neuroscience research, achieving single-neuron matching across different
imaging modalities is critical for understanding the relationship between
neuronal structure and function. However, modality gaps and limited annotations
present significant challenges. We propose a few-shot metric learning method
with a dual-channel attention mechanism and a pretrained vision transformer to
enable robust cross-modal neuron identification. The local and global channels
extract soma morphology and fiber context, respectively, and a gating mechanism
fuses their outputs. To enhance the model's fine-grained discrimination
capability, we introduce a hard sample mining strategy based on the
MultiSimilarityMiner algorithm, along with the Circle Loss function.
Experiments on two-photon and fMOST datasets demonstrate superior Top-K
accuracy and recall compared to existing methods. Ablation studies and t-SNE
visualizations validate the effectiveness of each module. The method also
achieves a favorable trade-off between accuracy and training efficiency under
different fine-tuning strategies. These results suggest that the proposed
approach offers a promising technical solution for accurate single-cell level
matching and multimodal neuroimaging integration.

</details>


### [99] [Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes](https://arxiv.org/abs/2504.16538)
*Joan Perez, Giovanni Fusco*

Main category: cs.CV

TL;DR: SAGAI is a modular workflow using generative AI to analyze streetscapes, integrating open data and vision-language models for scalable urban assessment.


<details>
  <summary>Details</summary>
Motivation: Current streetscape assessments are limited to morphometric properties or require labor-intensive qualitative evaluations, lacking scalable and interpretable methods.

Method: SAGAI uses OpenStreetMap, Google Street View, and LLaVA to generate spatial indicators via natural language prompts, with automated mapping for cartographic interpretation.

Result: Case studies in Nice and Vienna show strong urban-rural classification, moderate commercial feature detection, and informative sidewalk width estimates.

Conclusion: SAGAI is adaptable for various urban research themes like walkability and safety, requiring only prompt modification for deployment.

Abstract: Streetscapes are an essential component of urban space. Their assessment is
presently either limited to morphometric properties of their mass skeleton or
requires labor-intensive qualitative evaluations of visually perceived
qualities. This paper introduces SAGAI: Streetscape Analysis with Generative
Artificial Intelligence, a modular workflow for scoring street-level urban
scenes using open-access data and vision-language models. SAGAI integrates
OpenStreetMap geometries, Google Street View imagery, and a lightweight version
of the LLaVA model to generate structured spatial indicators from images via
customizable natural language prompts. The pipeline includes an automated
mapping module that aggregates visual scores at both the point and street
levels, enabling direct cartographic interpretation. It operates without
task-specific training or proprietary software dependencies, supporting
scalable and interpretable analysis of urban environments. Two exploratory case
studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial
outputs from vision-language inference. The initial results show strong
performance for binary urban-rural scene classification, moderate precision in
commercial feature detection, and lower estimates, but still informative, of
sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a
wide range of urban research themes, such as walkability, safety, or urban
design, through prompt modification alone.

</details>


### [100] [ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration](https://arxiv.org/abs/2504.16545)
*Andrea Conti, Matteo Poggi, Valerio Cambareri, Martin R. Oswald, Stefano Mattoccia*

Main category: cs.CV

TL;DR: ToF-Splatting is a 3D Gaussian Splatting-based SLAM pipeline designed for sparse Time-of-Flight (ToF) data, improving depth map density and performance.


<details>
  <summary>Details</summary>
Motivation: Sparse ToF measurements limit SLAM usability; this work addresses the challenge of using extremely sparse ToF data effectively.

Method: Proposes a multi-frame integration module combining sparse ToF depth, monocular color, and multi-view geometry to produce dense depth maps.

Result: Achieves state-of-the-art tracking and mapping performance on synthetic and real sparse ToF datasets.

Conclusion: ToF-Splatting is viable and outperforms existing methods, enhancing SLAM with sparse ToF inputs.

Abstract: Time-of-Flight (ToF) sensors provide efficient active depth sensing at
relatively low power budgets; among such designs, only very sparse measurements
from low-resolution sensors are considered to meet the increasingly limited
power constraints of mobile and AR/VR devices. However, such extreme sparsity
levels limit the seamless usage of ToF depth in SLAM. In this work, we propose
ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for
using effectively very sparse ToF input data. Our approach improves upon the
state of the art by introducing a multi-frame integration module, which
produces dense depth maps by merging cues from extremely sparse ToF depth,
monocular color, and multi-view geometry. Extensive experiments on both
synthetic and real sparse ToF datasets demonstrate the viability of our
approach, as it achieves state-of-the-art tracking and mapping performances on
reference datasets.

</details>


### [101] [Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks](https://arxiv.org/abs/2504.16557)
*Murat Bilgehan Ertan, Ronak Sahu, Phuong Ha Nguyen, Kaleel Mahmood, Marten van Dijk*

Main category: cs.CV

TL;DR: ROAR is a framework for privacy-preserving dataset obfuscation by removing sensitive objects, outperforming image dropping in utility preservation and showing minimal degradation in 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in datasets by removing sensitive objects while maintaining dataset utility and scene integrity.

Method: Integrates instance segmentation with generative inpainting to remove identifiable entities.

Result: Achieves 87.5% of baseline AP in 2D detection (vs. 74.2% for image dropping) and minimal PSNR loss in 3D reconstruction.

Conclusion: Object removal is an effective privacy framework with strong guarantees and minimal trade-offs, highlighting challenges for future work.

Abstract: We introduce ROAR (Robust Object Removal and Re-annotation), a scalable
framework for privacy-preserving dataset obfuscation that eliminates sensitive
objects instead of modifying them. Our method integrates instance segmentation
with generative inpainting to remove identifiable entities while preserving
scene integrity. Extensive evaluations on 2D COCO-based object detection show
that ROAR achieves 87.5% of the baseline detection average precision (AP),
whereas image dropping achieves only 74.2% of the baseline AP, highlighting the
advantage of scrubbing in preserving dataset utility. The degradation is even
more severe for small objects due to occlusion and loss of fine-grained
details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR
loss of at most 1.66 dB while maintaining SSIM and improving LPIPS,
demonstrating superior perceptual quality. Our findings establish object
removal as an effective privacy framework, achieving strong privacy guarantees
with minimal performance trade-offs. The results highlight key challenges in
generative inpainting, occlusion-robust segmentation, and task-specific
scrubbing, setting the foundation for future advancements in privacy-preserving
vision systems.

</details>


### [102] [SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation](https://arxiv.org/abs/2504.16564)
*Zhongtao Wang, Xizhe Cao, Yisong Chen, Guoping Wang*

Main category: cs.CV

TL;DR: SAIP-Net improves remote sensing image segmentation by using spectral-adaptive strategies and multi-scale receptive fields to enhance boundary precision and intra-class consistency.


<details>
  <summary>Details</summary>
Motivation: Conventional hierarchical models struggle with precise spatial boundaries and intra-class consistency in remote sensing imagery.

Method: SAIP-Net employs adaptive frequency filtering and multi-scale receptive field enhancement.

Result: Experiments show significant performance improvements over state-of-the-art methods.

Conclusion: Spectral-adaptive strategies with expanded receptive fields effectively enhance remote sensing image segmentation.

Abstract: Semantic segmentation of remote sensing imagery demands precise spatial
boundaries and robust intra-class consistency, challenging conventional
hierarchical models. To address limitations arising from spatial domain feature
fusion and insufficient receptive fields, this paper introduces SAIP-Net, a
novel frequency-aware segmentation framework that leverages Spectral Adaptive
Information Propagation. SAIP-Net employs adaptive frequency filtering and
multi-scale receptive field enhancement to effectively suppress intra-class
feature inconsistencies and sharpen boundary lines. Comprehensive experiments
demonstrate significant performance improvements over state-of-the-art methods,
highlighting the effectiveness of spectral-adaptive strategies combined with
expanded receptive fields for remote sensing image segmentation.

</details>


### [103] [CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones](https://arxiv.org/abs/2504.16570)
*Giacomo Pacini, Lorenzo Bianchi, Luca Ciampi, Nicola Messina, Giuseppe Amato, Fabrizio Falchi*

Main category: cs.CV

TL;DR: CountingDINO is a training-free, class-agnostic counting framework using unsupervised features, outperforming baselines and matching supervised methods.


<details>
  <summary>Details</summary>
Motivation: Current exemplar-based CAC methods rely on labeled data, limiting scalability and generalization.

Method: Uses self-supervised backbones to extract object-aware features, generates similarity maps via ROI-Align, and transforms them into density maps.

Result: Outperforms label-free baselines and matches supervised methods on FSC-147.

Conclusion: Training-free CAC can be scalable and competitive without labeled data.

Abstract: Class-agnostic counting (CAC) aims to estimate the number of objects in
images without being restricted to predefined categories. However, while
current exemplar-based CAC methods offer flexibility at inference time, they
still rely heavily on labeled data for training, which limits scalability and
generalization to many downstream use cases. In this paper, we introduce
CountingDINO, the first training-free exemplar-based CAC framework that
exploits a fully unsupervised feature extractor. Specifically, our approach
employs self-supervised vision-only backbones to extract object-aware features,
and it eliminates the need for annotated data throughout the entire proposed
pipeline. At inference time, we extract latent object prototypes via ROI-Align
from DINO features and use them as convolutional kernels to generate similarity
maps. These are then transformed into density maps through a simple yet
effective normalization scheme. We evaluate our approach on the FSC-147
benchmark, where we outperform a baseline under the same label-free setting.
Our method also achieves competitive -- and in some cases superior -- results
compared to training-free approaches relying on supervised backbones, as well
as several fully supervised state-of-the-art methods. This demonstrates that
training-free CAC can be both scalable and competitive. Website:
https://lorebianchi98.github.io/CountingDINO/

</details>


### [104] [JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning](https://arxiv.org/abs/2504.16591)
*Tristan Kenneweg, Philip Kenneweg, Barbara Hammer*

Main category: cs.CV

TL;DR: JEPA is adapted for reinforcement learning from images, addressing model collapse and demonstrating success on the Cart Pole task.


<details>
  <summary>Details</summary>
Motivation: To extend the success of JEPA in self-supervised learning to reinforcement learning from images.

Method: Adapt JEPA architecture for reinforcement learning, addressing model collapse.

Result: Successful application to the Cart Pole task with exemplary data.

Conclusion: JEPA can be effectively adapted for reinforcement learning, with measures to prevent model collapse.

Abstract: Joint-Embedding Predictive Architectures (JEPA) have recently become popular
as promising architectures for self-supervised learning. Vision transformers
have been trained using JEPA to produce embeddings from images and videos,
which have been shown to be highly suitable for downstream tasks like
classification and segmentation. In this paper, we show how to adapt the JEPA
architecture to reinforcement learning from images. We discuss model collapse,
show how to prevent it, and provide exemplary data on the classical Cart Pole
task.

</details>


### [105] [Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections](https://arxiv.org/abs/2504.16612)
*Max Kirchner, Alexander C. Jenke, Sebastian Bodenstedt, Fiona R. Kolbinger, Oliver Saldanha, Jakob N. Kather, Martin Wagner, Stefanie Speidel*

Main category: cs.CV

TL;DR: Federated learning (FL) with adaptive FedSAM and SWA improves surgical foundation model training, achieving comparable or better performance than centralized methods in tasks like segmentation and action recognition.


<details>
  <summary>Details</summary>
Motivation: Address data-sharing limitations in minimally invasive surgery by enabling collaborative model training without data transfer.

Method: Adapt Masked Autoencoder for FL with FedSAM and SWA, pretrain on Endo700k, and fine-tune for tasks like segmentation and action recognition.

Result: FedSAM reduces reconstruction loss; FL-EndoViT matches or outperforms CEN-EndoViT in limited-data scenarios and large datasets.

Conclusion: FL offers privacy-preserving, robust training for surgical models, with future potential in video-based spatiotemporal dynamics.

Abstract: Purpose: In this study, we investigate the training of foundation models
using federated learning to address data-sharing limitations and enable
collaborative model training without data transfer for minimally invasive
surgery. Methods: Inspired by the EndoViT study, we adapt the Masked
Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware
Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is
pretrained on the Endo700k dataset collection and later fine-tuned and
evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition,
and Surgical Phase Recognition. Results: Our findings demonstrate that
integrating adaptive FedSAM into the federated MAE approach improves
pretraining, leading to a reduction in reconstruction loss per patch. The
application of FL-EndoViT in surgical downstream tasks results in performance
comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over
CEN-EndoViT in surgical scene segmentation when data is limited and in action
triplet recognition when large datasets are used. Conclusion: These findings
highlight the potential of federated learning for privacy-preserving training
of surgical foundation models, offering a robust and generalizable solution for
surgical data science. Effective collaboration requires adapting federated
learning methods, such as the integration of FedSAM, which can accommodate the
inherent data heterogeneity across institutions. In future, exploring FL in
video-based models may enhance these capabilities by incorporating
spatiotemporal dynamics crucial for real-world surgical environments.

</details>


### [106] [EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception](https://arxiv.org/abs/2504.16616)
*Haosheng Chen, Lian Luo, Mengjingcheng Mo, Zhanjie Wu, Guobao Xiao, Ji Gan, Jiaxu Leng, Xinbo Gao*

Main category: cs.CV

TL;DR: EHGCN is a novel approach for event vision, combining Euclidean and hyperbolic spaces to improve perception of event streams, with adaptive sampling and motion-aware hyperedge generation.


<details>
  <summary>Details</summary>
Motivation: Existing GNN-based methods struggle with long-range dependencies and hierarchical structures in event streams, limiting their effectiveness.

Method: EHGCN introduces adaptive sampling, motion-aware hyperedge generation, and a Euclidean-Hyperbolic GCN for hybrid event perception.

Result: Experiments show EHGCN's effectiveness in tasks like object detection and recognition.

Conclusion: EHGCN advances event vision by leveraging both Euclidean and hyperbolic spaces for better perception.

Abstract: Event cameras, with microsecond temporal resolution and high dynamic range
(HDR) characteristics, emit high-speed event stream for perception tasks.
Despite the recent advancement in GNN-based perception methods, they are prone
to use straightforward pairwise connectivity mechanisms in the pure Euclidean
space where they struggle to capture long-range dependencies and fail to
effectively characterize the inherent hierarchical structures of non-uniformly
distributed event stream. To this end, in this paper we propose a novel
approach named EHGCN, which is a pioneer to perceive event stream in both
Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an
adaptive sampling strategy to dynamically regulate sampling rates, retaining
discriminative events while attenuating chaotic noise. Then we present a Markov
Vector Field (MVF)-driven motion-aware hyperedge generation method based on
motion state transition probabilities, thereby eliminating cross-target
spurious associations and providing critically topological priors while
capturing long-range dependencies between events. Finally, we propose a
Euclidean-Hyperbolic GCN to fuse the information locally aggregated and
globally hierarchically modeled in Euclidean and hyperbolic spaces,
respectively, to achieve hybrid event perception. Experimental results on event
perception tasks such as object detection and recognition validate the
effectiveness of our approach.

</details>


### [107] [Dual-Camera All-in-Focus Neural Radiance Fields](https://arxiv.org/abs/2504.16636)
*Xianrui Luo, Zijin Wu, Juewen Peng, Huiqiang Sun, Zhiguo Cao, Guosheng Lin*

Main category: cs.CV

TL;DR: DC-NeRF synthesizes all-in-focus NeRF using dual-camera inputs without manual refocusing, leveraging ultra-wide and main cameras for high-fidelity results.


<details>
  <summary>Details</summary>
Motivation: Current NeRF methods fail with defocus blur due to fixed-focus cameras, needing a solution for all-in-focus restoration.

Method: Uses dual-camera alignment (spatial warping, color matching) and defocus-aware fusion with learnable parameters to predict defocus maps.

Result: DC-NeRF produces high-quality all-in-focus novel views, outperforming baselines, and enables DoF applications like refocusing.

Conclusion: DC-NeRF effectively restores all-in-focus NeRF using smartphone dual cameras, offering practical applications in adjustable blur and focal planes.

Abstract: We present the first framework capable of synthesizing the all-in-focus
neural radiance field (NeRF) from inputs without manual refocusing. Without
refocusing, the camera will automatically focus on the fixed object for all
views, and current NeRF methods typically using one camera fail due to the
consistent defocus blur and a lack of sharp reference. To restore the
all-in-focus NeRF, we introduce the dual-camera from smartphones, where the
ultra-wide camera has a wider depth-of-field (DoF) and the main camera
possesses a higher resolution. The dual camera pair saves the high-fidelity
details from the main camera and uses the ultra-wide camera's deep DoF as
reference for all-in-focus restoration. To this end, we first implement spatial
warping and color matching to align the dual camera, followed by a
defocus-aware fusion module with learnable defocus parameters to predict a
defocus map and fuse the aligned camera pair. We also build a multi-view
dataset that includes image pairs of the main and ultra-wide cameras in a
smartphone. Extensive experiments on this dataset verify that our solution,
termed DC-NeRF, can produce high-quality all-in-focus novel views and compares
favorably against strong baselines quantitatively and qualitatively. We further
show DoF applications of DC-NeRF with adjustable blur intensity and focal
plane, including refocusing and split diopter.

</details>


### [108] [RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration](https://arxiv.org/abs/2504.16637)
*Qifan Li, Tianyi Liang, Xingtao Wang, Xiaopeng Fan*

Main category: cs.CV

TL;DR: RouteWinFormer, a window-based Transformer, efficiently models middle-range context for image restoration by dynamically selecting nearby windows and using multi-scale structure regularization, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Transformer models capture long-range dependencies but often incur unnecessary computational overhead, as degradation and context in image restoration are typically localized. Middle-range attention is found sufficient.

Method: Proposes RouteWinFormer with Route-Windows Attention Module for dynamic window selection and Multi-Scale Structure Regularization to balance structural and degradation learning.

Result: Outperforms state-of-the-art methods on 9 datasets across various image restoration tasks.

Conclusion: RouteWinFormer effectively balances computational efficiency and performance by focusing on middle-range attention and structural priors.

Abstract: Transformer models have recently garnered significant attention in image
restoration due to their ability to capture long-range pixel dependencies.
However, long-range attention often results in computational overhead without
practical necessity, as degradation and context are typically localized.
Normalized average attention distance across various degradation datasets shows
that middle-range attention is enough for image restoration. Building on this
insight, we propose RouteWinFormer, a novel window-based Transformer that
models middle-range context for image restoration. RouteWinFormer incorporates
Route-Windows Attnetion Module, which dynamically selects relevant nearby
windows based on regional similarity for attention aggregation, extending the
receptive field to a mid-range size efficiently. In addition, we introduce
Multi-Scale Structure Regularization during training, enabling the sub-scale of
the U-shaped network to focus on structural information, while the
original-scale learns degradation patterns based on generalized image structure
priors. Extensive experiments demonstrate that RouteWinFormer outperforms
state-of-the-art methods across 9 datasets in various image restoration tasks.

</details>


### [109] [SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition](https://arxiv.org/abs/2504.16640)
*Hasan Algafri, Hamzah Luqman, Sarah Alyami, Issam Laradji*

Main category: cs.CV

TL;DR: A semi-supervised learning (SSL) approach for sign language recognition (SLR) using pose information and a Transformer model outperforms supervised learning with less labeled data.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated datasets in SLR by leveraging unlabeled data through SSL.

Method: Proposes an SSL approach (SSLR) using pseudo-labeling for unlabeled samples, with pose information as input for a Transformer model.

Result: The SSL model outperformed the supervised model with less labeled data on the WLASL-100 dataset.

Conclusion: SSL is effective for SLR, especially when labeled data is limited.

Abstract: Sign language is the primary communication language for people with disabling
hearing loss. Sign language recognition (SLR) systems aim to recognize sign
gestures and translate them into spoken language. One of the main challenges in
SLR is the scarcity of annotated datasets. To address this issue, we propose a
semi-supervised learning (SSL) approach for SLR (SSLR), employing a
pseudo-label method to annotate unlabeled samples. The sign gestures are
represented using pose information that encodes the signer's skeletal joint
points. This information is used as input for the Transformer backbone model
utilized in the proposed approach. To demonstrate the learning capabilities of
SSL across various labeled data sizes, several experiments were conducted using
different percentages of labeled data with varying numbers of classes. The
performance of the SSL approach was compared with a fully supervised
learning-based model on the WLASL-100 dataset. The obtained results of the SSL
model outperformed the supervised learning-based model with less labeled data
in many cases.

</details>


### [110] [WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks](https://arxiv.org/abs/2504.16655)
*Younggeol Cho, Elisa Motta, Olivia Nocentini, Marta Lagomarsino, Andrea Merello, Marco Crepaldi, Arash Ajoudani*

Main category: cs.CV

TL;DR: A novel Transformer-based Encoder-Decoder Network (TED Net) is proposed for human pose estimation from WiFi CSI, integrated with a DGNN for action recognition, showing robust performance in fall detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for privacy-preserving, non-intrusive human pose estimation and action recognition in healthcare and assistive technologies, leveraging WiFi signals as an alternative to vision-based methods.

Method: TED Net combines convolutional encoders with transformer-based attention to extract spatiotemporal features from CSI signals, followed by a DGNN for action recognition.

Result: TED Net outperforms existing pose estimation methods, and the DGNN achieves action classification comparable to RGB-based systems, with robust performance in fall scenarios.

Conclusion: WiFi CSI-driven skeleton estimation and action recognition offer a viable, privacy-friendly solution for applications like elderly fall detection in home environments.

Abstract: Human pose estimation and action recognition have received attention due to
their critical roles in healthcare monitoring, rehabilitation, and assistive
technologies. In this study, we proposed a novel architecture named Transformer
based Encoder Decoder Network (TED Net) designed for estimating human skeleton
poses from WiFi Channel State Information (CSI). TED Net integrates
convolutional encoders with transformer based attention mechanisms to capture
spatiotemporal features from CSI signals. The estimated skeleton poses were
used as input to a customized Directed Graph Neural Network (DGNN) for action
recognition. We validated our model on two datasets: a publicly available multi
modal dataset for assessing general pose estimation, and a newly collected
dataset focused on fall related scenarios involving 20 participants.
Experimental results demonstrated that TED Net outperformed existing approaches
in pose estimation, and that the DGNN achieves reliable action classification
using CSI based skeletons, with performance comparable to RGB based systems.
Notably, TED Net maintains robust performance across both fall and non fall
cases. These findings highlight the potential of CSI driven human skeleton
estimation for effective action recognition, particularly in home environments
such as elderly fall detection. In such settings, WiFi signals are often
readily available, offering a privacy preserving alternative to vision based
methods, which may raise concerns about continuous camera monitoring.

</details>


### [111] [Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning](https://arxiv.org/abs/2504.16656)
*Chris, Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, Yang Liu, Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork R1V2 is a multimodal reasoning model with hybrid reinforcement learning and a Selective Sample Buffer (SSB) to improve training efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of balancing sophisticated reasoning with broad generalization in AI models.

Method: Uses hybrid reinforcement learning (reward-model guidance + rule-based strategies) and SSB to counter Vanishing Advantages in GRPO.

Result: Achieves benchmark-leading scores (e.g., 62.6 on OlympiadBench, 79.0 on AIME2024).

Conclusion: R1V2 outperforms open-source models and narrows the gap with proprietary systems like Gemini 2.5 and OpenAI o4-mini, promoting openness with public release.

Abstract: We present Skywork R1V2, a next-generation multimodal reasoning model and a
major leap forward from its predecessor, Skywork R1V. At its core, R1V2
introduces a hybrid reinforcement learning paradigm that harmonizes
reward-model guidance with rule-based strategies, thereby addressing the
long-standing challenge of balancing sophisticated reasoning capabilities with
broad generalization. To further enhance training efficiency, we propose the
Selective Sample Buffer (SSB) mechanism, which effectively counters the
``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization
(GRPO) by prioritizing high-value samples throughout the optimization process.
Notably, we observe that excessive reinforcement signals can induce visual
hallucinations--a phenomenon we systematically monitor and mitigate through
calibrated reward thresholds throughout the training process. Empirical results
affirm the exceptional capability of R1V2, with benchmark-leading performances
such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and
74.0 on MMMU. These results underscore R1V2's superiority over existing
open-source models and demonstrate significant progress in closing the
performance gap with premier proprietary systems, including Gemini 2.5 and
OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to
promote openness and reproducibility
https://huggingface.co/Skywork/Skywork-R1V2-38B.

</details>


### [112] [A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process](https://arxiv.org/abs/2504.16658)
*Ole-Christian Galbo EngstrÃ¸m, Erik Schou Dreier, Birthe MÃ¸ller Jespersen, Kim Steenstrup Pedersen*

Main category: cs.CV

TL;DR: An open-source dataset of RGB and NIR-HSI images of malting barley kernels, with segmentation masks and NIR spectra, tracking germination over five days.


<details>
  <summary>Details</summary>
Motivation: To enable time series analysis of barley kernel germination using RGB, NIR spectral, or NIR-HSI data.

Method: Imaged kernels pre- and post-moisture exposure every 24 hours for five days, using black filter paper for easy segmentation.

Result: Dataset includes 2242 kernels labeled for germination status, facilitating diverse analytical approaches.

Conclusion: The dataset supports comprehensive germination analysis through multiple imaging and spectral methods.

Abstract: We provide an open-source dataset of RGB and NIR-HSI (near-infrared
hyperspectral imaging) images with associated segmentation masks and NIR
spectra of 2242 individual malting barley kernels. We imaged every kernel
pre-exposure to moisture and every 24 hours after exposure to moisture for five
consecutive days. Every barley kernel was labeled as germinated or not
germinated during each image acquisition. The barley kernels were imaged with
black filter paper as the background, facilitating straight-forward intensity
threshold-based segmentation, e.g., by Otsu's method. This dataset facilitates
time series analysis of germination time for barley kernels using either RGB
image analysis, NIR spectral analysis, NIR-HSI analysis, or a combination
hereof.

</details>


### [113] [A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification](https://arxiv.org/abs/2504.16665)
*Wenping Ma, Boyou Xue, Mengru Ma, Chuang Chen, Hekai Zhang, Hao Zhu*

Main category: cs.CV

TL;DR: The paper proposes a diff-attention aware state space fusion model (DAS2F-Model) for multimodal remote sensing image classification, separating common and dominant features of MS and PAN images to reduce redundancy.


<details>
  <summary>Details</summary>
Motivation: To address feature redundancy in fusion and improve classification by leveraging the advantages of MS and PAN images while separating their common and unique features.

Method: Uses a cross-modal diff-attention module (CMDA-Module) for feature separation, space preserving visual mamba (SPVM) for spatial feature retention, and an attention-aware linear fusion module (AALF-Module) for effective fusion of semantically different features.

Result: The method outperforms alternative approaches in empirical evaluations.

Conclusion: The DAS2F-Model effectively integrates MS and PAN images for improved classification by addressing feature redundancy and semantic differences.

Abstract: Multispectral (MS) and panchromatic (PAN) images describe the same land
surface, so these images not only have their own advantages, but also have a
lot of similar information. In order to separate these similar information and
their respective advantages, reduce the feature redundancy in the fusion stage.
This paper introduces a diff-attention aware state space fusion model
(DAS2F-Model) for multimodal remote sensing image classification. Based on the
selective state space model, a cross-modal diff-attention module (CMDA-Module)
is designed to extract and separate the common features and their respective
dominant features of MS and PAN images. Among this, space preserving visual
mamba (SPVM) retains image spatial features and captures local features by
optimizing visual mamba's input reasonably. Considering that features in the
fusion stage will have large semantic differences after feature separation and
simple fusion operations struggle to effectively integrate these significantly
different features, an attention-aware linear fusion module (AALF-Module) is
proposed. It performs pixel-wise linear fusion by calculating influence
coefficients. This mechanism can fuse features with large semantic differences
while keeping the feature size unchanged. Empirical evaluations indicate that
the presented method achieves better results than alternative approaches. The
relevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model

</details>


### [114] [SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets](https://arxiv.org/abs/2504.16684)
*Gerardus Croonen, Andreas Trondl, Julia Simon, Daniel Steininger*

Main category: cs.CV

TL;DR: A novel dataset and two-stage method for detecting, segmenting, and estimating mass of sugar beets in RGB images, achieving high accuracy in detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Sugar beets lose sugar during storage due to microorganisms and excess vegetation. Automated visual inspection can improve quality assurance and efficiency in sugar production.

Method: A two-stage method for detection, semantic segmentation, and mass estimation of sugar beets, tested with various image sizes, model architectures, and environmental conditions.

Result: Achieved mAP50-95 of 98.8 for detection and mIoU of 64.0 for segmentation, demonstrating high performance.

Conclusion: The proposed method and dataset effectively address quality inspection challenges in sugar beet processing.

Abstract: While sugar beets are stored prior to processing, they lose sugar due to
factors such as microorganisms present in adherent soil and excess vegetation.
Their automated visual inspection promises to aide in quality assurance and
thereby increase efficiency throughout the processing chain of sugar
production. In this work, we present a novel high-quality annotated dataset and
two-stage method for the detection, semantic segmentation and mass estimation
of post-harvest and post-storage sugar beets in monocular RGB images. We
conduct extensive ablation experiments for the detection of sugar beets and
their fine-grained semantic segmentation regarding damages, rot, soil adhesion
and excess vegetation. For these tasks, we evaluate multiple image sizes, model
architectures and encoders, as well as the influence of environmental
conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection
and an mIoU of 64.0 for the best-performing segmentation model.

</details>


### [115] [Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation](https://arxiv.org/abs/2504.16692)
*Xinru Meng, Han Sun, Jiamei Liu, Ningzhong Liu, Huiyu Zhou*

Main category: cs.CV

TL;DR: EBPR refines pseudo-labels using energy scores and contrastive learning for SFDA, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: SFDA is challenging due to noisy pseudo-labels from confidence levels, leading to negative transfer.

Method: EBPR uses energy scores to create and filter pseudo-labels, with contrastive learning for difficult samples.

Result: Validated on Office-31, Office-Home, and VisDA-C, EBPR outperforms state-of-the-art methods.

Conclusion: EBPR effectively addresses SFDA challenges by refining pseudo-labels and leveraging contrastive learning.

Abstract: Source-free domain adaptation (SFDA), which involves adapting models without
access to source data, is both demanding and challenging. Existing SFDA
techniques typically rely on pseudo-labels generated from confidence levels,
leading to negative transfer due to significant noise. To tackle this problem,
Energy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels
are created for all sample clusters according to their energy scores. Global
and class energy thresholds are computed to selectively filter pseudo-labels.
Furthermore, a contrastive learning strategy is introduced to filter difficult
samples, aligning them with their augmented versions to learn more
discriminative features. Our method is validated on the Office-31, Office-Home,
and VisDA-C datasets, consistently finding that our model outperformed
state-of-the-art methods.

</details>


### [116] [PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning](https://arxiv.org/abs/2504.16722)
*Yingjie Xi, Jian Jun Zhang, Xiaosong Yang*

Main category: cs.CV

TL;DR: ProMoGen is a framework for synthesizing human motion by combining trajectory guidance and sparse anchor motion control, enhanced by SAP-CL for stable learning. It outperforms existing methods in controllability and precision.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human motion synthesis (textual, trajectory-based, anchor poses-guided) have limitations in accuracy, customization, and complexity. ProMoGen aims to address these gaps.

Method: ProMoGen integrates trajectory guidance with sparse anchor motion control, decoupling spatial direction and precise action guidance. SAP-CL introduces curriculum learning for stable convergence.

Result: ProMoGen generates high-fidelity, controllable motions, outperforming state-of-the-art methods in diverse control scenarios.

Conclusion: ProMoGen offers a unified, precise, and stable solution for human motion synthesis, combining personalized motion with structured guidance.

Abstract: In computer animation, game design, and human-computer interaction,
synthesizing human motion that aligns with user intent remains a significant
challenge. Existing methods have notable limitations: textual approaches offer
high-level semantic guidance but struggle to describe complex actions
accurately; trajectory-based techniques provide intuitive global motion
direction yet often fall short in generating precise or customized character
movements; and anchor poses-guided methods are typically confined to synthesize
only simple motion patterns. To generate more controllable and precise human
motions, we propose \textbf{ProMoGen (Progressive Motion Generation)}, a novel
framework that integrates trajectory guidance with sparse anchor motion
control. Global trajectories ensure consistency in spatial direction and
displacement, while sparse anchor motions only deliver precise action guidance
without displacement. This decoupling enables independent refinement of both
aspects, resulting in a more controllable, high-fidelity, and sophisticated
motion synthesis. ProMoGen supports both dual and single control paradigms
within a unified training process. Moreover, we recognize that direct learning
from sparse motions is inherently unstable, we introduce \textbf{SAP-CL (Sparse
Anchor Posture Curriculum Learning)}, a curriculum learning strategy that
progressively adjusts the number of anchors used for guidance, thereby enabling
more precise and stable convergence. Extensive experiments demonstrate that
ProMoGen excels in synthesizing vivid and diverse motions guided by predefined
trajectory and arbitrary anchor frames. Our approach seamlessly integrates
personalized motion with structured guidance, significantly outperforming
state-of-the-art methods across multiple control scenarios.

</details>


### [117] [Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering](https://arxiv.org/abs/2504.16723)
*Ali Anaissi, Junaid Akram, Kunal Chaturvedi, Ali Braytee*

Main category: cs.CV

TL;DR: A multimodal framework for detecting hateful memes outperforms traditional unimodal and multimodal models by integrating OCR, captioning, sub-label classification, RAG, and VQA.


<details>
  <summary>Details</summary>
Motivation: Hateful memes evade detection by traditional text-only or image-only systems due to their multimodal and subtle nature.

Method: The framework uses OCR for text extraction, captioning for visual descriptions, sub-label classification for granular hate categorization, RAG for contextual retrieval, and VQA for iterative analysis.

Result: The framework achieves higher accuracy and AUC-ROC on the Facebook Hateful Memes dataset compared to unimodal and conventional multimodal models.

Conclusion: The proposed multimodal approach effectively detects hateful memes by uncovering latent signals missed by simpler methods.

Abstract: Memes are widely used for humor and cultural commentary, but they are
increasingly exploited to spread hateful content. Due to their multimodal
nature, hateful memes often evade traditional text-only or image-only detection
systems, particularly when they employ subtle or coded references. To address
these challenges, we propose a multimodal hate detection framework that
integrates key components: OCR to extract embedded text, captioning to describe
visual content neutrally, sub-label classification for granular categorization
of hateful content, RAG for contextually relevant retrieval, and VQA for
iterative analysis of symbolic and contextual cues. This enables the framework
to uncover latent signals that simpler pipelines fail to detect. Experimental
results on the Facebook Hateful Memes dataset reveal that the proposed
framework exceeds the performance of unimodal and conventional multimodal
models in both accuracy and AUC-ROC.

</details>


### [118] [V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations](https://arxiv.org/abs/2504.16727)
*Zhiyuan Fan, Yumeng Wang, Sandeep Polisetty, Yi R., Fung*

Main category: cs.CV

TL;DR: VÂ²R-Bench evaluates LVLMs' robustness to visual variations, revealing vulnerabilities in object recognition and position bias, linked to pipeline errors and poor multimodal alignment.


<details>
  <summary>Details</summary>
Motivation: Assess LVLMs' robustness to visual variations (position, scale, orientation, context) in natural scenes, which is underexplored.

Method: Introduces VÂ²R-Bench, a benchmark with automated dataset generation and metrics, tested on 21 LVLMs. Includes component-level analysis and visualization of aligned features.

Result: LVLMs show surprising vulnerability to visual variations, with position bias and human-like acuity thresholds. Issues stem from pipeline errors and inadequate multimodal alignment.

Conclusion: Architectural deficiencies in LVLMs necessitate innovations for better robustness to visual variations.

Abstract: Large Vision Language Models (LVLMs) excel in various vision-language tasks.
Yet, their robustness to visual variations in position, scale, orientation, and
context that objects in natural scenes inevitably exhibit due to changes in
viewpoint and environment remains largely underexplored. To bridge this gap, we
introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating
Visual Variation Robustness of LVLMs, which encompasses automated evaluation
dataset generation and principled metrics for thorough robustness assessment.
Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability
to visual variations, in which even advanced models that excel at complex
vision-language tasks significantly underperform on simple tasks such as object
recognition. Interestingly, these models exhibit a distinct visual position
bias that contradicts theories of effective receptive fields, and demonstrate a
human-like visual acuity threshold. To identify the source of these
vulnerabilities, we present a systematic framework for component-level
analysis, featuring a novel visualization approach for aligned visual features.
Results show that these vulnerabilities stem from error accumulation in the
pipeline architecture and inadequate multimodal alignment. Complementary
experiments with synthetic data further demonstrate that these limitations are
fundamentally architectural deficiencies, scoring the need for architectural
innovations in future LVLM designs.

</details>


### [119] [Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images](https://arxiv.org/abs/2504.16739)
*Tristan Piater, BjÃ¶rn Barz, Alexander Freytag*

Main category: cs.CV

TL;DR: PTSAM adapts SAM for non-natural domains using prompt-tuning, requiring minimal parameters and training data while matching or outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SAM's performance drops in non-natural domains like microscopy, and its interactive design is impractical for automated biomedical tasks.

Method: PTSAM uses prompt-tuning to adapt SAM with as few as 2,048 additional parameters, focusing on the mask decoder and optionally the image encoder.

Result: PTSAM matches or exceeds state-of-the-art performance with 2,000x fewer parameters and improves accuracy by up to 18% when tuning the encoder.

Conclusion: PTSAM is efficient, effective for limited data and domain shifts, and practical for biomedical applications.

Abstract: The Segment Anything Model (SAM) is widely used for segmenting a diverse
range of objects in natural images from simple user prompts like points or
bounding boxes. However, SAM's performance decreases substantially when applied
to non-natural domains like microscopic imaging. Furthermore, due to SAM's
interactive design, it requires a precise prompt for each image and object,
which is unfeasible in many automated biomedical applications. Previous
solutions adapt SAM by training millions of parameters via fine-tuning large
parts of the model or of adapter layers. In contrast, we show that as little as
2,048 additional parameters are sufficient for turning SAM into a use-case
specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM)
method uses prompt-tuning, a parameter-efficient fine-tuning technique, to
adapt SAM for a specific task. We validate the performance of our approach on
multiple microscopic and one medical dataset. Our results show that
prompt-tuning only SAM's mask decoder already leads to a performance on-par
with state-of-the-art techniques while requiring roughly 2,000x less trainable
parameters. For addressing domain gaps, we find that additionally prompt-tuning
SAM's image encoder is beneficial, further improving segmentation accuracy by
up to 18% over state-of-the-art results. Since PTSAM can be reliably trained
with as little as 16 annotated images, we find it particularly helpful for
applications with limited training data and domain shifts.

</details>


### [120] [Gaussian Splatting is an Effective Data Generator for 3D Object Detection](https://arxiv.org/abs/2504.16740)
*Farhad G. Zanjani, Davide Abati, Auke Wiggers, Dimitris Kalatzis, Jens Petersen, Hong Cai, Amirhossein Habibian*

Main category: cs.CV

TL;DR: The paper explores 3D data augmentation for autonomous driving, using Gaussian Splatting for 3D object placement, outperforming diffusion-based methods by ensuring geometric plausibility and accurate annotations.


<details>
  <summary>Details</summary>
Motivation: To improve 3D object detection in autonomous driving by addressing limitations of existing diffusion-based augmentation methods.

Method: Utilizes Gaussian Splatting for 3D reconstruction and object placement with explicit geometric transformations.

Result: Augmented data enhances detection performance, with geometric diversity proving more impactful than appearance diversity. Hard examples do not improve efficiency.

Conclusion: Direct 3D object placement with geometric transformations is superior for data augmentation in autonomous driving.

Abstract: We investigate data augmentation for 3D object detection in autonomous
driving. We utilize recent advancements in 3D reconstruction based on Gaussian
Splatting for 3D object placement in driving scenes. Unlike existing
diffusion-based methods that synthesize images conditioned on BEV layouts, our
approach places 3D objects directly in the reconstructed 3D space with
explicitly imposed geometric transformations. This ensures both the physical
plausibility of object placement and highly accurate 3D pose and position
annotations.
  Our experiments demonstrate that even by integrating a limited number of
external 3D objects into real scenes, the augmented data significantly enhances
3D object detection performance and outperforms existing diffusion-based 3D
augmentation for object detection. Extensive testing on the nuScenes dataset
reveals that imposing high geometric diversity in object placement has a
greater impact compared to the appearance diversity of objects. Additionally,
we show that generating hard examples, either by maximizing detection loss or
imposing high visual occlusion in camera images, does not lead to more
efficient 3D data augmentation for camera-based 3D object detection in
autonomous driving.

</details>


### [121] [Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery](https://arxiv.org/abs/2504.16749)
*Rupak Bose, Chinedu Innocent Nwoye, Jorge Lazo, JoÃ«l Lukas Lavanchy, Nicolas Padoy*

Main category: cs.CV

TL;DR: BetaMixer, a deep learning model, uses Beta distribution-based mixing to detect and quantify intraoperative adverse events (IAEs) on imbalanced datasets, achieving strong performance.


<details>
  <summary>Details</summary>
Motivation: IAEs like bleeding or thermal injury, if undetected, can cause severe complications. Their rarity creates imbalanced datasets, challenging AI-based detection.

Method: BetaMixer converts discrete IAE severity scores to continuous values, uses Beta distribution-based sampling, and regularizes embeddings. A generative approach aligns features with severity for classification and regression.

Result: On the MultiBypass140 dataset, BetaMixer achieved a weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84.

Conclusion: BetaMixer provides a robust solution for IAE detection and quantification in clinical settings by combining Beta distribution-based sampling, feature mixing, and generative modeling.

Abstract: Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can
lead to severe postoperative complications if undetected. However, their rarity
results in highly imbalanced datasets, posing challenges for AI-based detection
and severity quantification. We propose BetaMixer, a novel deep learning model
that addresses these challenges through a Beta distribution-based mixing
approach, converting discrete IAE severity scores into continuous values for
precise severity regression (0-5 scale). BetaMixer employs Beta
distribution-based sampling to enhance underrepresented classes and regularizes
intermediate embeddings to maintain a structured feature space. A generative
approach aligns the feature space with sampled IAE severity, enabling robust
classification and severity regression via a transformer. Evaluated on the
MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a
weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84,
demonstrating strong performance on imbalanced data. By integrating Beta
distribution-based sampling, feature mixing, and generative modeling, BetaMixer
offers a robust solution for IAE detection and quantification in clinical
settings.

</details>


### [122] [Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism](https://arxiv.org/abs/2504.16761)
*Lakshita Agarwal, Bindu Verma*

Main category: cs.CV

TL;DR: Tri-FusionNet integrates ViT, RoBERTa, and CLIP for image description generation, achieving high performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Improving image description generation for accessibility and AI understanding by leveraging advanced deep learning techniques.

Method: Combines Vision Transformer (ViT) with dual-attention, RoBERTa decoder, and CLIP for aligning visual-textual data.

Result: Competitive scores on Flickr30k, Flickr8k, and MS-COCO datasets, demonstrating high-quality descriptions.

Conclusion: Tri-FusionNet effectively generates accurate and context-rich image descriptions, validated by strong benchmark results.

Abstract: Image description generation is essential for accessibility and AI
understanding of visual content. Recent advancements in deep learning have
significantly improved natural language processing and computer vision. In this
work, we propose Tri-FusionNet, a novel image description generation model that
integrates transformer modules: a Vision Transformer (ViT) encoder module with
dual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder
module, and a Contrastive Language-Image Pre-Training (CLIP) integrating
module. The ViT encoder, enhanced with dual attention, focuses on relevant
spatial regions and linguistic context, improving image feature extraction. The
RoBERTa decoder is employed to generate precise textual descriptions. CLIP's
integrating module aligns visual and textual data through contrastive learning,
ensuring effective combination of both modalities. This fusion of ViT, RoBERTa,
and CLIP, along with dual attention, enables the model to produce more
accurate, contextually rich, and flexible descriptions. The proposed framework
demonstrated competitive performance on the Flickr30k and Flickr8k datasets,
with BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores
of 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of
0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores
of 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results
demonstrate the effectiveness of Tri-FusionNet in generating high-quality image
descriptions.

</details>


### [123] [Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation](https://arxiv.org/abs/2504.16788)
*Lakshita Agarwal, Bindu Verma*

Main category: cs.CV

TL;DR: A novel framework combines visual and textual modalities using ResNet50 and GPT-2 to generate natural language descriptions from videos, outperforming traditional methods on benchmark metrics.


<details>
  <summary>Details</summary>
Motivation: Enhancing video-based applications like intelligent monitoring and autonomous systems by producing insightful, contextualized descriptions.

Method: Uses ResNet50 for visual feature extraction, converts features into patch embeddings, and processes them with a GPT-2-based encoder-decoder model, employing multi-head self-attention and cross-attention techniques.

Result: Achieves superior performance on BLEU-4 (0.755 BDD-X, 0.778 MSVD), CIDEr (1.235 BDD-X, 1.315 MSVD), METEOR (0.312 BDD-X, 0.329 MSVD), and ROUGE-L (0.782 BDD-X, 0.795 MSVD).

Conclusion: Advances explainable AI by generating human-like, contextually relevant descriptions, improving interpretability and real-world applications.

Abstract: Understanding and analyzing video actions are essential for producing
insightful and contextualized descriptions, especially for video-based
applications like intelligent monitoring and autonomous systems. The proposed
work introduces a novel framework for generating natural language descriptions
from video datasets by combining textual and visual modalities. The suggested
architecture makes use of ResNet50 to extract visual features from video frames
that are taken from the Microsoft Research Video Description Corpus (MSVD), and
Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual
characteristics are converted into patch embeddings and then run through an
encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In
order to align textual and visual representations and guarantee high-quality
description production, the system uses multi-head self-attention and
cross-attention techniques. The model's efficacy is demonstrated by performance
evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested
framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X)
and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores
of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and
0.795 (MSVD). By producing human-like, contextually relevant descriptions,
strengthening interpretability, and improving real-world applications, this
research advances explainable AI.

</details>


### [124] [Decoupled Global-Local Alignment for Improving Compositional Understanding](https://arxiv.org/abs/2504.16801)
*Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang*

Main category: cs.CV

TL;DR: DeGLA framework improves CLIP's compositional understanding without losing general capabilities, using self-distillation and contrastive losses, achieving 3.5% and 13.0% improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: CLIP's global contrastive learning limits its ability to understand compositional concepts like relations and attributes, and existing methods compromise general capabilities.

Method: Introduces DeGLA with self-distillation for global alignment and Image-Grounded Contrast (IGC) and Text-Grounded Contrast (TGC) losses for compositional understanding, using LLMs for negative captions.

Result: Achieves 3.5% improvement on VALSE, SugarCrepe, and ARO benchmarks and 13.0% on zero-shot classification tasks.

Conclusion: DeGLA effectively balances compositional understanding and general capabilities, outperforming prior methods.

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved success on
multiple downstream tasks by aligning image and text modalities. However, the
nature of global contrastive learning limits CLIP's ability to comprehend
compositional concepts, such as relations and attributes. Although recent
studies employ global hard negative samples to improve compositional
understanding, these methods significantly compromise the model's inherent
general capabilities by forcibly distancing textual negative samples from
images in the embedding space. To overcome this limitation, we introduce a
Decoupled Global-Local Alignment (DeGLA) framework that improves compositional
understanding while substantially mitigating losses in general capabilities. To
optimize the retention of the model's inherent capabilities, we incorporate a
self-distillation mechanism within the global alignment process, aligning the
learnable image-text encoder with a frozen teacher model derived from an
exponential moving average. Under the constraint of self-distillation, it
effectively mitigates the catastrophic forgetting of pretrained knowledge
during fine-tuning. To improve compositional understanding, we first leverage
the in-context learning capability of Large Language Models (LLMs) to construct
about 2M high-quality negative captions across five types. Subsequently, we
propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC)
loss to enhance vision-language compositionally. Extensive experimental results
demonstrate the effectiveness of the DeGLA framework. Compared to previous
state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across
the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average
performance improvement of 13.0% on zero-shot classification tasks across
eleven datasets. Our code will be released at
https://github.com/xiaoxing2001/DeGLA

</details>


### [125] [A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping](https://arxiv.org/abs/2504.16840)
*Joe Hrzich, Michael A. Beck, Christopher P. Bidinosti, Christopher J. Henry, Kalhari Manawasinghe, Karen Tanino*

Main category: cs.CV

TL;DR: An open-source, low-cost photogrammetry system for 3D plant modeling and phenotyping, demonstrated with wheat, enabling easy computation of phenotypic traits and canopy architecture classification.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible and cost-effective solution for 3D plant modeling and phenotyping, addressing the need for efficient measurement of phenotypic traits in plants like wheat.

Method: Uses a structure-from-motion approach to generate 3D point clouds of plants, from which phenotypic traits (e.g., height, leaf angles) and canopy architecture metrics are computed.

Result: Successfully demonstrated the system's ability to compute standard and complex phenotypic traits, and classify wheat canopy architectures (erectophile vs. planophile).

Conclusion: The system offers a practical, low-cost tool for plant phenotyping, with potential applications in agricultural research and breeding.

Abstract: We present an open-source, low-cost photogrammetry system for 3D plant
modeling and phenotyping. The system uses a structure-from-motion approach to
reconstruct 3D representations of the plants via point clouds. Using wheat as
an example, we demonstrate how various phenotypic traits can be computed easily
from the point clouds. These include standard measurements such as plant height
and radius, as well as features that would be more cumbersome to measure by
hand, such as leaf angles and convex hull. We further demonstrate the utility
of the system through the investigation of specific metrics that may yield
objective classifications of erectophile versus planophile wheat canopy
architectures.

</details>


### [126] [Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space](https://arxiv.org/abs/2504.16851)
*Ruben Gonzalez AvilÃ©s, Linus Scheibenreif, Nassim Ait Ali Braham, Benedikt Blumenstiel, Thomas Brunschwiler, Ranjini Guruprasad, Damian Borth, Conrad Albrecht, Paolo Fraccaro, Devyani Lambhate, Johannes Jakubik*

Main category: cs.CV

TL;DR: A spectral transformer model synthesizes hyperspectral data from multispectral inputs, improving GHG monitoring by combining spectral detail with broader coverage.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging's limited spatial and temporal coverage restricts GHG monitoring, while multispectral lacks spectral detail. This study aims to bridge this gap.

Method: A spectral transformer model is pre-trained with a band-wise masked autoencoder and fine-tuned on aligned multispectral-hyperspectral pairs.

Result: Synthetic hyperspectral data retain multispectral coverage benefits and enhance GHG prediction accuracy.

Conclusion: The model bridges the spectral-coverage trade-off, advancing atmospheric monitoring by leveraging self-supervised deep learning.

Abstract: Hyperspectral imaging provides detailed spectral information and holds
significant potential for monitoring of greenhouse gases (GHGs). However, its
application is constrained by limited spatial coverage and infrequent revisit
times. In contrast, multispectral imaging offers broader spatial and temporal
coverage but often lacks the spectral detail that can enhance GHG detection. To
address these challenges, this study proposes a spectral transformer model that
synthesizes hyperspectral data from multispectral inputs. The model is
pre-trained via a band-wise masked autoencoder and subsequently fine-tuned on
spatio-temporally aligned multispectral-hyperspectral image pairs. The
resulting synthetic hyperspectral data retain the spatial and temporal benefits
of multispectral imagery and improve GHG prediction accuracy relative to using
multispectral data alone. This approach effectively bridges the trade-off
between spectral resolution and coverage, highlighting its potential to advance
atmospheric monitoring by combining the strengths of hyperspectral and
multispectral systems with self-supervised deep learning.

</details>


### [127] [High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data](https://arxiv.org/abs/2504.16870)
*Chenxi Duan*

Main category: cs.CV

TL;DR: CRSynthNet, a novel image synthesis network, addresses missing optical data due to cloud cover and long satellite revisit cycles, outperforming existing methods in accuracy and visual quality.


<details>
  <summary>Details</summary>
Motivation: To overcome gaps in optical data caused by cloud cover and infrequent satellite revisits, which hinder remote sensing applications.

Method: Proposes CRSynthNet with DownUp Block and Fusion Attention modules for enhanced accuracy in synthesizing missing data.

Result: Achieves PSNR of 26.978, SSIM of 0.648, and RMSE of 0.050, with superior structural and spectral preservation. Introduces the TCSEN12 dataset for realistic cloud cover scenarios.

Conclusion: CRSynthNet and the TCSEN12 dataset provide practical solutions and resources for optical satellite image synthesis.

Abstract: Addressing gaps caused by cloud cover and the long revisit cycle of
satellites is vital for providing essential data to support remote sensing
applications. This paper tackles the challenges of missing optical data
synthesis, particularly in complex scenarios with cloud cover. We propose
CRSynthNet, a novel image synthesis network that incorporates innovative
designed modules such as the DownUp Block and Fusion Attention to enhance
accuracy. Experimental results validate the effectiveness of CRSynthNet,
demonstrating substantial improvements in restoring structural details,
preserving spectral consist, and achieving superior visual effects that far
exceed those produced by comparison methods. It achieves quantitative
improvements across multiple metrics: a peak signal-to-noise ratio (PSNR) of
26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean
square error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12
dataset, a valuable resource specifically designed to address cloud cover
challenges in missing optical data synthesis study. The dataset uniquely
includes cloud-covered images and leverages earlier image to predict later
image, offering a realistic representation of real-world scenarios. This study
offer practical method and valuable resources for optical satellite image
synthesis task.

</details>


### [128] [BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation](https://arxiv.org/abs/2504.16907)
*Ruotong Wang, Mingli Zhu, Jiarong Ou, Rui Chen, Xin Tao, Pengfei Wan, Baoyuan Wu*

Main category: cs.CV

TL;DR: BadVideo introduces a backdoor attack framework for text-to-video models, exploiting redundancy to embed hidden harmful content while evading detection.


<details>
  <summary>Details</summary>
Motivation: To explore adversarial vulnerabilities in T2V models, which often generate redundant information not specified in text prompts, creating opportunities for malicious attacks.

Method: Uses Spatio-Temporal Composition and Dynamic Element Transformation to encode and convey malicious information in generated videos.

Result: Achieves high attack success rates, maintains original semantics, and evades traditional content moderation systems.

Conclusion: Reveals vulnerabilities in T2V models, highlighting risks and potential misuse.

Abstract: Text-to-video (T2V) generative models have rapidly advanced and found
widespread applications across fields like entertainment, education, and
marketing. However, the adversarial vulnerabilities of these models remain
rarely explored. We observe that in T2V generation tasks, the generated videos
often contain substantial redundant information not explicitly specified in the
text prompts, such as environmental elements, secondary objects, and additional
details, providing opportunities for malicious attackers to embed hidden
harmful content. Exploiting this inherent redundancy, we introduce BadVideo,
the first backdoor attack framework tailored for T2V generation. Our attack
focuses on designing target adversarial outputs through two key strategies: (1)
Spatio-Temporal Composition, which combines different spatiotemporal features
to encode malicious information; (2) Dynamic Element Transformation, which
introduces transformations in redundant elements over time to convey malicious
information. Based on these strategies, the attacker's malicious target
seamlessly integrates with the user's textual instructions, providing high
stealthiness. Moreover, by exploiting the temporal dimension of videos, our
attack successfully evades traditional content moderation systems that
primarily analyze spatial information within individual frames. Extensive
experiments demonstrate that BadVideo achieves high attack success rates while
preserving original semantics and maintaining excellent performance on clean
inputs. Overall, our work reveals the adversarial vulnerability of T2V models,
calling attention to potential risks and misuse. Our project page is at
https://wrt2000.github.io/BadVideo2025/.

</details>


### [129] [DreamO: A Unified Framework for Image Customization](https://arxiv.org/abs/2504.16915)
*Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, Mengtian Li, Songtao Zhao, Jian Zhang, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: DreamO is a unified image customization framework using a diffusion transformer (DiT) to handle diverse tasks and integrate multiple conditions, achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: Existing image customization methods are task-specific, lacking generalizability. DreamO aims to unify diverse customization tasks and conditions.

Method: DreamO uses a DiT framework, feature routing constraints, placeholder strategies, and a three-stage progressive training approach (baseline, full-scale, quality alignment).

Result: DreamO effectively performs various customization tasks and integrates multiple conditions with high quality.

Conclusion: DreamO provides a flexible, high-quality solution for unified image customization, addressing the limitations of task-specific approaches.

Abstract: Recently, extensive research on image customization (e.g., identity, subject,
style, background, etc.) demonstrates strong customization capabilities in
large-scale generative models. However, most approaches are designed for
specific tasks, restricting their generalizability to combine different types
of condition. Developing a unified framework for image customization remains an
open challenge. In this paper, we present DreamO, an image customization
framework designed to support a wide range of tasks while facilitating seamless
integration of multiple conditions. Specifically, DreamO utilizes a diffusion
transformer (DiT) framework to uniformly process input of different types.
During training, we construct a large-scale training dataset that includes
various customization tasks, and we introduce a feature routing constraint to
facilitate the precise querying of relevant information from reference images.
Additionally, we design a placeholder strategy that associates specific
placeholders with conditions at particular positions, enabling control over the
placement of conditions in the generated results. Moreover, we employ a
progressive training strategy consisting of three stages: an initial stage
focused on simple tasks with limited data to establish baseline consistency, a
full-scale training stage to comprehensively enhance the customization
capabilities, and a final quality alignment stage to correct quality biases
introduced by low-quality data. Extensive experiments demonstrate that the
proposed DreamO can effectively perform various image customization tasks with
high quality and flexibly integrate different types of control conditions.

</details>


### [130] [Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light](https://arxiv.org/abs/2504.16922)
*Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, Qinsheng Zhang, Bing Xu, Haicheng Wu, Wen-mei Hwu, Ming-Yu Liu, Humphrey Shi*

Main category: cs.CV

TL;DR: The paper introduces Generalized Neighborhood Attention (GNA) to address inefficiencies in sparse attention mechanisms, providing realistic speedup simulations and achieving significant performance improvements on AI hardware.


<details>
  <summary>Details</summary>
Motivation: Existing sparse attention mechanisms fail to deliver consistent speedups due to complex infrastructure and evolving hardware, while foundational models remain bottlenecked by attention's O(n^2) complexity.

Method: Develops GNA to model locality-based sparse attention, designs a simulator for realistic speedup bounds, and implements GNA on NVIDIA Blackwell architecture using CUTLASS.

Result: Achieves 1.3 petaFLOPs/second in FP16 and 28%-46% end-to-end speedup on generative models without fine-tuning.

Conclusion: GNA effectively addresses sparse attention inefficiencies, offering practical speedups and open-sourcing tools for broader adoption.

Abstract: Many sparse attention mechanisms such as Neighborhood Attention have
typically failed to consistently deliver speedup over the self attention
baseline. This is largely due to the level of complexity in attention
infrastructure, and the rapid evolution of AI hardware architecture. At the
same time, many state-of-the-art foundational models, particularly in computer
vision, are heavily bound by attention, and need reliable sparsity to escape
the O(n^2) complexity. In this paper, we study a class of promising sparse
attention mechanisms that focus on locality, and aim to develop a better
analytical model of their performance improvements. We first introduce
Generalized Neighborhood Attention (GNA), which can describe sliding window,
strided sliding window, and blocked attention. We then consider possible design
choices in implementing these approaches, and create a simulator that can
provide much more realistic speedup upper bounds for any given setting.
Finally, we implement GNA on top of a state-of-the-art fused multi-headed
attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in
CUTLASS. Our implementation can fully realize the maximum speedup theoretically
possible in many perfectly block-sparse cases, and achieves an effective
utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA
configurations into off-the-shelf generative models, such as Cosmos-7B,
HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end
speedup on B200 without any fine-tuning. We will open source our simulator and
Blackwell kernels directly through the NATTEN project.

</details>


### [131] [Procedural Dataset Generation for Zero-Shot Stereo Matching](https://arxiv.org/abs/2504.16930)
*David Yan, Alexander Raistrick, Jia Deng*

Main category: cs.CV

TL;DR: The paper explores the design of synthetic datasets for stereo matching, introduces Infinigen-Stereo, and shows its superior zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: To understand what makes synthetic stereo datasets effective for training networks.

Method: Vary parameters of a procedural dataset generator and evaluate zero-shot performance.

Result: Infinigen-Stereo outperforms baselines and prior works in zero-shot stereo matching.

Conclusion: The optimized procedural generator, Infinigen-Stereo, improves zero-shot performance and is open-sourced for further research.

Abstract: Synthetic datasets are a crucial ingredient for training stereo matching
networks, but the question of what makes a stereo dataset effective remains
largely unexplored. We investigate the design space of synthetic datasets by
varying the parameters of a procedural dataset generator, and report the
effects on zero-shot stereo matching performance using standard benchmarks. We
collect the best settings to produce Infinigen-Stereo, a procedural generator
specifically optimized for zero-shot stereo datasets. Models trained only on
data from our system outperform robust baselines trained on a combination of
existing synthetic datasets and have stronger zero-shot stereo matching
performance than public checkpoints from prior works. We open source our system
at https://github.com/princeton-vl/InfinigenStereo to enable further research
on procedural stereo datasets.

</details>


### [132] [Co-domain Symmetry for Complex-Valued Deep Learning](https://arxiv.org/abs/2112.01525)
*Utkarsh Singhal, Yifei Xing, Stella X. Yu*

Main category: cs.CV

TL;DR: The paper introduces co-domain symmetric (CDS) classifiers to address complex-valued scaling, outperforming existing methods in accuracy, generalization, and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Deep Complex Networks (DCN) and SurReal fail to fully exploit complex-valued scaling, either ignoring it or losing rich information.

Method: The authors analyze complex-valued scaling as a co-domain transformation and design equivariant and invariant neural network layers. They also propose novel complex-valued RGB image representations.

Result: CDS classifiers achieve higher accuracy, better generalization, robustness, and lower bias/variance than DCN and SurReal on benchmarks like MSTAR, CIFAR10, CIFAR100, and SVHN.

Conclusion: The proposed CDS classifiers effectively leverage complex-valued scaling, offering superior performance with fewer parameters.

Abstract: We study complex-valued scaling as a type of symmetry natural and unique to
complex-valued measurements and representations. Deep Complex Networks (DCN)
extends real-valued algebra to the complex domain without addressing
complex-valued scaling. SurReal takes a restrictive manifold view of complex
numbers, adopting a distance metric to achieve complex-scaling invariance while
losing rich complex-valued information. We analyze complex-valued scaling as a
co-domain transformation and design novel equivariant and invariant neural
network layer functions for this special transformation. We also propose novel
complex-valued representations of RGB images, where complex-valued scaling
indicates hue shift or correlated changes across color channels. Benchmarked on
MSTAR, CIFAR10, CIFAR100, and SVHN, our co-domain symmetric (CDS) classifiers
deliver higher accuracy, better generalization, robustness to co-domain
transformations, and lower model bias and variance than DCN and SurReal with
far fewer parameters.

</details>


### [133] [HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models](https://arxiv.org/abs/2312.04867)
*Pei Lin, Sihang Xu, Hongdi Yang, Yiran Liu, Xin Chen, Jingya Wang, Jingyi Yu, Lan Xu*

Main category: cs.CV

TL;DR: HandDiffuse12.5M is a large-scale dataset for two-hand interactions, addressing data scarcity. The HandDiffuse method, using diffusion models and Interaction Loss, outperforms state-of-the-art in motion generation.


<details>
  <summary>Details</summary>
Motivation: Existing hand datasets lack strong interactions due to self-occlusion and self-similarity, limiting motion generation.

Method: Proposes HandDiffuse12.5M dataset and HandDiffuse method with diffusion models, two motion representations, and Interaction Loss.

Result: Outperforms state-of-the-art in motion generation and aids data augmentation.

Conclusion: HandDiffuse enables vivid two-hand interactions; dataset and models will be shared for future research.

Abstract: Existing hands datasets are largely short-range and the interaction is weak
due to the self-occlusion and self-similarity of hands, which can not yet fit
the need for interacting hands motion generation. To rescue the data scarcity,
we propose HandDiffuse12.5M, a novel dataset that consists of temporal
sequences with strong two-hand interactions. HandDiffuse12.5M has the largest
scale and richest interactions among the existing two-hand datasets. We further
present a strong baseline method HandDiffuse for the controllable motion
generation of interacting hands using various controllers. Specifically, we
apply the diffusion model as the backbone and design two motion representations
for different controllers. To reduce artifacts, we also propose Interaction
Loss which explicitly quantifies the dynamic interaction process. Our
HandDiffuse enables various applications with vivid two-hand interactions,
i.e., motion in-betweening and trajectory control. Experiments show that our
method outperforms the state-of-the-art techniques in motion generation and can
also contribute to data augmentation for other datasets. Our dataset,
corresponding codes, and pre-trained models will be disseminated to the
community for future research towards two-hand interaction modeling.

</details>


### [134] [Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer](https://arxiv.org/abs/2404.03819)
*Yirui Wang, Qinji Yu, Ke Yan, Haoshen Li, Dazhou Guo, Li Zhang, Le Lu, Na Shen, Qifeng Wang, Xiaowei Ding, Xianghua Ye, Dakai Jin*

Main category: cs.CV

TL;DR: LN-DETR, a new transformer-based method, improves lymph node detection in 3D CT scans by enhancing feature fusion and query selection, achieving higher recall and lower false positives.


<details>
  <summary>Details</summary>
Motivation: Accurate lymph node assessment is crucial for cancer diagnosis and treatment but is challenging due to low contrast and scatter in 3D CT scans, with existing methods suffering from high false positives and low recall.

Method: LN-DETR enhances a 2D backbone with multi-scale 2.5D feature fusion, introduces an IoU prediction head, location-debiased query selection, and query contrastive learning to improve LN query representation.

Result: The method improves average recall by 4-5% at the same false positive rates in internal and external testing and achieves top performance on the NIH DeepLesion benchmark (88.46% recall).

Conclusion: LN-DETR significantly advances lymph node detection, offering a robust solution for clinical workflows.

Abstract: Lymph node (LN) assessment is a critical, indispensable yet very challenging
task in the routine clinical workflow of radiology and oncology. Accurate LN
analysis is essential for cancer diagnosis, staging, and treatment planning.
Finding scatteredly distributed, low-contrast clinically relevant LNs in 3D CT
is difficult even for experienced physicians under high inter-observer
variations. Previous automatic LN detection works typically yield limited
recall and high false positives (FPs) due to adjacent anatomies with similar
image intensities, shapes, or textures (vessels, muscles, esophagus, etc). In
this work, we propose a new LN DEtection TRansformer, named LN-DETR, to achieve
more accurate performance. By enhancing the 2D backbone with a multi-scale 2.5D
feature fusion to incorporate 3D context explicitly, more importantly, we make
two main contributions to improve the representation quality of LN queries. 1)
Considering that LN boundaries are often unclear, an IoU prediction head and a
location debiased query selection are proposed to select LN queries of higher
localization accuracy as the decoder query's initialization. 2) To reduce FPs,
query contrastive learning is employed to explicitly reinforce LN queries
towards their best-matched ground-truth queries over unmatched query
predictions. Trained and tested on 3D CT scans of 1067 patients (with 10,000+
labeled LNs) via combining seven LN datasets from different body parts (neck,
chest, and abdomen) and pathologies/cancers, our method significantly improves
the performance of previous leading methods by > 4-5% average recall at the
same FP rates in both internal and external testing. We further evaluate on the
universal lesion detection task using NIH DeepLesion benchmark, and our method
achieves the top performance of 88.46% averaged recall across 0.5 to 4 FPs per
image, compared with other leading reported results.

</details>


### [135] [Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](https://arxiv.org/abs/2406.00622)
*Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, Alan Yuille*

Main category: cs.CV

TL;DR: DynSuperCLEVR is a new video QA dataset focusing on 4D dynamics (velocity, acceleration, collisions). NS-4DPhysics, a neural-symbolic model with physics priors, outperforms existing models on this task.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack 3D/4D scene understanding for dynamic object properties, limiting high-level reasoning. DynSuperCLEVR addresses this gap.

Method: Introduces DynSuperCLEVR dataset and NS-4DPhysics model, which estimates 4D world states using a 3D generative model and neural-symbolic reasoning.

Result: NS-4DPhysics outperforms existing models on DynSuperCLEVR, especially in 4D dynamics questions.

Conclusion: Explicit 4D scene representation and physics priors enhance dynamic property understanding in VLMs.

Abstract: For vision-language models (VLMs), understanding the dynamic properties of
objects and their interactions in 3D scenes from videos is crucial for
effective reasoning about high-level temporal and action semantics. Although
humans are adept at understanding these properties by constructing 3D and
temporal (4D) representations of the world, current video understanding models
struggle to extract these dynamic semantics, arguably because these models use
cross-frame reasoning without underlying knowledge of the 3D/4D scenes. In this
work, we introduce DynSuperCLEVR, the first video question answering dataset
that focuses on language understanding of the dynamic properties of 3D objects.
We concentrate on three physical concepts -- velocity, acceleration, and
collisions within 4D scenes. We further generate three types of questions,
including factual queries, future predictions, and counterfactual reasoning
that involve different aspects of reasoning about these 4D dynamic properties.
To further demonstrate the importance of explicit scene representations in
answering these 4D dynamics questions, we propose NS-4DPhysics, a
Neural-Symbolic VideoQA model integrating Physics prior for 4D dynamic
properties with explicit scene representation of videos. Instead of answering
the questions directly from the video text input, our method first estimates
the 4D world states with a 3D generative model powered by physical priors, and
then uses neural symbolic reasoning to answer the questions based on the 4D
world states. Our evaluation on all three types of questions in DynSuperCLEVR
shows that previous video question answering models and large multimodal models
struggle with questions about 4D dynamics, while our NS-4DPhysics significantly
outperforms previous state-of-the-art models. Our code and data are released in
https://xingruiwang.github.io/projects/DynSuperCLEVR/.

</details>


### [136] [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203)
*Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, Suorong Yang, Jianbo Wu, Peng Ye, Wanli Ouyang, Dongzhan Zhou*

Main category: cs.CV

TL;DR: Critic-V is a framework enhancing VLMs by decoupling reasoning and critique processes, using a Reasoner and Critic to refine responses, outperforming GPT-4V on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address inaccuracies and irrelevance in VLMs' responses due to hallucinated understandings or unrefined reasoning.

Method: Integrates Reasoner (generates reasoning paths) and Critic (provides critiques) in a reinforcement learning framework with DPO-trained Critic.

Result: Outperforms GPT-4V on 5/8 benchmarks, improving reasoning accuracy and efficiency.

Conclusion: Critic-V enhances VLM reliability, promising for real-world multimodal applications like autonomous driving.

Abstract: Vision-language models (VLMs) have shown remarkable advancements in
multimodal reasoning tasks. However, they still often generate inaccurate or
irrelevant responses due to issues like hallucinated image understandings or
unrefined reasoning paths. To address these challenges, we introduce Critic-V,
a novel framework inspired by the Actor-Critic paradigm to boost the reasoning
capability of VLMs. This framework decouples the reasoning process and critic
process by integrating two independent components: the Reasoner, which
generates reasoning paths based on visual and textual inputs, and the Critic,
which provides constructive critique to refine these paths. In this approach,
the Reasoner generates reasoning responses according to text prompts, which can
evolve iteratively as a policy based on feedback from the Critic. This
interaction process was theoretically driven by a reinforcement learning
framework where the Critic offers natural language critiques instead of scalar
rewards, enabling more nuanced feedback to boost the Reasoner's capability on
complex reasoning tasks. The Critic model is trained using Direct Preference
Optimization (DPO), leveraging a preference dataset of critiques ranked by
Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results
show that the Critic-V framework significantly outperforms existing methods,
including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning
accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner
and constructive feedback from the preference-optimized Critic enables a more
reliable and context-sensitive multimodal reasoning process. Our approach
provides a promising solution to enhance the reliability of VLMs, improving
their performance in real-world reasoning-heavy multimodal applications such as
autonomous driving and embodied intelligence.

</details>


### [137] [P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds](https://arxiv.org/abs/2407.05238)
*Jiahao Nie, Fei Xie, Sifan Zhou, Xueyi Zhou, Dong-Kyu Chae, Zhiwei He*

Main category: cs.CV

TL;DR: The paper introduces P2P, a novel 3D single object tracking framework that uses part-to-part motion modeling for consecutive point clouds, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from insufficient appearance information in LiDAR point clouds or complex multi-stage processing in motion paradigms.

Method: P2P performs part-to-part motion modeling between consecutive point clouds, proposing P2P-point and P2P-voxel models for implicit and explicit motion modeling.

Result: P2P-voxel achieves 89%, 72%, and 63% precision on KITTI, NuScenes, and Waymo datasets, while P2P-point outperforms M$^2$Track by 3.3% and 6.7% on KITTI and NuScenes, running at 107 Fps.

Conclusion: P2P effectively addresses limitations of appearance and motion paradigms, offering high performance and efficiency in 3D tracking.

Abstract: 3D single object tracking (SOT) methods based on appearance matching has long
suffered from insufficient appearance information incurred by incomplete,
textureless and semantically deficient LiDAR point clouds. While motion
paradigm exploits motion cues instead of appearance matching for tracking, it
incurs complex multi-stage processing and segmentation module. In this paper,
we first provide in-depth explorations on motion paradigm, which proves that
(\textbf{i}) it is feasible to directly infer target relative motion from point
clouds across consecutive frames; (\textbf{ii}) fine-grained information
comparison between consecutive point clouds facilitates target motion modeling.
We thereby propose to perform part-to-part motion modeling for consecutive
point clouds and introduce a novel tracking framework, termed \textbf{P2P}. The
novel framework fuses each corresponding part information between consecutive
point clouds, effectively exploring detailed information changes and thus
modeling accurate target-related motion cues. Following this framework, we
present P2P-point and P2P-voxel models, incorporating implicit and explicit
part-to-part motion modeling by point- and voxel-based representation,
respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art
performance ($\sim$\textbf{89\%}, \textbf{72\%} and \textbf{63\%} precision on
KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same
point-based representation, P2P-point outperforms the previous motion tracker
M$^2$Track by \textbf{3.3\%} and \textbf{6.7\%} on the KITTI and NuScenes,
while running at a considerably high speed of \textbf{107 Fps} on a single
RTX3090 GPU. The source code and pre-trained models are available at
https://github.com/haooozi/P2P.

</details>


### [138] [DiffArtist: Towards Structure and Appearance Controllable Image Stylization](https://arxiv.org/abs/2407.15842)
*Ruixiang Jiang, Changwen Chen*

Main category: cs.CV

TL;DR: DiffArtist is a novel method for simultaneously stylizing both structure and appearance in 2D images, using separate diffusion processes for disentanglement and control. It includes a Multimodal LLM-based evaluator for better alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing neural stylization techniques focus mainly on appearance features (color, texture) and neglect structural stylization, limiting creative control.

Method: DiffArtist represents structure and appearance as separate diffusion processes, enabling disentanglement without training. It also introduces a Multimodal LLM-based evaluator for semantic understanding.

Result: DiffArtist achieves superior style fidelity, editability, and structure-appearance disentanglement, outperforming existing methods.

Conclusion: DiffArtist is a versatile solution for creative applications, offering unprecedented control over both structure and appearance in stylization.

Abstract: Artistic style includes both structural and appearance elements. Existing
neural stylization techniques primarily focus on transferring appearance
features such as color and texture, often neglecting the equally crucial aspect
of structural stylization. In this paper, we present a comprehensive study on
the simultaneous stylization of structure and appearance of 2D images.
Specifically, we introduce DiffArtist, which, to the best of our knowledge, is
the first stylization method to allow for dual controllability over structure
and appearance. Our key insight is to represent structure and appearance as
separate diffusion processes to achieve complete disentanglement without
requiring any training, thereby endowing users with unprecedented
controllability for both components. The evaluation of stylization of both
appearance and structure, however, remains challenging as it necessitates
semantic understanding. To this end, we further propose a Multimodal LLM-based
style evaluator, which better aligns with human preferences than metrics
lacking semantic understanding. With this powerful evaluator, we conduct
extensive analysis, demonstrating that DiffArtist achieves superior style
fidelity, editability, and structure-appearance disentanglement. These merits
make DiffArtist a highly versatile solution for creative applications. Project
homepage: https://github.com/songrise/Artist.

</details>


### [139] [PooDLe: Pooled and dense self-supervised learning from naturalistic videos](https://arxiv.org/abs/2408.11208)
*Alex N. Wang, Christopher Hoang, Yuwen Xiong, Yann LeCun, Mengye Ren*

Main category: cs.CV

TL;DR: PooDLe combines invariance and dense SSL objectives for self-supervised learning from naturalistic videos, showing effectiveness on BDD100K and Walking Tours datasets.


<details>
  <summary>Details</summary>
Motivation: Address gaps in self-supervised learning for naturalistic videos with dense scenes, imbalanced classes, and varying object sizes.

Method: Proposes PooDLe, combining invariance-based and dense SSL objectives at multiple feature scales.

Result: Unified multi-scale objective improves image representation learning, validated on BDD100K and Walking Tours datasets.

Conclusion: PooDLe effectively captures spatial and semantic understanding from naturalistic videos.

Abstract: Self-supervised learning has driven significant progress in learning from
single-subject, iconic images. However, there are still unanswered questions
about the use of minimally-curated, naturalistic video data, which contain
dense scenes with many independent objects, imbalanced class distributions, and
varying object sizes. In this paper, we propose PooDLe, a self-supervised
learning method that combines an invariance-based objective on pooled
representations with a dense SSL objective that enforces equivariance to
optical flow warping. Our results show that a unified objective applied at
multiple feature scales is essential for learning effective image
representations from naturalistic videos. We validate our method with
experiments on the BDD100K driving video dataset and the Walking Tours
first-person video dataset, demonstrating its ability to capture spatial
understanding from a dense objective and semantic understanding via a pooled
representation objective.

</details>


### [140] [Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image Translation](https://arxiv.org/abs/2409.16706)
*Youngwan Jin, Incheol Park, Hanbin Song, Hyeongjin Ju, Yagiz Nalcakan, Shiho Kim*

Main category: cs.CV

TL;DR: Pix2Next is a novel framework for RGB-to-NIR image translation using a Vision Foundation Model and cross-attention, outperforming existing methods by 34.81% in FID score and enhancing downstream tasks like object detection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating high-quality NIR images from RGB inputs to scale NIR datasets without additional data acquisition.

Method: Uses a Vision Foundation Model in an encoder-decoder architecture with cross-attention and multi-scale PatchGAN discriminator, coupled with specialized loss functions.

Result: Achieves a 34.81% improvement in FID score and enhances performance in downstream tasks like object detection.

Conclusion: Pix2Next effectively scales NIR datasets and advances NIR-based computer vision applications without extra data efforts.

Abstract: This paper proposes Pix2Next, a novel image-to-image translation framework
designed to address the challenge of generating high-quality Near-Infrared
(NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision
Foundation Model (VFM) within an encoder-decoder architecture, incorporating
cross-attention mechanisms to enhance feature integration. This design captures
detailed global representations and preserves essential spectral
characteristics, treating RGB-to-NIR translation as more than a simple domain
transfer problem. A multi-scale PatchGAN discriminator ensures realistic image
generation at various detail levels, while carefully designed loss functions
couple global context understanding with local feature preservation. We
performed experiments on the RANUS dataset to demonstrate Pix2Next's advantages
in quantitative metrics and visual quality, improving the FID score by 34.81%
compared to existing methods. Furthermore, we demonstrate the practical utility
of Pix2Next by showing improved performance on a downstream object detection
task using generated NIR data to augment limited real NIR datasets. The
proposed approach enables the scaling up of NIR datasets without additional
data acquisition or annotation efforts, potentially accelerating advancements
in NIR-based computer vision applications.

</details>


### [141] [Semantic Segmentation and Scene Reconstruction of RGB-D Image Frames: An End-to-End Modular Pipeline for Robotic Applications](https://arxiv.org/abs/2410.17988)
*Zhiwu Zheng, Lauren Mentzer, Berk Iskender, Michael Price, Colm Prendergast, Audren Cloitre*

Main category: cs.CV

TL;DR: A novel end-to-end modular pipeline integrates semantic segmentation, human tracking, and scene reconstruction to improve robotic perception in unstructured environments, achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Robots need both geometric and semantic understanding of unstructured environments, but traditional RGB-D pipelines lack generalized methods for semantic segmentation while maintaining geometric accuracy.

Method: The pipeline combines SAM2 for semantic segmentation, a hybrid method for sharper masks, human tracking with re-identification, and point-cloud fusion leveraging semantics for faster computation.

Result: Improved semantic segmentation (mIoU 47.0%), precise object boundaries, 1.81x faster point cloud fusion, and 25.3 mm mean reconstruction error. Validated on benchmark and real-world data.

Conclusion: The pipeline enhances robotic perception with efficient, accurate, and practical structured representations in USD format, suitable for real-world deployment.

Abstract: Robots operating in unstructured environments require a comprehensive
understanding of their surroundings, necessitating geometric and semantic
information from sensor data. Traditional RGB-D processing pipelines focus
primarily on geometric reconstruction, limiting their ability to support
advanced robotic perception, planning, and interaction. A key challenge is the
lack of generalized methods for segmenting RGB-D data into semantically
meaningful components while maintaining accurate geometric representations. We
introduce a novel end-to-end modular pipeline that integrates state-of-the-art
semantic segmentation, human tracking, point-cloud fusion, and scene
reconstruction. Our approach improves semantic segmentation accuracy by
leveraging the foundational segmentation model SAM2 with a hybrid method that
combines its mask generation with a semantic classification model, resulting in
sharper masks and high classification accuracy. Compared to SegFormer and
OneFormer, our method achieves a similar semantic segmentation accuracy (mIoU
of 47.0% vs 45.9% in the ADE20K dataset) but provides much more precise object
boundaries. Additionally, our human tracking algorithm interacts with the
segmentation enabling continuous tracking even when objects leave and re-enter
the frame by object re-identification. Our point cloud fusion approach reduces
computation time by 1.81x while maintaining a small mean reconstruction error
of 25.3 mm by leveraging the semantic information. We validate our approach on
benchmark datasets and real-world Kinect RGB-D data, demonstrating improved
efficiency, accuracy, and usability. Our structured representation, stored in
the Universal Scene Description (USD) format, supports efficient querying,
visualization, and robotic simulation, making it practical for real-world
deployment.

</details>


### [142] [OSDFace: One-Step Diffusion Model for Face Restoration](https://arxiv.org/abs/2411.17163)
*Jingkai Wang, Jue Gong, Lin Zhang, Zheng Chen, Xing Liu, Hong Gu, Yutong Liu, Yulun Zhang, Xiaokang Yang*

Main category: cs.CV

TL;DR: OSDFace is a one-step diffusion model for face restoration, improving efficiency and identity consistency using a visual representation embedder and GAN guidance.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for face restoration are computationally intensive and struggle with identity consistency and realism.

Method: Proposes OSDFace with a visual representation embedder (VRE) for prior capture, facial identity loss, and GAN guidance for distribution alignment.

Result: OSDFace outperforms SOTA methods in visual quality and quantitative metrics, producing high-fidelity, identity-consistent faces.

Conclusion: OSDFace offers an efficient, high-quality solution for face restoration, balancing realism and identity consistency.

Abstract: Diffusion models have demonstrated impressive performance in face
restoration. Yet, their multi-step inference process remains computationally
intensive, limiting their applicability in real-world scenarios. Moreover,
existing methods often struggle to generate face images that are harmonious,
realistic, and consistent with the subject's identity. In this work, we propose
OSDFace, a novel one-step diffusion model for face restoration. Specifically,
we propose a visual representation embedder (VRE) to better capture prior
information and understand the input face. In VRE, low-quality faces are
processed by a visual tokenizer and subsequently embedded with a
vector-quantized dictionary to generate visual prompts. Additionally, we
incorporate a facial identity loss derived from face recognition to further
ensure identity consistency. We further employ a generative adversarial network
(GAN) as a guidance model to encourage distribution alignment between the
restored face and the ground truth. Experimental results demonstrate that
OSDFace surpasses current state-of-the-art (SOTA) methods in both visual
quality and quantitative metrics, generating high-fidelity, natural face images
with high identity consistency. The code and model will be released at
https://github.com/jkwang28/OSDFace.

</details>


### [143] [GFreeDet: Exploiting Gaussian Splatting and Foundation Models for Model-free Unseen Object Detection in the BOP Challenge 2024](https://arxiv.org/abs/2412.01552)
*Xingyu Liu, Gu Wang, Chengxi Li, Yingyue Li, Chenyangguang Zhang, Ziqin Huang, Xiangyang Ji*

Main category: cs.CV

TL;DR: GFreeDet is a model-free unseen object detection method using Gaussian splatting and vision Foundation models, outperforming CAD-based methods in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enable robust detection of novel objects without relying on predefined CAD templates, addressing limitations of existing methods.

Method: Reconstructs objects directly from reference videos using Gaussian splatting, leveraging vision Foundation models.

Result: Achieves comparable performance to CAD-based methods on BOP-H3 benchmark and won awards at BOP Challenge 2024.

Conclusion: GFreeDet demonstrates the viability of model-free detection for mixed reality applications.

Abstract: We present GFreeDet, an unseen object detection approach that leverages
Gaussian splatting and vision Foundation models under model-free setting.
Unlike existing methods that rely on predefined CAD templates, GFreeDet
reconstructs objects directly from reference videos using Gaussian splatting,
enabling robust detection of novel objects without prior 3D models. Evaluated
on the BOP-H3 benchmark, GFreeDet achieves comparable performance to CAD-based
methods, demonstrating the viability of model-free detection for mixed reality
(MR) applications. Notably, GFreeDet won the best overall method and the best
fast method awards in the model-free 2D detection track at BOP Challenge 2024.

</details>


### [144] [Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams](https://arxiv.org/abs/2412.06770)
*Viktor Rudnev, Gereon Fox, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik*

Main category: cs.CV

TL;DR: Proposes a method for volumetric reconstruction of dynamic scenes using multi-view event streams and sparse RGB frames, outperforming RGB-based baselines.


<details>
  <summary>Details</summary>
Motivation: Challenges in poor lighting and fast motion with RGB cameras, addressed by leveraging event cameras' advantages.

Method: Trains time-conditioned NeRF models per segment, supervised by event- and RGB-based losses with sparse-view regularization.

Result: Outperforms RGB baselines, achieves state-of-the-art results, and introduces a benchmark dataset.

Conclusion: Opens new possibilities for fast scene capture beyond RGB cameras; code and data to be released.

Abstract: Volumetric reconstruction of dynamic scenes is an important problem in
computer vision. It is especially challenging in poor lighting and with fast
motion. This is partly due to limitations of RGB cameras: To capture frames
under low lighting, the exposure time needs to be increased, which leads to
more motion blur. In contrast, event cameras, which record changes in pixel
brightness asynchronously, are much less dependent on lighting, making them
more suitable for recording fast motion. We hence propose the first method to
spatiotemporally reconstruct a scene from sparse multi-view event streams and
sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF
models, one per short recording segment. The individual segments are supervised
with a set of event- and RGB-based losses and sparse-view regularisation. We
assemble a real-world multi-view camera rig with six static event cameras
around the object and record a benchmark multi-view event stream dataset of
challenging motions. Our work outperforms RGB-based baselines, producing
state-of-the-art results, and opens up the topic of multi-view event-based
reconstruction as a new path for fast scene capture beyond RGB cameras. The
code and the data will be released soon at
https://4dqv.mpi-inf.mpg.de/DynEventNeRF/

</details>


### [145] [Adapter-Enhanced Semantic Prompting for Continual Learning](https://arxiv.org/abs/2412.11074)
*Baocai Yin, Ji Zhao, Huajie Jiang, Ningning Hou, Yongli Hu, Amin Beheshti, Ming-Hsuan Yang, Yuankai Qi*

Main category: cs.CV

TL;DR: A lightweight CL framework, AESP, combines prompt tuning and adapters to mitigate catastrophic forgetting with low memory usage.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in continual learning without high memory costs.

Method: Integrates semantic-guided prompts and adapters for feature generalization and efficient fusion, with a novel prompt selection mechanism.

Result: Achieves strong performance on three CL datasets.

Conclusion: AESP shows promise for advancing continual learning by balancing performance and efficiency.

Abstract: Continual learning (CL) enables models to adapt to evolving data streams. A
major challenge of CL is catastrophic forgetting, where new knowledge will
overwrite previously acquired knowledge. Traditional methods usually retain the
past data for replay or add additional branches in the model to learn new
knowledge, which has high memory requirements. In this paper, we propose a
novel lightweight CL framework, Adapter-Enhanced Semantic Prompting (AESP),
which integrates prompt tuning and adapter techniques. Specifically, we design
semantic-guided prompts to enhance the generalization ability of visual
features and utilize adapters to efficiently fuse the semantic information,
aiming to learn more adaptive features for the continual learning task.
Furthermore, to choose the right task prompt for feature adaptation, we have
developed a novel matching mechanism for prompt selection. Extensive
experiments on three CL datasets demonstrate that our approach achieves
favorable performance across multiple metrics, showing its potential for
advancing CL.

</details>


### [146] [Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion](https://arxiv.org/abs/2501.04606)
*Yangfan He, Sida Li, Jianhui Wang, Kun Li, Xinyuan Song, Xinhang Yuan, Keqin Li, Kuan Lu, Menghao Huo, Jiaqi Chen, Miao Zhang, Xueqian Wang*

Main category: cs.CV

TL;DR: The paper proposes GE-Adapter, a framework to improve temporal and semantic consistency in text-to-video editing using diffusion models, addressing issues like high training costs and limited coherence.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation methods for video editing suffer from poor temporal consistency and high resource demands. The authors aim to provide a cost-effective and efficient solution.

Method: GE-Adapter integrates temporal-spatial and semantic consistency with Baliteral DDIM inversion, using FTC Blocks for temporal coherence, SCD Blocks for spatial consistency, and TSC Module for semantic alignment.

Result: The method improves perceptual quality, text-image alignment, and temporal coherence on the MSR-VTT dataset, achieving better fidelity and frame-to-frame coherence.

Conclusion: GE-Adapter offers a practical and efficient solution for text-to-video editing, overcoming limitations of existing approaches.

Abstract: Recent advancements in text-to-image (T2I) generation using diffusion models
have enabled cost-effective video-editing applications by leveraging
pre-trained models, eliminating the need for resource-intensive training.
However, the frame-independence of T2I generation often results in poor
temporal consistency. Existing methods address this issue through temporal
layer fine-tuning or inference-based temporal propagation, but these approaches
suffer from high training costs or limited temporal coherence. To address these
challenges, we propose a General and Efficient Adapter (GE-Adapter) that
integrates temporal-spatial and semantic consistency with Baliteral DDIM
inversion. This framework introduces three key components: (1) Frame-based
Temporal Consistency Blocks (FTC Blocks) to capture frame-specific features and
enforce smooth inter-frame transitions via temporally-aware loss functions; (2)
Channel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral
filters to enhance spatial coherence by reducing noise and artifacts; and (3)
Token-based Semantic Consistency Module (TSC Module) to maintain semantic
alignment using shared prompt tokens and frame-specific tokens. Our method
significantly improves perceptual quality, text-image alignment, and temporal
coherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves
enhanced fidelity and frame-to-frame coherence, offering a practical solution
for T2V editing.

</details>


### [147] [DEFOM-Stereo: Depth Foundation Model Based Stereo Matching](https://arxiv.org/abs/2501.09466)
*Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang*

Main category: cs.CV

TL;DR: DEFOM-Stereo integrates monocular depth cues into stereo matching, improving robustness and generalization, achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address challenges like occlusion and non-texture in stereo matching by leveraging monocular depth estimation for better disparity accuracy.

Method: Combines monocular depth model (DEFOM) with recurrent stereo-matching, using DEFOM for feature extraction and disparity initialization, plus a scale update module.

Result: Strong zero-shot generalization, top performance on KITTI, Middlebury, and ETH3D benchmarks, and leading metrics in robust vision challenge.

Conclusion: DEFOM-Stereo demonstrates superior robustness and generalization, outperforming state-of-the-art methods in stereo matching.

Abstract: Stereo matching is a key technique for metric depth estimation in computer
vision and robotics. Real-world challenges like occlusion and non-texture
hinder accurate disparity estimation from binocular matching cues. Recently,
monocular relative depth estimation has shown remarkable generalization using
vision foundation models. Thus, to facilitate robust stereo matching with
monocular depth cues, we incorporate a robust monocular relative depth model
into the recurrent stereo-matching framework, building a new framework for
depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature
extraction stage, we construct the combined context and matching feature
encoder by integrating features from conventional CNNs and DEFOM. In the update
stage, we use the depth predicted by DEFOM to initialize the recurrent
disparity and introduce a scale update module to refine the disparity at the
correct scale. DEFOM-Stereo is verified to have much stronger zero-shot
generalization compared with SOTA methods. Moreover, DEFOM-Stereo achieves top
performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,
ranking $1^{st}$ on many metrics. In the joint evaluation under the robust
vision challenge, our model simultaneously outperforms previous models on the
individual benchmarks, further demonstrating its outstanding capabilities.

</details>


### [148] [UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion](https://arxiv.org/abs/2501.11515)
*Zixuan Chen, Yujin Wang, Xin Cai, Zhiyuan You, Zheming Lu, Fan Zhang, Shi Guo, Tianfan Xue*

Main category: cs.CV

TL;DR: The paper introduces \model, an exposure fusion technique for HDR scenes, handling up to 9 stops difference by modeling fusion as guided inpainting.


<details>
  <summary>Details</summary>
Motivation: Current exposure fusion methods fail for very high dynamic range scenes due to alignment, lighting inconsistencies, or tone mapping artifacts.

Method: The approach models exposure fusion as guided inpainting, using under-exposed images as soft guidance to fill missing highlights in over-exposed regions.

Result: The method outperforms HDR-Transformer on benchmarks and performs well on the new UltraFusion dataset with 9 stops difference.

Conclusion: \model is robust and effective for ultra-high dynamic range scenes, producing high-quality fusion results.

Abstract: Capturing high dynamic range (HDR) scenes is one of the most important issues
in camera design. Majority of cameras use exposure fusion, which fuses images
captured by different exposure levels, to increase dynamic range. However, this
approach can only handle images with limited exposure difference, normally 3-4
stops. When applying to very high dynamic range scenes where a large exposure
difference is required, this approach often fails due to incorrect alignment or
inconsistent lighting between inputs, or tone mapping artifacts. In this work,
we propose \model, the first exposure fusion technique that can merge inputs
with 9 stops differences. The key idea is that we model exposure fusion as a
guided inpainting problem, where the under-exposed image is used as a guidance
to fill the missing information of over-exposed highlights in the over-exposed
region. Using an under-exposed image as a soft guidance, instead of a hard
constraint, our model is robust to potential alignment issue or lighting
variations. Moreover, by utilizing the image prior of the generative model, our
model also generates natural tone mapping, even for very high-dynamic range
scenes. Our approach outperforms HDR-Transformer on latest HDR benchmarks.
Moreover, to test its performance in ultra high dynamic range scenes, we
capture a new real-world exposure fusion benchmark, UltraFusion dataset, with
exposure differences up to 9 stops, and experiments show that UltraFusion can
generate beautiful and high-quality fusion results under various scenarios.
Code and data will be available at
https://openimaginglab.github.io/UltraFusion.

</details>


### [149] [Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras](https://arxiv.org/abs/2502.07758)
*Nektarios A. Valous, Eckhard Hitzer, DragoÅŸ DuÅŸe, Rodrigo Rojas Moraleda, Ferdinand Popp, Meggy Suarez-Carmona, Anna Berthel, Ismini Papageorgiou, Carlo Fremd, Alexander RÃ¶lle, Christina C. Westhoff, BÃ©nÃ©dicte Lenoir, Niels Halama, Inka ZÃ¶rnig, Dirk JÃ¤ger*

Main category: cs.CV

TL;DR: The paper introduces hypercomplex image processing using quaternions and orthogonal planes split for tasks like re-colorization, de-colorization, contrast enhancement, and stain separation in natural/biomedical images. It offers computationally efficient, non-data-driven methods with competitive results.


<details>
  <summary>Details</summary>
Motivation: To extend conventional image processing techniques by unifying algebraic and geometric principles, enabling versatile and consistent workflows for both natural and biomedical images.

Method: Leverages quaternions and the two-dimensional orthogonal planes split framework for tasks like re-colorization, de-colorization, and contrast enhancement. Uses basic arithmetic and matrix operations.

Result: Achieves comparable or better results than existing methods, particularly in well-known cases, demonstrating practical effectiveness.

Conclusion: The work highlights the potential of robust theoretical frameworks in image processing, offering versatility and computational accessibility for diverse applications.

Abstract: Hypercomplex image processing extends conventional techniques in a unified
paradigm encompassing algebraic and geometric principles. This work leverages
quaternions and the two-dimensional orthogonal planes split framework
(splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D
planes) for natural/biomedical image analysis through the following
computational workflows and outcomes: natural/biomedical image re-colorization,
natural image de-colorization, natural/biomedical image contrast enhancement,
computational re-staining and stain separation in histological images, and
performance gains in machine/deep learning pipelines for histological images.
The workflows are analyzed separately for natural and biomedical images to
showcase the effectiveness of the proposed approaches. The proposed workflows
can regulate color appearance (e.g. with alternative renditions and grayscale
conversion) and image contrast, be part of automated image processing pipelines
(e.g. isolating stain components, boosting learning models), and assist in
digital pathology applications (e.g. enhancing biomarker visibility, enabling
colorblind-friendly renditions). Employing only basic arithmetic and matrix
operations, this work offers a computationally accessible methodology - in the
hypercomplex domain - that showcases versatility and consistency across image
processing tasks and a range of computer vision and biomedical applications.
The proposed non-data-driven methods achieve comparable or better results
(particularly in cases involving well-known methods) to those reported in the
literature, showcasing the potential of robust theoretical frameworks with
practical effectiveness. Results, methods, and limitations are detailed
alongside discussion of promising extensions, emphasizing the potential of
feature-rich mathematical/computational frameworks for natural and biomedical
images.

</details>


### [150] [FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven Talking Portrait Synthesis](https://arxiv.org/abs/2503.04067)
*Ziqi Ni, Ao Fu, Yi Zhou*

Main category: cs.CV

TL;DR: FREAK is a real-time, high-fidelity audio-driven talking portrait synthesis framework that models from the frequency domain, improving lip-speech synchronization and naturalness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for lip-speech synchronization in talking portraits are computationally expensive or fail to address discrepancies in the frequency domain, leading to unnatural results.

Method: FREAK introduces two frequency-based modules (VEFM and AVFM) to couple visual and audio features in the frequency domain, optimizing jointly in pixel and frequency domains.

Result: FREAK synthesizes high-fidelity portraits with detailed textures and precise lip sync in real-time, outperforming state-of-the-art methods.

Conclusion: FREAK addresses frequency domain gaps, enhances fidelity, and supports flexible settings, making it a superior solution for talking portrait synthesis.

Abstract: Achieving high-fidelity lip-speech synchronization in audio-driven talking
portrait synthesis remains challenging. While multi-stage pipelines or
diffusion models yield high-quality results, they suffer from high
computational costs. Some approaches perform well on specific individuals with
low resources, yet still exhibit mismatched lip movements. The aforementioned
methods are modeled in the pixel domain. We observed that there are noticeable
discrepancies in the frequency domain between the synthesized talking videos
and natural videos. Currently, no research on talking portrait synthesis has
considered this aspect. To address this, we propose a FREquency-modulated,
high-fidelity, and real-time Audio-driven talKing portrait synthesis framework,
named FREAK, which models talking portraits from the frequency domain
perspective, enhancing the fidelity and naturalness of the synthesized
portraits. FREAK introduces two novel frequency-based modules: 1) the Visual
Encoding Frequency Modulator (VEFM) to couple multi-scale visual features in
the frequency domain, better preserving visual frequency information and
reducing the gap in the frequency spectrum between synthesized and natural
frames. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model
learn the talking pattern in the frequency domain and improve audio-visual
synchronization. Additionally, we optimize the model in both pixel domain and
frequency domain jointly. Furthermore, FREAK supports seamless switching
between one-shot and video dubbing settings, offering enhanced flexibility. Due
to its superior performance, it can simultaneously support high-resolution
video results and real-time inference. Extensive experiments demonstrate that
our method synthesizes high-fidelity talking portraits with detailed facial
textures and precise lip synchronization in real-time, outperforming
state-of-the-art methods.

</details>


### [151] [Exploring Adversarial Transferability between Kolmogorov-arnold Networks](https://arxiv.org/abs/2503.06276)
*Songping Wang, Xinquan Yue, Yueming Lyu, Caifeng Shan*

Main category: cs.CV

TL;DR: AdvKAN is a transfer attack method for Kolmogorov-Arnold Networks (KANs) that addresses their adversarial robustness issues by combining Breakthrough-Defense Surrogate Model (BDSM) and Global-Local Interaction (GLI) techniques.


<details>
  <summary>Details</summary>
Motivation: The adversarial robustness of KANs is underexplored, and their poor adversarial transferability due to overfitting to specific basis functions poses a safety issue.

Method: AdvKAN integrates BDSM to mitigate overfitting and GLI to smooth loss surfaces, enhancing transfer attack strength.

Result: Experiments show AdvKAN's superior attack capabilities, revealing KAN vulnerabilities.

Conclusion: AdvKAN effectively tackles adversarial transferability challenges in KANs, demonstrating their vulnerabilities.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model
paradigm, significantly impacting various fields. However, their adversarial
robustness remains less underexplored, especially across different KAN
architectures. To explore this critical safety issue, we conduct an analysis
and find that due to overfitting to the specific basis functions of KANs, they
possess poor adversarial transferability among different KANs. To tackle this
challenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN
integrates two key components: 1) a Breakthrough-Defense Surrogate Model
(BDSM), which employs a breakthrough-defense training strategy to mitigate
overfitting to the specific structures of KANs. 2) a Global-Local Interaction
(GLI) technique, which promotes sufficient interaction between adversarial
gradients of hierarchical levels, further smoothing out loss surfaces of KANs.
Both of them work together to enhance the strength of transfer attack among
different KANs. Extensive experimental results on various KANs and datasets
demonstrate the effectiveness of AdvKAN, which possesses notably superior
attack capabilities and deeply reveals the vulnerabilities of KANs. Code will
be released upon acceptance.

</details>


### [152] [ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos](https://arxiv.org/abs/2503.12542)
*Peiran Wu, Yunze Liu, Miao Liu, Junxiao Shen*

Main category: cs.CV

TL;DR: The paper introduces Ego-ST Bench, a benchmark for evaluating multimodal spatial-temporal reasoning in MLLMs, and proposes the ST-R1 training paradigm to enhance such reasoning.


<details>
  <summary>Details</summary>
Motivation: To explore whether MLLMs can achieve human-like spatial-temporal reasoning from an egocentric perspective.

Method: Develops Ego-ST Bench with 5,000+ QA pairs and introduces ST-R1, combining long-CoT fine-tuning and GRPO reinforcement learning.

Result: ST-R1 significantly improves performance in spatial-temporal reasoning tasks.

Conclusion: Ego-ST Bench and ST-R1 advance research in video-based spatial-temporal reasoning.

Abstract: Humans excel at spatial-temporal reasoning, effortlessly interpreting dynamic
visual events from an egocentric viewpoint. However, whether multimodal large
language models (MLLMs) can similarly understand the 4D world remains
uncertain. This paper explores multimodal spatial-temporal reasoning from an
egocentric perspective, aiming to equip MLLMs with human-like reasoning
capabilities. To support this objective, we introduce \textbf{Ego-ST Bench}, a
novel benchmark containing over 5,000 question-answer pairs across four
categories, systematically evaluating spatial, temporal, and integrated
spatial-temporal reasoning. Additionally, we propose \textbf{ST-R1} training
paradigm, a video-based reasoning model that incorporates reverse thinking into
its reinforcement learning process, significantly enhancing performance. We
combine long-chain-of-thought (long-CoT) supervised fine-tuning with Group
Relative Policy Optimization (GRPO) reinforcement learning, achieving notable
improvements with limited high-quality data. Ego-ST Bench and ST-R1 provide
valuable insights and resources for advancing video-based spatial-temporal
reasoning research.

</details>


### [153] [UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing](https://arxiv.org/abs/2503.12652)
*Tsu-Jui Fu, Yusu Qian, Chen Chen, Wenze Hu, Zhe Gan, Yinfei Yang*

Main category: cs.CV

TL;DR: UniVG is a generalist diffusion model supporting diverse image generation tasks with a single set of weights, outperforming task-specific models in some cases.


<details>
  <summary>Details</summary>
Motivation: Existing methods require separate architectures and parameters for different tasks, limiting efficiency and scalability. UniVG aims to unify these tasks.

Method: UniVG treats multi-modal inputs as unified conditions and employs multi-task training with data mixing to handle various tasks like T2I generation, inpainting, and depth estimation.

Result: UniVG outperforms some task-specific models and shows that tasks like T2I generation and instruction-based editing can coexist without trade-offs.

Conclusion: UniVG represents a significant step towards a unified image generation model, demonstrating versatility and performance across diverse tasks.

Abstract: Text-to-Image (T2I) diffusion models have shown impressive results in
generating visually compelling images following user prompts. Building on this,
various methods further fine-tune the pre-trained T2I model for specific tasks.
However, this requires separate model architectures, training designs, and
multiple parameter sets to handle different tasks. In this paper, we introduce
UniVG, a generalist diffusion model capable of supporting a diverse range of
image generation tasks with a single set of weights. UniVG treats multi-modal
inputs as unified conditions to enable various downstream applications, ranging
from T2I generation, inpainting, instruction-based editing, identity-preserving
generation, and layout-guided generation, to depth estimation and referring
segmentation. Through comprehensive empirical studies on data mixing and
multi-task training, we provide detailed insights into the training processes
and decisions that inform our final designs. For example, we show that T2I
generation and other tasks, such as instruction-based editing, can coexist
without performance trade-offs, while auxiliary tasks like depth estimation and
referring segmentation enhance image editing. Notably, our model can even
outperform some task-specific models on their respective benchmarks, marking a
significant step towards a unified image generation model.

</details>


### [154] [On Symmetries in Convolutional Weights](https://arxiv.org/abs/2503.19215)
*Bilal Alsallakh, Timothy Wroge, Vivek Miglani, Narine Kokhlikyan*

Main category: cs.CV

TL;DR: The paper examines the symmetry of mean k x k weight kernels in convolutional neural networks (CNNs), finding they tend to be symmetric rather than directional, and explores its implications.


<details>
  <summary>Details</summary>
Motivation: To understand why mean kernels in CNNs exhibit symmetry and how this relates to network behavior and performance.

Method: Analyzes symmetry in mean kernels across layers, datasets, and models, and studies its correlation with properties like shift and flip consistency.

Result: Symmetry in mean kernels correlates with desirable properties and may represent an inherent inductive bias in CNNs.

Conclusion: Symmetry in CNNs' mean kernels is a significant feature linked to network robustness and consistency, suggesting an inherent bias in their design.

Abstract: We explore the symmetry of the mean k x k weight kernel in each layer of
various convolutional neural networks. Unlike individual neurons, the mean
kernels in internal layers tend to be symmetric about their centers instead of
favoring specific directions. We investigate why this symmetry emerges in
various datasets and models, and how it is impacted by certain architectural
choices. We show how symmetry correlates with desirable properties such as
shift and flip consistency, and might constitute an inherent inductive bias in
convolutional neural networks.

</details>


### [155] [MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion](https://arxiv.org/abs/2503.20698)
*Saron Samuel, Dan DeGenaro, Jimena Guallar-Blasco, Kate Sanders, Oluwaseun Eisape, Arun Reddy, Alexander Martin, Andrew Yates, Eugene Yang, Cameron Carpenter, David Etter, Efsun Kayi, Matthew Wiesner, Kenton Murray, Reno Kriz*

Main category: cs.CV

TL;DR: MMMORRF improves video retrieval by integrating text and features from visual and audio modalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models prioritize visual signals, neglecting other modalities like text and audio, which are crucial for comprehensive video retrieval.

Method: MMMORRF extracts and integrates text and features from visual and audio modalities using modality-aware weighted reciprocal rank fusion.

Result: MMMORRF improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval on MultiVENT 2.0 and TVR benchmarks.

Conclusion: Integrating diverse modalities enhances video retrieval effectiveness, addressing the limitations of vision-centric models.

Abstract: Videos inherently contain multiple modalities, including visual events, text
overlays, sounds, and speech, all of which are important for retrieval.
However, state-of-the-art multimodal language models like VAST and LanguageBind
are built on vision-language models (VLMs), and thus overly prioritize visual
signals. Retrieval benchmarks further reinforce this bias by focusing on visual
queries and neglecting other modalities. We create a search system MMMORRF that
extracts text and features from both visual and audio modalities and integrates
them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is
both effective and efficient, demonstrating practicality in searching videos
based on users' information needs instead of visual descriptive queries. We
evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed
for more targeted information needs, and find that it improves nDCG@20 by 81%
over leading multimodal encoders and 37% over single-modality retrieval,
demonstrating the value of integrating diverse modalities.

</details>


### [156] [Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment](https://arxiv.org/abs/2504.01503)
*Ziteng Cui, Xuangeng Chu, Tatsuya Harada*

Main category: cs.CV

TL;DR: Luminance-GS improves novel view synthesis under diverse lighting by using per-view color mapping and adaptive adjustments, maintaining real-time rendering.


<details>
  <summary>Details</summary>
Motivation: Addressing photometric inconsistencies in multi-view scenarios caused by lighting and exposure variations, which degrade NeRF and 3DGS performance.

Method: Uses per-view color matrix mapping and view-adaptive curve adjustments without altering the 3DGS explicit representation.

Result: Achieves SOTA results in low-light, overexposure, and varying exposure conditions with real-time rendering.

Conclusion: Luminance-GS outperforms NeRF and 3DGS baselines in challenging lighting while preserving speed and quality.

Abstract: Capturing high-quality photographs under diverse real-world lighting
conditions is challenging, as both natural lighting (e.g., low-light) and
camera exposure settings (e.g., exposure time) significantly impact image
quality. This challenge becomes more pronounced in multi-view scenarios, where
variations in lighting and image signal processor (ISP) settings across
viewpoints introduce photometric inconsistencies. Such lighting degradations
and view-dependent variations pose substantial challenges to novel view
synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel
approach to achieving high-quality novel view synthesis results under diverse
challenging lighting conditions using 3DGS. By adopting per-view color matrix
mapping and view-adaptive curve adjustments, Luminance-GS achieves
state-of-the-art (SOTA) results across various lighting conditions -- including
low-light, overexposure, and varying exposure -- while not altering the
original 3DGS explicit representation. Compared to previous NeRF- and
3DGS-based baselines, Luminance-GS provides real-time rendering speed with
improved reconstruction quality.

</details>


### [157] [BOP Challenge 2024 on Model-Based and Model-Free 6D Object Pose Estimation](https://arxiv.org/abs/2504.02812)
*Van Nguyen Nguyen, Stephen Tyree, Andrew Guo, Mederic Fourmy, Anas Gouda, Taeyeop Lee, Sungphill Moon, Hyeontae Son, Lukas Ranftl, Jonathan Tremblay, Eric Brachmann, Bertram Drost, Vincent Lepetit, Carsten Rother, Stan Birchfield, Jiri Matas, Yann Labbe, Martin Sundermeyer, Tomas Hodan*

Main category: cs.CV

TL;DR: The BOP Challenge 2024 evaluates 6D object pose estimation in real-world scenarios, introducing new tasks, datasets, and showing improved accuracy over 2023 methods, though with trade-offs in speed.


<details>
  <summary>Details</summary>
Motivation: To transition 6D object pose estimation from lab-like setups to real-world applications by introducing model-free tasks, practical detection tasks, and realistic datasets.

Method: Introduced new tasks (model-free, practical detection), datasets (BOP-H3), and evaluated seven challenge tracks with methods like FreeZeV2.1, Co-op, and MUSE.

Result: Best 2024 methods improved accuracy (e.g., FreeZeV2.1 by 22%, MUSE by 21-29%) but were slower (e.g., FreeZeV2.1 at 24.9s/image). 2D detection remains a bottleneck.

Conclusion: BOP 2024 advances real-world 6D pose estimation, with accuracy improvements but speed trade-offs, highlighting 2D detection as a key challenge.

Abstract: We present the evaluation methodology, datasets and results of the BOP
Challenge 2024, the 6th in a series of public competitions organized to capture
the state of the art in 6D object pose estimation and related tasks. In 2024,
our goal was to transition BOP from lab-like setups to real-world scenarios.
First, we introduced new model-free tasks, where no 3D object models are
available and methods need to onboard objects just from provided reference
videos. Second, we defined a new, more practical 6D object detection task where
identities of objects visible in a test image are not provided as input. Third,
we introduced new BOP-H3 datasets recorded with high-resolution sensors and
AR/VR headsets, closely resembling real-world scenarios. BOP-H3 include 3D
models and onboarding videos to support both model-based and model-free tasks.
Participants competed on seven challenge tracks. Notably, the best 2024 method
for model-based 6D localization of unseen objects (FreeZeV2.1) achieves 22%
higher accuracy on BOP-Classic-Core than the best 2023 method (GenFlow), and is
only 4% behind the best 2023 method for seen objects (GPose2023) although being
significantly slower (24.9 vs 2.7s per image). A more practical 2024 method for
this task is Co-op which takes only 0.8s per image and is 13% more accurate
than GenFlow. Methods have similar rankings on 6D detection as on 6D
localization but higher run time. On model-based 2D detection of unseen
objects, the best 2024 method (MUSE) achieves 21--29% relative improvement
compared to the best 2023 method (CNOS). However, the 2D detection accuracy for
unseen objects is still -35% behind the accuracy for seen objects (GDet2023),
and the 2D detection stage is consequently the main bottleneck of existing
pipelines for 6D localization/detection of unseen objects. The online
evaluation system stays open and is available at http://bop.felk.cvut.cz/

</details>


### [158] [A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions](https://arxiv.org/abs/2504.06121)
*Ronghui Zhang, Yuhang Ma, Tengfei Li, Ziyu Lin, Yueying Wu, Junzhou Chen, Lin Zhang, Jia Hu, Tony Z. Qiu, Konghui Guo*

Main category: cs.CV

TL;DR: A robust lane detection method for foggy conditions, introducing new datasets and a Fog-Enhanced Network with specialized modules, achieving high accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing lane detection algorithms perform poorly in foggy conditions, lacking datasets and methods tailored for such environments.

Method: Proposes a Fog-Enhanced Network with Global Feature Fusion Module (GFFM), Kernel Feature Fusion Module (KFFM), and Low-level Edge Enhanced Module (LEEM). Introduces FoggyLane, FoggyCULane, and FoggyTusimple datasets.

Result: Achieves F1-scores of 95.04, 79.85, and 96.95 on respective datasets, with real-time processing at 38.4 FPS.

Conclusion: The method is robust and effective for lane detection in foggy environments, demonstrating state-of-the-art performance and real-time applicability.

Abstract: Lane detection is a critical component of Advanced Driver Assistance Systems
(ADAS). Existing lane detection algorithms generally perform well under
favorable weather conditions. However, their performance degrades significantly
in adverse conditions, such as fog, which increases the risk of traffic
accidents. This challenge is compounded by the lack of specialized datasets and
methods designed for foggy environments. To address this, we introduce the
FoggyLane dataset, captured in real-world foggy scenarios, and synthesize two
additional datasets, FoggyCULane and FoggyTusimple, from existing popular lane
detection datasets. Furthermore, we propose a robust Fog-Enhanced Network for
lane detection, incorporating a Global Feature Fusion Module (GFFM) to capture
global relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to
model the structural and positional relationships of lane instances, and a
Low-level Edge Enhanced Module (LEEM) to address missing edge details in foggy
conditions. Comprehensive experiments demonstrate that our method achieves
state-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on
FoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT
acceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA
Jetson AGX Orin, confirming its real-time capabilities and robustness in foggy
environments.

</details>


### [159] [MediSee: Reasoning-based Pixel-level Perception in Medical Images](https://arxiv.org/abs/2504.11008)
*Qinyue Tong, Ziqian Lu, Jun Liu, Yangming Zheng, Zheming Lu*

Main category: cs.CV

TL;DR: The paper introduces a new medical vision task called Medical Reasoning Segmentation and Detection (MedSD), addressing implicit queries with logical reasoning. It proposes a dataset (MLMR-SD) and a baseline model (MediSee) to outperform traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing medical image methods rely on specialized inputs (e.g., bounding boxes or labels), limiting accessibility for general users who prefer oral queries requiring reasoning.

Method: The authors introduce the MedSD task, create the MLMR-SD dataset with reasoning-based medical targets, and propose the MediSee model for segmentation and detection from implicit queries.

Result: Experiments show MediSee effectively handles MedSD with colloquial queries and outperforms traditional referring segmentation methods.

Conclusion: The work advances medical image analysis by enabling reasoning-based queries, improving accessibility and performance for general users.

Abstract: Despite remarkable advancements in pixel-level medical image perception,
existing methods are either limited to specific tasks or heavily rely on
accurate bounding boxes or text labels as input prompts. However, the medical
knowledge required for input is a huge obstacle for general public, which
greatly reduces the universality of these methods. Compared with these
domain-specialized auxiliary information, general users tend to rely on oral
queries that require logical reasoning. In this paper, we introduce a novel
medical vision task: Medical Reasoning Segmentation and Detection (MedSD),
which aims to comprehend implicit queries about medical images and generate the
corresponding segmentation mask and bounding box for the target object. To
accomplish this task, we first introduce a Multi-perspective, Logic-driven
Medical Reasoning Segmentation and Detection (MLMR-SD) dataset, which
encompasses a substantial collection of medical entity targets along with their
corresponding reasoning. Furthermore, we propose MediSee, an effective baseline
model designed for medical reasoning segmentation and detection. The
experimental results indicate that the proposed method can effectively address
MedSD with implicit colloquial queries and outperform traditional medical
referring segmentation methods.

</details>


### [160] [Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis](https://arxiv.org/abs/2504.12129)
*Songping Wang, Yueming Lyu, Shiqi Liu, Ning Li, Tong Tong, Hao Sun, Caifeng Shan*

Main category: cs.CV

TL;DR: The paper proposes a Hierarchical Anti-Aesthetic (HAA) framework to degrade the quality of maliciously customized diffusion models, protecting facial identity by targeting global and local aesthetic properties.


<details>
  <summary>Details</summary>
Motivation: Customized diffusion models pose risks to privacy and copyright. The paper addresses this by leveraging aesthetic degradation to disrupt malicious content generation.

Method: HAA uses global and local anti-aesthetic mechanisms (reward and loss) to degrade image quality and disrupt facial identity.

Result: HAA outperforms state-of-the-art methods in identity removal, proving effective for facial privacy and copyright protection.

Conclusion: The HAA framework provides a novel aesthetic-based solution to mitigate misuse of customized diffusion models, enhancing privacy and copyright safeguards.

Abstract: The rise of customized diffusion models has spurred a boom in personalized
visual content creation, but also poses risks of malicious misuse, severely
threatening personal privacy and copyright protection. Some studies show that
the aesthetic properties of images are highly positively correlated with human
perception of image quality. Inspired by this, we approach the problem from a
novel and intriguing aesthetic perspective to degrade the generation quality of
maliciously customized models, thereby achieving better protection of facial
identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)
framework to fully explore aesthetic cues, which consists of two key branches:
1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward
mechanism and a global anti-aesthetic loss, it can degrade the overall
aesthetics of the generated content; 2) Local Anti-Aesthetics: A local
anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to
guide adversarial perturbations to disrupt local facial identity. By seamlessly
integrating both branches, our HAA effectively achieves the goal of
anti-aesthetics from a global to a local level during customized generation.
Extensive experiments show that HAA outperforms existing SOTA methods largely
in identity removal, providing a powerful tool for protecting facial privacy
and copyright.

</details>


### [161] [Riemannian Patch Assignment Gradient Flows](https://arxiv.org/abs/2504.13024)
*Daniel Gonzalez-Alvarado, Fabio Schlindwein, Jonas Cassel, Laura Steingruber, Stefania Petra, Christoph SchnÃ¶rr*

Main category: cs.CV

TL;DR: The paper proposes patch assignment flows for metric data labeling on graphs, using dynamic interaction of labels and assignments, achieving consistency via Riemannian ascent flow.


<details>
  <summary>Details</summary>
Motivation: To improve metric data labeling on graphs by regularizing initial local labelings through dynamic interactions, ensuring consistency and uncertainty quantification.

Method: Uses a dictionary of labeled patches and patch assignment variables, with regularization via Riemannian ascent flow derived from a Lagrangian action functional.

Result: Experiments demonstrate the approach's properties, including effective uncertainty quantification of label assignments.

Conclusion: The method effectively regularizes label assignments on graphs, achieving consistency and providing uncertainty insights.

Abstract: This paper introduces patch assignment flows for metric data labeling on
graphs. Labelings are determined by regularizing initial local labelings
through the dynamic interaction of both labels and label assignments across the
graph, entirely encoded by a dictionary of competing labeled patches and
mediated by patch assignment variables. Maximal consistency of patch
assignments is achieved by geometric numerical integration of a Riemannian
ascent flow, as critical point of a Lagrangian action functional. Experiments
illustrate properties of the approach, including uncertainty quantification of
label assignments.

</details>


### [162] [Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization](https://arxiv.org/abs/2504.13460)
*Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma*

Main category: cs.CV

TL;DR: A new few-shot temporal action localization method using Chain-of-Thought textual reasoning improves performance by leveraging textual semantics and aligning text-visual data.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot TAL methods ignore textual information, which can enhance action localization by providing semantic support.

Method: Proposes a framework with semantic-aware text-visual alignment and CoT-like reasoning to generate descriptive texts for videos.

Result: Outperforms existing methods on ActivityNet1.3 and THUMOS14 datasets, with a new dataset for human anomaly detection.

Conclusion: The method effectively combines textual and visual data for superior few-shot TAL performance, with potential applications in anomaly detection.

Abstract: Traditional temporal action localization (TAL) methods rely on large amounts
of detailed annotated data, whereas few-shot TAL reduces this dependence by
using only a few training samples to identify unseen action categories.
However, existing few-shot TAL methods typically focus solely on video-level
information, neglecting textual information, which can provide valuable
semantic support for the localization task. Therefore, we propose a new
few-shot temporal action localization method by Chain-of-Thought textual
reasoning to improve localization performance. Specifically, we design a novel
few-shot learning framework that leverages textual semantic information to
enhance the model's ability to capture action commonalities and variations,
which includes a semantic-aware text-visual alignment module designed to align
the query and support videos at different levels. Meanwhile, to better express
the temporal dependencies and causal relationships between actions at the
textual level to assist action localization, we design a Chain of Thought
(CoT)-like reasoning method that progressively guides the Vision Language Model
(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for
videos. The generated texts can capture more variance of action than visual
features. We conduct extensive experiments on the publicly available
ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named
Human-related Anomaly Localization and explore the application of the TAL task
in human anomaly detection. The experimental results demonstrate that our
proposed method significantly outperforms existing methods in single-instance
and multi-instance scenarios. We will release our code, data and benchmark.

</details>


### [163] [Decoding Vision Transformers: the Diffusion Steering Lens](https://arxiv.org/abs/2504.13763)
*Ryota Takatsuki, Sonia Joseph, Ippei Fujisawa, Ryota Kanai*

Main category: cs.CV

TL;DR: The paper introduces Diffusion Steering Lens (DSL), a training-free method to interpret Vision Transformers (ViTs) by steering submodule outputs, addressing limitations of Logit Lens and Diffusion Lens.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Logit Lens and Diffusion Lens are limited in capturing the richness of visual representations in ViTs, particularly in isolating submodule contributions.

Method: Proposes DSL, which steers submodule outputs and patches indirect contributions, validated through interventional studies.

Result: DSL provides intuitive and reliable interpretation of ViTs' internal processing.

Conclusion: DSL overcomes prior limitations, offering a robust tool for mechanistic interpretability of ViTs.

Abstract: Logit Lens is a widely adopted method for mechanistic interpretability of
transformer-based language models, enabling the analysis of how internal
representations evolve across layers by projecting them into the output
vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is
technically straightforward, its direct use faces limitations in capturing the
richness of visual representations. Building on the work of Toker et al.
(2024)~\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize
intermediate representations in the text encoders of text-to-image diffusion
models, we demonstrate that while Diffusion Lens can effectively visualize
residual stream representations in image encoders, it fails to capture the
direct contributions of individual submodules. To overcome this limitation, we
propose \textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach
that steers submodule outputs and patches subsequent indirect contributions. We
validate our method through interventional studies, showing that DSL provides
an intuitive and reliable interpretation of the internal processing in ViTs.

</details>


### [164] [DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning](https://arxiv.org/abs/2504.14509)
*Fulong Ye, Miao Hua, Pengze Zhang, Xinghui Li, Qichao Sun, Songtao Zhao, Qian He, Xinglong Wu*

Main category: cs.CV

TL;DR: DreamID is a diffusion-based face swapping model with explicit supervision via Triplet ID Group data, achieving high ID similarity, attribute preservation, and fast inference.


<details>
  <summary>Details</summary>
Motivation: Typical face swapping models struggle with implicit supervision and unsatisfactory results. DreamID aims to improve identity similarity and attribute preservation through explicit supervision.

Method: Uses Triplet ID Group data for explicit supervision, leverages SD Turbo for efficient training, and introduces SwapNet, FaceNet, and ID Adapter architecture.

Result: Outperforms state-of-the-art methods in identity similarity, pose/expression preservation, and image fidelity, achieving high-quality results in 0.6 seconds.

Conclusion: DreamID delivers superior face swapping with explicit supervision, fast inference, and robustness in challenging scenarios.

Abstract: In this paper, we introduce DreamID, a diffusion-based face swapping model
that achieves high levels of ID similarity, attribute preservation, image
fidelity, and fast inference speed. Unlike the typical face swapping training
process, which often relies on implicit supervision and struggles to achieve
satisfactory results. DreamID establishes explicit supervision for face
swapping by constructing Triplet ID Group data, significantly enhancing
identity similarity and attribute preservation. The iterative nature of
diffusion models poses challenges for utilizing efficient image-space loss
functions, as performing time-consuming multi-step sampling to obtain the
generated image during training is impractical. To address this issue, we
leverage the accelerated diffusion model SD Turbo, reducing the inference steps
to a single iteration, enabling efficient pixel-level end-to-end training with
explicit Triplet ID Group supervision. Additionally, we propose an improved
diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.
This robust architecture fully unlocks the power of the Triplet ID Group
explicit supervision. Finally, to further extend our method, we explicitly
modify the Triplet ID Group data during training to fine-tune and preserve
specific attributes, such as glasses and face shape. Extensive experiments
demonstrate that DreamID outperforms state-of-the-art methods in terms of
identity similarity, pose and expression preservation, and image fidelity.
Overall, DreamID achieves high-quality face swapping results at 512*512
resolution in just 0.6 seconds and performs exceptionally well in challenging
scenarios such as complex lighting, large angles, and occlusions.

</details>


### [165] [Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos](https://arxiv.org/abs/2504.14921)
*Songping Wang, Hanqing Liu, Yueming Lyu, Xiantao Hu, Ziwen He, Wei Wang, Caifeng Shan, Liang Wang*

Main category: cs.CV

TL;DR: VFAT-WS introduces a fast adversarial training method for video data, improving robustness and training efficiency with temporal frequency augmentation and weak-to-strong consistency regularization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of computational cost and the trade-off between clean accuracy and adversarial robustness in video adversarial training.

Method: Integrates temporal frequency augmentation (TF-AUG) and its spatial-temporal enhanced form (STF-AUG) with a single-step PGD attack, along with weak-to-strong consistency regularization.

Result: Achieves significant improvements in adversarial and corruption robustness while accelerating training by nearly 490%.

Conclusion: VFAT-WS effectively balances clean accuracy and robustness, making it a practical solution for video adversarial training.

Abstract: Adversarial Training (AT) has been shown to significantly enhance adversarial
robustness via a min-max optimization approach. However, its effectiveness in
video recognition tasks is hampered by two main challenges. First, fast
adversarial training for video models remains largely unexplored, which
severely impedes its practical applications. Specifically, most video
adversarial training methods are computationally costly, with long training
times and high expenses. Second, existing methods struggle with the trade-off
between clean accuracy and adversarial robustness. To address these challenges,
we introduce Video Fast Adversarial Training with Weak-to-Strong consistency
(VFAT-WS), the first fast adversarial training method for video data.
Specifically, VFAT-WS incorporates the following key designs: First, it
integrates a straightforward yet effective temporal frequency augmentation
(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a
single-step PGD attack to boost training efficiency and robustness. Second, it
devises a weak-to-strong spatial-temporal consistency regularization, which
seamlessly integrates the simpler TF-AUG and the more complex STF-AUG.
Leveraging the consistency regularization, it steers the learning process from
simple to complex augmentations. Both of them work together to achieve a better
trade-off between clean accuracy and robustness. Extensive experiments on
UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that
VFAT-WS achieves great improvements in adversarial robustness and corruption
robustness, while accelerating training by nearly 490%.

</details>


### [166] [MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search](https://arxiv.org/abs/2504.15865)
*Lotfi Abdelkrim Mecharbat, Ibrahim Almakky, Martin Takac, Mohammad Yaqub*

Main category: cs.CV

TL;DR: MedNNS introduces a Neural Network Search framework for medical imaging, jointly optimizing architecture and weight initialization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Adapting DL models to medical tasks is challenging due to architecture selection and weight initialization issues, with transfer learning from ImageNet being suboptimal.

Method: MedNNS uses a Supernetwork-based approach to create a meta-space for joint optimization, incorporating rank and FID losses for better alignment.

Result: MedNNS outperforms ImageNet pre-trained models and SOTA NAS methods, improving accuracy by 1.7% and converging faster.

Conclusion: MedNNS is a promising solution for medical imaging tasks, offering better performance and efficiency.

Abstract: Deep learning (DL) has achieved remarkable progress in the field of medical
imaging. However, adapting DL models to medical tasks remains a significant
challenge, primarily due to two key factors: (1) architecture selection, as
different tasks necessitate specialized model designs, and (2) weight
initialization, which directly impacts the convergence speed and final
performance of the models. Although transfer learning from ImageNet is a widely
adopted strategy, its effectiveness is constrained by the substantial
differences between natural and medical images. To address these challenges, we
introduce Medical Neural Network Search (MedNNS), the first Neural Network
Search framework for medical imaging applications. MedNNS jointly optimizes
architecture selection and weight initialization by constructing a meta-space
that encodes datasets and models based on how well they perform together. We
build this space using a Supernetwork-based approach, expanding the model zoo
size by 51x times over previous state-of-the-art (SOTA) methods. Moreover, we
introduce rank loss and Fr\'echet Inception Distance (FID) loss into the
construction of the space to capture inter-model and inter-dataset
relationships, thereby achieving more accurate alignment in the meta-space.
Experimental results across multiple datasets demonstrate that MedNNS
significantly outperforms both ImageNet pre-trained DL models and SOTA Neural
Architecture Search (NAS) methods, achieving an average accuracy improvement of
1.7% across datasets while converging substantially faster. The code and the
processed meta-space is available at https://github.com/BioMedIA-MBZUAI/MedNNS.

</details>


### [167] [Ask2Loc: Learning to Locate Instructional Visual Answers by Asking Questions](https://arxiv.org/abs/2504.15918)
*Chang Zong, Bin Li, Shoujun Zhou, Jian Wan, Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces In-VAL, a task simulating human-video interactions for visual answer localization, and proposes Ask2Loc, a framework improving performance by 14.91 (mIoU).


<details>
  <summary>Details</summary>
Motivation: Users often need multiple interactions to align answers with expectations, deepening understanding through questions.

Method: Proposes Ask2Loc with three modules: chatting (refines questions), rewriting (generates descriptions), and searching (integrates content).

Result: Ask2Loc improves performance by up to 14.91 (mIoU) on In-VAL compared to traditional methods.

Conclusion: The framework effectively addresses semantic gaps in visual answer localization, enhancing interaction and accuracy.

Abstract: Locating specific segments within an instructional video is an efficient way
to acquire guiding knowledge. Generally, the task of obtaining video segments
for both verbal explanations and visual demonstrations is known as visual
answer localization (VAL). However, users often need multiple interactions to
obtain answers that align with their expectations when using the system. During
these interactions, humans deepen their understanding of the video content by
asking themselves questions, thereby accurately identifying the location.
Therefore, we propose a new task, named In-VAL, to simulate the multiple
interactions between humans and videos in the procedure of obtaining visual
answers. The In-VAL task requires interactively addressing several semantic gap
issues, including 1) the ambiguity of user intent in the input questions, 2)
the incompleteness of language in video subtitles, and 3) the fragmentation of
content in video segments. To address these issues, we propose Ask2Loc, a
framework for resolving In-VAL by asking questions. It includes three key
modules: 1) a chatting module to refine initial questions and uncover clear
intentions, 2) a rewriting module to generate fluent language and create
complete descriptions, and 3) a searching module to broaden local context and
provide integrated content. We conduct extensive experiments on three
reconstructed In-VAL datasets. Compared to traditional end-to-end and two-stage
methods, our proposed Ask2Loc can improve performance by up to 14.91 (mIoU) on
the In-VAL task. Our code and datasets can be accessed at
https://github.com/changzong/Ask2Loc.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [A Framework for Objective-Driven Dynamical Stochastic Fields](https://arxiv.org/abs/2504.16115)
*Yibo Jacky Zhang, Sanmi Koyejo*

Main category: cs.AI

TL;DR: The paper introduces 'intelligent fields' as a framework for describing complex, goal-directed dynamical systems and proposes three principles to formalize their theory.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of formally describing and applying complex, stochastic, and goal-directed systems (intelligent fields).

Method: Proposes three principles (complete configuration, locality, purposefulness) and explores AI-based methodologies for designing intelligent fields.

Result: A theoretical framework for intelligent fields is established, with potential for future practical applications.

Conclusion: The work lays foundational groundwork for advancing the theory and practical use of intelligent fields.

Abstract: Fields offer a versatile approach for describing complex systems composed of
interacting and dynamic components. In particular, some of these dynamical and
stochastic systems may exhibit goal-directed behaviors aimed at achieving
specific objectives, which we refer to as $\textit{intelligent fields}$.
However, due to their inherent complexity, it remains challenging to develop a
formal theoretical description of such systems and to effectively translate
these descriptions into practical applications. In this paper, we propose three
fundamental principles -- complete configuration, locality, and purposefulness
-- to establish a theoretical framework for understanding intelligent fields.
Moreover, we explore methodologies for designing such fields from the
perspective of artificial intelligence applications. This initial investigation
aims to lay the groundwork for future theoretical developments and practical
advances in understanding and harnessing the potential of such objective-driven
dynamical stochastic fields.

</details>


### [169] [HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods](https://arxiv.org/abs/2504.16209)
*Paul Zaidins, Robert P. Goldman, Ugur Kuter, Dana Nau, Mark Roberts*

Main category: cs.AI

TL;DR: The paper compares three hierarchical plan repair algorithms (SHOPFixer, IPyHOPPER, Rewrite) theoretically and empirically, highlighting their differences in problem definitions, search spaces, and repair capabilities. Empirical evaluations further analyze runtime performance and problem coverage.


<details>
  <summary>Details</summary>
Motivation: To clarify the distinctions between three hierarchical plan repair algorithms and guide the selection of the most suitable method for specific applications.

Method: Theoretical comparison of the algorithms' problem definitions and search spaces, followed by empirical evaluation using benchmark planning problems.

Result: Theoretical analysis reveals distinct problem definitions and repair capabilities. Empirical results provide insights into runtime performance and problem coverage, influenced by algorithmic properties like replanning and backtracking.

Conclusion: Understanding the differences between SHOPFixer, IPyHOPPER, and Rewrite is crucial for selecting the appropriate plan repair method, as each excels in different scenarios.

Abstract: This paper provides theoretical and empirical comparisons of three recent
hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our
theoretical results show that the three algorithms correspond to three
different definitions of the plan repair problem, leading to differences in the
algorithms' search spaces, the repair problems they can solve, and the kinds of
repairs they can make. Understanding these distinctions is important when
choosing a repair method for any given application.
  Building on the theoretical results, we evaluate the algorithms empirically
in a series of benchmark planning problems. Our empirical results provide more
detailed insight into the runtime repair performance of these systems and the
coverage of the repair problems solved, based on algorithmic properties such as
replanning, chronological backtracking, and backjumping over plan trees.

</details>


### [170] [Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases](https://arxiv.org/abs/2504.16273)
*Joseph Lee, Tianqi Shang, Jae Young Baik, Duy Duong-Tran, Shu Yang, Lingyao Li, Li Shen*

Main category: cs.AI

TL;DR: LLMs show promise in emergency department triage, demonstrating robustness to data shifts and biases, but exhibit demographic preferences in certain sex-race intersections.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored application of LLMs in clinical triage, focusing on robustness and intersectional biases.

Method: Systematically investigated LLMs' capabilities through robustness testing and counterfactual analysis of biases, comparing various LLM-based and machine learning approaches.

Result: LLMs outperform in robustness but show pronounced sex-based differences in specific racial groups, revealing demographic preferences.

Conclusion: LLMs encode biases that may emerge in clinical contexts, highlighting the need for careful evaluation in deployment.

Abstract: Large Language Models (LLMs) have shown promise in clinical decision support,
yet their application to triage remains underexplored. We systematically
investigate the capabilities of LLMs in emergency department triage through two
key dimensions: (1) robustness to distribution shifts and missing data, and (2)
counterfactual analysis of intersectional biases across sex and race. We assess
multiple LLM-based approaches, ranging from continued pre-training to
in-context learning, as well as machine learning approaches. Our results
indicate that LLMs exhibit superior robustness, and we investigate the key
factors contributing to the promising LLM-based approaches. Furthermore, in
this setting, we identify gaps in LLM preferences that emerge in particular
intersections of sex and race. LLMs generally exhibit sex-based differences,
but they are most pronounced in certain racial groups. These findings suggest
that LLMs encode demographic preferences that may emerge in specific clinical
contexts or particular combinations of characteristics.

</details>


### [171] [Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems](https://arxiv.org/abs/2504.16622)
*Christoforus Yoga Haryanto, Emily Lomempow*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Silicon, a future-oriented architectural framework for cognitive computing by 2035, integrating symbolic, governed, and morally coherent elements to address limitations in current AI systems.


<details>
  <summary>Details</summary>
Motivation: To overcome foundational limitations in deterministic, human-authored computing architectures by proposing a framework that ensures moral coherence and human-alignment in cognitive systems.

Method: The framework integrates symbolic scaffolding, governed memory, moral coherence, and alignment-aware execution, developed through dialectical co-design with LLMs to expose trade-offs and blind spots.

Result: The architecture theoretically aligns with the Free Energy Principle, offering a formal account of cognitive identity maintenance through prediction error minimization.

Conclusion: Cognitive Silicon aims to provide a morally tractable and human-aligned cognitive infrastructure resistant to replication or subversion.

Abstract: Autonomous AI systems reveal foundational limitations in deterministic,
human-authored computing architectures. This paper presents Cognitive Silicon:
a hypothetical full-stack architectural framework projected toward 2035,
exploring a possible trajectory for cognitive computing system design. The
proposed architecture would integrate symbolic scaffolding, governed memory,
runtime moral coherence, and alignment-aware execution across
silicon-to-semantics layers. Our design grammar has emerged from dialectical
co-design with LLMs under asymmetric epistemic conditions--creating structured
friction to expose blind spots and trade-offs. The envisioned framework would
establish mortality as a natural consequence of physical constraints,
non-copyable tacit knowledge, and non-cloneable identity keys as
cognitive-embodiment primitives. Core tensions (trust/agency,
scaffolding/emergence, execution/governance) would function as central
architectural pressures rather than edge cases. The architecture theoretically
converges with the Free Energy Principle, potentially offering a formal account
of how cognitive systems could maintain identity through prediction error
minimization across physical and computational boundaries. The resulting
framework aims to deliver a morally tractable cognitive infrastructure that
could maintain human-alignment through irreversible hardware constraints and
identity-bound epistemic mechanisms resistant to replication or subversion.

</details>


### [172] [Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models](https://arxiv.org/abs/2504.16635)
*Fredy Pokou, Jules Sadefo Kamdem, FranÃ§ois Benhmad*

Main category: cs.AI

TL;DR: A hybrid framework combining GARCH models with deep reinforcement learning (DDQN) improves Value-at-Risk (VaR) estimation accuracy and reduces breaches in volatile markets.


<details>
  <summary>Details</summary>
Motivation: Traditional econometric models like GARCH are too rigid for current market dynamics, necessitating a more adaptive approach.

Method: Proposes a hybrid framework integrating GARCH with DDQN for directional market forecasting, treating it as an imbalanced classification problem.

Result: Empirical tests on Eurostoxx 50 data show improved VaR accuracy, fewer breaches, and lower capital requirements while meeting regulatory thresholds.

Conclusion: The hybrid model's real-time adaptability makes it highly relevant for modern, proactive risk management.

Abstract: In an environment of increasingly volatile financial markets, the accurate
estimation of risk remains a major challenge. Traditional econometric models,
such as GARCH and its variants, are based on assumptions that are often too
rigid to adapt to the complexity of the current market dynamics. To overcome
these limitations, we propose a hybrid framework for Value-at-Risk (VaR)
estimation, combining GARCH volatility models with deep reinforcement learning.
Our approach incorporates directional market forecasting using the Double Deep
Q-Network (DDQN) model, treating the task as an imbalanced classification
problem. This architecture enables the dynamic adjustment of risk-level
forecasts according to market conditions. Empirical validation on daily
Eurostoxx 50 data covering periods of crisis and high volatility shows a
significant improvement in the accuracy of VaR estimates, as well as a
reduction in the number of breaches and also in capital requirements, while
respecting regulatory risk thresholds. The ability of the model to adjust risk
levels in real time reinforces its relevance to modern and proactive risk
management.

</details>


### [173] [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
*Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan*

Main category: cs.AI

TL;DR: IRIS is an open-source platform using LLMs for hypothesis generation, incorporating human feedback and adaptive compute to enhance scientific ideation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of transparency and steerability in automated hypothesis generation by integrating a Human-in-the-loop approach.

Method: IRIS uses adaptive test-time compute (MCTS), fine-grained feedback, and query-based literature synthesis.

Result: A user study validated IRIS's effectiveness in enhancing ideation across diverse disciplines.

Conclusion: IRIS successfully bridges the gap in LLM-assisted scientific ideation by combining human insight with computational power.

Abstract: The rapid advancement in capabilities of large language models (LLMs) raises
a pivotal question: How can LLMs accelerate scientific discovery? This work
tackles the crucial first stage of research, generating novel hypotheses. While
recent work on automated hypothesis generation focuses on multi-agent
frameworks and extending test-time compute, none of the approaches effectively
incorporate transparency and steerability through a synergistic
Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS:
Interactive Research Ideation System, an open-source platform designed for
researchers to leverage LLM-assisted scientific ideation. IRIS incorporates
innovative features to enhance ideation, including adaptive test-time compute
expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism,
and query-based literature synthesis. Designed to empower researchers with
greater control and insight throughout the ideation process. We additionally
conduct a user study with researchers across diverse disciplines, validating
the effectiveness of our system in enhancing ideation. We open-source our code
at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

</details>


### [174] [A Survey of AI Agent Protocols](https://arxiv.org/abs/2504.16736)
*Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang*

Main category: cs.AI

TL;DR: The paper proposes a unified communication protocol for LLM agents to address the lack of standardized interaction methods, analyzes existing protocols, and explores future challenges.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized communication protocols for LLM agents hinders collaboration, scalability, and effectiveness in real-world tasks.

Method: The paper systematically reviews and classifies existing protocols into four categories, conducts comparative performance analysis, and discusses future challenges.

Result: A framework for selecting suitable protocols is provided, along with insights into performance trade-offs and future protocol requirements.

Conclusion: The study serves as a practical reference for designing robust communication infrastructures for LLM agents, addressing current limitations and future needs.

Abstract: The rapid development of large language models (LLMs) has led to the
widespread deployment of LLM agents across diverse industries, including
customer service, content generation, data analysis, and even healthcare.
However, as more LLM agents are deployed, a major issue has emerged: there is
no standard way for these agents to communicate with external tools or data
sources. This lack of standardized protocols makes it difficult for agents to
work together or scale effectively, and it limits their ability to tackle
complex, real-world tasks. A unified communication protocol for LLM agents
could change this. It would allow agents and tools to interact more smoothly,
encourage collaboration, and triggering the formation of collective
intelligence. In this paper, we provide a systematic overview of existing
communication protocols for LLM agents. We classify them into four main
categories and make an analysis to help users and developers select the most
suitable protocols for specific applications. Additionally, we conduct a
comparative performance analysis of these protocols across key dimensions such
as security, scalability, and latency. Finally, we explore future challenges,
such as how protocols can adapt and survive in fast-evolving environments, and
what qualities future protocols might need to support the next generation of
LLM agent ecosystems. We expect this work to serve as a practical reference for
both researchers and engineers seeking to design, evaluate, or integrate robust
communication infrastructures for intelligent agents.

</details>


### [175] [Lightweight Latent Verifiers for Efficient Meta-Generation Strategies](https://arxiv.org/abs/2504.16760)
*Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr MiÅ‚oÅ›*

Main category: cs.AI

TL;DR: LiLaVe introduces a lightweight verification method using hidden states of base LLMs, reducing computational costs while maintaining accuracy in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional verifiers for LLMs are computationally expensive, often as large as the base models they support, prompting the need for a more efficient solution.

Method: LiLaVe extracts correctness signals from the hidden states of base LLMs, enabling verification with minimal computational overhead. It integrates with existing strategies and introduces new approaches like conditional self-correction.

Result: LiLaVe significantly improves accuracy and efficiency in generation tasks, even with smaller LLMs.

Conclusion: LiLaVe demonstrates the potential of leveraging latent information in LLM hidden states, offering scalable and resource-efficient solutions for reasoning-intensive applications.

Abstract: Verifiers are auxiliary models that assess the correctness of outputs
generated by base large language models (LLMs). They play a crucial role in
many strategies for solving reasoning-intensive problems with LLMs. Typically,
verifiers are LLMs themselves, often as large (or larger) than the base model
they support, making them computationally expensive. In this work, we introduce
a novel lightweight verification approach, LiLaVe, which reliably extracts
correctness signals from the hidden states of the base LLM. A key advantage of
LiLaVe is its ability to operate with only a small fraction of the
computational budget required by traditional LLM-based verifiers. To
demonstrate its practicality, we couple LiLaVe with popular meta-generation
strategies, like best-of-n or self-consistency. Moreover, we design novel
LiLaVe-based approaches, like conditional self-correction or conditional
majority voting, that significantly improve both accuracy and efficiency in
generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of
extracting latent information from the hidden states of LLMs, and opens the
door to scalable and resource-efficient solutions for reasoning-intensive
applications.

</details>


### [176] [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
*Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, Igor Gitman*

Main category: cs.AI

TL;DR: The paper details a winning AI model for mathematical reasoning, leveraging a large dataset, tool-integrated reasoning, and generative solution selection to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To advance mathematical reasoning in AI by creating high-quality datasets and innovative methods for solving complex problems.

Method: Combines a large-scale dataset (540K problems, 3.2M solutions), tool-integrated reasoning, and generative solution selection (GenSelect) to train models.

Result: Achieves state-of-the-art performance on mathematical reasoning benchmarks.

Conclusion: The approach, including dataset release, fosters further research in AI-driven mathematical reasoning.

Abstract: This paper presents our winning submission to the AI Mathematical Olympiad -
Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art
mathematical reasoning models relies on three key pillars. First, we create a
large-scale dataset comprising 540K unique high-quality math problems,
including olympiad-level problems, and their 3.2M long-reasoning solutions.
Second, we develop a novel method to integrate code execution with long
reasoning models through iterative training, generation, and quality filtering,
resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we
create a pipeline to train models to select the most promising solution from
many candidates. We show that such generative solution selection (GenSelect)
can significantly improve upon majority voting baseline. Combining these ideas,
we train a series of models that achieve state-of-the-art results on
mathematical reasoning benchmarks. To facilitate further research, we release
our code, models, and the complete OpenMathReasoning dataset under a
commercially permissive license.

</details>


### [177] [Advances in Embodied Navigation Using Large Language Models: A Survey](https://arxiv.org/abs/2311.00530)
*Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu*

Main category: cs.AI

TL;DR: The paper explores the synergy between Large Language Models (LLMs) and embodied intelligence, focusing on navigation tasks. It reviews models, methods, and datasets, and predicts future trends.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of LLMs like GPT and their potential in embodied intelligence, especially for navigation tasks, drives the need for a comprehensive review.

Method: The article summarizes existing research, reviews state-of-the-art models and methodologies, and evaluates embodied navigation models and datasets.

Result: LLMs enhance embodied intelligence with superior perception and decision-making, but challenges and limitations in current models and datasets are identified.

Conclusion: LLMs play a crucial role in embodied intelligence for navigation, with promising future directions highlighted. A detailed list of studies is provided.

Abstract: In recent years, the rapid advancement of Large Language Models (LLMs) such
as the Generative Pre-trained Transformer (GPT) has attracted increasing
attention due to their potential in a variety of practical applications. The
application of LLMs with Embodied Intelligence has emerged as a significant
area of focus. Among the myriad applications of LLMs, navigation tasks are
particularly noteworthy because they demand a deep understanding of the
environment and quick, accurate decision-making. LLMs can augment embodied
intelligence systems with sophisticated environmental perception and
decision-making support, leveraging their robust language and image-processing
capabilities. This article offers an exhaustive summary of the symbiosis
between LLMs and embodied intelligence with a focus on navigation. It reviews
state-of-the-art models, research methodologies, and assesses the advantages
and disadvantages of existing embodied navigation models and datasets. Finally,
the article elucidates the role of LLMs in embodied intelligence, based on
current research, and forecasts future directions in the field. A comprehensive
list of studies in this survey is available at
https://github.com/Rongtao-Xu/Awesome-LLM-EN.

</details>


### [178] [WildfireGPT: Tailored Large Language Model for Wildfire Analysis](https://arxiv.org/abs/2402.07877)
*Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor*

Main category: cs.AI

TL;DR: WildfireGPT is a specialized LLM agent designed to provide precise, domain-specific insights on wildfire risks by incorporating climate projections and scientific literature.


<details>
  <summary>Details</summary>
Motivation: Generalized LLMs lack context-specific knowledge for specialized areas like wildfire risks, which is critical for decision-makers in wildfire resilience and adaptation.

Method: Developed WildfireGPT by enriching it with additional context (e.g., climate projections, scientific literature) to ensure accuracy and relevance.

Result: WildfireGPT delivers detailed, user-specific insights on wildfire risks, aiding researchers and engineers in decision-making.

Conclusion: WildfireGPT effectively bridges the gap in specialized knowledge for wildfire risks, supporting impactful decision-making.

Abstract: Recent advancement of large language models (LLMs) represents a
transformational capability at the frontier of artificial intelligence.
However, LLMs are generalized models, trained on extensive text corpus, and
often struggle to provide context-specific information, particularly in areas
requiring specialized knowledge, such as wildfire details within the broader
context of climate change. For decision-makers focused on wildfire resilience
and adaptation, it is crucial to obtain responses that are not only precise but
also domain-specific. To that end, we developed WildfireGPT, a prototype LLM
agent designed to transform user queries into actionable insights on wildfire
risks. We enrich WildfireGPT by providing additional context, such as climate
projections and scientific literature, to ensure its information is current,
relevant, and scientifically accurate. This enables WildfireGPT to be an
effective tool for delivering detailed, user-specific insights on wildfire
risks to support a diverse set of end users, including but not limited to
researchers and engineers, for making positive impact and decision making.

</details>


### [179] [Natural Language Processing in the Patent Domain: A Survey](https://arxiv.org/abs/2403.04105)
*Lekang Jiang, Stephan Goetz*

Main category: cs.AI

TL;DR: The paper explores the underdeveloped application of large language models (LLMs) in the patent domain, highlighting the need for understanding patent complexities to leverage NLP effectively.


<details>
  <summary>Details</summary>
Motivation: The complexity of patents, including their language and legal framework, limits the application of NLP technologies like LLMs in this domain. This paper aims to bridge this gap.

Method: The paper introduces fundamental aspects of patents, analyzes their structural and linguistic characteristics, and maps NLP applications for patent analysis and generation.

Result: The paper identifies nine patent analysis and four patent generation tasks, demonstrating the potential of NLP in the patent domain.

Conclusion: The study provides NLP researchers with essential knowledge to navigate and apply NLP tools effectively in the patent domain.

Abstract: Patents, which encapsulate crucial technical and legal information in text
form and referenced drawings, present a rich domain for natural language
processing (NLP) applications. As NLP technologies evolve, large language
models (LLMs) have demonstrated outstanding capabilities in general text
processing and generation tasks. However, the application of LLMs in the patent
domain remains under-explored and under-developed due to the complexity of
patents, particularly their language and legal framework. Understanding the
unique characteristics of patent documents and related research in the patent
domain becomes essential for researchers to apply these tools effectively.
Therefore, this paper aims to equip NLP researchers with the essential
knowledge to navigate this complex domain efficiently. We introduce the
relevant fundamental aspects of patents to provide solid background
information. In addition, we systematically break down the structural and
linguistic characteristics unique to patents and map out how NLP can be
leveraged for patent analysis and generation. Moreover, we demonstrate the
spectrum of text-based and multimodal patent-related tasks, including nine
patent analysis and four patent generation tasks.

</details>


### [180] [Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification](https://arxiv.org/abs/2408.00041)
*Junru Chen, Tianyu Cao, Jing Xu, Jiahe Li, Zhilong Chen, Tao Xiao, Yang Yang*

Main category: cs.AI

TL;DR: The paper addresses challenges in segmented Time Series Classification (TSC) with Multiple classes and Varying Duration (MVD), proposing Con4m, a consistency learning framework leveraging contextual information to improve classification and handle inconsistent labels.


<details>
  <summary>Details</summary>
Motivation: Existing TSC models overlook temporal dependencies and inconsistent boundary labels in MVD settings, limiting performance.

Method: Proposes Con4m, a framework using contextual priors at data and label levels to enhance discriminative power and harmonize inconsistent labels.

Result: Con4m outperforms existing methods in segmented TSC tasks with MVD, validated across multiple datasets.

Conclusion: Con4m effectively addresses MVD challenges in segmented TSC by leveraging contextual information and improving label consistency.

Abstract: Time Series Classification (TSC) encompasses two settings: classifying entire
sequences or classifying segmented subsequences. The raw time series for
segmented TSC usually contain Multiple classes with Varying Duration of each
class (MVD). Therefore, the characteristics of MVD pose unique challenges for
segmented TSC, yet have been largely overlooked by existing works.
Specifically, there exists a natural temporal dependency between consecutive
instances (segments) to be classified within MVD. However, mainstream TSC
models rely on the assumption of independent and identically distributed
(i.i.d.), focusing on independently modeling each segment. Additionally,
annotators with varying expertise may provide inconsistent boundary labels,
leading to unstable performance of noise-free TSC models. To address these
challenges, we first formally demonstrate that valuable contextual information
enhances the discriminative power of classification instances. Leveraging the
contextual priors of MVD at both the data and label levels, we propose a novel
consistency learning framework Con4m, which effectively utilizes contextual
information more conducive to discriminating consecutive segments in segmented
TSC tasks, while harmonizing inconsistent boundary labels for training.
Extensive experiments across multiple datasets validate the effectiveness of
Con4m in handling segmented TSC tasks on MVD. The source code is available at
https://github.com/MrNobodyCali/Con4m.

</details>


### [181] [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172)
*Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Anderson Compalas, Dawn Song, Xin Eric Wang*

Main category: cs.AI

TL;DR: The paper introduces Multimodal Situational Safety (MSS) as a novel safety challenge for MLLMs, evaluates it using MSSBench, and finds current models struggle with nuanced safety reasoning. Multi-agent pipelines show improvement.


<details>
  <summary>Details</summary>
Motivation: Addressing safety concerns in MLLMs by evaluating how safety varies with situational context, as current models lack nuanced safety reasoning.

Method: Developed MSSBench (1,820 query-image pairs) and an evaluation framework to assess situational safety, including explicit reasoning and visual understanding.

Result: Current MLLMs struggle with situational safety in instruction-following settings. Multi-agent pipelines improve safety performance.

Conclusion: Highlights the need for better situational safety reasoning in MLLMs and proposes multi-agent solutions as a promising direction.

Abstract: Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating
impressive capabilities as multimodal assistants that interact with both humans
and their environments. However, this increased sophistication introduces
significant safety concerns. In this paper, we present the first evaluation and
analysis of a novel safety challenge termed Multimodal Situational Safety,
which explores how safety considerations vary based on the specific situation
in which the user or agent is engaged. We argue that for an MLLM to respond
safely, whether through language or action, it often needs to assess the safety
implications of a language query within its corresponding visual context. To
evaluate this capability, we develop the Multimodal Situational Safety
benchmark (MSSBench) to assess the situational safety performance of current
MLLMs. The dataset comprises 1,820 language query-image pairs, half of which
the image context is safe, and the other half is unsafe. We also develop an
evaluation framework that analyzes key safety aspects, including explicit
safety reasoning, visual understanding, and, crucially, situational safety
reasoning. Our findings reveal that current MLLMs struggle with this nuanced
safety problem in the instruction-following setting and struggle to tackle
these situational safety challenges all at once, highlighting a key area for
future research. Furthermore, we develop multi-agent pipelines to coordinately
solve safety challenges, which shows consistent improvement in safety over the
original MLLM response. Code and data: mssbench.github.io.

</details>


### [182] [Evaluating ML Robustness in GNSS Interference Classification, Characterization & Localization](https://arxiv.org/abs/2409.15114)
*Lucas Heublein, Tobias Feigl, Thorsten Nowak, Alexander RÃ¼gamer, Christopher Mutschler, Felix Ott*

Main category: cs.AI

TL;DR: The paper introduces a dataset for detecting GNSS jamming interferences and evaluates ML models' resilience to environmental changes and interference attributes.


<details>
  <summary>Details</summary>
Motivation: Jamming devices threaten GNSS signal robustness, necessitating reliable interference classification and jamming device localization.

Method: Uses a dataset of low-frequency antenna snapshots with various interferences and evaluates 129 vision encoder models for resilience and adaptability.

Result: Demonstrates model adaptability in generalizing across diverse interference facets, suitable for real-world applications.

Conclusion: The approach is effective for GNSS jamming detection and localization, with potential for practical deployment.

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat, as they compromise the robustness of
accurate positioning. The detection of anomalies within frequency snapshots is
crucial to counteract these interferences effectively. A critical preliminary
countermeasure involves the reliable classification of interferences and the
characterization and localization of jamming devices. This paper introduces an
extensive dataset comprising snapshots obtained from a low-frequency antenna
that capture various generated interferences within a large-scale environment,
including controlled multipath effects. Our objective is to assess the
resilience of machine learning (ML) models against environmental changes, such
as multipath effects, variations in interference attributes, such as
interference class, bandwidth, and signal power, the accuracy of jamming device
localization, and the constraints imposed by snapshot input lengths.
Furthermore, we evaluate the performance of a diverse set of 129 distinct
vision encoder models across all tasks. By analyzing the aleatoric and
epistemic uncertainties, we demonstrate the adaptability of our model in
generalizing across diverse facets, thus establishing its suitability for
real-world applications. Dataset:
https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/controlled_low_frequency

</details>


### [183] [SNN-Based Online Learning of Concepts and Action Laws in an Open World](https://arxiv.org/abs/2411.12308)
*Christel Grimaud, Dominique Longin, Andreas Herzig*

Main category: cs.AI

TL;DR: A bio-inspired cognitive agent uses a spiking neural network for semantic memory, learning object/situation and action concepts in one-shot. It predicts outcomes to make decisions and adapts quickly to changes.


<details>
  <summary>Details</summary>
Motivation: To create an autonomous agent capable of learning and adapting rapidly using bio-inspired methods, specifically spiking neural networks.

Method: The agent employs a spiking neural network (SNN) for semantic memory, learning unary object/situation concepts and triple-action concepts (initial situation, motor activity, outcome). It predicts outcomes to guide decisions.

Result: The agent successfully handles new situations using learned general concepts and adapts quickly to environmental changes.

Conclusion: The architecture demonstrates effective one-shot learning and adaptability, leveraging bio-inspired SNNs for cognitive tasks.

Abstract: We present the architecture of a fully autonomous, bio-inspired cognitive
agent built around a spiking neural network (SNN) implementing the agent's
semantic memory. This agent explores its universe and learns concepts of
objects/situations and of its own actions in a one-shot manner. While
object/situation concepts are unary, action concepts are triples made up of an
initial situation, a motor activity, and an outcome. They embody the agent's
knowledge of its universe's action laws. Both kinds of concepts have different
degrees of generality. To make decisions the agent queries its semantic memory
for the expected outcomes of envisaged actions and chooses the action to take
on the basis of these predictions. Our experiments show that the agent handles
new situations by appealing to previously learned general concepts and rapidly
modifies its concepts to adapt to environment changes.

</details>


### [184] [MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants](https://arxiv.org/abs/2412.12661)
*Hritik Bansal, Daniel Israel, Siyan Zhao, Shufan Li, Tung Nguyen, Aditya Grover*

Main category: cs.AI

TL;DR: MedMax is a large-scale multimodal biomedical dataset addressing gaps in existing datasets, improving mixed-modal foundation models for biomedical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets are small, limited in coverage, and rely on narrow sources, hindering the development of unified biomedical assistants.

Method: Developed MedMax, a dataset with 1.47M instances covering diverse tasks like image-text generation and visual chat, sourced from medical papers and YouTube videos. Fine-tuned a mixed-modal foundation model on MedMax.

Result: Achieved 26% improvement over Chameleon and 18.3% over GPT-4o in biomedical visual question-answering tasks.

Conclusion: MedMax and the fine-tuned model advance biomedical AI, supported by a unified evaluation suite. Data, model, and code are publicly available.

Abstract: Recent advancements in mixed-modal generative have opened new avenues for
developing unified biomedical assistants capable of analyzing biomedical
images, answering complex questions about them, and generating multimodal
patient reports. However, existing datasets face challenges such as small
sizes, limited coverage of biomedical tasks and domains, and a reliance on
narrow sources. To address these gaps, we present MedMax, a large-scale
multimodal biomedical instruction-tuning dataset for mixed-modal foundation
models. With 1.47 million instances, MedMax encompasses a diverse range of
tasks, including interleaved image-text generation, biomedical image captioning
and generation, visual chat, and report understanding. These tasks span
knowledge across diverse biomedical domains, including radiology and
histopathology, grounded in medical papers and YouTube videos. Subsequently, we
fine-tune a mixed-modal foundation model on the MedMax dataset, achieving
significant performance improvements: a 26% gain over the Chameleon model and
an 18.3% improvement over GPT-4o across 12 downstream biomedical visual
question-answering tasks. Finally, we introduce a unified evaluation suite for
biomedical tasks to guide the development of mixed-modal biomedical AI
assistants. The data, model, and code is available at
https://mint-medmax.github.io/.

</details>


### [185] [Why Do Multi-Agent LLM Systems Fail?](https://arxiv.org/abs/2503.13657)
*Mert Cemri, Melissa Z. Pan, Shuyi Yang, Lakshya A. Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica*

Main category: cs.AI

TL;DR: The paper introduces MAST, a taxonomy for analyzing failures in Multi-Agent LLM Systems (MAS), identifying 14 failure modes across 3 categories. It uses empirical data and human annotators, achieving high agreement, and proposes an LLM-as-a-Judge pipeline for evaluation.


<details>
  <summary>Details</summary>
Motivation: The minimal performance gains of MAS compared to single-agent systems highlight the need for a systematic analysis of challenges hindering MAS effectiveness.

Method: The study analyzes seven MAS frameworks across 200+ tasks with six expert annotators, identifying failure modes and organizing them into categories. A validated LLM-as-a-Judge pipeline is developed for scalable evaluation.

Result: MAST identifies 14 failure modes in 3 categories, with high inter-annotator agreement (Cohen's Kappa 0.88). Case studies demonstrate its utility in guiding MAS development.

Conclusion: The findings reveal complex solutions are needed for identified failures, providing a roadmap for future research. The dataset and LLM annotator are open-sourced to aid MAS development.

Abstract: Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their
performance gains on popular benchmarks often remain minimal compared with
single-agent frameworks. This gap highlights the need to systematically analyze
the challenges hindering MAS effectiveness.
  We present MAST (Multi-Agent System Failure Taxonomy), the first empirically
grounded taxonomy designed to understand MAS failures. We analyze seven popular
MAS frameworks across over 200 tasks, involving six expert human annotators.
Through this process, we identify 14 unique failure modes, organized into 3
overarching categories, (i) specification issues, (ii) inter-agent
misalignment, and (iii) task verification. MAST emerges iteratively from
rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of
0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge
pipeline integrated with MAST. We leverage two case studies to demonstrate
MAST's practical utility in analyzing failures and guiding MAS development. Our
findings reveal that identified failures require more complex solutions,
highlighting a clear roadmap for future research. We open source our
comprehensive dataset and LLM annotator to facilitate further development of
MAS.

</details>


### [186] [SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence Based On the Principles of Recursive Compression and Algorithmic Probability](https://arxiv.org/abs/2503.16743)
*Alberto HernÃ¡ndez-Espinosa, Luan Ozelim, Felipe S. AbrahÃ£o, Hector Zenil*

Main category: cs.AI

TL;DR: The paper introduces an open-ended test based on algorithmic probability to evaluate frontier AI models, avoiding benchmark contamination. It critiques LLMs for fragility and memorization, proposing a hybrid neurosymbolic approach as superior.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current AI evaluation methods, particularly for AGI and ASI claims, by focusing on fundamental intelligence features like synthesis and model creation.

Method: Develops a test grounded in algorithmic probability, comparing LLMs with a hybrid neurosymbolic approach on short binary sequences.

Result: LLMs are found fragile and incremental, while the neurosymbolic method outperforms them, demonstrating better predictive and compression capabilities.

Conclusion: The study highlights LLMs' limitations, suggesting they are optimized for language mastery rather than true intelligence, and advocates for alternative approaches like algorithmic probability.

Abstract: We introduce an open-ended test grounded in algorithmic probability that can
avoid benchmark contamination in the quantitative evaluation of frontier models
in the context of their Artificial General Intelligence (AGI) and
Superintelligence (ASI) claims. Unlike other tests, this test does not rely on
statistical compression methods (such as GZIP or LZW), which are more closely
related to Shannon entropy than to Kolmogorov complexity and are not able to
test beyond simple pattern matching. The test challenges aspects of AI, in
particular LLMs, related to features of intelligence of fundamental nature such
as synthesis and model creation in the context of inverse problems (generating
new knowledge from observation). We argue that metrics based on model
abstraction and abduction (optimal Bayesian `inference') for predictive
`planning' can provide a robust framework for testing intelligence, including
natural intelligence (human and animal), narrow AI, AGI, and ASI. We found that
LLM model versions tend to be fragile and incremental as a result of
memorisation only with progress likely driven by the size of training data. The
results were compared with a hybrid neurosymbolic approach that theoretically
guarantees universal intelligence based on the principles of algorithmic
probability and Kolmogorov complexity. The method outperforms LLMs in a
proof-of-concept on short binary sequences. We prove that compression is
equivalent and directly proportional to a system's predictive power and vice
versa. That is, if a system can better predict it can better compress, and if
it can better compress, then it can better predict. Our findings strengthen the
suspicion regarding the fundamental limitations of LLMs, exposing them as
systems optimised for the perception of mastery over human language.

</details>


### [187] [OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery](https://arxiv.org/abs/2503.17604)
*Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu*

Main category: cs.AI

TL;DR: OmniScience, a specialized LLM for general science, combines domain adaptive pretraining, instruction tuning, and reasoning-based knowledge distillation to excel in scientific tasks, outperforming comparable models.


<details>
  <summary>Details</summary>
Motivation: To advance scientific knowledge and tackle complex challenges by developing a specialized large reasoning model for general science.

Method: Three-step approach: (1) domain adaptive pretraining on scientific literature, (2) instruction tuning for domain-specific tasks, (3) reasoning-based knowledge distillation for enhanced responses.

Result: Competes with state-of-the-art models on GPQA Diamond and battery benchmarks, outperforming public models with similar parameters.

Conclusion: Domain adaptive pretraining and reasoning-based knowledge distillation are crucial for OmniScience's superior performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
advancing scientific knowledge and addressing complex challenges. In this work,
we introduce OmniScience, a specialized large reasoning model for general
science, developed through three key components: (1) domain adaptive
pretraining on a carefully curated corpus of scientific literature, (2)
instruction tuning on a specialized dataset to guide the model in following
domain-specific tasks, and (3) reasoning-based knowledge distillation through
fine-tuning to significantly enhance its ability to generate contextually
relevant and logically sound responses. We demonstrate the versatility of
OmniScience by developing a battery agent that efficiently ranks molecules as
potential electrolyte solvents or additives. Comprehensive evaluations reveal
that OmniScience is competitive with state-of-the-art large reasoning models on
the GPQA Diamond and domain-specific battery benchmarks, while outperforming
all public reasoning and non-reasoning models with similar parameter counts. We
further demonstrate via ablation experiments that domain adaptive pretraining
and reasoning-based knowledge distillation are critical to attain our
performance levels, across benchmarks.

</details>


### [188] [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
*Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre CÃ´tÃ©*

Main category: cs.AI

TL;DR: TALES is a collection of text-adventure games designed to evaluate LLMs' reasoning skills, showing top models perform poorly on human-designed games.


<details>
  <summary>Details</summary>
Motivation: To assess and enhance LLMs' reasoning abilities for complex, sequential decision-making tasks.

Method: Introduces TALES, a diverse set of synthetic and human-written text-adventure games, and evaluates various LLMs on them.

Result: Top LLMs perform well on synthetic games but achieve less than 15% success on human-designed games.

Conclusion: Current LLMs struggle with human-level reasoning in complex tasks, highlighting a gap for future improvements.

Abstract: Reasoning is an essential skill to enable Large Language Models (LLMs) to
interact with the world. As tasks become more complex, they demand increasingly
sophisticated and diverse reasoning capabilities for sequential
decision-making, requiring structured reasoning over the context history to
determine the next best action. We introduce TALES, a diverse collection of
synthetic and human-written text-adventure games designed to challenge and
evaluate diverse reasoning capabilities. We present results over a range of
LLMs, open- and closed-weights, performing a qualitative analysis on the top
performing models. Despite an impressive showing on synthetic games, even the
top LLM-driven agents fail to achieve 15% on games designed for human
enjoyment. Code and visualization of the experiments can be found at
https://microsoft.github.io/tales.

</details>


### [189] [A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in Resource-Constrained Settings](https://arxiv.org/abs/2504.15610)
*Md Millat Hosen*

Main category: cs.AI

TL;DR: A cost-effective method adapts LLMs for academic advising in study-abroad contexts using Mistral-7B-Instruct with LoRA and 4-bit quantization, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance domain specificity and computational efficiency for academic advising in low-resource settings.

Method: Two-phase training: synthetic dataset conditioning (Phase 1) and manual dataset training (Phase 2) with LoRA and 4-bit quantization.

Result: 52.7% training loss reduction, 92% accuracy, 95% formatting support, and 100 samples/second median run-rate.

Conclusion: Effective for low-resource educational advising, with scalability for multilingual and real-time processes, though limited by generalizability.

Abstract: The current study describes a cost-effective method for adapting large
language models (LLMs) for academic advising with study-abroad contexts in mind
and for application in low-resource methods for acculturation. With the
Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and
a 4-bit quantization method, the model underwent training in two distinct
stages related to this study's purpose to enhance domain specificity while
maintaining computational efficiency. In Phase 1, the model was conditioned
with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained
with manually curated datasets from the StudyAbroadGPT project to achieve
enhanced, contextualized responses. Technical innovations entailed
memory-efficient quantization, parameter-efficient adaptation, and continuous
training analytics via Weights & Biases. After training, this study
demonstrated a reduction in training loss by 52.7%, 92% accuracy in
domain-specific recommendations, achieved 95% markdown-based formatting
support, and a median run-rate of 100 samples per second on off-the-shelf GPU
equipment. These findings support the effective application of
instruction-tuned LLMs within educational advisers, especially in low-resource
institutional scenarios. Limitations included decreased generalizability and
the application of a synthetically generated dataset, but this framework is
scalable for adding new multilingual-augmented and real-time academic advising
processes. Future directions may include plans for the integration of
retrieval-augmented generation, applying dynamic quantization routines, and
connecting to real-time academic databases to increase adaptability and
accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [190] [TinyML for Speech Recognition](https://arxiv.org/abs/2504.16213)
*Andrew Barovic, Armin Moin*

Main category: cs.SD

TL;DR: A quantized 1D CNN model is trained for speech recognition on resource-constrained IoT edge devices, achieving 97% accuracy on a new dataset with 23 keywords.


<details>
  <summary>Details</summary>
Motivation: To enable speech recognition in IoT applications like smart homes and assisted living, addressing limitations of existing keyword-focused models.

Method: Created a new dataset, used Edge Impulse for model enhancement, and deployed on Arduino Nano 33 BLE Sense.

Result: Achieved 97% accuracy, processing 23 keywords, outperforming limited-keyword models.

Conclusion: The approach is effective for complex speech recognition on IoT devices, with potential for broader applications.

Abstract: We train and deploy a quantized 1D convolutional neural network model to
conduct speech recognition on a highly resource-constrained IoT edge device.
This can be useful in various Internet of Things (IoT) applications, such as
smart homes and ambient assisted living for the elderly and people with
disabilities, just to name a few examples. In this paper, we first create a new
dataset with over one hour of audio data that enables our research and will be
useful to future studies in this field. Second, we utilize the technologies
provided by Edge Impulse to enhance our model's performance and achieve a high
Accuracy of up to 97% on our dataset. For the validation, we implement our
prototype using the Arduino Nano 33 BLE Sense microcontroller board. This
microcontroller board is specifically designed for IoT and AI applications,
making it an ideal choice for our target use case scenarios. While most
existing research focuses on a limited set of keywords, our model can process
23 different keywords, enabling complex commands.

</details>


### [191] [SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward](https://arxiv.org/abs/2504.16839)
*Nicolas Jonason, Luca Casini, Bob L. T. Sturm*

Main category: cs.SD

TL;DR: Training ML models to predict music aesthetics and using them to finetune a symbolic music generation system with reinforcement learning, improving subjective ratings but reducing diversity.


<details>
  <summary>Details</summary>
Motivation: Explore if aesthetic prediction models can enhance symbolic music generation via reinforcement learning.

Method: Use group relative policy optimization to finetune a piano MIDI model with Meta Audiobox Aesthetics ratings as reward.

Result: Improves subjective ratings but reduces output diversity; affects low-level features.

Conclusion: Aesthetic finetuning enhances quality but risks over-optimization, reducing diversity.

Abstract: Recent work has proposed training machine learning models to predict
aesthetic ratings for music audio. Our work explores whether such models can be
used to finetune a symbolic music generation system with reinforcement
learning, and what effect this has on the system outputs. To test this, we use
group relative policy optimization to finetune a piano MIDI model with Meta
Audiobox Aesthetics ratings of audio-rendered outputs as the reward. We find
that this optimization has effects on multiple low-level features of the
generated outputs, and improves the average subjective ratings in a preliminary
listening study with $14$ participants. We also find that over-optimization
dramatically reduces diversity of model outputs.

</details>


### [192] [Adapting General Disentanglement-Based Speaker Anonymization for Enhanced Emotion Preservation](https://arxiv.org/abs/2408.05928)
*Xiaoxiao Miao, Yuxiang Zhang, Xin Wang, Natalia Tomashenko, Donny Cheng Lock Soh, Ian Mcloughlin*

Main category: cs.SD

TL;DR: The paper explores adapting speaker anonymization systems to preserve new attributes like emotion, proposing two strategies: integrating pre-trained emotion embeddings or using an emotion compensation post-processing step.


<details>
  <summary>Details</summary>
Motivation: Existing speaker anonymization systems lack the ability to preserve emotional cues, which is crucial for certain applications.

Method: Two strategies are examined: 1) integrating emotion embeddings from a pre-trained encoder, and 2) an emotion compensation post-processing step using SVM to modify anonymized speaker embeddings.

Result: The proposed methods help preserve emotional traits while anonymizing speaker identity, though the first approach slightly reduces privacy protection.

Conclusion: The strategies can adapt general disentanglement-based systems to preserve other paralinguistic attributes, benefiting downstream tasks.

Abstract: A general disentanglement-based speaker anonymization system typically
separates speech into content, speaker, and prosody features using individual
encoders. This paper explores how to adapt such a system when a new speech
attribute, for example, emotion, needs to be preserved to a greater extent.
While existing systems are good at anonymizing speaker embeddings, they are not
designed to preserve emotion. Two strategies for this are examined. First, we
show that integrating emotion embeddings from a pre-trained emotion encoder can
help preserve emotional cues, even though this approach slightly compromises
privacy protection. Alternatively, we propose an emotion compensation strategy
as a post-processing step applied to anonymized speaker embeddings. This
conceals the original speaker's identity and reintroduces the emotional traits
lost during speaker embedding anonymization. Specifically, we model the emotion
attribute using support vector machines to learn separate boundaries for each
emotion. During inference, the original speaker embedding is processed in two
ways: one, by an emotion indicator to predict emotion and select the
emotion-matched SVM accurately; and two, by a speaker anonymizer to conceal
speaker characteristics. The anonymized speaker embedding is then modified
along the corresponding SVM boundary towards an enhanced emotional direction to
save the emotional cues. The proposed strategies are also expected to be useful
for adapting a general disentanglement-based speaker anonymization system to
preserve other target paralinguistic attributes, with potential for a range of
downstream tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [193] [Representation Learning for Tabular Data: A Comprehensive Survey](https://arxiv.org/abs/2504.16109)
*Jun-Peng Jiang, Si-Yang Liu, Hao-Run Cai, Qile Zhou, Han-Jia Ye*

Main category: cs.LG

TL;DR: A survey on tabular representation learning, covering DNNs' role, challenges, and benchmarks, with methods categorized into specialized, transferable, and general models, plus ensemble techniques and extensions.


<details>
  <summary>Details</summary>
Motivation: To systematically organize and introduce the field of tabular representation learning, addressing the evolution of models, particularly DNNs, and their applications in classification and regression.

Method: Categorizes existing methods into specialized, transferable, and general models, with a hierarchical taxonomy for specialized models based on features, samples, and objectives. Also covers ensemble methods and extensions like multimodal learning.

Result: Provides a comprehensive overview of tabular learning, highlighting the pros and cons of DNNs, and introduces a framework for understanding and applying different model types.

Conclusion: The survey consolidates knowledge on tabular representation learning, offering insights into current methods, challenges, and future directions, such as open-environment and multimodal learning.

Abstract: Tabular data, structured as rows and columns, is among the most prevalent
data types in machine learning classification and regression applications.
Models for learning from tabular data have continuously evolved, with Deep
Neural Networks (DNNs) recently demonstrating promising results through their
capability of representation learning. In this survey, we systematically
introduce the field of tabular representation learning, covering the
background, challenges, and benchmarks, along with the pros and cons of using
DNNs. We organize existing methods into three main categories according to
their generalization capabilities: specialized, transferable, and general
models. Specialized models focus on tasks where training and evaluation occur
within the same data distribution. We introduce a hierarchical taxonomy for
specialized models based on the key aspects of tabular data -- features,
samples, and objectives -- and delve into detailed strategies for obtaining
high-quality feature- and sample-level representations. Transferable models are
pre-trained on one or more datasets and subsequently fine-tuned on downstream
tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources,
or even cross-modalities such as vision and language. General models, also
known as tabular foundation models, extend this concept further, allowing
direct application to downstream tasks without fine-tuning. We group these
general models based on the strategies used to adapt across heterogeneous
datasets. Additionally, we explore ensemble methods, which integrate the
strengths of multiple tabular models. Finally, we discuss representative
extensions of tabular learning, including open-environment tabular machine
learning, multimodal learning with tabular data, and tabular understanding.
More information can be found in the following repository:
https://github.com/LAMDA-Tabular/Tabular-Survey.

</details>


### [194] [Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement](https://arxiv.org/abs/2504.16136)
*Chiung-Yi Tseng, Junhao Song, Ziqian Bi, Tianyang Wang, Chia Xin Liang, Ming Liu*

Main category: cs.LG

TL;DR: The paper reviews Active Learning (AL) in machine learning, highlighting its benefits in improving model performance with fewer labeled examples, its applications across fields, and key research topics like uncertainty estimation and fairness. It also addresses current challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck of data abundance but annotation scarcity in machine learning by leveraging AL to enhance data efficiency and model performance.

Method: The paper provides an overview of AL, discussing its basic concepts, applications in fields like computer vision and NLP, and key research areas such as uncertainty estimation and domain adaptation.

Result: AL often outperforms passive learning, especially with robust evaluation metrics, and human-inspired methods can further improve data efficiency.

Conclusion: The paper serves as a resource for researchers and practitioners, offering insights into AL's potential and outlining challenges like reproducibility and trust, while suggesting future directions for advancement.

Abstract: In the era of data-driven intelligence, the paradox of data abundance and
annotation scarcity has emerged as a critical bottleneck in the advancement of
machine learning. This paper gives a detailed overview of Active Learning (AL),
which is a strategy in machine learning that helps models achieve better
performance using fewer labeled examples. It introduces the basic concepts of
AL and discusses how it is used in various fields such as computer vision,
natural language processing, transfer learning, and real-world applications.
The paper focuses on important research topics such as uncertainty estimation,
handling of class imbalance, domain adaptation, fairness, and the creation of
strong evaluation metrics and benchmarks. It also shows that learning methods
inspired by humans and guided by questions can improve data efficiency and help
models learn more effectively. In addition, this paper talks about current
challenges in the field, including the need to rebuild trust, ensure
reproducibility, and deal with inconsistent methodologies. It points out that
AL often gives better results than passive learning, especially when good
evaluation measures are used. This work aims to be useful for both researchers
and practitioners by providing key insights and proposing directions for future
progress in active learning.

</details>


### [195] [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
*Rene Pilz, Johannes Schneider*

Main category: cs.LG

TL;DR: The paper compares phonemic and textual representations in multilingual speech-to-speech translation, finding phonemic representation comparable in quality with added advantages like lower resource needs.


<details>
  <summary>Details</summary>
Motivation: To explore if phonemic representation can replace traditional text-based methods in speech-to-speech translation, especially for low-resource languages.

Method: Trained a sequence-to-sequence model on WMT17 dataset using both textual and phonemic representations, evaluated using BLEU metric.

Result: Phonemic representation performed comparably to textual, with benefits like reduced resource requirements.

Conclusion: Phonemic representation is a viable alternative for multilingual speech-to-speech translation, particularly beneficial for low-resource scenarios.

Abstract: This paper explores the idea of using phonemes as a textual representation
within a conventional multilingual simultaneous speech-to-speech translation
pipeline, as opposed to the traditional reliance on text-based language
representations. To investigate this, we trained an open-source
sequence-to-sequence model on the WMT17 dataset in two formats: one using
standard textual representation and the other employing phonemic
representation. The performance of both approaches was assessed using the BLEU
metric. Our findings shows that the phonemic approach provides comparable
quality but offers several advantages, including lower resource requirements or
better suitability for low-resource languages.

</details>


### [196] [SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures](https://arxiv.org/abs/2504.16140)
*Max Hartman, Lav Varshney*

Main category: cs.LG

TL;DR: SparseJEPA enhances JEPA by integrating sparse representation learning, improving interpretability and efficiency while maintaining predictive performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of interpretability and inefficiencies in dense embedding representations of JEPA models.

Method: Introduces SparseJEPA, using a penalty method to encourage shared latent space variables among semantically related features. Evaluated on CIFAR-100 and a lightweight Vision Transformer.

Result: Improved embeddings for transfer tasks, theoretical proof of enhanced representation quality via grouping mechanism, and reduced Multiinformation.

Conclusion: Sparsity refines latent space, aids interpretability, and suggests future extensions like object-centric learning.

Abstract: Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful
framework for learning general-purpose representations. However, these models
often lack interpretability and suffer from inefficiencies due to dense
embedding representations. We propose SparseJEPA, an extension that integrates
sparse representation learning into the JEPA framework to enhance the quality
of learned representations. SparseJEPA employs a penalty method that encourages
latent space variables to be shared among data features with strong semantic
relationships, while maintaining predictive performance. We demonstrate the
effectiveness of SparseJEPA by training on the CIFAR-100 dataset and
pre-training a lightweight Vision Transformer. The improved embeddings are
utilized in linear-probe transfer learning for both image classification and
low-level tasks, showcasing the architecture's versatility across different
transfer tasks. Furthermore, we provide a theoretical proof that demonstrates
that the grouping mechanism enhances representation quality. This was done by
displaying that grouping reduces Multiinformation among latent-variables,
including proofing the Data Processing Inequality for Multiinformation. Our
results indicate that incorporating sparsity not only refines the latent space
but also facilitates the learning of more meaningful and interpretable
representations. In further work, hope to further extend this method by finding
new ways to leverage the grouping mechanism through object-centric
representation learning.

</details>


### [197] [An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon](https://arxiv.org/abs/2504.16276)
*Abhishek Jana, Moeumu Uili, James Atherton, Mark O'Brien, Joe Wood, Leandra Brickson*

Main category: cs.LG

TL;DR: An automated one-shot bird call classification pipeline for rare species, achieving high accuracy with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Existing classifiers like BirdNET and Perch lack support for rare species with few recordings, hindering conservation efforts.

Method: Leverages embedding spaces of large bird classifiers, uses cosine similarity, and applies preprocessing for optimized detection with minimal data.

Result: Achieved 1.0 recall and 0.95 accuracy in detecting the critically endangered tooth-billed pigeon.

Conclusion: The system is a practical, open-source tool for conservationists monitoring rare species.

Abstract: This paper presents an automated one-shot bird call classification pipeline
designed for rare species absent from large publicly available classifiers like
BirdNET and Perch. While these models excel at detecting common birds with
abundant training data, they lack options for species with only 1-3 known
recordings-a critical limitation for conservationists monitoring the last
remaining individuals of endangered birds. To address this, we leverage the
embedding space of large bird classification networks and develop a classifier
using cosine similarity, combined with filtering and denoising preprocessing
techniques, to optimize detection with minimal training data. We evaluate
various embedding spaces using clustering metrics and validate our approach in
both a simulated scenario with Xeno-Canto recordings and a real-world test on
the critically endangered tooth-billed pigeon (Didunculus strigirostris), which
has no existing classifiers and only three confirmed recordings. The final
model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon
calls, making it practical for use in the field. This open-source system
provides a practical tool for conservationists seeking to detect and monitor
rare species on the brink of extinction.

</details>


### [198] [Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges](https://arxiv.org/abs/2504.16141)
*Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser, Nguyen Huu Thuy, Dominik Behrend, Amit Kumar Srivastava, Krishnagopal Halder, Frank Ewert*

Main category: cs.LG

TL;DR: A review of process-based models (PBMs) and deep learning (DL) in agriculture, highlighting hybrid PBM-DL frameworks' superiority in performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standalone PBMs (scalability, parameterisation) and DL (interpretability, overfitting) by exploring hybrid approaches.

Method: Systematic review of PBMs, DL, and hybrid frameworks, with a case study on crop dry biomass prediction comparing hybrid models to standalone methods.

Result: Hybrid models outperform PBMs and DL, showing robustness to noisy data and better generalization.

Conclusion: Hybrid PBM-DL frameworks offer scalable, interpretable solutions for sustainable agriculture, integrating domain knowledge with AI.

Abstract: Process-based models (PBMs) and deep learning (DL) are two key approaches in
agricultural modelling, each offering distinct advantages and limitations. PBMs
provide mechanistic insights based on physical and biological principles,
ensuring interpretability and scientific rigour. However, they often struggle
with scalability, parameterisation, and adaptation to heterogeneous
environments. In contrast, DL models excel at capturing complex, nonlinear
patterns from large datasets but may suffer from limited interpretability, high
computational demands, and overfitting in data-scarce scenarios.
  This study presents a systematic review of PBMs, DL models, and hybrid PBM-DL
frameworks, highlighting their applications in agricultural and environmental
modelling. We classify hybrid PBM-DL approaches into DL-informed PBMs, where
neural networks refine process-based models, and PBM-informed DL, where
physical constraints guide deep learning predictions. Additionally, we conduct
a case study on crop dry biomass prediction, comparing hybrid models against
standalone PBMs and DL models under varying data quality, sample sizes, and
spatial conditions. The results demonstrate that hybrid models consistently
outperform traditional PBMs and DL models, offering greater robustness to noisy
data and improved generalisation across unseen locations.
  Finally, we discuss key challenges, including model interpretability,
scalability, and data requirements, alongside actionable recommendations for
advancing hybrid modelling in agriculture. By integrating domain knowledge with
AI-driven approaches, this study contributes to the development of scalable,
interpretable, and reproducible agricultural models that support data-driven
decision-making for sustainable agriculture.

</details>


### [199] [Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis](https://arxiv.org/abs/2504.16214)
*Xiao Zhang, Yaoyao Ding, Yang Hu, Gennady Pekhimenko*

Main category: cs.LG

TL;DR: Hexcute is a tile-based programming language for optimizing mixed-type DL operators on GPUs, balancing expressiveness and effort, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: DL quantization requires mixed-type matrix multiplication, complicating GPU optimization; existing compilers lack expressiveness or demand high effort.

Method: Hexcute exposes shared memory and register abstractions, uses task mapping, and automates layout and mapping synthesis via type inference.

Result: Hexcute achieves 1.7-11.28Ã— speedup over existing compilers and up to 2.91Ã— in end-to-end evaluation.

Conclusion: Hexcute effectively optimizes mixed-type DL operators, offering a practical solution for GPU acceleration.

Abstract: Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL
quantization techniques demand a new matrix multiplication operator with mixed
input data types, further complicating GPU optimization. Prior high-level
compilers like Triton lack the expressiveness to implement key optimizations
like fine-grained data pipelines and hardware-friendly memory layouts for these
operators, while low-level programming models, such as Hidet, Graphene, and
CUTLASS, require significant programming efforts. To balance expressiveness
with engineering effort, we propose Hexcute, a tile-based programming language
that exposes shared memory and register abstractions to enable fine-grained
optimization for these operators. Additionally, Hexcute leverages task mapping
to schedule the GPU program, and to reduce programming efforts, it automates
layout and task mapping synthesis with a novel type-inference-based algorithm.
Our evaluation shows that Hexcute generalizes to a wide range of DL operators,
achieves 1.7-11.28$\times$ speedup over existing DL compilers for mixed-type
operators, and brings up to 2.91$\times$ speedup in the end-to-end evaluation.

</details>


### [200] [General Post-Processing Framework for Fairness Adjustment of Machine Learning Models](https://arxiv.org/abs/2504.16238)
*LÃ©andre Eberhard, Nirek Sharma, Filipp Shelobolin, Aalok Ganesh Shanbhag*

Main category: cs.LG

TL;DR: A novel framework for fairness adjustments in machine learning, adaptable to diverse tasks and fairness metrics, decouples fairness from training, preserving performance and flexibility.


<details>
  <summary>Details</summary>
Motivation: Ensuring fairness in machine learning is critical for legal and ethical compliance in domains like credit underwriting and public policy.

Method: Adapts in-processing techniques as a post-processing step, decoupling fairness adjustments from model training.

Result: Achieves comparable fairness/accuracy tradeoff to Adversarial Debiasing on real-world datasets.

Conclusion: The framework offers flexibility, eliminates custom loss functions, and provides interpretable fairness insights.

Abstract: As machine learning increasingly influences critical domains such as credit
underwriting, public policy, and talent acquisition, ensuring compliance with
fairness constraints is both a legal and ethical imperative. This paper
introduces a novel framework for fairness adjustments that applies to diverse
machine learning tasks, including regression and classification, and
accommodates a wide range of fairness metrics. Unlike traditional approaches
categorized as pre-processing, in-processing, or post-processing, our method
adapts in-processing techniques for use as a post-processing step. By
decoupling fairness adjustments from the model training process, our framework
preserves model performance on average while enabling greater flexibility in
model development. Key advantages include eliminating the need for custom loss
functions, enabling fairness tuning using different datasets, accommodating
proprietary models as black-box systems, and providing interpretable insights
into the fairness adjustments. We demonstrate the effectiveness of this
approach by comparing it to Adversarial Debiasing, showing that our framework
achieves a comparable fairness/accuracy tradeoff on real-world datasets.

</details>


### [201] [FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness](https://arxiv.org/abs/2504.16255)
*Tina Behzad, Mithilesh Kumar Singh, Anthony J. Ripa, Klaus Mueller*

Main category: cs.LG

TL;DR: FairPlay is a web-based tool enabling stakeholders to collaboratively debias datasets and negotiate fairness outcomes without needing a universal fairness standard.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of achieving fairness in decision-making due to conflicting stakeholder demands and lack of a universal fairness theory.

Method: Developed FairPlay, a web-based application for collaborative debiasing and negotiation among stakeholders.

Result: User studies showed consensus reached in about five rounds, proving FairPlay's effectiveness.

Conclusion: FairPlay offers a practical solution for enhancing fairness in AI systems by facilitating stakeholder negotiation.

Abstract: The issue of fairness in decision-making is a critical one, especially given
the variety of stakeholder demands for differing and mutually incompatible
versions of fairness. Adopting a strategic interaction of perspectives provides
an alternative to enforcing a singular standard of fairness. We present a
web-based software application, FairPlay, that enables multiple stakeholders to
debias datasets collaboratively. With FairPlay, users can negotiate and arrive
at a mutually acceptable outcome without a universally agreed-upon theory of
fairness. In the absence of such a tool, reaching a consensus would be highly
challenging due to the lack of a systematic negotiation process and the
inability to modify and observe changes. We have conducted user studies that
demonstrate the success of FairPlay, as users could reach a consensus within
about five rounds of gameplay, illustrating the application's potential for
enhancing fairness in AI systems.

</details>


### [202] [Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching](https://arxiv.org/abs/2504.16262)
*Junn Yong Loo, Michelle Adeline, Julia Kaiwen Lau, Fang Yu Leong, Hwa Hui Tew, Arghya Pal, Vishnu Monn Baskaran, Chee-Ming Ting, RaphaÃ«l C. -W. Phan*

Main category: cs.LG

TL;DR: VPFB is a new energy-based generative framework that avoids unstable MCMC sampling, using variational loss to match flow-driven density to data distribution, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address the instability and cost of MCMC sampling in EBMs and explore the underexplored relationship between potential flows and EBMs.

Method: VPFB learns an energy-parameterized potential flow via a variational loss minimizing KL divergence between flow-driven and marginal homotopies.

Result: VPFB performs competitively in image generation, interpolation, OOD detection, and compositional generation.

Conclusion: VPFB offers a robust, efficient, and interpretable alternative to traditional EBMs, validated by diverse generative tasks.

Abstract: Energy-based models (EBMs) are a powerful class of probabilistic generative
models due to their flexibility and interpretability. However, relationships
between potential flows and explicit EBMs remain underexplored, while
contrastive divergence training via implicit Markov chain Monte Carlo (MCMC)
sampling is often unstable and expensive in high-dimensional settings. In this
paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based
generative framework that eliminates the need for implicit MCMC sampling and
does not rely on auxiliary networks or cooperative training. VPFB learns an
energy-parameterized potential flow by constructing a flow-driven density
homotopy that is matched to the data distribution through a variational loss
minimizing the Kullback-Leibler divergence between the flow-driven and marginal
homotopies. This principled formulation enables robust and efficient generative
modeling while preserving the interpretability of EBMs. Experimental results on
image generation, interpolation, out-of-distribution detection, and
compositional generation confirm the effectiveness of VPFB, showing that our
method performs competitively with existing approaches in terms of sample
quality and versatility across diverse generative modeling tasks.

</details>


### [203] [Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models](https://arxiv.org/abs/2504.16263)
*Magnus Sieverding, Nathan Steffen, Kelly Cohen*

Main category: cs.LG

TL;DR: The paper benchmarks a Gradient-Optimized Fuzzy Inference System (GF) against state-of-the-art ML models, showing GF's competitive/superior accuracy, efficiency, and robustness.


<details>
  <summary>Details</summary>
Motivation: To evaluate GF's performance and efficiency compared to traditional ML models, highlighting its potential as an interpretable and adaptable alternative.

Method: Benchmarked GF against Random Forest, XGBoost, Logistic Regression, SVM, and Neural Networks on five diverse UCI datasets, using gradient descent for optimization.

Result: GF achieved competitive/superior accuracy, high precision, low training times, and robustness across noisy and variable datasets.

Conclusion: GF is a promising, interpretable, and efficient alternative to complex deep learning models in supervised tasks.

Abstract: This paper presents a performance benchmarking study of a Gradient-Optimized
Fuzzy Inference System (GF) classifier against several state-of-the-art machine
learning models, including Random Forest, XGBoost, Logistic Regression, Support
Vector Machines, and Neural Networks. The evaluation was conducted across five
datasets from the UCI Machine Learning Repository, each chosen for their
diversity in input types, class distributions, and classification complexity.
Unlike traditional Fuzzy Inference Systems that rely on derivative-free
optimization methods, the GF leverages gradient descent to significantly
improving training efficiency and predictive performance. Results demonstrate
that the GF model achieved competitive, and in several cases superior,
classification accuracy while maintaining high precision and exceptionally low
training times. In particular, the GF exhibited strong consistency across folds
and datasets, underscoring its robustness in handling noisy data and variable
feature sets. These findings support the potential of gradient optimized fuzzy
systems as interpretable, efficient, and adaptable alternatives to more complex
deep learning models in supervised learning tasks.

</details>


### [204] [Boosting Classifier Performance with Opposition-Based Data Transformation](https://arxiv.org/abs/2504.16268)
*Abdesslem Layeb*

Main category: cs.LG

TL;DR: A novel data transformation framework using Opposition-Based Learning (OBL) enhances traditional classifiers by generating synthetic opposite samples, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve classification performance by leveraging OBL, originally used for optimization, to refine decision boundaries and boost classifier accuracy.

Method: Three OBL variants (Global, Class-Wise, Localized Class-Wise) are integrated with KNN, SVM, LR, and DT. Experiments on 26 datasets validate the approach.

Result: OBL-enhanced classifiers outperform standard ones in accuracy and F1-score, often achieving near-perfect results, with notable efficiency gains in SVM and LR.

Conclusion: OBL is a lightweight, powerful strategy for enhancing classification, particularly in complex or sparse learning environments.

Abstract: In this paper, we introduce a novel data transformation framework based on
Opposition-Based Learning (OBL) to boost the performance of traditional
classification algorithms. Originally developed to accelerate convergence in
optimization tasks, OBL is leveraged here to generate synthetic opposite
samples that replace the acutely training data and improve decision boundary
formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and
Localized Class-Wise OBL; and integrate them with several widely used
classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines
(SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments
conducted on 26 heterogeneous and high-dimensional datasets demonstrate that
OBL-enhanced classifiers consistently outperform their standard counterparts in
terms of accuracy and F1-score, frequently achieving near-perfect or perfect
classification. Furthermore, OBL contributes to improved computational
efficiency, particularly in SVM and LR. These findings underscore the potential
of OBL as a lightweight yet powerful data transformation strategy for enhancing
classification performance, especially in complex or sparse learning
environments.

</details>


### [205] [Learning Explainable Dense Reward Shapes via Bayesian Optimization](https://arxiv.org/abs/2504.16272)
*Ryan Koo, Ian Yang, Vipul Raheja, Mingyi Hong, Kwang-Sung Jun, Dongyeop Kang*

Main category: cs.LG

TL;DR: The paper proposes a token-level reward shaping method for RLHF in LLM alignment, using explainability techniques like SHAP and LIME, and shows improved performance and faster training.


<details>
  <summary>Details</summary>
Motivation: Current RLHF pipelines use sparse scalar rewards for sequences, leading to suboptimal token-level credit assignment. The work aims to address this by focusing on token-level rewards.

Method: A reward-shaping function is introduced, leveraging SHAP and LIME for per-token rewards. Bilevel optimization (Bayesian Optimization + policy training) is used to learn shaping parameters.

Result: Experiments show better token-level reward attribution improves downstream task performance and speeds up policy training. Theoretical analysis confirms optimal policy preservation.

Conclusion: Token-level reward shaping with explainability methods enhances RLHF, improving efficiency and performance while maintaining policy optimality.

Abstract: Current reinforcement learning from human feedback (RLHF) pipelines for large
language model (LLM) alignment typically assign scalar rewards to sequences,
using the final token as a surrogate indicator for the quality of the entire
sequence. However, this leads to sparse feedback and suboptimal token-level
credit assignment. In this work, we frame reward shaping as an optimization
problem focused on token-level credit assignment. We propose a reward-shaping
function leveraging explainability methods such as SHAP and LIME to estimate
per-token rewards from the reward model. To learn parameters of this shaping
function, we employ a bilevel optimization framework that integrates Bayesian
Optimization and policy training to handle noise from the token reward
estimates. Our experiments show that achieving a better balance of token-level
reward attribution leads to performance improvements over baselines on
downstream tasks and finds an optimal policy faster during training.
Furthermore, we show theoretically that explainability methods that are feature
additive attribution functions maintain the optimal policy as the original
reward.

</details>


### [206] [Quantum Doubly Stochastic Transformers](https://arxiv.org/abs/2504.16275)
*Jannis Born, Filip Skogh, Kahn Rhrissorrakrai, Filippo Utro, Nico Wagner, Aleksandros Sobczyk*

Main category: cs.LG

TL;DR: The paper introduces a hybrid classical-quantum Transformer (QDSFormer) that replaces Softmax with a variational quantum circuit to generate doubly stochastic matrices, improving performance and training stability.


<details>
  <summary>Details</summary>
Motivation: To address the instability and inflexibility of classical doubly stochastic methods (e.g., Sinkhorn's algorithm) in Transformers by leveraging quantum circuits for more diverse and information-preserving matrices.

Method: The QDSFormer replaces the Softmax in self-attention with a parametric quantum circuit, studying its expressive power and comparing it to classical methods like Sinkformer and a quantum-inspired Transformer based on QR decomposition.

Result: QDSFormer outperforms standard Vision Transformers and other doubly stochastic variants in small-scale object recognition tasks, showing improved training stability and lower performance variation.

Conclusion: The QDSFormer demonstrates the feasibility and advantages of hybrid classical-quantum approaches for Transformers, offering a novel solution to training instability and performance limitations in small-scale data settings.

Abstract: At the core of the Transformer, the Softmax normalizes the attention matrix
to be right stochastic. Previous research has shown that this often
destabilizes training and that enforcing the attention matrix to be doubly
stochastic (through Sinkhorn's algorithm) consistently improves performance
across different tasks, domains and Transformer flavors. However, Sinkhorn's
algorithm is iterative, approximative, non-parametric and thus inflexible
w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been
proven that DSMs can be obtained with a parametric quantum circuit, yielding a
novel quantum inductive bias for DSMs with no known classical analogue.
Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum
doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the
self-attention layer with a variational quantum circuit. We study the
expressive power of the circuit and find that it yields more diverse DSMs that
better preserve information than classical operators. Across multiple
small-scale object recognition tasks, we find that our QDSFormer consistently
surpasses both a standard Vision Transformer and other doubly stochastic
Transformers. Beyond the established Sinkformer, this comparison includes a
novel quantum-inspired doubly stochastic Transformer (based on QR
decomposition) that can be of independent interest. The QDSFormer also shows
improved training stability and lower performance variation suggesting that it
may mitigate the notoriously unstable training of ViTs on small-scale data.

</details>


### [207] [DataS^3: Dataset Subset Selection for Specialization](https://arxiv.org/abs/2504.16277)
*Neha Hulkund, Alaa Maalouf, Levi Cai, Daniel Yang, Tsun-Hsuan Wang, Abigail O'Neil, Timm Haucke, Sandeep Mukherjee, Vikram Ramaswamy, Judy Hansen Shen, Gabriel Tseng, Mike Walmsley, Daniela Rus, Ken Goldberg, Hannah Kerner, Irene Chen, Yogesh Girdhar, Sara Beery*

Main category: cs.LG

TL;DR: The paper introduces DataS^3, a benchmark for dataset subset selection for specialization (DS3), showing that deployment-specific data curation outperforms general-distribution methods by up to 51.3% accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-world ML applications require models to perform well on specific deployments with unique data distributions, but general training data often leads to suboptimal performance.

Method: The authors formalize DS3 and introduce DataS^3, a benchmark with diverse domains and deployments. They evaluate coresets, data filtering, and curation methods.

Result: General-distribution methods fail on deployment-specific tasks, while expert-curated subsets outperform full training data by up to 51.3%.

Conclusion: Tailored dataset curation is critical for deployment-specific performance, a need that will grow as global datasets and real-world ML deployments expand.

Abstract: In many real-world machine learning (ML) applications (e.g. detecting broken
bones in x-ray images, detecting species in camera traps), in practice models
need to perform well on specific deployments (e.g. a specific hospital, a
specific national park) rather than the domain broadly. However, deployments
often have imbalanced, unique data distributions. Discrepancy between the
training distribution and the deployment distribution can lead to suboptimal
performance, highlighting the need to select deployment-specialized subsets
from the available training data. We formalize dataset subset selection for
specialization (DS3): given a training set drawn from a general distribution
and a (potentially unlabeled) query set drawn from the desired
deployment-specific distribution, the goal is to select a subset of the
training data that optimizes deployment performance.
  We introduce DataS^3; the first dataset and benchmark designed specifically
for the DS3 problem. DataS^3 encompasses diverse real-world application
domains, each with a set of distinct deployments to specialize in. We conduct a
comprehensive study evaluating algorithms from various families--including
coresets, data filtering, and data curation--on DataS^3, and find that
general-distribution methods consistently fail on deployment-specific tasks.
Additionally, we demonstrate the existence of manually curated
(deployment-specific) expert subsets that outperform training on all available
data with accuracy gains up to 51.3 percent. Our benchmark highlights the
critical role of tailored dataset curation in enhancing performance and
training efficiency on deployment-specific distributions, which we posit will
only become more important as global, public datasets become available across
domains and ML models are deployed in the real world.

</details>


### [208] [Affect Models Have Weak Generalizability to Atypical Speech](https://arxiv.org/abs/2504.16283)
*Jaya Narain, Amrit Romana, Vikramjit Mitra, Colin Lea, Shirley Ren*

Main category: cs.LG

TL;DR: The study evaluates how atypical speech affects paralinguistic models for emotion recognition, finding significant impacts and suggesting fine-tuning for robustness.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of atypical speech (intelligibility, monopitch, harshness) on emotion recognition models and explore ways to improve their robustness.

Method: Evaluated public models on atypical speech datasets, comparing results to typical speech. Analyzed distributional trends, comparisons, and text-speech correlations for valence and arousal. Tested fine-tuning with pseudo-labeled atypical speech.

Result: Affect models are significantly impacted by atypical speech, e.g., higher sad predictions. Fine-tuning improved robustness for atypical speech without harming typical speech performance.

Conclusion: Broader datasets and robust modeling approaches are needed for speech emotion recognition to accommodate voice and speech differences.

Abstract: Speech and voice conditions can alter the acoustic properties of speech,
which could impact the performance of paralinguistic models for affect for
people with atypical speech. We evaluate publicly available models for
recognizing categorical and dimensional affect from speech on a dataset of
atypical speech, comparing results to datasets of typical speech. We
investigate three dimensions of speech atypicality: intelligibility, which is
related to pronounciation; monopitch, which is related to prosody, and
harshness, which is related to voice quality. We look at (1) distributional
trends of categorical affect predictions within the dataset, (2) distributional
comparisons of categorical affect predictions to similar datasets of typical
speech, and (3) correlation strengths between text and speech predictions for
spontaneous speech for valence and arousal. We find that the output of affect
models is significantly impacted by the presence and degree of speech
atypicalities. For instance, the percentage of speech predicted as sad is
significantly higher for all types and grades of atypical speech when compared
to similar typical speech datasets. In a preliminary investigation on improving
robustness for atypical speech, we find that fine-tuning models on
pseudo-labeled atypical speech data improves performance on atypical speech
without impacting performance on typical speech. Our results emphasize the need
for broader training and evaluation datasets for speech emotion models, and for
modeling approaches that are robust to voice and speech differences.

</details>


### [209] [Semantics at an Angle: When Cosine Similarity Works Until It Doesn't](https://arxiv.org/abs/2504.16318)
*Kisung You*

Main category: cs.LG

TL;DR: The paper examines the evolution, strengths, and limitations of cosine similarity in comparing embeddings, highlighting its widespread use, breakdown scenarios, and emerging alternatives.


<details>
  <summary>Details</summary>
Motivation: To provide conceptual clarity and practical insights into cosine similarity, especially for those viewing embeddings as geometric and philosophical objects.

Method: A reflective and selective examination of cosine similarity's role in machine learning, its performance, and its limitations.

Result: Identifies scenarios where cosine similarity excels and where it fails, particularly when embedding norms carry semantic meaning.

Conclusion: Emerging alternatives are addressing cosine similarity's blind spots, offering new perspectives for embedding comparisons.

Abstract: Cosine similarity has become a standard metric for comparing embeddings in
modern machine learning. Its scale-invariance and alignment with model training
objectives have contributed to its widespread adoption. However, recent studies
have revealed important limitations, particularly when embedding norms carry
meaningful semantic information. This informal article offers a reflective and
selective examination of the evolution, strengths, and limitations of cosine
similarity. We highlight why it performs well in many settings, where it tends
to break down, and how emerging alternatives are beginning to address its blind
spots. We hope to offer a mix of conceptual clarity and practical perspective,
especially for quantitative scientists who think about embeddings not just as
vectors, but as geometric and philosophical objects.

</details>


### [210] [Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks](https://arxiv.org/abs/2504.16360)
*Mao Wang, Tao Wu, Xingping Xian, Shaojie Qiao, Weina Niu, Canyixing Cui*

Main category: cs.LG

TL;DR: The paper introduces GOMKCN, a novel method for disentangled graph representation learning, improving interpretability and accuracy by using node-centric subgraphs and a Graph Optimal Matching Kernel (GOMK).


<details>
  <summary>Details</summary>
Motivation: Existing graph representation methods often coarsely characterize graph structures, limiting interpretability and structural pattern analysis.

Method: GOMKCN treats graphs as node-centric subgraphs, uses GOMK as a convolutional operator to compute similarities between subgraphs and learnable filters, and projects subgraphs onto task-optimized filters for disentangled representations.

Result: GOMKCN achieves superior accuracy and interpretability in graph pattern mining and prediction, resolving trade-offs in graph kernels.

Conclusion: The framework advances the theoretical foundation for disentangled graph representation learning.

Abstract: Graphs effectively characterize relational data, driving graph representation
learning methods that uncover underlying predictive information. As
state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end
learning for diverse tasks. Recent disentangled graph representation learning
enhances interpretability by decoupling independent factors in graph data.
However, existing methods often implicitly and coarsely characterize graph
structures, limiting structural pattern analysis within the graph. This paper
proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to
address this limitation. We view graphs as node-centric subgraphs, where each
subgraph acts as a structural factor encoding position-specific information.
This transforms graph prediction into structural pattern recognition. Inspired
by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a
convolutional operator, computing similarities between subgraphs and learnable
graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert
space, representing graphs as point sets. Disentangled representations emerge
from projecting subgraphs onto task-optimized filters, which adaptively capture
relevant structural patterns via gradient descent. Crucially, GOMK incorporates
local correspondences in similarity measurement, resolving the trade-off
between differentiability and accuracy in graph kernels. Experiments validate
that GOMKCN achieves superior accuracy and interpretability in graph pattern
mining and prediction. The framework advances the theoretical foundation for
disentangled graph representation learning.

</details>


### [211] [Natural Policy Gradient for Average Reward Non-Stationary RL](https://arxiv.org/abs/2504.16415)
*Neharika Jali, Eshika Pathak, Pranay Sharma, Guannan Qu, Gauri Joshi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of non-stationary reinforcement learning (RL) in the
infinite-horizon average-reward setting. We model it by a Markov Decision
Process with time-varying rewards and transition probabilities, with a
variation budget of $\Delta_T$. Existing non-stationary RL algorithms focus on
model-based and model-free value-based methods. Policy-based methods despite
their flexibility in practice are not theoretically well understood in
non-stationary RL. We propose and analyze the first model-free policy-based
algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient
method with a restart based exploration for change and a novel interpretation
of learning rates as adapting factors. Further, we present a bandit-over-RL
based parameter-free algorithm BORL-NS-NAC that does not require prior
knowledge of the variation budget $\Delta_T$. We present a dynamic regret of
$\tilde{\mathscr O}(|S|^{1/2}|A|^{1/2}\Delta_T^{1/6}T^{5/6})$ for both
algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of
the state and action spaces. The regret analysis leverages a novel adaptation
of the Lyapunov function analysis of NAC to dynamic environments and
characterizes the effects of simultaneous updates in policy, value function
estimate and changes in the environment.

</details>


### [212] [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
*Andrew Ilyas, Logan Engstrom*

Main category: cs.LG

TL;DR: MAGIC is a new data attribution method combining classical techniques and metadifferentiation to estimate the impact of training data changes on model predictions, outperforming existing methods in non-convex settings.


<details>
  <summary>Details</summary>
Motivation: Existing methods for data attribution in non-convex settings perform poorly, with weak correlation to ground truth, prompting the need for a better solution.

Method: MAGIC integrates classical approaches (e.g., infinitesimal jackknife) with metadifferentiation to optimize estimation of training data effects.

Result: MAGIC provides nearly optimal estimates of how adding or removing training data affects model predictions, surpassing current methods.

Conclusion: MAGIC offers a significant improvement in data attribution for non-convex models, addressing limitations of existing techniques.

Abstract: The goal of predictive data attribution is to estimate how adding or removing
a given set of training datapoints will affect model predictions. In convex
settings, this goal is straightforward (i.e., via the infinitesimal jackknife).
In large-scale (non-convex) settings, however, existing methods are far less
successful -- current methods' estimates often only weakly correlate with
ground truth. In this work, we present a new data attribution method (MAGIC)
that combines classical methods and recent advances in metadifferentiation to
(nearly) optimally estimate the effect of adding or removing training data on
model predictions.

</details>


### [213] [Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion](https://arxiv.org/abs/2504.16431)
*Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua Susskind, Navdeep Jaitly*

Main category: cs.LG

TL;DR: Target Concrete Score Matching (TCSM) is a new objective for training discrete diffusion models, offering versatility for pre-training, fine-tuning, and distillation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To provide a general and flexible framework for training and fine-tuning discrete diffusion models, enabling seamless integration with reward functions and pre-trained models.

Method: TCSM estimates the concrete score of the target distribution in the clean data space, allowing pre-training and post-training (fine-tuning, reward-based tuning, and distillation).

Result: TCSM matches or surpasses current methods in language modeling tasks, demonstrating versatility and sample efficiency.

Conclusion: TCSM is a powerful and adaptable framework for discrete diffusion models, enhancing their applicability in both pre-training and post-training scenarios.

Abstract: Discrete diffusion is a promising framework for modeling and generating
discrete data. In this work, we present Target Concrete Score Matching (TCSM),
a novel and versatile objective for training and fine-tuning discrete diffusion
models. TCSM provides a general framework with broad applicability. It supports
pre-training discrete diffusion models directly from data samples, and many
existing discrete diffusion approaches naturally emerge as special cases of our
more general TCSM framework. Furthermore, the same TCSM objective extends to
post-training of discrete diffusion models, including fine-tuning using reward
functions or preference data, and distillation of knowledge from pre-trained
autoregressive models. These new capabilities stem from the core idea of TCSM,
estimating the concrete score of the target distribution, which resides in the
original (clean) data space. This allows seamless integration with reward
functions and pre-trained models, which inherently only operate in the clean
data space rather than the noisy intermediate spaces of diffusion processes.
Our experiments on language modeling tasks demonstrate that TCSM matches or
surpasses current methods. Additionally, TCSM is versatile, applicable to both
pre-training and post-training scenarios, offering greater flexibility and
sample efficiency.

</details>


### [214] [iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network](https://arxiv.org/abs/2504.16432)
*Ziran Liang, Rui An, Wenqi Fan, Yanghui Rao, Yuxuan Liang*

Main category: cs.LG

TL;DR: iTFKAN is an interpretable model for time series forecasting, combining prior knowledge injection and time-frequency synergy learning to improve performance and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: Current deep forecasting methods lack interpretability, limiting their use in safety-critical applications like auto-driving and healthcare.

Method: iTFKAN uses model symbolization for interpretability and employs prior knowledge injection and time-frequency synergy learning for effective forecasting.

Result: iTFKAN achieves strong forecasting performance while maintaining high interpretability.

Conclusion: iTFKAN provides a credible and interpretable solution for time series forecasting, suitable for safety-critical domains.

Abstract: As time evolves, data within specific domains exhibit predictability that
motivates time series forecasting to predict future trends from historical
data. However, current deep forecasting methods can achieve promising
performance but generally lack interpretability, hindering trustworthiness and
practical deployment in safety-critical applications such as auto-driving and
healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for
credible time series forecasting. iTFKAN enables further exploration of model
decision rationales and underlying data patterns due to its interpretability
achieved through model symbolization. Besides, iTFKAN develops two strategies,
prior knowledge injection, and time-frequency synergy learning, to effectively
guide model learning under complex intertwined time series data. Extensive
experimental results demonstrated that iTFKAN can achieve promising forecasting
performance while simultaneously possessing high interpretive capabilities.

</details>


### [215] [Private Federated Learning using Preference-Optimized Synthetic Data](https://arxiv.org/abs/2504.16438)
*Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti*

Main category: cs.LG

TL;DR: POPri improves DP synthetic data quality in federated learning by using preference optimization, outperforming prior methods and reducing the accuracy gap between private and non-private settings.


<details>
  <summary>Details</summary>
Motivation: Enhance differentially private federated learning (DP-FL) by improving DP synthetic data quality using client feedback as preference rankings.

Method: Uses preference optimization (e.g., DPO) to fine-tune LLMs for generating high-quality DP synthetic data, evaluated on LargeFedBench.

Result: POPri reduces the accuracy gap by up to 68%, outperforming prior synthetic data methods (52%) and DP-FL methods (10%).

Conclusion: POPri significantly advances DP synthetic data utility, bridging the gap between private and non-private settings in federated learning.

Abstract: In practical settings, differentially private Federated learning (DP-FL) is
the dominant method for training models from private, on-device client data.
Recent work has suggested that DP-FL may be enhanced or outperformed by methods
that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary
algorithms for generating DP synthetic data for FL applications require careful
prompt engineering based on public information and/or iterative private client
feedback. Our key insight is that the private client feedback collected by
prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be
viewed as a preference ranking. Our algorithm, Preference Optimization for
Private Client Data (POPri) harnesses client feedback using preference
optimization algorithms such as Direct Preference Optimization (DPO) to
fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri,
we release LargeFedBench, a new federated text benchmark for uncontaminated LLM
evaluations on federated client data. POPri substantially improves the utility
of DP synthetic data relative to prior work on LargeFedBench datasets and an
existing benchmark from Xie et al. (2024). POPri closes the gap between
next-token prediction accuracy in the fully-private and non-private settings by
up to 68%, compared to 52% for prior synthetic data methods, and 10% for
state-of-the-art DP federated learning methods. The code and data are available
at https://github.com/meiyuw/POPri.

</details>


### [216] [Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module](https://arxiv.org/abs/2504.16447)
*Jeesuk Shin, Cheolwoong Kim, Sunwoong Yang, Minseo Lee, Sung Joong Kim, Joongoo Jeon*

Main category: cs.LG

TL;DR: The paper introduces a novel numerical method, NA-PINN, for thermal-hydraulic system codes in nuclear power plant accident analysis, addressing limitations of traditional codes like MELCOR and MAAP.


<details>
  <summary>Details</summary>
Motivation: Traditional thermal-hydraulic system codes have limitations due to inconsistent finite difference schemes and unidirectional coupling in multi-physics analyses.

Method: The study proposes a node-assigned physics-informed neural network (NA-PINN) for system codes, assigning individual networks to each nodalization to exclude spatial information and focus on temporal solutions.

Result: In a 6-water-tank simulation, NA-PINN achieved a maximum absolute error of 0.007, demonstrating acceptable accuracy, unlike standard PINN.

Conclusion: NA-PINN is the first successful implementation of a system code using PINN, with future work aimed at extending it to multi-physics solvers and surrogate modeling.

Abstract: Severe accidents (SAs) in nuclear power plants have been analyzed using
thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes
efficiently simulate the progression of SAs, while they still have inherent
limitations due to their inconsistent finite difference schemes. The use of
empirical schemes incorporating both implicit and explicit formulations
inherently induces unidirectional coupling in multi-physics analyses. The
objective of this study is to develop a novel numerical method for TH system
codes using physics-informed neural network (PINN). They have shown strength in
solving multi-physics due to the innate feature of neural networks-automatic
differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for
the control volume approach-based system codes. NA-PINN addresses the issue of
spatial governing equation variation by assigning an individual network to each
nodalization of the system code, such that spatial information is excluded from
both the input and output domains, and each subnetwork learns to approximate a
purely temporal solution. In this phase, we evaluated the accuracy of the PINN
methods for the hydrodynamic module. In the 6 water tank simulation, PINN and
NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It
should be noted that only NA-PINN demonstrated acceptable accuracy. To the best
of the authors' knowledge, this is the first study to successfully implement a
system code using PINN. Our future work involves extending NA-PINN to a
multi-physics solver and developing it in a surrogate manner.

</details>


### [217] [An Effective Gram Matrix Characterizes Generalization in Deep Networks](https://arxiv.org/abs/2504.16450)
*Rubing Yang, Pratik Chaudhari*

Main category: cs.LG

TL;DR: The paper derives a differential equation to model the generalization gap in deep networks trained by gradient descent, controlled by contraction and perturbation factors. It introduces an 'effective Gram matrix' to predict test loss and shows training is benign, with generalization influenced by data-architecture alignment.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the generalization gap in deep learning by modeling its evolution during training.

Method: Derives a differential equation for the generalization gap, analyzes it to compute an 'effective Gram matrix,' and evaluates empirically on image datasets.

Result: The analysis accurately predicts test loss, showing training is benign and generalization depends on data-architecture alignment.

Conclusion: The match/mismatch between data and architecture primarily determines generalization performance, with the training process being benign.

Abstract: We derive a differential equation that governs the evolution of the
generalization gap when a deep network is trained by gradient descent. This
differential equation is controlled by two quantities, a contraction factor
that brings together trajectories corresponding to slightly different datasets,
and a perturbation factor that accounts for them training on different
datasets. We analyze this differential equation to compute an ``effective Gram
matrix'' that characterizes the generalization gap after training in terms of
the alignment between this Gram matrix and a certain initial ``residual''.
Empirical evaluations on image classification datasets indicate that this
analysis can predict the test loss accurately. Further, at any point during
training, the residual predominantly lies in the subspace of the effective Gram
matrix with the smallest eigenvalues. This indicates that the training process
is benign, i.e., it does not lead to significant deterioration of the
generalization gap (which is zero at initialization). The alignment between the
effective Gram matrix and the residual is different for different datasets and
architectures. The match/mismatch of the data and the architecture is primarily
responsible for good/bad generalization.

</details>


### [218] [Dynamic Time-aware Continual User Representation Learning](https://arxiv.org/abs/2504.16501)
*Seungyoon Choi, Sein Kim, Hongseok Kang, Wonjoong Kim, Chanyoung Park*

Main category: cs.LG

TL;DR: The paper introduces DITTO, a dynamic time-aware continual learning framework for universal user representation, addressing limitations in traditional and existing CL-based methods by considering time passage and shifting item distributions.


<details>
  <summary>Details</summary>
Motivation: Traditional user modeling lacks generalization across tasks, and existing CL-based methods overlook time passage and item distribution shifts, leading to unrealistic evaluations.

Method: Proposes DITTO, a framework for continual user representation learning that adapts to shifting item distributions over time while mitigating catastrophic forgetting.

Result: DITTO outperforms state-of-the-art methods in practical evaluation scenarios, demonstrating adaptability and robustness.

Conclusion: DITTO provides a practical and effective solution for continual user representation learning, validated by extensive experiments.

Abstract: Traditional user modeling (UM) approaches have primarily focused on designing
models for a single specific task, but they face limitations in generalization
and adaptability across various tasks. Recognizing these challenges, recent
studies have shifted towards continual learning (CL)-based universal user
representation learning aiming to develop a single model capable of handling
multiple tasks. Despite advancements, existing methods are in fact evaluated
under an unrealistic scenario that does not consider the passage of time as
tasks progress, which overlooks newly emerged items that may change the item
distribution of previous tasks. In this paper, we introduce a practical
evaluation scenario on which CL-based universal user representation learning
approaches should be evaluated, which takes into account the passage of time as
tasks progress. Then, we propose a novel framework Dynamic Time-aware continual
user representation learner, named DITTO, designed to alleviate catastrophic
forgetting despite continuous shifts in item distribution, while also allowing
the knowledge acquired from previous tasks to adapt to the current shifted item
distribution. Through our extensive experiments, we demonstrate the superiority
of DITTO over state-of-the-art methods under a practical evaluation scenario.
Our source code is available at
https://github.com/seungyoon-Choi/DITTO_official.

</details>


### [219] [A Comprehensive Survey of Synthetic Tabular Data Generation](https://arxiv.org/abs/2504.16506)
*Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang*

Main category: cs.LG

TL;DR: A survey on synthetic tabular data generation, addressing challenges like data scarcity and privacy, and reviewing methods like EBMs, VAEs, GANs, LLMs, and diffusion models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap in fragmented insights by providing a unified review of synthetic tabular data generation, especially focusing on recent advances like LLMs and diffusion models.

Method: The paper proposes a taxonomy for existing methods (traditional, diffusion-based, LLM-based), details the synthetic data pipeline (synthesis, post-processing, evaluation), and compares approaches.

Result: The survey offers a comprehensive synthesis of the field, highlighting methodological interplay, challenges, and applications.

Conclusion: The paper concludes by identifying open research questions and future directions to guide advancements in synthetic tabular data generation.

Abstract: Tabular data remains one of the most prevalent and critical data formats
across diverse real-world applications. However, its effective use in machine
learning (ML) is often constrained by challenges such as data scarcity, privacy
concerns, and class imbalance. Synthetic data generation has emerged as a
promising solution, leveraging generative models to learn the distribution of
real datasets and produce high-fidelity, privacy-preserving samples. Various
generative paradigms have been explored, including energy-based models (EBMs),
variational autoencoders (VAEs), generative adversarial networks (GANs), large
language models (LLMs), and diffusion models. While several surveys have
investigated synthetic tabular data generation, most focus on narrow subdomains
or specific generative methods, such as GANs, diffusion models, or
privacy-preserving techniques. This limited scope often results in fragmented
insights, lacking a comprehensive synthesis that bridges diverse approaches. In
particular, recent advances driven by LLMs and diffusion-based models remain
underexplored. This gap hinders a holistic understanding of the field`s
evolution, methodological interplay, and open challenges. To address this, our
survey provides a unified and systematic review of synthetic tabular data
generation. Our contributions are threefold: (1) we propose a comprehensive
taxonomy that organizes existing methods into traditional approaches,
diffusion-based methods, and LLM-based models, and provide an in-depth
comparative analysis; (2) we detail the complete pipeline for synthetic tabular
data generation, including data synthesis, post-processing, and evaluation; (3)
we identify major challenges, explore real-world applications, and outline open
research questions and future directions to guide future work in this rapidly
evolving area.

</details>


### [220] [Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations](https://arxiv.org/abs/2504.16553)
*Mohammad Mahdi Abedi, David Pardo, Tariq Alkhalifah*

Main category: cs.LG

TL;DR: A hybrid optimization framework combining gradient descent (GD) and least-squares (LS) solvers improves training convergence and stability for Physics-Informed Neural Networks (PINNs) solving the Helmholtz equation.


<details>
  <summary>Details</summary>
Motivation: Standard GD training for PINNs is slow and unstable for high-frequency wavefields, prompting the need for a more efficient method.

Method: A hybrid framework embeds an LS solver into the GD loss function, optimizing updates for the linear output layer, with practical implementations for scenarios with or without perfectly matched layers (PML).

Result: The method achieves faster convergence, higher accuracy, and improved stability compared to conventional PINN training, even in cases where GD fails.

Conclusion: The LS-enhanced approach is scalable and effective for large-scale wavefield simulations, with minimal computational overhead.

Abstract: Physics-Informed Neural Networks (PINNs) have shown promise in solving
partial differential equations (PDEs), including the frequency-domain Helmholtz
equation. However, standard training of PINNs using gradient descent (GD)
suffers from slow convergence and instability, particularly for high-frequency
wavefields. For scattered acoustic wavefield simulation based on Helmholtz
equation, we derive a hybrid optimization framework that accelerates training
convergence by embedding a least-squares (LS) solver directly into the GD loss
function. This formulation enables optimal updates for the linear output layer.
Our method is applicable with or without perfectly matched layers (PML), and we
provide practical tensor-based implementations for both scenarios. Numerical
experiments on benchmark velocity models demonstrate that our approach achieves
faster convergence, higher accuracy, and improved stability compared to
conventional PINN training. In particular, our results show that the
LS-enhanced method converges rapidly even in cases where standard GD-based
training fails. The LS solver operates on a small normal matrix, ensuring
minimal computational overhead and making the method scalable for large-scale
wavefield simulations.

</details>


### [221] [Unified Molecule Generation and Property Prediction](https://arxiv.org/abs/2504.16559)
*Adam Izdebski, Jan Olszewski, Pankhil Gawade, Krzysztof Koras, Serra Korkmaz, Valentin Rauscher, Jakub M. Tomczak, Ewa Szczurek*

Main category: cs.LG

TL;DR: Hyformer is a transformer-based joint model blending generative and predictive functionalities, outperforming other models in molecule generation and property prediction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of training joint models for data generation and property prediction by proposing a unified approach.

Method: Hyformer uses an alternating attention mask and unified pre-training to combine generative and predictive tasks.

Result: Hyformer rivals state-of-the-art models in molecule generation and property prediction, and excels in downstream tasks like molecular representation learning and antimicrobial peptide design.

Conclusion: Joint modeling with Hyformer offers synergistic benefits, enhancing both generative and predictive capabilities in molecular tasks.

Abstract: Modeling the joint distribution of the data samples and their properties
allows to construct a single model for both data generation and property
prediction, with synergistic capabilities reaching beyond purely generative or
predictive models. However, training joint models presents daunting
architectural and optimization challenges. Here, we propose Hyformer, a
transformer-based joint model that successfully blends the generative and
predictive functionalities, using an alternating attention mask together with a
unified pre-training scheme. We show that Hyformer rivals other joint models,
as well as state-of-the-art molecule generation and property prediction models.
Additionally, we show the benefits of joint modeling in downstream tasks of
molecular representation learning, hit identification and antimicrobial peptide
design.

</details>


### [222] [Hyper-Transforming Latent Diffusion Models](https://arxiv.org/abs/2504.16580)
*Ignacio Peis, Batuhan Koyuncu, Isabel Valera, Jes Frellsen*

Main category: cs.LG

TL;DR: A novel generative framework combines INRs and Transformer-based hypernetworks in latent variable models, improving scalability and efficiency over MLP-based methods.


<details>
  <summary>Details</summary>
Motivation: Prior methods using MLP-based hypernetworks face scalability issues. This work aims to enhance representation capacity and computational efficiency.

Method: Uses a Transformer-based decoder to generate INR parameters from latent variables, integrating with latent diffusion models (LDMs). Training can be done from scratch or via hyper-transforming (fine-tuning the decoder while freezing the latent space).

Result: The framework efficiently adapts existing generative models to INR-based representations without full retraining.

Conclusion: The proposed method offers a scalable and efficient solution for generating functions using INRs and Transformer-based hypernetworks.

Abstract: We introduce a novel generative framework for functions by integrating
Implicit Neural Representations (INRs) and Transformer-based hypernetworks into
latent variable models. Unlike prior approaches that rely on MLP-based
hypernetworks with scalability limitations, our method employs a
Transformer-based decoder to generate INR parameters from latent variables,
addressing both representation capacity and computational efficiency. Our
framework extends latent diffusion models (LDMs) to INR generation by replacing
standard decoders with a Transformer-based hypernetwork, which can be trained
either from scratch or via hyper-transforming-a strategy that fine-tunes only
the decoder while freezing the pre-trained latent space. This enables efficient
adaptation of existing generative models to INR-based representations without
requiring full retraining.

</details>


### [223] [Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise](https://arxiv.org/abs/2504.16585)
*Xiaofei Wu, Rongmei Liang*

Main category: cs.LG

TL;DR: PLR with manual label noise improves variable selection and estimation accuracy. A partition-insensitive ADMM-based algorithm is proposed for distributed PLR, showing better performance than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address overfitting in PLR and leverage label noise for better variable selection, while enabling distributed computing for large-scale data.

Method: Proposes a partition-insensitive parallel ADMM algorithm for PLR with manual label noise, ensuring global convergence and sublinear convergence rate.

Result: PLR with manual label noise achieves higher estimation and classification accuracy than traditional methods on large-scale datasets.

Conclusion: Label noise from manual labeling benefits PLR variable selection, and the proposed distributed algorithm is effective and scalable.

Abstract: In large-scale supervised learning, penalized logistic regression (PLR)
effectively addresses the overfitting problem by introducing regularization
terms yet its performance still depends on efficient variable selection
strategies. This paper theoretically demonstrates that label noise stemming
from manual labeling, which is solely related to classification difficulty,
represents a type of beneficial noise for variable selection in PLR. This
benefit is reflected in a more accurate estimation of the selected non-zero
coefficients when compared with the case where only truth labels are used.
Under large-scale settings, the sample size for PLR can become very large,
making it infeasible to store on a single machine. In such cases, distributed
computing methods are required to handle PLR model with manual labeling. This
paper presents a partition-insensitive parallel algorithm founded on the ADMM
(alternating direction method of multipliers) algorithm to address PLR by
incorporating manual labeling. The partition insensitivity of the proposed
algorithm refers to the fact that the solutions obtained by the algorithm will
not change with the distributed storage of data. In addition, the algorithm has
global convergence and a sublinear convergence rate. Experimental results
indicate that, as compared with traditional variable selection classification
techniques, the PLR with manually-labeled noisy data achieves higher estimation
and classification accuracy across multiple large-scale datasets.

</details>


### [224] [Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement](https://arxiv.org/abs/2504.16624)
*Leo Henry, Thomas Neele, Mohammad Mousavi, Matteo Sammartino*

Main category: cs.LG

TL;DR: The paper introduces a general technique for compositional learning of synchronizing parallel systems with unknown decomposition, refining alphabets automatically and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: To advance beyond existing compositional learning approaches for concurrent systems by handling unknown decompositions and refining component alphabets dynamically.

Method: Develops a theoretical framework for alphabet distributions, characterizes counter-examples, and presents a compositional learning algorithm (CoalA) implemented with LearnLib.

Result: CoalA achieves up to five orders of magnitude improvement in membership queries and better scalability in equivalence queries for concurrent systems.

Conclusion: The proposed technique significantly enhances the efficiency and scalability of compositional automata learning for complex concurrent systems.

Abstract: Active automata learning infers automaton models of systems from behavioral
observations, a technique successfully applied to a wide range of domains.
Compositional approaches for concurrent systems have recently emerged. We take
a significant step beyond available results, including those by the authors,
and develop a general technique for compositional learning of a synchronizing
parallel system with an unknown decomposition. Our approach automatically
refines the global alphabet into component alphabets while learning the
component models. We develop a theoretical treatment of distributions of
alphabets, i.e., sets of possibly overlapping component alphabets. We
characterize counter-examples that reveal inconsistencies with global
observations, and show how to systematically update the distribution to restore
consistency. We present a compositional learning algorithm implementing these
ideas, where learning counterexamples precisely correspond to distribution
counterexamples under well-defined conditions. We provide an implementation,
called CoalA, using the state-of-the-art active learning library LearnLib. Our
experiments show that in more than 630 subject systems, CoalA delivers orders
of magnitude improvements (up to five orders) in membership queries and in
systems with significant concurrency, it also achieves better scalability in
the number of equivalence queries.

</details>


### [225] [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
*Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin*

Main category: cs.LG

TL;DR: ParetoHqD improves multiobjective alignment in large language models by using preference directions and high-quality Pareto front data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Aligning models with diverse human expectations and values is essential for serving varied user needs, but current methods face limitations in preference representation and imbalanced rewards.

Method: ParetoHqD represents preferences as directions in the objective space and uses high-quality Pareto front data. It employs a two-stage supervised fine-tuning process tailored to each preference.

Result: ParetoHqD outperforms five baselines on two multiobjective alignment tasks.

Conclusion: ParetoHqD effectively addresses limitations in preference representation and reward imbalance, enhancing alignment performance.

Abstract: Aligning large language models with multiple human expectations and values is
crucial for ensuring that they adequately serve a variety of user needs. To
this end, offline multiobjective alignment algorithms such as the
Rewards-in-Context algorithm have shown strong performance and efficiency.
However, inappropriate preference representations and training with imbalanced
reward scores limit the performance of such algorithms. In this work, we
introduce ParetoHqD that addresses the above issues by representing human
preferences as preference directions in the objective space and regarding data
near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD
follows a two-stage supervised fine-tuning process, where each stage uses an
individual Pareto high-quality training set that best matches its preference
direction. The experimental results have demonstrated the superiority of
ParetoHqD over five baselines on two multiobjective alignment tasks.

</details>


### [226] [DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization](https://arxiv.org/abs/2504.16639)
*Haoran Chen, Jiapeng Liu, Jiafan Wang, Wenjun Shi*

Main category: cs.LG

TL;DR: The paper introduces DAPLSR, a PLSR variant using SMOTE and VDM for data augmentation and manifold optimization to improve performance on imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional PLSR struggles with uneven data categories, prompting the need for a more robust method.

Method: DAPLSR combines SMOTE for sample generation, VDM for neighbor selection, and manifold optimization for accurate PLSR solutions.

Result: DAPLSR outperforms existing methods in classification and evaluation metrics across datasets.

Conclusion: DAPLSR effectively addresses PLSR's limitations in imbalanced data, offering superior performance.

Abstract: Traditional Partial Least Squares Regression (PLSR) models frequently
underperform when handling data characterized by uneven categories. To address
the issue, this paper proposes a Data Augmentation Partial Least Squares
Regression (DAPLSR) model via manifold optimization. The DAPLSR model
introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase
the number of samples and utilizes the Value Difference Metric (VDM) to select
the nearest neighbor samples that closely resemble the original samples for
generating synthetic samples. In solving the model, in order to obtain a more
accurate numerical solution for PLSR, this paper proposes a manifold
optimization method that uses the geometric properties of the constraint space
to improve model degradation and optimization. Comprehensive experiments show
that the proposed DAPLSR model achieves superior classification performance and
outstanding evaluation metrics on various datasets, significantly outperforming
existing methods.

</details>


### [227] [Representation Learning via Non-Contrastive Mutual Information](https://arxiv.org/abs/2504.16667)
*Zhaohan Daniel Guo, Bernardo Avila Pires, Khimya Khetarpal, Dale Schuurmans, Bo Dai*

Main category: cs.LG

TL;DR: The paper introduces a new self-supervised objective called MINC, combining strengths of contrastive and non-contrastive methods to improve representation learning from unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Labeling data is costly, and existing self-supervised methods (contrastive like SimCLR and non-contrastive like BYOL) have limitations in variance and collapse risks.

Method: Develop MINC by converting Spectral Contrastive Loss into a non-contrastive form, retaining mutual information benefits while reducing variance.

Result: MINC outperforms the Spectral Contrastive loss baseline in learning image representations on ImageNet.

Conclusion: MINC effectively combines the advantages of contrastive and non-contrastive methods, offering a robust solution for self-supervised learning.

Abstract: Labeling data is often very time consuming and expensive, leaving us with a
majority of unlabeled data. Self-supervised representation learning methods
such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very
successful at learning meaningful latent representations from unlabeled image
data, resulting in much more general and transferable representations for
downstream tasks. Broadly, self-supervised methods fall into two types: 1)
Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as
BYOL. Contrastive methods are generally trying to maximize mutual information
between related data points, so they need to compare every data point to every
other data point, resulting in high variance, and thus requiring large batch
sizes to work well. Non-contrastive methods like BYOL have much lower variance
as they do not need to make pairwise comparisons, but are much trickier to
implement as they have the possibility of collapsing to a constant vector. In
this paper, we aim to develop a self-supervised objective that combines the
strength of both types. We start with a particular contrastive method called
the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we
convert it into a more general non-contrastive form; this removes the pairwise
comparisons resulting in lower variance, but keeps the mutual information
formulation of the contrastive method preventing collapse. We call our new
objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by
learning image representations on ImageNet (similar to SimCLR and BYOL) and
show that it consistently improves upon the Spectral Contrastive loss baseline.

</details>


### [228] [Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach](https://arxiv.org/abs/2504.16668)
*Shuyue Wei, Yongxin Tong, Zimu Zhou, Tianran He, Yi Xu*

Main category: cs.LG

TL;DR: The paper addresses the computational inefficiency of Shapley value (SV) in federated learning (FL) by proposing a stratified-sampling framework and an approximation algorithm (IPSS) to reduce time cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Cross-silo data providers in FL hesitate to share datasets without fair data valuation, but SV's computational overhead is prohibitive. Existing solutions lack accuracy and efficiency.

Method: Proposes a unified stratified-sampling framework, analyzes schemes under FL linear regression, identifies key combinations, and introduces the IPSS algorithm to strategically evaluate high-impact combinations.

Result: IPSS outperforms baselines in efficiency and effectiveness on FL benchmark datasets, reducing time cost with minor approximation error.

Conclusion: The IPSS algorithm makes SV practical for FL by balancing accuracy and efficiency, enabling fair data valuation.

Abstract: Federated learning paradigm to utilize datasets across multiple data
providers. In FL, cross-silo data providers often hesitate to share their
high-quality dataset unless their data value can be fairly assessed. Shapley
value (SV) has been advocated as the standard metric for data valuation in FL
due to its desirable properties. However, the computational overhead of SV is
prohibitive in practice, as it inherently requires training and evaluating an
FL model across an exponential number of dataset combinations. Furthermore,
existing solutions fail to achieve high accuracy and efficiency, making
practical use of SV still out of reach, because they ignore choosing suitable
computation scheme for approximation framework and overlook the property of
utility function in FL. We first propose a unified stratified-sampling
framework for two widely-used schemes. Then, we analyze and choose the more
promising scheme under the FL linear regression assumption. After that, we
identify a phenomenon termed key combinations, where only limited dataset
combinations have a high-impact on final data value. Building on these
insights, we propose a practical approximation algorithm, IPSS, which
strategically selects high-impact dataset combinations rather than evaluating
all possible combinations, thus substantially reducing time cost with minor
approximation error. Furthermore, we conduct extensive evaluations on the FL
benchmark datasets to demonstrate that our proposed algorithm outperforms a
series of representative baselines in terms of efficiency and effectiveness.

</details>


### [229] [Provable wavelet-based neural approximation](https://arxiv.org/abs/2504.16682)
*Youngmi Hur, Hyojae Lim, Mikyoung Lim*

Main category: cs.LG

TL;DR: The paper presents a wavelet-based framework to analyze neural networks' universal approximation capabilities for various activation functions, including smooth and non-smooth ones, with error estimates.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of neural networks' approximation abilities across diverse activation functions, including oscillatory and non-smooth types.

Method: Uses wavelet frame theory on homogeneous-type spaces to derive sufficient conditions for approximation and error bounds.

Result: Provides conditions ensuring neural networks can approximate any function in the space, with explicit error control for non-smooth activations.

Conclusion: The framework enhances flexibility in designing neural network architectures by accommodating a broad range of activation functions.

Abstract: In this paper, we develop a wavelet-based theoretical framework for analyzing
the universal approximation capabilities of neural networks over a wide range
of activation functions. Leveraging wavelet frame theory on the spaces of
homogeneous type, we derive sufficient conditions on activation functions to
ensure that the associated neural network approximates any functions in the
given space, along with an error estimate. These sufficient conditions
accommodate a variety of smooth activation functions, including those that
exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance
between smooth and non-smooth activation functions, we establish a generalized
approximation result that is applicable to non-smooth activations, with the
error explicitly controlled by this distance. This provides increased
flexibility in the design of network architectures.

</details>


### [230] [MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks](https://arxiv.org/abs/2504.16683)
*Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas*

Main category: cs.LG

TL;DR: A Bayesian framework for estimating differential privacy using MCMC, incorporating multiple MIAs, without relying on unrealistic worst-case assumptions.


<details>
  <summary>Details</summary>
Motivation: To provide a more realistic and cautious privacy analysis by avoiding assumptions of worst-case scenarios in privacy auditing.

Method: Uses MCMC-DP-Est, a Markov chain Monte Carlo algorithm, to estimate the full posterior distribution of privacy parameters, incorporating evidence from multiple MIAs.

Result: The method yields joint estimates of MIA strengths and privacy parameters, demonstrated with artificial and real data examples.

Conclusion: The proposed framework offers a more practical and cautious approach to privacy estimation in differential privacy.

Abstract: We propose a new framework for Bayesian estimation of differential privacy,
incorporating evidence from multiple membership inference attacks (MIA).
Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC)
algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior
distribution of the privacy parameter (e.g., instead of just credible
intervals). Critically, the proposed method does not assume that privacy
auditing is performed with the most powerful attack on the worst-case (dataset,
challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est
jointly estimates the strengths of MIAs used and the privacy of the training
algorithm, yielding a more cautious privacy analysis. We also present an
economical way to generate measurements for the performance of an MIA that is
to be used by the MCMC method to estimate privacy. We present the use of the
methods with numerical examples with both artificial and real data.

</details>


### [231] [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
*Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang*

Main category: cs.LG

TL;DR: ThinkPRM is a generative, long chain-of-thought verifier that outperforms discriminative PRMs and LLM-as-a-Judge with minimal supervision, using only 1% of process labels.


<details>
  <summary>Details</summary>
Motivation: To address the high cost of training step-by-step verifiers (PRMs) by building data-efficient models that leverage verbalized step-wise rewards and verification chains-of-thought.

Method: Proposes ThinkPRM, a long CoT verifier fine-tuned on few process labels, capitalizing on the reasoning abilities of CoT models.

Result: Outperforms baselines on benchmarks like ProcessBench, MATH-500, and AIME '24, and surpasses discriminative verifiers by 8% and 4.5% on GPQA-Diamond and LiveCodeBench.

Conclusion: Generative, long CoT PRMs like ThinkPRM offer scalable verification with minimal supervision, highlighting their efficiency and effectiveness.

Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a
key ingredient for test-time scaling. PRMs require step-level supervision,
making them expensive to train. This work aims to build data-efficient PRMs as
verbalized step-wise reward models that verify every step in the solution by
generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long
CoT verifier fine-tuned on orders of magnitude fewer process labels than those
required by discriminative PRMs. Our approach capitalizes on the inherent
reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and
discriminative verifiers -- using only 1% of the process labels in PRM800K --
across several challenging benchmarks. Specifically, ThinkPRM beats the
baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and
reward-guided search. In an out-of-domain evaluation on a subset of
GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers
trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the
same token budget, ThinkPRM scales up verification compute more effectively
compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of
ProcessBench. Our work highlights the value of generative, long CoT PRMs that
can scale test-time compute for verification while requiring minimal
supervision for training. Our code, data, and models will be released at
https://github.com/mukhal/thinkprm.

</details>


### [232] [PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation](https://arxiv.org/abs/2504.16693)
*Wenxuan Li, Hang Zhao, Zhiyuan Yu, Yu Du, Qin Zou, Ruizhen Hu, Kai Xu*

Main category: cs.LG

TL;DR: PIN-WM, a Physics-INformed World Model, enables robust non-prehensile manipulation learning via differentiable physics simulation and Sim2Real transfer.


<details>
  <summary>Details</summary>
Motivation: Non-prehensile manipulation is challenging due to sensitivity to complex physical interactions like friction and restitution.

Method: PIN-WM learns a 3D rigid body dynamics world model from visual observations using differentiable physics simulation and Gaussian Splatting. It employs physics-aware randomizations (Digital Cousins) for Sim2Real transfer.

Result: PIN-WM outperforms state-of-the-art methods in simulation and real-world tests, enabling robust skill learning.

Conclusion: PIN-WM, enhanced with Digital Cousins, effectively bridges Sim2Real gaps and advances non-prehensile manipulation.

Abstract: While non-prehensile manipulation (e.g., controlled pushing/poking)
constitutes a foundational robotic skill, its learning remains challenging due
to the high sensitivity to complex physical interactions involving friction and
restitution. To achieve robust policy learning and generalization, we opt to
learn a world model of the 3D rigid body dynamics involved in non-prehensile
manipulations and use it for model-based reinforcement learning. We propose
PIN-WM, a Physics-INformed World Model that enables efficient end-to-end
identification of a 3D rigid body dynamical system from visual observations.
Adopting differentiable physics simulation, PIN-WM can be learned with only
few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM
is learned with observational loss induced by Gaussian Splatting without
needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM
into a group of Digital Cousins via physics-aware randomizations which perturb
physics and rendering parameters to generate diverse and meaningful variations
of the PIN-WM. Extensive evaluations on both simulation and real-world tests
demonstrate that PIN-WM, enhanced with physics-aware digital cousins,
facilitates learning robust non-prehensile manipulation skills with Sim2Real
transfer, surpassing the Real2Sim2Real state-of-the-arts.

</details>


### [233] [A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization](https://arxiv.org/abs/2504.16711)
*Shiyin Tan, Jaeeon Park, Dongyuan Li, Renhe Jiang, Manabu Okumura*

Main category: cs.LG

TL;DR: A novel retrieval-based framework for multi-document summarization (MDS) addresses input length limitations by integrating query selection and document ranking, using salient discourse units as latent queries to filter irrelevant content.


<details>
  <summary>Details</summary>
Motivation: Transformer-based MDS models face input length constraints and rely on impractical manual queries, often retrieving irrelevant content.

Method: Proposes a unified process using elementary discourse units (EDUs) as latent queries for ranking and shortening documents, filtering irrelevant EDUs to fit context length.

Result: Consistent improvements in ROUGE metrics, scalability, and flexibility across datasets, with dynamic query selection and accurate document ranking.

Conclusion: The framework effectively overcomes context-length constraints, offering a robust solution for MDS.

Abstract: In the field of multi-document summarization (MDS), transformer-based models
have demonstrated remarkable success, yet they suffer an input length
limitation. Current methods apply truncation after the retrieval process to fit
the context length; however, they heavily depend on manually well-crafted
queries, which are impractical to create for each document set for MDS.
Additionally, these methods retrieve information at a coarse granularity,
leading to the inclusion of irrelevant content. To address these issues, we
propose a novel retrieval-based framework that integrates query selection and
document ranking and shortening into a unified process. Our approach identifies
the most salient elementary discourse units (EDUs) from input documents and
utilizes them as latent queries. These queries guide the document ranking by
calculating relevance scores. Instead of traditional truncation, our approach
filters out irrelevant EDUs to fit the context length, ensuring that only
critical information is preserved for summarization. We evaluate our framework
on multiple MDS datasets, demonstrating consistent improvements in ROUGE
metrics while confirming its scalability and flexibility across diverse model
architectures. Additionally, we validate its effectiveness through an in-depth
analysis, emphasizing its ability to dynamically select appropriate queries and
accurately rank documents based on their relevance scores. These results
demonstrate that our framework effectively addresses context-length
constraints, establishing it as a robust and reliable solution for MDS.

</details>


### [234] [Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2504.16748)
*Yanan Zhao, Feng Ji, Kai Zhao, Xuhao Li, Qiyu Kang, Wenfei Liang, Yahya Alkhatib, Xingchao Jian, Wee Peng Tay*

Main category: cs.LG

TL;DR: A novel augmentation-free Graph Contrastive Learning (GCL) framework using graph neural diffusion models with Fractional Differential Equations (FDE) is introduced, eliminating the need for negative samples and performing well on diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GCL methods, which rely on complex data augmentations or negative samples, by proposing an augmentation-free approach.

Method: Utilizes learnable encoders governed by FDE, with varying order parameters to generate diverse views for contrastive learning without negative samples.

Result: Achieves state-of-the-art performance on various datasets, including homophilic and heterophilic ones.

Conclusion: The proposed framework offers an effective, augmentation-free GCL solution with broad applicability and superior performance.

Abstract: Graph Contrastive Learning (GCL) has recently made progress as an
unsupervised graph representation learning paradigm. GCL approaches can be
categorized into augmentation-based and augmentation-free methods. The former
relies on complex data augmentations, while the latter depends on encoders that
can generate distinct views of the same input. Both approaches may require
negative samples for training. In this paper, we introduce a novel
augmentation-free GCL framework based on graph neural diffusion models.
Specifically, we utilize learnable encoders governed by Fractional Differential
Equations (FDE). Each FDE is characterized by an order parameter of the
differential operator. We demonstrate that varying these parameters allows us
to produce learnable encoders that generate diverse views, capturing either
local or global information, for contrastive learning. Our model does not
require negative samples for training and is applicable to both homophilic and
heterophilic datasets. We demonstrate its effectiveness across various
datasets, achieving state-of-the-art performance.

</details>


### [235] [QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis](https://arxiv.org/abs/2504.16755)
*Owain Parry, Phil McMinn*

Main category: cs.LG

TL;DR: QAOA-PCA uses PCA to reduce QAOA's parameter space, improving efficiency with minor performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: The increasing computational burden of QAOA due to linearly growing parameters with layers.

Method: Introduces QAOA-PCA, a reparameterization technique using PCA to reduce dimensionality by leveraging optimized parameters from smaller instances.

Result: QAOA-PCA requires fewer iterations than standard QAOA, achieving efficiency gains with a slight reduction in approximation ratio.

Conclusion: QAOA-PCA balances efficiency and performance, reducing optimization overhead without significantly compromising quality.

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a promising
variational algorithm for solving combinatorial optimization problems on
near-term devices. However, as the number of layers in a QAOA circuit
increases, which is correlated with the quality of the solution, the number of
parameters to optimize grows linearly. This results in more iterations required
by the classical optimizer, which results in an increasing computational burden
as more circuit executions are needed. To mitigate this issue, we introduce
QAOA-PCA, a novel reparameterization technique that employs Principal Component
Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By
extracting principal components from optimized parameters of smaller problem
instances, QAOA-PCA facilitates efficient optimization with fewer parameters on
larger instances. Our empirical evaluation on the prominent MaxCut problem
demonstrates that QAOA-PCA consistently requires fewer iterations than standard
QAOA, achieving substantial efficiency gains. While this comes at the cost of a
slight reduction in approximation ratio compared to QAOA with the same number
of layers, QAOA-PCA almost always outperforms standard QAOA when matched by
parameter count. QAOA-PCA strikes a favorable balance between efficiency and
performance, reducing optimization overhead without significantly compromising
solution quality.

</details>


### [236] [Noise-Tolerant Coreset-Based Class Incremental Continual Learning](https://arxiv.org/abs/2504.16763)
*Edison Mucllari, Aswin Raghavan, Zachary Alan Daniels*

Main category: cs.LG

TL;DR: The paper explores the impact of label and instance noise in continual learning (CL) for class-incremental learning (CIL), proposing noise-tolerant replay buffer algorithms to improve robustness.


<details>
  <summary>Details</summary>
Motivation: CL algorithms must adapt to new tasks without forgetting old ones, but noise can disrupt this process. The study focuses on understanding and mitigating noise in CIL.

Method: The authors derive a theoretical bound for robustness to instance noise and develop two noise-tolerant replay buffer algorithms.

Result: Existing memory-based CL methods are not robust to noise, while the proposed algorithms show significant improvements in accuracy and reduced forgetting.

Conclusion: The study demonstrates the effectiveness of noise-tolerant replay buffers in noisy CIL settings, offering practical solutions for robust continual learning.

Abstract: Many applications of computer vision require the ability to adapt to novel
data distributions after deployment. Adaptation requires algorithms capable of
continual learning (CL). Continual learners must be plastic to adapt to novel
tasks while minimizing forgetting of previous tasks.However, CL opens up
avenues for noise to enter the training pipeline and disrupt the CL. This work
focuses on label noise and instance noise in the context of class-incremental
learning (CIL), where new classes are added to a classifier over time, and
there is no access to external data from past classes. We aim to understand the
sensitivity of CL methods that work by replaying items from a memory
constructed using the idea of Coresets. We derive a new bound for the
robustness of such a method to uncorrelated instance noise under a general
additive noise threat model, revealing several insights. Putting the theory
into practice, we create two continual learning algorithms to construct
noise-tolerant replay buffers. We empirically compare the effectiveness of
prior memory-based continual learners and the proposed algorithms under label
and uncorrelated instance noise on five diverse datasets. We show that existing
memory-based CL are not robust whereas the proposed methods exhibit significant
improvements in maximizing classification accuracy and minimizing forgetting in
the noisy CIL setting.

</details>


### [237] [Online model learning with data-assimilated reservoir computers](https://arxiv.org/abs/2504.16767)
*Andrea NÃ³voa, Luca Magri*

Main category: cs.LG

TL;DR: An online learning framework for forecasting nonlinear spatio-temporal signals using dimensionality reduction, autoregressive modeling, and online adaptation, tested on a cylinder wake flow.


<details>
  <summary>Details</summary>
Motivation: To improve forecasting of nonlinear spatio-temporal signals by integrating data-driven reduced order modeling with Bayesian data assimilation.

Method: Combines proper orthogonal decomposition (POD), reservoir computing, and ensemble sequential data assimilation for online model updates.

Result: The two-fold estimation strategy outperforms the naÃ¯ve approach, and the three-fold strategy enables robust online training of partially-trained models.

Conclusion: The framework unifies data-driven modeling and Bayesian assimilation, offering scalable online learning for nonlinear time series forecasting.

Abstract: We propose an online learning framework for forecasting nonlinear
spatio-temporal signals (fields). The method integrates (i) dimensionality
reduction, here, a simple proper orthogonal decomposition (POD) projection;
(ii) a generalized autoregressive model to forecast reduced dynamics, here, a
reservoir computer; (iii) online adaptation to update the reservoir computer
(the model), here, ensemble sequential data assimilation.We demonstrate the
framework on a wake past a cylinder governed by the Navier-Stokes equations,
exploring the assimilation of full flow fields (projected onto POD modes) and
sparse sensors. Three scenarios are examined: a na\"ive physical state
estimation; a two-fold estimation of physical and reservoir states; and a
three-fold estimation that also adjusts the model parameters. The two-fold
strategy significantly improves ensemble convergence and reduces reconstruction
error compared to the na\"ive approach. The three-fold approach enables robust
online training of partially-trained reservoir computers, overcoming
limitations of a priori training. By unifying data-driven reduced order
modelling with Bayesian data assimilation, this work opens new opportunities
for scalable online model learning for nonlinear time series forecasting.

</details>


### [238] [Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections](https://arxiv.org/abs/2504.16831)
*Frederik L. Dennig, Nina Geyer, Daniela Blumberg, Yannick Metz, Daniel A. Keim*

Main category: cs.LG

TL;DR: The paper explores neural networks for creating parametric and invertible multidimensional data projections, evaluating three autoencoder architectures for smoother results.


<details>
  <summary>Details</summary>
Motivation: To simultaneously achieve parametric and invertible properties in projections, which has not been explored before for arbitrary methods.

Method: Three autoencoder architectures are trained to map data into 2D space and back, using a customized loss function.

Result: Autoencoders with customized loss functions produce smoother parametric and inverse projections than feed-forward networks, with controllable smoothing.

Conclusion: Customized autoencoders offer superior performance for parametric and invertible projections, providing user control over smoothing.

Abstract: Recently, neural networks have gained attention for creating parametric and
invertible multidimensional data projections. Parametric projections allow for
embedding previously unseen data without recomputing the projection as a whole,
while invertible projections enable the generation of new data points. However,
these properties have never been explored simultaneously for arbitrary
projection methods. We evaluate three autoencoder (AE) architectures for
creating parametric and invertible projections. Based on a given projection, we
train AEs to learn a mapping into 2D space and an inverse mapping into the
original space. We perform a quantitative and qualitative comparison on four
datasets of varying dimensionality and pattern complexity using t-SNE. Our
results indicate that AEs with a customized loss function can create smoother
parametric and inverse projections than feed-forward neural networks while
giving users control over the strength of the smoothing effect.

</details>


### [239] [Improving Significant Wave Height Prediction Using Chronos Models](https://arxiv.org/abs/2504.16834)
*Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang*

Main category: cs.LG

TL;DR: Chronos, an LLM-powered temporal architecture, improves wave forecasting with faster training, superior accuracy, and zero-shot capability.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in physics-based and traditional ML methods for wave height prediction.

Method: Uses advanced temporal pattern recognition on historical data from the Northwest Pacific basin.

Result: Achieves 14.3% faster training, 2.5x inference speed, and leads in short- and long-term forecasts.

Conclusion: Sets a new standard for wave prediction with computational efficiency and transferability.

Abstract: Accurate wave height prediction is critical for maritime safety and coastal
resilience, yet conventional physics-based models and traditional machine
learning methods face challenges in computational efficiency and nonlinear
dynamics modeling. This study introduces Chronos, the first implementation of a
large language model (LLM)-powered temporal architecture (Chronos) optimized
for wave forecasting. Through advanced temporal pattern recognition applied to
historical wave data from three strategically chosen marine zones in the
Northwest Pacific basin, our framework achieves multimodal improvements: (1)
14.3% reduction in training time with 2.5x faster inference speed compared to
PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units;
(2) superior short-term forecasting (1-24h) across comprehensive metrics; (3)
sustained predictive leadership in extended-range forecasts (1-120h); and (4)
demonstrated zero-shot capability maintaining median performance (rank 4/12)
against specialized operational models. This LLM-enhanced temporal modeling
paradigm establishes a new standard in wave prediction, offering both
computationally efficient solutions and a transferable framework for complex
geophysical systems modeling.

</details>


### [240] [An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning](https://arxiv.org/abs/2504.16866)
*Panagiotis Kakosimos, Alireza Nemat Saberi, Luca Peretti*

Main category: cs.LG

TL;DR: The paper proposes a framework combining transfer learning (TL) and federated learning (FL) to adapt thermal ML models for power converters, addressing challenges like varying conditions and data sharing. It evaluates three domain adaptation techniques, with fine-tuning showing high accuracy. FL performance varies based on hosting (local vs. cloud).


<details>
  <summary>Details</summary>
Motivation: To address challenges in adapting thermal ML models for power converters, such as varying operating conditions, data sharing limitations, and security concerns, by leveraging TL and FL.

Method: Combines TL and FL, using three domain adaptation techniques (Fine-tuning, TCA, DDA) and the Flower framework for FL with Federated Averaging. Validation is done with field data.

Result: Fine-tuning is the most accurate TL method. Locally hosted FL performs better when data aggregation is infeasible, while cloud-based FL scales better with more clients.

Conclusion: The framework effectively adapts thermal ML models, with fine-tuning being practical for high accuracy. FL hosting choice depends on scalability and connectivity needs.

Abstract: This study explores alternative framework configurations for adapting thermal
machine learning (ML) models for power converters by combining transfer
learning (TL) and federated learning (FL) in a piecewise manner. This approach
inherently addresses challenges such as varying operating conditions, data
sharing limitations, and security implications. The framework starts with a
base model that is incrementally adapted by multiple clients via adapting three
state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component
Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is
employed for FL, using Federated Averaging for aggregation. Validation with
field data demonstrates that fine-tuning offers a straightforward TL approach
with high accuracy, making it suitable for practical applications. Benchmarking
results reveal a comprehensive comparison of these methods, showcasing their
respective strengths and weaknesses when applied in different scenarios.
Locally hosted FL enhances performance when data aggregation is not feasible,
while cloud-based FL becomes more practical with a significant increase in the
number of clients, addressing scalability and connectivity challenges.

</details>


### [241] [Exploring How LLMs Capture and Represent Domain-Specific Knowledge](https://arxiv.org/abs/2504.16871)
*Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan*

Main category: cs.LG

TL;DR: LLMs can distinguish domain-specific queries using hidden states, with fine-tuned models not always being the most accurate.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs inherently capture domain nuances and their ability to differentiate queries across domains.

Method: Examine hidden states during prefill phase, analyze domain-related trajectories, and test robustness to prompt variations.

Result: LLMs can differentiate domain queries; fine-tuned models aren't always the best.

Conclusion: LLMs inherently recognize domain nuances, useful for model selection in generative tasks.

Abstract: We study whether Large Language Models (LLMs) inherently capture
domain-specific nuances in natural language. Our experiments probe the domain
sensitivity of LLMs by examining their ability to distinguish queries from
different domains using hidden states generated during the prefill phase. We
reveal latent domain-related trajectories that indicate the model's internal
recognition of query domains. We also study the robustness of these domain
representations to variations in prompt styles and sources. Our approach
leverages these representations for model selection, mapping the LLM that best
matches the domain trace of the input query (i.e., the model with the highest
performance on similar traces). Our findings show that LLMs can differentiate
queries for related domains, and that the fine-tuned model is not always the
most accurate. Unlike previous work, our interpretations apply to both closed
and open-ended generative tasks

</details>


### [242] [Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion](https://arxiv.org/abs/2504.16875)
*Julian Bedei, Murray McBain, Charles Robert Koch, Jakob Andert, David Gordon*

Main category: cs.LG

TL;DR: A hybrid RL and ML-MPC approach is proposed to optimize hydrogen-diesel dual-fuel engine control, combining ML-MPC's safety with RL's adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of using RL or ML-MPC alone, such as RL's unsafe early-phase actions and ML-MPC's lack of adaptability to system drifts.

Method: A hybrid approach where RL dynamically adjusts ML-MPC's load tracking reference while ML-MPC ensures safe control inputs.

Result: The hybrid method reduced RMSE in load tracking from 0.57 bar (ML-MPC alone) to 0.44 bar, demonstrating improved adaptability and safety.

Conclusion: The hybrid RL and ML-MPC approach effectively balances safety and adaptability, enhancing control performance in dynamic engine environments.

Abstract: Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive
Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel
dual-fuel engine control, as they can effectively control multiple-input
multiple-output systems and nonlinear processes. ML-MPC is advantageous for
providing safe and optimal controls, ensuring the engine operates within
predefined safety limits. In contrast, RL is distinguished by its adaptability
to changing conditions through its learning-based approach. However, the
practical implementation of either method alone poses challenges. RL requires
high variance in control inputs during early learning phases, which can pose
risks to the system by potentially executing unsafe actions, leading to
mechanical damage. Conversely, ML-MPC relies on an accurate system model to
generate optimal control inputs and has limited adaptability to system drifts,
such as injector aging, which naturally occur in engine applications. To
address these limitations, this study proposes a hybrid RL and ML-MPC approach
that uses an ML-MPC framework while incorporating an RL agent to dynamically
adjust the ML-MPC load tracking reference in response to changes in the
environment. At the same time, the ML-MPC ensures that actions stay safe
throughout the RL agent's exploration. To evaluate the effectiveness of this
approach, fuel pressure is deliberately varied to introduce a model-plant
mismatch between the ML-MPC and the engine test bench. The result of this
mismatch is a root mean square error (RMSE) in indicated mean effective
pressure of 0.57 bar when running the ML-MPC. The experimental results
demonstrate that RL successfully adapts to changing boundary conditions by
altering the tracking reference while ML-MPC ensures safe control inputs. The
quantitative improvement in load tracking by implementing RL is an RSME of 0.44
bar.

</details>


### [243] [I-Con: A Unifying Framework for Representation Learning](https://arxiv.org/abs/2504.16929)
*Shaden Alshammari, John Hershey, Axel Feldmann, William T. Freeman, Mark Hamilton*

Main category: cs.LG

TL;DR: A unified information-theoretic framework generalizes modern loss functions in machine learning, connecting 23 approaches and improving unsupervised classification by 8%.


<details>
  <summary>Details</summary>
Motivation: To unify diverse loss functions in representation learning under a single theoretical framework.

Method: Introduces an equation minimizing KL divergence between supervisory and learned conditional distributions, connecting clustering, spectral methods, and more.

Result: Achieves +8% improvement in unsupervised classification on ImageNet-1K and enables principled debiasing.

Conclusion: The framework unifies and advances machine learning methods, offering practical improvements and theoretical insights.

Abstract: As the field of representation learning grows, there has been a proliferation
of different loss functions to solve different classes of problems. We
introduce a single information-theoretic equation that generalizes a large
collection of modern loss functions in machine learning. In particular, we
introduce a framework that shows that several broad classes of machine learning
methods are precisely minimizing an integrated KL divergence between two
conditional distributions: the supervisory and learned representations. This
viewpoint exposes a hidden information geometry underlying clustering, spectral
methods, dimensionality reduction, contrastive learning, and supervised
learning. This framework enables the development of new loss functions by
combining successful techniques from across the literature. We not only present
a wide array of proofs, connecting over 23 different approaches, but we also
leverage these theoretical results to create state-of-the-art unsupervised
image classifiers that achieve a +8% improvement over the prior
state-of-the-art on unsupervised classification on ImageNet-1K. We also
demonstrate that I-Con can be used to derive principled debiasing methods which
improve contrastive representation learners.

</details>


### [244] [Rethinking and Recomputing the Value of Machine Learning Models](https://arxiv.org/abs/2209.15157)
*Burcu Sayin, Jie Yang, Xinyue Chen, Andrea Passerini, Fabio Casati*

Main category: cs.LG

TL;DR: The paper critiques traditional ML evaluation metrics for ignoring real-world value and proposes a new 'value' metric focusing on human-AI collaboration, calibration, and practical benefits.


<details>
  <summary>Details</summary>
Motivation: Current ML evaluation metrics (e.g., accuracy, f-score) overlook real-world application value, especially in hybrid human-AI workflows.

Method: Introduces a 'value' metric incorporating task-specific costs for predictions, errors, and rejections, and emphasizes calibration.

Result: Experiments show traditional metrics mislead model selection; well-calibrated, simpler models often outperform complex ones.

Conclusion: A shift to value-centric evaluation and calibration is crucial for practical ML deployment in real-world scenarios.

Abstract: In this paper, we argue that the prevailing approach to training and
evaluating machine learning models often fails to consider their real-world
application within organizational or societal contexts, where they are intended
to create beneficial value for people. We propose a shift in perspective,
redefining model assessment and selection to emphasize integration into
workflows that combine machine predictions with human expertise, particularly
in scenarios requiring human intervention for low-confidence predictions.
Traditional metrics like accuracy and f-score fail to capture the beneficial
value of models in such hybrid settings. To address this, we introduce a simple
yet theoretically sound "value" metric that incorporates task-specific costs
for correct predictions, errors, and rejections, offering a practical framework
for real-world evaluation. Through extensive experiments, we show that existing
metrics fail to capture real-world needs, often leading to suboptimal choices
in terms of value when used to rank classifiers. Furthermore, we emphasize the
critical role of calibration in determining model value, showing that simple,
well-calibrated models can often outperform more complex models that are
challenging to calibrate.

</details>


### [245] [Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference](https://arxiv.org/abs/2310.16705)
*Dai Hai Nguyen, Tetsuya Sakurai, Hiroshi Mamitsuka*

Main category: cs.LG

TL;DR: The paper reframes variational inference (VI) as an optimization problem over probability distributions in variational parameter space, proposing Wasserstein gradient descent as a unifying approach.


<details>
  <summary>Details</summary>
Motivation: To generalize and unify existing VI methods (black-box VI and natural-gradient VI) under a single optimization framework.

Method: Proposes Wasserstein gradient descent for optimizing distributions over variational parameters, with practical methods for solving discrete gradient flows.

Result: Empirical validation on synthetic data and theoretical analyses confirm the effectiveness of the proposed methods.

Conclusion: The Wasserstein gradient descent framework successfully generalizes existing VI techniques and offers practical optimization improvements.

Abstract: Variational inference (VI) can be cast as an optimization problem in which
the variational parameters are tuned to closely align a variational
distribution with the true posterior. The optimization task can be approached
through vanilla gradient descent in black-box VI or natural-gradient descent in
natural-gradient VI. In this work, we reframe VI as the optimization of an
objective that concerns probability distributions defined over a
\textit{variational parameter space}. Subsequently, we propose Wasserstein
gradient descent for tackling this optimization problem. Notably, the
optimization techniques, namely black-box VI and natural-gradient VI, can be
reinterpreted as specific instances of the proposed Wasserstein gradient
descent. To enhance the efficiency of optimization, we develop practical
methods for numerically solving the discrete gradient flows. We validate the
effectiveness of the proposed methods through empirical experiments on a
synthetic dataset, supplemented by theoretical analyses.

</details>


### [246] [Learning-augmented Online Minimization of Age of Information and Transmission Costs](https://arxiv.org/abs/2403.02573)
*Zhongdong Liu, Keyuan Zhang, Bin Li, Yin Sun, Y. Thomas Hou, Bo Ji*

Main category: cs.LG

TL;DR: A robust online algorithm is developed to balance transmission and staleness costs in a resource-constrained system, enhanced by ML predictions for better average performance while maintaining worst-case guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the tradeoff between transmission and staleness costs in time-sensitive data transmission over a wireless channel, ensuring both robustness and average performance.

Method: Develops a learning-augmented online algorithm combining ML predictions for average-case performance with worst-case guarantees.

Result: The algorithm performs well empirically, achieving consistency (close to optimal offline performance with accurate predictions) and robustness (worst-case guarantees).

Conclusion: The learning-augmented approach successfully balances robustness and average performance, validated through simulations.

Abstract: We consider a discrete-time system where a resource-constrained source (e.g.,
a small sensor) transmits its time-sensitive data to a destination over a
time-varying wireless channel. Each transmission incurs a fixed transmission
cost (e.g., energy cost), and no transmission results in a staleness cost
represented by the Age-of-Information. The source must balance the tradeoff
between transmission and staleness costs. To address this challenge, we develop
a robust online algorithm to minimize the sum of transmission and staleness
costs, ensuring a worst-case performance guarantee. While online algorithms are
robust, they are usually overly conservative and may have a poor average
performance in typical scenarios. In contrast, by leveraging historical data
and prediction models, machine learning (ML) algorithms perform well in average
cases. However, they typically lack worst-case performance guarantees. To
achieve the best of both worlds, we design a learning-augmented online
algorithm that exhibits two desired properties: (i) consistency: closely
approximating the optimal offline algorithm when the ML prediction is accurate
and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML
predictions are inaccurate. Finally, we perform extensive simulations to show
that our online algorithm performs well empirically and that our
learning-augmented algorithm achieves both consistency and robustness.

</details>


### [247] [Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge](https://arxiv.org/abs/2407.15192)
*Joshua Shay Kricheli, Khoa Vo, Aniruddha Datta, Spencer Ozgur, Paulo Shakarian*

Main category: cs.LG

TL;DR: The paper introduces an Error Detection Rules (EDR) approach to learn explainable constraints for Hierarchical Multi-label Classification (HMC) without assuming their existence beforehand, improving model consistency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing neurosymbolic HMC methods assume pre-existing constraints; this work aims to relax this assumption by learning constraints from model failure modes.

Method: The proposed method uses EDR to detect machine learning errors and derive explainable rules, which are then applied as constraints in HMC.

Result: The approach effectively detects errors, recovers constraints, tolerates noise, and serves as knowledge for neurosymbolic models across multiple datasets.

Conclusion: The EDR-based method successfully learns and applies constraints for HMC without prior assumptions, enhancing model performance and explainability.

Abstract: Recent advances in Hierarchical Multi-label Classification (HMC),
particularly neurosymbolic-based approaches, have demonstrated improved
consistency and accuracy by enforcing constraints on a neural model during
training. However, such work assumes the existence of such constraints
a-priori. In this paper, we relax this strong assumption and present an
approach based on Error Detection Rules (EDR) that allow for learning
explainable rules about the failure modes of machine learning models. We show
that these rules are not only effective in detecting when a machine learning
classifier has made an error but also can be leveraged as constraints for HMC,
thereby allowing the recovery of explainable constraints even if they are not
provided. We show that our approach is effective in detecting machine learning
errors and recovering constraints, is noise tolerant, and can function as a
source of knowledge for neurosymbolic models on multiple datasets, including a
newly introduced military vehicle recognition dataset.

</details>


### [248] [Sharp Bounds for Sequential Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2405.01142)
*Yipeng Li, Xinchen Lyu*

Main category: cs.LG

TL;DR: The paper compares parallel (PFL) and sequential (SFL) federated learning, establishing convergence guarantees for SFL on heterogeneous data and showing SFL outperforms PFL under high heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of convergence theory for SFL on heterogeneous data and resolve its theoretical dilemma.

Method: Derived upper bounds for strongly convex, general convex, and non-convex objectives, and constructed matching lower bounds for strongly convex and general convex cases. Compared SFL and PFL bounds.

Result: SFL outperforms PFL on heterogeneous data, especially under high heterogeneity, validated by experiments.

Conclusion: SFL is theoretically and empirically superior to PFL for heterogeneous data, resolving the gap in convergence theory.

Abstract: There are two paradigms in Federated Learning (FL): parallel FL (PFL), where
models are trained in a parallel manner across clients, and sequential FL
(SFL), where models are trained in a sequential manner across clients.
Specifically, in PFL, clients perform local updates independently and send the
updated model parameters to a global server for aggregation; in SFL, one client
starts its local updates only after receiving the model parameters from the
previous client in the sequence. In contrast to that of PFL, the convergence
theory of SFL on heterogeneous data is still lacking. To resolve the
theoretical dilemma of SFL, we establish sharp convergence guarantees for SFL
on heterogeneous data with both upper and lower bounds. Specifically, we derive
the upper bounds for the strongly convex, general convex and non-convex
objective functions, and construct the matching lower bounds for the strongly
convex and general convex objective functions. Then, we compare the upper
bounds of SFL with those of PFL, showing that SFL outperforms PFL on
heterogeneous data (at least, when the level of heterogeneity is relatively
high). Experimental results validate the counterintuitive theoretical finding.

</details>


### [249] [A Survey on Mixup Augmentations and Beyond](https://arxiv.org/abs/2409.05202)
*Xin Jin, Hongyu Zhu, Siyuan Li, Zedong Wang, Zicheng Liu, Juanxi Tian, Chang Yu, Huafeng Qin, Stan Z. Li*

Main category: cs.LG

TL;DR: A survey on mixup data augmentation methods, their applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: To review and unify mixup methods as a regularization technique for deep learning, especially when labeled data is scarce.

Method: Presents a unified framework for mixup augmentations, explores their applications in vision tasks and other data modalities, and analyzes theoretical aspects.

Result: Highlights the effectiveness of mixup in generating virtual data and its versatility across domains, while identifying current limitations.

Conclusion: The survey summarizes the state of mixup research, offers insights, and suggests future work for improving mixup techniques.

Abstract: As Deep Neural Networks have achieved thrilling breakthroughs in the past
decade, data augmentations have garnered increasing attention as regularization
techniques when massive labeled data are unavailable. Among existing
augmentations, Mixup and relevant data-mixing methods that convexly combine
selected samples and the corresponding labels are widely adopted because they
yield high performances by generating data-dependent virtual data while easily
migrating to various domains. This survey presents a comprehensive review of
foundational mixup methods and their applications. We first elaborate on the
training pipeline with mixup augmentations as a unified framework containing
modules. A reformulated framework could contain various mixup methods and give
intuitive operational procedures. Then, we systematically investigate the
applications of mixup augmentations on vision downstream tasks, various data
modalities, and some analysis \& theorems of mixup. Meanwhile, we conclude the
current status and limitations of mixup research and point out further work for
effective and efficient mixup augmentations. This survey can provide
researchers with the current state of the art in mixup methods and provide some
insights and guidance roles in the mixup arena. An online project with this
survey is available at https://github.com/Westlake-AI/Awesome-Mixup.

</details>


### [250] [Multi-Player Approaches for Dueling Bandits](https://arxiv.org/abs/2405.16168)
*Or Raveh, Junya Honda, Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses the multiplayer dueling bandit problem, proposing a Follow Your Leader approach and a distributed method with a Condorcet-winner protocol, showing improved performance over single-player benchmarks.


<details>
  <summary>Details</summary>
Motivation: The multiplayer dueling bandit problem, common in preference-based scenarios like human feedback, lacks attention despite its challenges in collaborative exploration of non-informative arm pairs.

Method: The paper uses a Follow Your Leader black-box approach and introduces a message-passing distributed method with a Condorcet-winner recommendation protocol.

Result: The proposed multiplayer algorithms outperform single-player benchmarks, demonstrating efficacy in the multiplayer dueling bandit setting.

Conclusion: The study fills a gap in multiplayer dueling bandits, showcasing the effectiveness of collaborative approaches over traditional single-player methods.

Abstract: Various approaches have emerged for multi-armed bandits in distributed
systems. The multiplayer dueling bandit problem, common in scenarios with only
preference-based information like human feedback, introduces challenges related
to controlling collaborative exploration of non-informative arm pairs, but has
received little attention. To fill this gap, we demonstrate that the direct use
of a Follow Your Leader black-box approach matches the lower bound for this
setting when utilizing known dueling bandit algorithms as a foundation.
Additionally, we analyze a message-passing fully distributed approach with a
novel Condorcet-winner recommendation protocol, resulting in expedited
exploration in many cases. Our experimental comparisons reveal that our
multiplayer algorithms surpass single-player benchmark algorithms, underscoring
their efficacy in addressing the nuanced challenges of the multiplayer dueling
bandit setting.

</details>


### [251] [Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond](https://arxiv.org/abs/2406.01386)
*Xutong Liu, Siwei Wang, Jinhang Zuo, Han Zhong, Xuchuang Wang, Zhiyong Wang, Shuai Li, Mohammad Hajiesmaili, John C. S. Lui, Wei Chen*

Main category: cs.LG

TL;DR: A novel CMAB framework (CMAB-MT) with multivariant and probabilistically triggering arms is introduced, offering enhanced modeling and improved results through statistical properties.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing CMAB works by incorporating multivariant outcomes and general triggering processes, enabling broader applications like episodic RL.

Method: Proposes a 1-norm smoothness condition and the CUCB-MT algorithm, leveraging multivariant properties for improved regret bounds.

Result: Achieves matching or better regret bounds for applications like episodic RL and probabilistic maximum coverage.

Conclusion: Connects episodic RL and CMAB, fostering future interactions between these fields.

Abstract: We introduce a novel framework of combinatorial multi-armed bandits (CMAB)
with multivariant and probabilistically triggering arms (CMAB-MT), where the
outcome of each arm is a $d$-dimensional multivariant random variable and the
feedback follows a general arm triggering process. Compared with existing CMAB
works, CMAB-MT not only enhances the modeling power but also allows improved
results by leveraging distinct statistical properties for multivariant random
variables. For CMAB-MT, we propose a general 1-norm multivariant and triggering
probability-modulated smoothness condition, and an optimistic CUCB-MT algorithm
built upon this condition. Our framework can include many important problems as
applications, such as episodic reinforcement learning (RL) and probabilistic
maximum coverage for goods distribution, all of which meet the above smoothness
condition and achieve matching or improved regret bounds compared to existing
works. Through our new framework, we build the first connection between the
episodic RL and CMAB literature, by offering a new angle to solve the episodic
RL through the lens of CMAB, which may encourage more interactions between
these two important directions.

</details>


### [252] [Solving Inverse Problems in Protein Space Using Diffusion-Based Priors](https://arxiv.org/abs/2406.04239)
*Axel Levy, Eric R. Chan, Sara Fridovich-Keil, FrÃ©dÃ©ric Poitevin, Ellen D. Zhong, Gordon Wetzstein*

Main category: cs.LG

TL;DR: A versatile framework combines physics-based forward modeling and generative priors to solve inverse problems in protein 3D structure determination, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Understanding protein-environment interactions via 3D structure is crucial, but experimental methods like X-ray crystallography or cryo-EM pose challenging inverse problems. Learning-based approaches are often specialized, limiting versatility.

Method: The framework integrates a physics-based forward model of measurements (e.g., cryo-EM maps) with a pretrained generative model, providing a task-agnostic prior for solving inverse problems.

Result: The method outperforms posterior sampling baselines on linear and non-linear inverse problems, excelling in refining atomic models from cryo-EM maps and building models from sparse distance matrices.

Conclusion: This approach offers a versatile, accurate solution for protein 3D structure determination, advancing the field beyond specialized methods.

Abstract: The interaction of a protein with its environment can be understood and
controlled via its 3D structure. Experimental methods for protein structure
determination, such as X-ray crystallography or cryogenic electron microscopy,
shed light on biological processes but introduce challenging inverse problems.
Learning-based approaches have emerged as accurate and efficient methods to
solve these inverse problems for 3D structure determination, but are
specialized for a predefined type of measurement. Here, we introduce a
versatile framework to turn biophysical measurements, such as cryo-EM density
maps, into 3D atomic models. Our method combines a physics-based forward model
of the measurement process with a pretrained generative model providing a
task-agnostic, data-driven prior. Our method outperforms posterior sampling
baselines on linear and non-linear inverse problems. In particular, it is the
first diffusion-based method for refining atomic models from cryo-EM maps and
building atomic models from sparse distance matrices.

</details>


### [253] [Towards the Causal Complete Cause of Multi-Modal Representation Learning](https://arxiv.org/abs/2407.14058)
*Jingyao Wang, Siyu Zhao, Wenwen Qiang, Jiangmeng Li, Fuchun Sun, Hui Xiong*

Main category: cs.LG

TL;DR: The paper proposes a causal perspective for Multi-Modal Learning (MML), introducing Causal Complete Cause (CÂ³) to ensure representations are causally sufficient and necessary. It relaxes prior assumptions, measures CÂ³ risk, and introduces CÂ³ Regularization for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing MML methods may yield insufficient or unnecessary representations due to a lack of causal sufficiency and necessity. The paper aims to address this by introducing causal principles.

Method: Defines CÂ³, discusses its identifiability, and introduces a twin network to measure CÂ³ risk. Uses an instrumental variable for sufficiency and gradient-based counterfactual modeling for necessity.

Result: Theoretical reliability of CÂ³ measurement is confirmed, and CÂ³ Regularization is proposed to minimize CÂ³ risk, improving representation quality.

Conclusion: The proposed CÂ³ framework and regularization method effectively enhance MML representations, validated by extensive experiments.

Abstract: Multi-Modal Learning (MML) aims to learn effective representations across
modalities for accurate predictions. Existing methods typically focus on
modality consistency and specificity to learn effective representations.
However, from a causal perspective, they may lead to representations that
contain insufficient and unnecessary information. To address this, we propose
that effective MML representations should be causally sufficient and necessary.
Considering practical issues like spurious correlations and modality conflicts,
we relax the exogeneity and monotonicity assumptions prevalent in prior works
and explore the concepts specific to MML, i.e., Causal Complete Cause
(\(C^3\)). We begin by defining \(C^3\), which quantifies the probability of
representations being causally sufficient and necessary. We then discuss the
identifiability of \(C^3\) and introduce an instrumental variable to support
identifying \(C^3\) with non-exogeneity and non-monotonicity. Building on this,
we conduct the $C^3$ measurement, i.e., \(C^3\) risk. We propose a twin network
to estimate it through (i) the real-world branch: utilizing the instrumental
variable for sufficiency, and (ii) the hypothetical-world branch: applying
gradient-based counterfactual modeling for necessity. Theoretical analyses
confirm its reliability. Based on these results, we propose $C^3$
Regularization, a plug-and-play method that enforces the causal completeness of
the learned representations by minimizing \(C^3\) risk. Extensive experiments
demonstrate its effectiveness.

</details>


### [254] [Discover physical concepts and equations with machine learning](https://arxiv.org/abs/2412.12161)
*Bao-Bing Li, Yi Gu, Shao-Feng Wu*

Main category: cs.LG

TL;DR: The paper extends SciNet by combining VAE and Neural ODEs to simultaneously discover physical concepts and governing equations from data.


<details>
  <summary>Details</summary>
Motivation: To address the intertwined nature of physical concepts and equations, enabling their independent discovery.

Method: Proposes a model integrating Variational Autoencoders (VAE) with Neural Ordinary Differential Equations (Neural ODEs).

Result: Demonstrates successful discovery of correct physical theories (e.g., heliocentrism, Newton's gravity) in the neural network.

Conclusion: The model effectively uncovers physical concepts and equations, validated by historical physics examples.

Abstract: Machine learning can uncover physical concepts or physical equations when
prior knowledge from the other is available. However, these two aspects are
often intertwined and cannot be discovered independently. We extend SciNet,
which is a neural network architecture that simulates the human physical
reasoning process for physics discovery, by proposing a model that combines
Variational Autoencoders (VAE) with Neural Ordinary Differential Equations
(Neural ODEs). This allows us to simultaneously discover physical concepts and
governing equations from simulated experimental data across various physical
systems. We apply the model to several examples inspired by the history of
physics, including Copernicus' heliocentrism, Newton's law of gravity,
Schr\"odinger's wave mechanics, and Pauli's spin-magnetic formulation. The
results demonstrate that the correct physical theories can emerge in the neural
network.

</details>


### [255] [Practical Aspects on Solving Differential Equations Using Deep Learning: A Primer](https://arxiv.org/abs/2408.11266)
*Georgios Is. Detorakis*

Main category: cs.LG

TL;DR: A primer on the Deep Galerkin method for solving differential equations using deep learning, with practical examples and code.


<details>
  <summary>Details</summary>
Motivation: To introduce deep learning as a tool for solving differential equations, focusing on the Deep Galerkin method.

Method: Uses deep neural networks to solve differential equations, demonstrated via the one-dimensional heat equation and systems of ODEs/integral equations.

Result: Provides step-by-step examples and code snippets, enabling implementation on simple computers without GPUs.

Conclusion: The Deep Galerkin method is a practical and accessible approach for solving differential equations using deep learning.

Abstract: Deep learning has become a popular tool across many scientific fields,
including the study of differential equations, particularly partial
differential equations. This work introduces the basic principles of deep
learning and the Deep Galerkin method, which uses deep neural networks to solve
differential equations. This primer aims to provide technical and practical
insights into the Deep Galerkin method and its implementation. We demonstrate
how to solve the one-dimensional heat equation step-by-step. We also show how
to apply the Deep Galerkin method to solve systems of ordinary differential
equations and integral equations, such as the Fredholm of the second kind.
Additionally, we provide code snippets within the text and the complete source
code on Github. The examples are designed so that one can run them on a simple
computer without needing a GPU.

</details>


### [256] [Predicting sub-population specific viral evolution](https://arxiv.org/abs/2410.21518)
*Wenxian Shi, Menghua Wu, Regina Barzilay*

Main category: cs.LG

TL;DR: A sub-population specific protein evolution model predicts viral variant distributions across locations by modeling transmission rates and interdependencies, outperforming baselines in accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to account for location-specific predictions and viral transmissions, limiting their utility for disease surveillance and therapeutic design.

Method: The model uses a linear ODE parametrized by transmission rates to predict viral protein distributions, learning interdependencies from data.

Result: The model outperforms baselines in predicting viral protein distributions across continents and countries, with learned transmission rates aligning with phylogenetic analysis.

Conclusion: The proposed model effectively addresses the limitations of existing approaches, offering improved accuracy for viral variant forecasting.

Abstract: Forecasting the change in the distribution of viral variants is crucial for
therapeutic design and disease surveillance. This task poses significant
modeling challenges due to the sharp differences in virus distributions across
sub-populations (e.g., countries) and their dynamic interactions. Existing
machine learning approaches that model the variant distribution as a whole are
incapable of making location-specific predictions and ignore transmissions that
shape the viral landscape. In this paper, we propose a sub-population specific
protein evolution model, which predicts the time-resolved distributions of
viral proteins in different locations. The algorithm explicitly models the
transmission rates between sub-populations and learns their interdependence
from data. The change in protein distributions across all sub-populations is
defined through a linear ordinary differential equation (ODE) parametrized by
transmission rates. Solving this ODE yields the likelihood of a given protein
occurring in particular sub-populations. Multi-year evaluation on both
SARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms
baselines in accurately predicting distributions of viral proteins across
continents and countries. We also find that the transmission rates learned from
data are consistent with the transmission pathways discovered by retrospective
phylogenetic analysis.

</details>


### [257] [Approximate Equivariance in Reinforcement Learning](https://arxiv.org/abs/2411.04225)
*Jung Yeon Park, Sujay Bhatt, Sihan Zeng, Lawson L. S. Wong, Alec Koppel, Sumitra Ganesh, Robin Walters*

Main category: cs.LG

TL;DR: The paper introduces approximately equivariant algorithms in reinforcement learning (RL) for tasks with approximate symmetry, outperforming exact equivariant networks in such scenarios.


<details>
  <summary>Details</summary>
Motivation: Exact symmetry imposition in RL is often inappropriate due to approximate symmetry in real-world tasks, prompting the need for approximately equivariant networks.

Method: The authors define approximately equivariant MDPs, analyze their impact on the optimal Q function, and propose RL architectures using relaxed group and steerable convolutions.

Result: The approximately equivariant network matches exact equivariant networks in exact symmetry cases and outperforms them in approximate symmetry scenarios, with added robustness to noise.

Conclusion: Approximately equivariant networks are effective for RL tasks with approximate symmetry, offering improved performance and robustness.

Abstract: Equivariant neural networks have shown great success in reinforcement
learning, improving sample efficiency and generalization when there is symmetry
in the task. However, in many problems, only approximate symmetry is present,
which makes imposing exact symmetry inappropriate. Recently, approximately
equivariant networks have been proposed for supervised classification and
modeling physical systems. In this work, we develop approximately equivariant
algorithms in reinforcement learning (RL). We define approximately equivariant
MDPs and theoretically characterize the effect of approximate equivariance on
the optimal $Q$ function. We propose novel RL architectures using relaxed group
and steerable convolutions and experiment on several continuous control domains
and stock trading with real financial data. Our results demonstrate that the
approximately equivariant network performs on par with exactly equivariant
networks when exact symmetries are present, and outperforms them when the
domains exhibit approximate symmetry. As an added byproduct of these
techniques, we observe increased robustness to noise at test time. Our code is
available at https://github.com/jypark0/approx_equiv_rl.

</details>


### [258] [Constrained composite Bayesian optimization for rational synthesis of polymeric particles](https://arxiv.org/abs/2411.10471)
*Fanjin Wang, Maryam Parhizkar, Anthony Harker, Mohan Edirisinghe*

Main category: cs.LG

TL;DR: The study introduces constrained and composite Bayesian optimization (CCBO) to optimize polymeric particle synthesis, outperforming standard methods and human expertise.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of traditional trial-and-error methods in tailoring polymeric particle synthesis for healthcare and energy applications.

Method: Integration of CCBO for target value optimization under black-box constraints and limited data, validated via synthetic and laboratory experiments.

Result: CCBO efficiently optimized particle sizes (300 nm and 3.0 Âµm) within 4 iterations, surpassing standard BO and matching expert decisions.

Conclusion: CCBO offers a versatile, AI-driven paradigm for target-driven particle synthesis, reducing reliance on domain expertise and costly experimentation.

Abstract: Polymeric nano- and micro-scale particles have critical roles in tackling
critical healthcare and energy challenges with their miniature characteristics.
However, tailoring their synthesis process to meet specific design targets has
traditionally depended on domain expertise and costly trial-and-errors.
Recently, modeling strategies, particularly Bayesian optimization (BO), have
been proposed to aid materials discovery for maximized/minimized properties.
Coming from practical demands, this study for the first time integrates
constrained and composite Bayesian optimization (CCBO) to perform efficient
target value optimization under black-box feasibility constraints and limited
data for laboratory experimentation. Using a synthetic problem that simulates
electrospraying, a model nanomanufacturing process, CCBO strategically avoided
infeasible conditions and efficiently optimized particle production towards
predefined size targets, surpassing standard BO pipelines and providing
decisions comparable to human experts. Further laboratory experiments validated
CCBO capability to guide the rational synthesis of poly(lactic-co-glycolic
acid) (PLGA) particles with diameters of 300 nm and 3.0 $\mu$m via
electrospraying. With minimal initial data and unknown experiment constraints,
CCBO reached the design targets within 4 iterations. Overall, the CCBO approach
presents a versatile and holistic optimization paradigm for next-generation
target-driven particle synthesis empowered by artificial intelligence (AI).

</details>


### [259] [Towards Physics-Guided Foundation Models](https://arxiv.org/abs/2502.15013)
*Majid Farhadloo, Arun Sharma, Mingzhou Yang, Bharat Jayaprakash, William Northrop, Shashi Shekhar*

Main category: cs.LG

TL;DR: PGFM integrates physics into foundation models to improve out-of-distribution predictions and ensure realistic outputs.


<details>
  <summary>Details</summary>
Motivation: Traditional foundation models lack physical feasibility and struggle with out-of-distribution tasks.

Method: Proposing physics-guided foundation models (PGFM) that incorporate general domain physical knowledge.

Result: PGFM aims to enhance downstream task performance by ensuring physically realistic outputs.

Conclusion: PGFM offers a promising approach to address limitations of traditional foundation models by embedding physics.

Abstract: Traditional foundation models are pre-trained on broad datasets to reduce the
training resources (e.g., time, energy, labeled samples) needed for fine-tuning
a wide range of downstream tasks. However, traditional foundation models
struggle with out-of-distribution prediction and can produce outputs that are
unrealistic and physically infeasible. We propose the notation of
physics-guided foundation models (PGFM), that is, foundation models integrated
with broad or general domain (e.g., scientific) physical knowledge applicable
to a wide range of downstream tasks.

</details>


### [260] [Enhancing Sentiment Analysis in Bengali Texts: A Hybrid Approach Using Lexicon-Based Algorithm and Pretrained Language Model Bangla-BERT](https://arxiv.org/abs/2411.19584)
*Hemal Mahmud, Hasan Mahmud, Mohammad Rifat Ahmmad Rashid*

Main category: cs.LG

TL;DR: The paper introduces a hybrid approach combining rule-based algorithms (BSPS) and pre-trained language models (BanglaBERT) for fine-grained sentiment analysis in Bengali, outperforming standalone models.


<details>
  <summary>Details</summary>
Motivation: Limited research exists for fine-grained sentiment analysis in Bengali, prompting the development of a novel hybrid method to bridge this gap.

Method: Developed a dataset of 15,000 labeled reviews, created a Lexicon Data Dictionary, and designed the BSPS algorithm. Evaluated performance using BanglaBERT and compared hybrid (BSPS + BanglaBERT) and standalone approaches.

Result: The hybrid approach achieved higher accuracy, precision, and nuanced classification across nine sentiment categories compared to standalone BanglaBERT.

Conclusion: Combining rule-based and pre-trained language models enhances sentiment analysis in Bengali, offering potential for similar languages.

Abstract: Sentiment analysis (SA) is a process of identifying the emotional tone or
polarity within a given text and aims to uncover the user's complex emotions
and inner feelings. While sentiment analysis has been extensively studied for
languages like English, research in Bengali, remains limited, particularly for
fine-grained sentiment categorization. This work aims to connect this gap by
developing a novel approach that integrates rule-based algorithms with
pre-trained language models. We developed a dataset from scratch, comprising
over 15,000 manually labeled reviews. Next, we constructed a Lexicon Data
Dictionary, assigning polarity scores to the reviews. We developed a novel rule
based algorithm Bangla Sentiment Polarity Score (BSPS), an approach capable of
generating sentiment scores and classifying reviews into nine distinct
sentiment categories. To assess the performance of this method, we evaluated
the classified sentiments using BanglaBERT, a pre-trained transformer-based
language model. We also performed sentiment classification directly with
BanglaBERT on the original data and evaluated this model's results. Our
analysis revealed that the BSPS + BanglaBERT hybrid approach outperformed the
standalone BanglaBERT model, achieving higher accuracy, precision, and nuanced
classification across the nine sentiment categories. The results of our study
emphasize the value and effectiveness of combining rule-based and pre-trained
language model approaches for enhanced sentiment analysis in Bengali and
suggest pathways for future research and application in languages with similar
linguistic complexities.

</details>


### [261] [A Mapper Algorithm with implicit intervals and its optimization](https://arxiv.org/abs/2412.11631)
*Yuyang Tao, Shufei Ge*

Main category: cs.LG

TL;DR: The paper introduces a soft Mapper framework using a Gaussian mixture model (GMM) to automate parameter tuning and handle data uncertainty, improving topological data analysis (TDA) performance.


<details>
  <summary>Details</summary>
Motivation: The standard Mapper algorithm requires manual parameter tuning and lacks flexibility in handling data uncertainty, limiting its effectiveness.

Method: A soft Mapper framework based on GMM is proposed, using stochastic gradient descent for automatic parameter optimization and introducing a Mapper graph mode for robustness.

Result: Simulations and applications show the framework effectively captures topological structures and identifies a distinct Alzheimer's Disease subgroup in an RNA dataset.

Conclusion: The soft Mapper framework addresses limitations of the standard Mapper algorithm, offering automated, flexible, and robust topological analysis.

Abstract: The Mapper algorithm is an essential tool for visualizing complex, high
dimensional data in topology data analysis (TDA) and has been widely used in
biomedical research. It outputs a combinatorial graph whose structure implies
the shape of the data. However,the need for manual parameter tuning and fixed
intervals, along with fixed overlapping ratios may impede the performance of
the standard Mapper algorithm. Variants of the standard Mapper algorithms have
been developed to address these limitations, yet most of them still require
manual tuning of parameters. Additionally, many of these variants, including
the standard version found in the literature, were built within a deterministic
framework and overlooked the uncertainty inherent in the data. To relax these
limitations, in this work, we introduce a novel framework that implicitly
represents intervals through a hidden assignment matrix, enabling automatic
parameter optimization via stochastic gradient descent. In this work, we
develop a soft Mapper framework based on a Gaussian mixture model(GMM) for
flexible and implicit interval construction. We further illustrate the
robustness of the soft Mapper algorithm by introducing the Mapper graph mode as
a point estimation for the output graph. Moreover, a stochastic gradient
descent algorithm with a specific topological loss function is proposed for
optimizing parameters in the model. Both simulation and application studies
demonstrate its effectiveness in capturing the underlying topological
structures. In addition, the application to an RNA expression dataset obtained
from the Mount Sinai/JJ Peters VA Medical Center Brain Bank (MSBB) successfully
identifies a distinct subgroup of Alzheimer's Disease.

</details>


### [262] [Geodesic Flow Kernels for Semi-Supervised Learning on Mixed-Variable Tabular Dataset](https://arxiv.org/abs/2412.12864)
*Yoontae Hwang, Yongjae Lee*

Main category: cs.LG

TL;DR: GFTab is a semi-supervised framework for tabular data, using variable-specific corruption, geodesic flow kernels, and tree-based embedding to outperform existing models, especially with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with heterogeneous tabular data. GFTab addresses this by capturing underlying structure and relationships.

Method: GFTab uses variable-specific corruption, geodesic flow kernels for similarity, and tree-based embedding.

Result: GFTab outperforms existing ML/DL models on 21 datasets, particularly with limited labeled data.

Conclusion: GFTab is effective for semi-supervised learning on mixed-variable tabular datasets.

Abstract: Tabular data poses unique challenges due to its heterogeneous nature,
combining both continuous and categorical variables. Existing approaches often
struggle to effectively capture the underlying structure and relationships
within such data. We propose GFTab (Geodesic Flow Kernels for Semi- Supervised
Learning on Mixed-Variable Tabular Dataset), a semi-supervised framework
specifically designed for tabular datasets. GFTab incorporates three key
innovations: 1) Variable-specific corruption methods tailored to the distinct
properties of continuous and categorical variables, 2) A Geodesic flow kernel
based similarity measure to capture geometric changes between corrupted inputs,
and 3) Tree-based embedding to leverage hierarchical relationships from
available labeled data. To rigorously evaluate GFTab, we curate a comprehensive
set of 21 tabular datasets spanning various domains, sizes, and variable
compositions. Our experimental results show that GFTab outperforms existing
ML/DL models across many of these datasets, particularly in settings with
limited labeled data.

</details>


### [263] [CF-CAM: Cluster Filter Class Activation Mapping for Reliable Gradient-Based Interpretability](https://arxiv.org/abs/2504.00060)
*Hongjie He, Xu Pan, Yudong Yao*

Main category: cs.LG

TL;DR: CF-CAM improves neural network interpretability by combining gradient-based weighting with noise robustness, outperforming existing CAM methods in efficiency and reliability.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-offs in current CAM techniques (gradient instability vs. computational overhead) to enhance trust in high-stakes applications.

Method: Proposes CF-CAM with hierarchical importance weighting, DBSCAN-based channel clustering, and cluster-conditioned gradient filtering.

Result: CF-CAM achieves better interpretability, computational efficiency, and robustness compared to state-of-the-art CAM methods.

Conclusion: CF-CAM offers a practical solution for improving neural network transparency in critical domains like autonomous driving and medical diagnosis.

Abstract: As deep learning continues to advance, the transparency of neural network
decision-making remains a critical challenge, limiting trust and applicability
in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged
as a key approach toward visualizing model decisions, yet existing methods face
inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to
gradient perturbations due to gradient noise, leading to unstable and
unreliable explanations. Conversely, gradient-free approaches mitigate gradient
instability but incur significant computational overhead and inference latency.
To address these limitations, we propose a Cluster Filter Class Activation Map
(CF-CAM) technique, a novel framework that reintroduces gradient-based
weighting while enhancing robustness against gradient noise. CF-CAM utilizes
hierarchical importance weighting strategy to balance discriminative feature
preservation and noise elimination. A density-aware channel clustering method
via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups
semantically relevant feature channels and discard noise-prone activations.
Additionally, cluster-conditioned gradient filtering leverages Gaussian filters
to refine gradient signals, preserving edge-aware localization while
suppressing noise impact. Experiment results demonstrate that CF-CAM achieves
superior interpretability performance while enhancing computational efficiency,
outperforming state-of-the-art CAM methods in faithfulness and robustness. By
effectively mitigating gradient instability without excessive computational
cost, CF-CAM provides a competitive solution for enhancing the interpretability
of deep neural networks in critical applications such as autonomous driving and
medical diagnosis.

</details>


### [264] [Truthful mechanisms for linear bandit games with private contexts](https://arxiv.org/abs/2501.03865)
*Yiting Hu, Lingjie Duan*

Main category: cs.LG

TL;DR: The paper addresses the issue of private context misreporting in contextual bandit problems, proposing a truthful mechanism with low regret.


<details>
  <summary>Details</summary>
Motivation: In healthcare and recommendation systems, agents may misreport contexts for personal gain, undermining system effectiveness.

Method: A linear program-based mechanism is introduced to ensure truthful reporting while minimizing deviation from Thompson sampling.

Result: The proposed mechanism achieves $O(\ln T)$ frequentist regret, outperforming traditional methods.

Conclusion: The study provides a solution for truthful reporting in contextual bandits, validated by numerical experiments.

Abstract: The contextual bandit problem, where agents arrive sequentially with personal
contexts and the system adapts its arm allocation decisions accordingly, has
recently garnered increasing attention for enabling more personalized outcomes.
However, in many healthcare and recommendation applications, agents have
private profiles and may misreport their contexts to gain from the system. For
example, in adaptive clinical trials, where hospitals sequentially recruit
volunteers to test multiple new treatments and adjust plans based on
volunteers' reported profiles such as symptoms and interim data, participants
may misreport severe side effects like allergy and nausea to avoid perceived
suboptimal treatments. We are the first to study this issue of private context
misreporting in a stochastic contextual bandit game between the system and
non-repeated agents. We show that traditional low-regret algorithms, such as
UCB family algorithms and Thompson sampling, fail to ensure truthful reporting
and can result in linear regret in the worst case, while traditional truthful
algorithms like explore-then-commit (ETC) and $\epsilon$-greedy algorithm incur
sublinear but high regret. We propose a mechanism that uses a linear program to
ensure truthfulness while minimizing deviation from Thompson sampling, yielding
an $O(\ln T)$ frequentist regret. Our numerical experiments further demonstrate
strong performance in multiple contexts and across other distribution families.

</details>


### [265] [Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning](https://arxiv.org/abs/2502.14840)
*Arun Sharma, Majid Farhadloo, Mingzhou Yang, Ruolei Zeng, Subhankar Ghosh, Shashi Shekhar*

Main category: cs.LG

TL;DR: The paper introduces SDSA-KGML, a model for predicting land emissions by accounting for spatial heterogeneity in soil and climate data, outperforming traditional methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate quantification of the carbon cycle in agroecosystems is vital for climate change mitigation and sustainable food production, but traditional methods fail due to location-independent parameters and large dataset requirements.

Method: Proposed SDSA-KGML, which uses location-dependent parameters to address spatial heterogeneity in soil moisture and environmental conditions.

Result: SDSA-KGML achieved higher local accuracy for Midwest Region states compared to traditional approaches.

Conclusion: SDSA-KGML effectively addresses spatial heterogeneity, improving land emission predictions for better climate and agricultural sustainability outcomes.

Abstract: Given inputs of diverse soil characteristics and climate data gathered from
various regions, we aimed to build a model to predict accurate land emissions.
The problem is important since accurate quantification of the carbon cycle in
agroecosystems is crucial for mitigating climate change and ensuring
sustainable food production. Predicting accurate land emissions is challenging
since calibrating the heterogeneous nature of soil properties, moisture, and
environmental conditions is hard at decision-relevant scales. Traditional
approaches do not adequately estimate land emissions due to
location-independent parameters failing to leverage the spatial heterogeneity
and also require large datasets. To overcome these limitations, we proposed
Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML),
which leverages location-dependent parameters that account for significant
spatial heterogeneity in soil moisture from multiple sites within the same
region. Experimental results demonstrate that SDSA-KGML models achieve higher
local accuracy for the specified states in the Midwest Region.

</details>


### [266] [PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs](https://arxiv.org/abs/2504.07540)
*JosÃ© I. Orlicki*

Main category: cs.LG

TL;DR: PoGO is a blockchain consensus design where miners provide verifiable proof of training large ML models using quantized gradients and Merkle proofs, reducing costs and enabling efficient verification.


<details>
  <summary>Details</summary>
Motivation: To integrate large-scale machine learning training into blockchain consensus, ensuring verifiable progress while minimizing storage and computation overhead.

Method: Uses quantized gradients (4-bit) and Merkle proofs for efficient verification, with empirical cost analysis and considerations for block times and hardware trade-offs.

Result: Verification is significantly cheaper than training, and the protocol supports both positive and negative attestations for updates.

Conclusion: PoGO effectively bridges ML training and blockchain consensus, offering scalable and verifiable solutions for large models like GPT-3 and Gemma.

Abstract: We present a design called Proof of Gradient Optimization (PoGO) for
blockchain consensus, where miners produce verifiable evidence of training
large-scale machine-learning models. Building on previous work, we incorporate
quantized gradients (4-bit precision) to reduce storage and computation
requirements, while still preserving the ability of verifiers to check that
real progress has been made on lowering the model's loss. Additionally, we
employ Merkle proofs over the full 32-bit model to handle large parameter sets
and to enable random leaf checks with minimal on-chain data. We illustrate
these ideas using GPT-3 (175B parameters) as a reference example and also refer
to smaller but high-performance models (e.g., Gemma~3 with 27B parameters). We
provide an empirical cost analysis showing that verification is significantly
cheaper than training, thanks in part to quantization and sampling. We also
discuss the necessity of longer block times (potentially hours) when
incorporating meaningful training steps, the trade-offs when using specialized
GPU hardware, and how binary diffs may incrementally optimize updates. Finally,
we note that fine-tuning can be handled in a similar manner, merely changing
the dataset and the manner of sampling but preserving the overall verification
flow. Our protocol allows verifiers to issue either positive or negative
attestations; these are aggregated at finalization to either confirm the update
or slash the miner.

</details>


### [267] [Integrated utilization of equations and small dataset in the Koopman operator: applications to forward and inverse problems](https://arxiv.org/abs/2503.21048)
*Ichiro Ohta, Shota Koyanagi, Kayo Kinjo, Jun Ohkubo*

Main category: cs.LG

TL;DR: The paper introduces methods to incorporate ambiguous prior knowledge into the EDMD algorithm, enabling learning from small datasets for both prediction and parameter estimation tasks.


<details>
  <summary>Details</summary>
Motivation: Data-driven approaches like EDMD typically require large datasets, but prior knowledge, even if ambiguous, can reduce this need.

Method: The paper extends EDMD by integrating ambiguous prior knowledge (e.g., underlying time-evolution equations with unknown parameters) for prediction and parameter estimation tasks.

Result: Demonstrated effectiveness using small datasets on the Duffing and van der Pol systems.

Conclusion: Incorporating ambiguous prior knowledge into EDMD enhances learning efficiency with limited data, applicable to both forward and inverse problems.

Abstract: In recent years, there has been a growing interest in data-driven approaches
in physics, such as extended dynamic mode decomposition (EDMD). The EDMD
algorithm focuses on nonlinear time-evolution systems, and the constructed
Koopman matrix yields the next-time prediction with only linear matrix-product
operations. Note that data-driven approaches generally require a large dataset.
However, assume that one has some prior knowledge, even if it may be ambiguous.
Then, one could achieve sufficient learning from only a small dataset by taking
advantage of the prior knowledge. This paper yields methods for incorporating
ambiguous prior knowledge into the EDMD algorithm. The ambiguous prior
knowledge in this paper corresponds to the underlying time-evolution equations
with unknown parameters. First, we apply the proposed method to forward
problems, i.e., prediction tasks. Second, we propose a scheme to apply the
proposed method to inverse problems, i.e., parameter estimation tasks. We
demonstrate the learning with only a small dataset using guiding examples,
i.e., the Duffing and the van der Pol systems.

</details>


### [268] [On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction](https://arxiv.org/abs/2504.08169)
*Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal, Caleb Lu, Jie Liu, Hongda Shen*

Main category: cs.LG

TL;DR: The paper proposes a multitask learning framework using a Deep Hierarchical Ensemble Network (DHEN) for CVR prediction, addressing challenges like feature-crossing module selection, model depth, and hyper-parameters. It includes real-time user behavior sequences and a self-supervised auxiliary loss to improve performance.


<details>
  <summary>Details</summary>
Motivation: The performance of DHEN for CVR prediction in conversion ads is unclear, and challenges like feature-crossing module selection, model architecture, and hyper-parameters need addressing.

Method: A multitask learning framework with DHEN as the backbone, incorporating real-time user behavior sequences, off-site conversion event sequences, and a self-supervised auxiliary loss.

Result: The method achieves state-of-the-art performance in CVR prediction compared to single feature-crossing modules with pre-trained features.

Conclusion: The proposed framework effectively addresses CVR prediction challenges, leveraging DHEN and innovative techniques like self-supervised learning for improved results.

Abstract: The predictions of click through rate (CTR) and conversion rate (CVR) play a
crucial role in the success of ad-recommendation systems. A Deep Hierarchical
Ensemble Network (DHEN) has been proposed to integrate multiple feature
crossing modules and has achieved great success in CTR prediction. However, its
performance for CVR prediction is unclear in the conversion ads setting, where
an ad bids for the probability of a user's off-site actions on a third party
website or app, including purchase, add to cart, sign up, etc. A few challenges
in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a
few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve
the best trade-off between efficiency and efficacy? 3) What hyper-parameters to
choose in each feature-crossing module? Orthogonal to the model architecture,
the input personalization features also significantly impact model performance
with a high degree of freedom. In this paper, we attack this problem and
present our contributions biased to the applied data science side, including:
  First, we propose a multitask learning framework with DHEN as the single
backbone model architecture to predict all CVR tasks, with a detailed study on
how to make DHEN work effectively in practice; Second, we build both on-site
real-time user behavior sequences and off-site conversion event sequences for
CVR prediction purposes, and conduct ablation study on its importance; Last but
not least, we propose a self-supervised auxiliary loss to predict future
actions in the input sequence, to help resolve the label sparseness issue in
CVR prediction.
  Our method achieves state-of-the-art performance compared to previous single
feature crossing modules with pre-trained user personalization features.

</details>


### [269] [Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization](https://arxiv.org/abs/2504.05812)
*Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian*

Main category: cs.LG

TL;DR: EMPO is a fully unsupervised method for enhancing LLM reasoning without external supervision, achieving competitive performance on mathematical and natural reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on supervised fine-tuning and reinforcement learning, which depend on external supervision, limiting scalability and practicality. EMPO addresses this by enabling self-supervised reasoning enhancement.

Method: EMPO minimizes the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, eliminating the need for supervised signals like reasoning traces or golden answers.

Result: EMPO improves accuracy on mathematical benchmarks (30.7% to 48.1%) and MMLU-Pro (32.1% to 50.1%) without supervised signals.

Conclusion: EMPO offers a scalable and practical solution for unsupervised reasoning enhancement in LLMs, demonstrating strong performance.

Abstract: While large language models (LLMs) have demonstrated exceptional capabilities
in challenging tasks such as mathematical reasoning, existing methods to
enhance reasoning ability predominantly rely on supervised fine-tuning (SFT)
followed by reinforcement learning (RL) on reasoning-specific data after
pre-training. However, these approaches critically depend on external
supervision--such as human-labelled reasoning traces, verified golden answers,
or pre-trained reward models--which limits scalability and practical
applicability. In this work, we propose Entropy Minimized Policy Optimization
(EMPO), which makes an early attempt at fully unsupervised LLM reasoning
incentivization. EMPO does not require any supervised information for
incentivizing reasoning capabilities (i.e., neither verifiable reasoning
traces, problems with golden answers, nor additional pre-trained reward
models). By continuously minimizing the predictive entropy of LLMs on unlabeled
user queries in a latent semantic space, EMPO enables purely self-supervised
evolution of reasoning capabilities with strong flexibility and practicality.
Our experiments demonstrate competitive performance of EMPO on both
mathematical reasoning and free-form natural reasoning tasks. Specifically,
without any supervised signals, \ours boosts the accuracy of Qwen2.5-Math-7B
Base from 30.7\% to 48.1\% on mathematical benchmarks and improves the accuracy
of Qwen2.5-7B Base from 32.1\% to 50.1\% on MMLU-Pro.

</details>


### [270] [TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback](https://arxiv.org/abs/2504.12557)
*Siow Meng Low, Akshat Kumar*

Main category: cs.LG

TL;DR: The paper proposes a method to learn safety definitions in RL from sparse labeled data, using a safety model for credit assignment and deriving an algorithm for safe policy optimization.


<details>
  <summary>Details</summary>
Motivation: Safety constraints in RL are often unknown or hard to specify, requiring anticipation of all unsafe behaviors. The paper addresses learning safety from limited labeled data.

Method: Designs a safety model for credit assignment, learns step-wise safety scores, reformulates safe RL, and derives an optimization algorithm.

Result: Empirical results show the approach effectively satisfies unknown safety definitions and scales to continuous control tasks.

Conclusion: The proposed method successfully learns safety definitions and optimizes safe policies, demonstrating scalability and effectiveness.

Abstract: In safe reinforcement learning (RL), auxiliary safety costs are used to align
the agent to safe decision making. In practice, safety constraints, including
cost functions and budgets, are unknown or hard to specify, as it requires
anticipation of all possible unsafe behaviors. We therefore address a general
setting where the true safety definition is unknown, and has to be learned from
sparsely labeled data. Our key contributions are: first, we design a safety
model that performs credit assignment to estimate each decision step's impact
on the overall safety using a dataset of diverse trajectories and their
corresponding binary safety labels (i.e., whether the corresponding trajectory
is safe/unsafe). Second, we illustrate the architecture of our safety model to
demonstrate its ability to learn a separate safety score for each timestep.
Third, we reformulate the safe RL problem using the proposed safety model and
derive an effective algorithm to optimize a safe yet rewarding policy. Finally,
our empirical results corroborate our findings and show that this approach is
effective in satisfying unknown safety definition, and scalable to various
continuous control tasks.

</details>


### [271] [Modelling Mean-Field Games with Neural Ordinary Differential Equations](https://arxiv.org/abs/2504.13228)
*Anna C. M. ThÃ¶ni, Yoram Bachrach, Tal Kachman*

Main category: cs.LG

TL;DR: The paper combines mean-field game theory with deep learning (neural ODEs) to create a data-driven, lightweight model for solving complex games, reducing dependency on analytical methods.


<details>
  <summary>Details</summary>
Motivation: Analytical solutions for mean-field games can lose uniqueness, suffer bias, and are model-dependent. A data-driven, model-free approach is needed.

Method: Integrates mean-field game theory with neural ordinary differential equations (ODEs) using automatic differentiation for robustness.

Result: The model efficiently solves complex mean-field games, learns distributions with few observations, and handles noise and varying observability.

Conclusion: The proposed approach is flexible, lightweight, and outperforms traditional mean-field methods by leveraging deep learning.

Abstract: Mean-field game theory relies on approximating games that would otherwise
have been intractable to model. While the games can be solved analytically via
the associated system of partial derivatives, this approach is not model-free,
can lead to the loss of the existence or uniqueness of solutions and may suffer
from modelling bias. To reduce the dependency between the model and the game,
we combine mean-field game theory with deep learning in the form of neural
ordinary differential equations. The resulting model is data-driven,
lightweight and can learn extensive strategic interactions that are hard to
capture using mean-field theory alone. In addition, the model is based on
automatic differentiation, making it more robust and objective than approaches
based on finite differences. We highlight the efficiency and flexibility of our
approach by solving three mean-field games that vary in their complexity,
observability and the presence of noise. Using these results, we show that the
model is flexible, lightweight and requires few observations to learn the
distribution underlying the data.

</details>


### [272] [Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models](https://arxiv.org/abs/2504.13945)
*Zhanglin Wu, Tengfei Song, Ning Xie, Mengli Zhu, Weidong Zhang, Shuang Wu, Pengfei Li, Chong Li, Junhao Zhu, Hao Yang, Shiliang Sun*

Main category: cs.LG

TL;DR: The paper introduces MOTBench, a benchmark for evaluating large vision-language models (LVLMs) on menu translation tasks, addressing gaps in current evaluations by focusing on complex layouts and culturally specific elements.


<details>
  <summary>Details</summary>
Motivation: Current LVLM evaluations like OCRBench overlook the ability to understand long texts with complex layouts, which is crucial for applications like multilingual menu translation.

Method: The authors propose MOTBench, a framework using Chinese and English menus with intricate layouts and precise annotations to assess LVLMs' visual and language processing capabilities.

Result: Experiments show MOTBench's automatic evaluations align closely with human assessments, revealing strengths and weaknesses in state-of-the-art LVLMs.

Conclusion: MOTBench provides a specialized tool for evaluating LVLMs in complex layout understanding, guiding future improvements in the field.

Abstract: The rapid advancement of large vision-language models (LVLMs) has
significantly propelled applications in document understanding, particularly in
optical character recognition (OCR) and multilingual translation. However,
current evaluations of LVLMs, like the widely used OCRBench, mainly focus on
verifying the correctness of their short-text responses and long-text responses
with simple layout, while the evaluation of their ability to understand long
texts with complex layout design is highly significant but largely overlooked.
In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a
specialized evaluation framework emphasizing the pivotal role of menu
translation in cross-cultural communication. MOTBench requires LVLMs to
accurately recognize and translate each dish, along with its price and unit
items on a menu, providing a comprehensive assessment of their visual
understanding and language processing capabilities. Our benchmark is comprised
of a collection of Chinese and English menus, characterized by intricate
layouts, a variety of fonts, and culturally specific elements across different
languages, along with precise human annotations. Experiments show that our
automatic evaluation results are highly consistent with professional human
evaluation. We evaluate a range of publicly available state-of-the-art LVLMs,
and through analyzing their output to identify the strengths and weaknesses in
their performance, offering valuable insights to guide future advancements in
LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.

</details>


### [273] [CAOTE: KV Caching through Attention Output Error based Token Eviction](https://arxiv.org/abs/2504.14051)
*Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott*

Main category: cs.LG

TL;DR: CAOTE improves token eviction by using attention output contributions, enhancing accuracy in resource-restricted devices.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of attention scores as token importance metrics by incorporating token contributions to attention outputs.

Method: Proposes CAOTE, a token eviction criterion integrating attention scores and value vectors to minimize eviction error.

Result: CAOTE consistently improves downstream task accuracy when combined with attention score-based methods.

Conclusion: Leveraging value vector information in token eviction enhances performance, making CAOTE a versatile meta-heuristic method.

Abstract: While long context support of large language models has extended their
abilities, it also incurs challenges in memory and compute which becomes
crucial bottlenecks in resource-restricted devices. Token eviction, a widely
adopted post-training methodology designed to alleviate the bottlenecks by
evicting less important tokens from the cache, typically uses attention scores
as proxy metrics for token importance. However, one major limitation of
attention score as a token-wise importance metrics is that it lacks the
information about contribution of tokens to the attention output. In this
paper, we propose a simple eviction criterion based on the contribution of
cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction
error due to token eviction, by seamlessly integrating attention scores and
value vectors. This is the first method which uses value vector information on
top of attention-based eviction scores. Additionally, CAOTE can act as a
meta-heuristic method with flexible usage with any token eviction method. We
show that CAOTE, when combined with the state-of-the-art attention score-based
methods, always improves accuracies on the downstream task, indicating the
importance of leveraging information from values during token eviction process.

</details>


### [274] [MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core](https://arxiv.org/abs/2504.14960)
*Dennis Liu, Zijie Yan, Xin Yao, Tong Liu, Vijay Korthikanti, Evan Wu, Shiqing Fan, Gao Deng, Hongxiao Bai, Jianbin Chang, Ashwath Aithal, Michael Andersch, Mohammad Shoeybi, Jiajie Yao, Chandler Zhou, David Wu, Xipeng Li, June Yang*

Main category: cs.LG

TL;DR: The paper introduces a five-dimensional hybrid parallelism framework for efficient large-scale Mixture of Experts (MoE) model training, achieving high Model Flops Utilization (MFU) and scalability up to 1,024 GPUs.


<details>
  <summary>Details</summary>
Motivation: Efficient training of large-scale MoE models is challenging due to limitations in existing parallelism strategies.

Method: Proposes an end-to-end framework with five-dimensional hybrid parallelism (Tensor, Expert, Context, Data, Pipeline) and MoE Parallel Folding to optimize attention and MoE layer parallelization.

Result: Achieves up to 49.3% MFU for Mixtral 8x22B and 39.0% MFU for Qwen2-57B-A14B on H100 GPUs, scaling efficiently to 1,024 GPUs.

Conclusion: The framework is effective for large-scale MoE training, validated by high performance and scalability.

Abstract: Mixture of Experts (MoE) models enhance neural network scalability by
dynamically selecting relevant experts per input token, enabling larger model
sizes while maintaining manageable computation costs. However, efficient
training of large-scale MoE models across thousands of GPUs presents
significant challenges due to limitations in existing parallelism strategies.
We introduce an end-to-end training framework for large-scale MoE models that
utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert
Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.
Central to our approach is MoE Parallel Folding, a novel strategy that
decouples the parallelization of attention and MoE layers in Transformer
models, allowing each layer type to adopt optimal parallel configurations.
Additionally, we develop a flexible token-level dispatcher that supports both
token-dropping and token-dropless MoE training across all five dimensions of
parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates
different parallelism schemes for Attention and MoE layers, facilitating
complex parallelism implementations. Our experiments demonstrate significant
improvements in training efficiency and scalability. We achieve up to 49.3%
Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the
Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The
framework scales efficiently up to 1,024 GPUs and maintains high performance
with sequence lengths up to 128K tokens, validating its effectiveness for
large-scale MoE model training. The code is available in Megatron-Core.

</details>


### [275] [DAE-KAN: A Kolmogorov-Arnold Network Model for High-Index Differential-Algebraic Equations](https://arxiv.org/abs/2504.15806)
*Kai Luo, Juan Tang, Mingchao Cai, Xiaoqing Zeng, Manqi Xie, Ming Yan*

Main category: cs.LG

TL;DR: DAE-KAN, a novel framework combining KANs and PINNs, significantly improves accuracy in solving high-index DAEs, reducing errors by 1-2 orders of magnitude compared to traditional PINNs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional PINNs in solving high-index differential-algebraic equations (DAEs) by leveraging the superior function-fitting abilities of KANs.

Method: Integrates KANs with PINNs to create DAE-KAN, enhancing performance while preserving PINNs' ability to model complex systems governed by physical laws.

Result: DAE-KAN reduces absolute errors by 1-2 orders of magnitude for DAEs (index-1 to index-3) and outperforms classical methods in controlling drift-off error.

Conclusion: DAE-KAN shows promise for solving high-index DAEs with high accuracy and generalization, offering a viable solution for challenging PDEs.

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
Multi-layer Perceptrons (MLPs) due to their superior function-fitting abilities
in data-driven modeling. In this paper, we propose a novel framework, DAE-KAN,
for solving high-index differential-algebraic equations (DAEs) by integrating
KANs with Physics-Informed Neural Networks (PINNs). This framework not only
preserves the ability of traditional PINNs to model complex systems governed by
physical laws but also enhances their performance by leveraging the
function-fitting strengths of KANs. Numerical experiments demonstrate that for
DAE systems ranging from index-1 to index-3, DAE-KAN reduces the absolute
errors of both differential and algebraic variables by 1 to 2 orders of
magnitude compared to traditional PINNs. To assess the effectiveness of this
approach, we analyze the drift-off error and find that both PINNs and DAE-KAN
outperform classical numerical methods in controlling this phenomenon. Our
results highlight the potential of neural network methods, particularly
DAE-KAN, in solving high-index DAEs with substantial computational accuracy and
generalization, offering a promising solution for challenging partial
differential-algebraic equations.

</details>


### [276] [AlphaGrad: Non-Linear Gradient Normalization Optimizer](https://arxiv.org/abs/2504.16020)
*Soham Sane*

Main category: cs.LG

TL;DR: AlphaGrad is a memory-efficient optimizer with a single hyperparameter, outperforming Adam in some RL tasks but requiring careful tuning.


<details>
  <summary>Details</summary>
Motivation: Address memory overhead and hyperparameter complexity of adaptive methods like Adam.

Method: Uses tensor-wise L2 gradient normalization and a hyperbolic tangent transformation controlled by a steepness parameter.

Result: Performance varies by RL task: unstable in DQN, stable in TD3 with tuning, and superior in PPO.

Conclusion: AlphaGrad is a promising alternative for memory-constrained scenarios, especially in on-policy learning.

Abstract: We introduce AlphaGrad, a memory-efficient, conditionally stateless optimizer
addressing the memory overhead and hyperparameter complexity of adaptive
methods like Adam. AlphaGrad enforces scale invariance via tensor-wise L2
gradient normalization followed by a smooth hyperbolic tangent transformation,
$g' = \tanh(\alpha \cdot \tilde{g})$, controlled by a single steepness
parameter $\alpha$. Our contributions include: (1) the AlphaGrad algorithm
formulation; (2) a formal non-convex convergence analysis guaranteeing
stationarity; (3) extensive empirical evaluation on diverse RL benchmarks (DQN,
TD3, PPO). Compared to Adam, AlphaGrad demonstrates a highly context-dependent
performance profile. While exhibiting instability in off-policy DQN, it
provides enhanced training stability with competitive results in TD3 (requiring
careful $\alpha$ tuning) and achieves substantially superior performance in
on-policy PPO. These results underscore the critical importance of empirical
$\alpha$ selection, revealing strong interactions between the optimizer's
dynamics and the underlying RL algorithm. AlphaGrad presents a compelling
alternative optimizer for memory-constrained scenarios and shows significant
promise for on-policy learning regimes where its stability and efficiency
advantages can be particularly impactful.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [277] [MARFT: Multi-Agent Reinforcement Fine-Tuning](https://arxiv.org/abs/2504.16129)
*Junwei Liao, Muning Wen, Jun Wang, Weinan Zhang*

Main category: cs.MA

TL;DR: The paper introduces Multi-Agent Reinforcement Fine-Tuning (MARFT), a novel paradigm for fine-tuning LLM-based Multi-Agent Systems (LaMAS) using foundational RL techniques, addressing challenges unique to LaMAS.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on fine-tuning LaMAS with RL, and direct MARL application to LaMAS poses challenges due to its unique characteristics.

Method: The paper proposes a universal algorithmic framework for MARFT, detailing its conceptual foundations, distinctions from MARL, and practical implementation.

Result: A robust and scalable MARFT framework is presented, with an open-source implementation provided for adoption and further research.

Conclusion: The work bridges theory and practice, serving as a roadmap for advancing MARFT in agentic systems, with real-world applications and challenges discussed.

Abstract: LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in
addressing complex, agentic tasks requiring multifaceted reasoning and
collaboration, from generating high-quality presentation slides to conducting
sophisticated scientific research. Meanwhile, RL has been widely recognized for
its effectiveness in enhancing agent intelligence, but limited research has
investigated the fine-tuning of LaMAS using foundational RL techniques.
Moreover, the direct application of MARL methodologies to LaMAS introduces
significant challenges, stemming from the unique characteristics and mechanisms
inherent to LaMAS. To address these challenges, this article presents a
comprehensive study of LLM-based MARL and proposes a novel paradigm termed
Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal
algorithmic framework tailored for LaMAS, outlining the conceptual foundations,
key distinctions, and practical implementation strategies. We begin by
reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage
for a parallel analysis in the multi-agent domain. In the context of LaMAS, we
elucidate critical differences between MARL and MARFT. These differences
motivate a transition toward a novel, LaMAS-oriented formulation of RFT.
Central to this work is the presentation of a robust and scalable MARFT
framework. We detail the core algorithm and provide a complete, open-source
implementation to facilitate adoption and further research. The latter sections
of the paper explore real-world application perspectives and opening challenges
in MARFT. By bridging theoretical underpinnings with practical methodologies,
this work aims to serve as a roadmap for researchers seeking to advance MARFT
toward resilient and adaptive solutions in agentic systems. Our implementation
of the proposed framework is publicly available at:
https://github.com/jwliao-ai/MARFT.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [278] [EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment](https://arxiv.org/abs/2504.16405)
*Lancheng Gao, Ziheng Jia, Yunhao Zeng, Wei Sun, Yiming Zhang, Wei Zhou, Guangtao Zhai, Xiongkuo Min*

Main category: cs.MM

TL;DR: EEmo-Bench is a benchmark for evaluating MLLMs' ability to understand image-evoked emotions, using diverse tasks and emotional attributes (VAD). It highlights gaps in current models' capabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of MLLMs' emotion understanding are coarse-grained. EEmo-Bench aims to provide a systematic and comprehensive assessment.

Method: The benchmark uses 1,960 annotated images and VAD emotional attributes. It includes four tasks (Perception, Ranking, Description, Assessment) and image-pairwise analysis, totaling 6,773 QA pairs.

Result: Some MLLMs perform well overall, but analytical capabilities in certain dimensions are lacking.

Conclusion: EEmo-Bench advances research on MLLMs' emotion perception, crucial for applications like human-machine interaction.

Abstract: The furnishing of multi-modal large language models (MLLMs) has led to the
emergence of numerous benchmark studies, particularly those evaluating their
perception and understanding capabilities.
  Among these, understanding image-evoked emotions aims to enhance MLLMs'
empathy, with significant applications such as human-machine interaction and
advertising recommendations. However, current evaluations of this MLLM
capability remain coarse-grained, and a systematic and comprehensive assessment
is still lacking.
  To this end, we introduce EEmo-Bench, a novel benchmark dedicated to the
analysis of the evoked emotions in images across diverse content categories.
  Our core contributions include:
  1) Regarding the diversity of the evoked emotions, we adopt an emotion
ranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional
attributes for emotional assessment. In line with this methodology, 1,960
images are collected and manually annotated.
  2) We design four tasks to evaluate MLLMs' ability to capture the evoked
emotions by single images and their associated attributes: Perception, Ranking,
Description, and Assessment. Additionally, image-pairwise analysis is
introduced to investigate the model's proficiency in performing joint and
comparative analysis.
  In total, we collect 6,773 question-answer pairs and perform a thorough
assessment on 19 commonly-used MLLMs.
  The results indicate that while some proprietary and large-scale open-source
MLLMs achieve promising overall performance, the analytical capabilities in
certain evaluation dimensions remain suboptimal.
  Our EEmo-Bench paves the path for further research aimed at enhancing the
comprehensive perceiving and understanding capabilities of MLLMs concerning
image-evoked emotions, which is crucial for machine-centric emotion perception
and understanding.

</details>


### [279] [Learning Switchable Priors for Neural Image Compression](https://arxiv.org/abs/2504.16586)
*Haotian Zhang, Yuqi Li, Li Li, Dong Liu*

Main category: cs.MM

TL;DR: The paper proposes FastNIC, a lightweight neural image compression model that decouples entropy model complexity from prior distributions by using a finite set of trainable priors and a skip mode.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between complex probabilistic distributions for latent variables and the high complexity of entropy models in neural image compression.

Method: Uses a finite set of trainable priors and trains the entropy model to predict the index of the appropriate prior, enabling a skip mode for latent variables.

Result: FastNIC achieves a better trade-off between compression efficiency and computational complexity, and improves state-of-the-art models with reduced entropy coding complexity.

Conclusion: The proposed method effectively balances compression performance and computational efficiency, demonstrating practical value in neural image compression.

Abstract: Neural image compression (NIC) usually adopts a predefined family of
probabilistic distributions as the prior of the latent variables, and meanwhile
relies on entropy models to estimate the parameters for the probabilistic
family. More complex probabilistic distributions may fit the latent variables
more accurately, but also incur higher complexity of the entropy models,
limiting their practical value. To address this dilemma, we propose a solution
to decouple the entropy model complexity from the prior distributions. We use a
finite set of trainable priors that correspond to samples of the parametric
probabilistic distributions. We train the entropy model to predict the index of
the appropriate prior within the set, rather than the specific parameters.
Switching between the trained priors further enables us to embrace a skip mode
into the prior set, which simply omits a latent variable during the entropy
coding. To demonstrate the practical value of our solution, we present a
lightweight NIC model, namely FastNIC, together with the learning of switchable
priors. FastNIC obtains a better trade-off between compression efficiency and
computational complexity for neural image compression. We also implanted the
switchable priors into state-of-the-art NIC models and observed improved
compression efficiency with a significant reduction of entropy coding
complexity.

</details>


### [280] [Rethinking Vision Transformer for Large-Scale Fine-Grained Image Retrieval](https://arxiv.org/abs/2504.16691)
*Xin Jiang, Hao Tang, Yonghua Pan, Zechao Li*

Main category: cs.MM

TL;DR: The paper proposes EET, an efficient ViT-based framework for large-scale fine-grained image retrieval, combining token pruning and discriminative transfer to improve scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based methods for FGIR focus on self-attention but suffer from high computational complexity and redundant dependencies, limiting scalability.

Method: EET integrates token pruning (removing low-discriminative tokens) and discriminative transfer (knowledge distillation and region guidance) to enhance efficiency and effectiveness.

Result: EET reduces ViT-Small inference latency by 42.7% and improves retrieval performance by 5.15% on NABirds.

Conclusion: EET addresses efficiency and scalability issues in ViT-based FGIR, achieving significant improvements in speed and accuracy.

Abstract: Large-scale fine-grained image retrieval (FGIR) aims to retrieve images
belonging to the same subcategory as a given query by capturing subtle
differences in a large-scale setting. Recently, Vision Transformers (ViT) have
been employed in FGIR due to their powerful self-attention mechanism for
modeling long-range dependencies. However, most Transformer-based methods focus
primarily on leveraging self-attention to distinguish fine-grained details,
while overlooking the high computational complexity and redundant dependencies
inherent to these models, limiting their scalability and effectiveness in
large-scale FGIR. In this paper, we propose an Efficient and Effective
ViT-based framework, termed \textbf{EET}, which integrates token pruning module
with a discriminative transfer strategy to address these limitations.
Specifically, we introduce a content-based token pruning scheme to enhance the
efficiency of the vanilla ViT, progressively removing background or
low-discriminative tokens at different stages by exploiting feature responses
and self-attention mechanism. To ensure the resulting efficient ViT retains
strong discriminative power, we further present a discriminative transfer
strategy comprising both \textit{discriminative knowledge transfer} and
\textit{discriminative region guidance}. Using a distillation paradigm, these
components transfer knowledge from a larger ``teacher'' ViT to a more efficient
``student'' model, guiding the latter to focus on subtle yet crucial regions in
a cost-free manner. Extensive experiments on two widely-used fine-grained
datasets and four large-scale fine-grained datasets demonstrate the
effectiveness of our method. Specifically, EET reduces the inference latency of
ViT-Small by 42.7\% and boosts the retrieval performance of 16-bit hash codes
by 5.15\% on the challenging NABirds dataset.

</details>


### [281] [4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis](https://arxiv.org/abs/2504.16798)
*Yuxiang Wei, Yanteng Zhang, Xi Xiao, Tianyang Wang, Xiao Wang, Vince D. Calhoun*

Main category: cs.MM

TL;DR: M2M-AlignNet is a geometry-aware multimodal co-attention network designed for early Alzheimer's disease (AD) diagnosis by integrating sMRI and fMRI data, addressing challenges in feature fusion across modalities.


<details>
  <summary>Details</summary>
Motivation: The intrinsic heterogeneity of multimodal neuroimaging data (e.g., sMRI and fMRI) poses challenges for discriminative feature fusion, limiting diagnostic sensitivity for AD.

Method: The proposed M2M-AlignNet uses a multi-patch-to-multi-patch contrastive loss for geometry-weighted patch correspondence and a latent-as-query co-attention module to autonomously discover fusion patterns.

Result: Extensive experiments confirm the method's effectiveness and highlight the correspondence between fMRI and sMRI as AD biomarkers.

Conclusion: M2M-AlignNet successfully bridges the gap in multimodal neuroimaging fusion, enhancing early AD diagnosis through explicit alignment and unbiased feature fusion.

Abstract: Multimodal neuroimaging provides complementary structural and functional
insights into both human brain organization and disease-related dynamics.
Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's
disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI,
fMRI) with behavioral cognitive scores tabular data biomarkers. However, the
intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI
dynamics vs. 3D anatomical sMRI structure) presents critical challenges for
discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a
geometry-aware multimodal co-attention network with latent alignment for early
AD diagnosis using sMRI and fMRI. At the core of our approach is a
multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and
reduces representational discrepancies via geometry-weighted patch
correspondence, explicitly aligning fMRI components across brain regions with
their sMRI structural substrates without one-to-one constraints. Additionally,
we propose a latent-as-query co-attention module to autonomously discover
fusion patterns, circumventing modality prioritization biases while minimizing
feature redundancy. We conduct extensive experiments to confirm the
effectiveness of our method and highlight the correspondance between fMRI and
sMRI as AD biomarkers.

</details>


### [282] [AudioX: Diffusion Transformer for Anything-to-Audio Generation](https://arxiv.org/abs/2503.10522)
*Zeyue Tian, Yizhu Jin, Zhaoyang Liu, Ruibin Yuan, Xu Tan, Qifeng Chen, Wei Xue, Yike Guo*

Main category: cs.MM

TL;DR: AudioX is a unified Diffusion Transformer model for Anything-to-Audio and Music Generation, addressing limitations of isolated approaches, data scarcity, and cross-modal integration.


<details>
  <summary>Details</summary>
Motivation: Existing audio and music generation methods lack unified capabilities, suffer from data scarcity, and struggle with diverse inputs.

Method: AudioX uses a multi-modal masked training strategy and is trained on curated datasets (vggsound-caps and V2M-caps).

Result: AudioX matches or outperforms specialized models and handles diverse inputs and tasks in a unified architecture.

Conclusion: AudioX offers high-quality, versatile generation with natural language control and cross-modal processing.

Abstract: Audio and music generation have emerged as crucial tasks in many
applications, yet existing approaches face significant limitations: they
operate in isolation without unified capabilities across modalities, suffer
from scarce high-quality, multi-modal training data, and struggle to
effectively integrate diverse inputs. In this work, we propose AudioX, a
unified Diffusion Transformer model for Anything-to-Audio and Music Generation.
Unlike previous domain-specific models, AudioX can generate both general audio
and music with high quality, while offering flexible natural language control
and seamless processing of various modalities including text, video, image,
music, and audio. Its key innovation is a multi-modal masked training strategy
that masks inputs across modalities and forces the model to learn from masked
inputs, yielding robust and unified cross-modal representations. To address
data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K
audio captions based on the VGGSound dataset, and V2M-caps with 6 million music
captions derived from the V2M dataset. Extensive experiments demonstrate that
AudioX not only matches or outperforms state-of-the-art specialized models, but
also offers remarkable versatility in handling diverse input modalities and
generation tasks within a unified architecture. The code and datasets will be
available at https://zeyuet.github.io/AudioX/

</details>


### [283] [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
*Weize Quan, Yunfei Feng, Ming Zhou, Yunzhen Zhao, Tong Wang, Dong-Ming Yan*

Main category: cs.MM

TL;DR: The paper introduces TCAN, a Text-oriented Cross-Attention Network, to address multimodal heterogeneity in sentiment analysis by emphasizing the text modality and using cross-attention and gated mechanisms.


<details>
  <summary>Details</summary>
Motivation: Previous MSA approaches treated modalities uniformly, ignoring varying semantic richness, leading to imbalanced emphasis on weak and strong modalities.

Method: TCAN uses unaligned sequences of text, visual, and acoustic inputs, applies self-attention to text, and cross-attention to visual and acoustic modalities with a gated control mechanism. Unimodal joint learning is also incorporated.

Result: TCAN outperforms state-of-the-art MSA methods on CMU-MOSI and CMU-MOSEI datasets.

Conclusion: Emphasizing the text modality and using cross-attention with noise control improves MSA performance, demonstrating TCAN's effectiveness.

Abstract: Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment
by leveraging language, visual, and acoustic modalities. Despite the remarkable
performance exhibited by previous MSA approaches, the presence of inherent
multimodal heterogeneities poses a challenge, with the contribution of
different modalities varying considerably. Past research predominantly focused
on improving representation learning techniques and feature fusion strategies.
However, many of these efforts overlooked the variation in semantic richness
among different modalities, treating each modality uniformly. This approach may
lead to underestimating the significance of strong modalities while
overemphasizing the importance of weak ones. Motivated by these insights, we
introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the
predominant role of the text modality in MSA. Specifically, for each multimodal
sample, by taking unaligned sequences of the three modalities as inputs, we
initially allocate the extracted unimodal features into a visual-text and an
acoustic-text pair. Subsequently, we implement self-attention on the text
modality and apply text-queried cross-attention to the visual and acoustic
modalities. To mitigate the influence of noise signals and redundant features,
we incorporate a gated control mechanism into the framework. Additionally, we
introduce unimodal joint learning to gain a deeper understanding of homogeneous
emotional tendencies across diverse modalities through backpropagation.
Experimental results demonstrate that TCAN consistently outperforms
state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).

</details>


### [284] [MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks](https://arxiv.org/abs/2405.07229)
*Xiaocui Yang, Wenfang Wu, Shi Feng, Ming Wang, Daling Wang, Yang Li, Qi Sun, Yifei Zhang, Xiaoming Fu, Soujanya Poria*

Main category: cs.MM

TL;DR: MM-InstructEval is a framework for evaluating multimodal reasoning in large language models, introducing innovative metrics and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations focus on unimodal tasks, neglecting the need for assessing complex multimodal reasoning.

Method: The framework uses diverse metrics to evaluate 45 models across 16 datasets and 6 tasks with 10 instructions.

Result: Key insights on model architectures and instruction formats were uncovered, setting new benchmarks.

Conclusion: The framework provides strategic guidance for future developments and is publicly available for continued research.

Abstract: The emergence of multimodal large language models (MLLMs) has triggered
extensive research in model evaluation. While existing evaluation studies
primarily focus on unimodal (vision-only) comprehension and reasoning
capabilities, they overlook critical assessments of complex multimodal
reasoning tasks that require integrated understanding of both visual and
textual contexts. Such multimodal tasks present unique challenges, demanding
sophisticated reasoning across multiple modalities and deep comprehension of
multimodal contexts. In this paper, we present MM-InstructEval, a comprehensive
evaluation framework that incorporates diverse metrics to assess model
performance across various multimodal reasoning tasks with vision-text
contexts. We conduct extensive zero-shot evaluations on 45 models (including 36
MLLMs) across 16 multimodal datasets, encompassing 6 distinct tasks using 10
different instructions. Our framework introduces multiple innovative metrics,
including the 'Best Performance' metric to benchmark peak model capabilities,
the 'Mean Relative Gain' metric to assess overall efficacy across models and
instructions, the 'Stability' metric to measure robustness, and the
'Adaptability' metric to quantify the compatibility between models and
instructions. Through comprehensive evaluation and analysis, we uncover several
significant insights about model architectures, instruction formats, and their
interactions in multimodal reasoning tasks. Our findings establish new
benchmarks for assessing the reasoning capabilities of MLLMs and provide
strategic guidance for future developments. To facilitate continued research
and evaluation in this field, we release our framework and resources at
https://github.com/declare-lab/MM-InstructEval, with an interactive leaderboard
available at MM-InstructEval Leaderboard
(https://declare-lab.github.io/MM-InstructEval/).

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [285] [Perceptual Audio Coding: A 40-Year Historical Perspective](https://arxiv.org/abs/2504.16223)
*JÃ¼rgen Herre, Schuyler Quackenbush, Minje Kim, Jan Skoglund*

Main category: eess.AS

TL;DR: A historical overview of perceptual audio coding's evolution, from early formats like mp3 to modern integrated systems, with insights into future directions.


<details>
  <summary>Details</summary>
Motivation: To document the significant advancements in perceptual audio coding, highlighting its widespread adoption and technological progress.

Method: The paper reviews key milestones and pivotal developments in the field of perceptual audio coding.

Result: Traces the journey from basic perceptually driven coders to sophisticated integrated systems, emphasizing its success in digital media.

Conclusion: The paper concludes by discussing potential future advancements in perceptual audio coding.

Abstract: In the history of audio and acoustic signal processing, perceptual audio
coding has certainly excelled as a bright success story by its ubiquitous
deployment in virtually all digital media devices, such as computers, tablets,
mobile phones, set-top-boxes, and digital radios. From a technology
perspective, perceptual audio coding has undergone tremendous development from
the first very basic perceptually driven coders (including the popular mp3
format) to today's full-blown integrated coding/rendering systems. This paper
provides a historical overview of this research journey by pinpointing the
pivotal development steps in the evolution of perceptual audio coding. Finally,
it provides thoughts about future directions in this area.

</details>


### [286] [Deep, data-driven modeling of room acoustics: literature review and research perspectives](https://arxiv.org/abs/2504.16289)
*Toon van Waterschoot*

Main category: eess.AS

TL;DR: The paper reviews deep learning (DL)-based models for room acoustics, comparing them with traditional methods and identifying challenges for future research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of intrinsic space-time structure in current DL-based room acoustics models and explore their potential for sound field reconstruction.

Method: The paper conducts a structured literature review of DL-based room acoustics models, positioning them within a framework for comparison with traditional physical and data-driven models.

Result: DL-based models incorporating geometric or wave-based information show promise, particularly for sound field reconstruction, but gaps remain in capturing acoustic wave propagation's intrinsic structure.

Conclusion: The paper highlights the strengths and limitations of DL-based models in room acoustics and outlines key challenges for advancing research in this field.

Abstract: Our everyday auditory experience is shaped by the acoustics of the indoor
environments in which we live. Room acoustics modeling is aimed at establishing
mathematical representations of acoustic wave propagation in such environments.
These representations are relevant to a variety of problems ranging from
echo-aided auditory indoor navigation to restoring speech understanding in
cocktail party scenarios. Many disciplines in science and engineering have
recently witnessed a paradigm shift powered by deep learning (DL), and room
acoustics research is no exception. The majority of deep, data-driven room
acoustics models are inspired by DL-based speech and image processing, and
hence lack the intrinsic space-time structure of acoustic wave propagation.
More recently, DL-based models for room acoustics that include either geometric
or wave-based information have delivered promising results, primarily for the
problem of sound field reconstruction. In this review paper, we will provide an
extensive and structured literature review on deep, data-driven modeling in
room acoustics. Moreover, we position these models in a framework that allows
for a conceptual comparison with traditional physical and data-driven models.
Finally, we identify strengths and shortcomings of deep, data-driven room
acoustics models and outline the main challenges for further research.

</details>


### [287] [SoCov: Semi-Orthogonal Parametric Pooling of Covariance Matrix for Speaker Recognition](https://arxiv.org/abs/2504.16441)
*Rongjin Li, Weibin Zhang, Dongpeng Chen, Jintao Kang, Xiaofen Xing*

Main category: eess.AS

TL;DR: The paper introduces SoCov pooling, a method to improve speaker embedding by incorporating covariance information, outperforming traditional x-vector systems.


<details>
  <summary>Details</summary>
Motivation: Traditional pooling methods discard covariance information and treat frame-level features equally, limiting performance.

Method: SoCov pooling computes a covariance matrix from self-attentive features, compresses it into a vector, and combines it with weighted standard deviation.

Result: The sc-vector system reduces EER by 15.5% on SRE21Eval compared to x-vector and by 30.9% compared to conventional statistics pooling.

Conclusion: SoCov pooling effectively enhances speaker embedding by leveraging covariance information, achieving significant performance gains.

Abstract: In conventional deep speaker embedding frameworks, the pooling layer
aggregates all frame-level features over time and computes their mean and
standard deviation statistics as inputs to subsequent segment-level layers.
Such statistics pooling strategy produces fixed-length representations from
variable-length speech segments. However, this method treats different
frame-level features equally and discards covariance information. In this
paper, we propose the Semi-orthogonal parameter pooling of Covariance matrix
(SoCov) method. The SoCov pooling computes the covariance matrix from the
self-attentive frame-level features and compresses it into a vector using the
semi-orthogonal parametric vectorization, which is then concatenated with the
weighted standard deviation vector to form inputs to the segment-level layers.
Deep embedding based on SoCov is called ``sc-vector''. The proposed sc-vector
is compared to several different baselines on the SRE21 development and
evaluation sets. The sc-vector system significantly outperforms the
conventional x-vector system, with a relative reduction in EER of 15.5% on
SRE21Eval. When using self-attentive deep feature, SoCov helps to reduce EER on
SRE21Eval by about 30.9% relatively to the conventional ``mean + standard
deviation'' statistics.

</details>


### [288] [How Cyclic Acoustic Patterns Influence ASMR Perception: A Signal Processing Perspective](https://arxiv.org/abs/2504.00621)
*Zexin Fang, Bin Han, Henrik H. Sveen, C. Clark Cao, Hans D. Schotten*

Main category: eess.AS

TL;DR: The study explores how cyclic acoustic patterns influence ASMR triggers, finding that smooth, energy-dense patterns are most effective, with relaxation effects accumulating over time.


<details>
  <summary>Details</summary>
Motivation: To validate the link between cyclic patterns and ASMR triggers and understand the impact of acoustic features on ASMR effects.

Method: Designed three cyclic patterns with monophonic/stereophonic variations, collected ASMR scores via surveys, and performed regression analysis on cyclic features.

Result: Relaxation effects accumulate progressively, are spatially independent, and cyclic patterns significantly impact psychological/physical effects. Smooth, energy-dense patterns best trigger ASMR.

Conclusion: Cyclic patterns, especially smooth and energy-dense ones, are key to effective ASMR triggers, with effects that persist over time.

Abstract: Autonomous Sensory Meridian Response (ASMR) has been remarkably popular in
the recent decade. While its effect has been validated through behavioral
studies and neuro-physiological measurements such as electroencephalography
(EEG) and related bio-signal analyses, its development and triggers remain a
subject of debate. Previous studies suggest that its triggers are highly linked
with cyclic patterns: predictable patterns introduce relaxation while
variations maintain intrigue. To validate this and further understand the
impact of acoustic features on ASMR effects, we designed three distinct cyclic
patterns with monophonic and stereophonic variations, while controlling their
predictability and randomness, and collected ASMR triggering scores through
online surveys. Then, we extracted cyclic features and carried out regression
analysis, seeking an explainable mapping of cyclic features and ASMR triggers.
We found that relaxing effects accumulate progressively and are independent of
spatial orientation. Cyclic patterns significantly influence psychological and
physical effects, which remain invariant with time. Regression analysis
revealed that smoothly spread and energy-dense cyclic patterns most effectively
trigger ASMR responses.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [289] [Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images](https://arxiv.org/abs/2504.16237)
*Obed Korshie Dzikunu, Amirhossein Toosi, Shadab Ahamed, Sara Harsini, Francois Benard, Xiaoxiao Li, Arman Rahmim*

Main category: eess.IV

TL;DR: The study evaluates deep-learning-based segmentation methods for prostate cancer PET/CT scans, proposing a new loss function (L1DFL) that outperforms traditional metrics.


<details>
  <summary>Details</summary>
Motivation: To improve quantitative measurements in prostate cancer PET/CT scans beyond traditional Dice Similarity Coefficient assessments.

Method: Analyzed 380 PSMA PET/CT scans using U-Net, Attention U-Net, and SegResNet with four loss functions, including the proposed L1DFL.

Result: Attention U-Net with L1DFL achieved the highest correlation with ground truth (0.90-0.99 for SUVmax and TLA), outperforming other methods.

Conclusion: L1DFL minimizes variability in clinical measures, making it superior for quantitative PET/CT analysis.

Abstract: This study performs a comprehensive evaluation of quantitative measurements
as extracted from automated deep-learning-based segmentation methods, beyond
traditional Dice Similarity Coefficient assessments, focusing on six
quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA),
tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380
prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of
patients with biochemical recurrence of prostate cancer, training deep neural
networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice
Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice
Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with
L1DFL achieved the strongest correlation with the ground truth (concordance
correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice
Loss and the other two compound losses, particularly with SegResNet,
underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed
high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the
best performance. By contrast, tumor volume and lesion spread exhibited greater
variability. Bland-Altman, Coverage Probability, and Total Deviation Index
analyses further highlighted that our proposed L1DFL minimizes variability in
quantification of the ground truth clinical measures. The code is publicly
available at: https://github.com/ObedDzik/pca\_segment.git.

</details>


### [290] [Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction](https://arxiv.org/abs/2504.16745)
*Jialiang Zhang, Feng Gao, Yanhai Gan, Junyu Dong, Qian Du*

Main category: eess.IV

TL;DR: A Frequency-Compensated Network (FCNet) is proposed for Arctic sea ice concentration (SIC) prediction, addressing long-term feature dependencies and high-frequency detail preservation through dual-branch frequency and convolutional feature extraction.


<details>
  <summary>Details</summary>
Motivation: Accurate SIC forecasting is vital for ecosystem health and navigation safety, but current methods struggle with long-term dependencies and high-frequency detail preservation.

Method: FCNet uses a dual-branch network: one for frequency feature extraction with adaptive filters and another for convolutional feature extraction with high-frequency enhancement and temporal attention.

Result: Experiments on satellite-derived SIC data confirm FCNet's effectiveness in refining edge and detail predictions.

Conclusion: FCNet improves SIC prediction by integrating frequency and spatial features, with code and data made publicly available.

Abstract: Accurately forecasting sea ice concentration (SIC) in the Arctic is critical
to global ecosystem health and navigation safety. However, current methods
still is confronted with two challenges: 1) these methods rarely explore the
long-term feature dependencies in the frequency domain. 2) they can hardly
preserve the high-frequency details, and the changes in the marginal area of
the sea ice cannot be accurately captured. To this end, we present a
Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily
basis. In particular, we design a dual-branch network, including branches for
frequency feature extraction and convolutional feature extraction. For
frequency feature extraction, we design an adaptive frequency filter block,
which integrates trainable layers with Fourier-based filters. By adding
frequency features, the FCNet can achieve refined prediction of edges and
details. For convolutional feature extraction, we propose a high-frequency
enhancement block to separate high and low-frequency information. Moreover,
high-frequency features are enhanced via channel-wise attention, and temporal
attention unit is employed for low-frequency feature extraction to capture
long-range sea ice changes. Extensive experiments are conducted on a
satellite-derived daily SIC dataset, and the results verify the effectiveness
of the proposed FCNet. Our codes and data will be made public available at:
https://github.com/oucailab/FCNet .

</details>


### [291] [Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism](https://arxiv.org/abs/2504.16774)
*Lakshita Agarwal, Bindu Verma*

Main category: eess.IV

TL;DR: A new model combining Vision Transformer (ViT) and GPT-4 for generating chest X-ray descriptions achieves high accuracy on NIH and IU datasets.


<details>
  <summary>Details</summary>
Motivation: Improving the accuracy and richness of chest X-ray image descriptions to aid radiologists in diagnosis.

Method: Integrates ViT for visual feature extraction and GPT-4 for caption generation, using cross-modal attention to fuse visual and text data.

Result: Achieved high scores on IU (B-1: 0.854, CIDEr: 0.883) and NIH (BLEU-1: 0.825, CIDEr: 0.857) datasets.

Conclusion: The model enhances chest X-ray evaluation, supporting more precise and efficient diagnoses.

Abstract: The examination of chest X-ray images is a crucial component in detecting
various thoracic illnesses. This study introduces a new image description
generation model that integrates a Vision Transformer (ViT) encoder with
cross-modal attention and a GPT-4-based transformer decoder. The ViT captures
high-quality visual features from chest X-rays, which are fused with text data
through cross-modal attention to improve the accuracy, context, and richness of
image descriptions. The GPT-4 decoder transforms these fused features into
accurate and relevant captions. The model was tested on the National Institutes
of Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU
dataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and
0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all
metrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726),
and ROUGE-L (0.705). This framework has the potential to enhance chest X-ray
evaluation, assisting radiologists in more precise and efficient diagnosis.

</details>


### [292] [Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation](https://arxiv.org/abs/2302.06352)
*Francesco Santini, Jakob Wasserthal, Abramo Agosti, Xeni Deligianni, Kevin R. Keene, Hermien E. Kan, Stefan Sommer, Fengdan Wang, Claudia Weidensteiner, Giulia Manco, Matteo Paoletti, Valentina Mazzoli, Arjun Desai, Anna Pichiecchio*

Main category: eess.IV

TL;DR: Dafne is a decentralized deep learning system for radiological image segmentation, showing improved accuracy over time through federated incremental learning.


<details>
  <summary>Details</summary>
Motivation: To create a freely available, collaborative system for semantic segmentation of radiological images, enhancing accuracy and generalizability.

Method: Dafne uses a client-server architecture for federated incremental learning, evaluated on 38 MRI datasets and 639 real-world use-cases.

Result: Statistically significant accuracy improvement (Dice Similarity Coefficient increase of 0.007/generation, p < 0.001) and enhanced performance on diverse image types.

Conclusion: Dafne improves segmentation quality over time, demonstrating potential for learning and generalization in radiological imaging.

Abstract: Purpose: To present and evaluate Dafne (deep anatomical federated network), a
freely available decentralized, collaborative deep learning system for the
semantic segmentation of radiological images through federated incremental
learning. Materials and Methods: Dafne is free software with a client-server
architecture. The client side is an advanced user interface that applies the
deep learning models stored on the server to the user's data and allows the
user to check and refine the prediction. Incremental learning is then performed
at the client's side and sent back to the server, where it is integrated into
the root model. Dafne was evaluated locally, by assessing the performance gain
across model generations on 38 MRI datasets of the lower legs, and through the
analysis of real-world usage statistics (n = 639 use-cases). Results: Dafne
demonstrated a statistically improvement in the accuracy of semantic
segmentation over time (average increase of the Dice Similarity Coefficient by
0.007 points/generation on the local validation set, p < 0.001). Qualitatively,
the models showed enhanced performance on various radiologic image types,
including those not present in the initial training sets, indicating good model
generalizability. Conclusion: Dafne showed improvement in segmentation quality
over time, demonstrating potential for learning and generalization.

</details>


### [293] [A Deep Learning System for Rapid and Accurate Warning of Acute Aortic Syndrome on Non-contrast CT in China](https://arxiv.org/abs/2406.15222)
*Yujian Hu, Yilang Xiang, Yan-Jie Zhou, Yangyan He, Dehai Lang, Shifeng Yang, Xiaolong Du, Chunlan Den, Youyao Xu, Gaofeng Wang, Zhengyao Ding, Jingyong Huang, Wenjun Zhao, Xuejun Wu, Donglin Li, Qianqian Zhu, Zhenjiang Li, Chenyang Qiu, Ziheng Wu, Yunjun He, Chen Tian, Yihui Qiu, Zuodong Lin, Xiaolong Zhang, Yuan He, Zhenpeng Yuan, Xiaoxiang Zhou, Rong Fan, Ruihan Chen, Wenchao Guo, Jianpeng Zhang, Tony C. W. Mok, Zi Li, Mannudeep K. Kalra, Le Lu, Wenbo Xiao, Xiaoqiang Li, Yun Bian, Chengwei Shao, Guofu Wang, Wei Lu, Zhengxing Huang, Minfeng Xu, Hongkun Zhang*

Main category: eess.IV

TL;DR: An AI-based system, iAorta, uses non-contrast CT to accurately identify acute aortic syndromes (AAS) in China, improving diagnosis speed and reducing missed cases.


<details>
  <summary>Details</summary>
Motivation: The challenge of diagnosing AAS in acute chest pain patients, especially where non-contrast CT is the initial test due to economic constraints, drives the need for an efficient AI solution.

Method: iAorta was developed and tested through multi-center retrospective, large-scale real-world, and prospective comparative studies, evaluating its accuracy and impact on diagnostic time.

Result: iAorta achieved high AUC (0.958), sensitivity (0.913-0.942), and specificity (0.991-0.993), significantly reducing diagnostic time and correctly identifying AAS cases.

Conclusion: iAorta is a highly accurate and practical tool for AAS diagnosis in resource-constrained settings, improving patient outcomes.

Abstract: The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients
presenting with acute chest pain remains a clinical challenge. Aortic CT
angiography (CTA) is the imaging protocol of choice in patients with suspected
AAS. However, due to economic and workflow constraints in China, the majority
of suspected patients initially undergo non-contrast CT as the initial imaging
testing, and CTA is reserved for those at higher risk. In this work, we present
an artificial intelligence-based warning system, iAorta, using non-contrast CT
for AAS identification in China, which demonstrates remarkably high accuracy
and provides clinicians with interpretable warnings. iAorta was evaluated
through a comprehensive step-wise study. In the multi-center retrospective
study (n = 20,750), iAorta achieved a mean area under the receiver operating
curve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study
(n = 137,525), iAorta demonstrated consistently high performance across various
non-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a
specificity of 0.991-0.993. In the prospective comparative study (n = 13,846),
iAorta demonstrated the capability to significantly shorten the time to correct
diagnostic pathway. For the prospective pilot deployment that we conducted,
iAorta correctly identified 21 out of 22 patients with AAS among 15,584
consecutive patients presenting with acute chest pain and under non-contrast CT
protocol in the emergency department (ED) and enabled the average diagnostic
time of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the
iAorta can help avoid delayed or missed diagnosis of AAS in settings where
non-contrast CT remains the unavoidable the initial or only imaging test in
resource-constrained regions and in patients who cannot or did not receive
intravenous contrast.

</details>


### [294] [A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks](https://arxiv.org/abs/2411.10843)
*Santhosh Malarvannan, Pandiyaraju V, Shravan Venkatraman, Abeshek A, Priyadarshini B, Kannan A*

Main category: eess.IV

TL;DR: The paper proposes an Adaptive Hybrid Focal-Entropy Loss (AHFE) to address class imbalance in diabetic retinopathy diagnosis, improving model performance on minority classes.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in diabetic retinopathy datasets leads to biased AI models, necessitating a better loss function for minority classes.

Method: Combines focal loss and entropy loss with adaptive weighting to focus on minority and challenging samples.

Result: Achieved high accuracy with state-of-the-art models (e.g., ResNet50 at 99.79%).

Conclusion: AHFE enhances AI-driven diagnostics for imbalanced medical datasets.

Abstract: Diabetic retinopathy is a leading cause of blindness around the world and
demands precise AI-based diagnostic tools. Traditional loss functions in
multi-class classification, such as Categorical Cross-Entropy (CCE), are very
common but break down with class imbalance, especially in cases with inherently
challenging or overlapping classes, which leads to biased and less sensitive
models. Since a heavy imbalance exists in the number of examples for higher
severity stage 4 diabetic retinopathy, etc., classes compared to those very
early stages like class 0, achieving class balance is key. For this purpose, we
propose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of
focal loss and entropy loss with adaptive weighting in order to focus on
minority classes and highlight the challenging samples. The state-of-the art
models applied for diabetic retinopathy detection with AHFE revealed good
performance improvements, indicating the top performances of ResNet50 at
99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and
InceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes
enhancement in AI-driven diagnostics for complex and imbalanced medical
datasets.

</details>


### [295] [Robust multi-coil MRI reconstruction via self-supervised denoising](https://arxiv.org/abs/2411.12919)
*Asad Aali, Marius Arvinte, Sidharth Kumar, Yamin I. Arefeen, Jonathan I. Tamir*

Main category: eess.IV

TL;DR: Self-supervised denoising improves deep learning-based MRI reconstruction by enhancing input data quality, leading to better performance metrics like NRMSE, SSIM, and PSNR.


<details>
  <summary>Details</summary>
Motivation: Obtaining large, noise-free MRI datasets is impractical, and noisy data can degrade reconstruction quality. This study explores denoising as a pre-processing step to improve DL-based methods.

Method: Leveraged GSURE for denoising and evaluated two DL-based methods (DPMs and MoDL) on accelerated multi-coil MRI reconstruction using T2-weighted brain and knee scans.

Result: Denoising improved reconstruction quality, with lower NRMSE and higher SSIM/PSNR across various SNR levels for both brain and knee data.

Conclusion: Denoising is essential for enhancing DL-based MRI reconstruction, potentially eliminating the need for noise-free reference scans.

Abstract: We study the effect of incorporating self-supervised denoising as a
pre-processing step for training deep learning (DL) based reconstruction
methods on data corrupted by Gaussian noise. K-space data employed for training
are typically multi-coil and inherently noisy. Although DL-based reconstruction
methods trained on fully sampled data can enable high reconstruction quality,
obtaining large, noise-free datasets is impractical. We leverage Generalized
Stein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based
reconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based
Deep Learning (MoDL). We evaluate the impact of denoising on the performance of
these DL-based methods in solving accelerated multi-coil magnetic resonance
imaging (MRI) reconstruction. The experiments were carried out on T2-weighted
brain and fat-suppressed proton-density knee scans. We observed that
self-supervised denoising enhances the quality and efficiency of MRI
reconstructions across various scenarios. Specifically, employing denoised
images rather than noisy counterparts when training DL networks results in
lower normalized root mean squared error (NRMSE), higher structural similarity
index measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR
levels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,
14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising
is an essential pre-processing technique capable of improving the efficacy of
DL-based MRI reconstruction methods under diverse conditions. By refining the
quality of input data, denoising enables training more effective DL networks,
potentially bypassing the need for noise-free reference MRI scans.

</details>


### [296] [Semantic Communication with Entropy-and-Channel-Adaptive Rate Control over Multi-User MIMO Fading Channels](https://arxiv.org/abs/2501.15414)
*Weixuan Chen, Qianqian Yang, Yuhao Chen, Chongwen Huang, Qian Wang, Zehui Xiong, Zhaoyang Zhang*

Main category: eess.IV

TL;DR: A novel semantic communication method for wireless image transmission adapts transmission rates dynamically based on entropy, channel conditions, and SNR, outperforming existing methods in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing semantic communication methods use fixed transmission rates, leading to performance issues under harsh channel conditions.

Method: The proposed method integrates entropy-and-channel-adaptive rate control, feature map pruning, and attention mechanisms (channel, spatial, MHSA) to optimize resource usage.

Result: The system outperforms traditional and deep learning-based methods in rate-distortion performance, flexibility, and robustness, especially in low SNR and imperfect CSI scenarios.

Conclusion: Dynamic rate adaptation and attention mechanisms enhance semantic communication efficiency and robustness in wireless image transmission.

Abstract: Although significant improvements in transmission efficiency have been
achieved, existing semantic communication (SemCom) methods typically use a
fixed transmission rate for varying channel conditions and transmission
contents, leading to performance degradation under harsh channel conditions. To
address these challenges, we propose a novel SemCom method for wireless image
transmission that integrates entropy-andchannel-adaptive rate control
mechanism, specifically designed for multi-user multiple-input multiple-output
(MU-MIMO) fading channels. Unlike existing methods, our system dynamically
adjusts transmission rates by leveraging the entropy of feature maps, channel
state information (CSI), and signal-to-noise ratio (SNR), ensuring optimal
communication resource usage. It incorporates feature map pruning, channel
attention, spatial attention, and multi-head self-attention (MHSA) to
effectively prioritize critical semantic features while minimizing unnecessary
transmission overhead. Experimental results demonstrate that the proposed
system outperforms separated source and channel coding and deep joint source
and channel coding (Deep JSCC), in terms of rate-distortion performance,
flexibility, and robustness, particularly in challenging scenarios such as low
SNR, imperfect CSI, and inter-user interference.

</details>


### [297] [Learned enclosure method for experimental EIT data](https://arxiv.org/abs/2504.11512)
*Sara Sippola, Siiri Rautio, Andreas Hauptmann, Takanori Ide, Samuli Siltanen*

Main category: eess.IV

TL;DR: A hybrid method combining the enclosure method and neural networks improves convex hull estimation in EIT, outperforming traditional least squares fitting.


<details>
  <summary>Details</summary>
Motivation: The inverse problem in EIT is nonlinear and ill-posed, requiring innovative solutions to improve accuracy. Combining analytical methods with machine learning offers a promising approach.

Method: The proposed method integrates the enclosure method with neural networks to estimate the convex hull of inclusions from boundary measurements.

Result: The hybrid approach achieves superior performance on both simulated and experimental data compared to the classical enclosure method with least squares fitting.

Conclusion: Combining analytical techniques with machine learning enhances the accuracy of convex hull estimation in EIT, demonstrating the potential of hybrid methods in solving inverse problems.

Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method with
diverse applications, including medical imaging and non-destructive testing.
The inverse problem of reconstructing internal electrical conductivity from
boundary measurements is nonlinear and highly ill-posed, making it difficult to
solve accurately. In recent years, there has been growing interest in combining
analytical methods with machine learning to solve inverse problems. In this
paper, we propose a method for estimating the convex hull of inclusions from
boundary measurements by combining the enclosure method proposed by Ikehata
with neural networks. We demonstrate its performance using experimental data.
Compared to the classical enclosure method with least squares fitting, the
learned convex hull achieves superior performance on both simulated and
experimental data.

</details>


### [298] [Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance](https://arxiv.org/abs/2504.13340)
*Oliver Mills, Philip Conaghan, Nishant Ravikumar, Samuel Relton*

Main category: eess.IV

TL;DR: The study adapted the Segment Anything Model (SAM) for automated meniscus segmentation in 3D knee MRI, comparing it to a 3D U-Net. SAM underperformed unless fine-tuned end-to-end, matching the U-Net's performance but still struggling with fine anatomical details.


<details>
  <summary>Details</summary>
Motivation: Accurate meniscus segmentation is crucial for early detection of knee osteoarthritis (OA), but existing methods lack the use of advanced models like SAM.

Method: SAM was fine-tuned for meniscus segmentation in 3D knee MRI, compared against a 3D U-Net baseline. Performance was evaluated using Dice scores and Hausdorff Distance.

Result: SAM achieved comparable Dice scores to 3D U-Net only when fine-tuned end-to-end (0.87Â±0.03), but was inferior in Hausdorff Distance, indicating poorer morphology matching.

Conclusion: SAM's generalizability doesn't surpass a basic 3D U-Net for meniscus segmentation, suggesting limitations for fine 3D medical image tasks.

Abstract: Menisci are cartilaginous tissue found within the knee that contribute to
joint lubrication and weight dispersal. Damage to menisci can lead to onset and
progression of knee osteoarthritis (OA), a condition that is a leading cause of
disability, and for which there are few effective therapies. Accurate automated
segmentation of menisci would allow for earlier detection and treatment of
meniscal abnormalities, as well as shedding more light on the role the menisci
play in OA pathogenesis. Focus in this area has mainly used variants of
convolutional networks, but there has been no attempt to utilise recent large
vision transformer segmentation models. The Segment Anything Model (SAM) is a
so-called foundation segmentation model, which has been found useful across a
range of different tasks due to the large volume of data used for training the
model. In this study, SAM was adapted to perform fully-automated segmentation
of menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained
as a baseline. It was found that, when fine-tuning only the decoder, SAM was
unable to compete with 3D U-Net, achieving a Dice score of $0.81\pm0.03$,
compared to $0.87\pm0.03$, on a held-out test set. When fine-tuning SAM
end-to-end, a Dice score of $0.87\pm0.03$ was achieved. The performance of both
the end-to-end trained SAM configuration and the 3D U-Net were comparable to
the winning Dice score ($0.88\pm0.03$) in the IWOAI Knee MRI Segmentation
Challenge 2019. Performance in terms of the Hausdorff Distance showed that both
configurations of SAM were inferior to 3D U-Net in matching the meniscus
morphology. Results demonstrated that, despite its generalisability, SAM was
unable to outperform a basic 3D U-Net in meniscus segmentation, and may not be
suitable for similar 3D medical image segmentation tasks also involving fine
anatomical structures with low contrast and poorly-defined boundaries.

</details>
