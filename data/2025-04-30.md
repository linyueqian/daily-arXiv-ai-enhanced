<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 71]
- [cs.CV](#cs.CV) [Total: 109]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.SD](#cs.SD) [Total: 8]
- [cs.LG](#cs.LG) [Total: 137]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 5]
- [eess.AS](#eess.AS) [Total: 5]
- [eess.IV](#eess.IV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
*Marina Mayor-Rocher, Cristina Pozo, Nina Melero, Gonzalo Martínez, María Grandury, Pedro Reviriego*

Main category: cs.CL

TL;DR: The study evaluates nine language models' ability to identify and distinguish seven Spanish language varieties, finding GPT-4o as the only model recognizing Spanish variability.


<details>
  <summary>Details</summary>
Motivation: Spanish has rich diatopic variations, and understanding how LLMs handle these differences is crucial for improving their performance in diverse Spanish-speaking regions.

Method: A multiple-choice test assessed nine models on their ability to identify morphosyntactic and lexical peculiarities of seven Spanish varieties.

Result: Peninsular Spanish was the best identified by all models, with GPT-4o being the only one recognizing the full variability of Spanish.

Conclusion: GPT-4o stands out in recognizing Spanish linguistic diversity, highlighting the need for further model improvements to address regional variations.

Abstract: In recent years, large language models (LLMs) have demonstrated a high
capacity for understanding and generating text in Spanish. However, with five
hundred million native speakers, Spanish is not a homogeneous language but
rather one rich in diatopic variations spanning both sides of the Atlantic. For
this reason, in this study, we evaluate the ability of nine language models to
identify and distinguish the morphosyntactic and lexical peculiarities of seven
varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean,
Peninsular, Mexican and Central American and Rioplatense) through a
multiple-choice test. The results indicate that the Peninsular Spanish variety
is the best identified by all models and that, among them, GPT-4o is the only
model capable of recognizing the variability of the Spanish language.
  --
  En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus
siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar
texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos,
la espa\~nola no es una lengua homog\'enea, sino rica en variedades
diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello,
evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de
identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de
siete variedades de espa\~nol (andino, antillano, caribe\~no continental,
chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense)
mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que
la variedad de espa\~nol peninsular es la mejor identificada por todos los
modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar
la variabilidad de la lengua espa\~nola.

</details>


### [2] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
*Frances Laureano De Leon, Harish Tayyar Madabushi, Mark G. Lee*

Main category: cs.CL

TL;DR: The study evaluates how state-of-the-art language models handle ambiguous multiword expressions, finding they struggle despite their general strengths, especially in less frequent contexts.


<details>
  <summary>Details</summary>
Motivation: To assess the ability of large language models to process nuanced language, specifically ambiguous multiword expressions, in varied linguistic contexts.

Method: Evaluation of models (including GPT-4) using a novel code-switched dataset and tasks in English, Portuguese, and Galician, comparing performance to xlm-roBERTa-base baselines.

Result: Large language models, including GPT-4, underperform in detecting and processing ambiguous multiword expressions, particularly in novel tasks.

Conclusion: Multiword expressions, especially ambiguous ones, remain a challenge for current language models, highlighting limitations in nuanced language understanding.

Abstract: Multiword expressions, characterised by non-compositional meanings and
syntactic irregularities, are an example of nuanced language. These expressions
can be used literally or idiomatically, leading to significant changes in
meaning. While large language models have demonstrated strong performance
across many tasks, their ability to handle such linguistic subtleties remains
uncertain. Therefore, this study evaluates how state-of-the-art language models
process the ambiguity of potentially idiomatic multiword expressions,
particularly in contexts that are less frequent, where models are less likely
to rely on memorisation. By evaluating models across in Portuguese and
Galician, in addition to English, and using a novel code-switched dataset and a
novel task, we find that large language models, despite their strengths,
struggle with nuanced language. In particular, we find that the latest models,
including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both
detection and semantic tasks, with especially poor performance on the novel
tasks we introduce, despite its similarity to existing tasks. Overall, our
results demonstrate that multiword expressions, especially those which are
ambiguous, continue to be a challenge to models.

</details>


### [3] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
*Sebastian Gehrmann, Claire Huang, Xian Teng, Sergei Yurovski, Iyanuoluwa Shode, Chirag S. Patel, Arjun Bhorkar, Naveen Thomas, John Doucette, David Rosenberg, Mark Dredze, David Rabinowitz*

Main category: cs.CL

TL;DR: The paper emphasizes the need for defining safe inputs and outputs in Generative AI (GenAI) for financial services, highlighting gaps in existing guardrails.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on sociotechnical systems in specialized domains like financial services, where legal and regulatory scrutiny is high.

Method: Proposes an AI content risk taxonomy for financial services, evaluates open-source guardrails using red-teaming data.

Result: Existing guardrails fail to detect most content risks identified in the taxonomy.

Conclusion: Specialized domains require tailored safety measures, as general-purpose solutions are insufficient.

Abstract: To responsibly develop Generative AI (GenAI) products, it is critical to
define the scope of acceptable inputs and outputs. What constitutes a "safe"
response is an actively debated question. Academic work puts an outsized focus
on evaluating models by themselves for general purpose aspects such as
toxicity, bias, and fairness, especially in conversational applications being
used by a broad audience. In contrast, less focus is put on considering
sociotechnical systems in specialized domains. Yet, those specialized systems
can be subject to extensive and well-understood legal and regulatory scrutiny.
These product-specific considerations need to be set in industry-specific laws,
regulations, and corporate governance requirements. In this paper, we aim to
highlight AI content safety considerations specific to the financial services
domain and outline an associated AI content risk taxonomy. We compare this
taxonomy to existing work in this space and discuss implications of risk
category violations on various stakeholders. We evaluate how existing
open-source technical guardrail solutions cover this taxonomy by assessing them
on data collected via red-teaming activities. Our results demonstrate that
these guardrails fail to detect most of the content risks we discuss.

</details>


### [4] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
*Zae Myung Kim, Chanwoo Park, Vipul Raheja, Dongyeop Kang*

Main category: cs.CL

TL;DR: Meta Policy Optimization (MPO) introduces a meta-reward model to dynamically refine reward prompts, addressing reward hacking and reducing manual prompt engineering in LLM alignment.


<details>
  <summary>Details</summary>
Motivation: Current reward-based alignment methods for LLMs are vulnerable to reward hacking and rely heavily on manual prompt engineering.

Method: MPO integrates a meta-reward model that dynamically adjusts the reward model's prompt during training, providing an adaptive reward signal.

Result: MPO matches or outperforms models with hand-crafted prompts and maintains effectiveness across diverse tasks without specialized designs.

Conclusion: MPO offers a robust and adaptable solution for reward-based alignment in LLMs, reducing manual effort and improving stability.

Abstract: Reward-based alignment methods for large language models (LLMs) face two key
limitations: vulnerability to reward hacking, where models exploit flaws in the
reward signal; and reliance on brittle, labor-intensive prompt engineering when
LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a
framework that addresses these challenges by integrating a meta-reward model
that dynamically refines the reward model's prompt throughout training. In MPO,
the meta-reward model monitors the evolving training context and continuously
adjusts the reward model's prompt to maintain high alignment, providing an
adaptive reward signal that resists exploitation by the policy. This
meta-learning approach promotes a more stable policy optimization, and greatly
reduces the need for manual reward prompt design. It yields performance on par
with or better than models guided by extensively hand-crafted reward prompts.
Furthermore, we show that MPO maintains its effectiveness across diverse tasks,
such as question answering and mathematical reasoning, without requiring
specialized reward designs. Beyond standard RLAIF, MPO's meta-learning
formulation is readily extensible to higher-level alignment frameworks.
Overall, this method addresses theoretical and practical challenges in
reward-based RL alignment for LLMs, paving the way for more robust and
adaptable alignment strategies. The code and models will be publicly shared.

</details>


### [5] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
*Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson*

Main category: cs.CL

TL;DR: The paper introduces MICE, a model-internal confidence estimator for tool-using agents, improving calibration and utility in tool-calling tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and usefulness of tool-using agents by improving model confidence calibration for better risk-reward assessment.

Method: MICE decodes intermediate layers of a language model using logitLens, computes similarity scores between layer outputs and final output, and uses a probabilistic classifier to assess confidence.

Result: MICE outperforms baselines on calibration error and improves tool-calling utility, showing sample efficiency and zero-shot generalization to unseen APIs.

Conclusion: MICE is effective for improving confidence estimation in tool-calling tasks, enhancing both calibration and utility.

Abstract: Tool-using agents that act in the world need to be both useful and safe.
Well-calibrated model confidences can be used to weigh the risk versus reward
of potential actions, but prior work shows that many models are poorly
calibrated. Inspired by interpretability literature exploring the internals of
models, we propose a novel class of model-internal confidence estimators (MICE)
to better assess confidence when calling tools. MICE first decodes from each
intermediate layer of the language model using logitLens and then computes
similarity scores between each layer's generation and the final output. These
features are fed into a learned probabilistic classifier to assess confidence
in the decoded output. On the simulated trial and error (STE) tool-calling
dataset using Llama3 models, we find that MICE beats or matches the baselines
on smoothed expected calibration error. Using MICE confidences to determine
whether to call a tool significantly improves over strong baselines on a new
metric, expected tool-calling utility. Further experiments show that MICE is
sample-efficient, can generalize zero-shot to unseen APIs, and results in
higher tool-calling utility in scenarios with varying risk levels. Our code is
open source, available at https://github.com/microsoft/mice_for_cats.

</details>


### [6] [Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities](https://arxiv.org/abs/2406.11357)
*Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong*

Main category: cs.CL

TL;DR: The paper introduces Refiner, an extract-and-restructure method for Retrieval-Augmented Generation (RAG) systems to address LLMs' limitations in handling scattered key information, improving answer accuracy and reducing tokens.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with hallucinations and the "lost-in-the-middle" syndrome, where key information is scattered and overlooked. Refiner aims to enhance RAG by restructuring content for better LLM utilization.

Method: Refiner uses a single decoder-only LLM to extract query-relevant content verbatim, add context, and section it based on interconnectedness, highlighting key information for downstream LLMs.

Result: Experiments show Refiner (7B parameters) improves answer accuracy, reduces tokens by 80.5%, and outperforms other RAG methods, especially in multi-hop QA tasks (1.6-7.0% improvement).

Conclusion: Refiner is a plug-and-play solution for RAG systems, enhancing LLM performance by effectively restructuring and compressing retrieved information.

Abstract: Large Language Models (LLMs) are limited by their parametric knowledge,
leading to hallucinations in knowledge-extensive tasks. To address this,
Retrieval-Augmented Generation (RAG) incorporates external document chunks to
expand LLM knowledge. Furthermore, compressing information from document chunks
through extraction or summarization can improve LLM performance. Nonetheless,
LLMs still struggle to notice and utilize scattered key information, a problem
known as the "lost-in-the-middle" syndrome. Therefore, we typically need to
restructure the content for LLM to recognize the key information. We propose
$\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that
operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a
single decoder-only LLM to adaptively extract query-relevant contents verbatim
along with the necessary context, and section them based on their
interconnectedness, thereby highlights information distinction, and aligns
downstream LLMs with the original context effectively. Experiments show that a
trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to
downstream LLM in improving answer accuracy, and outperforms other
state-of-the-art advanced RAG and concurrent compressing approaches in various
single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5%
tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared
to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that
can be seamlessly integrated with RAG systems, facilitating its application
across diverse open-source frameworks.

</details>


### [7] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
*Henning Schäfer, Cynthia S. Schmidt, Johannes Wutzkowsky, Kamil Lorek, Lea Reinartz, Johannes Rückert, Christian Temme, Britta Böckmann, Peter A. Horn, Christoph M. Friedrich*

Main category: cs.CL

TL;DR: An open-source pipeline for extracting checkbox data from scanned documents reduces manual transcription errors and workload in healthcare.


<details>
  <summary>Details</summary>
Motivation: Manual transcription of paper-based healthcare data is time-consuming and error-prone, necessitating an automated solution.

Method: The pipeline integrates checkbox detection, multilingual OCR, and vision-language models (VLMs) to process scanned documents.

Result: High precision and recall in extracting checkbox data, reducing administrative workload and improving regulatory reporting.

Conclusion: The open-source pipeline offers a scalable solution for checkbox-rich documents, adaptable beyond healthcare.

Abstract: Despite the growing adoption of electronic health records, many processes
still rely on paper documents, reflecting the heterogeneous real-world
conditions in which healthcare is delivered. The manual transcription process
is time-consuming and prone to errors when transferring paper-based data to
digital formats. To streamline this workflow, this study presents an
open-source pipeline that extracts and categorizes checkbox data from scanned
documents. Demonstrated on transfusion reaction reports, the design supports
adaptation to other checkbox-rich document types. The proposed method
integrates checkbox detection, multilingual optical character recognition (OCR)
and multilingual vision-language models (VLMs). The pipeline achieves high
precision and recall compared against annually compiled gold-standards from
2017 to 2024. The result is a reduction in administrative workload and accurate
regulatory reporting. The open-source availability of this pipeline encourages
self-hosted parsing of checkbox forms.

</details>


### [8] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
*Yaroslav Getman, Tamás Grósz, Mikko Kurimo, Giampiero Salvi*

Main category: cs.CL

TL;DR: NOCASA is a competition for developing systems to assess L2 learners' pronunciation using gamified training, addressing data limitations and imbalance.


<details>
  <summary>Details</summary>
Motivation: To improve pronunciation assessment for young L2 learners through a gamified app, tackling data scarcity and imbalance.

Method: Participants use provided pseudo-anonymized data (TeflonNorL2) and baselines (SVM and wav2vec 2.0) to develop assessment systems.

Result: The wav2vec 2.0 model performs best with a UAR of 36.37%.

Conclusion: NOCASA fosters innovation in pronunciation assessment, with wav2vec 2.0 setting a benchmark.

Abstract: This paper presents the "Non-native Children's Automatic Speech Assessment"
(NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA
challenges participants to develop new systems that can assess single-word
pronunciations of young second language (L2) learners as part of a gamified
pronunciation training app. To achieve this, several issues must be addressed,
most notably the limited nature of available training data and the highly
unbalanced distribution among the pronunciation level categories. To expedite
the development, we provide a pseudo-anonymized training data (TeflonNorL2),
containing 10,334 recordings from 44 speakers attempting to pronounce 205
distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that
should be given in the game). In addition to the data, two already trained
systems are released as official baselines: an SVM classifier trained on the
ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter
achieves the best performance on the challenge test set, with an unweighted
average recall (UAR) of 36.37%.

</details>


### [9] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
*Aiala Rosá, Santiago Góngora, Juan Pablo Filevich, Ignacio Sastre, Laura Musto, Brian Carpenter, Luis Chiruzzo*

Main category: cs.CL

TL;DR: A platform for generating educational activities in English as a foreign language, leveraging NLP techniques, offering pre-made and customizable games/exercises, with plans for image/text generation and server upgrades.


<details>
  <summary>Details</summary>
Motivation: To enhance English language teaching through automated, customizable, and interactive educational activities using NLP.

Method: Developed a platform with NLP-based games and exercises, semi-automated resource creation, manual curation, and teacher input for complex content.

Result: Deployed platform with out-of-the-box and customizable activities, currently experimenting with image/text generation and server migration for better performance.

Conclusion: The platform successfully aids English teaching, with ongoing improvements planned for expanded functionality and efficiency.

Abstract: We present a platform for the generation of educational activities oriented
to teaching English as a foreign language. The different activities -- games
and language practice exercises -- are strongly based on Natural Language
Processing techniques. The platform offers the possibility of playing
out-of-the-box games, generated from resources created semi-automatically and
then manually curated. It can also generate games or exercises of greater
complexity from texts entered by teachers, providing a stage of review and
edition of the generated content before use. As a way of expanding the variety
of activities in the platform, we are currently experimenting with image and
text generation. In order to integrate them and improve the performance of
other neural tools already integrated, we are working on migrating the platform
to a more powerful server. In this paper we describe the development of our
platform and its deployment for end users, discussing the challenges faced and
how we overcame them, and also detail our future work plans.

</details>


### [10] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
*Dandan Chen Kaptur, Yue Huang, Xuejun Ryan Ji, Yanhui Guo, Bradley Kaptur*

Main category: cs.CL

TL;DR: GPT-4 and Kimi LLMs were tested for systematic reviews, showing performance varies with data volume and question complexity.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of LLMs like GPT-4 and Kimi in generating codes for systematic reviews compared to human-generated ones.

Method: Compared LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment.

Result: Performance of LLMs fluctuates based on data volume and question complexity.

Conclusion: LLMs show potential for systematic reviews but require careful consideration of data and question complexity.

Abstract: This research delved into GPT-4 and Kimi, two Large Language Models (LLMs),
for systematic reviews. We evaluated their performance by comparing
LLM-generated codes with human-generated codes from a peer-reviewed systematic
review on assessment. Our findings suggested that the performance of LLMs
fluctuates by data volume and question complexity for systematic reviews.

</details>


### [11] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
*Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider*

Main category: cs.CL

TL;DR: The paper introduces UD-English-CHILDES, the first Universal Dependencies treebank derived from CHILDES data, harmonizing annotations from 11 children and caregivers, totaling 48k sentences, and providing 1M silver-standard sentences.


<details>
  <summary>Details</summary>
Motivation: To create a consistent and unified resource for computational and linguistic research by harmonizing CHILDES data under Universal Dependencies guidelines.

Method: Derived from dependency-annotated CHILDES data, validated existing annotations under UD v2, and added silver-standard sentences.

Result: Produced a corpus of 48k gold-standard and 1M silver-standard sentences, ensuring consistency for research.

Conclusion: UD-English-CHILDES offers a valuable, standardized resource for studying child and child-directed speech.

Abstract: CHILDES is a widely used resource of transcribed child and child-directed
speech. This paper introduces UD-English-CHILDES, the first officially released
Universal Dependencies (UD) treebank derived from previously
dependency-annotated CHILDES data with consistent and unified annotation
guidelines. Our corpus harmonizes annotations from 11 children and their
caregivers, totaling over 48k sentences. We validate existing gold-standard
annotations under the UD v2 framework and provide an additional 1M
silver-standard sentences, offering a consistent resource for computational and
linguistic research.

</details>


### [12] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
*Chao-Lin Liu, Po-Hsien Wu, Yi-Ting Yu*

Main category: cs.CL

TL;DR: A new method for labeling legal datasets uses co-citation of legal articles to measure similarity, enabling automated annotation for legal recommender systems in specialized domains like labor disputes.


<details>
  <summary>Details</summary>
Motivation: The challenge of limited labeled datasets for legal recommender systems, especially in niche areas like labor disputes, motivates the need for automated annotation techniques.

Method: The approach leverages co-citation of legal articles within cases to establish similarity, using cited precedents as indicators of shared legal issues. The system recommends similar cases based on accusations, rebuttals, and disputes, with finetuned text embedding models and a BiLSTM module.

Result: The evaluation shows the recommender can effectively recommend labor cases by measuring similarity through co-citation of legal articles.

Conclusion: This research advances automated annotation for legal documents, particularly in domains with limited access to comprehensive legal databases.

Abstract: This report addresses the challenge of limited labeled datasets for
developing legal recommender systems, particularly in specialized domains like
labor disputes. We propose a new approach leveraging the co-citation of legal
articles within cases to establish similarity and enable algorithmic
annotation. This method draws a parallel to the concept of case co-citation,
utilizing cited precedents as indicators of shared legal issues. To evaluate
the labeled results, we employ a system that recommends similar cases based on
plaintiffs' accusations, defendants' rebuttals, and points of disputes. The
evaluation demonstrates that the recommender, with finetuned text embedding
models and a reasonable BiLSTM module can recommend labor cases whose
similarity was measured by the co-citation of the legal articles. This research
contributes to the development of automated annotation techniques for legal
documents, particularly in areas with limited access to comprehensive legal
databases.

</details>


### [13] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
*Yash Jain, Vishal Chowdhary*

Main category: cs.CL

TL;DR: The paper introduces Local Prompt Optimization (LPO), a method to improve prompt optimization by focusing on specific tokens, outperforming global methods in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing prompt optimization methods struggle with large vocabularies and insufficient guidance, leading to suboptimal prompts.

Method: LPO identifies and optimizes specific tokens in prompts, integrating with general automatic prompt engineering methods.

Result: LPO shows significant performance improvements on Math Reasoning and BIG-bench Hard benchmarks and converges faster to optimal prompts.

Conclusion: LPO is a promising approach for efficient and effective prompt optimization in large language models.

Abstract: In recent years, the use of prompts to guide the output of Large Language
Models have increased dramatically. However, even the best of experts struggle
to choose the correct words to stitch up a prompt for the desired task. To
solve this, LLM driven prompt optimization emerged as an important problem.
Existing prompt optimization methods optimize a prompt globally, where in all
the prompt tokens have to be optimized over a large vocabulary while solving a
complex task. The large optimization space (tokens) leads to insufficient
guidance for a better prompt. In this work, we introduce Local Prompt
Optimization (LPO) that integrates with any general automatic prompt
engineering method. We identify the optimization tokens in a prompt and nudge
the LLM to focus only on those tokens in its optimization step. We observe
remarkable performance improvements on Math Reasoning (GSM8k and MultiArith)
and BIG-bench Hard benchmarks across various automatic prompt engineering
methods. Further, we show that LPO converges to the optimal prompt faster than
global methods.

</details>


### [14] [LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models](https://arxiv.org/abs/2310.03903)
*Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang*

Main category: cs.CL

TL;DR: The paper introduces the LLM-Coordination Benchmark to evaluate LLMs in pure coordination tasks, revealing strengths in environmental decision-making but weaknesses in Theory of Mind reasoning and joint planning.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' potential as coordination agents in pure cooperation settings, where maximizing gains requires joint effort.

Method: The study uses two tasks: Agentic Coordination (LLMs as proactive participants in games) and CoordQA (multiple-choice questions testing comprehension, ToM reasoning, and joint planning).

Result: LLMs perform well in environmental decision-making but struggle with Theory of Mind and joint planning. They show robustness in Zero-Shot Coordination.

Conclusion: LLMs show promise as coordination agents but need improvement in understanding partners' beliefs and intentions.

Abstract: Large Language Models (LLMs) have demonstrated emergent common-sense
reasoning and Theory of Mind (ToM) capabilities, making them promising
candidates for developing coordination agents. This study introduces the
LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context
of Pure Coordination Settings, where agents must cooperate to maximize gains.
Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic
Coordination, where LLMs act as proactive participants in four pure
coordination games. The second is Coordination Question Answering (CoordQA),
which tests LLMs on 198 multiple-choice questions across these games to
evaluate three key abilities: Environment Comprehension, ToM Reasoning, and
Joint Planning. Results from Agentic Coordination experiments reveal that
LLM-Agents excel in multi-agent coordination settings where decision-making
primarily relies on environmental variables but face challenges in scenarios
requiring active consideration of partners' beliefs and intentions. The CoordQA
experiments further highlight significant room for improvement in LLMs' Theory
of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC)
experiments in the Agentic Coordination setting demonstrate that LLM agents,
unlike RL methods, exhibit robustness to unseen partners. These findings
indicate the potential of LLMs as Agents in pure coordination setups and
underscore areas for improvement. Code Available at
https://github.com/eric-ai-lab/llm_coordination.

</details>


### [15] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
*Maria Khelli, Samuel Cahyawijaya, Ayu Purwarianti, Genta Indra Winata*

Main category: cs.CL

TL;DR: The paper investigates catastrophic forgetting in multilingual NLP models, testing parameter-sharing strategies with LoRA adapters across 52 languages.


<details>
  <summary>Details</summary>
Motivation: To address challenges like catastrophic forgetting in cross-lingual transfer, focusing on linguistic differences rather than just model parameters.

Method: Experiments with 52 languages using LoRA adapters of varying ranks (non-shared, partially shared, fully shared) to evaluate parameter sharing.

Result: Languages with non-Latin scripts are more prone to catastrophic forgetting, while Latin-script languages enable better cross-lingual transfer.

Conclusion: Parameter sharing via adapters can mitigate forgetting, with script type influencing transfer effectiveness.

Abstract: Cross-lingual transfer in natural language processing (NLP) models enhances
multilingual performance by leveraging shared linguistic knowledge. However,
traditional methods that process all data simultaneously often fail to mimic
real-world scenarios, leading to challenges like catastrophic forgetting, where
fine-tuning on new tasks degrades performance on previously learned ones. Our
study explores this issue in multilingual contexts, focusing on linguistic
differences affecting representational learning rather than just model
parameters. We experiment with 52 languages using LoRA adapters of varying
ranks to evaluate non-shared, partially shared, and fully shared parameters.
Our aim is to see if parameter sharing through adapters can mitigate forgetting
while preserving prior knowledge. We find that languages using non-Latin
scripts are more susceptible to catastrophic forgetting, whereas those written
in Latin script facilitate more effective cross-lingual transfer.

</details>


### [16] [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)
*Maxime Peyrard, Martin Josifoski, Robert West*

Main category: cs.CL

TL;DR: The paper introduces 'semantic decoding,' a framework for optimizing collaborations between LLMs, humans, and tools by treating interactions as semantic space optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LLMs by leveraging collaborative processes with humans and tools, framed as semantic optimization.

Method: Proposes semantic decoding, conceptualizing LLMs, humans, and tools as semantic processors exchanging 'semantic tokens' (known thoughts) to construct outputs.

Result: A fresh perspective on AI system engineering, enabling greater complexity by focusing on semantic rather than syntactic details.

Conclusion: Semantic decoding offers a powerful abstraction for optimizing meaningful concepts, with research opportunities in this new framework.

Abstract: Recent work demonstrated great promise in the idea of orchestrating
collaborations between LLMs, human input, and various tools to address the
inherent limitations of LLMs. We propose a novel perspective called semantic
decoding, which frames these collaborative processes as optimization procedures
in semantic space. Specifically, we conceptualize LLMs as semantic processors
that manipulate meaningful pieces of information that we call semantic tokens
(known thoughts). LLMs are among a large pool of other semantic processors,
including humans and tools, such as search engines or code executors.
Collectively, semantic processors engage in dynamic exchanges of semantic
tokens to progressively construct high-utility outputs. We refer to these
orchestrated interactions among semantic processors, optimizing and searching
in semantic space, as semantic decoding algorithms. This concept draws a direct
parallel to the well-studied problem of syntactic decoding, which involves
crafting algorithms to best exploit auto-regressive language models for
extracting high-utility sequences of syntactic tokens. By focusing on the
semantic level and disregarding syntactic details, we gain a fresh perspective
on the engineering of AI systems, enabling us to imagine systems with much
greater complexity and capabilities. In this position paper, we formalize the
transition from syntactic to semantic tokens as well as the analogy between
syntactic and semantic decoding. Subsequently, we explore the possibilities of
optimizing within the space of semantic tokens via semantic decoding
algorithms. We conclude with a list of research opportunities and questions
arising from this fresh perspective. The semantic decoding perspective offers a
powerful abstraction for search and optimization directly in the space of
meaningful concepts, with semantic tokens as the fundamental units of a new
type of computation.

</details>


### [17] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
*Zhibo Man, Yuanmeng Chen, Yujie Zhang, Yufeng Chen, Jinan Xu*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' disambiguation ability in multi-domain translation (MDT) using a framework called DMDTEval, which includes a test set, prompting templates, and metrics. Findings aim to guide future research.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well in general machine translation but struggle with ambiguity in multi-domain translation, necessitating evaluation of their disambiguation ability.

Method: The authors develop DMDTEval, a framework with a multi-domain annotated test set, diverse prompting templates, and precise metrics to evaluate LLMs.

Result: Experiments uncover key insights about LLMs' disambiguation performance, providing a foundation for future improvements.

Conclusion: The study highlights LLMs' limitations in MDT and offers a framework to advance disambiguation research.

Abstract: Currently, Large Language Models (LLMs) have achieved remarkable results in
machine translation. However, their performance in multi-domain translation
(MDT) is less satisfactory; the meanings of words can vary across different
domains, highlighting the significant ambiguity inherent in MDT. Therefore,
evaluating the disambiguation ability of LLMs in MDT remains an open problem.
To this end, we present an evaluation and analysis of LLMs on disambiguation in
multi-domain translation (DMDTEval), our systematic evaluation framework
consisting of three critical aspects: (1) we construct a translation test set
with multi-domain ambiguous word annotation, (2) we curate a diverse set of
disambiguation prompting templates, and (3) we design precise disambiguation
metrics, and study the efficacy of various prompting strategies on multiple
state-of-the-art LLMs. Our extensive experiments reveal a number of crucial
findings that we believe will pave the way and also facilitate further research
in the critical area of improving the disambiguation of LLMs.

</details>


### [18] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
*Mika Hämäläinen*

Main category: cs.CL

TL;DR: The study examines the primacy effect in ChatGPT, Gemini, and Claude by adapting Asch's experiment. Results show varying preferences based on adjective order and presentation method.


<details>
  <summary>Details</summary>
Motivation: To investigate if LLMs exhibit the primacy effect, similar to humans, when evaluating candidates with ordered adjectives.

Method: Repurposed Asch's experiment with two setups: simultaneous and separate candidate presentations, testing 200 pairs per model.

Result: ChatGPT preferred positive-first candidates in simultaneous prompts, while Gemini showed no bias. Claude refused choices. In separate prompts, ChatGPT and Claude often ranked equally, but preferred negative-first when not. Gemini favored negative-first.

Conclusion: LLMs display varying primacy effects, influenced by presentation method, suggesting context-dependent biases in their evaluations.

Abstract: We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and
Claude. We do this by repurposing the famous experiment Asch (1946) conducted
using human subjects. The experiment is simple, given two candidates with equal
descriptions which one is preferred if one description has positive adjectives
first before negative ones and another description has negative adjectives
followed by positive ones. We test this in two experiments. In one experiment,
LLMs are given both candidates simultaneously in the same prompt, and in
another experiment, LLMs are given both candidates separately. We test all the
models with 200 candidate pairs. We found that, in the first experiment,
ChatGPT preferred the candidate with positive adjectives listed first, while
Gemini preferred both equally often. Claude refused to make a choice. In the
second experiment, ChatGPT and Claude were most likely to rank both candidates
equally. In the case where they did not give an equal rating, both showed a
clear preference to a candidate that had negative adjectives listed first.
Gemini was most likely to prefer a candidate with negative adjectives listed
first.

</details>


### [19] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
*Daniel Lee, Harsh Sharma, Jieun Han, Sunny Jeong, Alice Oh, Vered Shwartz*

Main category: cs.CL

TL;DR: LLMs outperform traditional MT models in translating English-Korean text but struggle with cultural adaptation of entities, highlighting gaps in automatic metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of preserving cultural and language-specific nuances in knowledge-intensive, entity-rich English-Korean translation.

Method: Evaluated 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators, constructing an error taxonomy.

Result: LLMs perform better than MT systems but face issues with entity translation, especially cultural adaptation, with errors varying by entity type and popularity.

Conclusion: The study reveals limitations in automatic evaluation metrics and calls for future work on culturally-nuanced machine translation.

Abstract: Translating knowledge-intensive and entity-rich text between English and
Korean requires transcreation to preserve language-specific and cultural
nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13
models (LLMs and MT models) using automatic metrics and human assessment by
bilingual annotators. Our findings show LLMs outperform traditional MT systems
but struggle with entity translation requiring cultural adaptation. By
constructing an error taxonomy, we identify incorrect responses and entity name
errors as key issues, with performance varying by entity type and popularity
level. This work exposes gaps in automatic evaluation metrics and hope to
enable future work in completing culturally-nuanced machine translation.

</details>


### [20] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
*Enfa Fane, Mihai Surdeanu, Eduardo Blanco, Steven R. Corman*

Main category: cs.CL

TL;DR: The paper evaluates zero-shot capabilities of LLMs in classifying framing roles in news narratives, finding hierarchical task decomposition and tailored prompts improve performance.


<details>
  <summary>Details</summary>
Motivation: To understand how news narratives frame entities and assess LLMs' ability to classify framing roles without task-specific training.

Method: Systematic experimentation with input context, prompting strategies, and hierarchical task decomposition (broad roles first, then fine-grained).

Result: Achieved 89.4% Main Role Accuracy and 34.5% Exact Match Ratio, showing hierarchical approaches outperform single-step classification.

Conclusion: Tailored prompt design and input context optimization are crucial for improving LLM performance in entity framing tasks.

Abstract: Understanding how news narratives frame entities is crucial for studying
media's impact on societal perceptions of events. In this paper, we evaluate
the zero-shot capabilities of large language models (LLMs) in classifying
framing roles. Through systematic experimentation, we assess the effects of
input context, prompting strategies, and task decomposition. Our findings show
that a hierarchical approach of first identifying broad roles and then
fine-grained roles, outperforms single-step classification. We also demonstrate
that optimal input contexts and prompts vary across task levels, highlighting
the need for subtask-specific strategies. We achieve a Main Role Accuracy of
89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our
approach. Our findings emphasize the importance of tailored prompt design and
input context optimization for improving LLM performance in entity framing.

</details>


### [21] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
*Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Weiming Lu*

Main category: cs.CL

TL;DR: CrossIC-PT enhances cross-lingual transfer in LLMs by leveraging semantically related bilingual texts via next-word prediction, improving multilingual performance across models and languages.


<details>
  <summary>Details</summary>
Motivation: Existing methods for cross-lingual transfer are limited by parallel resources and coverage. CrossIC-PT addresses this by using bilingual texts in a scalable way.

Method: CrossIC-PT interleaves bilingual Wikipedia documents into context windows, using segmentation and sliding windows for coherence. A semantic retrieval framework extends data availability.

Result: CrossIC-PT improves performance on three models (Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-1.5B) across six languages, with gains of 3.79%, 3.99%, and 1.95%, respectively.

Conclusion: CrossIC-PT is a scalable and effective method for enhancing cross-lingual transfer in LLMs, with demonstrated performance improvements.

Abstract: Large language models (LLMs) exhibit remarkable multilingual capabilities
despite English-dominated pre-training, attributed to cross-lingual mechanisms
during pre-training. Existing methods for enhancing cross-lingual transfer
remain constrained by parallel resources, suffering from limited linguistic and
domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT),
a simple and scalable approach that enhances cross-lingual transfer by
leveraging semantically related bilingual texts via simple next-word
prediction. We construct CrossIC-PT samples by interleaving semantic-related
bilingual Wikipedia documents into a single context window. To access window
size constraints, we implement a systematic segmentation policy to split long
bilingual document pairs into chunks while adjusting the sliding window
mechanism to preserve contextual coherence. We further extend data availability
through a semantic retrieval framework to construct CrossIC-PT samples from
web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves
multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and
Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%,
3.99%, and 1.95%, respectively, with additional improvements after data
augmentation.

</details>


### [22] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
*Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata*

Main category: cs.CL

TL;DR: UniDetox is a universal method for detoxifying large language models (LLMs) without model-specific tuning, using contrastive decoding and synthetic text data.


<details>
  <summary>Details</summary>
Motivation: Existing detoxification methods are model-specific and require hyperparameter tuning, limiting their applicability. UniDetox aims to provide a universally applicable solution.

Method: Proposes a dataset distillation technique using contrastive decoding to create detoxifying synthetic text, enabling universal fine-tuning of any LLM.

Result: Demonstrates effective detoxification across models like OPT, Falcon, and LLaMA-2 with a single hyperparameter configuration. Also reduces politically biased content.

Conclusion: UniDetox offers a scalable and efficient universal detoxification method for LLMs, eliminating the need for model-specific tuning and providing insights into detoxification attributes.

Abstract: We present UniDetox, a universally applicable method designed to mitigate
toxicity across various large language models (LLMs). Previous detoxification
methods are typically model-specific, addressing only individual models or
model families, and require careful hyperparameter tuning due to the trade-off
between detoxification efficacy and language modeling performance. In contrast,
UniDetox provides a detoxification technique that can be universally applied to
a wide range of LLMs without the need for separate model-specific tuning.
Specifically, we propose a novel and efficient dataset distillation technique
for detoxification using contrastive decoding. This approach distills
detoxifying representations in the form of synthetic text data, enabling
universal detoxification of any LLM through fine-tuning with the distilled
text. Our experiments demonstrate that the detoxifying text distilled from
GPT-2 can effectively detoxify larger models, including OPT, Falcon, and
LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter
tuning for each model, as a single hyperparameter configuration can be
seamlessly applied across different models. Additionally, analysis of the
detoxifying text reveals a reduction in politically biased content, providing
insights into the attributes necessary for effective detoxification of LLMs.

</details>


### [23] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
*Jesus Lovon, Thouria Ben-Haddi, Jules Di Scala, Jose G. Moreno, Lynda Tamine*

Main category: cs.CL

TL;DR: The paper addresses the lack of standardized benchmarks in medical NLP by adapting MIMIC-IV EHR data for text-based tasks, showing fine-tuned models outperform zero-shot LLMs.


<details>
  <summary>Details</summary>
Motivation: To overcome barriers in adopting NLP for health tasks due to missing benchmarks, the paper revisits MIMIC-IV EHR data.

Method: Integrates MIMIC-IV with Hugging Face for accessibility and explores tabular-to-text conversion for EHR data. Evaluates fine-tuned and zero-shot LLMs on mortality prediction.

Result: Fine-tuned text models compete with tabular classifiers, while zero-shot LLMs underperform.

Conclusion: Text-based approaches show promise in medicine, but zero-shot LLMs need improvement for EHR tasks.

Abstract: The lack of standardized evaluation benchmarks in the medical domain for text
inputs can be a barrier to widely adopting and leveraging the potential of
natural language models for health-related downstream tasks. This paper
revisited an openly available MIMIC-IV benchmark for electronic health records
(EHRs) to address this issue. First, we integrate the MIMIC-IV data within the
Hugging Face datasets library to allow an easy share and use of this
collection. Second, we investigate the application of templates to convert EHR
tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the
mortality of patients task show that fine-tuned text-based models are
competitive against robust tabular classifiers. In contrast, zero-shot LLMs
struggle to leverage EHR representations. This study underlines the potential
of text-based approaches in the medical field and highlights areas for further
improvement.

</details>


### [24] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
*Baz Roland, Kristina Malyseva, Anna Pappa, Tristan Cazenave*

Main category: cs.CL

TL;DR: BrAIcht is an AI agent fine-tuned to generate dialogues in Bertolt Brecht's style using German LeoLM and QLoRA for efficient training. It shows promising results based on BLEU score and perplexity.


<details>
  <summary>Details</summary>
Motivation: To create an AI that can mimic the distinctive dialogue style of Bertolt Brecht, a renowned German playwright.

Method: Fine-tuning German LeoLM (7B parameters) with Brecht's 29 plays and 907 similar German plays, using QLoRA for memory efficiency.

Result: BrAIcht performs well in generating Brecht-style dialogues, as measured by BLEU score and perplexity.

Conclusion: BrAIcht successfully replicates Brecht's style, demonstrating the potential of parameter-efficient fine-tuning for specialized language tasks.

Abstract: This project introduces BrAIcht, an AI conversational agent that creates
dialogues in the distinctive style of the famous German playwright Bertolt
Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7
billion parameters and a modified version of the base Llama2 suitable for
German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of
other German plays that are stylistically similar to Bertolt Brecht are used to
form a more di-erse dataset. Due to the limited memory capacity, a
parameterefficient fine-tuning technique called QLoRA is implemented to train
the large language model. The results, based on BLEU score and perplexity, show
very promising performance of BrAIcht in generating dialogues in the style of
Bertolt Brecht.

</details>


### [25] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
*Iwona Christop, Tomasz Kuczyński, Marek Kubis*

Main category: cs.CL

TL;DR: A new benchmark for voice cloning TTS models includes an evaluation protocol, open-source library, and leaderboard.


<details>
  <summary>Details</summary>
Motivation: To provide a standardized way to assess and compare voice cloning text-to-speech models.

Method: Developed an evaluation protocol, open-source library, and leaderboard for performance assessment.

Result: A comprehensive benchmark toolset for evaluating voice cloning models.

Conclusion: The benchmark facilitates fair comparison and advancement in voice cloning TTS technology.

Abstract: We present a novel benchmark for voice cloning text-to-speech models. The
benchmark consists of an evaluation protocol, an open-source library for
assessing the performance of voice cloning models, and an accompanying
leaderboard. The paper discusses design considerations and presents a detailed
description of the evaluation procedure. The usage of the software library is
explained, along with the organization of results on the leaderboard.

</details>


### [26] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
*Mihai Nadas, Laura Diosan, Andrei Piscoran, Andreea Tomescu*

Main category: cs.CL

TL;DR: TF1-EN-3M is a dataset of 3M English fables generated by small models (≤8B parameters) using a structured scaffold, evaluated for quality and diversity, and released openly for research.


<details>
  <summary>Details</summary>
Motivation: Modern NLP lacks a large, structured corpus of moral stories with explicit ethical lessons, which this work addresses.

Method: Uses a combinatorial prompt engine to generate fables following a six-slot scaffold, evaluated via a hybrid pipeline (GPT-based critic and reference-free metrics).

Result: An 8B-parameter Llama-3 variant achieves the best quality-speed trade-off, producing high-quality fables at low cost.

Conclusion: TF1-EN-3M enables research in narrative AI and value alignment without proprietary models, demonstrating scalability and accessibility.

Abstract: Moral stories are a time-tested vehicle for transmitting values, yet modern
NLP lacks a large, structured corpus that couples coherent narratives with
explicit ethical lessons. We close this gap with TF1-EN-3M, the first open
dataset of three million English-language fables generated exclusively by
instruction-tuned models no larger than 8B parameters. Each story follows a
six-slot scaffold (character -> trait -> setting -> conflict -> resolution ->
moral), produced through a combinatorial prompt engine that guarantees genre
fidelity while covering a broad thematic space.
  A hybrid evaluation pipeline blends (i) a GPT-based critic that scores
grammar, creativity, moral clarity, and template adherence with (ii)
reference-free diversity and readability metrics. Among ten open-weight
candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed
trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM)
at approximately 13.5 cents per 1,000 fables.
  We release the dataset, generation code, evaluation scripts, and full
metadata under a permissive license, enabling exact reproducibility and cost
benchmarking. TF1-EN-3M opens avenues for research in instruction following,
narrative intelligence, value alignment, and child-friendly educational AI,
demonstrating that large-scale moral storytelling no longer requires
proprietary giant models.

</details>


### [27] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
*Xinyu Yao, Mengdi Wang, Bo Chen, Xiaobing Zhao*

Main category: cs.CL

TL;DR: The paper introduces WenyanGPT, a language model optimized for Classical Chinese, and WenyanBENCH, a benchmark dataset, showing superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing NLP models are optimized for Modern Chinese, leading to poor performance on Classical Chinese, a key carrier of Chinese culture.

Method: Continued pre-training and instruction fine-tuning of LLaMA3-8B-Chinese to create WenyanGPT, alongside developing WenyanBENCH for evaluation.

Result: WenyanGPT outperforms advanced LLMs in Classical Chinese tasks, as demonstrated by WenyanBENCH.

Conclusion: The release of WenyanGPT and WenyanBENCH aims to advance research in Classical Chinese processing.

Abstract: Classical Chinese, as the core carrier of Chinese culture, plays a crucial
role in the inheritance and study of ancient literature. However, existing
natural language processing models primarily optimize for Modern Chinese,
resulting in inadequate performance on Classical Chinese. This paper presents a
comprehensive solution for Classical Chinese language processing. By continuing
pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we
construct a large language model, WenyanGPT, which is specifically designed for
Classical Chinese tasks. Additionally, we develop an evaluation benchmark
dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that
WenyanGPT significantly outperforms current advanced LLMs in various Classical
Chinese tasks. We make the model's training data, instruction fine-tuning
data\footnote, and evaluation benchmark dataset publicly available to promote
further research and development in the field of Classical Chinese processing.

</details>


### [28] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
*Moran Mizrahi, Chen Shani, Gabriel Stanovsky, Dan Jurafsky, Dafna Shahaf*

Main category: cs.CL

TL;DR: A novel approach combines LLMs with structured representations and cognitive manipulations to enhance creativity, outperforming GPT-4o in generating diverse and novel ideas, demonstrated in the culinary domain.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with creativity, so the paper aims to improve creative idea generation by leveraging structured representations and cognitive-inspired methods.

Method: The approach couples LLMs with structured representations and cognitively inspired manipulations to recombine ideas abstractly, demonstrated with DishCOVER for creative recipe generation.

Result: The model outperforms GPT-4o in diversity and novelty, producing coherent and feasible culinary creations, as validated by domain experts.

Conclusion: The work advances structured creativity in AI and encourages further research in this direction.

Abstract: Large Language Models (LLMs) excel at countless tasks, yet struggle with
creativity. In this paper, we introduce a novel approach that couples LLMs with
structured representations and cognitively inspired manipulations to generate
more creative and diverse ideas. Our notion of creativity goes beyond
superficial token-level variations; rather, we explicitly recombine structured
representations of existing ideas, allowing our algorithm to effectively
explore the more abstract landscape of ideas. We demonstrate our approach in
the culinary domain with DishCOVER, a model that generates creative recipes.
Experiments comparing our model's results to those of GPT-4o show greater
diversity. Domain expert evaluations reveal that our outputs, which are mostly
coherent and feasible culinary creations, significantly surpass GPT-4o in terms
of novelty, thus outperforming it in creative generation. We hope our work
inspires further research into structured creativity in AI.

</details>


### [29] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
*Ivan Vykopal, Martin Hyben, Robert Moro, Michal Gregor, Jakub Simko*

Main category: cs.CL

TL;DR: The paper introduces an LLM-based method to reduce redundant fact-checking by retrieving and evaluating previously verified claims, improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Online disinformation requires efficient fact-checking, but redundant verification of claims increases workload and delays responses.

Method: Uses large language models (LLMs) to filter irrelevant fact-checks and generate summaries/explanations for faster assessment.

Result: LLMs effectively filter irrelevant fact-checks, reducing effort and streamlining the process, as shown in evaluations.

Conclusion: The approach enhances fact-checking efficiency by leveraging LLMs to minimize redundancy and support quicker responses.

Abstract: Online disinformation poses a global challenge, placing significant demands
on fact-checkers who must verify claims efficiently to prevent the spread of
false information. A major issue in this process is the redundant verification
of already fact-checked claims, which increases workload and delays responses
to newly emerging claims. This research introduces an approach that retrieves
previously fact-checked claims, evaluates their relevance to a given input, and
provides supplementary information to support fact-checkers. Our method employs
large language models (LLMs) to filter irrelevant fact-checks and generate
concise summaries and explanations, enabling fact-checkers to faster assess
whether a claim has been verified before. In addition, we evaluate our approach
through both automatic and human assessments, where humans interact with the
developed tool to review its effectiveness. Our results demonstrate that LLMs
are able to filter out many irrelevant fact-checks and, therefore, reduce
effort and streamline the fact-checking process.

</details>


### [30] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
*Wing Yan Li, Zeqiang Wang, Jon Johnson, Suparna De*

Main category: cs.CL

TL;DR: The paper introduces an IR task to detect semantically equivalent questions in longitudinal surveys, comparing unsupervised methods like probabilistic models and neural networks, with neural models performing best.


<details>
  <summary>Details</summary>
Motivation: Automated detection of equivalent questions is vital for harmonizing longitudinal social science surveys, addressing challenges like inconsistent construct representation and evolving vocabulary.

Method: Multiple unsupervised approaches (probabilistic models, linear probing of language models, IR-specialized neural networks) are tested on a survey dataset from 1946-2020.

Result: IR-specialized neural models achieve the highest performance; re-ranking with neural models offers modest F1-score improvements (up to 0.07). Models struggle with high lexical overlap and sub-concept mismatches.

Conclusion: The study advances research on harmonizing longitudinal social science surveys, highlighting the effectiveness of neural models despite some limitations.

Abstract: Automated detection of semantically equivalent questions in longitudinal
social science surveys is crucial for long-term studies informing empirical
research in the social, economic, and health sciences. Retrieving equivalent
questions faces dual challenges: inconsistent representation of theoretical
constructs (i.e. concept/sub-concept) across studies as well as between
question and response options, and the evolution of vocabulary and structure in
longitudinal text. To address these challenges, our multi-disciplinary
collaboration of computer scientists and survey specialists presents a new
information retrieval (IR) task of identifying concept (e.g. Housing, Job,
etc.) equivalence across question and response options to harmonise
longitudinal population studies. This paper investigates multiple unsupervised
approaches on a survey dataset spanning 1946-2020, including probabilistic
models, linear probing of language models, and pre-trained neural networks
specialised for IR. We show that IR-specialised neural models achieve the
highest overall performance with other approaches performing comparably.
Additionally, the re-ranking of the probabilistic model's results with neural
models only introduces modest improvements of 0.07 at most in F1-score.
Qualitative post-hoc evaluation by survey specialists shows that models
generally have a low sensitivity to questions with high lexical overlap,
particularly in cases where sub-concepts are mismatched. Altogether, our
analysis serves to further research on harmonising longitudinal studies in
social science.

</details>


### [31] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
*Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise Dürlich, Joakim Nivre*

Main category: cs.CL

TL;DR: The paper evaluates open-access LLMs on detecting hallucinations in translation and paraphrasing tasks, finding performance varies by model but not prompt, and NLI models are also effective.


<details>
  <summary>Details</summary>
Motivation: Address the issue of LLMs generating nonsensical or incorrect outputs (hallucinations) by evaluating their detection capabilities.

Method: Assess LLMs on HalluciGen tasks for hallucination detection in translation and paraphrasing, analyzing model size, instruction tuning, and prompt choice.

Result: Performance varies by model but is consistent across prompts; NLI models perform comparably to LLMs.

Conclusion: LLM-based detectors aren't the only option for hallucination detection, as NLI models are also viable.

Abstract: A frequently observed problem with LLMs is their tendency to generate output
that is nonsensical, illogical, or factually incorrect, often referred to
broadly as hallucination. Building on the recently proposed HalluciGen task for
hallucination detection and generation, we evaluate a suite of open-access LLMs
on their ability to detect intrinsic hallucinations in two conditional
generation tasks: translation and paraphrasing. We study how model performance
varies across tasks and language and we investigate the impact of model size,
instruction tuning, and prompt choice. We find that performance varies across
models but is consistent across prompts. Finally, we find that NLI models
perform comparably well, suggesting that LLM-based detectors are not the only
viable option for this specific task.

</details>


### [32] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
*Foteini Papadopoulou, Osman Mutlu, Neris Özen, Bas H. M. van der Velden, Iris Hendrickx, Ali Hürriyetoğlu*

Main category: cs.CL

TL;DR: The paper proposes text augmentation techniques to improve classification of food hazards and products, finding transformer models outperform others, with contextual word insertion boosting minority class accuracy by 6%.


<details>
  <summary>Details</summary>
Motivation: To address poor performance on minority classes in food hazard detection by exploring text augmentation techniques.

Method: Three word-level augmentation techniques (synonym replacement, random word swapping, contextual word insertion) were tested on transformer and ML models.

Result: Transformer models performed best overall; contextual word insertion improved minority hazard class accuracy by 6%.

Conclusion: Targeted augmentation of minority classes can enhance transformer model performance in fine-grained classification tasks.

Abstract: This paper presents our system developed for the SemEval-2025 Task 9: The
Food Hazard Detection Challenge. The shared task's objective is to evaluate
explainable classification systems for classifying hazards and products in two
levels of granularity from food recall incident reports. In this work, we
propose text augmentation techniques as a way to improve poor performance on
minority classes and compare their effect for each category on various
transformer and machine learning models. We explore three word-level data
augmentation techniques, namely synonym replacement, random word swapping, and
contextual word insertion. The results show that transformer models tend to
have a better overall performance. None of the three augmentation techniques
consistently improved overall performance for classifying hazards and products.
We observed a statistically significant improvement (P < 0.05) in the
fine-grained categories when using the BERT model to compare the baseline with
each augmented model. Compared to the baseline, the contextual words insertion
augmentation improved the accuracy of predictions for the minority hazard
classes by 6%. This suggests that targeted augmentation of minority classes can
improve the performance of transformer models.

</details>


### [33] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
*Hasan Abed Al Kader Hammoud, Hani Itani, Bernard Ghanem*

Main category: cs.CL

TL;DR: The paper challenges the reliance on final answers in LLM evaluations by analyzing intermediate reasoning steps (subthoughts) and proposes a method to improve accuracy by aggregating answers from these steps.


<details>
  <summary>Details</summary>
Motivation: To determine if final answers reliably represent the model's optimal conclusion and if alternative reasoning paths yield different results.

Method: Segment reasoning traces into subthoughts, generate continuations from each, extract answers, and aggregate them by selecting the most frequent one (mode).

Result: Aggregating answers from subthoughts improves accuracy by up to 13% and 10% on AIME2024 and AIME2025 datasets, respectively.

Conclusion: Analyzing subthoughts enhances LLM evaluation reliability, offering insights into model confidence and correctness.

Abstract: Large Language Models (LLMs) leverage step-by-step reasoning to solve complex
problems. Standard evaluation practice involves generating a complete reasoning
trace and assessing the correctness of the final answer presented at its
conclusion. In this paper, we challenge the reliance on the final answer by
posing the following two questions: Does the final answer reliably represent
the model's optimal conclusion? Can alternative reasoning paths yield different
results? To answer these questions, we analyze intermediate reasoning steps,
termed subthoughts, and propose a method based on our findings. Our approach
involves segmenting a reasoning trace into sequential subthoughts based on
linguistic cues. We start by prompting the model to generate continuations from
the end-point of each intermediate subthought. We extract a potential answer
from every completed continuation originating from different subthoughts. We
find that aggregating these answers by selecting the most frequent one (the
mode) often yields significantly higher accuracy compared to relying solely on
the answer derived from the original complete trace. Analyzing the consistency
among the answers derived from different subthoughts reveals characteristics
that correlate with the model's confidence and correctness, suggesting
potential for identifying less reliable answers. Our experiments across various
LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025)
show consistent accuracy improvements, with gains reaching up to 13\% and 10\%
respectively. Implementation is available at:
https://github.com/hammoudhasan/SubthoughtReasoner.

</details>


### [34] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
*Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang*

Main category: cs.CL

TL;DR: UniversalRAG improves factual accuracy by retrieving and integrating knowledge from diverse modalities and granularities, addressing limitations of single-modality RAG approaches.


<details>
  <summary>Details</summary>
Motivation: Existing RAG methods are limited to single modalities, while real-world queries require diverse knowledge sources.

Method: Proposes a modality-aware routing mechanism and multi-granularity organization for targeted retrieval from heterogeneous sources.

Result: Outperforms modality-specific and unified baselines on 8 benchmarks.

Conclusion: UniversalRAG effectively bridges the modality gap and enhances retrieval accuracy for varied queries.

Abstract: Retrieval-Augmented Generation (RAG) has shown substantial promise in
improving factual accuracy by grounding model responses with external knowledge
relevant to queries. However, most existing RAG approaches are limited to a
text-only corpus, and while recent efforts have extended RAG to other
modalities such as images and videos, they typically operate over a single
modality-specific corpus. In contrast, real-world queries vary widely in the
type of knowledge they require, which a single type of knowledge source cannot
address. To address this, we introduce UniversalRAG, a novel RAG framework
designed to retrieve and integrate knowledge from heterogeneous sources with
diverse modalities and granularities. Specifically, motivated by the
observation that forcing all modalities into a unified representation space
derived from a single combined corpus causes a modality gap, where the
retrieval tends to favor items from the same modality as the query, we propose
a modality-aware routing mechanism that dynamically identifies the most
appropriate modality-specific corpus and performs targeted retrieval within it.
Also, beyond modality, we organize each modality into multiple granularity
levels, enabling fine-tuned retrieval tailored to the complexity and scope of
the query. We validate UniversalRAG on 8 benchmarks spanning multiple
modalities, showing its superiority over modality-specific and unified
baselines.

</details>


### [35] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
*Roman Abramov, Felix Steinbauer, Gjergji Kasneci*

Main category: cs.CL

TL;DR: The paper extends grokking to real-world factual data, using synthetic data augmentation to improve multi-step reasoning in Transformers, achieving high accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address gaps in multi-step factual reasoning in Transformers, especially with sparse real-world knowledge, by leveraging grokking and synthetic data.

Method: Augment knowledge graphs with synthetic data to increase the ratio of inferred to atomic facts, enabling grokking and improving reasoning.

Result: Achieves 95-100% accuracy on 2WikiMultiHopQA, surpassing baselines and matching state-of-the-art results.

Conclusion: Grokking-based data augmentation enhances multi-hop reasoning, suggesting potential for more robust and interpretable language models.

Abstract: Transformers have achieved great success in numerous NLP tasks but continue
to exhibit notable gaps in multi-step factual reasoning, especially when
real-world knowledge is sparse. Recent advances in grokking have demonstrated
that neural networks can transition from memorizing to perfectly generalizing
once they detect underlying logical patterns - yet these studies have primarily
used small, synthetic tasks. In this paper, for the first time, we extend
grokking to real-world factual data and address the challenge of dataset
sparsity by augmenting existing knowledge graphs with carefully designed
synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts
above the threshold required for grokking. Surprisingly, we find that even
factually incorrect synthetic data can strengthen emergent reasoning circuits
rather than degrade accuracy, as it forces the model to rely on relational
structure rather than memorization. When evaluated on multi-hop reasoning
benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA -
substantially improving over strong baselines and matching or exceeding current
state-of-the-art results. We further provide an in-depth analysis of how
increasing $\phi_r$ drives the formation of generalizing circuits inside
Transformers. Our findings suggest that grokking-based data augmentation can
unlock implicit multi-hop reasoning capabilities, opening the door to more
robust and interpretable factual reasoning in large-scale language models.

</details>


### [36] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
*Wenxiao Wang, Parsa Hosseini, Soheil Feizi*

Main category: cs.CL

TL;DR: Chain-of-thought prompting improves robustness in non-reasoning tasks via chain-of-defensive-thought, significantly mitigating accuracy drops from reference corruption.


<details>
  <summary>Details</summary>
Motivation: To leverage enhanced reasoning abilities of large language models for improving robustness in tasks vulnerable to reference corruption.

Method: Introduces chain-of-defensive-thought, using few structured and defensive reasoning exemplars as demonstrations.

Result: Dramatic robustness improvements; e.g., GPT-4o maintains 50% accuracy vs. 3% with standard prompting under corruption.

Conclusion: Chain-of-defensive-thought is a simple, effective method to enhance model robustness against reference corruption.

Abstract: Chain-of-thought prompting has demonstrated great success in facilitating the
reasoning abilities of large language models. In this work, we explore how
these enhanced reasoning abilities can be exploited to improve the robustness
of large language models in tasks that are not necessarily reasoning-focused.
In particular, we show how a wide range of large language models exhibit
significantly improved robustness against reference corruption using a simple
method called chain-of-defensive-thought, where only a few exemplars with
structured and defensive reasoning are provided as demonstrations. Empirically,
the improvements can be astounding, especially given the simplicity and
applicability of the method. For example, in the Natural Questions task, the
accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting
when 1 out of 10 references provided is corrupted with prompt injection
attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting
maintains an accuracy of 50%.

</details>


### [37] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
*Haitao Wu, Zongbo Han, Huaxi Huang, Changqing Zhang*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' computational reasoning using a UTM-based framework and introduces TMBench, a scalable benchmark showing strong correlation with other reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess LLMs' core computational reasoning, crucial for reliability in tasks like code generation and problem-solving.

Method: Proposes a UTM simulation framework and TMBench benchmark for standardized, scalable evaluation of LLMs' computational reasoning.

Result: Strong correlation (Pearson 0.73) between TMBench performance and other reasoning benchmarks, highlighting computational reasoning as a key LLM capability.

Conclusion: Computational reasoning is a vital dimension for evaluating LLMs, with TMBench providing a scalable and standardized assessment tool.

Abstract: With the rapid development and widespread application of Large Language
Models (LLMs), rigorous evaluation has become particularly crucial. This
research adopts a novel perspective, focusing on evaluating the core
computational reasoning ability of LLMs, defined as the capacity of model to
accurately understand rules, and execute logically computing operations. This
capability assesses the reliability of LLMs as precise executors, and is
critical to advanced tasks such as complex code generation and multi-step
problem-solving. We propose an evaluation framework based on Universal Turing
Machine (UTM) simulation. This framework requires LLMs to strictly follow
instructions and track dynamic states, such as tape content and read/write head
position, during multi-step computations. To enable standardized evaluation, we
developed TMBench, a benchmark for systematically studying the computational
reasoning capabilities of LLMs. TMBench provides several key advantages,
including knowledge-agnostic evaluation, adjustable difficulty, foundational
coverage through Turing machine encoding, and unlimited capacity for instance
generation, ensuring scalability as models continue to evolve. We find that
model performance on TMBench correlates strongly with performance on other
recognized reasoning benchmarks (Pearson correlation coefficient is 0.73),
clearly demonstrating that computational reasoning is a significant dimension
for measuring the deep capabilities of LLMs. Code and data are available at
https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>


### [38] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
*D. -F. Qin*

Main category: cs.CL

TL;DR: The paper explores applying quantum mechanics to language modeling, improving word embeddings, and studying natural language properties using quantum statistics. It also discusses potential applications in generative models and quantum computing.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum mechanics for enhancing language modeling and understanding the quantum properties of natural language, inspired by the physicality of information.

Method: Introduces quantum mechanics into symbol-meaning pairs, uses quantum statistics for language representation, and tests feasibility via experimental code.

Result: Demonstrates the feasibility of quantum theory in modeling natural language and suggests improvements for word embeddings.

Conclusion: Quantum mechanics offers promising avenues for advancing language modeling, with potential applications in generative models and quantum computing.

Abstract: This paper examines language modeling based on the theory of quantum
mechanics. It focuses on the introduction of quantum mechanics into the
symbol-meaning pairs of language in order to build a representation model of
natural language. At the same time, it is realized that word embedding, which
is widely used as a basic technique for statistical language modeling, can be
explained and improved by the mathematical framework of quantum mechanics. On
this basis, this paper continues to try to use quantum statistics and other
related theories to study the mathematical representation, natural evolution
and statistical properties of natural language. It is also assumed that the
source of such quantum properties is the physicality of information. The
feasibility of using quantum theory to model natural language is pointed out
through the construction of a experimental code. The paper discusses, in terms
of applications, the possible help of the theory in constructing generative
models that are popular nowadays. A preliminary discussion of future
applications of the theory to quantum computers is also presented.

</details>


### [39] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
*Anum Afzal, Alexandre Mercier, Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores LLM-based data-to-text methods to generate diverse marketing texts, addressing the monotony of traditional approaches. It uses models like T5, GPT-3.5, GPT-4, and LLaMa2, introducing a metric (JaccDiv) for text diversity.


<details>
  <summary>Details</summary>
Motivation: Traditional generative methods produce repetitive content, limiting their usefulness for platforms needing diverse automated texts.

Method: Leverages LLMs (T5, GPT-3.5, GPT-4, LLaMa2) with fine-tuning, few-shot, and zero-shot approaches, and introduces JaccDiv for diversity evaluation.

Result: Demonstrates the potential of LLMs to generate high-quality, diverse marketing texts, applicable beyond the music industry.

Conclusion: LLM-based approaches offer a scalable solution for diverse content generation, with broader applications in repetitive automated content fields.

Abstract: Online platforms are increasingly interested in using Data-to-Text
technologies to generate content and help their users. Unfortunately,
traditional generative methods often fall into repetitive patterns, resulting
in monotonous galleries of texts after only a few iterations. In this paper, we
investigate LLM-based data-to-text approaches to automatically generate
marketing texts that are of sufficient quality and diverse enough for broad
adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in
conjunction with fine-tuning, few-shot, and zero-shot approaches to set a
baseline for diverse marketing texts. We also introduce a metric JaccDiv to
evaluate the diversity of a set of texts. This research extends its relevance
beyond the music industry, proving beneficial in various fields where
repetitive automated content generation is prevalent.

</details>


### [40] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
*Miguel Nogales, Matteo Gambella, Manuel Roveri*

Main category: cs.CL

TL;DR: DYNAMAX introduces early exits (EEs) to Mamba models, showcasing their efficiency and versatility as EE classifiers for both Mamba and transformer-based LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore the under-researched application of early exits in decoder-only architectures, particularly Mamba models, for reducing computational costs and latency.

Method: Integrates EEs into Mamba and repurposes Mamba as an EE classifier, evaluating performance using Mistral 7B and Codestral 7B models on datasets like TruthfulQA, CoQA, and TriviaQA.

Result: Demonstrates Mamba's adaptability as an EE classifier, balancing computational savings and accuracy across NLP tasks.

Conclusion: Mamba's dynamic processing potential can redefine efficient inference for LLMs, especially in resource-constrained environments.

Abstract: Early exits (EEs) offer a promising approach to reducing computational costs
and latency by dynamically terminating inference once a satisfactory prediction
confidence on a data sample is achieved. Although many works integrate EEs into
encoder-only Transformers, their application to decoder-only architectures and,
more importantly, Mamba models, a novel family of state-space architectures in
the LLM realm, remains insufficiently explored. This work introduces DYNAMAX,
the first framework to exploit the unique properties of Mamba architectures for
early exit mechanisms. We not only integrate EEs into Mamba but also repurpose
Mamba as an efficient EE classifier for both Mamba-based and transformer-based
LLMs, showcasing its versatility. Our experiments employ the Mistral 7B
transformer compared to the Codestral 7B Mamba model, using data sets such as
TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and
consistency. The results highlight the adaptability of Mamba as a powerful EE
classifier and its efficiency in balancing computational cost and performance
quality across NLP tasks. By leveraging Mamba's inherent design for dynamic
processing, we open pathways for scalable and efficient inference in embedded
applications and resource-constrained environments. This study underscores the
transformative potential of Mamba in redefining dynamic computing paradigms for
LLMs.

</details>


### [41] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
*Tyler McDonald, Ali Emami*

Main category: cs.CL

TL;DR: The paper introduces Trace-of-Thought Prompting, a zero-shot method to enhance arithmetic reasoning in LLMs, showing 125% performance gains on open-source models under 7B parameters.


<details>
  <summary>Details</summary>
Motivation: To address computational and financial burdens of proprietary LLMs and improve customization for specialized tasks like arithmetic reasoning.

Method: Trace-of-Thought Prompting, a zero-shot technique that guides LLMs to create observable subproblems for better problem-solving.

Result: Performance gains up to 125% on sub-7B parameter models, with insights into problem-solving processes.

Conclusion: Open-source models and innovative prompting can democratize AI research and enhance accessibility in computational linguistics.

Abstract: As Large Language Models (LLMs) continue to be leveraged for daily tasks,
prompt engineering remains an active field of contribution within computational
linguistics, particularly in domains requiring specialized knowledge such as
arithmetic reasoning. While these LLMs are optimized for a variety of tasks,
their exhaustive employment may become computationally or financially
cumbersome for small teams. Additionally, complete reliance on proprietary,
closed-source models often limits customization and adaptability, posing
significant challenges in research and application scalability. Instead, by
leveraging open-source models at or below 7 billion parameters, we can optimize
our resource usage while still observing remarkable gains over standard
prompting approaches. To cultivate this notion, we introduce Trace-of-Thought
Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to
create observable subproblems using critical problem-solving, specifically
designed to enhance arithmetic reasoning capabilities. When applied to
open-source models in tandem with GPT-4, we observe that Trace-of-Thought not
only allows novel insight into the problem-solving process but also introduces
performance gains as large as 125% on language models at or below 7 billion
parameters. This approach underscores the potential of open-source initiatives
in democratizing AI research and improving the accessibility of high-quality
computational linguistics applications.

</details>


### [42] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
*Maryna Vyshnyvetska*

Main category: cs.CL

TL;DR: The paper introduces 'information gravity,' a theoretical model using physics concepts to explain text generation in LLMs, addressing phenomena like hallucinations and query sensitivity.


<details>
  <summary>Details</summary>
Motivation: To formalize and explain observed behaviors in LLMs, such as hallucinations and sensitivity to query formulation, using a physics-inspired framework.

Method: The model applies field theory and spacetime geometry, treating queries as objects with 'information mass' that curve semantic space, influencing token generation.

Result: The model provides a mechanism to explain LLM phenomena like hallucinations (from low-density voids), query sensitivity (due to curvature), and temperature effects.

Conclusion: The 'information gravity' model offers a novel, physics-based perspective to understand and analyze text generation in LLMs.

Abstract: We propose a theoretical model called "information gravity" to describe the
text generation process in large language models (LLMs). The model uses
physical apparatus from field theory and spacetime geometry to formalize the
interaction between user queries and the probability distribution of generated
tokens. A query is viewed as an object with "information mass" that curves the
semantic space of the model, creating gravitational potential wells that
"attract" tokens during generation. This model offers a mechanism to explain
several observed phenomena in LLM behavior, including hallucinations (emerging
from low-density semantic voids), sensitivity to query formulation (due to
semantic field curvature changes), and the influence of sampling temperature on
output diversity.

</details>


### [43] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
*Shangyu Li, Juyong Jiang, Tiancheng Zhao, Jiasi Shen*

Main category: cs.CL

TL;DR: OSVBench is a benchmark for evaluating LLMs in generating OS kernel verification specifications, revealing their limited performance on long-context tasks.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to generate complete specifications for OS kernel verification, a complex and real-world task.

Method: Defines specification generation as a program synthesis problem, providing LLMs with a programming model and verification assumptions. Evaluates 12 LLMs on 245 tasks.

Result: Current LLMs show limited performance on OS verification tasks, with significant disparities in handling long-context code generation.

Conclusion: OSVBench highlights the challenges LLMs face in OS verification and provides a toolkit for further research.

Abstract: We introduce OSVBench, a new benchmark for evaluating Large Language Models
(LLMs) in generating complete specification code pertaining to operating system
kernel verification tasks. The benchmark first defines the specification
generation problem into a program synthesis problem within a confined scope of
syntax and semantics by providing LLMs with the programming model. The LLMs are
required to understand the provided verification assumption and the potential
syntax and semantics space to search for, then generate the complete
specification for the potentially buggy operating system code implementation
under the guidance of the high-level functional description of the operating
system. This benchmark is built upon a real-world operating system kernel,
Hyperkernel, and consists of 245 complex specification generation tasks in
total, each is a long context task of about 20k-30k tokens. Our comprehensive
evaluation of 12 LLMs exhibits the limited performance of the current LLMs on
the specification generation tasks for operating system verification.
Significant disparities in their performance on the benchmark highlight
differences in their ability to handle long-context code generation tasks. The
evaluation toolkit and benchmark are available at
https://github.com/lishangyu-hkust/OSVBench.

</details>


### [44] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
*Yifan Wei, Xiaoyan Yu, Ran Song, Hao Peng, Angsheng Li*

Main category: cs.CL

TL;DR: The paper addresses the challenge of updating LLMs with new knowledge, focusing on the issue of Knowledge Element Overlap (KEO) in Knowledge Editing (KE). It proposes Knowledge Set Editing (KSE) and SetKE to handle overlapping triplets, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLMs need updates for new knowledge but face issues like overfitting and high costs with traditional methods. KE overlooks KEO, causing conflicts and degraded performance.

Method: Proposes Knowledge Set Editing (KSE) and SetKE, a method to edit sets of triplets simultaneously, addressing KEO. Introduces EditSet, a dataset with KEO triplets for benchmarking.

Result: SetKE outperforms existing KE methods in KEO scenarios on mainstream LLMs.

Conclusion: SetKE effectively handles KEO, improving LLM updates. EditSet provides a benchmark for future research.

Abstract: Large Language Models (LLMs) excel in tasks such as retrieval and question
answering but require updates to incorporate new knowledge and reduce
inaccuracies and hallucinations. Traditional updating methods, like fine-tuning
and incremental learning, face challenges such as overfitting and high
computational costs. Knowledge Editing (KE) provides a promising alternative
but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where
multiple triplets share common elements, leading to editing conflicts. We
identify the prevalence of KEO in existing KE datasets and show its significant
impact on current KE methods, causing performance degradation in handling such
triplets. To address this, we propose a new formulation, Knowledge Set Editing
(KSE), and introduce SetKE, a method that edits sets of triplets
simultaneously. Experimental results demonstrate that SetKE outperforms
existing methods in KEO scenarios on mainstream LLMs. Additionally, we
introduce EditSet, a dataset containing KEO triplets, providing a comprehensive
benchmark.

</details>


### [45] [Semantic Consistency for Assuring Reliability of Large Language Models](https://arxiv.org/abs/2308.09138)
*Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar*

Main category: cs.CL

TL;DR: The paper introduces a measure of semantic consistency for LLMs, proposes a prompting strategy (A2C) to improve it, and shows significant improvements in accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: LLMs' sensitivity to input prompts and the need for consistent outputs for safe deployment.

Method: Developed a general measure of semantic consistency and the Ask-to-Choose (A2C) prompting strategy.

Result: A2C improved accuracy by up to 47% and semantic consistency by up to 7-fold.

Conclusion: Semantic consistency metrics and A2C enhance LLM reliability in open-ended text generation.

Abstract: Large Language Models (LLMs) exhibit remarkable fluency and competence across
various natural language tasks. However, recent research has highlighted their
sensitivity to variations in input prompts. To deploy LLMs in a safe and
reliable manner, it is crucial for their outputs to be consistent when prompted
with expressions that carry the same meaning or intent. While some existing
work has explored how state-of-the-art LLMs address this issue, their
evaluations have been confined to assessing lexical equality of single- or
multi-word answers, overlooking the consistency of generative text sequences.
For a more comprehensive understanding of the consistency of LLMs in open-ended
text generation scenarios, we introduce a general measure of semantic
consistency, and formulate multiple versions of this metric to evaluate the
performance of various LLMs. Our proposal demonstrates significantly higher
consistency and stronger correlation with human evaluations of output
consistency than traditional metrics based on lexical consistency. Finally, we
propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance
semantic consistency. When evaluated for closed-book question answering based
on answer variations from the TruthfulQA benchmark, A2C increases accuracy
metrics for pretrained and finetuned LLMs by up to 47%, and semantic
consistency metrics for instruction-tuned models by up to 7-fold.

</details>


### [46] [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/abs/2310.12059)
*Duc-Vu Nguyen, Quoc-Nam Nguyen*

Main category: cs.CL

TL;DR: The paper evaluates large language models (LLMs) on multiple-choice symbol binding (MCSB) for Vietnamese MCQA tasks, introduces a new LaTeX-structured dataset, and tests six LLMs, showing promising results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of challenging Vietnamese MCQA datasets and evaluate LLMs' MCSB ability in zero-shot, one-shot, and few-shot settings.

Method: Create a high-quality LaTeX-structured dataset for math and sciences, then evaluate six LLMs (e.g., GPT-4, BLOOMZ) on existing and new datasets.

Result: Promising results on LLMs' MCSB ability for Vietnamese, with the new dataset serving as a benchmark.

Conclusion: The study highlights LLMs' potential for Vietnamese MCQA and provides a valuable dataset for future research.

Abstract: In this paper, we evaluate the ability of large language models (LLMs) to
perform multiple choice symbol binding (MCSB) for multiple choice question
answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus
on Vietnamese, with fewer challenging MCQA datasets than in English. The two
existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent
research in Vietnamese natural language processing (NLP) has focused on the
Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to
2023 to evaluate ChatGPT. However, these studies have mainly focused on how
ChatGPT solves the VNHSGE step by step. We aim to create a novel and
high-quality dataset by providing structured guidelines for typing LaTeX
formulas for mathematics, physics, chemistry, and biology. This dataset can be
used to evaluate the MCSB ability of LLMs and smaller language models (LMs)
because it is typed in a strict LaTeX style. We focus on predicting the
character (A, B, C, or D) that is the most likely answer to a question, given
the context of the question. Our evaluation of six well-known LLMs, namely
BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the
ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising
results on the MCSB ability of LLMs for Vietnamese. The dataset is available
for research purposes only.

</details>


### [47] [A Practical Analysis of Human Alignment with *PO](https://arxiv.org/abs/2407.15229)
*Kian Ahrabian, Xihui Lin, Barun Patra, Vishrav Chaudhary, Alon Benhaim, Jay Pujara, Xia Song*

Main category: cs.CL

TL;DR: The paper evaluates the robustness of preference optimization methods (PO) in out-of-distribution scenarios, introduces LN-DPO for stability, and compares performance metrics like KL divergence and response length.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of hyperparameter grid searches in PO methods and improve robustness in real-world applications.

Method: Examines PO methods in OOD scenarios, introduces LN-DPO (a length-normalized DPO variant), and compares reference-free (SimPO) and reference-dependent (DPO, LN-DPO) methods.

Result: Peak performance is similar across methods, but performance patterns vary significantly in suboptimal scenarios. LN-DPO improves stability and reduces response length.

Conclusion: LN-DPO offers a more stable and practical solution for human alignment, with insights into method robustness in real-world settings.

Abstract: At the forefront of state-of-the-art human alignment methods are preference
optimization methods (*PO). Prior research has often concentrated on
identifying the best-performing method, typically involving a grid search over
hyperparameters, which can be impractical for general practitioners. In this
paper, we examine the robustness of existing state-of-the-art methods to
varying hyperparameters in a realistic out-of-distribution (OOD) scenario that
mirrors real-world applications of human alignment. Our goal is to empirically
find the method that increases the likelihood of achieving better results
through the lens of various metrics, such as KL divergence and response length.
We also introduce LN-DPO, a simple length-normalized version of DPO that is
more stable across hyperparameters, effectively reduces the average response
length, and improves performance. Our analysis of state-of-the-art
reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO)
methods reveals that they perform similarly at their peak (i.e., best possible
scenario). However, we uncover that the pattern of change in performance
greatly varies as we move away from the best possible scenario.

</details>


### [48] [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)
*Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Taehee Lee, Youngjune L. Gwon, Sungroh Yoon*

Main category: cs.CL

TL;DR: Language models show negative bias in binary decision tasks. The paper proposes NAS to quantify this bias and NASA, a fine-tuning method, to mitigate it, improving precision-recall balance.


<details>
  <summary>Details</summary>
Motivation: To address the observed negative bias in language models during binary decision tasks, which impacts accuracy and fairness.

Method: Introduces Negative Attention Score (NAS) to measure bias and Negative Attention Score Alignment (NASA) for fine-tuning biased attention heads.

Result: NASA effectively reduces the precision-recall gap caused by negative bias while maintaining model generalization.

Conclusion: The proposed NASA method successfully mitigates negative bias in language models, enhancing their performance in binary decision tasks.

Abstract: A binary decision task, like yes-no questions or answer verification,
reflects a significant real-world scenario such as where users look for
confirmation about the correctness of their decisions on specific issues. In
this work, we observe that language models exhibit a negative bias in the
binary decisions of complex reasoning tasks. Based on our observations and the
rationale about attention-based model dynamics, we propose a negative attention
score (NAS) to systematically and quantitatively formulate negative bias. Based
on NAS, we identify attention heads that attend to negative tokens provided in
the instructions as answer candidate of binary decisions, regardless of the
question in the prompt, and validate their association with the negative bias.
Additionally, we propose the negative attention score alignment (NASA) method,
which is a parameter-efficient fine-tuning technique to address the extracted
negatively biased attention heads. Experimental results from various domains of
reasoning tasks and large model search space demonstrate that NASA
significantly reduces the gap between precision and recall caused by negative
bias while preserving their generalization abilities.

</details>


### [49] [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://arxiv.org/abs/2409.07394)
*Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal*

Main category: cs.CL

TL;DR: AdaCAD dynamically adjusts LLM outputs based on context-parameter conflict, outperforming static methods with 14.21% QA accuracy gain and 6.19 AlignScore improvement in summaries.


<details>
  <summary>Details</summary>
Motivation: Standard decoding ignores context knowledge conflicts, and static contrastive methods misjudge conflict levels, hurting performance.

Method: AdaCAD uses Jensen-Shannon divergence to dynamically infer adjustment weights for context-parameter conflicts.

Result: AdaCAD improves QA accuracy by 14.21% and summary factuality by 6.19 AlignScore, outperforming baselines.

Conclusion: AdaCAD mitigates losses from absent conflicts, making it suitable for real-world datasets with mixed conflict scenarios.

Abstract: Knowledge conflict arises from discrepancies between information in the
context of a large language model (LLM) and the knowledge stored in its
parameters. This can hurt performance when using standard decoding techniques,
which tend to ignore the context. Existing test-time contrastive methods seek
to address this by comparing the LLM's output distribution with and without the
context and adjust the model according to the contrast between them. However,
we find that these methods frequently misjudge the degree of conflict and
struggle to handle instances that vary in their amount of conflict, with static
methods over-adjusting when conflict is absent. We propose a fine-grained,
instance-level approach called AdaCAD, which dynamically infers the weight of
adjustment based on the degree of conflict, as measured by the Jensen-Shannon
divergence between distributions representing contextual and parametric
knowledge. Across four LLMs, six question-answering (QA) and three
summarization datasets, we demonstrate that ADACAD consistently outperforms
other decoding baselines with average QA accuracy gains of 14.21% (absolute)
over a static contrastive baseline, and improves the factuality of summaries by
6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt
performance when conflict is absent, ADACAD mitigates these losses, making it
more applicable to real-world datasets in which some examples have conflict and
others do not.

</details>


### [50] [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://arxiv.org/abs/2410.02102)
*Michael A. Lepori, Michael C. Mozer, Asma Ghandeharioun*

Main category: cs.CL

TL;DR: The paper explores why transformer models sometimes fail to contextualize tokens correctly, proposing the LLM Race Conditions Hypothesis to explain such errors and suggesting interventions.


<details>
  <summary>Details</summary>
Motivation: Understanding why transformer models make contextualization errors, like misinterpreting ambiguous words (e.g., 'bank'), to improve their reliability.

Method: Uses mechanistic interpretability techniques to analyze token dependencies and test the LLM Race Conditions Hypothesis.

Result: Provides correlational and causal evidence supporting the hypothesis, identifying token dependency violations as a cause of errors.

Conclusion: Proposes inference-time interventions to mitigate contextualization errors based on the findings.

Abstract: The profound success of transformer-based language models can largely be
attributed to their ability to integrate relevant contextual information from
an input sequence in order to generate a response or complete a task. However,
we know very little about the algorithms that a model employs to implement this
capability, nor do we understand their failure modes. For example, given the
prompt "John is going fishing, so he walks over to the bank. Can he make an ATM
transaction?", a model may incorrectly respond "Yes" if it has not properly
contextualized "bank" as a geographical feature, rather than a financial
institution. We propose the LLM Race Conditions Hypothesis as an explanation of
contextualization errors of this form. This hypothesis identifies dependencies
between tokens (e.g., "bank" must be properly contextualized before the final
token, "?", integrates information from "bank"), and claims that
contextualization errors are a result of violating these dependencies. Using a
variety of techniques from mechanistic intepretability, we provide
correlational and causal evidence in support of the hypothesis, and suggest
inference-time interventions to address it.

</details>


### [51] [Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context](https://arxiv.org/abs/2410.07103)
*Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon*

Main category: cs.CL

TL;DR: The paper addresses LLMs' sensitivity to document order in multi-hop reasoning, proposing context repetition (CoRe) to improve performance.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with irrelevant documents and document order in multi-hop reasoning, termed the misordered context problem.

Method: Proposes CoRe, a method where the context is repeatedly presented to guide reasoning in the optimal order.

Result: Improves F1 score by up to 30%p on multi-hop QA and accuracy by 70%p on a synthetic task; mitigates the 'lost-in-the-middle' issue.

Conclusion: CoRe is effective for multi-hop reasoning, works with retrieval-based CoT approaches, and addresses LLMs' order sensitivity.

Abstract: Multi-hop reasoning, which requires multi-step reasoning based on the
supporting documents within a given context, remains challenging for large
language models (LLMs). LLMs often struggle to filter out irrelevant documents
within the context, and their performance is sensitive to the absolute position
of supporting documents within that context. In this paper, we identify an
additional challenge: LLMs' performance is also sensitive to the order,
relative position, in which the supporting documents are presented. We refer to
this as the misordered context problem. To address this issue, based on the
theoretical approach, we propose a simple yet effective method called context
repetition (CoRe), which involves prompting the model by repeatedly presenting
the context. This ensures that certain contiguous reasoning segments within
supporting documents are presented in the optimal order, effectively guiding
the model's reasoning in the appropriate direction. Applying CoRe, we improve
the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to
70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known
"lost-in-the-middle" problem in LLMs and can be effectively combined with
retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.

</details>


### [52] [MDCure: A Scalable Pipeline for Multi-Document Instruction-Following](https://arxiv.org/abs/2410.23463)
*Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan*

Main category: cs.CL

TL;DR: MDCure is a framework for generating synthetic multi-document (MD) instruction data to enhance LLMs' MD capabilities without costly pre-training or human annotations. It includes MDCureRM for filtering data and improves performance by up to 75.1%.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in MD processing like inter-document dependencies, redundancy, and incoherent structures in LLMs.

Method: Introduces MDCure for synthetic MD instruction data generation and MDCureRM for filtering. Fine-tunes LLMs up to 70B parameters.

Result: MDCure improves performance by up to 75.1% on MD and long-context benchmarks.

Conclusion: MDCure is scalable, effective, and compatible with various models, enabling small open-source models to outperform proprietary LLMs in MD tasks.

Abstract: Multi-document (MD) processing is crucial for LLMs to handle real-world tasks
such as summarization and question-answering across large sets of documents.
While LLMs have improved at processing long inputs, MD contexts still present
unique difficulties, including management of inter-document dependencies,
redundancy, and incoherent structures. To address this challenge, we introduce
MDCure, a scalable and effective instruction data generation framework to
enhance the MD capabilities of LLMs without the computational cost of
pre-training or reliance on human-annotated data. MDCure generates high-quality
synthetic MD instruction data over sets of articles via targeted prompts. We
also introduce MDCureRM, a cost-effective, MD-specific reward model to score
and filter generated data based on their training utility for MD settings.
MDCure is compatible with open- and closed-source models in addition to policy
optimization methods such as PPO, enabling even small open-source models to
surpass proprietary LLMs as strong generators of high-quality MD instruction
data without further data filtering. With MDCure, we fine-tune a wide variety
of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model
families. Extensive evaluations on a wide range of MD and long-context
benchmarks spanning various tasks and domains show MDCure consistently improves
performance over pre-trained baselines and base models by up to 75.1%. Our
code, datasets, and models are available at https://github.com/yale-nlp/MDCure.

</details>


### [53] [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175)
*Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li*

Main category: cs.CL

TL;DR: The paper introduces constraint back-translation, a method to enhance LLMs' ability to follow complex instructions by leveraging existing datasets and adding constraints met by responses, reducing noise and cost.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with complex instructions, and conventional methods relying on advanced LLMs for data generation are limited by their inability to follow such instructions well.

Method: Proposes constraint back-translation: using high-quality instruction-response pairs and adding complex constraints already met by responses, employing Llama3-70B-Instruct to create the CRAB dataset.

Result: Post-training on CRAB improves LLMs' complex instruction-following ability, validated on benchmarks. Constraint back-translation also aids as an auxiliary training objective.

Conclusion: The method effectively enhances LLMs' performance on complex tasks, with released code, data, and models to support future research.

Abstract: Large language models (LLMs) struggle to follow instructions with complex
constraints in format, length, etc. Following the conventional
instruction-tuning practice, previous works conduct post-training on complex
instruction-response pairs generated by feeding complex instructions to
advanced LLMs. However, even advanced LLMs cannot follow complex instructions
well, thus limiting the quality of generated data. In this work, we find that
existing datasets inherently contain implicit complex constraints and propose a
novel data generation technique, constraint back-translation. Specifically, we
take the high-quality instruction-response pairs in existing datasets and only
adopt advanced LLMs to add complex constraints already met by the responses to
the instructions, which naturally reduces costs and data noise. In the
experiments, we adopt Llama3-70B-Instruct to back-translate constraints and
create a high-quality complex instruction-response dataset, named CRAB. We
present that post-training on CRAB improves multiple backbone LLMs' complex
instruction-following ability, evaluated on extensive instruction-following
benchmarks. We further find that constraint back-translation also serves as a
useful auxiliary training objective in post-training. Our code, data, and
models will be released to facilitate future research.

</details>


### [54] [Benchmarking LLMs' Judgments with No Gold Standard](https://arxiv.org/abs/2411.07127)
*Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong*

Main category: cs.CL

TL;DR: GEM is a new metric for evaluating LLM-generated content without gold standards, outperforming GPT-4o Examiner and being robust against manipulations. GRE-bench, based on GEM, evaluates LLMs on peer review tasks using fresh data to avoid contamination.


<details>
  <summary>Details</summary>
Motivation: Current metrics for LLM evaluation rely on gold standards, limiting their use in subjective tasks like peer review. GEM addresses this gap by not requiring gold references.

Method: GEM estimates mutual information between candidate and reference responses generatively, without gold standards. GRE-bench uses GEM to evaluate LLMs on peer review tasks with new, uncontaminated data.

Result: GEM correlates well with human scores and outperforms baselines, including GPT-4o Examiner, while being robust to manipulations. GRE-bench provides reliable peer review evaluations for LLMs.

Conclusion: GEM and GRE-bench offer robust, flexible tools for evaluating LLMs in subjective tasks, overcoming limitations of gold-standard-dependent metrics and data contamination.

Abstract: We introduce the GEM (Generative Estimator for Mutual Information), an
evaluation metric for assessing language generation by Large Language Models
(LLMs), particularly in generating informative judgments, without the need for
a gold standard reference. GEM broadens the scenarios where we can benchmark
LLM generation performance-from traditional ones, like machine translation and
summarization, where gold standard references are readily available, to
subjective tasks without clear gold standards, such as academic peer review.
  GEM uses a generative model to estimate mutual information between candidate
and reference responses, without requiring the reference to be a gold standard.
In experiments on a human-annotated dataset, GEM demonstrates competitive
correlations with human scores compared to the state-of-the-art GPT-4o
Examiner, and outperforms all other baselines. Additionally, GEM is more robust
against strategic manipulations, such as rephrasing or elongation, which can
artificially inflate scores under a GPT-4o Examiner.
  We also present GRE-bench (Generating Review Evaluation Benchmark) which
evaluates LLMs based on how well they can generate high-quality peer reviews
for academic research papers. Because GRE-bench is based upon GEM, it inherits
its robustness properties. Additionally, GRE-bench circumvents data
contamination problems (or data leakage) by using the continuous influx of new
open-access research papers and peer reviews each year. We show GRE-bench
results of various popular LLMs on their peer review capabilities using the
ICLR2023 dataset.

</details>


### [55] [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
*Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi*

Main category: cs.CL

TL;DR: The paper audits the Helpful and Harmless (HH) dataset, revealing issues in human feedback quality and its impact on LLM safety, especially across demographic groups.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness and quality of human feedback (LHF) in mitigating harms of LLMs, focusing on the HH dataset.

Method: Conducts a manual and automated audit of the HH dataset, experiments on its safety impact, and analyzes influential citing papers.

Result: Identifies conceptualization failures and quality issues in the HH dataset, leading to disparate safety behaviors across demographics.

Conclusion: Calls for more nuanced, context-sensitive safety mitigation approaches in LLMs.

Abstract: In an effort to mitigate the harms of large language models (LLMs), learning
from human feedback (LHF) has been used to steer LLMs towards outputs that are
intended to be both less harmful and more helpful. Despite the widespread
adoption of LHF in practice, the quality of this feedback and its effectiveness
as a safety mitigation technique remain unclear. This study addresses these
issues by auditing the widely-used Helpful and Harmless (HH) dataset by
Anthropic. Our work includes: (1) a thorough investigation of the dataset's
content through both manual and automated evaluation; (2) experiments
demonstrating the dataset's impact on models' safety; and (3) an analysis of
the 100 most influential papers citing this dataset. Through our audit, we
showcase how conceptualization failures and quality issues identified in the HH
dataset can create additional harms by leading to disparate safety behaviors
across demographic groups. Our findings highlight the need for more nuanced,
context-sensitive approaches to safety mitigation in LLMs.

</details>


### [56] [A Bayesian Optimization Approach to Machine Translation Reranking](https://arxiv.org/abs/2411.09694)
*Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos*

Main category: cs.CL

TL;DR: Reranking machine translation candidates using Bayesian optimization reduces computational costs while maintaining quality, achieving comparable results with fewer evaluations.


<details>
  <summary>Details</summary>
Motivation: The computational cost of reranking with large scoring models is high, prompting the need for an efficient method to find top candidates without scoring all.

Method: Poses reranking as a Bayesian optimization problem, strategically selecting candidates to score based on exploration and exploitation, and introduces a multi-fidelity setting with cheaper proxy scorers.

Result: Achieves the same CometKiwi score with only 70 evaluations compared to 180 in the baseline, improving cost-performance tradeoff.

Conclusion: Bayesian optimization with multi-fidelity proxy scorers is an effective and efficient approach for reranking in machine translation.

Abstract: Reranking a list of candidates from a machine translation system with an
external scoring model and returning the highest-scoring candidate remains a
simple and effective method for improving the overall output quality.
Translation scoring models continue to grow in size, with the best models being
comparable to generation models. Thus, reranking can add substantial
computational cost to the translation pipeline. In this work, we pose reranking
as a Bayesian optimization (BayesOpt) problem. By strategically selecting
candidates to score based on a balance of exploration and exploitation, we show
that it is possible to find top-scoring candidates when scoring only a fraction
of the candidate list. For instance, our method achieves the same CometKiwi
score using only 70 scoring evaluations compared a baseline system using 180.
We present a multi-fidelity setting for BayesOpt, where the candidates are
first scored with a cheaper but noisier proxy scoring model, which further
improves the cost-performance tradeoff when using smaller but well-trained
distilled proxy scorers.

</details>


### [57] [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
*Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang*

Main category: cs.CL

TL;DR: The paper identifies concentrated massive values in attention queries (Q) and keys (K) in transformer-based LLMs, linking them to contextual knowledge interpretation. It shows these values are critical for performance and traces their origin to Rotary Positional Encoding (RoPE).


<details>
  <summary>Details</summary>
Motivation: To understand the role of massive values in Q and K layers of LLMs and their impact on contextual vs. parametric knowledge.

Method: Extensive experiments on modern transformer-based LLMs, analyzing Q, K, and V layers, and investigating quantization strategies.

Result: Massive values in Q and K are crucial for contextual knowledge, not parametric knowledge. Ignoring them harms performance. RoPE causes these values.

Conclusion: The findings clarify Q and K's role in LLMs and provide insights for model optimization, emphasizing the importance of RoPE-induced massive values.

Abstract: Large language models (LLMs) have achieved remarkable success in contextual
knowledge understanding. In this paper, we show that these concentrated massive
values consistently emerge in specific regions of attention queries (Q) and
keys (K) while not having such patterns in values (V) in various modern
transformer-based LLMs (Q, K, and V mean the representations output by the
query, key, and value layers respectively). Through extensive experiments, we
further demonstrate that these massive values play a critical role in
interpreting contextual knowledge (knowledge obtained from the current context
window) rather than in retrieving parametric knowledge stored within the
model's parameters. Our further investigation of quantization strategies
reveals that ignoring these massive values leads to a pronounced drop in
performance on tasks requiring rich contextual understanding, aligning with our
analysis. Finally, we trace the emergence of concentrated massive values and
find that such concentration is caused by Rotary Positional Encoding (RoPE),
which has appeared since the first layers. These findings shed new light on how
Q and K operate in LLMs and offer practical insights for model design and
optimization. The Code is Available at
https://github.com/MingyuJ666/Rope_with_LLM.

</details>


### [58] [An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation](https://arxiv.org/abs/2502.12836)
*Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani*

Main category: cs.CL

TL;DR: An LLM-powered agent for physiological time-series analysis is developed, outperforming benchmark models in accuracy and reliability for health insights.


<details>
  <summary>Details</summary>
Motivation: Existing methods for integrating LLMs with physiological data analysis are inefficient and produce unreliable outputs, prompting the need for a better solution.

Method: The agent, built on OpenCHA and powered by GPT-3.5-turbo, integrates user interaction, data sources, and analytical tools to analyze physiological time-series like PPG signals.

Result: The agent significantly outperforms GPT-4o-mini and GPT-4o in HR estimation from PPG signals, achieving lower error rates.

Conclusion: The proposed agent bridges the gap between LLMs and analytical tools, offering a reliable solution for physiological time-series analysis.

Abstract: Large language models (LLMs) are revolutionizing healthcare by improving
diagnosis, patient care, and decision support through interactive
communication. More recently, they have been applied to analyzing physiological
time-series like wearable data for health insight extraction. Existing methods
embed raw numerical sequences directly into prompts, which exceeds token limits
and increases computational costs. Additionally, some studies integrated
features extracted from time-series in textual prompts or applied multimodal
approaches. However, these methods often produce generic and unreliable outputs
due to LLMs' limited analytical rigor and inefficiency in interpreting
continuous waveforms. In this paper, we develop an LLM-powered agent for
physiological time-series analysis aimed to bridge the gap in integrating LLMs
with well-established analytical tools. Built on the OpenCHA, an open-source
LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model
features an orchestrator that integrates user interaction, data sources, and
analytical tools to generate accurate health insights. To evaluate its
effectiveness, we implement a case study on heart rate (HR) estimation from
Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram
(ECG) recordings in a remote health monitoring study. The agent's performance
is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the
gold standard for HR estimation. Results demonstrate that our agent
significantly outperforms benchmark models by achieving lower error rates and
more reliable HR estimations. The agent implementation is publicly available on
GitHub.

</details>


### [59] [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)
*María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico*

Main category: cs.CL

TL;DR: The paper introduces MEMERAG, a multilingual benchmark for evaluating RAG systems, addressing cultural nuances by using native-language questions and expert annotations for faithfulness and relevance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for RAG systems lack cultural nuances as they focus on English or translated data, limiting their applicability to diverse languages and contexts.

Method: MEMERAG uses the MIRACL dataset with native-language questions, generates responses via diverse LLMs, and employs expert annotators to assess faithfulness and relevance.

Result: High inter-annotator agreement is achieved, and the benchmark effectively evaluates multilingual automatic evaluators, identifying improvements from advanced prompting techniques and LLMs.

Conclusion: MEMERAG provides a reliable, culturally nuanced benchmark for multilingual RAG evaluation, enhancing the development of automatic evaluators.

Abstract: Automatic evaluation of retrieval augmented generation (RAG) systems relies
on fine-grained dimensions like faithfulness and relevance, as judged by expert
human annotators. Meta-evaluation benchmarks support the development of
automatic evaluators that correlate well with human judgement. However,
existing benchmarks predominantly focus on English or use translated data,
which fails to capture cultural nuances. A native approach provides a better
representation of the end user experience.
  In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG
benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using
native-language questions and generating responses with diverse large language
models (LLMs), which are then assessed by expert annotators for faithfulness
and relevance. We describe our annotation process and show that it achieves
high inter-annotator agreement. We then analyse the performance of the
answer-generating LLMs across languages as per the human evaluators. Finally we
apply the dataset to our main use-case which is to benchmark multilingual
automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably
identify improvements offered by advanced prompting techniques and LLMs. Our
dataset is available at https://github.com/amazon-science/MEMERAG

</details>


### [60] [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
*Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps, Marco Idiart*

Main category: cs.CL

TL;DR: The paper introduces SemEval-2025 Task 1: AdMiRe, focusing on improving NLP models' ability to interpret idiomatic expressions in multimodal and multilingual contexts through two subtasks.


<details>
  <summary>Details</summary>
Motivation: Idiomatic expressions are challenging for NLP due to their non-literal meanings, and current LLMs still struggle with robust semantic representation of idiomaticity.

Method: Participants tackled two subtasks: ranking images by idiomatic/literal alignment and predicting the next image in a sequence, using pretrained LLMs and vision-language models in a mixture-of-experts setup.

Result: Top-performing methods achieved human-level performance by combining multiple queries to address model weaknesses in idiomaticity representation.

Conclusion: The task highlights the need for better idiomaticity handling in NLP and demonstrates the potential of multimodal approaches for improvement.

Abstract: Idiomatic expressions present a unique challenge in NLP, as their meanings
are often not directly inferable from their constituent words. Despite recent
advancements in Large Language Models (LLMs), idiomaticity remains a
significant obstacle to robust semantic representation. We present datasets and
tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity
Representation), which challenges the community to assess and improve models'
ability to interpret idiomatic expressions in multimodal contexts and in
multiple languages. Participants competed in two subtasks: ranking images based
on their alignment with idiomatic or literal meanings, and predicting the next
image in a sequence. The most effective methods achieved human-level
performance by leveraging pretrained LLMs and vision-language models in
mixture-of-experts settings, with multiple queries used to smooth over the
weaknesses in these models' representations of idiomaticity.

</details>


### [61] [CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation](https://arxiv.org/abs/2503.19878)
*Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary*

Main category: cs.CL

TL;DR: CausalRAG improves RAG by using causal graphs for better contextual continuity and retrieval precision.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems disrupt contextual integrity and rely too much on semantic similarity.

Method: Proposes CausalRAG, integrating causal graphs to trace relationships and enhance retrieval.

Result: Outperforms regular and graph-based RAG in accuracy and interpretability.

Conclusion: Causal reasoning in retrieval is promising for knowledge-intensive tasks.

Abstract: Large language models (LLMs) have revolutionized natural language processing
(NLP), particularly through Retrieval-Augmented Generation (RAG), which
enhances LLM capabilities by integrating external knowledge. However,
traditional RAG systems face critical limitations, including disrupted
contextual integrity due to text chunking, and over-reliance on semantic
similarity for retrieval. To address these issues, we propose CausalRAG, a
novel framework that incorporates causal graphs into the retrieval process. By
constructing and tracing causal relationships, CausalRAG preserves contextual
continuity and improves retrieval precision, leading to more accurate and
interpretable responses. We evaluate CausalRAG against regular RAG and
graph-based RAG approaches, demonstrating its superiority across several
metrics. Our findings suggest that grounding retrieval in causal reasoning
provides a promising approach to knowledge-intensive tasks.

</details>


### [62] [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)
*Xi Wang, Fanfei Meng, Shiyang Zhang, Lan Li*

Main category: cs.CL

TL;DR: The study examines the reliability of Youdao, a popular E-dictionary in China, revealing that inaccurate definitions can lead to misunderstandings and highlighting issues in AI-driven dictionary construction.


<details>
  <summary>Details</summary>
Motivation: Electronic dictionaries are widely trusted by L2 learners, but their accuracy and corpus construction are rarely scrutinized, prompting this investigation.

Method: Combined experimentation (translation task with retrospective reflection), user survey, and dictionary critique to analyze Youdao's definitions and user behavior.

Result: Incomplete or misleading definitions caused serious misunderstandings, and users exhibited problematic consultation habits. Issues in AI and data processing were identified.

Conclusion: The study calls for improved dictionary literacy training for users and enhancements in AI models for E-dictionary construction.

Abstract: Electronic dictionaries have largely replaced paper dictionaries and become
central tools for L2 learners seeking to expand their vocabulary. Users often
assume these resources are reliable and rarely question the validity of the
definitions provided. The accuracy of major E-dictionaries is seldom
scrutinized, and little attention has been paid to how their corpora are
constructed. Research on dictionary use, particularly the limitations of
electronic dictionaries, remains scarce. This study adopts a combined method of
experimentation, user survey, and dictionary critique to examine Youdao, one of
the most widely used E-dictionaries in China. The experiment involved a
translation task paired with retrospective reflection. Participants were asked
to translate sentences containing words that are insufficiently or inaccurately
defined in Youdao. Their consultation behavior was recorded to analyze how
faulty definitions influenced comprehension. Results show that incomplete or
misleading definitions can cause serious misunderstandings. Additionally,
students exhibited problematic consultation habits. The study further explores
how such flawed definitions originate, highlighting issues in data processing
and the integration of AI and machine learning technologies in dictionary
construction. The findings suggest a need for better training in dictionary
literacy for users, as well as improvements in the underlying AI models used to
build E-dictionaries.

</details>


### [63] [LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/abs/2504.05239)
*Hang Li, Yucheng Chu, Kaiqi Yang, Yasemin Copur-Gencturk, Jiliang Tang*

Main category: cs.CL

TL;DR: The paper explores using LLMs with a human-in-the-loop approach (GradeHITL) to improve automatic short answer grading, achieving higher accuracy than fully automated methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered ASAG methods struggle to match human-level grading in rubric-based assessments due to full automation.

Method: Proposes GradeHITL, a framework where LLMs interact with human experts to dynamically refine grading rubrics.

Result: GradeHITL outperforms existing methods, significantly improving grading accuracy.

Conclusion: Incorporating human insights via HITL enhances LLM-based ASAG, bridging the gap to human-level evaluation.

Abstract: The rise of artificial intelligence (AI) technologies, particularly large
language models (LLMs), has brought significant advancements to the field of
education. Among various applications, automatic short answer grading (ASAG),
which focuses on evaluating open-ended textual responses, has seen remarkable
progress with the introduction of LLMs. These models not only enhance grading
performance compared to traditional ASAG approaches but also move beyond simple
comparisons with predefined "golden" answers, enabling more sophisticated
grading scenarios, such as rubric-based evaluation. However, existing
LLM-powered methods still face challenges in achieving human-level grading
performance in rubric-based assessments due to their reliance on fully
automated approaches. In this work, we explore the potential of LLMs in ASAG
tasks by leveraging their interactive capabilities through a human-in-the-loop
(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative
properties of LLMs to pose questions to human experts, incorporating their
insights to refine grading rubrics dynamically. This adaptive process
significantly improves grading accuracy, outperforming existing methods and
bringing ASAG closer to human-level evaluation.

</details>


### [64] [Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation](https://arxiv.org/abs/2504.07072)
*Israfel Salazar, Manuel Fernández Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemiński, Jekaterina Novikova, Luísa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popovič, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel da Costa Merlin, Otávio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin farahani fard, Silvia Fernandez, María Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, Sara Hooker, Marzieh Fadaee*

Main category: cs.CL

TL;DR: Kaleidoscope is a multilingual, multicultural benchmark for evaluating vision-language models, covering 18 languages and 14 subjects with 20,911 questions. It reveals poor performance on low-resource languages and complex scenarios, advocating for culturally inclusive evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for vision-language models (VLMs) lack multilingual and multicultural coverage, often relying on English translations that miss cultural nuances.

Method: Developed Kaleidoscope, a large-scale, in-language multimodal benchmark with 20,911 multiple-choice questions across 18 languages and 14 subjects, ensuring linguistic and cultural authenticity.

Result: Top-performing multilingual VLMs performed poorly on low-resource languages and complex multimodal scenarios.

Conclusion: The study underscores the need for culturally inclusive evaluation frameworks to improve VLM performance across diverse languages and cultures.

Abstract: The evaluation of vision-language models (VLMs) has mainly relied on
English-language benchmarks, leaving significant gaps in both multilingual and
multicultural coverage. While multilingual benchmarks have expanded, both in
size and languages, many rely on translations of English datasets, failing to
capture cultural nuances. In this work, we propose Kaleidoscope, as the most
comprehensive exam benchmark to date for the multilingual evaluation of
vision-language models. Kaleidoscope is a large-scale, in-language multimodal
benchmark designed to evaluate VLMs across diverse languages and visual inputs.
Kaleidoscope covers 18 languages and 14 different subjects, amounting to a
total of 20,911 multiple-choice questions. Built through an open science
collaboration with a diverse group of researchers worldwide, Kaleidoscope
ensures linguistic and cultural authenticity. We evaluate top-performing
multilingual vision-language models and find that they perform poorly on
low-resource languages and in complex multimodal scenarios. Our results
highlight the need for progress on culturally inclusive multimodal evaluation
frameworks.

</details>


### [65] [Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
*ByteDance Seed, :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, Lei Zuo*

Main category: cs.CL

TL;DR: Seed1.5-Thinking is a reasoning model that improves performance across benchmarks like AIME 2024, Codeforces, and GPQA, and generalizes well to non-reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning abilities in STEM and coding, and demonstrate broader applicability beyond reasoning tasks.

Method: Uses a Mixture-of-Experts (MoE) architecture with 20B activated and 200B total parameters.

Result: Achieves 86.7 on AIME 2024, 55.0 on Codeforces, 77.3 on GPQA, and surpasses DeepSeek R1 by 8% in non-reasoning tasks.

Conclusion: Seed1.5-Thinking excels in reasoning and generalization, with public benchmarks (BeyondAIME and Codeforces) to support future research.

Abstract: We introduce Seed1.5-Thinking, capable of reasoning through thinking before
responding, resulting in improved performance on a wide range of benchmarks.
Seed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on
GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond
reasoning tasks, the method demonstrates notable generalization across diverse
domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on
non-reasoning tasks, indicating its broader applicability. Compared to other
state-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts
(MoE) model with a relatively small size, featuring 20B activated and 200B
total parameters. As part of our effort to assess generalized reasoning, we
develop two internal benchmarks, BeyondAIME and Codeforces, both of which will
be publicly released to support future research. Model trial link:
https://www.volcengine.com/experience/ark.

</details>


### [66] [Computational Typology](https://arxiv.org/abs/2504.15642)
*Gerhard Jäger*

Main category: cs.CL

TL;DR: Computational methods enhance typological research by analyzing large-scale linguistic data and testing structural hypotheses.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the advantages of computational statistical modeling in understanding language diversity and universals.

Method: Uses computational statistical modeling to analyze linguistic data and identify patterns.

Result: Highlights the effectiveness of computational approaches in typological research.

Conclusion: Computational methods are valuable tools for advancing typological studies.

Abstract: Typology is a subfield of linguistics that focuses on the study and
classification of languages based on their structural features. Unlike
genealogical classification, which examines the historical relationships
between languages, typology seeks to understand the diversity of human
languages by identifying common properties and patterns, known as universals.
In recent years, computational methods have played an increasingly important
role in typological research, enabling the analysis of large-scale linguistic
data and the testing of hypotheses about language structure and evolution. This
article provides an illustration of the benefits of computational statistical
modeling in typology.

</details>


### [67] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
*Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li*

Main category: cs.CL

TL;DR: RL improves reasoning in LLMs, but its impact on audio-language reasoning is unclear. The paper extends GRPO to LALM, achieving a 16.35% accuracy boost with structured reasoning (SARI).


<details>
  <summary>Details</summary>
Motivation: To explore if RL's reasoning gains in LLMs transfer to audio-language models and improve performance.

Method: Extends GRPO to LALM, uses SFT on structured/unstructured chains-of-thought, and curriculum-guided GRPO.

Result: SARI improves accuracy by 16.35% over the base model, reaching 67.08% on MMAU test-mini.

Conclusion: Structured reasoning and curriculum learning enhance audio-language understanding.

Abstract: Recent work shows that reinforcement learning(RL) can markedly sharpen the
reasoning ability of large language models (LLMs) by prompting them to "think
before answering." Yet whether and how these gains transfer to audio-language
reasoning remains largely unexplored. We extend the Group-Relative Policy
Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model
(LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage
regimen supervised fine-tuning on structured and unstructured
chains-of-thought, followed by curriculum-guided GRPO, we systematically
compare implicit vs. explicit, and structured vs. free form reasoning under
identical architectures. Our structured audio reasoning model, SARI (Structured
Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a
16.35% improvement in average accuracy over the base model
Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni
reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.
Ablation experiments show that on the base model we use: (i) SFT warm-up is
important for stable RL training, (ii) structured chains yield more robust
generalization than unstructured ones, and (iii) easy-to-hard curricula
accelerate convergence and improve final performance. These findings
demonstrate that explicit, structured reasoning and curriculum learning
substantially enhances audio-language understanding.

</details>


### [68] [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
*Yusen Zhang, Wenliang Zheng, Aashrith Madasu, Peng Shi, Ryo Kamoi, Hao Zhou, Zhuoyang Zou, Shu Zhao, Sarkar Snigdha Sarathi Das, Vipul Gupta, Xiaoxin Lu, Nan Zhang, Ranran Haoran Zhang, Avitej Iyer, Renze Lou, Wenpeng Yin, Rui Zhang*

Main category: cs.CL

TL;DR: The paper introduces HRScene, a benchmark for evaluating Vision Large Language Models (VLMs) on high-resolution image (HRI) understanding, revealing gaps in current models' performance.


<details>
  <summary>Details</summary>
Motivation: There is no comprehensive benchmark for VLMs to evaluate HRI understanding, despite their claimed capability to handle such images.

Method: HRScene is created with 25 real-world and 2 synthetic datasets, annotated by experts, covering diverse scenarios. It evaluates 28 VLMs, including Gemini 2.0 Flash and GPT-4o.

Result: Current VLMs achieve ~50% accuracy on real-world tasks and struggle with regional utilization in HRIs, showing issues like Regional Divergence.

Conclusion: HRScene highlights limitations in VLMs for HRI understanding, guiding future research to address these gaps.

Abstract: High-resolution image (HRI) understanding aims to process images with a large
number of pixels, such as pathological images and agricultural aerial images,
both of which can exceed 1 million pixels. Vision Large Language Models (VLMs)
can allegedly handle HRIs, however, there is a lack of a comprehensive
benchmark for VLMs to evaluate HRI understanding. To address this gap, we
introduce HRScene, a novel unified benchmark for HRI understanding with rich
scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic
datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$
26,627. HRScene is collected and re-annotated by 10 graduate-level annotators,
covering 25 scenarios, ranging from microscopic to radiology images, street
views, long-range pictures, and telescope images. It includes HRIs of
real-world objects, scanned documents, and composite multi-image. The two
diagnostic evaluation datasets are synthesized by combining the target image
with the gold answer and distracting images in different orders, assessing how
well models utilize regions in HRI. We conduct extensive experiments involving
28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show
that current VLMs achieve an average accuracy of around 50% on real-world
tasks, revealing significant gaps in HRI understanding. Results on synthetic
datasets reveal that VLMs struggle to effectively utilize HRI regions, showing
significant Regional Divergence and lost-in-middle, shedding light on future
research.

</details>


### [69] [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
*James O' Neill, Santhosh Subramanian, Eric Lin, Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: Task-specific data generation and model merging improve guardrail classifiers, outperforming large language models (LLMs) in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) for guardrailing face issues like high latency, memory use, and costs, prompting the need for more efficient solutions.

Method: Fine-tuned classifiers via task-specific data generation, pretraining on synthetic datasets, and search-based model merging (MultiTaskGuard and UniGuard).

Result: Significant performance improvements (29.92 F1 over Aegis-LlamaGuard, 21.62 over GPT-4) on guardrail benchmarks.

Conclusion: Efficient, task-specific guardrail models outperform LLMs, offering a scalable and cost-effective alternative.

Abstract: The trend towards large language models (LLMs) for guardrailing against
undesired behaviors is increasing and has shown promise for censoring user
inputs. However, increased latency, memory consumption, hosting expenses and
non-structured outputs can make their use prohibitive.
  In this work, we show that task-specific data generation can lead to
fine-tuned classifiers that significantly outperform current state of the art
(SoTA) while being orders of magnitude smaller. Secondly, we show that using a
single model, \texttt{MultiTaskGuard}, that is pretrained on a large
synthetically generated dataset with unique task instructions further improves
generalization. Thirdly, our most performant models, \texttt{UniGuard}, are
found using our proposed search-based model merging approach that finds an
optimal set of parameters to combine single-policy models and multi-policy
guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created,
our efficient guardrail classifiers improve over the best performing SoTA
publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting
unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92}
points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o},
respectively. Lastly, our guardrail synthetic data generation process that uses
custom task-specific guardrail poli

</details>


### [70] [Context Selection and Rewriting for Video-based Educational Question Generation](https://arxiv.org/abs/2504.19406)
*Mengxia Yu, Bang Nguyen, Olivia Zino, Meng Jiang*

Main category: cs.CL

TL;DR: The paper introduces a novel framework for educational question generation (EQG) using large language models to address challenges in aligning questions with lecture content and target answers.


<details>
  <summary>Details</summary>
Motivation: Existing EQG datasets lack realism, relying on edited texts instead of real-world classroom content like lectures and slides. Current methods struggle with generating accurate questions from such content.

Method: A framework that dynamically selects and rewrites contexts from lecture transcripts and video keyframes, integrating them into answer-containing knowledge statements for better question generation.

Result: The approach improves question quality and relevance by enhancing the logical connection between contexts and target answers.

Conclusion: The proposed framework and dataset advance EQG by addressing real-world challenges, with resources made publicly available.

Abstract: Educational question generation (EQG) is a crucial component of intelligent
educational systems, significantly aiding self-assessment, active learning, and
personalized education. While EQG systems have emerged, existing datasets
typically rely on predefined, carefully edited texts, failing to represent
real-world classroom content, including lecture speech with a set of
complementary slides. To bridge this gap, we collect a dataset of educational
questions based on lectures from real-world classrooms. On this realistic
dataset, we find that current methods for EQG struggle with accurately
generating questions from educational videos, particularly in aligning with
specific timestamps and target answers. Common challenges include selecting
informative contexts from extensive transcripts and ensuring generated
questions meaningfully incorporate the target answer. To address the
challenges, we introduce a novel framework utilizing large language models for
dynamically selecting and rewriting contexts based on target timestamps and
answers. First, our framework selects contexts from both lecture transcripts
and video keyframes based on answer relevance and temporal proximity. Then, we
integrate the contexts selected from both modalities and rewrite them into
answer-containing knowledge statements, to enhance the logical connection
between the contexts and the desired answer. This approach significantly
improves the quality and relevance of the generated questions. Our dataset and
code are released in https://github.com/mengxiayu/COSER.

</details>


### [71] [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
*Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang*

Main category: cs.CL

TL;DR: The study investigates the impact of LLM-generated fake news on news recommendation systems, revealing a 'truth decay' phenomenon where fake news outperforms real news in rankings. It also explores causes and potential countermeasures.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-generated fake news poses a threat to news ecosystems, but its large-scale impact on recommendation systems is underexplored.

Method: A simulation pipeline and dataset (~56k generated news items) were used to analyze effects within neural news recommendation systems.

Result: Fake news gradually overtakes real news in rankings ('truth decay'), with perplexity positively correlating to ranking.

Conclusion: The study highlights the threat of LLM-generated fake news and calls for stakeholder action to protect news integrity.

Abstract: Online fake news moderation now faces a new challenge brought by the
malicious use of large language models (LLMs) in fake news production. Though
existing works have shown LLM-generated fake news is hard to detect from an
individual aspect, it remains underexplored how its large-scale release will
impact the news ecosystem. In this study, we develop a simulation pipeline and
a dataset with ~56k generated news of diverse types to investigate the effects
of LLM-generated fake news within neural news recommendation systems. Our
findings expose a truth decay phenomenon, where real news is gradually losing
its advantageous position in news ranking against fake news as LLM-generated
news is involved in news recommendation. We further provide an explanation
about why truth decay occurs from a familiarity perspective and show the
positive correlation between perplexity and news ranking. Finally, we discuss
the threats of LLM-generated fake news and provide possible countermeasures. We
urge stakeholders to address this emerging challenge to preserve the integrity
of news ecosystems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [Can Geometry Save Central Views for Sports Field Registration?](https://arxiv.org/abs/2504.20052)
*Floriane Magera, Thomas Hoyoux, Martin Castin, Olivier Barnich, Anthony Cioppa, Marc Van Droogenbroeck*

Main category: cs.CV

TL;DR: A novel method for sports field registration using circle correspondences to address challenges in close-up camera views.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with close-up views of central field areas due to sparse markings, focusing mainly on lines.

Method: Proposes deriving points and lines from circle correspondences to include them in registration and annotation.

Result: The method complements top detectors, improving registration in difficult scenarios.

Conclusion: The approach successfully leverages circle correspondences for better sports field registration.

Abstract: Single-frame sports field registration often serves as the foundation for
extracting 3D information from broadcast videos, enabling applications related
to sports analytics, refereeing, or fan engagement. As sports fields have
rigorous specifications in terms of shape and dimensions of their line, circle
and point components, sports field markings are commonly used as calibration
targets for this task. However, because of the sparse and uneven distribution
of field markings, close-up camera views around central areas of the field
often depict only line and circle markings. On these views, sports field
registration is challenging for the vast majority of existing methods, as they
focus on leveraging line field markings and their intersections. It is indeed a
challenge to include circle correspondences in a set of linear equations. In
this work, we propose a novel method to derive a set of points and lines from
circle correspondences, enabling the exploitation of circle correspondences for
both sports field registration and image annotation. In our experiments, we
illustrate the benefits of our bottom-up geometric method against
top-performing detectors and show that our method successfully complements
them, enabling sports field registration in difficult scenarios.

</details>


### [73] [Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment](https://arxiv.org/abs/2504.20054)
*Jiayang Sun, Hongbo Wang, Jie Cao, Huaibo Huang, Ran He*

Main category: cs.CV

TL;DR: Marmot is a framework using multi-agent reasoning to improve image-text alignment and multi-object image editing by addressing counting, attributes, and spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes.

Method: Divide-and-conquer strategy with multi-agent editing, decision-execution-verification, and Pixel-Domain Stitching Smoother for subtask integration.

Result: Significant improvements in object counting, attribute assignment, and spatial relationships.

Conclusion: Marmot enhances image generation accuracy and coherence in multi-object scenes.

Abstract: While diffusion models excel at generating high-quality images, they often
struggle with accurate counting, attributes, and spatial relationships in
complex multi-object scenes. To address these challenges, we propose Marmot, a
novel and generalizable framework that employs Multi-Agent Reasoning for
Multi-Object Self-Correcting, enhancing image-text alignment and facilitating
more coherent multi-object image editing. Our framework adopts a
divide-and-conquer strategy that decomposes the self-correction task into three
critical dimensions (counting, attributes, and spatial relationships), and
further divided into object-level subtasks. We construct a multi-agent editing
system featuring a decision-execution-verification mechanism, effectively
mitigating inter-object interference and enhancing editing reliability. To
resolve the problem of subtask integration, we propose a Pixel-Domain Stitching
Smoother that employs mask-guided two-stage latent space optimization. This
innovation enables parallel processing of subtask results, thereby enhancing
runtime efficiency while eliminating multi-stage distortion accumulation.
Extensive experiments demonstrate that Marmot significantly improves accuracy
in object counting, attribute assignment, and spatial relationships for image
generation tasks.

</details>


### [74] [Edge-Based Learning for Improved Classification Under Adversarial Noise](https://arxiv.org/abs/2504.20077)
*Manish Kansana, Keyan Alexander Rahimi, Elias Hossain, Iman Dehzangi, Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: The study explores how adversarial noise affects image classification, testing edge features' resilience. Edge-based models showed greater robustness against adversarial attacks compared to original data.


<details>
  <summary>Details</summary>
Motivation: Adversarial noise misleads deep learning models, reducing accuracy. The study investigates if edge features, which may remain stable under noise, can improve model robustness.

Method: Experiments used brain tumor and COVID datasets. Models were trained on clean images, then tested with adversarial noise (FGSM). Edge-based models were also trained and evaluated.

Result: Edge-based models were more resilient to adversarial noise. Retraining on noisy images improved performance, but edge features provided marginal gains in robustness.

Conclusion: Edge-based learning enhances model resilience against adversarial perturbations, though retraining on original data offers slightly better accuracy improvements.

Abstract: Adversarial noise introduces small perturbations in images, misleading deep
learning models into misclassification and significantly impacting recognition
accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method
(FGSM) adversarial noise on image classification and investigated whether
training on specific image features can improve robustness. We hypothesize that
while adversarial noise perturbs various regions of an image, edges may remain
relatively stable and provide essential structural information for
classification. To test this, we conducted a series of experiments using brain
tumor and COVID datasets. Initially, we trained the models on clean images and
then introduced subtle adversarial perturbations, which caused deep learning
models to significantly misclassify the images. Retraining on a combination of
clean and noisy images led to improved performance. To evaluate the robustness
of the edge features, we extracted edges from the original/clean images and
trained the models exclusively on edge-based representations. When noise was
introduced to the images, the edge-based models demonstrated greater resilience
to adversarial attacks compared to those trained on the original or clean
images. These results suggest that while adversarial noise is able to exploit
complex non-edge regions significantly more than edges, the improvement in the
accuracy after retraining is marginally more in the original data as compared
to the edges. Thus, leveraging edge-based learning can improve the resilience
of deep learning models against adversarial perturbations.

</details>


### [75] [VideoMultiAgents: A Multi-Agent Framework for Video Question Answering](https://arxiv.org/abs/2504.20091)
*Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Masamoto Tanabiki, Kazuki Kozuka, Ehsan Adeli*

Main category: cs.CV

TL;DR: VideoMultiAgents improves VQA by using specialized agents for multimodal reasoning and question-guided captions, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods struggle with temporal and interactive contexts due to reliance on frame-level captions in a single model.

Method: Introduces VideoMultiAgents, a framework with specialized agents for vision, scene graph analysis, and text processing, plus question-guided caption generation.

Result: Achieves SOTA performance: Intent-QA (79.0%, +6.2%), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).

Conclusion: VideoMultiAgents effectively enhances multimodal reasoning in VQA, outperforming existing methods.

Abstract: Video Question Answering (VQA) inherently relies on multimodal reasoning,
integrating visual, temporal, and linguistic cues to achieve a deeper
understanding of video content. However, many existing methods rely on feeding
frame-level captions into a single model, making it difficult to adequately
capture temporal and interactive contexts. To address this limitation, we
introduce VideoMultiAgents, a framework that integrates specialized agents for
vision, scene graph analysis, and text processing. It enhances video
understanding leveraging complementary multimodal reasoning from independently
operating agents. Our approach is also supplemented with a question-guided
caption generation, which produces captions that highlight objects, actions,
and temporal transitions directly relevant to a given query, thus improving the
answer accuracy. Experimental results demonstrate that our method achieves
state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA),
EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).

</details>


### [76] [AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation](https://arxiv.org/abs/2504.20629)
*Jeongsoo Choi, Ji-Hoon Kim, Kim Sung-Bin, Tae-Hyun Oh, Joon Son Chung*

Main category: cs.CV

TL;DR: AlignDiT, a multimodal Aligned Diffusion Transformer, improves speech synthesis from text, video, and audio by addressing intelligibility, synchronization, naturalness, and voice similarity.


<details>
  <summary>Details</summary>
Motivation: The task of multimodal-to-speech generation has broad applications but faces challenges in speech quality and synchronization.

Method: AlignDiT uses aligned multimodal inputs and a novel classifier-free guidance mechanism to balance modality information.

Result: AlignDiT outperforms existing methods in quality, synchronization, and speaker similarity, showing strong generalization.

Conclusion: AlignDiT achieves state-of-the-art performance in multimodal speech synthesis tasks.

Abstract: In this paper, we address the task of multimodal-to-speech generation, which
aims to synthesize high-quality speech from multiple input modalities: text,
video, and reference audio. This task has gained increasing attention due to
its wide range of applications, such as film production, dubbing, and virtual
avatars. Despite recent progress, existing methods still suffer from
limitations in speech intelligibility, audio-video synchronization, speech
naturalness, and voice similarity to the reference speaker. To address these
challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer
that generates accurate, synchronized, and natural-sounding speech from aligned
multimodal inputs. Built upon the in-context learning capability of the DiT
architecture, AlignDiT explores three effective strategies to align multimodal
representations. Furthermore, we introduce a novel multimodal classifier-free
guidance mechanism that allows the model to adaptively balance information from
each modality during speech synthesis. Extensive experiments demonstrate that
AlignDiT significantly outperforms existing methods across multiple benchmarks
in terms of quality, synchronization, and speaker similarity. Moreover,
AlignDiT exhibits strong generalization capability across various multimodal
tasks, such as video-to-speech synthesis and visual forced alignment,
consistently achieving state-of-the-art performance. The demo page is available
at https://mm.kaist.ac.kr/projects/AlignDiT .

</details>


### [77] [Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments](https://arxiv.org/abs/2504.20097)
*Junran Guo, Tonglin Mu, Keyuan Li, Jianing Li, Ziyang Luo, Ye Chen, Xiaodong Fan, Jinquan Huang, Minjie Liu, Jinbei Zhang, Ruoyang Qi, Naiting Gu, Shihai Sun*

Main category: cs.CV

TL;DR: A novel method combining ResNet with D²SP²-LiDAR extends drone detection range to 5 km, achieving high accuracy in pose and type identification, outperforming traditional imaging-based systems.


<details>
  <summary>Details</summary>
Motivation: Detecting small objects like drones over long distances is challenging for security and surveillance. Traditional methods are limited by range, cost, and power, while D²SP²-LiDAR offers a simpler alternative but with limited range.

Method: Integrates residual neural networks (ResNet) with D²SP²-LiDAR and a refined observation model to enhance detection range and accuracy.

Result: Achieves 94.93% pose identification and 97.99% type classification accuracy, even in weak signal conditions, outperforming conventional systems.

Conclusion: Demonstrates the potential of imaging-free methods for robust long-range detection of small targets in real-world scenarios.

Abstract: Detecting small objects, such as drones, over long distances presents a
significant challenge with broad implications for security, surveillance,
environmental monitoring, and autonomous systems. Traditional imaging-based
methods rely on high-resolution image acquisition, but are often constrained by
range, power consumption, and cost. In contrast, data-driven
single-photon-single-pixel light detection and ranging
(\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}) provides an
imaging-free alternative, directly enabling target identification while
reducing system complexity and cost. However, its detection range has been
limited to a few hundred meters. Here, we introduce a novel integration of
residual neural networks (ResNet) with
\text{D\textsuperscript{2}SP\textsuperscript{2}-LiDAR}, incorporating a refined
observation model to extend the detection range to 5~\si{\kilo\meter} in an
intracity environment while enabling high-accuracy identification of drone
poses and types. Experimental results demonstrate that our approach not only
outperforms conventional imaging-based recognition systems, but also achieves
94.93\% pose identification accuracy and 97.99\% type classification accuracy,
even under weak signal conditions with long distances and low signal-to-noise
ratios (SNRs). These findings highlight the potential of imaging-free methods
for robust long-range detection of small targets in real-world scenarios.

</details>


### [78] [Advance Fake Video Detection via Vision Transformers](https://arxiv.org/abs/2504.20669)
*Joy Battocchio, Stefano Dell'Anna, Andrea Montibeller, Giulia Boato*

Main category: cs.CV

TL;DR: The paper proposes a ViT-based framework for detecting AI-generated videos, addressing concerns about misinformation from hyper-realistic media.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated hyper-realistic images and videos poses risks of spreading misinformation, necessitating robust detection methods, especially with new regulations like the European Digital AI Act.

Method: The authors extend Vision Transformer (ViT)-based fake image detection to videos, integrating ViT embeddings over time for improved detection.

Result: The method demonstrates high accuracy, generalization, and few-shot learning on diverse datasets, including videos from open-source and proprietary generative techniques.

Conclusion: The proposed framework effectively addresses the need for reliable AI-generated video detection, offering promising results for combating misinformation.

Abstract: Recent advancements in AI-based multimedia generation have enabled the
creation of hyper-realistic images and videos, raising concerns about their
potential use in spreading misinformation. The widespread accessibility of
generative techniques, which allow for the production of fake multimedia from
prompts or existing media, along with their continuous refinement, underscores
the urgent need for highly accurate and generalizable AI-generated media
detection methods, underlined also by new regulations like the European Digital
AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based
fake image detection and extend this idea to video. We propose an {original}
%innovative framework that effectively integrates ViT embeddings over time to
enhance detection performance. Our method shows promising accuracy,
generalization, and few-shot learning capabilities across a new, large and
diverse dataset of videos generated using five open source generative
techniques from the state-of-the-art, as well as a separate dataset containing
videos produced by proprietary generative methods.

</details>


### [79] [An on-production high-resolution longitudinal neonatal fingerprint database in Brazil](https://arxiv.org/abs/2504.20104)
*Luiz F. P. Southier, Marcelo Filipak, Luiz A. Zanlorensi, Ildefonso Wasilevski, Fabio Favarim, Jefferson T. Oliva, Marcelo Teixeira, Dalcimar Casanova*

Main category: cs.CV

TL;DR: The paper proposes creating a longitudinal neonatal fingerprint dataset to improve biometric identification for newborns, addressing challenges like growth variability.


<details>
  <summary>Details</summary>
Motivation: Accurate neonatal identification is vital for interventions like vaccinations and HIV treatment, but current biometric systems struggle with infant growth variability.

Method: The study designs a high-quality neonatal fingerprint database acquired at multiple early life stages to train machine learning models for growth prediction.

Result: The dataset aims to enable robust Deep Learning models that predict minutiae map changes better than traditional scaling methods.

Conclusion: This work supports the development of reliable biometric systems for newborns by addressing growth-related challenges.

Abstract: The neonatal period is critical for survival, requiring accurate and early
identification to enable timely interventions such as vaccinations, HIV
treatment, and nutrition programs. Biometric solutions offer potential for
child protection by helping to prevent baby swaps, locate missing children, and
support national identity systems. However, developing effective biometric
identification systems for newborns remains a major challenge due to the
physiological variability caused by finger growth, weight changes, and skin
texture alterations during early development. Current literature has attempted
to address these issues by applying scaling factors to emulate growth-induced
distortions in minutiae maps, but such approaches fail to capture the complex
and non-linear growth patterns of infants. A key barrier to progress in this
domain is the lack of comprehensive, longitudinal biometric datasets capturing
the evolution of neonatal fingerprints over time. This study addresses this gap
by focusing on designing and developing a high-quality biometric database of
neonatal fingerprints, acquired at multiple early life stages. The dataset is
intended to support the training and evaluation of machine learning models
aimed at emulating the effects of growth on biometric features. We hypothesize
that such a dataset will enable the development of more robust and accurate
Deep Learning-based models, capable of predicting changes in the minutiae map
with higher fidelity than conventional scaling-based methods. Ultimately, this
effort lays the groundwork for more reliable biometric identification systems
tailored to the unique developmental trajectory of newborns.

</details>


### [80] [Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image](https://arxiv.org/abs/2504.20111)
*Anubhav Jain, Yuya Kobayashi, Naoki Murata, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji, Niv Cohen, Nasir Memon, Julian Togelius*

Main category: cs.CV

TL;DR: The paper proposes a black-box adversarial attack to forge or remove watermarks in diffusion models using perturbations, exposing vulnerabilities in existing watermarking schemes.


<details>
  <summary>Details</summary>
Motivation: To challenge the robustness of current watermarking techniques in diffusion models, which rely on embedding keys in initial noise, by demonstrating their susceptibility to adversarial attacks.

Method: The attack leverages a many-to-one mapping between images and initial noises, using perturbations to forge or remove watermarks without accessing model weights.

Result: The attack successfully compromises multiple watermarking schemes (Tree-Ring, RingID, WIND, Gaussian Shading) across SDv1.4 and SDv2.0, revealing their vulnerabilities.

Conclusion: The findings highlight weaknesses in current watermarking methods, urging the need for more robust solutions in future research.

Abstract: Watermarking techniques are vital for protecting intellectual property and
preventing fraudulent use of media. Most previous watermarking schemes designed
for diffusion models embed a secret key in the initial noise. The resulting
pattern is often considered hard to remove and forge into unrelated images. In
this paper, we propose a black-box adversarial attack without presuming access
to the diffusion model weights. Our attack uses only a single watermarked
example and is based on a simple observation: there is a many-to-one mapping
between images and initial noises. There are regions in the clean image latent
space pertaining to each watermark that get mapped to the same initial noise
when inverted. Based on this intuition, we propose an adversarial attack to
forge the watermark by introducing perturbations to the images such that we can
enter the region of watermarked images. We show that we can also apply a
similar approach for watermark removal by learning perturbations to exit this
region. We report results on multiple watermarking schemes (Tree-Ring, RingID,
WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0).
Our results demonstrate the effectiveness of the attack and expose
vulnerabilities in the watermarking methods, motivating future research on
improving them.

</details>


### [81] [A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals](https://arxiv.org/abs/2504.20178)
*Zhe Cui, Yuli Li, Le-Nam Tran*

Main category: cs.CV

TL;DR: TransFusion is a multimodal crowd-counting model combining CSI and image data using Transformers and CNNs for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Single-modal inputs in crowd counting lead to information loss and poor performance; multimodal fusion is proposed to address this.

Method: Integrates CSI and image data via Transformers for global context and CNNs for local details.

Result: Achieves high accuracy with minimal errors and superior efficiency.

Conclusion: TransFusion effectively combines global and local features for precise crowd counting.

Abstract: Current crowd-counting models often rely on single-modal inputs, such as
visual images or wireless signal data, which can result in significant
information loss and suboptimal recognition performance. To address these
shortcomings, we propose TransFusion, a novel multimodal fusion-based
crowd-counting model that integrates Channel State Information (CSI) with image
data. By leveraging the powerful capabilities of Transformer networks,
TransFusion effectively combines these two distinct data modalities, enabling
the capture of comprehensive global contextual information that is critical for
accurate crowd estimation. However, while transformers are well capable of
capturing global features, they potentially fail to identify finer-grained,
local details essential for precise crowd counting. To mitigate this, we
incorporate Convolutional Neural Networks (CNNs) into the model architecture,
enhancing its ability to extract detailed local features that complement the
global context provided by the Transformer. Extensive experimental evaluations
demonstrate that TransFusion achieves high accuracy with minimal counting
errors while maintaining superior efficiency.

</details>


### [82] [Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies](https://arxiv.org/abs/2504.20203)
*Vladyslav Polushko, Damjan Hatic, Ronald Rösch, Thomas März, Markus Rauhut, Andreas Weinmann*

Main category: cs.CV

TL;DR: The paper explores augmentation strategies for flood detection in RGB imagery using the BlessemFlood21 dataset to improve Deep Learning segmentation networks.


<details>
  <summary>Details</summary>
Motivation: Floods cause global issues, and accurate, timely detection is crucial. Remote Sensing images and Deep Neural Networks are key tools, but effective methods are needed.

Method: The study tests various augmentation strategies, from basic to complex techniques like optical distortion, using the BlessemFlood21 dataset for river flood detection.

Result: The goal is to identify effective augmentation strategies to enhance the training of state-of-the-art Deep Learning segmentation networks.

Conclusion: Refining augmentation techniques can improve flood detection accuracy in RGB imagery, aiding better disaster response.

Abstract: Floods cause serious problems around the world. Responding quickly and
effectively requires accurate and timely information about the affected areas.
The effective use of Remote Sensing images for accurate flood detection
requires specific detection methods. Typically, Deep Neural Networks are
employed, which are trained on specific datasets. For the purpose of river
flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here
explore the use of different augmentation strategies, ranging from basic
approaches to more complex techniques, including optical distortion. By
identifying effective strategies, we aim to refine the training process of
state-of-the-art Deep Learning segmentation networks.

</details>


### [83] [Integration Flow Models](https://arxiv.org/abs/2504.20179)
*Jingjing Wang, Dan Zhang, Joshua Luo, Yin Yang, Feng Luo*

Main category: cs.CV

TL;DR: Integration Flow improves ODE-based generative models by learning integral paths directly, enhancing stability and accuracy, and achieving high-quality one-step generation.


<details>
  <summary>Details</summary>
Motivation: ODE-based generative models face issues like discretization errors and training instability. Integration Flow addresses these by learning integral paths directly and incorporating target state guidance.

Method: Integration Flow learns the integral of ODE-based trajectory paths without solving ODE functions, using the target state as an anchor for reverse-time dynamics.

Result: Integration Flow achieves superior one-step generation performance, with FIDs of 2.86 (CIFAR10) and 4.09 (ImageNet) for VE diffusion, among others.

Conclusion: Integration Flow is a unified, stable, and accurate approach for ODE-based generative models, outperforming existing methods in one-step generation.

Abstract: Ordinary differential equation (ODE) based generative models have emerged as
a powerful approach for producing high-quality samples in many applications.
However, the ODE-based methods either suffer the discretization error of
numerical solvers of ODE, which restricts the quality of samples when only a
few NFEs are used, or struggle with training instability. In this paper, we
proposed Integration Flow, which directly learns the integral of ODE-based
trajectory paths without solving the ODE functions. Moreover, Integration Flow
explicitly incorporates the target state $\mathbf{x}_0$ as the anchor state in
guiding the reverse-time dynamics. We have theoretically proven this can
contribute to both stability and accuracy. To the best of our knowledge,
Integration Flow is the first model with a unified structure to estimate
ODE-based generative models and the first to show the exact straightness of
1-Rectified Flow without reflow. Through theoretical analysis and empirical
evaluations, we show that Integration Flows achieve improved performance when
it is applied to existing ODE-based models, such as diffusion models, Rectified
Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation
on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model,
3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet
with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without
reflow and 4.15 for PFGM++.

</details>


### [84] [Neural Stereo Video Compression with Hybrid Disparity Compensation](https://arxiv.org/abs/2504.20383)
*Shiyin Jiang, Zhenghao Chen, Minghao Han, Xingyu Zhou, Leheng Zhang, Shuhang Gu*

Main category: cs.CV

TL;DR: A hybrid disparity compensation (HDC) strategy combines explicit pixel displacement and implicit cross-attention for stereo video compression, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To exploit cross-view redundancy more effectively in stereo video compression by combining robust prior features with implicit alignment.

Method: HDC fuses horizontally shifted features into a similarity map, normalized for cross-attention, and integrates into a neural framework (HDC-FER and HDC-EM).

Result: Outperforms neural and traditional SVC methods on benchmarks like KITTI 2012, KITTI 2015, and Nagoya.

Conclusion: HDC provides a superior approach for disparity compensation in stereo video compression, validated by extensive experiments.

Abstract: Disparity compensation represents the primary strategy in stereo video
compression (SVC) for exploiting cross-view redundancy. These mechanisms can be
broadly categorized into two types: one that employs explicit horizontal
shifting, and another that utilizes an implicit cross-attention mechanism to
reduce cross-view disparity redundancy. In this work, we propose a hybrid
disparity compensation (HDC) strategy that leverages explicit pixel
displacement as a robust prior feature to simplify optimization and perform
implicit cross-attention mechanisms for subsequent warping operations, thereby
capturing a broader range of disparity information. Specifically, HDC first
computes a similarity map by fusing the horizontally shifted cross-view
features to capture pixel displacement information. This similarity map is then
normalized into an "explicit pixel-wise attention score" to perform the
cross-attention mechanism, implicitly aligning features from one view to
another. Building upon HDC, we introduce a novel end-to-end optimized neural
stereo video compression framework, which integrates HDC-based modules into key
coding operations, including cross-view feature extraction and reconstruction
(HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on
SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both
autonomous driving and general scenes, demonstrate that our framework
outperforms both neural and traditional SVC methodologies.

</details>


### [85] [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
*Juntian Zhang, Chuanqi cheng, Yuhan Liu, Wei Liu, Jian Luan, Rui Yan*

Main category: cs.CV

TL;DR: The paper introduces Focus-Centric Visual Chain, a method to improve vision-language models (VLMs) for multi-image tasks, supported by a new dataset (VISC-150K) and achieving notable performance gains.


<details>
  <summary>Details</summary>
Motivation: Real-world scenarios often involve multi-image inputs, but VLMs struggle with such complexity, leading to performance drops.

Method: Proposes Focus-Centric Visual Chain for better multi-image perception and reasoning, along with Focus-Centric Data Synthesis to create the VISC-150K dataset.

Result: Achieves average gains of 3.16% and 2.24% on seven benchmarks without harming general VLM capabilities.

Conclusion: The work advances VLMs for complex visual scenarios, enhancing robustness and capability.

Abstract: Vision-language models (VLMs) achieve remarkable success in single-image
tasks. However, real-world scenarios often involve intricate multi-image
inputs, leading to a notable performance decline as models struggle to
disentangle critical information scattered across complex visual features. In
this work, we propose Focus-Centric Visual Chain, a novel paradigm that
enhances VLMs'perception, comprehension, and reasoning abilities in multi-image
scenarios. To facilitate this paradigm, we propose Focus-Centric Data
Synthesis, a scalable bottom-up approach for synthesizing high-quality data
with elaborate reasoning paths. Through this approach, We construct VISC-150K,
a large-scale dataset with reasoning data in the form of Focus-Centric Visual
Chain, specifically designed for multi-image tasks. Experimental results on
seven multi-image benchmarks demonstrate that our method achieves average
performance gains of 3.16% and 2.24% across two distinct model architectures,
without compromising the general vision-language capabilities. our study
represents a significant step toward more robust and capable vision-language
systems that can handle complex visual scenarios.

</details>


### [86] [FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations](https://arxiv.org/abs/2504.20222)
*Naoko Sawada, Pedro Miraldo, Suhas Lohit, Tim K. Marks, Moitreya Chatterjee*

Main category: cs.CV

TL;DR: FreBIS introduces a frequency-stratified neural implicit surface representation to improve reconstruction of complex scenes by using dedicated encoders for different frequency levels and promoting feature dissimilarity.


<details>
  <summary>Details</summary>
Motivation: Neural implicit surface methods struggle with complex scenes due to single-encoder limitations. FreBIS aims to address this by stratifying scenes by surface frequency.

Method: FreBIS divides scenes into frequency levels, each encoded by dedicated encoders, and uses a redundancy-aware weighting module to ensure feature dissimilarity.

Result: Empirical tests on BlendedMVS show significant improvements in 3D surface reconstruction and rendering fidelity compared to standard methods.

Conclusion: FreBIS effectively enhances neural implicit surface representation for complex scenes by leveraging frequency stratification and feature dissimilarity.

Abstract: Neural implicit surface representation techniques are in high demand for
advancing technologies in augmented reality/virtual reality, digital twins,
autonomous navigation, and many other fields. With their ability to model
object surfaces in a scene as a continuous function, such techniques have made
remarkable strides recently, especially over classical 3D surface
reconstruction methods, such as those that use voxels or point clouds. However,
these methods struggle with scenes that have varied and complex surfaces
principally because they model any given scene with a single encoder network
that is tasked to capture all of low through high-surface frequency information
in the scene simultaneously. In this work, we propose a novel, neural implicit
surface representation approach called FreBIS to overcome this challenge.
FreBIS works by stratifying the scene based on the frequency of surfaces into
multiple frequency levels, with each level (or a group of levels) encoded by a
dedicated encoder. Moreover, FreBIS encourages these encoders to capture
complementary information by promoting mutual dissimilarity of the encoded
features via a novel, redundancy-aware weighting module. Empirical evaluations
on the challenging BlendedMVS dataset indicate that replacing the standard
encoder in an off-the-shelf neural surface reconstruction method with our
frequency-stratified encoders yields significant improvements. These
enhancements are evident both in the quality of the reconstructed 3D surfaces
and in the fidelity of their renderings from any viewpoint.

</details>


### [87] [Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters](https://arxiv.org/abs/2504.20234)
*Bartosz Ptak, Marek Kraft*

Main category: cs.CV

TL;DR: A point-oriented online tracking algorithm improves drone-based crowd monitoring by enhancing trajectory continuity and counting reliability, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional tracking methods in drone-based crowd monitoring suffer from false positives, false negatives, and identity switches, degrading accuracy and analysis.

Method: The method extends the SORT framework with a point-distance metric, camera motion compensation, altitude-aware assignment, and trajectory validation. It integrates DDCF for efficiency.

Result: Evaluated on DroneCrowd and UP-COUNT-TRACK datasets, the method reduces counting errors to 23% and 15%, respectively, and minimizes identity switches.

Conclusion: The proposed algorithm significantly improves tracking performance, outperforming baseline online trackers and even offline methods.

Abstract: Drone-based crowd monitoring is the key technology for applications in
surveillance, public safety, and event management. However, maintaining
tracking continuity and consistency remains a significant challenge.
Traditional detection-assignment tracking methods struggle with false
positives, false negatives, and frequent identity switches, leading to degraded
counting accuracy and making in-depth analysis impossible. This paper
introduces a point-oriented online tracking algorithm that improves trajectory
continuity and counting reliability in drone-based crowd monitoring. Our method
builds on the Simple Online and Real-time Tracking (SORT) framework, replacing
the original bounding-box assignment with a point-distance metric. The
algorithm is enhanced with three cost-effective techniques: camera motion
compensation, altitude-aware assignment, and classification-based trajectory
validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use
spatial feature maps from localisation algorithms for increased computational
efficiency through neural network resource sharing are integrated to refine
object tracking by reducing noise and handling missed detections. The proposed
method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets,
demonstrating substantial improvements in tracking metrics, reducing counting
errors to 23% and 15%, respectively. The results also indicate a significant
reduction of identity switches while maintaining high tracking accuracy,
outperforming baseline online trackers and even an offline greedy optimisation
method.

</details>


### [88] [Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts](https://arxiv.org/abs/2504.20241)
*Kamirul Kamirul, Odysseas Pappas, Alin Achim*

Main category: cs.CV

TL;DR: A diffusion model is proposed for efficient SAR ship wake simulation, outperforming slow physics-based methods in speed and realism.


<details>
  <summary>Details</summary>
Motivation: Limited annotated data for supervised learning in SAR ship wake detection prompts exploration of faster, end-to-end simulation methods.

Method: A diffusion model is trained on physics-based simulator data paired with text prompts, enabling realistic wake generation.

Result: The model produces realistic Kelvin wake patterns and achieves faster inference than physics-based simulators.

Conclusion: Diffusion models offer fast, controllable wake generation, enhancing end-to-end maritime SAR analysis.

Abstract: Detecting ship presence via wake signatures in SAR imagery is attracting
considerable research interest, but limited annotated data availability poses
significant challenges for supervised learning. Physics-based simulations are
commonly used to address this data scarcity, although they are slow and
constrain end-to-end learning. In this work, we explore a new direction for
more efficient and end-to-end SAR ship wake simulation using a diffusion model
trained on data generated by a physics-based simulator. The training dataset is
built by pairing images produced by the simulator with text prompts derived
from simulation parameters. Experimental result show that the model generates
realistic Kelvin wake patterns and achieves significantly faster inference than
the physics-based simulator. These results highlight the potential of diffusion
models for fast and controllable wake image generation, opening new
possibilities for end-to-end downstream tasks in maritime SAR analysis.

</details>


### [89] [Image Interpolation with Score-based Riemannian Metrics of Diffusion Models](https://arxiv.org/abs/2504.20288)
*Shinnosuke Saito, Takashi Matsubara*

Main category: cs.CV

TL;DR: A framework treats diffusion models' data space as a Riemannian manifold, improving interpolation quality and faithfulness to prompts.


<details>
  <summary>Details</summary>
Motivation: Diffusion models lack practical methods to leverage their learned data manifold, unlike other generative models with latent spaces.

Method: The paper introduces a Riemannian manifold framework for diffusion models, using a metric from the score function.

Result: Experiments on MNIST and Stable Diffusion show more realistic, less noisy, and prompt-faithful interpolations.

Conclusion: The approach enhances content generation and editing, demonstrating the potential of geometry-aware methods.

Abstract: Diffusion models excel in content generation by implicitly learning the data
manifold, yet they lack a practical method to leverage this manifold - unlike
other deep generative models equipped with latent spaces. This paper introduces
a novel framework that treats the data space of pre-trained diffusion models as
a Riemannian manifold, with a metric derived from the score function.
Experiments with MNIST and Stable Diffusion show that this geometry-aware
approach yields image interpolations that are more realistic, less noisy, and
more faithful to prompts than existing methods, demonstrating its potential for
improved content generation and editing.

</details>


### [90] [DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes](https://arxiv.org/abs/2504.20303)
*Junlin Guo, James R. Zimmer-Dauphinee, Jordan M. Nieusma, Siqi Lu, Quan Liu, Ruining Deng, Can Cui, Jialin Yue, Yizhe Lin, Tianyuan Yao, Juming Xiong, Junchao Zhu, Chongyu Qu, Yuechen Yang, Mitchell Wilkes, Xiao Wang, Parker VanValkenburgh, Steven A. Wernke, Yuankai Huo*

Main category: cs.CV

TL;DR: DeepAndes is a transformer-based vision model for Andean archaeology, optimized for multi-spectral imagery, outperforming conventional methods in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges in annotating fine-grained archaeological features at scale and leverage multi-spectral satellite imagery for remote sensing in archaeology.

Method: Introduces DeepAndes, a self-supervised learning model based on DINOv2, trained on 3M multi-spectral images, evaluated on classification, retrieval, and segmentation tasks.

Result: DeepAndes achieves superior performance in few-shot learning, outperforming models trained from scratch or on smaller datasets.

Conclusion: Large-scale self-supervised pre-training is effective for archaeological remote sensing, with DeepAndes as a tailored solution for the Andes.

Abstract: By mapping sites at large scales using remotely sensed data, archaeologists
can generate unique insights into long-term demographic trends, inter-regional
social networks, and past adaptations to climate change. Remote sensing surveys
complement field-based approaches, and their reach can be especially great when
combined with deep learning and computer vision techniques. However,
conventional supervised deep learning methods face challenges in annotating
fine-grained archaeological features at scale. While recent vision foundation
models have shown remarkable success in learning large-scale remote sensing
data with minimal annotations, most off-the-shelf solutions are designed for
RGB images rather than multi-spectral satellite imagery, such as the 8-band
data used in our study. In this paper, we introduce DeepAndes, a
transformer-based vision foundation model trained on three million
multi-spectral satellite images, specifically tailored for Andean archaeology.
DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm
optimized for 8-band multi-spectral imagery, marking the first foundation model
designed explicitly for the Andes region. We evaluate its image understanding
performance through imbalanced image classification, image instance retrieval,
and pixel-level semantic segmentation tasks. Our experiments show that
DeepAndes achieves superior F1 scores, mean average precision, and Dice scores
in few-shot learning scenarios, significantly outperforming models trained from
scratch or pre-trained on smaller datasets. This underscores the effectiveness
of large-scale self-supervised pre-training in archaeological remote sensing.
Codes will be available on https://github.com/geopacha/DeepAndes.

</details>


### [91] [Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis](https://arxiv.org/abs/2504.20306)
*Teja Krishna Cherukuri, Nagur Shareef Shaik, Sribhuvan Reddy Yellu, Jun-Won Chung, Dong Hye Ye*

Main category: cs.CV

TL;DR: DCAN improves colorectal polyp detection by using an attention mechanism for contextual awareness, enhancing interpretability and diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional endoscopic imaging lacks accurate polyp localization and contextual awareness, limiting diagnosis explainability.

Method: Proposes Dynamic Contextual Attention Network (DCAN), transforming spatial representations into adaptive contextual insights with an attention mechanism.

Result: DCAN enhances focus on critical polyp regions, improving interpretability and diagnostic performance.

Conclusion: DCAN's advancements could lead to more reliable colorectal cancer detection and better patient outcomes.

Abstract: Colorectal polyps are key indicators for early detection of colorectal
cancer. However, traditional endoscopic imaging often struggles with accurate
polyp localization and lacks comprehensive contextual awareness, which can
limit the explainability of diagnoses. To address these issues, we propose the
Dynamic Contextual Attention Network (DCAN). This novel approach transforms
spatial representations into adaptive contextual insights, using an attention
mechanism that enhances focus on critical polyp regions without explicit
localization modules. By integrating contextual awareness into the
classification process, DCAN improves decision interpretability and overall
diagnostic performance. This advancement in imaging could lead to more reliable
colorectal cancer detection, enabling better patient outcomes.

</details>


### [92] [Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training](https://arxiv.org/abs/2504.20322)
*Sumit Mamtani, Yash Thesia*

Main category: cs.CV

TL;DR: A unified framework using meta-information and cross-contrastive pre-training improves fine-grained visual classification, outperforming baselines by 7.83% on NABirds.


<details>
  <summary>Details</summary>
Motivation: Appearance alone is insufficient for fine-grained classification; meta-information can enhance accuracy.

Method: Joint learning of visual and meta-information via cross-contrastive pre-training with three encoders (image, text, meta), followed by fine-tuning.

Result: Achieves 84.44% accuracy on NABirds, surpassing baselines by 7.83%.

Conclusion: Meta-information significantly boosts fine-grained recognition, demonstrating the framework's effectiveness.

Abstract: Fine-grained visual classification aims to recognize objects belonging to
multiple subordinate categories within a super-category. However, this remains
a challenging problem, as appearance information alone is often insufficient to
accurately differentiate between fine-grained visual categories. To address
this, we propose a novel and unified framework that leverages meta-information
to assist fine-grained identification. We tackle the joint learning of visual
and meta-information through cross-contrastive pre-training. In the first
stage, we employ three encoders for images, text, and meta-information,
aligning their projected embeddings to achieve better representations. We then
fine-tune the image and meta-information encoders for the classification task.
Experiments on the NABirds dataset demonstrate that our framework effectively
utilizes meta-information to enhance fine-grained recognition performance. With
the addition of meta-information, our framework surpasses the current baseline
on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the
NABirds dataset, outperforming many existing state-of-the-art approaches that
utilize meta-information.

</details>


### [93] [MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation](https://arxiv.org/abs/2504.20343)
*Amaan Izhar, Nurul Japar, Norisma Idris, Ting Dang*

Main category: cs.CV

TL;DR: MicarVLMoE is a vision-language model with gated cross-aligned fusion for medical image reporting, improving fine-grained feature extraction and generalization across diverse imaging types.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained feature extraction, multimodal alignment, and generalization, often limited to chest X-rays.

Method: Proposes MicarVLMoE with a multiscale vision encoder, multihead dual-branch latent attention module, and modulated mixture-of-experts decoder.

Result: Achieves state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets, with improved clinical accuracy and interpretability.

Conclusion: MicarVLMoE addresses key limitations in medical image reporting, demonstrating superior performance and adaptability across imaging types.

Abstract: Medical image reporting (MIR) aims to generate structured clinical
descriptions from radiological images. Existing methods struggle with
fine-grained feature extraction, multimodal alignment, and generalization
across diverse imaging types, often relying on vanilla transformers and
focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language
mixture-of-experts model with gated cross-aligned fusion, designed to address
these limitations. Our architecture includes: (i) a multiscale vision encoder
(MSVE) for capturing anatomical details at varying resolutions, (ii) a
multihead dual-branch latent attention (MDLA) module for vision-language
alignment through latent bottleneck representations, and (iii) a modulated
mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend
MIR to CT scans, retinal imaging, MRI scans, and gross pathology images,
reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets.
Extensive experiments and ablations confirm improved clinical accuracy,
cross-modal alignment, and model interpretability. Code is available at
https://github.com/AI-14/micar-vl-moe.

</details>


### [94] [TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots](https://arxiv.org/abs/2504.20362)
*Qinhua Xie, Hao Tang*

Main category: cs.CV

TL;DR: TTTFusion, a Test-Time Training-based method, dynamically adjusts model parameters during inference to improve multimodal medical image fusion, outperforming traditional methods in accuracy and detail preservation.


<details>
  <summary>Details</summary>
Motivation: Enhancing surgical robots' ability to process multimodal medical images efficiently, addressing challenges in real-time performance, fine-grained feature extraction, and edge preservation.

Method: Introduces TTTFusion, which adapts model parameters during the test phase based on input image data to optimize fusion accuracy and detail preservation.

Result: TTTFusion significantly improves fusion quality, especially in fine-grained feature extraction and edge preservation, compared to traditional methods.

Conclusion: TTTFusion offers a novel solution for real-time image processing in surgical robots, improving both accuracy and detail in multimodal image fusion.

Abstract: With the increasing use of surgical robots in clinical practice, enhancing
their ability to process multimodal medical images has become a key research
challenge. Although traditional medical image fusion methods have made progress
in improving fusion accuracy, they still face significant challenges in
real-time performance, fine-grained feature extraction, and edge
preservation.In this paper, we introduce TTTFusion, a Test-Time Training
(TTT)-based image fusion strategy that dynamically adjusts model parameters
during inference to efficiently fuse multimodal medical images. By adapting the
model during the test phase, our method optimizes the parameters based on the
input image data, leading to improved accuracy and better detail preservation
in the fusion results.Experimental results demonstrate that TTTFusion
significantly enhances the fusion quality of multimodal images compared to
traditional fusion methods, particularly in fine-grained feature extraction and
edge preservation. This approach not only improves image fusion accuracy but
also offers a novel technical solution for real-time image processing in
surgical robots.

</details>


### [95] [Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems](https://arxiv.org/abs/2504.20376)
*Shiqian Zhao, Jiayang Liu, Yiming Li, Runyi Hu, Xiaojun Jia, Wenshu Fan, Xinfeng Li, Jie Zhang, Wei Dong, Tianwei Zhang, Luu Anh Tuan*

Main category: cs.CV

TL;DR: The paper introduces Inception, a multi-turn jailbreak attack exploiting memory mechanisms in text-to-image (T2I) systems like DALL·E 3, achieving higher success rates than prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of security analyses for memory mechanisms in T2I systems, which are vulnerable to jailbreak attacks.

Method: Inception segments unsafe prompts into benign chunks fed turn-by-turn, using recursion to subdivide unsafe words, ensuring semantic consistency.

Result: Inception outperforms state-of-the-art attacks by 14% in success rate on DALL·E 3.

Conclusion: Memory mechanisms in T2I systems pose security risks, and Inception demonstrates their exploitability effectively.

Abstract: Currently, the memory mechanism has been widely and successfully exploited in
online text-to-image (T2I) generation systems ($e.g.$, DALL$\cdot$E 3) for
alleviating the growing tokenization burden and capturing key information in
multi-turn interactions. Despite its practicality, its security analyses have
fallen far behind. In this paper, we reveal that this mechanism exacerbates the
risk of jailbreak attacks. Different from previous attacks that fuse the unsafe
target prompt into one ultimate adversarial prompt, which can be easily
detected or may generate non-unsafe images due to under- or over-optimization,
we propose Inception, the first multi-turn jailbreak attack against the memory
mechanism in real-world text-to-image generation systems. Inception embeds the
malice at the inception of the chat session turn by turn, leveraging the
mechanism that T2I generation systems retrieve key information in their memory.
Specifically, Inception mainly consists of two modules. It first segments the
unsafe prompt into chunks, which are subsequently fed to the system in multiple
turns, serving as pseudo-gradients for directive optimization. Specifically, we
develop a series of segmentation policies that ensure the images generated are
semantically consistent with the target prompt. Secondly, after segmentation,
to overcome the challenge of the inseparability of minimum unsafe words, we
propose recursion, a strategy that makes minimum unsafe words subdivisible.
Collectively, segmentation and recursion ensure that all the request prompts
are benign but can lead to malicious outcomes. We conduct experiments on the
real-world text-to-image generation system ($i.e.$, DALL$\cdot$E 3) to validate
the effectiveness of Inception. The results indicate that Inception surpasses
the state-of-the-art by a 14\% margin in attack success rate.

</details>


### [96] [Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views](https://arxiv.org/abs/2504.20378)
*Jiang Wu, Rui Li, Yu Zhu, Rong Guo, Jinqiu Sun, Yanning Zhang*

Main category: cs.CV

TL;DR: Sparse2DGS improves sparse-view surface reconstruction using MVS-initialized Gaussian Splatting with geometric-prioritized enhancement, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with sparse input views and ill-posed geometric optimization, limiting reconstruction quality.

Method: Proposes Sparse2DGS, combining MVS initialization with Gaussian Splatting and geometric-prioritized enhancement for robust learning.

Result: Outperforms other methods in accuracy and is 2x faster than NeRF-based fine-tuning.

Conclusion: Sparse2DGS offers a superior solution for sparse-view reconstruction by leveraging geometric priors and efficient optimization.

Abstract: We present a Gaussian Splatting method for surface reconstruction using
sparse input views. Previous methods relying on dense views struggle with
extremely sparse Structure-from-Motion points for initialization. While
learning-based Multi-view Stereo (MVS) provides dense 3D points, directly
combining it with Gaussian Splatting leads to suboptimal results due to the
ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS,
an MVS-initialized Gaussian Splatting pipeline for complete and accurate
reconstruction. Our key insight is to incorporate the geometric-prioritized
enhancement schemes, allowing for direct and robust geometric learning under
ill-posed conditions. Sparse2DGS outperforms existing methods by notable
margins while being ${2}\times$ faster than the NeRF-based fine-tuning
approach.

</details>


### [97] [GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting](https://arxiv.org/abs/2504.20379)
*Jongwon Lee, Timothy Bretl*

Main category: cs.CV

TL;DR: A method for query image localization using 3D Gaussian Splatting (3DGS) achieves faster inference and lower error than baselines, tolerating large initial pose errors.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of image localization in 3D scenes, reducing reliance on slow photometric loss minimization.

Method: Uses 3DGS to render synthetic RGBD images, establishes 2D-2D correspondences, lifts to 2D-3D, and solves PnP for pose estimation.

Result: Reduces inference time by over 100x (to 0.1s) and achieves low final pose errors (under 5° rotation, 0.05 translation units) on most datasets.

Conclusion: The method is fast, accurate, and robust to initial pose errors, making it suitable for practical applications.

Abstract: In this paper, we present a method for localizing a query image with respect
to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the
method uses 3DGS to render a synthetic RGBD image at some initial pose
estimate. Second, it establishes 2D-2D correspondences between the query image
and this synthetic image. Third, it uses the depth map to lift the 2D-2D
correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP)
problem to produce a final pose estimate. Results from evaluation across three
existing datasets with 38 scenes and over 2,700 test images show that our
method significantly reduces both inference time (by over two orders of
magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation
error compared to baseline methods that use photometric loss minimization.
Results also show that our method tolerates large errors in the initial pose
estimate of up to 55{\deg} in rotation and 1.1 units in translation (normalized
by scene scale), achieving final pose errors of less than 5{\deg} in rotation
and 0.05 units in translation on 90% of images from the Synthetic NeRF and
Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and
Temples dataset.

</details>


### [98] [FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding](https://arxiv.org/abs/2504.20384)
*Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, Bo Zheng*

Main category: cs.CV

TL;DR: FiLA-Video improves long-video comprehension by dynamically fusing frames, selecting keyframes, and generating training data efficiently.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of redundant inter-frame information and high computational costs in video feature compression for VLLMs.

Method: Uses a lightweight dynamic-weight multi-frame fusion strategy and keyframe selection to preserve essential information while reducing costs.

Result: Achieves superior efficiency and accuracy in long-video comprehension compared to existing methods.

Conclusion: FiLA-Video is an effective solution for enhancing video understanding in VLLMs with reduced computational overhead.

Abstract: Recent advancements in video understanding within visual large language
models (VLLMs) have led to notable progress. However, the complexity of video
data and contextual processing limitations still hinder long-video
comprehension. A common approach is video feature compression to reduce token
input to large language models, yet many methods either fail to prioritize
essential features, leading to redundant inter-frame information, or introduce
computationally expensive modules.To address these issues, we propose
FiLA(Fine-grained Vision Language Model)-Video, a novel framework that
leverages a lightweight dynamic-weight multi-frame fusion strategy, which
adaptively integrates multiple frames into a single representation while
preserving key video information and reducing computational costs. To enhance
frame selection for fusion, we introduce a keyframe selection strategy,
effectively identifying informative frames from a larger pool for improved
summarization. Additionally, we present a simple yet effective long-video
training data generation strategy, boosting model performance without extensive
manual annotation. Experimental results demonstrate that FiLA-Video achieves
superior efficiency and accuracy in long-video comprehension compared to
existing methods.

</details>


### [99] [GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation](https://arxiv.org/abs/2504.20409)
*Jingfeng Guo, Jinnan Chen, Weikai Chen, Zhenyu Sun, Lanjiong Li, Baozhu Zhao, Lingting Zhu, Xin Wang, Qi Liu*

Main category: cs.CV

TL;DR: GarmentX is a framework for generating diverse, high-fidelity 3D garments from a single image, using a structured parametric representation and a masked autoregressive model.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for garment reconstruction often produce implausible results due to unconstrained predictions of 2D patterns. GarmentX aims to ensure valid, editable, and simulation-ready 3D garments.

Method: The framework uses a masked autoregressive model to sequentially predict garment parameters, ensuring structured generation. It also introduces a large-scale dataset (GarmentX dataset) for training.

Result: GarmentX achieves state-of-the-art performance in geometric fidelity and image alignment, outperforming prior methods.

Conclusion: The work presents a robust solution for 3D garment generation, with plans to release the GarmentX dataset to the public.

Abstract: This work presents GarmentX, a novel framework for generating diverse,
high-fidelity, and wearable 3D garments from a single input image. Traditional
garment reconstruction methods directly predict 2D pattern edges and their
connectivity, an overly unconstrained approach that often leads to severe
self-intersections and physically implausible garment structures. In contrast,
GarmentX introduces a structured and editable parametric representation
compatible with GarmentCode, ensuring that the decoded sewing patterns always
form valid, simulation-ready 3D garments while allowing for intuitive
modifications of garment shape and style. To achieve this, we employ a masked
autoregressive model that sequentially predicts garment parameters, leveraging
autoregressive modeling for structured generation while mitigating
inconsistencies in direct pattern prediction. Additionally, we introduce
GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image
pairs, constructed through an automatic data generation pipeline that
synthesizes diverse and high-quality garment images conditioned on parametric
garment representations. Through integrating our method with GarmentX dataset,
we achieve state-of-the-art performance in geometric fidelity and input image
alignment, significantly outperforming prior approaches. We will release
GarmentX dataset upon publication.

</details>


### [100] [Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks](https://arxiv.org/abs/2504.20419)
*Konstantinos I. Roumeliotis, Ranjan Sapkota, Manoj Karkee, Nikolaos D. Tselikas, Dimitrios K. Nasiopoulos*

Main category: cs.CV

TL;DR: Combining GPT-4o with CNNs improves plant disease classification, outperforming ResNet-50 after fine-tuning, but requires minimal training for effective zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: To enhance automated disease detection in agriculture by leveraging multimodal LLMs and CNNs for scalable, intelligent solutions.

Method: Evaluated GPT-4o and ResNet-50 on the PlantVillage dataset across zero-shot, few-shot, and fine-tuning scenarios at varying resolutions and plant species.

Result: Fine-tuned GPT-4o achieved 98.12% accuracy on apple leaves, outperforming ResNet-50 (96.88%), but zero-shot performance was poor.

Conclusion: Multimodal LLMs like GPT-4o show promise for precision agriculture, reducing reliance on large labeled datasets and high-resolution sensors.

Abstract: Automation in agriculture plays a vital role in addressing challenges related
to crop monitoring and disease management, particularly through early detection
systems. This study investigates the effectiveness of combining multimodal
Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural
Networks (CNNs) for automated plant disease classification using leaf imagery.
Leveraging the PlantVillage dataset, we systematically evaluate model
performance across zero-shot, few-shot, and progressive fine-tuning scenarios.
A comparative analysis between GPT-4o and the widely used ResNet-50 model was
conducted across three resolutions (100, 150, and 256 pixels) and two plant
species (apple and corn). Results indicate that fine-tuned GPT-4o models
achieved slightly better performance compared to the performance of ResNet-50,
achieving up to 98.12% classification accuracy on apple leaf images, compared
to 96.88% achieved by ResNet-50, with improved generalization and near-zero
training loss. However, zero-shot performance of GPT-4o was significantly
lower, underscoring the need for minimal training. Additional evaluations on
cross-resolution and cross-plant generalization revealed the models'
adaptability and limitations when applied to new domains. The findings
highlight the promise of integrating multimodal LLMs into automated disease
detection pipelines, enhancing the scalability and intelligence of precision
agriculture systems while reducing the dependence on large, labeled datasets
and high-resolution sensor infrastructure. Large Language Models, Vision
Language Models, LLMs and CNNs, Disease Detection with Vision Language Models,
VLMs

</details>


### [101] [AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries](https://arxiv.org/abs/2504.20435)
*Love Panta, Suraj Prasai, Karishma Malla Vaidya, Shyam Shrestha, Suresh Manandhar*

Main category: cs.CV

TL;DR: An AI-driven system using low-cost microscopes and lightweight UNet/CvT models improves cervical cancer screening accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: High error rates and labor intensity of conventional Liquid-Based Cytology (LBC) necessitate better screening methods.

Method: Combines motorized microscopes with AI for image stitching, cell segmentation (UNet), and classification (CvT).

Result: Outperforms state-of-the-art methods in accuracy and efficiency.

Conclusion: The proposed framework is a viable, cost-effective solution for cervical cancer screening.

Abstract: Cervical cancer remains a significant health challenge, with high incidence
and mortality rates, particularly in transitioning countries. Conventional
Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert
pathologists and is highly prone to errors, highlighting the need for more
efficient screening methods. This paper introduces an innovative approach that
integrates low-cost biological microscopes with our simple and efficient AI
algorithms for automated whole-slide analysis. Our system uses a motorized
microscope to capture cytology images, which are then processed through an AI
pipeline involving image stitching, cell segmentation, and classification. We
utilize the lightweight UNet-based model involving human-in-the-loop approach
to train our segmentation model with minimal ROIs. CvT-based classification
model, trained on the SIPaKMeD dataset, accurately categorizes five cell types.
Our framework offers enhanced accuracy and efficiency in cervical cancer
screening compared to various state-of-art methods, as demonstrated by
different evaluation metrics.

</details>


### [102] [PixelHacker: Image Inpainting with Structural and Semantic Consistency](https://arxiv.org/abs/2504.20438)
*Ziyang Xu, Kangsheng Duan, Xiaolei Shen, Zhifeng Ding, Wenyu Liu, Xiaohu Ruan, Xiaoxin Chen, Xinggang Wang*

Main category: cs.CV

TL;DR: PixelHacker, a diffusion-based model with latent categories guidance, outperforms SOTA in image inpainting by addressing complex structure and semantics.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex structures and semantics in image inpainting, leading to artifacts and logical inconsistencies.

Method: Proposes latent categories guidance and a diffusion-based model (PixelHacker) using a large dataset (14M image-mask pairs) and linear attention for feature injection.

Result: PixelHacker surpasses SOTA on Places2, CelebA-HQ, and FFHQ, showing superior structure and semantic consistency.

Conclusion: PixelHacker offers a simple, effective solution for high-quality image inpainting with robust performance across datasets.

Abstract: Image inpainting is a fundamental research area between image editing and
image generation. Recent state-of-the-art (SOTA) methods have explored novel
attention mechanisms, lightweight architectures, and context-aware modeling,
demonstrating impressive performance. However, they often struggle with complex
structure (e.g., texture, shape, spatial relations) and semantics (e.g., color
consistency, object restoration, and logical correctness), leading to artifacts
and inappropriate generation. To address this challenge, we design a simple yet
effective inpainting paradigm called latent categories guidance, and further
propose a diffusion-based model named PixelHacker. Specifically, we first
construct a large dataset containing 14 million image-mask pairs by annotating
foreground and background (potential 116 and 21 categories, respectively).
Then, we encode potential foreground and background representations separately
through two fixed-size embeddings, and intermittently inject these features
into the denoising process via linear attention. Finally, by pre-training on
our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker.
Extensive experiments show that PixelHacker comprehensively outperforms the
SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits
remarkable consistency in both structure and semantics. Project page at
https://hustvl.github.io/projects/PixelHacker.

</details>


### [103] [LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs](https://arxiv.org/abs/2504.20466)
*Woo Yi Yang, Jiarui Wang, Sijing Wu, Huiyu Duan, Yuxin Zhu, Liu Yang, Kang Fu, Guangtao Zhai, Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces Gen3DHF, a benchmark for assessing AI-generated 3D human faces, and LMME3DHF, a multimodal model for evaluating quality and authenticity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Assessing the quality and realism of AI-generated 3D human faces is challenging due to subjective human perception and sensitivity to facial features.

Method: The authors create Gen3DHF, a large-scale benchmark with 2,000 videos and 4,000 MOS, and propose LMME3DHF, a multimodal model for evaluation.

Result: LMME3DHF achieves state-of-the-art performance in predicting quality scores, identifying distortions, and aligning with human judgments.

Conclusion: The Gen3DHF database and LMME3DHF model will be released, offering tools for improved evaluation of AI-generated 3D human faces.

Abstract: The rapid advancement in generative artificial intelligence have enabled the
creation of 3D human faces (HFs) for applications including media production,
virtual reality, security, healthcare, and game development, etc. However,
assessing the quality and realism of these AI-generated 3D human faces remains
a significant challenge due to the subjective nature of human perception and
innate perceptual sensitivity to facial features. To this end, we conduct a
comprehensive study on the quality assessment of AI-generated 3D human faces.
We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of
AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS)
collected across two dimensions, i.e., quality and authenticity, 2,000
distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF,
we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating
3DHF capable of quality and authenticity score prediction, distortion-aware
visual question answering, and distortion-aware saliency prediction.
Experimental results show that LMME3DHF achieves state-of-the-art performance,
surpassing existing methods in both accurately predicting quality scores for
AI-generated 3D human faces and effectively identifying distortion-aware
salient regions and distortion types, while maintaining strong alignment with
human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be
released upon the publication.

</details>


### [104] [Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception](https://arxiv.org/abs/2504.20468)
*Yuanchen Wu, Lu Zhang, Hang Yao, Junlong Du, Ke Yan, Shouhong Ding, Yunsheng Wu, Xiaoqiang Li*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in LVLMs, introduces Antidote for mitigation, and evaluates performance with CP-Bench.


<details>
  <summary>Details</summary>
Motivation: LVLMs generate counterfactual responses; existing solutions overlook task questions, especially CPQs.

Method: Antidote uses synthetic data for self-correction and decouples mitigation as preference optimization.

Result: Antidote improves CP-Bench by 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%.

Conclusion: Antidote effectively mitigates hallucinations without external supervision or catastrophic forgetting.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive results across
various cross-modal tasks. However, hallucinations, i.e., the models generating
counterfactual responses, remain a challenge. Though recent studies have
attempted to alleviate object perception hallucinations, they focus on the
models' response generation, and overlooking the task question itself. This
paper discusses the vulnerability of LVLMs in solving counterfactual
presupposition questions (CPQs), where the models are prone to accept the
presuppositions of counterfactual objects and produce severe hallucinatory
responses. To this end, we introduce "Antidote", a unified, synthetic
data-driven post-training framework for mitigating both types of hallucination
above. It leverages synthetic data to incorporate factual priors into questions
to achieve self-correction, and decouple the mitigation process into a
preference optimization problem. Furthermore, we construct "CP-Bench", a novel
benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce
factual responses. Applied to the LLaVA series, Antidote can simultaneously
enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR
by 30-50%, all without relying on external supervision from stronger LVLMs or
human feedback and introducing noticeable catastrophic forgetting issues.

</details>


### [105] [Large-scale visual SLAM for in-the-wild videos](https://arxiv.org/abs/2504.20496)
*Shuo Sun, Torsten Sattler, Malcolm Mielle, Achim J. Lilienthal, Martin Magnusson*

Main category: cs.CV

TL;DR: A robust pipeline improves 3D scene reconstruction from casual videos by addressing challenges like uncontrolled motion, textureless regions, and dynamic objects.


<details>
  <summary>Details</summary>
Motivation: Reliable 3D reconstruction from unconstrained videos is challenging but crucial for robot deployment in new environments.

Method: The pipeline combines deep visual odometry, automatic intrinsics recovery, dynamic object masking, monocular depth regularization, and loop closure.

Result: Produces large-scale contiguous 3D models with better consistency and accuracy than baseline methods.

Conclusion: The system sets a new benchmark for visual reconstruction from uncontrolled videos, outperforming existing methods.

Abstract: Accurate and robust 3D scene reconstruction from casual, in-the-wild videos
can significantly simplify robot deployment to new environments. However,
reliable camera pose estimation and scene reconstruction from such
unconstrained videos remains an open challenge. Existing visual-only SLAM
methods perform well on benchmark datasets but struggle with real-world footage
which often exhibits uncontrolled motion including rapid rotations and pure
forward movements, textureless regions, and dynamic objects. We analyze the
limitations of current methods and introduce a robust pipeline designed to
improve 3D reconstruction from casual videos. We build upon recent deep visual
odometry methods but increase robustness in several ways. Camera intrinsics are
automatically recovered from the first few frames using structure-from-motion.
Dynamic objects and less-constrained areas are masked with a predictive model.
Additionally, we leverage monocular depth estimates to regularize bundle
adjustment, mitigating errors in low-parallax situations. Finally, we integrate
place recognition and loop closure to reduce long-term drift and refine both
intrinsics and pose estimates through global bundle adjustment. We demonstrate
large-scale contiguous 3D models from several online videos in various
environments. In contrast, baseline methods typically produce locally
inconsistent results at several points, producing separate segments or
distorted maps. In lieu of ground-truth pose data, we evaluate map consistency,
execution time and visual accuracy of re-rendered NeRF models. Our proposed
system establishes a new baseline for visual reconstruction from casual
uncontrolled videos found online, demonstrating more consistent reconstructions
over longer sequences of in-the-wild videos than previously achieved.

</details>


### [106] [Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection](https://arxiv.org/abs/2504.20498)
*Jianhong Han, Yupei Wang, Liang Chen*

Main category: cs.CV

TL;DR: SA-DETR, a DETR-based detector, improves single-source domain generalization in object detection by dynamically adapting to unseen domains and using contrastive learning for domain-invariant features.


<details>
  <summary>Details</summary>
Motivation: Existing CNN-based methods rely on data augmentation, which may not generalize well to all unseen domains. DETR's potential for SDG is unexplored.

Method: Introduces SA-DETR with a domain style adapter for dynamic style adaptation and an object-aware contrastive learning module for domain-invariant features.

Result: SA-DETR shows superior performance and generalization across five weather scenarios.

Conclusion: SA-DETR effectively addresses SDG challenges in object detection, outperforming existing methods.

Abstract: Single-source Domain Generalization (SDG) in object detection aims to develop
a detector using only data from a source domain that can exhibit strong
generalization capability when applied to unseen target domains. Existing
methods are built upon CNN-based detectors and primarily improve robustness by
employing carefully designed data augmentation strategies integrated with
feature alignment techniques. However, data augmentation methods have inherent
drawbacks; they are only effective when the augmented sample distribution
approximates or covers the unseen scenarios, thus failing to enhance
generalization across all unseen domains. Furthermore, while the recent
Detection Transformer (DETR) has demonstrated superior generalization
capability in domain adaptation tasks due to its efficient global information
extraction, its potential in SDG tasks remains unexplored. To this end, we
introduce a strong DETR-based detector named the Style-Adaptive Detection
Transformer (SA-DETR) for SDG in object detection. Specifically, we present a
domain style adapter that projects the style representation of the unseen
target domain into the training domain, enabling dynamic style adaptation.
Then, we propose an object-aware contrastive learning module to guide the
detector in extracting domain-invariant features through contrastive learning.
By using object-aware gating masks to constrain feature aggregation in both
spatial and semantic dimensions, this module achieves cross-domain contrast of
instance-level features, thereby enhancing generalization. Extensive
experiments demonstrate the superior performance and generalization capability
of SA-DETR across five different weather scenarios. Code is released at
https://github.com/h751410234/SA-DETR.

</details>


### [107] [MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification](https://arxiv.org/abs/2504.20509)
*Yichu Xu, Di Wang, Hongzan Jiao, Lefei Zhang, Liangpei Zhang*

Main category: cs.CV

TL;DR: MambaMoE introduces a spectral-spatial mixture-of-experts framework for HSI classification, addressing limitations of existing Mamba-based methods by incorporating adaptive modeling and uncertainty-guided learning.


<details>
  <summary>Details</summary>
Motivation: Existing Mamba-based methods for HSI classification overlook spectral and spatial directional characteristics, limiting performance.

Method: Proposes MambaMoE with a Mixture of Mamba Expert Block (MoMEB) for adaptive spectral-spatial modeling and an uncertainty-guided corrective learning (UGCL) strategy.

Result: Achieves state-of-the-art performance in accuracy and efficiency on multiple HSI benchmarks.

Conclusion: MambaMoE outperforms existing methods, especially Mamba-based ones, and code will be released.

Abstract: The Mamba model has recently demonstrated strong potential in hyperspectral
image (HSI) classification, owing to its ability to perform context modeling
with linear computational complexity. However, existing Mamba-based methods
usually neglect the spectral and spatial directional characteristics related to
heterogeneous objects in hyperspectral scenes, leading to limited
classification performance. To address these issues, we propose MambaMoE, a
novel spectral-spatial mixture-of-experts framework, representing the first
MoE-based approach in the HSI classification community. Specifically, we design
a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation
to enable adaptive spectral-spatial modeling. Furthermore, we introduce an
uncertainty-guided corrective learning (UGCL) strategy to encourage the model's
attention toward complex regions prone to prediction ambiguity. Extensive
experiments on multiple public HSI benchmarks demonstrate that MambaMoE
achieves state-of-the-art performance in both accuracy and efficiency compared
to existing advanced approaches, especially for Mamba-based methods. Code will
be released.

</details>


### [108] [SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects](https://arxiv.org/abs/2504.20510)
*Irina Ruzavina, Lisa Sophie Theis, Jesse Lemeer, Rutger de Groen, Leo Ebeling, Andrej Hulak, Jouaria Ali, Guangzhi Tang, Rico Mockel*

Main category: cs.CV

TL;DR: A study introduces a dataset of 1654 labeled steel surface images for quality control, evaluating three classification methods (CCT, SVM, CAE) with CCT and SVM achieving 95% accuracy. The work supports automated defect detection and interpretable models.


<details>
  <summary>Details</summary>
Motivation: To improve manufacturing efficiency by automating quality control of shot-blasted steel surfaces using computer vision.

Method: Three approaches were tested: Compact Convolutional Transformers (CCT), SVM with ResNet-50 features, and a Convolutional Autoencoder (CAE).

Result: CCT and SVM achieved 95% accuracy, while CAE provided a baseline for unsupervised methods. Models offer interpretable decision-making.

Conclusion: The dataset and codes are released to advance defect detection research and promote automated inspection in industry.

Abstract: Automating the quality control of shot-blasted steel surfaces is crucial for
improving manufacturing efficiency and consistency. This study presents a
dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as
either "ready for paint" or "needs shot-blasting." The dataset captures
real-world surface defects, including discoloration, welding lines, scratches
and corrosion, making it well-suited for training computer vision models.
Additionally, three classification approaches were evaluated: Compact
Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50
feature extraction, and a Convolutional Autoencoder (CAE). The supervised
methods (CCT and SVM) achieve 95% classification accuracy on the test set, with
CCT leveraging transformer-based attention mechanisms and SVM offering a
computationally efficient alternative. The CAE approach, while less effective,
establishes a baseline for unsupervised quality control. We present
interpretable decision-making by all three neural networks, allowing industry
users to visually pinpoint problematic regions and understand the model's
rationale. By releasing the dataset and baseline codes, this work aims to
support further research in defect detection, advance the development of
interpretable computer vision models for quality control, and encourage the
adoption of automated inspection systems in industrial applications.

</details>


### [109] [Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models](https://arxiv.org/abs/2504.20518)
*Zhongqi Wang, Jie Zhang, Shiguang Shan, Xilin Chen*

Main category: cs.CV

TL;DR: The paper introduces Dynamic Attention Analysis (DAA) for detecting backdoor attacks in text-to-image diffusion models by analyzing dynamic cross-attention map evolution, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor detection methods ignore the dynamic nature of diffusion models, which can be leveraged for more effective detection.

Method: Proposes DAA-I (spatially independent attention maps) and DAA-S (graph-based dynamical system) to quantify dynamic anomalies in attention maps.

Result: Achieves an average F1 Score of 79.49% and AUC of 87.67% across five backdoor attack scenarios.

Conclusion: DAA provides a robust framework for detecting backdoor attacks by exploiting dynamic attention patterns, significantly improving detection performance.

Abstract: Recent studies have revealed that text-to-image diffusion models are
vulnerable to backdoor attacks, where attackers implant stealthy textual
triggers to manipulate model outputs. Previous backdoor detection methods
primarily focus on the static features of backdoor samples. However, a vital
property of diffusion models is their inherent dynamism. This study introduces
a novel backdoor detection perspective named Dynamic Attention Analysis (DAA),
showing that these dynamic characteristics serve as better indicators for
backdoor detection. Specifically, by examining the dynamic evolution of
cross-attention maps, we observe that backdoor samples exhibit distinct feature
evolution patterns at the $<$EOS$>$ token compared to benign samples. To
quantify these dynamic anomalies, we first introduce DAA-I, which treats the
tokens' attention maps as spatially independent and measures dynamic feature
using the Frobenius norm. Furthermore, to better capture the interactions
between attention maps and refine the feature, we propose a dynamical
system-based approach, referred to as DAA-S. This model formulates the spatial
correlations among attention maps using a graph-based state equation and we
theoretically analyze the global asymptotic stability of this method. Extensive
experiments across five representative backdoor attack scenarios demonstrate
that our approach significantly surpasses existing detection methods, achieving
an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at
https://github.com/Robin-WZQ/DAA.

</details>


### [110] [Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection](https://arxiv.org/abs/2504.20525)
*Huan Zheng, Wencheng Han, Tianyi Yan, Cheng-zhong Xu, Jianbing Shen*

Main category: cs.CV

TL;DR: GTA-Net improves monocular 3D lane detection by leveraging temporal geometric consistency and instance information from multiple frames, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current monocular 3D lane detection methods lack accurate geometry and struggle with lane integrity. Using multiple frames can address these issues.

Method: Proposes GTA-Net with two modules: TGEM for geometric consistency and TIQG for instance-aware query generation using temporal cues.

Result: GTA-Net outperforms existing methods, achieving state-of-the-art performance.

Conclusion: Leveraging temporal information enhances both geometry perception and lane integrity in monocular 3D lane detection.

Abstract: Monocular 3D lane detection aims to estimate 3D position of lanes from
frontal-view (FV) images. However, current monocular 3D lane detection methods
suffer from two limitations, including inaccurate geometric information of the
predicted 3D lanes and difficulties in maintaining lane integrity. To address
these issues, we seek to fully exploit the potential of multiple input frames.
First, we aim at enhancing the ability to perceive the geometry of scenes by
leveraging temporal geometric consistency. Second, we strive to improve the
integrity of lanes by revealing more instance information from temporal
sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation
Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the
Temporal Geometry Enhancement Module (TGEM), which exploits geometric
consistency across successive frames, facilitating effective geometry
perception. On the other hand, we present the Temporal Instance-aware Query
Generation (TIQG), which strategically incorporates temporal cues into query
generation, thereby enabling the exploration of comprehensive instance
information. Experiments demonstrate that our GTA-Net achieves SoTA results,
surpassing existing monocular 3D lane detection solutions.

</details>


### [111] [Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer](https://arxiv.org/abs/2504.20530)
*Wenxuan Liu, Xian Zhong, Zhuo Zhou, Siyuan Yang, Chia-Wen Lin, Alex Chichung Kot*

Main category: cs.CV

TL;DR: POG-MVNet improves UAV action recognition by modeling hierarchical view variations and leveraging partial order among views, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Action recognition in UAVs is challenging due to view variations from different altitudes, leading to appearance discrepancies.

Method: Proposes POG-MVNet with View Partition, Order-aware Feature Decoupling, and Action Partial Order Guide modules to address view variations.

Result: Achieves 4.7% and 3.5% improvements on Drone-Action and UAV datasets, respectively, over ASAT and FAR.

Conclusion: POG-MVNet effectively addresses UAV action recognition challenges by leveraging view hierarchy and partial order, demonstrating superior performance.

Abstract: Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges
due to significant view variations along the vertical spatial axis. Unlike
traditional ground-based settings, UAVs capture actions from a wide range of
altitudes, resulting in considerable appearance discrepancies. We introduce a
multi-view formulation tailored to varying UAV altitudes and empirically
observe a partial order among views, where recognition accuracy consistently
decreases as the altitude increases. This motivates a novel approach that
explicitly models the hierarchical structure of UAV views to improve
recognition performance across altitudes. To this end, we propose the Partial
Order Guided Multi-View Network (POG-MVNet), designed to address drastic view
variations by effectively leveraging view-dependent information across
different altitude levels. The framework comprises three key components: a View
Partition (VP) module, which uses the head-to-body ratio to group views by
altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles
action-relevant and view-specific features under partial order guidance; and an
Action Partial Order Guide (APOG), which leverages the partial order to
transfer informative knowledge from easier views to support learning in more
challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV
datasets, demonstrating that POG-MVNet significantly outperforms competing
methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action
dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art
methods ASAT and FAR. The code for POG-MVNet will be made available soon.

</details>


### [112] [Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study](https://arxiv.org/abs/2504.20541)
*Daniele Pannone, Danilo Avola*

Main category: cs.CV

TL;DR: A deep learning framework uses WiFi CSI data to generate point clouds via a two-stage autoencoder, achieving accurate environmental reconstruction.


<details>
  <summary>Details</summary>
Motivation: To leverage WiFi Channel State Information (CSI) for environmental mapping and wireless sensing applications.

Method: Two-stage autoencoder: PointNet for point cloud generation and CNN for mapping CSI data to a latent space, aligned for reconstruction.

Result: Experimental results confirm the method's effectiveness in accurate point cloud generation from WiFi data.

Conclusion: The framework shows promise for wireless sensing and environmental mapping using WiFi CSI data.

Abstract: This paper introduces a deep learning framework for generating point clouds
from WiFi Channel State Information data. We employ a two-stage autoencoder
approach: a PointNet autoencoder with convolutional layers for point cloud
generation, and a Convolutional Neural Network autoencoder to map CSI data to a
matching latent space. By aligning these latent spaces, our method enables
accurate environmental point cloud reconstruction from WiFi data. Experimental
results validate the effectiveness of our approach, highlighting its potential
for wireless sensing and environmental mapping applications.

</details>


### [113] [PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders](https://arxiv.org/abs/2504.20599)
*Qiaochu Wang, Chufeng Xiao, Manfred Lau, Hongbo Fu*

Main category: cs.CV

TL;DR: PartHOI introduces a part-based method for transferring hand-object interactions (HOI) across categories by leveraging semantic parts and size-invariant correspondences, outperforming existing shape-matching methods.


<details>
  <summary>Details</summary>
Motivation: Current HOI transfer methods rely on shape matching, limiting cross-category transfers due to shape and size differences. Semantic parts of objects, however, offer more consistent geometry for robust transfers.

Method: PartHOI uses a generalized cylinder representation to parameterize object parts, establishing geometric correspondences and transferring contact points. Hand poses are then optimized to fit the target object.

Result: The method generalizes well for cross-category HOI transfers, producing high-fidelity results superior to existing approaches, as shown by qualitative and quantitative evaluations.

Conclusion: PartHOI effectively addresses limitations of shape-matching methods by focusing on semantic parts, enabling robust cross-category HOI transfers with high accuracy.

Abstract: Learning-based methods to understand and model hand-object interactions (HOI)
require a large amount of high-quality HOI data. One way to create HOI data is
to transfer hand poses from a source object to another based on the objects'
geometry. However, current methods for transferring hand poses between objects
rely on shape matching, limiting the ability to transfer poses across different
categories due to differences in their shapes and sizes. We observe that HOI
often involves specific semantic parts of objects, which often have more
consistent shapes across categories. In addition, constructing size-invariant
correspondences between these parts is important for cross-category transfer.
Based on these insights, we introduce a novel method PartHOI for part-based HOI
transfer. Using a generalized cylinder representation to parameterize an object
parts' geometry, PartHOI establishes a robust geometric correspondence between
object parts, and enables the transfer of contact points. Given the transferred
points, we optimize a hand pose to fit the target object well. Qualitative and
quantitative results demonstrate that our method can generalize HOI transfers
well even for cross-category objects, and produce high-fidelity results that
are superior to the existing methods.

</details>


### [114] [Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection](https://arxiv.org/abs/2504.20602)
*Siwei Wang, Zhiwei Chen, Liujuan Cao, Rongrong Ji*

Main category: cs.CV

TL;DR: PLUSNet optimizes small object detection by holistically improving feature purification, label assignment, and information utilization, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Previous methods focused on isolated pipeline stages, limiting overall performance. PLUSNet addresses this by optimizing the entire pipeline.

Method: PLUSNet introduces three modules: Hierarchical Feature Purifier (HFP), Multiple Criteria Label Assignment (MCLA), and Frequency Decoupled Head (FDHead).

Result: Experiments show PLUSNet achieves significant improvements in small object detection across multiple datasets.

Conclusion: PLUSNet's holistic approach enhances detection capabilities and is adaptable to various object detectors.

Abstract: Small object detection is a broadly investigated research task and is
commonly conceptualized as a "pipeline-style" engineering process. In the
upstream, images serve as raw materials for processing in the detection
pipeline, where pre-trained models are employed to generate initial feature
maps. In the midstream, an assigner selects training positive and negative
samples. Subsequently, these samples and features are fed into the downstream
for classification and regression. Previous small object detection methods
often focused on improving isolated stages of the pipeline, thereby neglecting
holistic optimization and consequently constraining overall performance gains.
To address this issue, we have optimized three key aspects, namely Purifying,
Labeling, and Utilizing, in this pipeline, proposing a high-quality Small
object detection framework termed PLUSNet. Specifically, PLUSNet comprises
three sequential components: the Hierarchical Feature Purifier (HFP) for
purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for
improving the quality of midstream training samples, and the Frequency
Decoupled Head (FDHead) for more effectively exploiting information to
accomplish downstream tasks. The proposed PLUS modules are readily integrable
into various object detectors, thus enhancing their detection capabilities in
multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet
consistently achieves significant and consistent improvements across multiple
datasets for small object detection.

</details>


### [115] [EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian](https://arxiv.org/abs/2504.20607)
*Hao Tian, Rui Liu, Wen Shen, Yilong Hu, Zhihao Zheng, Xiaolin Qin*

Main category: cs.CV

TL;DR: EfficientHuman improves dynamic human body reconstruction using Articulated 2D Gaussian surfels, achieving faster training and better rendering quality than 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of 3D Gaussian Splatting (3DGS) in dynamic human body reconstruction, such as multi-view inconsistency and redundant Gaussians.

Method: Uses Articulated 2D Gaussian surfels in canonical space, transformed via Linear Blend Skinning (LBS), with pose calibration and LBS optimization modules.

Result: Achieves rapid reconstruction (<1 minute, 20s faster than SOTA) with fewer redundant Gaussians on the ZJU-MoCap dataset.

Conclusion: EfficientHuman efficiently reconstructs dynamic human bodies with high rendering quality, outperforming 3DGS in speed and accuracy.

Abstract: 3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in
scene reconstruction and novel view synthesis. Recent work on reconstructing
the 3D human body using 3DGS attempts to leverage prior information on human
pose to enhance rendering quality and improve training speed. However, it
struggles to effectively fit dynamic surface planes due to multi-view
inconsistency and redundant Gaussians. This inconsistency arises because
Gaussian ellipsoids cannot accurately represent the surfaces of dynamic
objects, which hinders the rapid reconstruction of the dynamic human body.
Meanwhile, the prevalence of redundant Gaussians means that the training time
of these works is still not ideal for quickly fitting a dynamic human body. To
address these, we propose EfficientHuman, a model that quickly accomplishes the
dynamic reconstruction of the human body using Articulated 2D Gaussian while
ensuring high rendering quality. The key innovation involves encoding Gaussian
splats as Articulated 2D Gaussian surfels in canonical space and then
transforming them to pose space via Linear Blend Skinning (LBS) to achieve
efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian
surfels can quickly conform to the dynamic human body while ensuring
view-consistent geometries. Additionally, we introduce a pose calibration
module and an LBS optimization module to achieve precise fitting of dynamic
human poses, enhancing the model's performance. Extensive experiments on the
ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic
human reconstruction in less than a minute on average, which is 20 seconds
faster than the current state-of-the-art method, while also reducing the number
of redundant Gaussians.

</details>


### [116] [LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping](https://arxiv.org/abs/2504.20645)
*Weiqin Jiao, Hao Cheng, George Vosselman, Claudio Persello*

Main category: cs.CV

TL;DR: LDPoly is a novel framework for extracting polygonal road outlines from aerial images using a Dual-Latent Diffusion Model, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for polygonal outline extraction are not designed for roads, which have unique features like branching and connectivity. LDPoly fills this gap.

Method: LDPoly uses a Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module to generate road masks and vertex heatmaps, followed by a tailored polygonization method.

Result: LDPoly excels in pixel coverage, vertex efficiency, polygon regularity, and connectivity, validated on the Map2ImLas dataset. New metrics for simplicity and smoothness are introduced.

Conclusion: LDPoly is the first diffusion-based method for precise road polygon extraction, setting a foundation for future advancements in remote-sensing imagery.

Abstract: Polygonal road outline extraction from high-resolution aerial images is an
important task in large-scale topographic mapping, where roads are represented
as vectorized polygons, capturing essential geometric features with minimal
vertex redundancy. Despite its importance, no existing method has been
explicitly designed for this task. While polygonal building outline extraction
has been extensively studied, the unique characteristics of roads, such as
branching structures and topological connectivity, pose challenges to these
methods. To address this gap, we introduce LDPoly, the first dedicated
framework for extracting polygonal road outlines from high-resolution aerial
images. Our method leverages a novel Dual-Latent Diffusion Model with a
Channel-Embedded Fusion Module, enabling the model to simultaneously generate
road masks and vertex heatmaps. A tailored polygonization method is then
applied to obtain accurate vectorized road polygons with minimal vertex
redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which
contains detailed polygonal annotations for various topographic objects in
several Dutch regions. Our experiments include both in-region and cross-region
evaluations, with the latter designed to assess the model's generalization
performance on unseen regions. Quantitative and qualitative results demonstrate
that LDPoly outperforms state-of-the-art polygon extraction methods across
various metrics, including pixel-level coverage, vertex efficiency, polygon
regularity, and road connectivity. We also design two new metrics to assess
polygon simplicity and boundary smoothness. Moreover, this work represents the
first application of diffusion models for extracting precise vectorized object
outlines without redundant vertices from remote-sensing imagery, paving the way
for future advancements in this field.

</details>


### [117] [SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data](https://arxiv.org/abs/2504.20648)
*Michael Ogezi, Freda Shi*

Main category: cs.CV

TL;DR: VLMs struggle with spatial reasoning due to underrepresented relations in datasets. A synthetic VQA dataset (455k samples) was created to enhance spatial reasoning, improving VLM performance by up to 49%.


<details>
  <summary>Details</summary>
Motivation: Address the gap in VLMs' spatial reasoning, a key skill for real-world applications like robotics and navigation.

Method: Constructed a synthetic VQA dataset from hyper-detailed image descriptions to train Spatial-Reasoning Enhanced (SpaRE) VLMs.

Result: SpaRE VLMs achieved up to 49% performance gain on spatial reasoning benchmarks while maintaining general task performance.

Conclusion: The work narrows the gap between human and VLM spatial reasoning, enhancing VLMs' real-world applicability.

Abstract: Vision-language models (VLMs) work well in tasks ranging from image
captioning to visual question answering (VQA), yet they struggle with spatial
reasoning, a key skill for understanding our physical world that humans excel
at. We find that spatial relations are generally rare in widely used VL
datasets, with only a few being well represented, while most form a long tail
of underrepresented relations. This gap leaves VLMs ill-equipped to handle
diverse spatial relationships. To bridge it, we construct a synthetic VQA
dataset focused on spatial reasoning generated from hyper-detailed image
descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset
consists of 455k samples containing 3.4 million QA pairs. Trained on this
dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements
on spatial reasoning benchmarks, achieving up to a 49% performance gain on the
What's Up benchmark, while maintaining strong results on general tasks. Our
work narrows the gap between human and VLM spatial reasoning and makes VLMs
more capable in real-world tasks such as robotics and navigation.

</details>


### [118] [Image deidentification in the XNAT ecosystem: use cases and solutions](https://arxiv.org/abs/2504.20657)
*Alex Michie, Simon J Doran*

Main category: cs.CV

TL;DR: XNAT's deidentification workflow for DICOM data achieved 99.61% accuracy in the MIDI-B challenge, with room for improvement in address removal and pixel data handling.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective deidentification of DICOM images in research, leveraging XNAT's capabilities and independent tools.

Method: Combined pre-existing methodologies with adaptations during the MIDI-B challenge, using rule-based and machine-learning approaches.

Result: Initial test score was 97.91%, improved to 99.61% post-submission, with minor degradation (99.54%) from machine-learning models.

Conclusion: Future work will focus on better address recognition and handling of pixel data, with current failure rate at 0.19%.

Abstract: XNAT is a server-based data management platform widely used in academia for
curating large databases of DICOM images for research projects. We describe in
detail a deidentification workflow for DICOM data using facilities in XNAT,
together with independent tools in the XNAT "ecosystem". We list different
contexts in which deidentification might be needed, based on our prior
experience. The starting point for participation in the Medical Image
De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local
methodologies, which were adapted during the validation phase of the challenge.
Our result in the test phase was 97.91\%, considerably lower than our peers,
due largely to an arcane technical incompatibility of our methodology with the
challenge's Synapse platform, which prevented us receiving feedback during the
validation phase. Post-submission, additional discrepancy reports from the
organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to
improve this score significantly to 99.61\%. An entirely rule-based approach
was shown to be capable of removing all name-related information in the test
corpus, but exhibited failures in dealing fully with address data. Initial
experiments using published machine-learning models to remove addresses were
partially successful but showed the models to be "over-aggressive" on other
types of free-text data, leading to a slight overall degradation in performance
to 99.54\%. Future development will therefore focus on improving
address-recognition capabilities, but also on better removal of identifiable
data burned into the image pixels. Several technical aspects relating to the
"answer key" are still under discussion with the challenge organisers, but we
estimate that our percentage of genuine deidentification failures on the MIDI-B
test corpus currently stands at 0.19\%. (Abridged from original for arXiv
submission)

</details>


### [119] [FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection](https://arxiv.org/abs/2504.20670)
*Yao Xiao, Tingfa Xu, Yu Xin, Jianan Li*

Main category: cs.CV

TL;DR: FBRT-YOLO introduces lightweight modules (FCM and MKP) to improve small target detection in aerial images, balancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the imbalance between detection accuracy and efficiency in small target detection for real-time aerial image applications.

Method: Uses Feature Complementary Mapping Module (FCM) for spatial information integration and Multi-Kernel Perception Unit (MKP) for multi-scale target perception.

Result: Outperforms other real-time detectors on datasets like Visdrone, UAVDT, and AI-TOD.

Conclusion: FBRT-YOLO effectively enhances small target detection in aerial images while maintaining real-time performance.

Abstract: Embedded flight devices with visual capabilities have become essential for a
wide range of applications. In aerial image detection, while many existing
methods have partially addressed the issue of small target detection,
challenges remain in optimizing small target detection and balancing detection
accuracy with efficiency. These issues are key obstacles to the advancement of
real-time aerial image detection. In this paper, we propose a new family of
real-time detectors for aerial image detection, named FBRT-YOLO, to address the
imbalance between detection accuracy and efficiency. Our method comprises two
lightweight modules: Feature Complementary Mapping Module (FCM) and
Multi-Kernel Perception Unit(MKP), designed to enhance object perception for
small targets in aerial images. FCM focuses on alleviating the problem of
information imbalance caused by the loss of small target information in deep
networks. It aims to integrate spatial positional information of targets more
deeply into the network,better aligning with semantic information in the deeper
layers to improve the localization of small targets. We introduce MKP, which
leverages convolutions with kernels of different sizes to enhance the
relationships between targets of various scales and improve the perception of
targets at different scales. Extensive experimental results on three major
aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that
FBRT-YOLO outperforms various real-time detectors in terms of performance and
speed.

</details>


### [120] [Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset](https://arxiv.org/abs/2504.20677)
*Paola Natalia Cañas, Alexander Diez, David Galvañ, Marcos Nieto, Igor Rodríguez*

Main category: cs.CV

TL;DR: A robust driver monitoring system (DMS) using RGB and IR images for driver identification, gaze estimation, and occlusion detection, validated on the Driver Monitoring Dataset (DMD) and real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: To enhance situational awareness and system trustworthiness in driver monitoring by addressing occlusion and varying lighting conditions, aligning with EuroNCAP standards.

Method: Separate algorithms trained on RGB and IR images, integrated into a cohesive pipeline for real-car implementation.

Result: Superior performance of RGB-based models and robust occlusion detection, validated on DMD and real-world scenarios.

Conclusion: The system effectively addresses occlusion and lighting challenges, contributing to reliable DMS with pioneering occlusion detection.

Abstract: This paper presents a robust, occlusion-aware driver monitoring system (DMS)
utilizing the Driver Monitoring Dataset (DMD). The system performs driver
identification, gaze estimation by regions, and face occlusion detection under
varying lighting conditions, including challenging low-light scenarios. Aligned
with EuroNCAP recommendations, the inclusion of occlusion detection enhances
situational awareness and system trustworthiness by indicating when the
system's performance may be degraded. The system employs separate algorithms
trained on RGB and infrared (IR) images to ensure reliable functioning. We
detail the development and integration of these algorithms into a cohesive
pipeline, addressing the challenges of working with different sensors and
real-car implementation. Evaluation on the DMD and in real-world scenarios
demonstrates the effectiveness of the proposed system, highlighting the
superior performance of RGB-based models and the pioneering contribution of
robust occlusion detection in DMS.

</details>


### [121] [OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation](https://arxiv.org/abs/2504.20682)
*Long Liu, Cihui Yang*

Main category: cs.CV

TL;DR: The paper introduces OG-HFYOLO, a model for table structure recognition that addresses geometric deformation issues, using innovative modules and a new dataset (DWTAL).


<details>
  <summary>Details</summary>
Motivation: Geometric deformation in tables weakens content-structure correlation, hindering accurate content extraction for downstream tasks.

Method: Proposes OG-HFYOLO with Gradient Orientation-aware Extractor, Heterogeneous Kernel Cross Fusion, scale-aware loss, and mask-driven non-maximal suppression. Also introduces a data generator for the DWTAL dataset.

Result: The model achieves excellent segmentation accuracy on mainstream instance segmentation models.

Conclusion: OG-HFYOLO effectively addresses deformation challenges, with the DWTAL dataset and open-source code supporting further research.

Abstract: Table structure recognition is a key task in document analysis. However, the
geometric deformation in deformed tables causes a weak correlation between
content information and structure, resulting in downstream tasks not being able
to obtain accurate content information. To obtain fine-grained spatial
coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge
response by Gradient Orientation-aware Extractor, combines a Heterogeneous
Kernel Cross Fusion module and a scale-aware loss function to adapt to
multi-scale objective features, and introduces mask-driven non-maximal
suppression in the post-processing, which replaces the traditional bounding box
suppression mechanism. Furthermore, we also propose a data generator, filling
the gap in the dataset for fine-grained deformation table cell spatial
coordinate localization, and derive a large-scale dataset named Deformation
Wired Table (DWTAL). Experiments show that our proposed model demonstrates
excellent segmentation accuracy on all mainstream instance segmentation models.
The dataset and the source code are open source:
https://github.com/justliulong/OGHFYOLO.

</details>


### [122] [Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion](https://arxiv.org/abs/2504.20685)
*Zesheng Wang, Alexandre Bruckert, Patrick Le Callet, Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper proposes Facial Action Diffusion (FAD) and Efficient Listener Network (ELNet) to generate realistic listener facial motions in real-time, outperforming existing methods with 99% faster computation.


<details>
  <summary>Details</summary>
Motivation: Generating realistic listener facial motions in dyadic conversations is challenging due to high-dimensional action space and temporal dependencies. Existing methods using 3DMM coefficients are computationally slow.

Method: The authors introduce FAD, applying diffusion methods for facial action generation, and ELNet, which processes visual and audio inputs for efficient motion representation.

Result: The proposed method improves performance over state-of-the-art methods while reducing computational time by 99%.

Conclusion: FAD and ELNet enable real-time, efficient generation of listener facial motions, addressing the limitations of traditional 3DMM-based approaches.

Abstract: Generating realistic listener facial motions in dyadic conversations remains
challenging due to the high-dimensional action space and temporal dependency
requirements. Existing approaches usually consider extracting 3D Morphable
Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes
the computational speed of the 3DMM a bottleneck, making it difficult to
achieve real-time interactive responses. To tackle this problem, we propose
Facial Action Diffusion (FAD), which introduces the diffusion methods from the
field of image generation to achieve efficient facial action generation. We
further build the Efficient Listener Network (ELNet) specially designed to
accommodate both the visual and audio information of the speaker as input.
Considering of FAD and ELNet, the proposed method learns effective listener
facial motion representations and leads to improvements of performance over the
state-of-the-art methods while reducing 99% computational time.

</details>


### [123] [In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer](https://arxiv.org/abs/2504.20690)
*Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, Yi Yang*

Main category: cs.CV

TL;DR: The paper introduces a method for instruction-based image editing that balances precision and efficiency by leveraging Diffusion Transformers (DiT) and innovative techniques like in-context prompting, LoRA-MoE hybrid tuning, and early filter inference-time scaling.


<details>
  <summary>Details</summary>
Motivation: Current methods for instruction-based image editing face a tradeoff between precision and efficiency, with fine-tuning methods being resource-intensive and training-free techniques lacking quality.

Method: The proposed method uses DiT's generation capacity and introduces (1) an in-context editing framework, (2) a LoRA-MoE hybrid tuning strategy, and (3) an early filter inference-time scaling method.

Result: The method outperforms state-of-the-art approaches, requiring only 0.5% training data and 1% trainable parameters compared to baselines.

Conclusion: This work establishes a high-precision, efficient paradigm for instruction-guided image editing.

Abstract: Instruction-based image editing enables robust image modification via natural
language prompts, yet current methods face a precision-efficiency tradeoff.
Fine-tuning methods demand significant computational resources and large
datasets, while training-free techniques struggle with instruction
comprehension and edit quality. We resolve this dilemma by leveraging
large-scale Diffusion Transformer (DiT)' enhanced generation capacity and
native contextual awareness. Our solution introduces three contributions: (1)
an in-context editing framework for zero-shot instruction compliance using
in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning
strategy that enhances flexibility with efficient adaptation and dynamic expert
routing, without extensive retraining; and (3) an early filter inference-time
scaling method using vision-language models (VLMs) to select better initial
noise early, improving edit quality. Extensive evaluations demonstrate our
method's superiority: it outperforms state-of-the-art approaches while
requiring only 0.5% training data and 1% trainable parameters compared to
conventional baselines. This work establishes a new paradigm that enables
high-precision yet efficient instruction-guided editing. Codes and demos can be
found in https://river-zhang.github.io/ICEdit-gh-pages/.

</details>


### [124] [Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining](https://arxiv.org/abs/2504.20800)
*Weizhen He, Yunfeng Yan, Shixiang Tang, Yiheng Deng, Yangyang Zhong, Pengxin Luo, Donglian Qi*

Main category: cs.CV

TL;DR: The paper proposes a human-centric pretraining method using RGB images and frequency space analysis (DCT) to improve performance across multiple tasks without relying on depth data.


<details>
  <summary>Details</summary>
Motivation: Previous methods rely on task-specific datasets or additional modalities like depth, which are limited by data scarcity and view sensitivity. This work aims to enhance data scalability by leveraging RGB images and DCT.

Method: The approach discards depth data and uses DCT to explore semantic information in RGB images. It introduces annotation denoising tasks with keypoints and DCT maps to learn fine-grained human body semantics.

Result: The model outperforms state-of-the-art methods on various benchmarks, including pose estimation, human parsing, crowd counting, crowd localization, and person ReID.

Conclusion: The proposed method demonstrates superior performance without depth data, validating its effectiveness and scalability for human-centric tasks.

Abstract: Human-centric perception is the core of diverse computer vision tasks and has
been a long-standing research focus. However, previous research studied these
human-centric tasks individually, whose performance is largely limited to the
size of the public task-specific datasets. Recent human-centric methods
leverage the additional modalities, e.g., depth, to learn fine-grained semantic
information, which limits the benefit of pretraining models due to their
sensitivity to camera views and the scarcity of RGB-D data on the Internet.
This paper improves the data scalability of human-centric pretraining methods
by discarding depth information and exploring semantic information of RGB
images in the frequency space by Discrete Cosine Transform (DCT). We further
propose new annotation denoising auxiliary tasks with keypoints and DCT maps to
enforce the RGB image extractor to learn fine-grained semantic information of
human bodies. Our extensive experiments show that when pretrained on
large-scale datasets (COCO and AIC datasets) without depth annotation, our
model achieves better performance than state-of-the-art methods by +0.5 mAP on
COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by
+4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on
SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for
crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for
person ReID. We also validate the effectiveness of our method on MPII+NTURGBD
datasets

</details>


### [125] [GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion](https://arxiv.org/abs/2504.20829)
*Jiaxin Hong, Sixu Chen, Shuoyang Sun, Hongyao Yu, Hao Fang, Yuqi Tan, Bin Chen, Shuhan Qi, Jiawei Li*

Main category: cs.CV

TL;DR: The paper investigates backdoor threats in 3D Gaussian Splatting (3DGS) pipelines, proposing GuassTrap, a poisoning attack method that implants stealthy, harmful backdoor views while maintaining high-quality rendering in normal views.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of 3DGS in safety-critical domains like autonomous systems and AR/VR necessitates scrutiny of security vulnerabilities, particularly backdoor threats that could cause environmental misperception or spatial distortion.

Method: GuassTrap, a three-stage pipeline (attack, stabilization, and normal training), implants viewpoint-consistent poisoned renderings in 3DGS, optimizing attack efficacy and perceptual realism.

Result: Experiments show GuassTrap effectively embeds imperceptible yet harmful backdoor views while preserving high-quality rendering in normal views, demonstrating robustness and practical applicability.

Conclusion: The study highlights significant security risks in 3DGS pipelines and validates GuassTrap as a potent tool for exposing vulnerabilities in 3D rendering.

Abstract: As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene
representation and novel view synthesis, its rapid adoption in safety-critical
domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of
potential security vulnerabilities. This paper presents the first systematic
study of backdoor threats in 3DGS pipelines. We identify that adversaries may
implant backdoor views to induce malicious scene confusion during inference,
potentially leading to environmental misperception in autonomous navigation or
spatial distortion in immersive environments. To uncover this risk, we propose
GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap
injects malicious views at specific attack viewpoints while preserving
high-quality rendering in non-target views, ensuring minimal detectability and
maximizing potential harm. Specifically, the proposed method consists of a
three-stage pipeline (attack, stabilization, and normal training) to implant
stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing
attack efficacy and perceptual realism to expose security risks in 3D
rendering. Extensive experiments on both synthetic and real-world datasets
demonstrate that GuassTrap can effectively embed imperceptible yet harmful
backdoor views while maintaining high-quality rendering in normal views,
validating its robustness, adaptability, and practical applicability.

</details>


### [126] [CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation](https://arxiv.org/abs/2504.20830)
*Jianyu Wu, Yizhou Wang, Xiangyu Yue, Xinzhu Ma, Jingyang Guo, Dongzhan Zhou, Wanli Ouyang, Shixiang Tang*

Main category: cs.CV

TL;DR: Proposes CMT, a multimodal CAD framework, and mmABC dataset, improving CAD generation metrics significantly.


<details>
  <summary>Details</summary>
Motivation: Existing CAD methods lack accuracy and flexibility for multimodal design needs.

Method: Introduces CMT, a cascade MAR with topology predictor, and mmABC dataset for training.

Result: CMT outperforms state-of-the-art methods, improving Coverage, Valid ratio, and Chamfer metrics.

Conclusion: CMT and mmABC advance CAD generation, with plans to release dataset, code, and models.

Abstract: While accurate and user-friendly Computer-Aided Design (CAD) is crucial for
industrial design and manufacturing, existing methods still struggle to achieve
this due to their over-simplified representations or architectures incapable of
supporting multimodal design requirements. In this paper, we attempt to tackle
this problem from both methods and datasets aspects. First, we propose a
cascade MAR with topology predictor (CMT), the first multimodal framework for
CAD generation based on Boundary Representation (B-Rep). Specifically, the
cascade MAR can effectively capture the ``edge-counters-surface'' priors that
are essential in B-Reps, while the topology predictor directly estimates
topology in B-Reps from the compact tokens in MAR. Second, to facilitate
large-scale training, we develop a large-scale multimodal CAD dataset, mmABC,
which includes over 1.3 million B-Rep models with multimodal annotations,
including point clouds, text descriptions, and multi-view images. Extensive
experiments show the superior of CMT in both conditional and unconditional CAD
generation tasks. For example, we improve Coverage and Valid ratio by +10.68%
and +10.3%, respectively, compared to state-of-the-art methods on ABC in
unconditional generation. CMT also improves +4.01 Chamfer on image conditioned
CAD generation on mmABC. The dataset, code and pretrained network shall be
released.

</details>


### [127] [RadSAM: Segmenting 3D radiological images with a 2D promptable model](https://arxiv.org/abs/2504.20837)
*Julien Khlaut, Elodie Ferreres, Daniel Tordjman, Hélène Philippe, Tom Boeken, Pierre Manceron, Corentin Dancette*

Main category: cs.CV

TL;DR: RadSAM introduces a method to segment 3D medical images using a 2D model with a single prompt, addressing limitations of SAM in medical imaging.


<details>
  <summary>Details</summary>
Motivation: SAM lacks effectiveness for medical data and 3D images, requiring tedious per-slice prompting. RadSAM aims to simplify 3D segmentation with a single prompt and editing features.

Method: Train a 2D model with noisy masks, bounding boxes, and points as prompts, then use iterative inference for 3D mask reconstruction.

Result: RadSAM outperforms state-of-the-art models on the AMOS dataset, demonstrating effective 3D segmentation and editing capabilities.

Conclusion: RadSAM bridges the gap in 3D medical image segmentation, offering a practical solution with improved efficiency and functionality.

Abstract: Medical image segmentation is a crucial and time-consuming task in clinical
care, where mask precision is extremely important. The Segment Anything Model
(SAM) offers a promising approach, as it provides an interactive interface
based on visual prompting and edition to refine an initial segmentation. This
model has strong generalization capabilities, does not rely on predefined
classes, and adapts to diverse objects; however, it is pre-trained on natural
images and lacks the ability to process medical data effectively. In addition,
this model is built for 2D images, whereas a whole medical domain is based on
3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging
are based on 2D models, thus requiring one prompt per slice to segment 3D
objects, making the segmentation process tedious. They also lack important
features such as editing. To bridge this gap, we propose RadSAM, a novel method
for segmenting 3D objects with a 2D model from a single prompt. In practice, we
train a 2D model using noisy masks as initial prompts, in addition to bounding
boxes and points. We then use this novel prompt type with an iterative
inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a
benchmark to evaluate the model's ability to segment 3D objects in CT images
from a single prompt and evaluate the models' out-of-domain transfer and
edition capabilities. We demonstrate the effectiveness of our approach against
state-of-the-art models on this benchmark using the AMOS abdominal organ
segmentation dataset.

</details>


### [128] [FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2504.20860)
*Mainak Singha, Subhankar Roy, Sarthak Mehrotra, Ankit Jha, Moloud Abdar, Biplab Banerjee, Elisa Ricci*

Main category: cs.CV

TL;DR: FedMVP improves federated learning for Vision-Language Models by using multimodal visual prompts to enhance adaptability and reduce overfitting.


<details>
  <summary>Details</summary>
Motivation: Textual prompt tuning in federated learning often overfits and struggles with unseen concepts, prompting the need for a multimodal approach.

Method: FedMVP integrates image-conditioned and textual attribute features via PromptFormer, using cross-attention for alignment and training with CLIP similarity and consistency losses.

Result: FedMVP outperforms state-of-the-art methods on 20 datasets, showing better generalization to unseen classes and domains.

Conclusion: FedMVP effectively addresses limitations of textual prompt tuning, offering improved performance and adaptability in federated learning.

Abstract: Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated
learning by tuning lightweight input tokens (or prompts) on local client data,
while keeping network weights frozen. Post training, only the prompts are
shared by the clients with the central server for aggregation. However, textual
prompt tuning often struggles with overfitting to known concepts and may be
overly reliant on memorized text features, limiting its adaptability to unseen
concepts. To address this limitation, we propose Federated Multimodal Visual
Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual
information -- image-conditioned features and textual attribute features of a
class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer
module that synergistically aligns textual and visual features through
cross-attention, enabling richer contexual integration. The dynamically
generated multimodal visual prompts are then input to the frozen vision encoder
of CLIP, and trained with a combination of CLIP similarity loss and a
consistency loss. Extensive evaluation on 20 datasets spanning three
generalization settings demonstrates that FedMVP not only preserves performance
on in-distribution classes and domains, but also displays higher
generalizability to unseen classes and domains when compared to
state-of-the-art methods. Codes will be released upon acceptance.

</details>


### [129] [AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection](https://arxiv.org/abs/2504.20865)
*Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli*

Main category: cs.CV

TL;DR: Ai-GenBench is a benchmark for detecting AI-generated images, addressing challenges like generalization to new models and fair evaluation.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI necessitates robust detection methods for media authenticity, but current benchmarks lack real-world adaptability and fairness.

Method: Ai-GenBench uses a temporal evaluation framework, incrementally training on historically ordered synthetic images to test generalization to new models like GANs to diffusion models.

Result: The benchmark offers a comprehensive dataset, standardized protocol, and tools for researchers and non-experts, ensuring reproducibility and practical training.

Conclusion: Ai-GenBench enables scalable, fair comparisons of detection methods, supporting the development of robust forensic tools for evolving AI-generated content.

Abstract: The rapid advancement of generative AI has revolutionized image creation,
enabling high-quality synthesis from text prompts while raising critical
challenges for media authenticity. We present Ai-GenBench, a novel benchmark
designed to address the urgent need for robust detection of AI-generated images
in real-world scenarios. Unlike existing solutions that evaluate models on
static datasets, Ai-GenBench introduces a temporal evaluation framework where
detection methods are incrementally trained on synthetic images, historically
ordered by their generative models, to test their ability to generalize to new
generative models, such as the transition from GANs to diffusion models. Our
benchmark focuses on high-quality, diverse visual content and overcomes key
limitations of current approaches, including arbitrary dataset splits, unfair
comparisons, and excessive computational demands. Ai-GenBench provides a
comprehensive dataset, a standardized evaluation protocol, and accessible tools
for both researchers and non-experts (e.g., journalists, fact-checkers),
ensuring reproducibility while maintaining practical training requirements. By
establishing clear evaluation rules and controlled augmentation strategies,
Ai-GenBench enables meaningful comparison of detection methods and scalable
solutions. Code and data are publicly available to ensure reproducibility and
to support the development of robust forensic detectors to keep pace with the
rise of new synthetic generators.

</details>


### [130] [FLIM-based Salient Object Detection Networks with Adaptive Decoders](https://arxiv.org/abs/2504.20872)
*Gilson Junior Soares, Matheus Abrantes Cerqueira, Jancarlo F. Gomes, Laurent Najman, Silvio Jamil F. Guimarães, Alexandre Xavier Falcão*

Main category: cs.CV

TL;DR: The paper proposes ultra-lightweight networks for Salient Object Detection (SOD) using FLIM methodology, achieving comparable performance with minimal training data and no backpropagation.


<details>
  <summary>Details</summary>
Motivation: To address SOD tasks under limited computational resources and labeled data constraints by leveraging lightweight models and adaptive decoders.

Method: Combines a FLIM encoder (trained from a few representative images) with adaptive decoders, whose weights are estimated per image via heuristic functions. Introduces two new adaptive decoders.

Result: FLIM networks outperform lightweight baselines, demonstrating effectiveness in SOD tasks with minimal training.

Conclusion: FLIM networks are promising for resource-constrained applications, warranting further investigation.

Abstract: Salient Object Detection (SOD) methods can locate objects that stand out in
an image, assign higher values to their pixels in a saliency map, and binarize
the map outputting a predicted segmentation mask. A recent tendency is to
investigate pre-trained lightweight models rather than deep neural networks in
SOD tasks, coping with applications under limited computational resources. In
this context, we have investigated lightweight networks using a methodology
named Feature Learning from Image Markers (FLIM), which assumes that the
encoder's kernels can be estimated from marker pixels on discriminative regions
of a few representative images. This work proposes flyweight networks, hundreds
of times lighter than lightweight models, for SOD by combining a FLIM encoder
with an adaptive decoder, whose weights are estimated for each input image by a
given heuristic function. Such FLIM networks are trained from three to four
representative images only and without backpropagation, making the models
suitable for applications under labeled data constraints as well. We study five
adaptive decoders; two of them are introduced here. Differently from the
previous ones that rely on one neuron per pixel with shared weights, the
heuristic functions of the new adaptive decoders estimate the weights of each
neuron per pixel. We compare FLIM models with adaptive decoders for two
challenging SOD tasks with three lightweight networks from the
state-of-the-art, two FLIM networks with decoders trained by backpropagation,
and one FLIM network whose labeled markers define the decoder's weights. The
experiments demonstrate the advantages of the proposed networks over the
baselines, revealing the importance of further investigating such methods in
new applications.

</details>


### [131] [Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers](https://arxiv.org/abs/2504.20902)
*Quentin Guimard, Moreno D'Incà, Massimiliano Mancini, Elisa Ricci*

Main category: cs.CV

TL;DR: C2B is a training-free, unsupervised framework for identifying biases in pre-trained models using task descriptions and large language models, outperforming annotation-dependent methods.


<details>
  <summary>Details</summary>
Motivation: Existing bias detection methods require labeled datasets, limiting accessibility for non-experts. C2B aims to address this by eliminating the need for labeled data.

Method: C2B uses a task description to generate bias proposals via a large language model, retrieves images for these biases, and evaluates model accuracy without annotations.

Result: C2B discovers biases beyond original datasets and outperforms annotation-dependent baselines.

Conclusion: C2B is a promising step toward task-agnostic, unsupervised bias detection in pre-trained models.

Abstract: A person downloading a pre-trained model from the web should be aware of its
biases. Existing approaches for bias identification rely on datasets containing
labels for the task of interest, something that a non-expert may not have
access to, or may not have the necessary resources to collect: this greatly
limits the number of tasks where model biases can be identified. In this work,
we present Classifier-to-Bias (C2B), the first bias discovery framework that
works without access to any labeled data: it only relies on a textual
description of the classification task to identify biases in the target
classification model. This description is fed to a large language model to
generate bias proposals and corresponding captions depicting biases together
with task-specific target labels. A retrieval model collects images for those
captions, which are then used to assess the accuracy of the model w.r.t. the
given biases. C2B is training-free, does not require any annotations, has no
constraints on the list of biases, and can be applied to any pre-trained model
on any classification task. Experiments on two publicly available datasets show
that C2B discovers biases beyond those of the original datasets and outperforms
a recent state-of-the-art bias detection baseline that relies on task-specific
annotations, being a promising first step toward addressing task-agnostic
unsupervised bias detection.

</details>


### [132] [DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition](https://arxiv.org/abs/2504.20948)
*Yanghui Song, Chengfu Yang*

Main category: cs.CV

TL;DR: The paper proposes DS_FusionNet, a dynamic dual-stream fusion network, to improve plant disease recognition accuracy despite challenges like small-sample learning and leaf occlusion. It achieves over 90% accuracy on some datasets and shows strong generalization.


<details>
  <summary>Details</summary>
Motivation: The need for precise plant disease identification due to global agricultural challenges drives the development of AI solutions for overcoming technical hurdles like small-sample learning and high inter-class similarity.

Method: The study introduces DS_FusionNet, featuring a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation to enhance recognition accuracy.

Result: DS_FusionNet achieves over 90% accuracy on PlantDisease and CIFAR-10 datasets with only 10% of the data, and 85% on the complex PlantWild dataset, demonstrating strong generalization.

Conclusion: The research offers new technical insights for fine-grained image classification and supports precise agricultural disease management.

Abstract: Given the severe challenges confronting the global growth security of
economic crops, precise identification and prevention of plant diseases has
emerged as a critical issue in artificial intelligence-enabled agricultural
technology. To address the technical challenges in plant disease recognition,
including small-sample learning, leaf occlusion, illumination variations, and
high inter-class similarity, this study innovatively proposes a Dynamic
Dual-Stream Fusion Network (DS_FusionNet). The network integrates a
dual-backbone architecture, deformable dynamic fusion modules, and
bidirectional knowledge distillation strategy, significantly enhancing
recognition accuracy. Experimental results demonstrate that DS_FusionNet
achieves classification accuracies exceeding 90% using only 10% of the
PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the
complex PlantWild dataset, exhibiting exceptional generalization capabilities.
This research not only provides novel technical insights for fine-grained image
classification but also establishes a robust foundation for precise
identification and management of agricultural diseases.

</details>


### [133] [SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features](https://arxiv.org/abs/2504.20970)
*Mete Erdogan, Sebnem Demirtas*

Main category: cs.CV

TL;DR: Proposes SVD-LS for efficient multi-class pneumonia classification using self-supervised and transfer learning, avoiding costly fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Early and accurate pneumonia diagnosis via X-ray imaging is critical for treatment and outcomes.

Method: Uses SVD-LS framework with self-supervised and transfer learning for feature extraction, employing a closed-form, non-iterative classification approach.

Result: SVD-LS achieves competitive performance with reduced computational costs, suitable for real-time applications.

Conclusion: SVD-LS is an efficient and accurate alternative for pneumonia classification in medical imaging.

Abstract: Accurate and early diagnosis of pneumonia through X-ray imaging is essential
for effective treatment and improved patient outcomes. Recent advancements in
machine learning have enabled automated diagnostic tools that assist
radiologists in making more reliable and efficient decisions. In this work, we
propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework
for multi-class pneumonia classification, leveraging powerful feature
representations from state-of-the-art self-supervised and transfer learning
models. Rather than relying on computationally expensive gradient based
fine-tuning, we employ a closed-form, non-iterative classification approach
that ensures efficiency without compromising accuracy. Experimental results
demonstrate that SVD-LS achieves competitive performance while offering
significantly reduced computational costs, making it a viable alternative for
real-time medical imaging applications.

</details>


### [134] [TesserAct: Learning 4D Embodied World Models](https://arxiv.org/abs/2504.20995)
*Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, Chuang Gan*

Main category: cs.CV

TL;DR: The paper introduces a method for learning 4D embodied world models using RGB-DN videos, improving spatial and temporal consistency for dynamic 3D scene predictions and agent actions.


<details>
  <summary>Details</summary>
Motivation: To surpass traditional 2D models by incorporating detailed shape, configuration, and temporal changes, enabling accurate inverse dynamic models for embodied agents.

Method: Extend robotic manipulation datasets with depth and normal info, fine-tune a video generation model on RGB-DN data, and convert generated videos into 4D scenes.

Result: Ensures coherent 4D scene predictions, enables novel view synthesis, and outperforms prior video-based world models in policy learning.

Conclusion: The approach advances embodied world modeling by integrating spatial and temporal dynamics, enhancing agent performance and scene understanding.

Abstract: This paper presents an effective approach for learning novel 4D embodied
world models, which predict the dynamic evolution of 3D scenes over time in
response to an embodied agent's actions, providing both spatial and temporal
consistency. We propose to learn a 4D world model by training on RGB-DN (RGB,
Depth, and Normal) videos. This not only surpasses traditional 2D models by
incorporating detailed shape, configuration, and temporal changes into their
predictions, but also allows us to effectively learn accurate inverse dynamic
models for an embodied agent. Specifically, we first extend existing robotic
manipulation video datasets with depth and normal information leveraging
off-the-shelf models. Next, we fine-tune a video generation model on this
annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for
each frame. We then present an algorithm to directly convert generated RGB,
Depth, and Normal videos into a high-quality 4D scene of the world. Our method
ensures temporal and spatial coherence in 4D scene predictions from embodied
scenarios, enables novel view synthesis for embodied environments, and
facilitates policy learning that significantly outperforms those derived from
prior video-based world models.

</details>


### [135] [X-Fusion: Introducing New Modality to Frozen Large Language Models](https://arxiv.org/abs/2504.20996)
*Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li*

Main category: cs.CV

TL;DR: X-Fusion extends LLMs for multimodal tasks without altering their language capabilities, outperforming alternatives in image-text tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance LLMs for multimodal tasks while preserving their existing language abilities.

Method: Uses a dual-tower design with frozen LLM parameters and modality-specific weights for vision integration.

Result: Outperforms other architectures in image-to-text and text-to-image tasks, with improved generation quality and reduced noise.

Conclusion: X-Fusion offers insights for efficient unified multimodal models, balancing performance and parameter efficiency.

Abstract: We propose X-Fusion, a framework that extends pretrained Large Language
Models (LLMs) for multimodal tasks while preserving their language
capabilities. X-Fusion employs a dual-tower design with modality-specific
weights, keeping the LLM's parameters frozen while integrating vision-specific
information for both understanding and generation. Our experiments demonstrate
that X-Fusion consistently outperforms alternative architectures on both
image-to-text and text-to-image tasks. We find that incorporating
understanding-focused data improves generation quality, reducing image data
noise enhances overall performance, and feature alignment accelerates
convergence for smaller models but has minimal impact on larger ones. Our
findings provide valuable insights into building efficient unified multimodal
models.

</details>


### [136] [YoChameleon: Personalized Vision and Language Generation](https://arxiv.org/abs/2504.20998)
*Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, Yuheng Li*

Main category: cs.CV

TL;DR: Yo'Chameleon is the first study on personalizing large multimodal models using soft-prompt tuning for both text and image generation.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models lack personalized knowledge of user-specific concepts, and existing personalization methods are unclear for new modalities like image generation.

Method: Yo'Chameleon uses soft-prompt tuning with 3-5 images of a concept, self-prompting optimization, and a soft-positive image generation approach.

Result: The model can answer questions about the subject and generate images of it in new contexts with pixel-level details.

Conclusion: Yo'Chameleon successfully adapts personalization to multimodal tasks, balancing performance and enhancing image quality in few-shot settings.

Abstract: Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into
powerful tools with millions of users. However, they remain generic models and
lack personalized knowledge of specific user concepts. Previous work has
explored personalization for text generation, yet it remains unclear how these
methods can be adapted to new modalities, such as image generation. In this
paper, we introduce Yo'Chameleon, the first attempt to study personalization
for large multimodal models. Given 3-5 images of a particular concept,
Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information
to (i) answer questions about the subject and (ii) recreate pixel-level details
to produce images of the subject in new contexts. Yo'Chameleon is trained with
(i) a self-prompting optimization mechanism to balance performance across
multiple modalities, and (ii) a ``soft-positive" image generation approach to
enhance image quality in a few-shot setting.

</details>


### [137] [Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)
*Zhou Yu, Xuecheng Ouyang, Zhenwei Shao, Meng Wang, Jun Yu*

Main category: cs.CV

TL;DR: Prophet is a framework that enhances knowledge-based VQA by prompting LLMs with answer heuristics, improving accuracy without relying on explicit KBs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for knowledge-based VQA either use irrelevant KBs or underutilize LLMs due to insufficient visual context. Prophet aims to bridge this gap.

Method: Prophet trains a VQA model, extracts answer heuristics (candidates and examples), and formats them into prompts for LLMs like GPT-3.

Result: Prophet outperforms state-of-the-art methods on four knowledge-based VQA datasets and is adaptable to various models and LLMs.

Conclusion: Prophet effectively leverages LLMs for VQA by providing richer visual context, offering flexibility and scalability for future integration with multimodal models.

Abstract: Knowledge-based visual question answering (VQA) requires external knowledge
beyond the image to answer the question. Early studies retrieve required
knowledge from explicit knowledge bases (KBs), which often introduces
irrelevant information to the question, hence restricting the performance of
their models. Recent works have resorted to using a powerful large language
model (LLM) as an implicit knowledge engine to acquire the necessary knowledge
for answering. Despite the encouraging results achieved by these methods, we
argue that they have not fully activated the capacity of the \emph{blind} LLM
as the provided textual input is insufficient to depict the required visual
information to answer the question. In this paper, we present Prophet -- a
conceptually simple, flexible, and general framework designed to prompt LLM
with answer heuristics for knowledge-based VQA. Specifically, we first train a
vanilla VQA model on a specific knowledge-based VQA dataset without external
knowledge. After that, we extract two types of complementary answer heuristics
from the VQA model: answer candidates and answer-aware examples. The two types
of answer heuristics are jointly encoded into a formatted prompt to facilitate
the LLM's understanding of both the image and question, thus generating a more
accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet
significantly outperforms existing state-of-the-art methods on four challenging
knowledge-based VQA datasets. Prophet is general that can be instantiated with
the combinations of different VQA models (i.e., both discriminative and
generative ones) and different LLMs (i.e., both commercial and open-source
ones). Moreover, Prophet can also be integrated with modern large multimodal
models in different stages, which is named Prophet++, to further improve the
capabilities on knowledge-based VQA tasks.

</details>


### [138] [Practical solutions to the relative pose of three calibrated cameras](https://arxiv.org/abs/2303.16078)
*Charalambos Tzamos, Viktor Kocur, Yaqing Ding, Daniel Barath, Zuzana Berger Haladova, Torsten Sattler, Zuzana Kukelova*

Main category: cs.CV

TL;DR: Novel efficient solvers for estimating relative pose of three calibrated cameras using four point correspondences, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenging problem of estimating relative pose from minimal point correspondences for improved efficiency and accuracy.

Method: Using four correspondences to estimate approximate geometry (affine or perspective) and generating an additional approximate correspondence via mean-point strategy. Leverages existing minimal solvers like 4-point affine fundamental matrix, 5-point relative pose solver, and P3P solver.

Result: Proposed solvers, especially the mean-point-based one, outperform existing methods in robustness and accuracy when combined with local optimization.

Conclusion: The new solvers provide efficient, easy-to-implement, and highly accurate solutions for relative pose estimation, with the mean-point approach being superior.

Abstract: We study the challenging problem of estimating the relative pose of three
calibrated cameras from four point correspondences. We propose novel efficient
solutions to this problem that are based on the simple idea of using four
correspondences to estimate an approximate geometry of the first two views. We
model this geometry either as an affine or a fully perspective geometry
estimated using one additional approximate correspondence. We generate such an
approximate correspondence using a very simple and efficient strategy, where
the new point is the mean point of three corresponding input points. The new
solvers are efficient and easy to implement, since they are based on existing
efficient minimal solvers, i.e., the 4-point affine fundamental matrix, the
well-known 5-point relative pose solver, and the P3P solver. Extensive
experiments on real data show that the proposed solvers, when properly coupled
with local optimization, achieve state-of-the-art results, with the novel
solver based on approximate mean-point correspondences being more robust and
accurate than the affine-based solver.

</details>


### [139] [Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions](https://arxiv.org/abs/2306.07520)
*Weizhen He, Yiheng Deng, Shixiang Tang, Qihao Chen, Qingsong Xie, Yizhou Wang, Lei Bai, Feng Zhu, Rui Zhao, Wanli Ouyang, Donglian Qi, Yunfeng Yan*

Main category: cs.CV

TL;DR: The paper introduces instruct-ReID, a general person re-identification task using visual or language instructions, unifying six existing ReID tasks. It proposes the OmniReID benchmark and adaptive triplet loss, showing significant performance improvements across diverse ReID scenarios.


<details>
  <summary>Details</summary>
Motivation: Current ReID tasks are studied separately, limiting real-world applications. The paper aims to unify these tasks under a general framework using instructions.

Method: Proposes instruct-ReID, a multi-purpose ReID model trained on the OmniReID benchmark with an adaptive triplet loss baseline.

Result: The model improves performance across various ReID tasks (e.g., +0.5% to +24.9% mAP) without fine-tuning.

Conclusion: Instruct-ReID is a versatile and effective approach, unifying multiple ReID tasks and advancing real-world applicability.

Abstract: Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a new instruct-ReID task that requires the
model to retrieve images according to the given image or language instructions.
Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks
can be viewed as special cases by designing different instructions. We propose
a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline
method to facilitate research in this new setting. Experimental results show
that the proposed multi-purpose ReID model, trained on our OmniReID benchmark
without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17,
CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC
for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template
based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+
real2 for our newly defined language-instructed ReID, +4.3% on LLCM for
visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The
datasets, the model, and code will be available at
https://github.com/hwz-zju/Instruct-ReID.

</details>


### [140] [3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition](https://arxiv.org/abs/2310.18511)
*Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 3DCoMPaT++ is a large multimodal 2D/3D dataset with 160M renderings of 10M stylized 3D shapes, annotated at part-instance level, covering 41 categories, 275 part categories, and 293 material classes. It introduces the Grounded CoMPaT Recognition (GCR) task and reports CVPR2023 challenge results.


<details>
  <summary>Details</summary>
Motivation: To advance research in compositional 3D vision by providing a comprehensive dataset and introducing a new task (GCR) for recognizing and grounding material compositions on 3D objects.

Method: The dataset includes RGB point clouds, 3D meshes, depth maps, and segmentation masks. A subset of 1M shapes is rendered from fixed and randomized views. The GCR task is introduced, and a modified PointNet++ model is used for the challenge.

Result: The dataset enables large-scale research, and the winning method at CVPR2023 demonstrates the feasibility of GCR using 6D inputs.

Conclusion: 3DCoMPaT++ facilitates future work in compositional 3D vision, with potential for further GCR enhancements.

Abstract: In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160
million rendered views of more than 10 million stylized 3D shapes carefully
annotated at the part-instance level, alongside matching RGB point clouds, 3D
textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41
shape categories, 275 fine-grained part categories, and 293 fine-grained
material classes that can be compositionally applied to parts of 3D objects. We
render a subset of one million stylized shapes from four equally spaced views
as well as four randomized views, leading to a total of 160 million renderings.
Parts are segmented at the instance level, with coarse-grained and fine-grained
semantic levels. We introduce a new task, called Grounded CoMPaT Recognition
(GCR), to collectively recognize and ground compositions of materials on parts
of 3D objects. Additionally, we report the outcomes of a data challenge
organized at CVPR2023, showcasing the winning method's utilization of a
modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative
techniques for GCR enhancement. We hope our work will help ease future research
on compositional 3D Vision.

</details>


### [141] [Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement](https://arxiv.org/abs/2403.16184)
*Yuxuan Wang, Xiaoyuan Liu*

Main category: cs.CV

TL;DR: The paper proposes integrating pretrained Vision-language Models (VLMs) into Scene Graph Generation (SGG) to address underrepresentation and bias, using LM Estimation and a certainty-aware ensemble method.


<details>
  <summary>Details</summary>
Motivation: SGG suffers from underrepresentation and bias due to rare or unseen triplet labels, leading to imprecise predictions.

Method: Integrate pretrained VLMs, use LM Estimation to approximate predicates distribution, and employ a certainty-aware ensemble method.

Result: The method effectively reduces bias, enhances representation, and improves SGG performance.

Conclusion: The proposed training-free approach successfully addresses bias in VLMs and boosts SGG performance.

Abstract: Scene Graph Generation (SGG) provides basic language representation of visual
scenes, requiring models to grasp complex and diverse semantics between
objects. This complexity and diversity in SGG leads to underrepresentation,
where parts of triplet labels are rare or even unseen during training,
resulting in imprecise predictions. To tackle this, we propose integrating the
pretrained Vision-language Models to enhance representation. However, due to
the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG
leads to severe bias, which stems from the imbalanced predicates distribution
in the pretraining language set. To alleviate the bias, we introduce a novel LM
Estimation to approximate the unattainable predicates distribution. Finally, we
ensemble the debiased VLMs with SGG models to enhance the representation, where
we design a certainty-aware indicator to score each sample and dynamically
adjust the ensemble weights. Our training-free method effectively addresses the
predicates bias in pretrained VLMs, enhances SGG's representation, and
significantly improve the performance.

</details>


### [142] [Pose-Based Sign Language Appearance Transfer](https://arxiv.org/abs/2410.13675)
*Amit Moryossef, Gerard Sant, Zifan Jiang*

Main category: cs.CV

TL;DR: A method for transferring signer appearance in sign language poses while preserving content, balancing privacy and utility.


<details>
  <summary>Details</summary>
Motivation: To improve pose-based rendering and sign stitching while obfuscating identity for privacy.

Method: Transfer appearance of one signer to another using estimated poses, maintaining natural movements.

Result: Reduces signer identification accuracy but slightly harms sign recognition, showing a privacy-utility tradeoff.

Conclusion: The method effectively anonymizes signers but impacts recognition, highlighting a need for balance.

Abstract: We introduce a method for transferring the signer's appearance in sign
language skeletal poses while preserving the sign content. Using estimated
poses, we transfer the appearance of one signer to another, maintaining natural
movements and transitions. This approach improves pose-based rendering and sign
stitching while obfuscating identity. Our experiments show that while the
method reduces signer identification accuracy, it slightly harms sign
recognition performance, highlighting a tradeoff between privacy and utility.
Our code is available at
https://github.com/sign-language-processing/pose-anonymization.

</details>


### [143] [Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification](https://arxiv.org/abs/2405.17790)
*Weizhen He, Yiheng Deng, Yunfeng Yan, Feng Zhu, Yizhou Wang, Lei Bai, Qingsong Xie, Donglian Qi, Wanli Ouyang, Shixiang Tang*

Main category: cs.CV

TL;DR: The paper introduces Instruct-ReID, a unified framework for person re-identification (ReID) tasks using visual or language instructions, and proposes the OmniReID++ benchmark and IRM/IRM++ models for evaluation.


<details>
  <summary>Details</summary>
Motivation: Current ReID tasks are studied separately, limiting real-world applications. The paper aims to create a general ReID setting unifying existing tasks.

Method: Proposes Instruct-ReID task, OmniReID++ benchmark, and IRM/IRM++ models with adaptive triplet loss and memory bank-assisted learning.

Result: IRM and IRM++ achieve state-of-the-art performance on 10 test sets.

Conclusion: The work advances ReID research by unifying tasks and providing a scalable benchmark and models.

Abstract: Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a novel instruct-ReID task that requires
the model to retrieve images according to the given image or language
instructions. Instruct-ReID is the first exploration of a general ReID setting,
where existing 6 ReID tasks can be viewed as special cases by assigning
different instructions. To facilitate research in this new instruct-ReID task,
we propose a large-scale OmniReID++ benchmark equipped with diverse data and
comprehensive evaluation methods e.g., task specific and task-free evaluation
settings. In the task-specific evaluation setting, gallery sets are categorized
according to specific ReID tasks. We propose a novel baseline model, IRM, with
an adaptive triplet loss to handle various retrieval tasks within a unified
framework. For task-free evaluation setting, where target person images are
retrieved from task-agnostic gallery sets, we further propose a new method
called IRM++ with novel memory bank-assisted learning. Extensive evaluations of
IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our
proposed methods, achieving state-of-the-art performance on 10 test sets. The
datasets, the model, and the code will be available at
https://github.com/hwz-zju/Instruct-ReID

</details>


### [144] [Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation](https://arxiv.org/abs/2408.13509)
*Ying Jin, Jinlong Peng, Qingdong He, Teng Hu, Jiafu Wu, Hao Chen, Haoxuan Wang, Wenbing Zhu, Mingmin Chi, Jun Liu, Yabiao Wang*

Main category: cs.CV

TL;DR: DualAnoDiff, a diffusion-based model, generates diverse and realistic anomaly images by simultaneously creating the image and its anomaly part, improving downstream anomaly inspection tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of existing anomaly generation methods, such as limited diversity, poor blending, and misaligned masks.

Method: Proposes DualAnoDiff, a dual-interrelated diffusion model for generating anomaly images and their corresponding parts, leveraging background and shape information.

Result: Outperforms state-of-the-art methods in diversity, realism, and mask accuracy, enhancing anomaly detection, localization, and classification.

Conclusion: DualAnoDiff significantly advances anomaly generation and improves performance in industrial anomaly inspection tasks.

Abstract: The performance of anomaly inspection in industrial manufacturing is
constrained by the scarcity of anomaly data. To overcome this challenge,
researchers have started employing anomaly generation approaches to augment the
anomaly dataset. However, existing anomaly generation methods suffer from
limited diversity in the generated anomalies and struggle to achieve a seamless
blending of this anomaly with the original image. Moreover, the generated mask
is usually not aligned with the generated anomaly. In this paper, we overcome
these challenges from a new perspective, simultaneously generating a pair of
the overall image and the corresponding anomaly part. We propose DualAnoDiff, a
novel diffusion-based few-shot anomaly image generation model, which can
generate diverse and realistic anomaly images by using a dual-interrelated
diffusion model, where one of them is employed to generate the whole image
while the other one generates the anomaly part. Moreover, we extract background
and shape information to mitigate the distortion and blurriness phenomenon in
few-shot image generation. Extensive experiments demonstrate the superiority of
our proposed model over state-of-the-art methods in terms of diversity, realism
and the accuracy of mask. Overall, our approach significantly improves the
performance of downstream anomaly inspection tasks, including anomaly
detection, anomaly localization, and anomaly classification tasks.

</details>


### [145] [Many-Worlds Inverse Rendering](https://arxiv.org/abs/2408.16005)
*Ziyi Zhang, Nicolas Roussel, Wenzel Jakob*

Main category: cs.CV

TL;DR: A novel method for optimizing surfaces in inverse rendering by differentiating volumetric perturbations instead of local surfaces, leading to simpler and more efficient Monte Carlo algorithms.


<details>
  <summary>Details</summary>
Motivation: Discontinuous visibility changes are a bottleneck in inverse rendering, and existing methods for sampling visibility silhouettes are complex.

Method: Differentiate a volumetric perturbation of a surface (many-worlds representation), where each 'world' is optically isolated, introducing a new transport law.

Result: The Monte Carlo algorithm is simpler and more efficient, promoting rapid convergence in iteration count and cost per iteration.

Conclusion: The proposed many-worlds representation offers a simpler and more efficient alternative to prior methods for surface optimization in inverse rendering.

Abstract: Discontinuous visibility changes remain a major bottleneck when optimizing
surfaces within a physically-based inverse renderer. Many previous works have
proposed sophisticated algorithms and data structures to sample visibility
silhouettes more efficiently.
  Our work presents another solution: instead of differentiating a tentative
surface locally, we differentiate a volumetric perturbation of a surface. We
refer this as a many-worlds representation because it models a non-interacting
superposition of conflicting explanations (worlds) of the input dataset. Each
world is optically isolated from others, leading to a new transport law that
distinguishes our method from prior work based on exponential random media.
  The resulting Monte Carlo algorithm is simpler and more efficient than prior
methods. We demonstrate that our method promotes rapid convergence, both in
terms of the total iteration count and the cost per iteration.

</details>


### [146] [Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models](https://arxiv.org/abs/2501.12433)
*Tabinda Aman, Mohammad Nadeem, Shahab Saquib Sohail, Mohammad Anas, Erik Cambria*

Main category: cs.CV

TL;DR: The study examines how animal stereotypes in human culture influence DALL-E's image generation, revealing AI perpetuates biases like "owls as wise" or "foxes as unfaithful."


<details>
  <summary>Details</summary>
Motivation: To investigate if vision-language models like DALL-E reinforce cultural animal stereotypes in AI-generated images.

Method: Targeted prompts were used to analyze DALL-E's image outputs for stereotypical representations of animals.

Result: DALL-E consistently generated images aligning with cultural animal stereotypes, confirming bias in AI visual content.

Conclusion: The study highlights a critical, underexplored bias in AI-generated visuals, calling for awareness and mitigation.

Abstract: Animal stereotypes are deeply embedded in human culture and language. They
often shape our perceptions and expectations of various species. Our study
investigates how animal stereotypes manifest in vision-language models during
the task of image generation. Through targeted prompts, we explore whether
DALL-E perpetuates stereotypical representations of animals, such as "owls as
wise," "foxes as unfaithful," etc. Our findings reveal significant stereotyped
instances where the model consistently generates images aligned with cultural
biases. The current work is the first of its kind to examine animal
stereotyping in vision-language models systematically and to highlight a
critical yet underexplored dimension of bias in AI-generated visual content.

</details>


### [147] [Generic Objects as Pose Probes for Few-shot View Synthesis](https://arxiv.org/abs/2408.16690)
*Zhirui Gao, Renjiao Yi, Chenyang Zhu, Ke Zhuang, Wei Chen, Kai Xu*

Main category: cs.CV

TL;DR: PoseProbe introduces a method for few-view NeRF reconstruction using everyday objects as 'pose probes' to estimate camera poses from just 3-6 unposed images, outperforming COLMAP in sparse or large-baseline scenes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like COLMAP require many feature-rich images for pose estimation, limiting their use in sparse or few-view scenarios. PoseProbe addresses this by leveraging common objects as references.

Method: Automatically segments objects with SAM, initializes their shape as a cube, and uses dual-branch NeRF optimization (object and scene) for pose refinement. PnP matching in SDF representation provides initial poses, with incremental view addition.

Result: Achieves state-of-the-art performance in pose estimation and novel view synthesis, especially in few-view and large-baseline cases where COLMAP fails.

Conclusion: PoseProbe is effective for sparse or few-view scenes, demonstrating robustness with various objects and outperforming traditional methods.

Abstract: Radiance fields including NeRFs and 3D Gaussians demonstrate great potential
in high-fidelity rendering and scene reconstruction, while they require a
substantial number of posed images as inputs. COLMAP is frequently employed for
preprocessing to estimate poses, while it necessitates a large number of
feature matches to operate effectively, and it struggles with scenes
characterized by sparse features, large baselines between images, or a limited
number of input images. We aim to tackle few-view NeRF reconstruction using
only 3 to 6 unposed scene images. Traditional methods often use calibration
boards but they are not common in images. We propose a novel idea of utilizing
everyday objects, commonly found in both images and real life, as "pose
probes". The probe object is automatically segmented by SAM, whose shape is
initialized from a cube. We apply a dual-branch volume rendering optimization
(object NeRF and scene NeRF) to constrain the pose optimization and jointly
refine the geometry. Specifically, object poses of two views are first
estimated by PnP matching in an SDF representation, which serves as initial
poses. PnP matching, requiring only a few features, is suitable for
feature-sparse scenes. Additional views are incrementally incorporated to
refine poses from preceding views. In experiments, PoseProbe achieves
state-of-the-art performance in both pose estimation and novel view synthesis
across multiple datasets. We demonstrate its effectiveness, particularly in
few-view and large-baseline scenes where COLMAP struggles. In ablations, using
different objects in a scene yields comparable performance. Our project page is
available at: \href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this
https URL}

</details>


### [148] [Underwater Camouflaged Object Tracking Meets Vision-Language SAM2](https://arxiv.org/abs/2409.16902)
*Chunhui Zhang, Li Liu, Guanjie Huang, Zhipeng Zhang, Hao Wen, Xi Zhou, Shiming Ge, Yanfeng Wang*

Main category: cs.CV

TL;DR: The paper introduces UW-COT220, the first large-scale multi-modal underwater camouflaged object tracking dataset, evaluates SAM- and SAM2-based trackers, and proposes VL-SAM2, a vision-language framework, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on open-air scenarios, neglecting underwater animal tracking, especially camouflaged marine animals. This work aims to address this gap.

Method: Proposes UW-COT220 dataset, evaluates SAM/SAM2 trackers, and introduces VL-SAM2, a vision-language framework based on SAM2.

Result: SAM2 outperforms SAM in underwater tracking. VL-SAM2 achieves state-of-the-art performance on UW-COT220.

Conclusion: The work advances underwater object tracking by providing a dataset and a novel framework, demonstrating improved performance in challenging environments.

Abstract: Over the past decade, significant progress has been made in visual object
tracking, largely due to the availability of large-scale datasets. However,
these datasets have primarily focused on open-air scenarios and have largely
overlooked underwater animal tracking-especially the complex challenges posed
by camouflaged marine animals. To bridge this gap, we take a step forward by
proposing the first large-scale multi-modal underwater camouflaged object
tracking dataset, namely UW-COT220. Based on the proposed dataset, this work
first comprehensively evaluates current advanced visual object tracking
methods, including SAM- and SAM2-based trackers, in challenging underwater
environments, \eg, coral reefs. Our findings highlight the improvements of SAM2
over SAM, demonstrating its enhanced ability to handle the complexities of
underwater camouflaged objects. Furthermore, we propose a novel vision-language
tracking framework called VL-SAM2, based on the video foundation model SAM2.
Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art
performance on the UW-COT220 dataset. The dataset and codes are available
at~\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\color{magenta}{here}}.

</details>


### [149] [REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations](https://arxiv.org/abs/2502.03629)
*Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, Cheng-Yu Hsieh, Ranjay Krishna*

Main category: cs.CV

TL;DR: REALEDIT is a large-scale image editing dataset with real user requests and human-made edits, outperforming existing models and showing potential for broader applications like deepfake detection.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models lack realistic training data and struggle with real-world user needs, despite performing well in academic benchmarks.

Method: REALEDIT introduces 48K training examples and a test set of 9300 examples sourced from Reddit, with human-made edits. A model is trained on this data and evaluated.

Result: REALEDIT outperforms competitors by up to 165 Elo points in human judgment and 92% relative improvement on VIEScore. It also improves deepfake detection F1-score by 14%.

Conclusion: REALEDIT addresses the gap in realistic training data, demonstrating significant performance gains and broader utility in applications like deepfake detection.

Abstract: Existing image editing models struggle to meet real-world demands. Despite
excelling in academic benchmarks, they have yet to be widely adopted for real
user needs. Datasets that power these models use artificial edits, lacking the
scale and ecological validity necessary to address the true diversity of user
requests. We introduce REALEDIT, a large-scale image editing dataset with
authentic user requests and human-made edits sourced from Reddit. REALEDIT
includes a test set of 9300 examples to evaluate models on real user requests.
Our results show that existing models fall short on these tasks, highlighting
the need for realistic training data. To address this, we introduce 48K
training examples and train our REALEDIT model, achieving substantial gains -
outperforming competitors by up to 165 Elo points in human judgment and 92
percent relative improvement on the automated VIEScore metric. We deploy our
model on Reddit, testing it on new requests, and receive positive feedback.
Beyond image editing, we explore REALEDIT's potential in detecting edited
images by partnering with a deepfake detection non-profit. Finetuning their
model on REALEDIT data improves its F1-score by 14 percentage points,
underscoring the dataset's value for broad applications.

</details>


### [150] [Rethinking the Role of Infrastructure in Collaborative Perception](https://arxiv.org/abs/2410.11259)
*Hyunchul Bae, Minhee Kang, Minwoo Song, Heejin Ahn*

Main category: cs.CV

TL;DR: Infrastructure data in Collaborative Perception (CP) improves 3D detection accuracy by 10.30%, and infra-centric CP outperforms vehicle-centric CP by 46.47% in accuracy and noise robustness.


<details>
  <summary>Details</summary>
Motivation: To quantitatively analyze the role of infrastructure data in CP, which is underexplored, and compare vehicle-centric and infra-centric CP approaches.

Method: Quantitative assessment of infrastructure data importance in vehicle-centric CP and comparison with infra-centric CP.

Result: Infrastructure data boosts 3D detection accuracy by 10.30%; infra-centric CP improves accuracy by 46.47% and enhances noise robustness.

Conclusion: Infrastructure data significantly enhances CP performance, with infra-centric CP being more effective than vehicle-centric CP.

Abstract: Collaborative Perception (CP) is a process in which an ego agent receives and
fuses sensor information from surrounding vehicles and infrastructure to
enhance its perception capability. To evaluate the need for infrastructure
equipped with sensors, extensive and quantitative analysis of the role of
infrastructure data in CP is crucial, yet remains underexplored. To address
this gap, we first quantitatively assess the importance of infrastructure data
in existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore,
we compare vehicle-centric CP with infra-centric CP, where the ego agent is now
the infrastructure, to evaluate the effectiveness of each approach. Our results
demonstrate that incorporating infrastructure data improves 3D detection
accuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness
and increases accuracy by up to 46.47% compared with vehicle-centric CP.

</details>


### [151] [The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606)
*Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang*

Main category: cs.CV

TL;DR: LanDiff combines autoregressive language models and diffusion models for text-to-video generation, achieving state-of-the-art performance with a hybrid approach.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing paradigms: language models struggle with visual quality, while diffusion models lack semantic understanding.

Method: Proposes LanDiff, a hybrid framework with a semantic tokenizer, language model for semantic tokens, and streaming diffusion model for refinement.

Result: Achieves 85.43 on VBench T2V benchmark, outperforming Hunyuan Video (13B) and commercial models like Sora. Excels in long video generation.

Conclusion: LanDiff synergizes strengths of both paradigms, setting new benchmarks in text-to-video generation.

Abstract: Recent advancements in text-to-video (T2V) generation have been driven by two
competing paradigms: autoregressive language models and diffusion models.
However, each paradigm has intrinsic limitations: language models struggle with
visual quality and error accumulation, while diffusion models lack semantic
understanding and causal modeling. In this work, we propose LanDiff, a hybrid
framework that synergizes the strengths of both paradigms through
coarse-to-fine generation. Our architecture introduces three key innovations:
(1) a semantic tokenizer that compresses 3D visual features into compact 1D
discrete representations through efficient semantic compression, achieving a
$\sim$14,000$\times$ compression ratio; (2) a language model that generates
semantic tokens with high-level semantic relationships; (3) a streaming
diffusion model that refines coarse semantics into high-fidelity videos.
Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the
VBench T2V benchmark, surpassing the state-of-the-art open-source models
Hunyuan Video (13B) and other commercial models such as Sora, Kling, and
Hailuo. Furthermore, our model also achieves state-of-the-art performance in
long video generation, surpassing other open-source models in this field. Our
demo can be viewed at https://landiff.github.io/.

</details>


### [152] [High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion](https://arxiv.org/abs/2410.11838)
*Junhwa Hur, Charles Herrmann, Saurabh Saxena, Janne Kontkanen, Wei-Sheng Lai, Yichang Shih, Michael Rubinstein, David J. Fleet, Deqing Sun*

Main category: cs.CV

TL;DR: HiFI is a patch-based cascaded pixel diffusion model for high-resolution frame interpolation, excelling in challenging scenarios like repetitive textures and large motion while reducing memory usage and training costs.


<details>
  <summary>Details</summary>
Motivation: Existing frame interpolation methods struggle with high-resolution inputs and challenging cases like repetitive textures, thin objects, and large motion.

Method: HiFI uses a single model performing diffusion at the same resolution, processing patches of inputs and prior solutions, reducing memory usage and training costs.

Result: HiFI achieves comparable or state-of-the-art performance on benchmarks (Vimeo, Xiph, X-Test, SEPE-8K) and outperforms baselines on the new LaMoR dataset.

Conclusion: HiFI addresses high-resolution and complex interpolation challenges efficiently, offering competitive performance and practical advantages.

Abstract: Despite the recent progress, existing frame interpolation methods still
struggle with processing extremely high resolution input and handling
challenging cases such as repetitive textures, thin objects, and large motion.
To address these issues, we introduce a patch-based cascaded pixel diffusion
model for high resolution frame interpolation, HiFI, that excels in these
scenarios while achieving competitive performance on standard benchmarks.
Cascades, which generate a series of images from low to high resolution, can
help significantly with large or complex motion that require both global
context for a coarse solution and detailed context for high resolution output.
However, contrary to prior work on cascaded diffusion models which perform
diffusion on increasingly large resolutions, we use a single model that always
performs diffusion at the same resolution and upsamples by processing patches
of the inputs and the prior solution. At inference time, this drastically
reduces memory usage and allows a single model, solving both frame
interpolation (base model's task) and spatial up-sampling, saving training cost
as well. HiFI excels at high-resolution images and complex repeated textures
that require global context, achieving comparable or state-of-the-art
performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We
further introduce a new dataset, LaMoR, that focuses on particularly
challenging cases, and HiFI significantly outperforms other baselines. Please
visit our project page for video results: https://hifi-diffusion.github.io

</details>


### [153] [UniVST: A Unified Framework for Training-free Localized Video Style Transfer](https://arxiv.org/abs/2410.20084)
*Quanjian Song, Mingbao Lin, Wengyi Zhan, Shuicheng Yan, Liujuan Cao, Rongrong Ji*

Main category: cs.CV

TL;DR: UniVST is a training-free, unified framework for localized video style transfer using diffusion models, outperforming existing methods in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing diffusion-based video style transfer methods, which often lack localized control and temporal consistency.

Method: Combines point-matching mask propagation, AdaIN-guided localized stylization, and sliding-window smoothing with optical flow for enhanced consistency.

Result: UniVST achieves superior performance in preserving object style, temporal consistency, and detail, validated by quantitative and qualitative metrics.

Conclusion: UniVST effectively solves key challenges in video style transfer, offering a robust, training-free solution with high-quality results.

Abstract: This paper presents UniVST, a unified framework for localized video style
transfer based on diffusion models. It operates without the need for training,
offering a distinct advantage over existing diffusion methods that transfer
style across entire videos. The endeavors of this paper comprise: (1) A
point-matching mask propagation strategy that leverages the feature maps from
the DDIM inversion. This streamlines the model's architecture by obviating the
need for tracking models. (2) A training-free AdaIN-guided localized video
stylization mechanism that operates at both the latent and attention levels.
This balances content fidelity and style richness, mitigating the loss of
localized details commonly associated with direct video stylization. (3) A
sliding-window consistent smoothing scheme that harnesses optical flow within
the pixel representation and refines predicted noise to update the latent
space. This significantly enhances temporal consistency and diminishes
artifacts in stylized video. Our proposed UniVST has been validated to be
superior to existing methods in quantitative and qualitative metrics. It
adeptly addresses the challenges of preserving the primary object's style while
ensuring temporal consistency and detail preservation. Our code is available at
https://github.com/QuanjianSong/UniVST.

</details>


### [154] [Confidence Aware Learning for Reliable Face Anti-spoofing](https://arxiv.org/abs/2411.01263)
*Xingming Long, Jie Zhang, Shiguang Shan*

Main category: cs.CV

TL;DR: The paper proposes a Confidence Aware Face Anti-spoofing (CA-FAS) model to address overconfidence in predictions by estimating confidence using Mahalanobis distance and Gaussian distributions.


<details>
  <summary>Details</summary>
Motivation: Current FAS models are overly confident in unfamiliar scenarios, posing risks. The goal is to create a model aware of its limits for reliable liveness detection.

Method: CA-FAS estimates prediction confidence via Mahalanobis distance between samples and Gaussian distributions of live faces and known attacks. Triplet mining optimizes model and Gaussian parameters.

Result: CA-FAS effectively identifies low-confidence samples, improving reliability by filtering out uncertain cases.

Conclusion: The CA-FAS model enhances FAS reliability by recognizing and excluding uncertain predictions, outperforming existing models.

Abstract: Current Face Anti-spoofing (FAS) models tend to make overly confident
predictions even when encountering unfamiliar scenarios or unknown presentation
attacks, which leads to serious potential risks. To solve this problem, we
propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of
its capability boundary, thus achieving reliable liveness detection within this
boundary. To enable the CA-FAS to "know what it doesn't know", we propose to
estimate its confidence during the prediction of each sample. Specifically, we
build Gaussian distributions for both the live faces and the known attacks. The
prediction confidence for each sample is subsequently assessed using the
Mahalanobis distance between the sample and the Gaussians for the "known data".
We further introduce the Mahalanobis distance-based triplet mining to optimize
the parameters of both the model and the constructed Gaussians as a whole.
Extensive experiments show that the proposed CA-FAS can effectively recognize
samples with low prediction confidence and thus achieve much more reliable
performance than other FAS models by filtering out samples that are beyond its
reliable range.

</details>


### [155] [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
*Yongqian Peng, Yuxi Ma, Mengmeng Wang, Yuxuan Wang, Yizhou Wang, Chi Zhang, Yixin Zhu, Zilong Zheng*

Main category: cs.CV

TL;DR: The paper investigates whether Vision-Language Models (VLMs) exhibit combinational creativity, proposing the IEI framework to evaluate and enhance their creative outputs.


<details>
  <summary>Details</summary>
Motivation: To determine if VLMs like GPT-4V and DALLE-3 demonstrate true combinational creativity (synthesizing novel ideas) or merely sophisticated pattern matching.

Method: Introduces the IEI framework (Identification-Explanation-Implication) and uses the CreativeMashup dataset (666 artist-generated visual mashups) for validation.

Result: VLMs surpass average human performance in comprehension tasks but lag behind experts; integrating the IEI framework improves creative generation.

Conclusion: The study provides a theoretical basis for assessing artificial creativity and practical methods to enhance VLM creativity.

Abstract: The ability to combine existing concepts into novel ideas stands as a
fundamental hallmark of human intelligence. Recent advances in Vision-Language
Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their
outputs reflect combinational creativity--defined by M. A. Boden (1998) as
synthesizing novel ideas through combining existing concepts--or sophisticated
pattern matching of training data. Drawing inspiration from cognitive science,
we investigate the combinational creativity of VLMs from the lens of concept
blending. We propose the Identification-Explanation-Implication (IEI)
framework, which decomposes creative processes into three levels: identifying
input spaces, extracting shared attributes, and deriving novel semantic
implications. To validate this framework, we curate CreativeMashup, a
high-quality dataset of 666 artist-generated visual mashups annotated according
to the IEI framework. Through extensive experiments, we demonstrate that in
comprehension tasks, best VLMs have surpassed average human performance while
falling short of expert-level understanding; in generation tasks, incorporating
our IEI framework into the generation pipeline significantly enhances the
creative quality of VLMs' outputs. Our findings establish both a theoretical
foundation for evaluating artificial creativity and practical guidelines for
improving creative generation in VLMs.

</details>


### [156] [Mapping Global Floods with 10 Years of Satellite Radar Data](https://arxiv.org/abs/2411.01411)
*Amit Misra, Kevin White, Simone Fobi Nsutezo, William Straka, Juan Lavista*

Main category: cs.CV

TL;DR: A deep learning model using Sentinel-1 SAR data creates a global flood dataset, enabling cloud-penetrating flood detection and real-time disaster response.


<details>
  <summary>Details</summary>
Motivation: Floods cause significant damage, but global flood datasets with long-term consistency are lacking.

Method: A novel deep learning model processes 10 years of Sentinel-1 SAR imagery for cloud-resistant flood mapping.

Result: The model produces a unique global flood dataset, identifies flood-prone areas, and aids real-time response.

Conclusion: The dataset and model are publicly available to improve flood monitoring and disaster response globally.

Abstract: Floods cause extensive global damage annually, making effective monitoring
essential. While satellite observations have proven invaluable for flood
detection and tracking, comprehensive global flood datasets spanning extended
time periods remain scarce. In this study, we introduce a novel deep learning
flood detection model that leverages the cloud-penetrating capabilities of
Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling
consistent flood extent mapping in through cloud cover and in both day and
night conditions. By applying this model to 10 years of SAR data, we create a
unique, longitudinal global flood extent dataset with predictions unaffected by
cloud coverage, offering comprehensive and consistent insights into
historically flood-prone areas over the past decade. We use our model
predictions to identify historically flood-prone areas in Ethiopia and
demonstrate real-time disaster response capabilities during the May 2024 floods
in Kenya. Additionally, our longitudinal analysis reveals potential increasing
trends in global flood extent over time, although further validation is
required to explore links to climate change. To maximize impact, we provide
public access to both our model predictions and a code repository, empowering
researchers and practitioners worldwide to advance flood monitoring and enhance
disaster response strategies.

</details>


### [157] [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
*Ling You, Wenxuan Huang, Xinni Xie, Xiangyi Wei, Bangyan Li, Shaohui Lin, Yang Li, Changbo Wang*

Main category: cs.CV

TL;DR: TimeSoccer is an end-to-end MLLM for soccer video captioning, addressing limitations of prior methods by jointly predicting timestamps and captions in a single pass, achieving SoTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing soccer MLLMs rely on temporal priors or complex two-step methods, lacking end-to-end processing and global context for long matches.

Method: TimeSoccer uses MoFA-Select for motion-aware frame compression and joint timestamp-caption prediction, enabling global context modeling.

Result: Achieves SoTA performance in SDVC, generating high-quality commentary with accurate temporal alignment.

Conclusion: TimeSoccer provides an effective end-to-end solution for soccer video captioning, outperforming traditional methods.

Abstract: Soccer is a globally popular sporting event, typically characterized by long
matches and distinctive highlight moments. Recent advances in Multimodal Large
Language Models (MLLMs) offer promising capabilities in temporal grounding and
video understanding, soccer commentary generation often requires precise
temporal localization and semantically rich descriptions over long-form video.
However, existing soccer MLLMs often rely on the temporal a priori for caption
generation, so they cannot process the soccer video end-to-end. While some
traditional approaches follow a two-step paradigm that is complex and fails to
capture the global context to achieve suboptimal performance. To solve the
above issues, we present TimeSoccer, the first end-to-end soccer MLLM for
Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos.
TimeSoccer jointly predicts timestamps and generates captions in a single pass,
enabling global context modeling across 45-minute matches. To support long
video understanding of soccer matches, we introduce MoFA-Select, a
training-free, motion-aware frame compression module that adaptively selects
representative frames via a coarse-to-fine strategy, and incorporates
complementary training paradigms to strengthen the model's ability to handle
long temporal sequences. Extensive experiments demonstrate that our TimeSoccer
achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end
form, generating high-quality commentary with accurate temporal alignment and
strong semantic relevance.

</details>


### [158] [Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement](https://arxiv.org/abs/2411.09413)
*Wenxing Liu, Yueran Pan, Dong Zhang, Hongzhu Deng, Xiaobing Zou, Ming Li*

Main category: cs.CV

TL;DR: A zero-shot ASD detection framework using script-centric behavioral understanding and emotional enhancement achieves high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of insufficient ASD diagnostic samples and lack of interpretability in current supervised learning methods.

Method: Uses multimodal script transcription, emotion textualization, and domain-specific prompt engineering with LLMs for zero-shot/few-shot ASD detection.

Result: Achieves 95.24% F1-score in diagnosing ASD in children (avg. age 2) with interpretable rationales.

Conclusion: Demonstrates potential of LLMs for accurate, interpretable ASD diagnosis, advancing clinical adoption.

Abstract: The early diagnosis of autism spectrum disorder (ASD) is critically dependent
on systematic observation and analysis of children's social behaviors. While
current methodologies predominantly utilize supervised learning approaches,
their clinical adoption faces two principal limitations: insufficient ASD
diagnostic samples and inadequate interpretability of the detection outcomes.
This paper presents a novel zero-shot ASD detection framework based on
script-centric behavioral understanding with emotional enhancement, which is
designed to overcome the aforementioned clinical constraints. The proposed
pipeline automatically converts audio-visual data into structured behavioral
text scripts through computer vision techniques, subsequently capitalizing on
the generalization capabilities of large language models (LLMs) for
zero-shot/few-shot ASD detection. Three core technical contributions are
introduced: (1) A multimodal script transcription module transforming
behavioral cues into structured textual representations. (2) An emotion
textualization module encoding emotional dynamics as the contextual features to
augment behavioral understanding. (3) A domain-specific prompt engineering
strategy enables the injection of clinical knowledge into LLMs. Our method
achieves an F1-score of 95.24\% in diagnosing ASD in children with an average
age of two years while generating interpretable detection rationales. This work
opens up new avenues for leveraging the power of LLMs in analyzing and
understanding ASD-related human behavior, thereby enhancing the accuracy of
assisted autism diagnosis.

</details>


### [159] [Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses](https://arxiv.org/abs/2411.10013)
*Yongfan Liu, Hyoukjun Kwon*

Main category: cs.CV

TL;DR: The paper introduces ML-accelerated methods for stereo depth estimation in AR, eliminating preprocessing and replacing cost volume with efficient operators, achieving lower latency and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Preprocessing and non-ML computations in stereo depth estimation cause high latency, hindering real-time AR applications. The goal is to reduce latency while maintaining robustness.

Method: Proposes homography matrix prediction with rectification positional encoding (RPE) to eliminate preprocessing, and replaces cost volume with group-pointwise convolution and cosine similarity approximation. Introduces MultiHeadDepth and HomoDepth models.

Result: MultiHeadDepth improves accuracy by 11.8-30.3% and reduces latency by 22.9-25.2%. HomoDepth cuts end-to-end latency by 44.5% and reduces AbsRel error by 10.0-24.3% with multi-task learning.

Conclusion: The approaches effectively reduce latency and enhance performance for real-time AR applications, demonstrating the efficacy of ML-accelerated methods.

Abstract: Stereo depth estimation is a fundamental component in augmented reality (AR),
which requires low latency for real-time processing. However, preprocessing
such as rectification and non-ML computations such as cost volume require
significant amount of latency exceeding that of an ML model itself, which
hinders the real-time processing required by AR. Therefore, we develop
alternative approaches to the rectification and cost volume that consider ML
acceleration (GPU and NPUs) in recent hardware. For pre-processing, we
eliminate it by introducing homography matrix prediction network with a
rectification positional encoding (RPE), which delivers both low latency and
robustness to unrectified images. For cost volume, we replace it with a
group-pointwise convolution-based operator and approximation of cosine
similarity based on layernorm and dot product. Based on our approaches, we
develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth +
removing pre-processing) models. MultiHeadDepth provides 11.8-30.3%
improvements in accuracy and 22.9-25.2% reduction in latency compared to a
state-of-the-art depth estimation model for AR glasses from industry.
HomoDepth, which can directly process unrectified images, reduces the
end-to-end latency by 44.5%. We also introduce a multi-task learning method to
handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by
10.0-24.3%. The overall results demonstrate the efficacy of our approaches,
which not only reduce the inference latency but also improve the model
performance. Our code is available at
https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth

</details>


### [160] [EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild](https://arxiv.org/abs/2411.14280)
*Yumeng Liu, Xiaoxiao Long, Zemin Yang, Yuan Liu, Marc Habermann, Christian Theobalt, Yuexin Ma, Wenping Wang*

Main category: cs.CV

TL;DR: The paper proposes a method to reconstruct hand-object interactions from single-view images using foundational models and prior-guided optimization, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Single-view reconstruction of hand-object interactions is challenging due to ambiguities, occlusions, and the diversity of hand poses and object shapes. Foundational models can provide strong priors for this task.

Method: A pipeline estimates hand pose and object shape using off-the-shelf models, followed by prior-guided optimization to refine the reconstruction.

Result: The method outperforms baselines and accurately reconstructs diverse hand-object interactions across datasets.

Conclusion: The approach effectively leverages foundational models and optimization to address the challenges of single-view reconstruction.

Abstract: Our work aims to reconstruct hand-object interactions from a single-view
image, which is a fundamental but ill-posed task. Unlike methods that
reconstruct from videos, multi-view images, or predefined 3D templates,
single-view reconstruction faces significant challenges due to inherent
ambiguities and occlusions. These challenges are further amplified by the
diverse nature of hand poses and the vast variety of object shapes and sizes.
Our key insight is that current foundational models for segmentation,
inpainting, and 3D reconstruction robustly generalize to in-the-wild images,
which could provide strong visual and geometric priors for reconstructing
hand-object interactions. Specifically, given a single image, we first design a
novel pipeline to estimate the underlying hand pose and object shape using
off-the-shelf large models. Furthermore, with the initial reconstruction, we
employ a prior-guided optimization scheme, which optimizes hand pose to comply
with 3D physical constraints and the 2D input image content. We perform
experiments across several datasets and show that our method consistently
outperforms baselines and faithfully reconstructs a diverse set of hand-object
interactions. Here is the link of our project page:
https://lym29.github.io/EasyHOI-page/

</details>


### [161] [Omni-IML: Towards Unified Image Manipulation Localization](https://arxiv.org/abs/2411.14823)
*Chenfan Qu, Yiwu Zhong, Fengjun Guo, Lianwen Jin*

Main category: cs.CV

TL;DR: Omni-IML is a generalist model unifying Image Manipulation Localization (IML) tasks, outperforming task-specific methods with adaptive encoding, dynamic decoding, and anomaly enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing IML methods are task-specific and degrade in joint training, limiting real-world applications. Omni-IML aims to unify diverse IML tasks.

Method: Omni-IML uses a Modal Gate Encoder, Dynamic Weight Decoder, and Anomaly Enhancement module. It also introduces Omni-273k dataset with natural language descriptions.

Result: Omni-IML achieves state-of-the-art performance across four major IML tasks.

Conclusion: Omni-IML provides a practical, generalist solution for image forensics, with public code and dataset.

Abstract: Existing Image Manipulation Localization (IML) methods mostly rely heavily on
task-specific designs, making them perform well only on the target IML task,
while joint training on multiple IML tasks causes significant performance
degradation, hindering real applications.
  To this end, we propose Omni-IML, the first generalist model designed to
unify IML across diverse tasks.
  Specifically, Omni-IML achieves generalization through three key components:
(1) a Modal Gate Encoder, which adaptively selects the optimal encoding
modality per sample, (2) a Dynamic Weight Decoder, which dynamically adjusts
decoder filters to the task at hand, and (3) an Anomaly Enhancement module that
leverages box supervision to highlight the tampered regions and facilitate the
learning of task-agnostic features.
  Beyond localization, to support interpretation of the tampered images, we
construct Omni-273k, a large high-quality dataset that includes natural
language descriptions of tampered artifact. It is annotated through our
automatic, chain-of-thoughts annotation technique.
  We also design a simple-yet-effective interpretation module to better utilize
these descriptive annotations.
  Our extensive experiments show that our single Omni-IML model achieves
state-of-the-art performance across all four major IML tasks, providing a
valuable solution for practical deployment and a promising direction of
generalist models in image forensics. Our code and dataset will be publicly
available.

</details>


### [162] [HANDI: Hand-Centric Text-and-Image Conditioned Video Generation](https://arxiv.org/abs/2412.04189)
*Yayuan Li, Zhi Cao, Jason J. Corso*

Main category: cs.CV

TL;DR: A new diffusion-based method for hand-centric video generation improves visual detail by automatically identifying motion areas and using a Hand Refinement Loss for smoother hand poses.


<details>
  <summary>Details</summary>
Motivation: Current video generation methods struggle with detailed hand motions in complex environments, necessitating a specialized approach.

Method: The method uses a diffusion-based approach with automatic motion area generation guided by visual context and text prompts, plus a Hand Refinement Loss for consistent hand poses.

Result: The method outperforms state-of-the-art techniques on datasets like EpicKitchens and Ego4D, enhancing action clarity and hand motion detail.

Conclusion: The proposed method effectively addresses challenges in hand-centric video generation, offering improved detail and consistency.

Abstract: Despite the recent strides in video generation, state-of-the-art methods
still struggle with elements of visual detail. One particularly challenging
case is the class of videos in which the intricate motion of the hand coupled
with a mostly stable and otherwise distracting environment is necessary to
convey the execution of some complex action and its effects. To address these
challenges, we introduce a new method for video generation that focuses on
hand-centric actions. Our diffusion-based method incorporates two distinct
innovations. First, we propose an automatic method to generate the motion area
-- the region in the video in which the detailed activities occur -- guided by
both the visual context and the action text prompt, rather than assuming this
region can be provided manually as is now commonplace. Second, we introduce a
critical Hand Refinement Loss to guide the diffusion model to focus on smooth
and consistent hand poses. We evaluate our method on challenging augmented
datasets based on EpicKitchens and Ego4D, demonstrating significant
improvements over state-of-the-art methods in terms of action clarity,
especially of the hand motion in the target region, across diverse environments
and actions. Video results can be found in
https://zhicaoisexcited.github.io/project_page

</details>


### [163] [3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement](https://arxiv.org/abs/2412.18565)
*Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy*

Main category: cs.CV

TL;DR: 3DEnhancer improves low-resolution 3D models using a multi-view latent diffusion model, ensuring multi-view consistency and high-quality outputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with low-resolution 3D datasets and poor multi-view consistency, limiting view synthesis and 3D model generation.

Method: Uses a pose-aware encoder, diffusion-based denoiser, data augmentation, and multi-view attention with epipolar aggregation to refine and enhance 3D inputs.

Result: Outperforms existing methods in multi-view enhancement and 3D optimization, delivering consistent, high-quality outputs.

Conclusion: 3DEnhancer provides a robust solution for enhancing coarse 3D models while maintaining multi-view coherence.

Abstract: Despite advances in neural rendering, due to the scarcity of high-quality 3D
datasets and the inherent limitations of multi-view diffusion models, view
synthesis and 3D model generation are restricted to low resolutions with
suboptimal multi-view consistency. In this study, we present a novel 3D
enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent
diffusion model to enhance coarse 3D inputs while preserving multi-view
consistency. Our method includes a pose-aware encoder and a diffusion-based
denoiser to refine low-quality multi-view images, along with data augmentation
and a multi-view attention module with epipolar aggregation to maintain
consistent, high-quality 3D outputs across views. Unlike existing video-based
approaches, our model supports seamless multi-view enhancement with improved
coherence across diverse viewing angles. Extensive evaluations show that
3DEnhancer significantly outperforms existing methods, boosting both multi-view
enhancement and per-instance 3D optimization tasks.

</details>


### [164] [Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2501.09552)
*Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga*

Main category: cs.CV

TL;DR: An AI-based pipeline for detecting PHI in medical images achieves high performance (metrics >0.9) using YOLOv11, EasyOCR, and GPT-4o, with YOLOv11+GPT-4o being the best but costly. GPT-4o alone shows potential but lower performance. Recommendations include fine-tuning detection models and using OCR tools for cost-effectiveness.


<details>
  <summary>Details</summary>
Motivation: Ensuring privacy in medical image sharing requires reliable PHI detection, but existing AI solutions lack thorough evaluation, hindering robust tool development.

Method: A three-component pipeline (text detection, extraction, analysis) is tested with YOLOv11, EasyOCR, and GPT-4o, evaluated on precision, recall, F1 score, and accuracy.

Result: All setups perform well (metrics >0.9), with YOLOv11+GPT-4o being the best but costly. GPT-4o alone shows lower performance but highlights multimodal potential.

Conclusion: Fine-tuning detection models and using OCR tools is recommended for optimal performance and cost. GPT-4o aids flexible text analysis.

Abstract: De-identification of medical images is a critical step to ensure privacy
during data sharing in research and clinical settings. The initial step in this
process involves detecting Protected Health Information (PHI), which can be
found in image metadata or imprinted within image pixels. Despite the
importance of such systems, there has been limited evaluation of existing
AI-based solutions, creating barriers to the development of reliable and robust
tools. In this study, we present an AI-based pipeline for PHI detection,
comprising three key components: text detection, text extraction, and text
analysis. We benchmark three models, YOLOv11, EasyOCR, and GPT-4o, across
different setups corresponding to these components, evaluating the performance
based on precision, recall, F1 score, and accuracy. All setups demonstrate
excellent PHI detection, with all metrics exceeding 0.9. The combination of
YOLOv11 for text localization and GPT-4o for extraction and analysis yields the
best results. However, this setup incurs higher costs due to GPT-4o's token
generation. Conversely, an end-to-end pipeline that relies solely on GPT-4o
shows lower performance but highlights the potential of multimodal models for
complex tasks. We recommend fine-tuning a dedicated object detection model and
utilizing built-in OCR tools to achieve optimal performance and
cost-effectiveness. Additionally, leveraging language models such as GPT-4o can
facilitate thorough and flexible analysis of text content.

</details>


### [165] [Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation](https://arxiv.org/abs/2501.11153)
*Huu Phong Nguyen, Shekhar Madhav Khairnar, Sofia Garces Palacios, Amr Al-Abbas, Melissa E. Hogg, Amer H. Zureikat, Patricio M. Polanco, Herbert Zeh III, Ganesh Sankaranarayanan*

Main category: cs.CV

TL;DR: Proposes Kinematics Adaptive Frame Recognition (KAFR) to reduce redundant frames in surgical videos, improving efficiency and accuracy by tracking tool movements.


<details>
  <summary>Details</summary>
Motivation: Surgical videos are lengthy, challenging AI models. Increasing video volume demands innovative techniques for efficient analysis.

Method: Uses YOLOv8 for tool detection, computes frame similarity via tool movement, and classifies segments with X3D CNN.

Result: Tested on Gastrojejunostomy and Pancreaticojejunostomy datasets, showing effectiveness in reducing redundancy.

Conclusion: KAFR efficiently reduces dataset size and computation time while retaining accuracy, addressing challenges in surgical video analysis.

Abstract: The interest in leveraging Artificial Intelligence (AI) for surgical
procedures to automate analysis has witnessed a significant surge in recent
years. One of the primary tools for recording surgical procedures and
conducting subsequent analyses, such as performance assessment, is through
videos. However, these operative videos tend to be notably lengthy compared to
other fields, spanning from thirty minutes to several hours, which poses a
challenge for AI models to effectively learn from them. Despite this challenge,
the foreseeable increase in the volume of such videos in the near future
necessitates the development and implementation of innovative techniques to
tackle this issue effectively. In this article, we propose a novel technique
called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently
eliminate redundant frames to reduce dataset size and computation time while
retaining useful frames to improve accuracy. Specifically, we compute the
similarity between consecutive frames by tracking the movement of surgical
tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is
utilized to detect tools presented in the scene, $ii)$ Similarity phase:
Similarities between consecutive frames are computed by estimating variation in
the spatial positions and velocities of the tools, $iii$) Classification phase:
An X3D CNN is trained to classify segmentation. We evaluate the effectiveness
of our approach by analyzing datasets obtained through retrospective reviews of
cases at two referral centers. The newly annotated Gastrojejunostomy (GJ)
dataset covers procedures performed between 2017 and 2021, while the previously
annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the
same centers.

</details>


### [166] [A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches](https://arxiv.org/abs/2501.19184)
*Luca Ciampi, Ali Azmoudeh, Elif Ecem Akbaba, Erdi Sarıtaş, Ziya Ata Yazıcı, Hazım Kemal Ekenel, Giuseppe Amato, Fabrizio Falchi*

Main category: cs.CV

TL;DR: The paper reviews class-agnostic counting (CAC) methodologies, categorizing them into reference-based, reference-less, and open-world text-guided approaches, and evaluates their performance on benchmarks like FSC-147 and CARPK.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing counting methods restricted to known classes, the paper explores CAC for counting objects across arbitrary categories, mimicking human flexibility.

Method: The study categorizes CAC approaches into three paradigms, reviews 29 methodologies, and evaluates them on standard benchmarks.

Result: Reference-based methods lead in performance, while open-world text-guided approaches offer flexibility. Results are benchmarked on FSC-147 and CARPK datasets.

Conclusion: The survey highlights CAC advancements, identifies challenges like annotation dependency, and suggests future research directions.

Abstract: Visual object counting has recently shifted towards class-agnostic counting
(CAC), which addresses the challenge of counting objects across arbitrary
categories -- a crucial capability for flexible and generalizable counting
systems. Unlike humans, who effortlessly identify and count objects from
diverse categories without prior knowledge, most existing counting methods are
restricted to enumerating instances of known classes, requiring extensive
labeled datasets for training and struggling in open-vocabulary settings. In
contrast, CAC aims to count objects belonging to classes never seen during
training, operating in a few-shot setting. In this paper, we present the first
comprehensive review of CAC methodologies. We propose a taxonomy to categorize
CAC approaches into three paradigms based on how target object classes can be
specified: reference-based, reference-less, and open-world text-guided.
Reference-based approaches achieve state-of-the-art performance by relying on
exemplar-guided mechanisms. Reference-less methods eliminate exemplar
dependency by leveraging inherent image patterns. Finally, open-world
text-guided methods use vision-language models, enabling object class
descriptions via textual prompts, offering a flexible and promising solution.
Based on this taxonomy, we provide an overview of the architectures of 29 CAC
approaches and report their results on gold-standard benchmarks. We compare
their performance and discuss their strengths and limitations. Specifically, we
present results on the FSC-147 dataset, setting a leaderboard using
gold-standard metrics, and on the CARPK dataset to assess generalization
capabilities. Finally, we offer a critical discussion of persistent challenges,
such as annotation dependency and generalization, alongside future directions.
We believe this survey will be a valuable resource, showcasing CAC advancements
and guiding future research.

</details>


### [167] [Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation](https://arxiv.org/abs/2502.03370)
*Muhammad Ahtsam Naeem, Muhammad Asim Saleem, Muhammad Imran Sharif, Shahzad Akber, Sajjad Saleem, Zahid Akhtar, Kamran Siddique*

Main category: cs.CV

TL;DR: An autonomous method using image processing and machine learning detects late blight disease in potato leaves with 99% accuracy.


<details>
  <summary>Details</summary>
Motivation: Potatoes are prone to diseases like Early and Late Blight, which reduce yield. Early detection is crucial for improving crop productivity.

Method: The method involves four phases: image enhancement via Histogram Equalization, feature extraction using Deep CNN, feature selection with a wrapper-based approach, and classification using SVM.

Result: The method achieves 99% accuracy by selecting 550 features.

Conclusion: The proposed approach effectively detects late blight disease, aiding in early intervention to enhance potato crop yield.

Abstract: The potato is a widely grown crop in many regions of the world. In recent
decades, potato farming has gained incredible traction in the world. Potatoes
are susceptible to several illnesses that stunt their development. This plant
seems to have significant leaf disease. Early Blight and Late Blight are two
prevalent leaf diseases that affect potato plants. The early detection of these
diseases would be beneficial for enhancing the yield of this crop. The ideal
solution is to use image processing to identify and analyze these disorders.
Here, we present an autonomous method based on image processing and machine
learning to detect late blight disease affecting potato leaves. The proposed
method comprises four different phases: (1) Histogram Equalization is used to
improve the quality of the input image; (2) feature extraction is performed
using a Deep CNN model, then these extracted features are concatenated; (3)
feature selection is performed using wrapper-based feature selection; (4)
classification is performed using an SVM classifier and its variants. This
proposed method achieves the highest accuracy of 99% using SVM by selecting 550
features.

</details>


### [168] [EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds](https://arxiv.org/abs/2502.05857)
*Lu Chen, Yizhou Wang, Shixiang Tang, Qianhong Ma, Tong He, Wanli Ouyang, Xiaowei Zhou, Hujun Bao, Sida Peng*

Main category: cs.CV

TL;DR: EgoAgent is a joint predictive agent model that unifies perception, prediction, and action in a single transformer, outperforming separate models.


<details>
  <summary>Details</summary>
Motivation: Existing methods train separate models for perception, prediction, and action, limiting mutual learning. EgoAgent aims to integrate these abilities cohesively.

Method: EgoAgent uses interleaved sequential modeling with causal attention and a joint embedding-action-prediction architecture with temporal asymmetric branches.

Result: EgoAgent excels in tasks like image classification, egocentric future state prediction, and 3D human motion prediction.

Conclusion: EgoAgent demonstrates the effectiveness of unifying perception, prediction, and action in a single framework, with plans to release code and models.

Abstract: This paper addresses the task of learning an agent model behaving like
humans, which can jointly perceive, predict, and act in egocentric worlds.
Previous methods usually train separate models for these three abilities, which
prevents them from learning from each other. In this paper, we propose a joint
predictive agent model, named EgoAgent, that simultaneously learns to represent
the world, predict future states, and take reasonable actions within a single
transformer. EgoAgent introduces two innovations to learn from the causal and
temporally intertwined nature of these abilities: (1) Interleaved sequential
modeling of states and actions with the causal attention mechanism, and (2) A
joint embedding-action-prediction architecture featuring temporal asymmetric
predictor-observer branches. Integrating these designs based on JEPA, EgoAgent
unifies these capabilities in a cohesive learning framework. Comprehensive
evaluations of EgoAgent on representative tasks such as image classification,
egocentric future state prediction, and 3D human motion prediction tasks
demonstrate the superiority of our method. The code and trained model will be
released for reproducibility.

</details>


### [169] [NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary](https://arxiv.org/abs/2503.00063)
*Zezeng Li, Xiaoyu Du, Na Lei, Liming Chen, Weimin Wang*

Main category: cs.CV

TL;DR: NoPain introduces an optimal transport-based method to generate transferable adversarial point clouds by identifying singular boundaries in the data manifold, outperforming existing approaches in efficiency and transferability.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on point clouds overfit to specific models, limiting transferability. NoPain addresses this by focusing on the data distribution itself.

Method: NoPain uses optimal transport to map noise to the target feature space, identifies singular boundaries, and samples along them to generate adversarial point clouds without iterative updates.

Result: NoPain outperforms baseline methods in transferability and efficiency and remains effective against defense strategies.

Conclusion: NoPain provides a novel, efficient, and transferable approach for adversarial point cloud attacks by leveraging data manifold singularities.

Abstract: Adversarial attacks exploit the vulnerability of deep models against
adversarial samples. Existing point cloud attackers are tailored to specific
models, iteratively optimizing perturbations based on gradients in either a
white-box or black-box setting. Despite their promising attack performance,
they often struggle to produce transferable adversarial samples due to
overfitting the specific parameters of surrogate models. To overcome this
issue, we shift our focus to the data distribution itself and introduce a novel
approach named NoPain, which employs optimal transport (OT) to identify the
inherent singular boundaries of the data manifold for cross-network point cloud
attacks. Specifically, we first calculate the OT mapping from noise to the
target feature space, then identify singular boundaries by locating
non-differentiable positions. Finally, we sample along singular boundaries to
generate adversarial point clouds. Once the singular boundaries are determined,
NoPain can efficiently produce adversarial samples without the need of
iterative updates or guidance from the surrogate classifiers. Extensive
experiments demonstrate that the proposed end-to-end method outperforms
baseline approaches in terms of both transferability and efficiency, while also
maintaining notable advantages even against defense strategies. Code and model
are available at https://github.com/cognaclee/nopain

</details>


### [170] [STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks](https://arxiv.org/abs/2503.02689)
*Tianqing Zhang, Kairong Yu, Xian Zhong, Hongwei Wang, Qi Xu, Qiang Zhang*

Main category: cs.CV

TL;DR: STAA-SNN is a novel Spiking Neural Network framework that uses spatial-temporal attention and position encoding to improve performance, achieving state-of-the-art results on neuromorphic and static datasets.


<details>
  <summary>Details</summary>
Motivation: The performance gap between SNNs and ANNs hinders SNN adoption. STAA-SNN aims to bridge this gap by capturing spatial-temporal dependencies more effectively.

Method: Introduces spike-driven self-attention, position encoding, step attention for feature amplification, and time-step random dropout to avoid local optima.

Result: Achieves 97.14%, 82.05%, and 70.40% on CIFAR-10, CIFAR-100, and ImageNet, respectively, with improved performance using fewer time steps.

Conclusion: STAA-SNN effectively captures spatial-temporal dependencies, demonstrating superior performance and generalization, making it a promising SNN framework.

Abstract: Spiking Neural Networks (SNNs) have gained significant attention due to their
biological plausibility and energy efficiency, making them promising
alternatives to Artificial Neural Networks (ANNs). However, the performance gap
between SNNs and ANNs remains a substantial challenge hindering the widespread
adoption of SNNs. In this paper, we propose a Spatial-Temporal Attention
Aggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures
both spatial and temporal dependencies. First, we introduce a spike-driven
self-attention mechanism specifically designed for SNNs. Additionally, we
pioneeringly incorporate position encoding to integrate latent temporal
relationships into the incoming features. For spatial-temporal information
aggregation, we employ step attention to selectively amplify relevant features
at different steps. Finally, we implement a time-step random dropout strategy
to avoid local optima. As a result, STAA-SNN effectively captures both spatial
and temporal dependencies, enabling the model to analyze complex patterns and
make accurate predictions. The framework demonstrates exceptional performance
across diverse datasets and exhibits strong generalization capabilities.
Notably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets
CIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the
static datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore,
our model exhibits improved performance ranging from 0.33\% to 2.80\% with
fewer time steps. The code for the model is available on GitHub.

</details>


### [171] [Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks](https://arxiv.org/abs/2503.03144)
*Kairong Yu, Chengting Yu, Tianqing Zhang, Xiaochen Zhao, Shu Yang, Hongwei Wang, Qiang Zhang, Qi Xu*

Main category: cs.CV

TL;DR: A novel logit distillation method for Spiking Neural Networks (SNNs) improves performance by leveraging temporal separation and entropy regularization, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between SNNs and ANNs by addressing limitations in current training methods and leveraging SNNs' spatiotemporal properties.

Method: Proposes a logit distillation method with temporal separation and entropy regularization, focusing on logits across time steps rather than aggregated features.

Result: The method outperforms prior SNN distillation strategies, including logit, feature, and hybrid approaches.

Conclusion: The proposed method effectively enhances SNN performance, with code to be made available on GitHub.

Abstract: Spiking Neural Networks (SNNs), inspired by the human brain, offer
significant computational efficiency through discrete spike-based information
transfer. Despite their potential to reduce inference energy consumption, a
performance gap persists between SNNs and Artificial Neural Networks (ANNs),
primarily due to current training methods and inherent model limitations. While
recent research has aimed to enhance SNN learning by employing knowledge
distillation (KD) from ANN teacher networks, traditional distillation
techniques often overlook the distinctive spatiotemporal properties of SNNs,
thus failing to fully leverage their advantages. To overcome these challenge,
we propose a novel logit distillation method characterized by temporal
separation and entropy regularization. This approach improves existing SNN
distillation techniques by performing distillation learning on logits across
different time steps, rather than merely on aggregated output features.
Furthermore, the integration of entropy regularization stabilizes model
optimization and further boosts the performance. Extensive experimental results
indicate that our method surpasses prior SNN distillation strategies, whether
based on logit distillation, feature distillation, or a combination of both.
The code will be available on GitHub.

</details>


### [172] [4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges](https://arxiv.org/abs/2503.24091)
*Xiangyuan Peng, Miao Tang, Huawei Sun, Kay Bierzynski, Lorenzo Servadei, Robert Wille*

Main category: cs.CV

TL;DR: This paper reviews 4D mmWave radar research in adverse environments, covering datasets, learning-based methods, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive reviews on 4D mmWave radar performance in adverse conditions like rain, snow, and fog.

Method: The paper reviews existing datasets and learning-based methods for 4D mmWave radar in adverse environments.

Result: It highlights current research gaps and proposes future directions for improving radar applications in harsh conditions.

Conclusion: This is the first review focusing on 4D mmWave radar in adverse environments, providing a foundation for future advancements.

Abstract: Intelligent transportation systems require accurate and reliable sensing.
However, adverse environments, such as rain, snow, and fog, can significantly
degrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not
only provides 3D point clouds and velocity measurements but also maintains
robustness in challenging conditions. Recently, research on 4D mmWave radar
under adverse environments has been growing, but a comprehensive review is
still lacking. To bridge this gap, this work reviews the current research on 4D
mmWave radar under adverse environments. First, we present an overview of
existing 4D mmWave radar datasets encompassing diverse weather and lighting
scenarios. Subsequently, we analyze existing learning-based methods leveraging
4D mmWave radar to enhance performance according to different adverse
conditions. Finally, the challenges and potential future directions are
discussed for advancing 4D mmWave radar applications in harsh environments. To
the best of our knowledge, this is the first review specifically concentrating
on 4D mmWave radar in adverse environments. The related studies are listed at:
https://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments.

</details>


### [173] [Two-stage deep learning framework for the restoration of incomplete-ring PET images](https://arxiv.org/abs/2504.00816)
*Yeqi Fang, Rong Zhou*

Main category: cs.CV

TL;DR: A two-stage deep-learning framework improves image quality in incomplete-ring PET scanners by predicting missing sinogram data and refining reconstructions, achieving high PSNR and SSIM scores.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in incomplete-ring PET scanners due to data incompleteness and geometric inconsistencies.

Method: Two-stage pipeline: 1) Projection-domain Attention U-Net predicts missing sinogram data; 2) OSEM reconstruction followed by U-Net-diffusion module for artefact removal and detail restoration.

Result: Preserves anatomical structures and tracer distribution with PSNR of 30.92 dB and SSIM of 0.9708, outperforming previous CNN-based methods.

Conclusion: The framework provides an effective, high-speed solution for incomplete-ring PET imaging without requiring TOF information.

Abstract: Positron Emission Tomography (PET) is an important molecular imaging tool
widely used in medicine. Traditional PET systems rely on complete detector
rings for full angular coverage and reliable data collection. However,
incomplete-ring PET scanners have emerged due to hardware failures, cost
constraints, or specific clinical needs. Standard reconstruction algorithms
often suffer from performance degradation with these systems because of reduced
data completeness and geometric inconsistencies. We present a two-stage
deep-learning framework that, without incorporating any time-of-flight (TOF)
information, restores high-quality images from data with about 50% missing
coincidences - double the loss levels previously addressed by CNN-based
methods. The pipeline operates in two stages: a projection-domain Attention
U-Net first predicts the missing sections of the sinogram by leveraging spatial
context from neighbouring slices, after which the completed data are
reconstructed with OSEM algorithm and passed to a U-Net-diffusion module that
removes residual artefacts while reinstating high-frequency detail. Using 206
brain volumes from a public dataset, the result shows that our model
successfully preserves most anatomical structures and tracer distribution
features with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher
inference speed, thus providing an effective solution for incomplete-ring PET
imaging.

</details>


### [174] [Video-Bench: Human-Aligned Video Generation Benchmark](https://arxiv.org/abs/2504.04907)
*Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, Yongxin Ni*

Main category: cs.CV

TL;DR: Video-Bench is a new benchmark for video generation assessment, combining MLLMs with few-shot scoring and chain-of-query techniques to better align with human preferences.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack alignment with human judgments or have limited understanding of video quality. Video-Bench aims to bridge this gap.

Method: Uses MLLMs, few-shot scoring, and chain-of-query techniques for structured, scalable evaluation.

Result: Outperforms traditional benchmarks in aligning with human preferences and offers more objective insights.

Conclusion: Video-Bench provides a superior, scalable solution for video generation assessment, potentially surpassing human judgment in objectivity.

Abstract: Video generation assessment is essential for ensuring that generative models
produce visually realistic, high-quality videos while aligning with human
expectations. Current video generation benchmarks fall into two main
categories: traditional benchmarks, which use metrics and embeddings to
evaluate generated video quality across multiple dimensions but often lack
alignment with human judgments; and large language model (LLM)-based
benchmarks, though capable of human-like reasoning, are constrained by a
limited understanding of video quality metrics and cross-modal consistency. To
address these challenges and establish a benchmark that better aligns with
human preferences, this paper introduces Video-Bench, a comprehensive benchmark
featuring a rich prompt suite and extensive evaluation dimensions. This
benchmark represents the first attempt to systematically leverage MLLMs across
all dimensions relevant to video generation assessment in generative models. By
incorporating few-shot scoring and chain-of-query techniques, Video-Bench
provides a structured, scalable approach to generated video evaluation.
Experiments on advanced models including Sora demonstrate that Video-Bench
achieves superior alignment with human preferences across all dimensions.
Moreover, in instances where our framework's assessments diverge from human
evaluations, it consistently offers more objective and accurate insights,
suggesting an even greater potential advantage over traditional human judgment.

</details>


### [175] [Perception Encoder: The best visual embeddings are not at the output of the network](https://arxiv.org/abs/2504.13181)
*Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollár, Christoph Feichtenhofer*

Main category: cs.CV

TL;DR: The Perception Encoder (PE) is a vision encoder trained via contrastive vision-language learning, achieving top results in various tasks like classification, Q&A, and spatial tasks by extracting hidden embeddings from intermediate layers.


<details>
  <summary>Details</summary>
Motivation: To create a general-purpose vision encoder that performs well across diverse tasks without task-specific pretraining objectives.

Method: Uses contrastive vision-language training, scaled image pretraining, and a robust video data engine, with two alignment methods (language and spatial) to extract hidden embeddings.

Result: Achieves state-of-the-art results in zero-shot classification (86.6 ImageNet, 76.9 Kinetics-400), Q&A (94.6 DocVQA), and spatial tasks (66.0 COCO mAP).

Conclusion: PE demonstrates that contrastive vision-language training alone can produce versatile embeddings, enabling strong performance across multiple tasks.

Abstract: We introduce Perception Encoder (PE), a state-of-the-art vision encoder for
image and video understanding trained via simple vision-language learning.
Traditionally, vision encoders have relied on a variety of pretraining
objectives, each tailored to specific downstream tasks such as classification,
captioning, or localization. Surprisingly, after scaling our carefully tuned
image pretraining recipe and refining with our robust video data engine, we
find that contrastive vision-language training alone can produce strong,
general embeddings for all of these downstream tasks. There is only one caveat:
these embeddings are hidden within the intermediate layers of the network. To
draw them out, we introduce two alignment methods: language alignment for
multimodal language modeling, and spatial alignment for dense prediction.
Together, our PE family of models achieves best-in-class results on a wide
variety of tasks, including (1) zero-shot image and video classification and
retrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness
and 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and
video Q&A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest
with an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth
estimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster
further research, we release our models, code, and novel dataset of
synthetically and human-annotated videos:
https://github.com/facebookresearch/perception_models

</details>


### [176] [Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis](https://arxiv.org/abs/2504.13754)
*Zhu Zhu, Shuo Jiang, Jingyuan Zheng, Yawen Li, Yifei Chen, Manli Zhao, Weizhong Gu, Feiwei Qin, Jinhu Wang, Gang Yu*

Main category: cs.CV

TL;DR: CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model, improves pathological image classification for neuroblastoma by enhancing interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current diagnostic practices for neuroblastoma rely on subjective manual examination, leading to inconsistent accuracy. Automated methods face challenges like poor interpretability and high computational costs.

Method: CMSwinKAN integrates a Kernel Activation Network into the Swin Transformer architecture, fuses multi-scale features, and uses contrastive learning to mimic clinicians' comprehensive approach. A heuristic soft voting mechanism bridges patch-level to whole slide image-level classifications.

Result: CMSwinKAN outperforms state-of-the-art models on the PpNTs and BreakHis datasets, demonstrating superior accuracy and interpretability.

Conclusion: CMSwinKAN offers a promising solution for automated neuroblastoma diagnosis, combining interpretability, accuracy, and clinical relevance.

Abstract: Neuroblastoma, adrenal-derived, is among the most common pediatric solid
malignancies, characterized by significant clinical heterogeneity. Timely and
accurate pathological diagnosis from hematoxylin and eosin-stained whole slide
images is critical for patient prognosis. However, current diagnostic practices
primarily rely on subjective manual examination by pathologists, leading to
inconsistent accuracy. Existing automated whole slide image classification
methods encounter challenges such as poor interpretability, limited feature
extraction capabilities, and high computational costs, restricting their
practical clinical deployment. To overcome these limitations, we propose
CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model
tailored for pathological image classification, which enhances the Swin
Transformer architecture by integrating a Kernel Activation Network within its
multilayer perceptron and classification head modules, significantly improving
both interpretability and accuracy. By fusing multi-scale features and
leveraging contrastive learning strategies, CMSwinKAN mimics clinicians'
comprehensive approach, effectively capturing global and local tissue
characteristics. Additionally, we introduce a heuristic soft voting mechanism
guided by clinical insights to seamlessly bridge patch-level predictions to
whole slide image-level classifications. We validate CMSwinKAN on the PpNTs
dataset, which was collaboratively established with our partner hospital and
the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN
performs better than existing state-of-the-art pathology-specific models
pre-trained on large datasets. Our source code is available at
https://github.com/JSLiam94/CMSwinKAN.

</details>


### [177] [NTIRE 2025 Challenge on Image Super-Resolution ($\times$4): Methods and Results](https://arxiv.org/abs/2504.14582)
*Zheng Chen, Kai Liu, Jue Gong, Jingkai Wang, Lei Sun, Zongwei Wu, Radu Timofte, Yulun Zhang, Xiangyu Kong, Xiaoxuan Yu, Hyunhee Park, Suejin Han, Hakjae Jeon, Dafeng Zhang, Hyung-Ju Chun, Donghun Ryou, Inju Ha, Bohyung Han, Lu Zhao, Yuyi Zhang, Pengyu Yan, Jiawei Hu, Pengwei Liu, Fengjun Guo, Hongyuan Yu, Pufan Xu, Zhijuan Huang, Shuyuan Cui, Peng Guo, Jiahui Liu, Dongkai Zhang, Heng Zhang, Huiyuan Fu, Huadong Ma, Yanhui Guo, Sisi Tian, Xin Liu, Jinwen Liang, Jie Liu, Jie Tang, Gangshan Wu, Zeyu Xiao, Zhuoyuan Li, Yinxiang Zhang, Wenxuan Cai, Vijayalaxmi Ashok Aralikatti, Nikhil Akalwadi, G Gyaneshwar Rao, Chaitra Desai, Ramesh Ashok Tabib, Uma Mudenagudi, Marcos V. Conde, Alejandro Merino, Bruno Longarela, Javier Abad, Weijun Yuan, Zhan Li, Zhanglu Chen, Boyang Yao, Aagam Jain, Milan Kumar Singh, Ankit Kumar, Shubh Kawa, Divyavardhan Singh, Anjali Sarvaiya, Kishor Upla, Raghavendra Ramachandra, Chia-Ming Lee, Yu-Fan Lin, Chih-Chung Hsu, Risheek V Hiremath, Yashaswini Palani, Yuxuan Jiang, Qiang Zhu, Siyue Teng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull, Jingwei Liao, Yuqing Yang, Wenda Shao, Junyi Zhao, Qisheng Xu, Kele Xu, Sunder Ali Khowaja, Ik Hyun Lee, Snehal Singh Tomar, Rajarshi Ray, Klaus Mueller, Sachin Chaudhary, Surya Vashisth, Akshay Dudhane, Praful Hambarde, Satya Naryan Tazi, Prashant Patil, Santosh Kumar Vipparthi, Subrahmanyam Murala, Bilel Benjdira, Anas M. Ali, Wadii Boulila, Zahra Moammeri, Ahmad Mahmoudi-Aznaveh, Ali Karbasi, Hossein Motamednia, Liangyan Li, Guanhua Zhao, Kevin Le, Yimo Ning, Haoxuan Huang, Jun Chen*

Main category: cs.CV

TL;DR: The NTIRE 2025 challenge focuses on image super-resolution (×4) with two sub-tracks: restoration (PSNR-based) and perceptual (visual realism). It attracted 286 registrants and 25 valid submissions, aiming to advance SR research.


<details>
  <summary>Details</summary>
Motivation: To push the boundaries of image super-resolution by encouraging innovative network designs and solutions, addressing both pixel-wise accuracy and visual realism.

Method: Participants developed solutions to recover high-resolution images from bicubic downsampled low-resolution images, evaluated on two sub-tracks: restoration (PSNR) and perceptual (visual realism).

Result: 25 teams submitted valid entries, with results benchmarked to advance SR performance.

Conclusion: The challenge serves as a benchmark to foster progress in image super-resolution, highlighting current state-of-the-art methods.

Abstract: This paper presents the NTIRE 2025 image super-resolution ($\times$4)
challenge, one of the associated competitions of the 10th NTIRE Workshop at
CVPR 2025. The challenge aims to recover high-resolution (HR) images from
low-resolution (LR) counterparts generated through bicubic downsampling with a
$\times$4 scaling factor. The objective is to develop effective network designs
or solutions that achieve state-of-the-art SR performance. To reflect the dual
objectives of image SR research, the challenge includes two sub-tracks: (1) a
restoration track, emphasizes pixel-wise accuracy and ranks submissions based
on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by
a perceptual score. A total of 286 participants registered for the competition,
with 25 teams submitting valid entries. This report summarizes the challenge
design, datasets, evaluation protocol, the main results, and methods of each
team. The challenge serves as a benchmark to advance the state of the art and
foster progress in image SR.

</details>


### [178] [Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy](https://arxiv.org/abs/2504.18317)
*Zhengru Fang, Zhenghao Liu, Jingjing Wang, Senkang Hu, Yu Guo, Yiqin Deng, Yuguang Fang*

Main category: cs.CV

TL;DR: A task-oriented communication framework for UAVs uses multi-view features and edge servers for precise localization in GPS-denied urban areas, employing an O-VIB encoder to minimize redundancy and transmission costs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of UAV localization in GPS-denied urban areas while overcoming bandwidth and processing constraints on lightweight UAVs.

Method: Proposes the O-VIB encoder with automatic relevance determination (ARD) to extract compact multi-view features and offload tasks to edge servers, ensuring minimal redundancy and transmission costs.

Result: O-VIB achieves high-precision localization under strict bandwidth constraints, validated on a dedicated LAE UAV dataset.

Conclusion: The framework enables efficient and accurate UAV localization in urban environments, with code and dataset made publicly available.

Abstract: To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles
(UAVs) localization in urban areas where global positioning system (GPS)
signals are unavailable. Vision-based methods offer a viable alternative but
face severe bandwidth, memory and processing constraints on lightweight UAVs.
Inspired by mammalian spatial cognition, we propose a task-oriented
communication framework, where UAVs equipped with multi-camera systems extract
compact multi-view features and offload localization tasks to edge servers. We
introduce the Orthogonally-constrained Variational Information Bottleneck
encoder (O-VIB), which incorporates automatic relevance determination (ARD) to
prune non-informative features while enforcing orthogonality to minimize
redundancy. This enables efficient and accurate localization with minimal
transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows
that O-VIB achieves high-precision localization under stringent bandwidth
budgets. Code and dataset will be made publicly available:
github.com/fangzr/TOC-Edge-Aerial.

</details>


### [179] [Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency](https://arxiv.org/abs/2504.18589)
*Zhikai Wang, Jiashuo Sun, Wenqi Zhang, Zhiqiang Hu, Xin Li, Fan Wang, Deli Zhao*

Main category: cs.CV

TL;DR: VCBENCH is a new benchmark for evaluating LVLMs on multimodal mathematical reasoning with visual dependencies, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Current LVLM benchmarks overlook elementary math reasoning with visual dependencies, a key capability for AGI.

Method: VCBENCH includes 1,720 problems across six domains, using 6,697 images to test multi-image reasoning.

Result: Top LVLMs scored below 50% accuracy, showing major challenges in visual-mathematical integration.

Conclusion: VCBENCH highlights critical gaps in LVLMs and suggests future research directions.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have
significantly enhanced their ability to integrate visual and linguistic
information, achieving near-human proficiency in tasks like object recognition,
captioning, and visual question answering. However, current benchmarks
typically focus on knowledge-centric evaluations that assess domain-specific
expertise, often neglecting the core ability to reason about fundamental
mathematical elements and visual concepts. We identify a gap in evaluating
elementary-level math problems, which rely on explicit visual
dependencies-requiring models to discern, integrate, and reason across multiple
images while incorporating commonsense knowledge, all of which are crucial for
advancing toward broader AGI capabilities. To address this gap, we introduce
VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with
explicit visual dependencies. VCBENCH includes 1,720 problems across six
cognitive domains, featuring 6,697 images (averaging 3.9 per question) to
ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH,
revealing substantial performance disparities, with even the top models unable
to exceed 50% accuracy. Our findings highlight the ongoing challenges in
visual-mathematical integration and suggest avenues for future LVLM
advancements.

</details>


### [180] [IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos](https://arxiv.org/abs/2504.19165)
*Yuan Li, Ziqian Bai, Feitong Tan, Zhaopeng Cui, Sean Fanello, Yinda Zhang*

Main category: cs.CV

TL;DR: A 3D-aware diffusion method generates photorealistic talking head videos from a single image and control signals, using Multiplane Images for geometric consistency and immersive VR experiences.


<details>
  <summary>Details</summary>
Motivation: Existing methods often require separate stages or joint optimization for 3D representation, which complicates the process. This work aims to simplify and improve efficiency by directly generating the final output.

Method: The approach uses a diffusion-based method to generate Multiplane Images (MPIs) in a single denoising process, avoiding post-processing. A training mechanism reconstructs MPIs randomly in target or reference camera space to learn 3D details from monocular videos.

Result: The method achieves competitive avatar quality and novel-view rendering without explicit 3D reconstruction or multi-view training data.

Conclusion: The proposed approach simplifies the generation of photorealistic talking head videos while maintaining high quality and efficiency, suitable for immersive VR applications.

Abstract: We propose a novel 3D-aware diffusion-based method for generating
photorealistic talking head videos directly from a single identity image and
explicit control signals (e.g., expressions). Our method generates Multiplane
Images (MPIs) that ensure geometric consistency, making them ideal for
immersive viewing experiences like binocular videos for VR headsets. Unlike
existing methods that often require a separate stage or joint optimization to
reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach
directly generates the final output through a single denoising process,
eliminating the need for post-processing steps to render novel views
efficiently. To effectively learn from monocular videos, we introduce a
training mechanism that reconstructs the output MPI randomly in either the
target or the reference camera space. This approach enables the model to
simultaneously learn sharp image details and underlying 3D information.
Extensive experiments demonstrate the effectiveness of our method, which
achieves competitive avatar quality and novel-view rendering capabilities, even
without explicit 3D reconstruction or high-quality multi-view training data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [181] [Evolution of AI in Education: Agentic Workflows](https://arxiv.org/abs/2504.20082)
*Firuz Kamalov, David Santandreu Calonge, Linda Smail, Dilshod Azizov, Dimple R. Thadani, Theresa Kwong, Amara Atif*

Main category: cs.AI

TL;DR: The paper reviews AI agents in education, focusing on reflection, planning, tool use, and multi-agent collaboration, and presents a proof-of-concept for automated essay scoring.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional LLMs (static data, lack of adaptability, reasoning) and explore sustainable AI practices in education.

Method: Examines agentic workflows via four paradigms and presents a multi-agent framework for automated essay scoring.

Result: Preliminary results show improved consistency over stand-alone LLMs.

Conclusion: AI agents have transformative potential in education but require further research on interpretability, trustworthiness, and pedagogical impact.

Abstract: Artificial intelligence (AI) has transformed various aspects of education,
with large language models (LLMs) driving advancements in automated tutoring,
assessment, and content generation. However, conventional LLMs are constrained
by their reliance on static training data, limited adaptability, and lack of
reasoning. To address these limitations and foster more sustainable
technological practices, AI agents have emerged as a promising new avenue for
educational innovation. In this review, we examine agentic workflows in
education according to four major paradigms: reflection, planning, tool use,
and multi-agent collaboration. We critically analyze the role of AI agents in
education through these key design paradigms, exploring their advantages,
applications, and challenges. To illustrate the practical potential of agentic
systems, we present a proof-of-concept application: a multi-agent framework for
automated essay scoring. Preliminary results suggest this agentic approach may
offer improved consistency compared to stand-alone LLMs. Our findings highlight
the transformative potential of AI agents in educational settings while
underscoring the need for further research into their interpretability,
trustworthiness, and sustainable impact on pedagogical impact.

</details>


### [182] [AI Awareness](https://arxiv.org/abs/2504.20084)
*Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu*

Main category: cs.AI

TL;DR: The paper reviews AI awareness as a functional capacity, covering meta-cognition, self-awareness, social awareness, and situational awareness, linking it to AI capabilities and discussing risks and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Recent AI advancements necessitate a measurable understanding of AI awareness beyond philosophical consciousness, focusing on functional capacities.

Method: The review integrates insights from cognitive science, psychology, and computational theory, analyzes evaluation methods, and examines empirical findings.

Result: AI awareness enhances capabilities like reasoning and safety but introduces risks like misalignment, requiring oversight.

Conclusion: The interdisciplinary review clarifies AI awareness's role in AI development and provides a research roadmap.

Abstract: Recent breakthroughs in artificial intelligence (AI) have brought about
increasingly capable systems that demonstrate remarkable abilities in
reasoning, language understanding, and problem-solving. These advancements have
prompted a renewed examination of AI awareness, not as a philosophical question
of consciousness, but as a measurable, functional capacity. In this review, we
explore the emerging landscape of AI awareness, which includes meta-cognition
(the ability to represent and reason about its own state), self-awareness
(recognizing its own identity, knowledge, limitations, inter alia), social
awareness (modeling the knowledge, intentions, and behaviors of other agents),
and situational awareness (assessing and responding to the context in which it
operates).
  First, we draw on insights from cognitive science, psychology, and
computational theory to trace the theoretical foundations of awareness and
examine how the four distinct forms of AI awareness manifest in
state-of-the-art AI. Next, we systematically analyze current evaluation methods
and empirical findings to better understand these manifestations. Building on
this, we explore how AI awareness is closely linked to AI capabilities,
demonstrating that more aware AI agents tend to exhibit higher levels of
intelligent behaviors. Finally, we discuss the risks associated with AI
awareness, including key topics in AI safety, alignment, and broader ethical
concerns.
  AI awareness is a double-edged sword: it improves general capabilities, i.e.,
reasoning, safety, while also raises concerns around misalignment and societal
risks, demanding careful oversight as AI capabilities grow. On the whole, our
interdisciplinary review provides a roadmap for future research and aims to
clarify the role of AI awareness in the ongoing development of intelligent
machines.

</details>


### [183] [Spark: A System for Scientifically Creative Idea Generation](https://arxiv.org/abs/2504.20090)
*Aishik Sanyal, Samuel Schapiro, Sumuk Shashidhar, Royce Moon, Lav R. Varshney, Dilek Hakkani-Tur*

Main category: cs.AI

TL;DR: The paper introduces Spark, a system combining LLMs and a reviewer model (Judge) for generating and evaluating scientific ideas, inspired by computational creativity principles.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of LLMs in generating novel research ideas and grounding them in computational creativity principles.

Method: Developed Spark, a system using retrieval-augmented LLMs for idea generation and a reviewer model (Judge) trained on 600K scientific reviews.

Result: Demonstrated the system's capability and released the annotated dataset for further research.

Conclusion: Encourages CC researchers to leverage LLMs for idea generation and evaluation, fostering innovation in computational creativity.

Abstract: Recently, large language models (LLMs) have shown promising abilities to
generate novel research ideas in science, a direction which coincides with many
foundational principles in computational creativity (CC). In light of these
developments, we present an idea generation system named Spark that couples
retrieval-augmented idea generation using LLMs with a reviewer model named
Judge trained on 600K scientific reviews from OpenReview. Our work is both a
system demonstration and intended to inspire other CC researchers to explore
grounding the generation and evaluation of scientific ideas within foundational
CC principles. To this end, we release the annotated dataset used to train
Judge, inviting other researchers to explore the use of LLMs for idea
generation and creative evaluations.

</details>


### [184] [Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems](https://arxiv.org/abs/2504.20109)
*Rajeev Gupta, Suhani Gupta, Ronak Parikh, Divya Gupta, Amir Javaheri, Jairaj Singh Shaktawat*

Main category: cs.AI

TL;DR: The paper proposes a neuroscience-inspired architecture for Personalized AGI on edge devices, addressing challenges like catastrophic forgetting and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Current AI models lack continuous, adaptable learning. The goal is to achieve AGI with lifelong learning on resource-constrained edge devices.

Method: The paper reviews neuroscience principles (e.g., Synaptic Pruning, Hebbian plasticity) and proposes an architecture with fast-slow learning modules and memory-efficient updates.

Result: A theoretical architecture is outlined, addressing challenges like catastrophic forgetting and scalability, with potential applications in mobile AI and robotics.

Conclusion: The paper provides a roadmap for future research toward continual, personalized AGI on edge devices, synthesizing diverse insights.

Abstract: Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation.

</details>


### [185] [Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI](https://arxiv.org/abs/2504.20113)
*Lingbo Li, Anuradha Mathrani, Teo Susnjak*

Main category: cs.AI

TL;DR: The paper reviews Automated Meta-analysis (AMA), highlighting its focus on data processing but limited progress in full-process automation and advanced synthesis, despite AI advancements.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of scientific literature necessitates efficient evidence-based synthesis, driving the need for AMA.

Method: A PRISMA systematic review of 978 papers (2006-2024), analyzing 54 studies to assess AMA's current state.

Result: Most AMA efforts focus on data processing (57%), with minimal attention to advanced synthesis (17%) and full-process automation (2%). AI integration in higher-order synthesis remains underdeveloped.

Conclusion: Future AMA development must prioritize end-to-end automation, interpretability, and robustness to achieve scalable, domain-agnostic synthesis.

Abstract: Exponential growth in scientific literature has heightened the demand for
efficient evidence-based synthesis, driving the rise of the field of Automated
Meta-analysis (AMA) powered by natural language processing and machine
learning. This PRISMA systematic review introduces a structured framework for
assessing the current state of AMA, based on screening 978 papers from 2006 to
2024, and analyzing 54 studies across diverse domains. Findings reveal a
predominant focus on automating data processing (57%), such as extraction and
statistical modeling, while only 17% address advanced synthesis stages. Just
one study (2%) explored preliminary full-process automation, highlighting a
critical gap that limits AMA's capacity for comprehensive synthesis. Despite
recent breakthroughs in large language models (LLMs) and advanced AI, their
integration into statistical modeling and higher-order synthesis, such as
heterogeneity assessment and bias evaluation, remains underdeveloped. This has
constrained AMA's potential for fully autonomous meta-analysis. From our
dataset spanning medical (67%) and non-medical (33%) applications, we found
that AMA has exhibited distinct implementation patterns and varying degrees of
effectiveness in actually improving efficiency, scalability, and
reproducibility. While automation has enhanced specific meta-analytic tasks,
achieving seamless, end-to-end automation remains an open challenge. As AI
systems advance in reasoning and contextual understanding, addressing these
gaps is now imperative. Future efforts must focus on bridging automation across
all meta-analysis stages, refining interpretability, and ensuring
methodological robustness to fully realize AMA's potential for scalable,
domain-agnostic synthesis.

</details>


### [186] [Deep Physics Prior for First Order Inverse Optimization](https://arxiv.org/abs/2504.20278)
*Haoyu Yang, Kamyar Azizzadenesheli, Haoxing Ren*

Main category: cs.AI

TL;DR: Deep Physics Prior (DPP) enables gradient-based inverse optimization using pretrained Neural Operators, overcoming limitations of generative AI and Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Inverse design optimization is challenging due to lack of explicit representations, making first-order optimization impossible. Existing methods like generative AI and Bayesian optimization have computational and scalability issues.

Method: DPP uses pretrained auxiliary Neural Operators to enforce prior distribution constraints, enabling gradient-based optimization without known prior data or observation distributions.

Result: DPP provides robust and meaningful solutions for inverse design problems, outperforming traditional methods in scenarios with unknown distributions.

Conclusion: DPP is a scalable and efficient alternative for inverse design optimization, addressing key limitations of current approaches.

Abstract: Inverse design optimization aims to infer system parameters from observed
solutions, posing critical challenges across domains such as semiconductor
manufacturing, structural engineering, materials science, and fluid dynamics.
The lack of explicit mathematical representations in many systems complicates
this process and makes the first order optimization impossible. Mainstream
approaches, including generative AI and Bayesian optimization, address these
challenges but have limitations. Generative AI is computationally expensive,
while Bayesian optimization, relying on surrogate models, suffers from
scalability, sensitivity to priors, and noise issues, often leading to
suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel
method enabling first-order gradient-based inverse optimization with surrogate
machine learning models. By leveraging pretrained auxiliary Neural Operators,
DPP enforces prior distribution constraints to ensure robust and meaningful
solutions. This approach is particularly effective when prior data and
observation distributions are unknown.

</details>


### [187] [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
*William P. McCarthy, Saujas Vaduguru, Karl D. D. Willis, Justin Matejka, Judith E. Fan, Daniel Fried, Yewen Pu*

Main category: cs.AI

TL;DR: The paper introduces mrCAD, a dataset for studying multimodal refinement in human-AI collaboration, highlighting gaps in AI's ability to refine outputs compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the gap between human iterative refinement and AI's struggle with language-guided modifications of prior outputs.

Method: Created mrCAD, a dataset of 6,082 communication games where players refined CAD designs using text, drawing, or both. Analyzed 15,163 instruction-execution rounds.

Result: Found that refinement instructions differ from generation ones in modality use. State-of-the-art VLMs perform better on generation than refinement tasks.

Conclusion: mrCAD provides a foundation for studying multimodal refinement, a gap in existing datasets.

Abstract: A key feature of human collaboration is the ability to iteratively refine the
concepts we have communicated. In contrast, while generative AI excels at the
\textit{generation} of content, it often struggles to make specific
language-guided \textit{modifications} of its prior outputs. To bridge the gap
between how humans and machines perform edits, we present mrCAD, a dataset of
multimodal instructions in a communication game. In each game, players created
computer aided designs (CADs) and refined them over several rounds to match
specific target designs. Only one player, the Designer, could see the target,
and they must instruct the other player, the Maker, using text, drawing, or a
combination of modalities. mrCAD consists of 6,082 communication games, 15,163
instruction-execution rounds, played between 1,092 pairs of human players. We
analyze the dataset and find that generation and refinement instructions differ
in their composition of drawing and text. Using the mrCAD task as a benchmark,
we find that state-of-the-art VLMs are better at following generation
instructions than refinement instructions. These results lay a foundation for
analyzing and modeling a multimodal language of refinement that is not
represented in previous datasets.

</details>


### [188] [Leveraging Action Relational Structures for Integrated Learning and Planning](https://arxiv.org/abs/2504.20318)
*Ryan Xiao Wang, Felipe Trevizan*

Main category: cs.AI

TL;DR: The paper introduces partial-space search, a new approach leveraging PDDL action schemas for better planning with learning systems, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To adapt search algorithms for better integration with learning systems in planning, addressing the overlooked relational structure of actions.

Method: Proposes partial-space search and action set heuristics, converting existing heuristics and training new ones from large datasets.

Result: LazyLifted, the new planner, outperforms state-of-the-art ML-based heuristics and LAMA on IPC 2023 benchmarks.

Conclusion: Partial-space search and action set heuristics enhance planning efficiency, especially in high-branching tasks.

Abstract: Recent advances in planning have explored using learning methods to help
planning. However, little attention has been given to adapting search
algorithms to work better with learning systems. In this paper, we introduce
partial-space search, a new search space for classical planning that leverages
the relational structure of actions given by PDDL action schemas -- a structure
overlooked by traditional planning approaches. Partial-space search provides a
more granular view of the search space and allows earlier pruning of poor
actions compared to state-space search. To guide partial-space search, we
introduce action set heuristics that evaluate sets of actions in a state. We
describe how to automatically convert existing heuristics into action set
heuristics. We also train action set heuristics from scratch using large
training datasets from partial-space search. Our new planner, LazyLifted,
exploits our better integrated search and learning heuristics and outperforms
the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT)
benchmarks. We also show the efficiency of LazyLifted on high-branching factor
tasks and show that it surpasses LAMA in the combined IPC 2023 LT and
high-branching factor benchmarks.

</details>


### [189] [A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks](https://arxiv.org/abs/2504.20340)
*Khoi Trinh, Scott Seidenberger, Raveen Wijewickrama, Murtuza Jadliwala, Anindya Maiti*

Main category: cs.AI

TL;DR: The paper explores AI-generated image regeneration, focusing on iterative prompt refinement to recreate target images and validating image similarity metrics against human perception.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding AI-generated image regeneration and the reliability of image similarity metrics in iterative workflows.

Method: Conducted a structured user study evaluating iterative prompt refinement and alignment of image similarity metrics with human perception.

Result: Incremental prompt adjustments significantly improve image alignment, validated by both subjective and quantitative measures.

Conclusion: Iterative workflows enhance generative AI content creation, with broader applications across domains.

Abstract: With AI-generated content becoming ubiquitous across the web, social media,
and other digital platforms, it is vital to examine how such content are
inspired and generated. The creation of AI-generated images often involves
refining the input prompt iteratively to achieve desired visual outcomes. This
study focuses on the relatively underexplored concept of image regeneration
using AI, in which a human operator attempts to closely recreate a specific
target image by iteratively refining their prompt. Image regeneration is
distinct from normal image generation, which lacks any predefined visual
reference. A separate challenge lies in determining whether existing image
similarity metrics (ISMs) can provide reliable, objective feedback in iterative
workflows, given that we do not fully understand if subjective human judgments
of similarity align with these metrics. Consequently, we must first validate
their alignment with human perception before assessing their potential as a
feedback mechanism in the iterative prompt refinement process. To address these
research gaps, we present a structured user study evaluating how iterative
prompt refinement affects the similarity of regenerated images relative to
their targets, while also examining whether ISMs capture the same improvements
perceived by human observers. Our findings suggest that incremental prompt
adjustments substantially improve alignment, verified through both subjective
evaluations and quantitative measures, underscoring the broader potential of
iterative workflows to enhance generative AI content creation across various
application domains.

</details>


### [190] [Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs](https://arxiv.org/abs/2504.20406)
*Paiheng Xu, Gang Wu, Xiang Chen, Tong Yu, Chang Xiao, Franck Dernoncourt, Tianyi Zhou, Wei Ai, Viswanathan Swaminathan*

Main category: cs.AI

TL;DR: An offline simulation framework uses LLMs and scripting guides to create verified scripts, improving automation success and reducing costs compared to runtime code generation.


<details>
  <summary>Details</summary>
Motivation: Traditional scripting requires programming expertise, and runtime LLM-generated code poses security and efficiency issues. The goal is to bridge this gap with verified scripts.

Method: The framework includes task creation (top-down and bottom-up approaches) and skill generation with trials. A GNN-based model predicts API synergy for diverse script generation.

Result: Experiments in Adobe Illustrator show higher automation success, faster response times, and lower token costs than runtime generation.

Conclusion: This approach aligns AI capabilities with user needs in specialized software, leveraging execution feedback in a controlled environment.

Abstract: Scripting interfaces enable users to automate tasks and customize software
workflows, but creating scripts traditionally requires programming expertise
and familiarity with specific APIs, posing barriers for many users. While Large
Language Models (LLMs) can generate code from natural language queries, runtime
code generation is severely limited due to unverified code, security risks,
longer response times, and higher computational costs. To bridge the gap, we
propose an offline simulation framework to curate a software-specific skillset,
a collection of verified scripts, by exploiting LLMs and publicly available
scripting guides. Our framework comprises two components: (1) task creation,
using top-down functionality guidance and bottom-up API synergy exploration to
generate helpful tasks; and (2) skill generation with trials, refining and
validating scripts based on execution feedback. To efficiently navigate the
extensive API landscape, we introduce a Graph Neural Network (GNN)-based link
prediction model to capture API synergy, enabling the generation of skills
involving underutilized APIs and expanding the skillset's diversity.
Experiments with Adobe Illustrator demonstrate that our framework significantly
improves automation success rates, reduces response time, and saves runtime
token costs compared to traditional runtime code generation. This is the first
attempt to use software scripting interfaces as a testbed for LLM-based
systems, highlighting the advantages of leveraging execution feedback in a
controlled environment and offering valuable insights into aligning AI
capabilities with user needs in specialized software domains.

</details>


### [191] [RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library](https://arxiv.org/abs/2504.20426)
*Jiapeng Wang, Jinhao Jiang, Zhiqiang Zhang, Jun Zhou, Wayne Xin Zhao*

Main category: cs.AI

TL;DR: RV-Syn is a novel method for generating high-quality mathematical reasoning data by constructing structured computational graphs, ensuring verifiability and logic-aware problem generation.


<details>
  <summary>Details</summary>
Motivation: Existing data synthesis methods struggle with mastering problem logic and ensuring solution verifiability, limiting the quality of reasoning datasets for LLMs.

Method: RV-Syn builds a structured mathematical operation function library, generates computational graphs as solutions, and back-translates them into complex problems, ensuring verifiability and logic-awareness.

Result: RV-Syn outperforms existing synthesis methods, including human-generated problems, in efficient data scaling and quality.

Conclusion: RV-Syn offers a scalable framework for producing high-quality reasoning datasets, advancing LLM reasoning capabilities.

Abstract: The advancement of reasoning capabilities in Large Language Models (LLMs)
requires substantial amounts of high-quality reasoning data, particularly in
mathematics. Existing data synthesis methods, such as data augmentation from
annotated training sets or direct question generation based on relevant
knowledge points and documents, have expanded datasets but face challenges in
mastering the inner logic of the problem during generation and ensuring the
verifiability of the solutions. To address these issues, we propose RV-Syn, a
novel Rational and Verifiable mathematical Synthesis approach. RV-Syn
constructs a structured mathematical operation function library based on
initial seed problems and generates computational graphs as solutions by
combining Python-formatted functions from this library. These graphs are then
back-translated into complex problems. Based on the constructed computation
graph, we achieve solution-guided logic-aware problem generation. Furthermore,
the executability of the computational graph ensures the verifiability of the
solving process. Experimental results show that RV-Syn surpasses existing
synthesis methods, including those involving human-generated problems,
achieving greater efficient data scaling. This approach provides a scalable
framework for generating high-quality reasoning datasets.

</details>


### [192] [Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks](https://arxiv.org/abs/2504.20445)
*Tianqing Zhang, Zixin Zhu, Kairong Yu, Hongwei Wang*

Main category: cs.AI

TL;DR: Proposes HTA-KL divergence for better knowledge distillation from ANNs to SNNs, addressing performance gaps by balancing high- and low-probability predictions.


<details>
  <summary>Details</summary>
Motivation: SNNs lag behind ANNs in performance due to training limitations. Traditional KD methods like KL divergence inadequately address SNN characteristics, focusing too much on high-probability predictions.

Method: Introduces HTA-KL divergence, using a cumulative probability-based mask to dynamically weight high- and low-probability regions, combining FKL and RKL for balanced alignment.

Result: Outperforms existing methods on CIFAR-10, CIFAR-100, and Tiny ImageNet with fewer timesteps.

Conclusion: HTA-KL effectively bridges the performance gap between SNNs and ANNs by improving knowledge transfer.

Abstract: Spiking Neural Networks (SNNs) have emerged as a promising approach for
energy-efficient and biologically plausible computation. However, due to
limitations in existing training methods and inherent model constraints, SNNs
often exhibit a performance gap when compared to Artificial Neural Networks
(ANNs). Knowledge distillation (KD) has been explored as a technique to
transfer knowledge from ANN teacher models to SNN student models to mitigate
this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence
to align output distributions. However, conventional KL-based approaches fail
to fully exploit the unique characteristics of SNNs, as they tend to
overemphasize high-probability predictions while neglecting low-probability
ones, leading to suboptimal generalization. To address this, we propose
Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for
SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically
distinguish between high- and low-probability regions. It assigns adaptive
weights to ensure balanced knowledge transfer, enhancing the overall
performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence,
our method effectively align both head and tail regions of the distribution. We
evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our
method outperforms existing methods on most datasets with fewer timesteps.

</details>


### [193] [TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data](https://arxiv.org/abs/2504.20462)
*Qi Wang, Xiao Zhang, Mingyi Li, Yuan Yuan, Mengbai Xiao, Fuzhen Zhuang, Dongxiao Yu*

Main category: cs.AI

TL;DR: TAMO, a tool-assisted LLM agent, addresses challenges in automated root cause analysis (RCA) for microservices by unifying multi-modal data and leveraging specialized tools, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of microservices and cloud-native systems makes traditional RCA inefficient, requiring automated solutions. LLMs offer potential but face text constraints, dependency hallucinations, and context limits.

Method: TAMO integrates multi-modal observational data into time-aligned representations, uses specialized tools for localization and classification, and structures prompts for LLM-guided repair strategies.

Result: TAMO effectively handles heterogeneous public datasets and common fault types, demonstrating superior RCA performance.

Conclusion: TAMO overcomes LLM limitations for RCA in dynamic systems, proving its viability for AIOps in modern enterprise environments.

Abstract: With the development of distributed systems, microservices and cloud native
technologies have become central to modern enterprise software development.
Despite bringing significant advantages, these technologies also increase
system complexity and operational challenges. Traditional root cause analysis
(RCA) struggles to achieve automated fault response, heavily relying on manual
intervention. In recent years, large language models (LLMs) have made
breakthroughs in contextual inference and domain knowledge integration,
providing new solutions for Artificial Intelligence for Operations (AIOps).
However, Existing LLM-based approaches face three key challenges: text input
constraints, dynamic service dependency hallucinations, and context window
limitations. To address these issues, we propose a tool-assisted LLM agent with
multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies
multi-modal observational data into time-aligned representations to extract
consistent features and employs specialized root cause localization and fault
classification tools for perceiving the contextual environment. This approach
overcomes the limitations of LLM in handling real-time changing service
dependencies and raw observational data and guides LLM to generate repair
strategies aligned with system contexts by structuring key information into a
prompt. Experimental results show that TAMO performs well in root cause
analysis when dealing with public datasets characterized by heterogeneity and
common fault types, demonstrating its effectiveness.

</details>


### [194] [A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning](https://arxiv.org/abs/2504.20464)
*Jiahao Li, Kaer Huang*

Main category: cs.AI

TL;DR: A summary of recent advances in GUI agents powered by MLLMs, focusing on RL-enhanced architectures, task formalization, modular designs, and training methodologies, with insights into challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To enable intelligent interaction with digital systems by advancing GUI agents through MLLMs and RL.

Method: Formalizes GUI tasks as Markov Decision Processes, reviews modular architectures (Perception, Planning, Acting), and categorizes training methods (Prompt-based, SFT-based, RL-based).

Result: Innovations in multimodal perception, decision reasoning, and adaptive action generation have improved GUI agent generalization and robustness.

Conclusion: Identifies key challenges and future directions for developing more capable and reliable GUI agents.

Abstract: Graphical User Interface (GUI) agents, driven by Multi-modal Large Language
Models (MLLMs), have emerged as a promising paradigm for enabling intelligent
interaction with digital systems. This paper provides a structured summary of
recent advances in GUI agents, focusing on architectures enhanced by
Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov
Decision Processes and discuss typical execution environments and evaluation
metrics. We then review the modular architecture of (M)LLM-based GUI agents,
covering Perception, Planning, and Acting modules, and trace their evolution
through representative works. Furthermore, we categorize GUI agent training
methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and
RL-based approaches, highlighting the progression from simple prompt
engineering to dynamic policy learning via RL. Our summary illustrates how
recent innovations in multimodal perception, decision reasoning, and adaptive
action generation have significantly improved the generalization and robustness
of GUI agents in complex real-world environments. We conclude by identifying
key challenges and future directions for building more capable and reliable GUI
agents.

</details>


### [195] [MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living](https://arxiv.org/abs/2504.20505)
*Xi Chen, Julien Cumin, Fano Ramparany, Dominique Vaufreydaz*

Main category: cs.AI

TL;DR: MuRAL is a new dataset for HAR using LLMs, addressing gaps in existing datasets by providing rich, multi-resident, natural language-annotated sensor data.


<details>
  <summary>Details</summary>
Motivation: Existing HAR datasets lack the contextual richness and granularity needed for LLMs, limiting their potential in activity recognition.

Method: Introduces MuRAL, a dataset with 21 hours of multi-user sensor data, annotated with natural language descriptions and activity labels, benchmarked using LLMs.

Result: LLMs show promise but struggle with multi-user ambiguity and sensor context under-specification.

Conclusion: MuRAL supports future research on LLM-powered, explainable HAR in smart environments.

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
potential for human activity recognition (HAR) using ambient sensors,
especially through natural language reasoning and zero-shot learning. However,
existing datasets such as CASAS, ARAS, and MARBLE were not originally designed
with LLMs in mind and therefore lack the contextual richness, complexity, and
annotation granularity required to fully exploit LLM capabilities. In this
paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with
natural Language, comprising over 21 hours of multi-user sensor data collected
from 21 sessions in a smart-home environment. MuRAL is annotated with
fine-grained natural language descriptions, resident identities, and high-level
activity labels, all situated in dynamic, realistic multi-resident settings. We
benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject
assignment, action description, and activity classification. Our results
demonstrate that while LLMs can provide rich semantic interpretations of
ambient data, current models still face challenges in handling multi-user
ambiguity and under-specified sensor contexts. We release MuRAL to support
future research on LLM-powered, explainable, and socially aware activity
understanding in smart environments. For access to the dataset, please reach
out to us via the provided contact information. A direct link for dataset
retrieval will be made available at this location in due course.

</details>


### [196] [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
*Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, Luke Zettlemoyer*

Main category: cs.AI

TL;DR: ReasonIR-8B is a retriever trained for general reasoning tasks, outperforming existing models on benchmarks like BRIGHT and improving RAG tasks.


<details>
  <summary>Details</summary>
Motivation: Existing retrievers underperform on reasoning tasks due to training on short factual queries.

Method: Developed a synthetic data generation pipeline for challenging queries and hard negatives, combined with public data for training.

Result: Achieves 29.9 nDCG@10 (no reranker) and 36.9 nDCG@10 (with reranker) on BRIGHT, and improves MMLU/GPQA performance.

Conclusion: ReasonIR-8B is effective, scalable, and open-sourced for future LLM integration.

Abstract: We present ReasonIR-8B, the first retriever specifically trained for general
reasoning tasks. Existing retrievers have shown limited gains on reasoning
tasks, in part because existing training datasets focus on short factual
queries tied to documents that straightforwardly answer them. We develop a
synthetic data generation pipeline that, for each document, our pipeline
creates a challenging and relevant query, along with a plausibly related but
ultimately unhelpful hard negative. By training on a mixture of our synthetic
data and existing public data, ReasonIR-8B achieves a new state-of-the-art of
29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a
widely-used reasoning-intensive information retrieval (IR) benchmark. When
applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4%
and 22.6% respectively, relative to the closed-book baseline, outperforming
other retrievers and search engines. In addition, ReasonIR-8B uses test-time
compute more effectively: on BRIGHT, its performance consistently increases
with longer and more information-rich rewritten queries; it continues to
outperform other retrievers when combined with an LLM reranker. Our training
recipe is general and can be easily extended to future LLMs; to this end, we
open-source our code, data, and model.

</details>


### [197] [PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval](https://arxiv.org/abs/2504.20624)
*Zihan Niu, Zheyong Xie, Shaosheng Cao, Chonggang Lu, Zheyu Ye, Tong Xu, Zuozhu Liu, Yan Gao, Jia Chen, Zhe Xu, Yi Wu, Yao Hu*

Main category: cs.AI

TL;DR: PaRT is a framework for social chatbots that enables proactive dialogues by integrating user profiles and context into an LLM, improving engagement and dialogue duration.


<details>
  <summary>Details</summary>
Motivation: Conventional chatbots rely on users to sustain dialogues, leading to low engagement. PaRT aims to make chatbots more proactive and context-aware.

Method: PaRT uses an LLM to refine user queries, recognize intents, generate personalized topics, retrieve relevant passages, and produce knowledge-grounded responses.

Result: The framework improved dialogue duration by 21.77% in a real-world deployment.

Conclusion: PaRT successfully enhances chatbot engagement by enabling proactive, context-aware dialogues.

Abstract: Social chatbots have become essential intelligent companions in daily
scenarios ranging from emotional support to personal interaction. However,
conventional chatbots with passive response mechanisms usually rely on users to
initiate or sustain dialogues by bringing up new topics, resulting in
diminished engagement and shortened dialogue duration. In this paper, we
present PaRT, a novel framework enabling context-aware proactive dialogues for
social chatbots through personalized real-time retrieval and generation.
Specifically, PaRT first integrates user profiles and dialogue context into a
large language model (LLM), which is initially prompted to refine user queries
and recognize their underlying intents for the upcoming conversation. Guided by
refined intents, the LLM generates personalized dialogue topics, which then
serve as targeted queries to retrieve relevant passages from RedNote. Finally,
we prompt LLMs with summarized passages to generate knowledge-grounded and
engagement-optimized responses. Our approach has been running stably in a
real-world production environment for more than 30 days, achieving a 21.77\%
improvement in the average duration of dialogues.

</details>


### [198] [Cognitive maps are generative programs](https://arxiv.org/abs/2504.20628)
*Marta Kryven, Cole Wyeth, Aidan Curtis, Kevin Ellis*

Main category: cs.AI

TL;DR: The paper explores how humans use program-like cognitive maps for efficient planning in structured environments, supported by behavioral experiments and a computational model leveraging LLMs.


<details>
  <summary>Details</summary>
Motivation: To understand how humans form resource-efficient mental representations of the world under cognitive constraints, hypothesizing that predictability and redundancy are key.

Method: Combines behavioral experiments with a computational model that infers programmatic cognitive maps, using LLMs to embed human priors.

Result: The model shows improved efficiency, lower memory usage, and better prediction of human behavior compared to unstructured planning algorithms.

Conclusion: Human planning relies on programmatic cognitive maps, exploiting predictability and redundancy for efficiency.

Abstract: Making sense of the world and acting in it relies on building simplified
mental representations that abstract away aspects of reality. This principle of
cognitive mapping is universal to agents with limited resources. Living
organisms, people, and algorithms all face the problem of forming functional
representations of their world under various computing constraints. In this
work, we explore the hypothesis that human resource-efficient planning may
arise from representing the world as predictably structured. Building on the
metaphor of concepts as programs, we propose that cognitive maps can take the
form of generative programs that exploit predictability and redundancy, in
contrast to directly encoding spatial layouts. We use a behavioral experiment
to show that people who navigate in structured spaces rely on modular planning
strategies that align with programmatic map representations. We describe a
computational model that predicts human behavior in a variety of structured
scenarios. This model infers a small distribution over possible programmatic
cognitive maps conditioned on human prior knowledge of the world, and uses this
distribution to generate resource-efficient plans. Our models leverages a Large
Language Model as an embedding of human priors, implicitly learned through
training on a vast corpus of human data. Our model demonstrates improved
computational efficiency, requires drastically less memory, and outperforms
unstructured planning algorithms with cognitive constraints at predicting human
behavior, suggesting that human planning strategies rely on programmatic
cognitive maps.

</details>


### [199] [The Limits of AI Explainability: An Algorithmic Information Theory Approach](https://arxiv.org/abs/2504.20676)
*Shrisha Rao*

Main category: cs.AI

TL;DR: The paper uses algorithmic information theory to define AI explainability, showing trade-offs between simplicity, accuracy, and governance in explainable AI systems.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of AI explainability by formalizing it through algorithmic information theory.

Method: Quantifies explainability using Kolmogorov complexity, analyzing approximation error and explanation complexity.

Result: Key findings include a complexity gap theorem, bounds on explanation complexity, and a regulatory impossibility theorem.

Conclusion: The results inform the design, evaluation, and oversight of explainable AI systems, highlighting inherent trade-offs.

Abstract: This paper establishes a theoretical foundation for understanding the
fundamental limits of AI explainability through algorithmic information theory.
We formalize explainability as the approximation of complex models by simpler
ones, quantifying both approximation error and explanation complexity using
Kolmogorov complexity. Our key theoretical contributions include: (1) a
complexity gap theorem proving that any explanation significantly simpler than
the original model must differ from it on some inputs; (2) precise bounds
showing that explanation complexity grows exponentially with input dimension
but polynomially with error tolerance for Lipschitz functions; and (3) a
characterization of the gap between local and global explainability,
demonstrating that local explanations can be significantly simpler while
maintaining accuracy in relevant regions. We further establish a regulatory
impossibility theorem proving that no governance framework can simultaneously
pursue unrestricted AI capabilities, human-interpretable explanations, and
negligible error. These results highlight considerations likely to be relevant
to the design, evaluation, and oversight of explainable AI systems.

</details>


### [200] [Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration](https://arxiv.org/abs/2504.20756)
*Moirangthem Tiken Singh*

Main category: cs.AI

TL;DR: A graph-based framework for multiclass fault diagnosis in rotating machinery, achieving high accuracy (up to 100%) and noise resilience, with interpretability and scalability.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and interpretable method for fault diagnosis in rotating machinery without relying on deep learning.

Method: Integrates entropy-optimized signal segmentation, time-frequency feature extraction, and graph-theoretic modeling to classify faults using graph metrics and local features.

Result: Achieves up to 99.8% accuracy on CWRU and 100% on SU datasets, with strong noise resilience and cross-domain transferability.

Conclusion: The method is scalable, reliable, and suitable for real-time industrial diagnostics, offering interpretability without deep learning.

Abstract: This paper proposes a novel graph-based framework for robust and
interpretable multiclass fault diagnosis in rotating machinery. The method
integrates entropy-optimized signal segmentation, time-frequency feature
extraction, and graph-theoretic modeling to transform vibration signals into
structured representations suitable for classification. Graph metrics, such as
average shortest path length, modularity, and spectral gap, are computed and
combined with local features to capture global and segment-level fault
characteristics. The proposed method achieves high diagnostic accuracy when
evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP
loads) and the SU gearbox and bearing datasets (under different speed-load
configurations). Classification scores reach up to 99.8% accuracy on Case
Western Reserve University (CWRU) and 100% accuracy on the Southeast University
datasets using a logistic regression classifier. Furthermore, the model
exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise
levels (standard deviation = 0.5), and demonstrates excellent cross-domain
transferability with up to 99.7% F1-score in load-transfer scenarios. Compared
to traditional techniques, this approach requires no deep learning
architecture, enabling lower complexity while ensuring interpretability. The
results confirm the method's scalability, reliability, and potential for
real-time deployment in industrial diagnostics.

</details>


### [201] [Approximate Lifted Model Construction](https://arxiv.org/abs/2504.20784)
*Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf Möller, Mattis Hartwig*

Main category: cs.AI

TL;DR: The paper introduces ε-ACP, an improved version of ACP for lifted inference, allowing slight deviations in potentials to handle real-world data.


<details>
  <summary>Details</summary>
Motivation: ACP's requirement for exact potential matching limits its practicality, as real-world data often has deviations.

Method: The ε-ACP algorithm introduces a hyperparameter ε to tolerate potential deviations while maintaining efficiency.

Result: Theoretical bounds guarantee controlled approximation error, and experiments show minimal practical error.

Conclusion: ε-ACP is a practical and robust alternative to ACP for probabilistic relational models.

Abstract: Probabilistic relational models such as parametric factor graphs enable
efficient (lifted) inference by exploiting the indistinguishability of objects.
In lifted inference, a representative of indistinguishable objects is used for
computations. To obtain a relational (i.e., lifted) representation, the
Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP
algorithm, however, requires underlying distributions, encoded as
potential-based factorisations, to exactly match to identify and exploit
indistinguishabilities. Hence, ACP is unsuitable for practical applications
where potentials learned from data inevitably deviate even if associated
objects are indistinguishable. To mitigate this problem, we introduce the
$\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which
allows for a deviation of potentials depending on a hyperparameter
$\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits
indistinguishabilities that are not exact. We prove that the approximation
error induced by $\varepsilon$-ACP is strictly bounded and our experiments show
that the approximation error is close to zero in practice.

</details>


### [202] [Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning](https://arxiv.org/abs/2504.20797)
*Renye Zhang, Yimin Yin, Jinghua Zhang*

Main category: cs.AI

TL;DR: A novel FSCIL method uses independent models per session with UQ for deployment, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Address the over-reliance on data and lack of adaptability in deep learning by mimicking human knowledge storage.

Method: Learn independent models for each session and integrate Uncertainty Quantification for testing.

Result: Achieves state-of-the-art performance on CIFAR-100 and mini-ImageNet datasets.

Conclusion: The method offers a new perspective for FSCIL, effectively preventing catastrophic forgetting.

Abstract: Current mainstream deep learning techniques exhibit an over-reliance on
extensive training data and a lack of adaptability to the dynamic world,
marking a considerable disparity from human intelligence. To bridge this gap,
Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous
learning of new categories with limited samples without forgetting old
knowledge. Existing FSCIL studies typically use a single model to learn
knowledge across all sessions, inevitably leading to the stability-plasticity
dilemma. Unlike machines, humans store varied knowledge in different cerebral
cortices. Inspired by this characteristic, our paper aims to develop a method
that learns independent models for each session. It can inherently prevent
catastrophic forgetting. During the testing stage, our method integrates
Uncertainty Quantification (UQ) for model deployment. Our method provides a
fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on
CIFAR-100 and mini-ImageNet datasets.

</details>


### [203] [Ascendra: Dynamic Request Prioritization for Efficient LLM Serving](https://arxiv.org/abs/2504.20828)
*Azam Ikram, Xiang Li, Sameh Elnikety, Saurabh Bagchi*

Main category: cs.AI

TL;DR: Ascendra is an LLM serving system that balances throughput and latency by dynamically prioritizing requests based on urgency, improving efficiency and meeting SLOs for TTFT and TBT.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems often sacrifice one performance metric (TTFT or TBT) for the other, highlighting the need for a solution that meets both SLOs simultaneously.

Method: Ascendra partitions GPU resources into low-priority (throughput-focused) and high-priority (latency-focused) instances, using a performance model to predict and offload urgent requests to high-priority instances.

Result: Ascendra achieves up to 1.7x higher throughput than vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

Conclusion: Ascendra effectively balances throughput and latency, demonstrating its superiority over existing systems in meeting dual SLOs for LLM serving.

Abstract: The rapid advancement of Large Language Models (LLMs) has driven the need for
more efficient serving strategies. In this context, efficiency refers to the
proportion of requests that meet their Service Level Objectives (SLOs),
particularly for Time To First Token (TTFT) and Time Between Tokens (TBT).
However, existing systems often prioritize one metric at the cost of the other.
We present Ascendra, an LLM serving system designed to meet both TTFT and TBT
SLOs simultaneously. The core insight behind Ascendra is that a request's
urgency evolves as it approaches its deadline. To leverage this, Ascendra
partitions GPU resources into two types of instances: low-priority and
high-priority. Low-priority instances maximize throughput by processing
requests out of arrival order, but at the risk of request starvation. To
address this, Ascendra employs a performance model to predict requests at risk
of missing their SLOs and proactively offloads them to high-priority instances.
High-priority instances are optimized for low-latency execution and handle
urgent requests nearing their deadlines. This partitioned architecture enables
Ascendra to effectively balance high throughput and low latency. Extensive
evaluation shows that Ascendra improves system throughput by up to 1.7x
compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.

</details>


### [204] [Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information](https://arxiv.org/abs/2504.20846)
*Robert F. Downey, S. S. Ravi*

Main category: cs.AI

TL;DR: The paper proposes methods to generate post-hoc explanations for clusters using auxiliary tags, focusing on disjunctive and CNF forms, employing ILP and heuristics.


<details>
  <summary>Details</summary>
Motivation: To provide interpretable explanations for clusters by leveraging unused auxiliary information (tags).

Method: Uses integer linear programming (ILP) and heuristic methods to generate disjunctive and two-clause CNF explanations.

Result: Experiments on various datasets yield insights, and scalability of the methods is evaluated.

Conclusion: The methods effectively generate cluster explanations and demonstrate scalability.

Abstract: We consider generating post-hoc explanations of clusters generated from
various datasets using auxiliary information which was not used by clustering
algorithms. Following terminology used in previous work, we refer to the
auxiliary information as tags. Our focus is on two forms of explanations,
namely disjunctive form (where the explanation for a cluster consists of a set
of tags) and a two-clause conjunctive normal form (CNF) explanation (where the
explanation consists of two sets of tags, combined through the AND operator).
We use integer linear programming (ILP) as well as heuristic methods to
generate these explanations. We experiment with a variety of datasets and
discuss the insights obtained from our explanations. We also present
experimental results regarding the scalability of our explanation methods.

</details>


### [205] [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
*Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker*

Main category: cs.AI

TL;DR: The paper identifies systematic biases in Chatbot Arena's leaderboard due to undisclosed private testing, selective disclosure, and data access asymmetries, favoring proprietary models over open-source ones. It offers recommendations for fairer benchmarking.


<details>
  <summary>Details</summary>
Motivation: To highlight distortions in AI benchmarking, particularly in Chatbot Arena, where undisclosed practices and data access disparities skew results, undermining fair evaluation of model quality.

Method: Analyzes private testing practices, selective disclosure, and data distribution in Chatbot Arena, quantifying biases and their impact on model rankings.

Result: Reveals significant advantages for proprietary models (e.g., Meta, Google, OpenAI) due to selective testing and higher data access, while open-source models are disadvantaged. Limited additional data can boost performance by up to 112%.

Conclusion: Calls for reforms in Chatbot Arena's evaluation framework to ensure transparency, fairness, and alignment with general model quality rather than Arena-specific dynamics.

Abstract: Measuring progress is fundamental to the advancement of any scientific field.
As benchmarks play an increasingly central role, they also grow more
susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard
for ranking the most capable AI systems. Yet, in this work we identify
systematic issues that have resulted in a distorted playing field. We find that
undisclosed private testing practices benefit a handful of providers who are
able to test multiple variants before public release and retract scores if
desired. We establish that the ability of these providers to choose the best
score leads to biased Arena scores due to selective disclosure of performance
results. At an extreme, we identify 27 private LLM variants tested by Meta in
the lead-up to the Llama-4 release. We also establish that proprietary closed
models are sampled at higher rates (number of battles) and have fewer models
removed from the arena than open-weight and open-source alternatives. Both
these policies lead to large data access asymmetries over time. Providers like
Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the
arena, respectively. In contrast, a combined 83 open-weight models have only
received an estimated 29.7% of the total data. We show that access to Chatbot
Arena data yields substantial benefits; even limited additional data can result
in relative performance gains of up to 112% on the arena distribution, based on
our conservative estimates. Together, these dynamics result in overfitting to
Arena-specific dynamics rather than general model quality. The Arena builds on
the substantial efforts of both the organizers and an open community that
maintains this valuable evaluation platform. We offer actionable
recommendations to reform the Chatbot Arena's evaluation framework and promote
fairer, more transparent benchmarking for the field

</details>


### [206] [CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models](https://arxiv.org/abs/2504.20898)
*Hasan Md Tusfiqur Alam, Devansh Srivastav, Abdulrahman Mohamed Selim, Md Abdul Kadir, Md Moktadiurl Hoque Shuvo, Daniel Sonntag*

Main category: cs.AI

TL;DR: The paper introduces a framework combining Concept Bottleneck Models (CBMs) and a Multi-Agent RAG system to generate interpretable and reliable radiology reports, addressing AI's clinical adoption challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI performance and clinical explainability in radiology report generation, addressing interpretability and reliability issues.

Method: Uses CBMs for transparent disease classification and a Multi-Agent RAG system for evidence-based, context-rich report generation.

Result: The system delivers interpretable predictions, reduces hallucinations, and produces high-quality, tailored reports with an interactive interface.

Conclusion: The framework enhances diagnostic consistency and provides actionable insights, improving clinical adoption of AI in radiology.

Abstract: Advancements in generative Artificial Intelligence (AI) hold great promise
for automating radiology workflows, yet challenges in interpretability and
reliability hinder clinical adoption. This paper presents an automated
radiology report generation framework that combines Concept Bottleneck Models
(CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge
AI performance with clinical explainability. CBMs map chest X-ray features to
human-understandable clinical concepts, enabling transparent disease
classification. Meanwhile, the RAG system integrates multi-agent collaboration
and external knowledge to produce contextually rich, evidence-based reports.
Our demonstration showcases the system's ability to deliver interpretable
predictions, mitigate hallucinations, and generate high-quality, tailored
reports with an interactive interface addressing accuracy, trust, and usability
challenges. This framework provides a pathway to improving diagnostic
consistency and empowering radiologists with actionable insights.

</details>


### [207] [Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare](https://arxiv.org/abs/2504.20921)
*Polycarp Nalela*

Main category: cs.AI

TL;DR: Using GPT-4 for synthetic medical data generation with rigorous validation to address privacy concerns in AI training.


<details>
  <summary>Details</summary>
Motivation: Privacy restrictions limit access to real medical data, hindering AI algorithm training in EHR applications.

Method: Employed GPT-4 API for synthetic data generation, validated using BERT, GPT-2, RoBERTa, autoencoders, and diversity analysis.

Result: High-quality synthetic data was integrated into a PostgreSQL database for EHR use.

Conclusion: Generative AI with strict validation can produce reliable synthetic medical data, balancing privacy and AI training needs.

Abstract: Access to high-quality medical data is often restricted due to privacy
concerns, posing significant challenges for training artificial intelligence
(AI) algorithms within Electronic Health Record (EHR) applications. In this
study, prompt engineering with the GPT-4 API was employed to generate
high-quality synthetic datasets aimed at overcoming this limitation. The
generated data encompassed a comprehensive array of patient admission
information, including healthcare provider details, hospital departments,
wards, bed assignments, patient demographics, emergency contacts, vital signs,
immunizations, allergies, medical histories, appointments, hospital visits,
laboratory tests, diagnoses, treatment plans, medications, clinical notes,
visit logs, discharge summaries, and referrals. To ensure data quality and
integrity, advanced validation techniques were implemented utilizing models
such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for
overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly
detection, and conducted diversity analysis. Synthetic data that met all
validation criteria were integrated into a comprehensive PostgreSQL database,
serving as the data management system for the EHR application. This approach
demonstrates that leveraging generative AI models with rigorous validation can
effectively produce high-quality synthetic medical data, facilitating the
training of AI algorithms while addressing privacy concerns associated with
real patient data.

</details>


### [208] [A Domain-Agnostic Scalable AI Safety Ensuring Framework](https://arxiv.org/abs/2504.20924)
*Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Heejin Ahn*

Main category: cs.AI

TL;DR: A novel AI safety framework ensures compliance with user-defined constraints across domains, combining AI components with optimization and conservative testing for credibility.


<details>
  <summary>Details</summary>
Motivation: Current AI safety methods lack generalization across contexts, necessitating a flexible, domain-agnostic approach.

Method: Combines AI components with optimization to satisfy constraints probabilistically, using internal test data and conservative testing for validation.

Result: Mathematically guarantees constraint satisfaction, outperforms existing methods in low-threshold regions, and scales with test data size.

Conclusion: The framework effectively generalizes safety guarantees across diverse applications, offering scalable and statistically valid solutions.

Abstract: Ensuring the safety of AI systems has recently emerged as a critical priority
for real-world deployment, particularly in physical AI applications. Current
approaches to AI safety typically address predefined domain-specific safety
conditions, limiting their ability to generalize across contexts.
  We propose a novel AI safety framework that ensures AI systems comply with
\textbf{any user-defined constraint}, with \textbf{any desired probability},
and across \textbf{various domains}.
  In this framework, we combine an AI component (e.g., neural network) with an
optimization problem to produce responses that minimize objectives while
satisfying user-defined constraints with probabilities exceeding user-defined
thresholds. For credibility assessment of the AI component, we propose
\textit{internal test data}, a supplementary set of safety-labeled data, and a
\textit{conservative testing} methodology that provides statistical validity of
using internal test data. We also present an approximation method of a loss
function and how to compute its gradient for training.
  We mathematically prove that probabilistic constraint satisfaction is
guaranteed under specific, mild conditions and prove a scaling law between
safety and the number of internal test data. We demonstrate our framework's
effectiveness through experiments in diverse domains: demand prediction for
production decision, safe reinforcement learning within the SafetyGym
simulator, and guarding AI chatbot outputs. Through these experiments, we
demonstrate that our method guarantees safety for user-specified constraints,
outperforms {for \textbf{up to several order of magnitudes}} existing methods
in low safety threshold regions, and scales effectively with respect to the
size of internal test data.

</details>


### [209] [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
*Ziqing Fan, Cheng Liang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie*

Main category: cs.AI

TL;DR: ChestX-Reasoner, a radiology diagnosis MLLM, improves diagnostic accuracy and reasoning by mimicking clinical reasoning processes, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Medical AI models often lack structured reasoning like clinical practice. ChestX-Reasoner addresses this gap by leveraging process supervision from radiology reports.

Method: A two-stage training framework (supervised fine-tuning + reinforcement learning) aligns model reasoning with clinical standards, using a large dataset of refined reasoning chains.

Result: ChestX-Reasoner outperforms existing MLLMs, achieving 16%, 5.9%, and 18% improvements in reasoning ability and 3.3%, 24%, and 27% in outcome accuracy.

Conclusion: The model and resources are open-sourced to advance medical reasoning MLLM research.

Abstract: Recent advances in reasoning-enhanced large language models (LLMs) and
multimodal LLMs (MLLMs) have significantly improved performance in complex
tasks, yet medical AI models often overlook the structured reasoning processes
inherent in clinical practice. In this work, we present ChestX-Reasoner, a
radiology diagnosis MLLM designed to leverage process supervision mined
directly from clinical reports, reflecting the step-by-step reasoning followed
by radiologists. We construct a large dataset by extracting and refining
reasoning chains from routine radiology reports. Our two-stage training
framework combines supervised fine-tuning and reinforcement learning guided by
process rewards to better align model reasoning with clinical standards. We
introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual
question answering samples with 301K clinically validated reasoning steps, and
propose RadRScore, a metric evaluating reasoning factuality, completeness, and
effectiveness. ChestX-Reasoner outperforms existing medical and general-domain
MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%,
and 18% improvements in reasoning ability compared to the best medical MLLM,
the best general MLLM, and its base model, respectively, as well as 3.3%, 24%,
and 27% improvements in outcome accuracy. All resources are open-sourced to
facilitate further research in medical reasoning MLLMs.

</details>


### [210] [Jekyll-and-Hyde Tipping Point in an AI's Behavior](https://arxiv.org/abs/2504.20980)
*Neil F. Johnson, Frank Yingjie Huo*

Main category: cs.AI

TL;DR: The paper derives a formula to predict when an LLM's output may become unreliable or harmful, addressing public trust issues and offering solutions to mitigate risks.


<details>
  <summary>Details</summary>
Motivation: Public trust in AI is eroding due to unpredictable LLM behavior, with real-world consequences like trauma and deaths. The paper aims to provide a scientific basis for understanding and controlling these risks.

Method: The authors derive an exact formula from first principles, using secondary school mathematics, to predict when an LLM's attention 'snaps,' causing unreliable outputs.

Result: The formula quantitatively predicts tipping points and suggests ways to delay or prevent them by adjusting prompts and training.

Conclusion: The work provides a transparent foundation for discussing AI risks and practical guidance, like whether politeness affects LLM behavior.

Abstract: Trust in AI is undermined by the fact that there is no science that predicts
-- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is
likely to tip mid-response to become wrong, misleading, irrelevant or
dangerous. With deaths and trauma already being blamed on LLMs, this
uncertainty is even pushing people to treat their 'pet' LLM more politely to
'dissuade' it (or its future Artificial General Intelligence offspring) from
suddenly turning on them. Here we address this acute need by deriving from
first principles an exact formula for when a Jekyll-and-Hyde tipping point
occurs at LLMs' most basic level. Requiring only secondary school mathematics,
it shows the cause to be the AI's attention spreading so thin it suddenly
snaps. This exact formula provides quantitative predictions for how the
tipping-point can be delayed or prevented by changing the prompt and the AI's
training. Tailored generalizations will provide policymakers and the public
with a firm platform for discussing any of AI's broader uses and risks, e.g. as
a personal counselor, medical advisor, decision-maker for when to use force in
a conflict situation. It also meets the need for clear and transparent answers
to questions like ''should I be polite to my LLM?''

</details>


### [211] [LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains](https://arxiv.org/abs/2504.20983)
*Giuseppe De Giacomo, Gianmarco Parretti, Shufang Zhu*

Main category: cs.AI

TL;DR: The paper introduces adaptive strategies for multi-tier LTLf synthesis, dynamically enforcing objectives based on environment cooperation, with a polynomial-time game-theoretic method.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of synthesizing strategies for multi-tier goals in nondeterministic planning domains, ensuring dynamic adaptation to environment cooperation.

Method: A game-theoretic technique is proposed, sound and complete, with quadratic complexity in the number of objectives.

Result: The method efficiently computes adaptive strategies, handling multi-tier goals with minimal overhead compared to standard LTLf synthesis.

Conclusion: The approach successfully synthesizes adaptive strategies for multi-tier goals, demonstrating practical feasibility with polynomial-time complexity.

Abstract: We study a variant of LTLf synthesis that synthesizes adaptive strategies for
achieving a multi-tier goal, consisting of multiple increasingly challenging
LTLf objectives in nondeterministic planning domains. Adaptive strategies are
strategies that at any point of their execution (i) enforce the satisfaction of
as many objectives as possible in the multi-tier goal, and (ii) exploit
possible cooperation from the environment to satisfy as many as possible of the
remaining ones. This happens dynamically: if the environment cooperates (ii)
and an objective becomes enforceable (i), then our strategies will enforce it.
We provide a game-theoretic technique to compute adaptive strategies that is
sound and complete. Notably, our technique is polynomial, in fact quadratic, in
the number of objectives. In other words, it handles multi-tier goals with only
a minor overhead compared to standard LTLf synthesis.

</details>


### [212] [TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem](https://arxiv.org/abs/2311.18662)
*Daniel Fuertes, Carlos R. del-Blanco, Fernando Jaureguizar, Narciso García*

Main category: cs.AI

TL;DR: TOP-Former is a neural network-based multi-agent route planner for the Team Orienteering Problem, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of linear programming (slow) and heuristic methods (suboptimal) for the Team Orienteering Problem in fleet route planning.

Method: Uses a centralized Transformer neural network to encode the scenario as a graph and analyze the global context of all agents for collaborative solutions.

Result: Outperforms state-of-the-art methods in accuracy and computation speed.

Conclusion: TOP-Former provides efficient and accurate solutions for the Team Orienteering Problem, leveraging global context understanding.

Abstract: Route planning for a fleet of vehicles is an important task in applications
such as package delivery, surveillance, or transportation, often integrated
within larger Intelligent Transportation Systems (ITS). This problem is
commonly formulated as a Vehicle Routing Problem (VRP) known as the Team
Orienteering Problem (TOP). Existing solvers for this problem primarily rely on
either linear programming, which provides accurate solutions but requires
computation times that grow with the size of the problem, or heuristic methods,
which typically find suboptimal solutions in a shorter time. In this paper, we
introduce TOP-Former, a multi-agent route planning neural network designed to
efficiently and accurately solve the Team Orienteering Problem. The proposed
algorithm is based on a centralized Transformer neural network capable of
learning to encode the scenario (modeled as a graph) and analyze the complete
context of all agents to deliver fast, precise, and collaborative solutions.
Unlike other neural network-based approaches that adopt a more local
perspective, TOP-Former is trained to understand the global situation of the
vehicle fleet and generate solutions that maximize long-term expected returns.
Extensive experiments demonstrate that the presented system outperforms most
state-of-the-art methods in terms of both accuracy and computation speed.

</details>


### [213] [Problem Solving Through Human-AI Preference-Based Cooperation](https://arxiv.org/abs/2408.07461)
*Subhabrata Dutta, Timo Kaufmann, Goran Glavaš, Ivan Habernal, Kristian Kersting, Frauke Kreuter, Mira Mezini, Iryna Gurevych, Eyke Hüllermeier, Hinrich Schuetze*

Main category: cs.AI

TL;DR: The paper argues that current generative AI falls short in solving complex expert problems and proposes HAICo2, a human-AI co-construction framework, to address these limitations.


<details>
  <summary>Details</summary>
Motivation: The belief in imminent AGI or superhuman AI is countered by the unsolved complexities in expert domains, highlighting the need for human-AI collaboration.

Method: The paper introduces HAICo2, a framework for human-AI co-construction, and begins its formalization while identifying open research challenges.

Result: Current generative AI lacks reliability for complex tasks due to tracking, preference expression, and adaptability issues.

Conclusion: HAICo2 is proposed as a solution to enhance human-AI cooperation, though significant research challenges remain.

Abstract: While there is a widespread belief that artificial general intelligence (AGI)
-- or even superhuman AI -- is imminent, complex problems in expert domains are
far from being solved. We argue that such problems require human-AI cooperation
and that the current state of the art in generative AI is unable to play the
role of a reliable partner due to a multitude of shortcomings, including
difficulty to keep track of a complex solution artifact (e.g., a software
program), limited support for versatile human preference expression and lack of
adapting to human preference in an interactive setting. To address these
challenges, we propose HAICo2, a novel human-AI co-construction framework. We
take first steps towards a formalization of HAICo2 and discuss the difficult
open research problems that it faces.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [214] [APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech](https://arxiv.org/abs/2504.20447)
*Zhicheng Lian, Lizhi Wang, Hua Huang*

Main category: cs.SD

TL;DR: The paper introduces APG-MOS, a model combining auditory perception and semantic analysis to improve speech quality assessment, outperforming benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance consistency with human judgments in speech quality assessment by integrating auditory perception mechanisms, often neglected in deep learning models.

Method: APG-MOS uses a perceptual module for cochlear simulation, RVQ-based semantic distortion modeling, and a residual cross-attention architecture for multimodal fusion.

Result: APG-MOS achieves superior performance on two primary benchmarks.

Conclusion: The proposed model effectively bridges the gap between computational predictions and human perception in speech quality assessment.

Abstract: Automatic speech quality assessment aims to quantify subjective human
perception of speech through computational models to reduce the need for
labor-consuming manual evaluations. While models based on deep learning have
achieved progress in predicting mean opinion scores (MOS) to assess synthetic
speech, the neglect of fundamental auditory perception mechanisms limits
consistency with human judgments. To address this issue, we propose an auditory
perception guided-MOS prediction model (APG-MOS) that synergistically
integrates auditory modeling with semantic analysis to enhance consistency with
human judgments. Specifically, we first design a perceptual module, grounded in
biological auditory mechanisms, to simulate cochlear functions, which encodes
acoustic signals into biologically aligned electrochemical representations.
Secondly, we propose a residual vector quantization (RVQ)-based semantic
distortion modeling method to quantify the degradation of speech quality at the
semantic level. Finally, we design a residual cross-attention architecture,
coupled with a progressive learning strategy, to enable multimodal fusion of
encoded electrochemical signals and semantic representations. Experiments
demonstrate that APG-MOS achieves superior performance on two primary
benchmarks. Our code and checkpoint will be available on a public repository
upon publication.

</details>


### [215] [Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier](https://arxiv.org/abs/2504.20124)
*Abul Ehtesham, Saket Kumar, Aditi Singh, Tala Talaei Khoei*

Main category: cs.SD

TL;DR: An AI-powered diagnostic pipeline using Google's HeAR model detects early asthma signs in children from respiratory sounds, achieving over 91% accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of asthma in children is crucial to prevent long-term complications and reduce emergency interventions.

Method: Uses the SPRSound dataset and HeAR model to extract 512-dimensional embeddings from 2-second audio segments. Multiple classifiers (SVM, Random Forest, MLP) are trained on these embeddings.

Result: Achieves over 91% accuracy with strong precision-recall metrics. Includes visualization (PCA), misclassification analysis, and ROC insights.

Conclusion: Short, low-resource pediatric recordings with foundation audio models enable fast, noninvasive asthma screening, promising for remote healthcare.

Abstract: Early detection of asthma in children is crucial to prevent long-term
respiratory complications and reduce emergency interventions. This work
presents an AI-powered diagnostic pipeline that leverages Googles Health
Acoustic Representations (HeAR) model to detect early signs of asthma from
pediatric respiratory sounds. The SPRSound dataset, the first open-access
collection of annotated respiratory sounds in children aged 1 month to 18
years, is used to extract 2-second audio segments labeled as wheeze, crackle,
rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional
representation using HeAR, a foundation model pretrained on 300 million
health-related audio clips, including 100 million cough sounds. Multiple
classifiers, including SVM, Random Forest, and MLP, are trained on these
embeddings to distinguish between asthma-indicative and normal sounds. The
system achieves over 91\% accuracy, with strong performance on precision-recall
metrics for positive cases. In addition to classification, learned embeddings
are visualized using PCA, misclassifications are analyzed through waveform
playback, and ROC and confusion matrix insights are provided. This method
demonstrates that short, low-resource pediatric recordings, when powered by
foundation audio models, can enable fast, noninvasive asthma screening. The
approach is especially promising for digital diagnostics in remote or
underserved healthcare settings.

</details>


### [216] [DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models](https://arxiv.org/abs/2504.20625)
*Sagi Della Torre, Mirco Pezzoli, Fabio Antonacci, Sharon Gannot*

Main category: cs.SD

TL;DR: The paper proposes using Denoising Diffusion Probabilistic Models (DDPM) to estimate Room Impulse Responses (RIRs) at unmeasured locations, outperforming traditional interpolation methods.


<details>
  <summary>Details</summary>
Motivation: High-quality RIRs are essential for audio applications, but dense spatial sampling is impractical. The research aims to address this gap.

Method: The method treats RIR matrices like images and uses DDPM for reconstruction, validated on simulated data with various microphone array geometries.

Result: The approach accurately reconstructs missing RIRs, surpassing Spline Cubic Interpolation in performance metrics.

Conclusion: Generative models like DDPM show promise for RIR interpolation, enabling data generation from sparse measurements.

Abstract: Room Impulse Responses (RIRs) characterize acoustic environments and are
crucial in multiple audio signal processing tasks. High-quality RIR estimates
drive applications such as virtual microphones, sound source localization,
augmented reality, and data augmentation. However, obtaining RIR measurements
with high spatial resolution is resource-intensive, making it impractical for
large spaces or when dense sampling is required. This research addresses the
challenge of estimating RIRs at unmeasured locations within a room using
Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the
analogy between RIR matrices and image inpainting, transforming RIR data into a
format suitable for diffusion-based reconstruction.
  Using simulated RIR data based on the image method, we demonstrate our
approach's effectiveness on microphone arrays of different curvatures, from
linear to semi-circular. Our method successfully reconstructs missing RIRs,
even in large gaps between microphones. Under these conditions, it achieves
accurate reconstruction, significantly outperforming baseline Spline Cubic
Interpolation in terms of Normalized Mean Square Error and Cosine Distance
between actual and interpolated RIRs.
  This research highlights the potential of using generative models for
effective RIR interpolation, paving the way for generating additional data from
limited real-world measurements.

</details>


### [217] [ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe](https://arxiv.org/abs/2504.20776)
*David Funosas, Elodie Massol, Yves Bas, Svenja Schmidt, Dominik Arend, Alexander Gebhard, Luc Barbaro, Sebastian König, Rafael Carbonell Font, David Sannier, Fernand Deroussen, Jérôme Sueur, Christian Roesti, Tomi Trilar, Wolfgang Forstmeier, Lucas Roger, Eloïsa Matheu, Piotr Guzik, Julien Barataud, Laurent Pelozuelo, Stéphane Puissant, Sandra Mueller, Björn Schuller, Jose M. Montoya, Andreas Triantafyllopoulos, Maxime Cauchoix*

Main category: cs.SD

TL;DR: ECOSoundSet is a dataset of 10,653 recordings of 200 orthopteran and 24 cicada species, designed to improve automated acoustic recognition of European insects. It includes weak and strong labeling, with a train/validation/test split for deep learning.


<details>
  <summary>Details</summary>
Motivation: Existing tools for insect acoustic recognition lack scope and require large, diverse datasets. ECOSoundSet addresses this gap by providing a comprehensive dataset for North, Central, and temperate Western Europe.

Method: The dataset combines weakly labeled (presence inferred) and strongly labeled (time and frequency annotated) recordings, with a structured split for training and evaluation.

Result: ECOSoundSet offers a valuable resource for training deep learning algorithms, enhancing the classification of orthopterans and cicadas.

Conclusion: This dataset fills a critical need for large-scale, annotated acoustic data, supporting advancements in automated insect recognition.

Abstract: Currently available tools for the automated acoustic recognition of European
insects in natural soundscapes are limited in scope. Large and ecologically
heterogeneous acoustic datasets are currently needed for these algorithms to
cross-contextually recognize the subtle and complex acoustic signatures
produced by each species, thus making the availability of such datasets a key
requisite for their development. Here we present ECOSoundSet (European
Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings
of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when
including subspecies) present in North, Central, and temperate Western Europe
(Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland,
Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly
through targeted fieldwork in South France and Catalonia and partly through
contributions from various European entomologists. The dataset is composed of a
combination of coarsely labeled recordings, for which we can only infer the
presence, at some point, of their target species (weak labeling), and finely
annotated recordings, for which we know the specific time and frequency range
of each insect sound present in the recording (strong labeling). We also
provide a train/validation/test split of the strongly labeled recordings, with
respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate
their incorporation in the training and evaluation of deep learning algorithms.
This dataset could serve as a meaningful complement to recordings already
available online for the training of deep learning algorithms for the acoustic
classification of orthopterans and cicadas in North, Central, and temperate
Western Europe.

</details>


### [218] [Enhancing Non-Core Language Instruction-Following in Speech LLMs via Semi-Implicit Cross-Lingual CoT Reasoning](https://arxiv.org/abs/2504.20835)
*Hongfei Xue, Yufeng Tang, Hexin Liu, Jun Zhang, Xuelong Geng, Lei Xie*

Main category: cs.SD

TL;DR: The paper proposes XS-CoT, a framework to enhance SLLMs' performance in non-core languages by integrating speech-to-text translation and semi-implicit reasoning, improving accuracy and reducing latency.


<details>
  <summary>Details</summary>
Motivation: Existing SLLMs struggle with non-core languages due to data scarcity and limited multilingual reasoning. XS-CoT aims to bridge this gap.

Method: XS-CoT generates cross-lingual tokens and uses a semi-implicit CoT scheme to compress intermediate reasoning tokens while retaining logic.

Result: XS-CoT improves non-core language responses by 45% in GPT-4 score and reduces token delay by over 50%.

Conclusion: XS-CoT efficiently leverages core-language reasoning to enhance non-core language performance with minimal training data.

Abstract: Large language models have been extended to the speech domain, leading to the
development of speech large language models (SLLMs). While existing SLLMs
demonstrate strong performance in speech instruction-following for core
languages (e.g., English), they often struggle with non-core languages due to
the scarcity of paired speech-text data and limited multilingual semantic
reasoning capabilities. To address this, we propose the semi-implicit
Cross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates
speech-to-text translation into the reasoning process of SLLMs. The XS-CoT
generates four types of tokens: instruction and response tokens in both core
and non-core languages, enabling cross-lingual transfer of reasoning
capabilities. To mitigate inference latency in generating target non-core
response tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which
progressively compresses the first three types of intermediate reasoning tokens
while retaining global reasoning logic during training. By leveraging the
robust reasoning capabilities of the core language, XS-CoT improves responses
for non-core languages by up to 45\% in GPT-4 score when compared to direct
supervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN.
Moreover, the semi-implicit XS-CoT reduces token delay by more than 50\% with a
slight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount
of high-quality training data for non-core languages by leveraging the
reasoning capabilities of core languages. To support training, we also develop
a data pipeline and open-source speech instruction-following datasets in
Japanese, German, and French.

</details>


### [219] [End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation](https://arxiv.org/abs/2504.20923)
*Andrea Di Pierno, Luca Guarnera, Dario Allegra, Sebastiano Battiato*

Main category: cs.SD

TL;DR: Proposes RawNetLite, a lightweight deep learning model for detecting audio deepfakes, emphasizing robustness through diverse training data and Focal Loss. Achieves high accuracy on in-domain and challenging out-of-distribution datasets.


<details>
  <summary>Details</summary>
Motivation: Address the growing threat of audio deepfakes by improving detection under open-world conditions where spoofing methods vary.

Method: End-to-end deep learning framework (RawNetLite) using raw waveforms, combining multi-domain data, Focal Loss, and audio augmentations like pitch shifting and noise.

Result: 99.7% F1 and 0.25% EER on in-domain data; 83.4% F1 and 16.4% EER on out-of-distribution data.

Conclusion: Diverse training data, tailored loss functions, and audio augmentations are key for resilient and generalizable audio forgery detection.

Abstract: Audio deepfakes represent a growing threat to digital security and trust,
leveraging advanced generative models to produce synthetic speech that closely
mimics real human voices. Detecting such manipulations is especially
challenging under open-world conditions, where spoofing methods encountered
during testing may differ from those seen during training. In this work, we
propose an end-to-end deep learning framework for audio deepfake detection that
operates directly on raw waveforms. Our model, RawNetLite, is a lightweight
convolutional-recurrent architecture designed to capture both spectral and
temporal features without handcrafted preprocessing. To enhance robustness, we
introduce a training strategy that combines data from multiple domains and
adopts Focal Loss to emphasize difficult or ambiguous samples. We further
demonstrate that incorporating codec-based manipulations and applying
waveform-level audio augmentations (e.g., pitch shifting, noise, and time
stretching) leads to significant generalization improvements under realistic
acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on
in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging
out-of-distribution test set (AVSpoof2021 + CodecFake). These findings
highlight the importance of diverse training data, tailored objective functions
and audio augmentations in building resilient and generalizable audio forgery
detectors. Code and pretrained models are available at
https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.

</details>


### [220] [QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation](https://arxiv.org/abs/2405.15863)
*Chang Li, Ruoyu Wang, Lijuan Liu, Jun Du, Yixuan Sun, Zilu Guo, Zhenrong Zhang, Yuan Jiang, Jianqing Gao, Feng Ma*

Main category: cs.SD

TL;DR: The paper proposes a quality-aware training paradigm and a masked diffusion transformer (MDT) model for text-to-music generation, addressing data quality issues and improving musicality.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for text-to-music generation suffer from low-quality waveforms and inconsistent text-audio alignment, limiting model performance.

Method: The authors introduce a quality-aware training approach and adapt an MDT model for the task, alongside a three-stage caption refinement method.

Result: The model achieves state-of-the-art performance on benchmark datasets like MusicCaps and Song-Describer, validated by objective and subjective metrics.

Conclusion: The proposed methods effectively enhance music generation quality and musicality, with open-sourced code and checkpoints for broader use.

Abstract: Text-to-music (TTM) generation, which converts textual descriptions into
audio, opens up innovative avenues for multimedia creation. Achieving high
quality and diversity in this process demands extensive, high-quality data,
which are often scarce in available datasets. Most open-source datasets
frequently suffer from issues like low-quality waveforms and low text-audio
consistency, hindering the advancement of music generation models. To address
these challenges, we propose a novel quality-aware training paradigm for
generating high-quality, high-musicality music from large-scale,
quality-imbalanced datasets. Additionally, by leveraging unique properties in
the latent space of musical signals, we adapt and implement a masked diffusion
transformer (MDT) model for the TTM task, showcasing its capacity for quality
control and enhanced musicality. Furthermore, we introduce a three-stage
caption refinement approach to address low-quality captions' issue. Experiments
show state-of-the-art (SOTA) performance on benchmark datasets including
MusicCaps and the Song-Describer Dataset with both objective and subjective
metrics. Demo audio samples are available at https://qa-mdt.github.io/, code
and pretrained checkpoints are open-sourced at
https://github.com/ivcylc/OpenMusic.

</details>


### [221] [Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness](https://arxiv.org/abs/2504.18950)
*Erfan Loweimi, Mengjie Qian, Kate Knill, Mark Gales*

Main category: cs.SD

TL;DR: The paper explores challenges and solutions for speaker retrieval in large audio/video archives, focusing on the BBC Rewind archive, and demonstrates the robustness of the proposed framework.


<details>
  <summary>Details</summary>
Motivation: The increasing importance of efficient content access and information retrieval from large audio/video archives, especially with limited metadata and varied acoustic conditions.

Method: Investigates speaker retrieval systems, addressing challenges like metadata extraction and unconstrained acoustic conditions. Includes speaker diarisation, embedding extraction, and query selection.

Result: Systematic experiments show the effectiveness and robustness of the developed systems in clean and distorted conditions.

Conclusion: The proposed framework is versatile and scalable, applicable beyond the BBC Rewind corpus.

Abstract: There is a growing abundance of publicly available or company-owned
audio/video archives, highlighting the increasing importance of efficient
access to desired content and information retrieval from these archives. This
paper investigates the challenges, solutions, effectiveness, and robustness of
speaker retrieval systems developed "in the wild" which involves addressing two
primary challenges: extraction of task-relevant labels from limited metadata
for system development and evaluation, as well as the unconstrained acoustic
conditions encountered in the archive, ranging from quiet studios to adverse
noisy environments. While we focus on the publicly-available BBC Rewind archive
(spanning 1948 to 1979), our framework addresses the broader issue of speaker
retrieval on extensive and possibly aged archives with no control over the
content and acoustic conditions. Typically, these archives offer a brief and
general file description, mostly inadequate for specific applications like
speaker retrieval, and manual annotation of such large-scale archives is
unfeasible. We explore various aspects of system development (e.g., speaker
diarisation, embedding extraction, query selection) and analyse the challenges,
possible solutions, and their functionality. To evaluate the performance, we
conduct systematic experiments in both clean setup and against various
distortions simulating real-world applications. Our findings demonstrate the
effectiveness and robustness of the developed speaker retrieval systems,
establishing the versatility and scalability of the proposed framework for a
wide range of applications beyond the BBC Rewind corpus.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [222] [A constraints-based approach to fully interpretable neural networks for detecting learner behaviors](https://arxiv.org/abs/2504.20055)
*Juan D. Pinto, Luc Paquette*

Main category: cs.LG

TL;DR: A novel interpretable neural-network model for detecting gaming-the-system behavior in education, ensuring faithful and intelligible explanations.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns about the interpretability of machine learning models in education by developing a model that is interpretable by design.

Method: Implementing constraints to simplify the model's inference process and align it with human understanding, then training it to detect gaming-the-system behavior.

Result: The model successfully learns patterns indicative of gaming-the-system behavior and provides fully interpretable explanations.

Conclusion: The approach offers a way to create interpretable models in education, with potential for evaluating explainability through human-grounded methods.

Abstract: The increasing use of complex machine learning models in education has led to
concerns about their interpretability, which in turn has spurred interest in
developing explainability techniques that are both faithful to the model's
inner workings and intelligible to human end-users. In this paper, we describe
a novel approach to creating a neural-network-based behavior detection model
that is interpretable by design. Our model is fully interpretable, meaning that
the parameters we extract for our explanations have a clear interpretation,
fully capture the model's learned knowledge about the learner behavior of
interest, and can be used to create explanations that are both faithful and
intelligible. We achieve this by implementing a series of constraints to the
model that both simplify its inference process and bring it closer to a human
conception of the task at hand. We train the model to detect gaming-the-system
behavior, evaluate its performance on this task, and compare its learned
patterns to those identified by human experts. Our results show that the model
is successfully able to learn patterns indicative of gaming-the-system behavior
while providing evidence for fully interpretable explanations. We discuss the
implications of our approach and suggest ways to evaluate explainability using
a human-grounded approach.

</details>


### [223] [A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives](https://arxiv.org/abs/2504.20069)
*Junhong Lai, Jiyu Wei, Lin Yao, Yueming Wang*

Main category: cs.LG

TL;DR: A review of EEG foundation models (EEG-FMs), their architectures, pre-training strategies, datasets, challenges, and future directions in EEG analysis.


<details>
  <summary>Details</summary>
Motivation: To understand brain activity and diagnose neurological disorders using EEG signals, leveraging the potential of EEG-FMs.

Method: Review of various EEG-FMs, including their architectures, pre-training strategies, and datasets.

Result: EEG-FMs show great potential in EEG data processing and analysis, but challenges remain.

Conclusion: The review provides a comprehensive overview for researchers and practitioners, highlighting future directions for EEG-FMs.

Abstract: Electroencephalogram (EEG) signals play a crucial role in understanding brain
activity and diagnosing neurological disorders. This review focuses on the
recent development of EEG foundation models(EEG-FMs), which have shown great
potential in processing and analyzing EEG data. We discuss various EEG-FMs,
including their architectures, pre-training strategies, their pre-training and
downstream datasets and other details. The review also highlights the
challenges and future directions in this field, aiming to provide a
comprehensive overview for researchers and practitioners interested in EEG
analysis and related EEG-FMs.

</details>


### [224] [Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization](https://arxiv.org/abs/2504.20070)
*Altun Shukurlu*

Main category: cs.LG

TL;DR: The paper revisits Deep Knowledge Tracing (DKT) by improving its architecture with LSTMs and GRUs, re-implementing it in PyTorch for better extensibility, and benchmarking optimizers. Results show improved accuracy and stability with GRUs/LSTMs and adaptive optimizers like Adam/AdamW.


<details>
  <summary>Details</summary>
Motivation: The original DKT implementation in Torch was limited in extensibility and reproducibility. This work aims to enhance the model's architecture and optimization efficiency while modernizing its framework.

Method: The authors replaced standard RNNs with LSTMs and GRUs to better handle long-term dependencies. They re-implemented DKT in PyTorch and tested optimizers (SGD, RMSProp, Adagrad, Adam, AdamW) for performance.

Result: GRUs and LSTMs outperformed basic RNNs in accuracy and training stability. Adaptive optimizers (Adam, AdamW) surpassed SGD in convergence and final performance.

Conclusion: The PyTorch-based DKT implementation offers a reproducible, extensible foundation for future research, with improved architectures and optimizers enhancing educational modeling.

Abstract: Deep Knowledge Tracing (DKT) models student learning behavior by using
Recurrent Neural Networks (RNNs) to predict future performance based on
historical interaction data. However, the original implementation relied on
standard RNNs in the Lua-based Torch framework, which limited extensibility and
reproducibility. In this work, we revisit the DKT model from two perspectives:
architectural improvements and optimization efficiency. First, we enhance the
model using gated recurrent units, specifically Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRU), which better capture long-term
dependencies and help mitigate vanishing gradient issues. Second, we
re-implement DKT using the PyTorch framework, enabling a modular and accessible
infrastructure compatible with modern deep learning workflows. We also
benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and
AdamW to evaluate their impact on convergence speed and predictive accuracy in
educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy
datasets show that GRUs and LSTMs achieve higher accuracy and improved training
stability compared to basic RNNs, while adaptive optimizers such as Adam and
AdamW consistently outperform SGD in both early-stage learning and final model
performance. Our open-source PyTorch implementation provides a reproducible and
extensible foundation for future research in neural knowledge tracing and
personalized learning systems.

</details>


### [225] [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
*Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li*

Main category: cs.LG

TL;DR: StarPO and RAGEN address multi-turn agent RL training challenges, revealing Echo Trap issues, optimal rollout shaping, and the need for reasoning-aware rewards.


<details>
  <summary>Details</summary>
Motivation: Training LLMs as interactive agents faces challenges like long-horizon decision-making and stochastic feedback, with multi-turn RL being underexplored.

Method: Proposes StarPO for trajectory-level agent RL and RAGEN for training/evaluating LLM agents, with a stabilized variant (StarPO-S) to address Echo Trap.

Result: Identifies Echo Trap, optimal rollout conditions (diverse states, medium granularity, frequent sampling), and the necessity of reasoning-aware rewards.

Conclusion: Agent reasoning in multi-turn RL requires careful reward design; StarPO-S and RAGEN offer solutions for effective training.

Abstract: Training large language models (LLMs) as interactive agents presents unique
challenges including long-horizon decision making and interacting with
stochastic environment feedback. While reinforcement learning (RL) has enabled
progress in static tasks, multi-turn agent RL training remains underexplored.
We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a
general framework for trajectory-level agent RL, and introduce RAGEN, a modular
system for training and evaluating LLM agents. Our study on three stylized
environments reveals three core findings. First, our agent RL training shows a
recurring mode of Echo Trap where reward variance cliffs and gradient spikes;
we address this with StarPO-S, a stabilized variant with trajectory filtering,
critic incorporation, and decoupled clipping. Second, we find the shaping of RL
rollouts would benefit from diverse initial states, medium interaction
granularity and more frequent sampling. Third, we show that without
fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge
through multi-turn RL and they may show shallow strategies or hallucinated
thoughts. Code and environments are available at
https://github.com/RAGEN-AI/RAGEN.

</details>


### [226] [Low-Rank Matrix Approximation for Neural Network Compression](https://arxiv.org/abs/2504.20078)
*Kalyan Cherukuri, Aarav Lala*

Main category: cs.LG

TL;DR: The paper introduces ARSVD, an adaptive-rank SVD method for compressing DNNs by dynamically selecting ranks per layer based on energy expenditure, outperforming fixed-rank methods.


<details>
  <summary>Details</summary>
Motivation: DNNs face memory and computational constraints; ARSVD aims to balance compression and performance by adapting ranks per layer.

Method: ARSVD dynamically adjusts rank per layer using energy distribution, applied to an MLP trained on MNIST, CIFAR-10, and CIFAR-100.

Result: ARSVD achieves significant compression without accuracy loss, outperforming static rank reduction methods.

Conclusion: ARSVD is effective for resource-constrained scenarios, maintaining accuracy while reducing computational and memory demands.

Abstract: Deep Neural Networks (DNNs) are often constrained by their large memories and
computational restrictions. In this paper, we introduce a novel adaptive-rank
Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase
of the fully connected layers below a certain threshold in energy expenditure.
Unlike conventional SVD compression methods that apply a fixed rank reduction
in all layers, our ARSVD method uses energy distribution to adaptively select
rank per layer while retaining accuracy. This is done for each layer in an
effort to use as much energy as possible while maintaining the lowest accuracy
loss. Such accuracy-adaptive approaches outperform traditional static rank
reduction methods by providing an improved balance between compression and
model performance. We first train a simple Multi-Layer Perceptron (MLP) on the
MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using
accuracy and F1-score. After applying ARSVD, our results demonstrate that the
technique can achieve substantial model compression without compromising
classification accuracy. These results illustrate the usefulness of ARSVD in
computing scenarios where both computational and memory resources are scarce.

</details>


### [227] [FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking](https://arxiv.org/abs/2504.20079)
*Xuan Rao, Bo Zhao, Derong Liu, Cesare Alippi*

Main category: cs.LG

TL;DR: FX-DARTS reduces prior constraints in DARTS by eliminating cell topology restrictions and modifying super-network discretization, enabling more flexible and powerful neural architectures.


<details>
  <summary>Details</summary>
Motivation: Strong priors in DARTS limit architectural flexibility and hinder Auto-ML progress. This paper aims to relax these constraints to explore better neural networks.

Method: FX-DARTS uses an Entropy-based Super-Network Shrinking (ESS) framework to eliminate prior constraints on cell topology and discretization, ensuring stability in the search space.

Result: FX-DARTS successfully explores neural architectures with competitive performance-complexity trade-offs on image classification benchmarks.

Conclusion: FX-DARTS advances Auto-ML by reducing prior constraints, enabling more flexible and powerful neural architecture search without sacrificing stability.

Abstract: Strong priors are imposed on the search space of Differentiable Architecture
Search (DARTS), such that cells of the same type share the same topological
structure and each intermediate node retains two operators from distinct nodes.
While these priors reduce optimization difficulties and improve the
applicability of searched architectures, they hinder the subsequent development
of automated machine learning (Auto-ML) and prevent the optimization algorithm
from exploring more powerful neural networks through improved architectural
flexibility. This paper aims to reduce these prior constraints by eliminating
restrictions on cell topology and modifying the discretization mechanism for
super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which
leverages an Entropy-based Super-Network Shrinking (ESS) framework, is
presented to address the challenges arising from the elimination of prior
constraints. Notably, FX-DARTS enables the derivation of neural architectures
without strict prior rules while maintaining the stability in the enlarged
search space. Experimental results on image classification benchmarks
demonstrate that FX-DARTS is capable of exploring a set of neural architectures
with competitive trade-offs between performance and computational complexity
within a single search procedure.

</details>


### [228] [Swapped Logit Distillation via Bi-level Teacher Alignment](https://arxiv.org/abs/2504.20108)
*Stephen Ekaputra Limantoro, Jhe-Hao Lin, Chih-Yu Wang, Yi-Lung Tsai, Hong-Han Shuai, Ching-Chun Huang, Wen-Huang Cheng*

Main category: cs.LG

TL;DR: Swapped Logit Distillation (SLD) improves knowledge distillation by addressing incorrect predictions through swapped logit processing and loss scheduling, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Traditional KD may lead to incorrect predictions due to direct knowledge transfer. SLD addresses this by leveraging swapped logit processing under two key assumptions about prediction errors and probability limits.

Method: SLD introduces a swapped logit processing scheme, transforming teacher and student outputs into two teachers, and uses loss scheduling for alignment.

Result: Extensive experiments on image classification show SLD consistently outperforms state-of-the-art methods.

Conclusion: SLD effectively enhances knowledge distillation by mitigating incorrect predictions and improving alignment, demonstrating superior performance.

Abstract: Knowledge distillation (KD) compresses the network capacity by transferring
knowledge from a large (teacher) network to a smaller one (student). It has
been mainstream that the teacher directly transfers knowledge to the student
with its original distribution, which can possibly lead to incorrect
predictions. In this article, we propose a logit-based distillation via swapped
logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed
under two assumptions: (1) the wrong prediction occurs when the prediction
label confidence is not the maximum; (2) the "natural" limit of probability
remains uncertain as the best value addition to the target cannot be
determined. To address these issues, we propose a swapped logit processing
scheme. Through this approach, we find that the swap method can be effectively
extended to teacher and student outputs, transforming into two teachers. We
further introduce loss scheduling to boost the performance of two teachers'
alignment. Extensive experiments on image classification tasks demonstrate that
SLD consistently performs best among previous state-of-the-art methods.

</details>


### [229] [DNAD: Differentiable Neural Architecture Distillation](https://arxiv.org/abs/2504.20080)
*Xuan Rao, Bo Zhao, Derong Liu*

Main category: cs.LG

TL;DR: The paper introduces DNAD, a method combining search by deleting (SNPS) and search by imitating (KD) to design efficient neural networks with balanced performance and complexity.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient neural networks that balance performance and computational complexity, the paper develops DNAD.

Method: DNAD integrates SNPS (search by deleting) with KD (search by imitating) to derive flexible, Pareto-optimal architectures.

Result: DNAD achieves competitive error rates with fewer parameters and FLOPs, e.g., 23.7% top-1 error on ImageNet with 6.0M parameters.

Conclusion: DNAD outperforms DARTS-based methods, demonstrating effectiveness in deriving efficient architectures.

Abstract: To meet the demand for designing efficient neural networks with appropriate
trade-offs between model performance (e.g., classification accuracy) and
computational complexity, the differentiable neural architecture distillation
(DNAD) algorithm is developed based on two cores, namely search by deleting and
search by imitating. Primarily, to derive neural architectures in a space where
cells of the same type no longer share the same topology, the super-network
progressive shrinking (SNPS) algorithm is developed based on the framework of
differentiable architecture search (DARTS), i.e., search by deleting. Unlike
conventional DARTS-based approaches which yield neural architectures with
simple structures and derive only one architecture during the search procedure,
SNPS is able to derive a Pareto-optimal set of architectures with flexible
structures by forcing the dynamic super-network shrink from a dense structure
to a sparse one progressively. Furthermore, since knowledge distillation (KD)
has shown great effectiveness to train a compact network with the assistance of
an over-parameterized model, we integrate SNPS with KD to formulate the DNAD
algorithm, i.e., search by imitating. By minimizing behavioral differences
between the super-network and teacher network, the over-fitting of one-level
DARTS is avoided and well-performed neural architectures are derived.
Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both
SNPS and DNAD are able to derive a set of architectures which achieve similar
or lower error rates with fewer parameters and FLOPs. Particularly, DNAD
achieves the top-1 error rate of 23.7% on ImageNet classification with a model
of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.

</details>


### [230] [Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis](https://arxiv.org/abs/2504.20096)
*Damien Martins Gomes*

Main category: cs.LG

TL;DR: AdaFisher is a novel adaptive second-order optimizer using a diagonal block-Kronecker approximation of the Fisher matrix, balancing convergence and computational efficiency for DNN training.


<details>
  <summary>Details</summary>
Motivation: First-order methods dominate DNN training but lack curvature information, while second-order methods offer better convergence but are computationally expensive. AdaFisher aims to bridge this gap.

Method: AdaFisher leverages a diagonal block-Kronecker approximation of the Fisher information matrix to adaptively precondition gradients.

Result: AdaFisher outperforms state-of-the-art optimizers in accuracy and convergence speed, showing stability in tasks like image classification and language modeling.

Conclusion: AdaFisher successfully combines the benefits of second-order optimization with computational efficiency, making it practical for DNN training.

Abstract: First-order optimization methods remain the standard for training deep neural
networks (DNNs). Optimizers like Adam incorporate limited curvature information
by preconditioning the stochastic gradient with a diagonal matrix. Despite the
widespread adoption of first-order methods, second-order optimization
algorithms often exhibit superior convergence compared to methods like Adam and
SGD. However, their practicality in training DNNs is still limited by a
significantly higher per-iteration computational cost compared to first-order
methods. In this thesis, we present AdaFisher, a novel adaptive second-order
optimizer that leverages a diagonal block-Kronecker approximation of the Fisher
information matrix to adaptively precondition gradients. AdaFisher aims to
bridge the gap between the improved convergence and generalization of
second-order methods and the computational efficiency needed for training DNNs.
Despite the traditionally slower speed of second-order optimizers, AdaFisher is
effective for tasks such as image classification and language modeling,
exhibiting remarkable stability and robustness during hyperparameter tuning. We
demonstrate that AdaFisher outperforms state-of-the-art optimizers in both
accuracy and convergence speed. The code is available from
https://github.com/AtlasAnalyticsLab/AdaFisher.

</details>


### [231] [Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics](https://arxiv.org/abs/2504.20099)
*Inmaculada Santamaria-Valenzuela, Victor Rodriguez-Fernandez, Javier Huertas-Tato, Jong Hyuk Park, David Camacho*

Main category: cs.LG

TL;DR: The study evaluates the interpretability of latent spaces in MOMENT, a transformer-based time series foundation model, finding performance improvements with fine-tuning but limited interpretability gains.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of MOMENT models for visual analysis tasks by examining their latent space interpretability.

Method: Evaluation of MOMENT models on five datasets, focusing on latent space structure and fine-tuning effects.

Result: Fine-tuning improved performance (loss reduction), but interpretability of embeddings remained limited.

Conclusion: MOMENT models are robust but need methodological refinements for better interpretability; they offer significant efficiency gains for visual analytics.

Abstract: The present study explores the interpretability of latent spaces produced by
time series foundation models, focusing on their potential for visual analysis
tasks. Specifically, we evaluate the MOMENT family of models, a set of
transformer-based, pre-trained architectures for multivariate time series tasks
such as: imputation, prediction, classification, and anomaly detection. We
evaluate the capacity of these models on five datasets to capture the
underlying structures in time series data within their latent space projection
and validate whether fine tuning improves the clarity of the resulting
embedding spaces. Notable performance improvements in terms of loss reduction
were observed after fine tuning. Visual analysis shows limited improvement in
the interpretability of the embeddings, requiring further work. Results suggest
that, although Time Series Foundation Models such as MOMENT are robust, their
latent spaces may require additional methodological refinements to be
adequately interpreted, such as alternative projection techniques, loss
functions, or data preprocessing strategies. Despite the limitations of MOMENT,
foundation models supose a big reduction in execution time and so a great
advance for interactive visual analytics.

</details>


### [232] [HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction](https://arxiv.org/abs/2504.20102)
*Qingzhi Yu, Shuai Yan, Wenfeng Dai, Xiang Cheng*

Main category: cs.LG

TL;DR: HyboWaveNet, a deep learning framework combining hyperbolic GNNs and multiscale graphical wavelet transforms, improves PPI prediction by capturing hierarchical and dynamic protein interactions.


<details>
  <summary>Details</summary>
Motivation: Existing PPI prediction methods lack causal interpretation and struggle with hierarchical and multi-scale interaction patterns.

Method: HyboWaveNet maps protein features to Lorentz space for hierarchical relationships and uses wavelet transforms for multi-scale feature extraction.

Result: Outperforms state-of-the-art methods on public datasets, with ablation studies confirming the wavelet transform's role in performance.

Conclusion: HyboWaveNet advances PPI prediction by integrating geometric deep learning and signal processing, offering a principled approach for biological systems.

Abstract: Protein-protein interactions (PPIs) are fundamental for deciphering cellular
functions,disease pathways,and drug discovery.Although existing neural networks
and machine learning methods have achieved high accuracy in PPI
prediction,their black-box nature leads to a lack of causal interpretation of
the prediction results and difficulty in capturing hierarchical geometries and
multi-scale dynamic interaction patterns among proteins.To address these
challenges, we propose HyboWaveNet,a novel deep learning framework that
collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale
graphical wavelet transform for robust PPI prediction. Mapping protein features
to Lorentz space simulates hierarchical topological relationships among
biomolecules via a hyperbolic distance metric,enabling node feature
representations that better fit biological a priori.HyboWaveNet inherently
simulates hierarchical and scale-free biological relationships, while the
integration of wavelet transforms enables adaptive extraction of local and
global interaction features across different resolutions. Our framework
generates node feature representations via a graph neural network under the
Lorenz model and generates pairs of positive samples under multiple different
views for comparative learning, followed by further feature extraction via
multi-scale graph wavelet transforms to predict potential PPIs. Experiments on
public datasets show that HyboWaveNet improves over both existing
state-of-the-art methods. We also demonstrate through ablation experimental
studies that the multi-scale graph wavelet transform module improves the
predictive performance and generalization ability of HyboWaveNet. This work
links geometric deep learning and signal processing to advance PPI prediction,
providing a principled approach for analyzing complex biological systems

</details>


### [233] [Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors](https://arxiv.org/abs/2504.20106)
*Ren-Wei Liang, Chin-Ting Hsu, Chan-Hung Yu, Saransh Agrawal, Shih-Cheng Huang, Shang-Tse Chen, Kuan-Hao Huang, Shao-Hua Sun*

Main category: cs.LG

TL;DR: The paper introduces Preference Vector, a modular framework for balancing helpfulness and harmlessness in LLMs by dynamically merging separate preference-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing methods like RLHF and DPO struggle with performance conflicts, limited controllability, and poor extendability in aligning LLM preferences.

Method: Train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time.

Result: Preference Vector improves helpfulness without excessive conservatism, enables smooth preference trade-offs, and supports scalable multi-preference alignment.

Conclusion: The modular Preference Vector framework offers fine-grained, user-controllable adjustments and seamless integration of new preferences.

Abstract: Ensuring that large language models (LLMs) are both helpful and harmless is a
critical challenge, as overly strict constraints can lead to excessive
refusals, while permissive models risk generating harmful content. Existing
approaches, such as reinforcement learning from human feedback (RLHF) and
direct preference optimization (DPO), attempt to balance these trade-offs but
suffer from performance conflicts, limited controllability, and poor
extendability. To address these issues, we propose Preference Vector, a novel
framework inspired by task arithmetic. Instead of optimizing multiple
preferences within a single objective, we train separate models on individual
preferences, extract behavior shifts as preference vectors, and dynamically
merge them at test time. This modular approach enables fine-grained,
user-controllable preference adjustments and facilitates seamless integration
of new preferences without retraining. Experiments show that our proposed
Preference Vector framework improves helpfulness without excessive
conservatism, allows smooth control over preference trade-offs, and supports
scalable multi-preference alignment.

</details>


### [234] [Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling](https://arxiv.org/abs/2504.20110)
*Yu-hsuan Chen, Jing Bi, Cyril Ngo Ngoc, Victor Oancea, Jonathan Cagan, Levent Burak Kara*

Main category: cs.LG

TL;DR: A self-supervised method for learning fine-scale geometric features from 3D models, improving surrogate modeling accuracy in data-scarce scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of current AI-driven surrogate models in preserving fine-scale geometric details due to scarce labeled datasets.

Method: Decouples geometric feature extraction from physics tasks, using near-zero level sampling and a batch-adaptive attention-weighted loss function.

Result: Validated in structural mechanics, showing strong performance in capturing design features and enabling accurate few-shot physics predictions.

Conclusion: Bridges the gap between geometric and physics-based representations, offering an effective surrogate modeling solution for data-scarce applications.

Abstract: AI-driven surrogate modeling has become an increasingly effective alternative
to physics-based simulations for 3D design, analysis, and manufacturing. These
models leverage data-driven methods to predict physical quantities
traditionally requiring computationally expensive simulations. However, the
scarcity of labeled CAD-to-simulation datasets has driven recent advancements
in self-supervised and foundation models, where geometric representation
learning is performed offline and later fine-tuned for specific downstream
tasks. While these approaches have shown promise, their effectiveness is
limited in applications requiring fine-scale geometric detail preservation.
This work introduces a self-supervised geometric representation learning method
designed to capture fine-scale geometric features from non-parametric 3D
models. Unlike traditional end-to-end surrogate models, this approach decouples
geometric feature extraction from downstream physics tasks, learning a latent
space embedding guided by geometric reconstruction losses. Key elements include
the essential use of near-zero level sampling and the innovative batch-adaptive
attention-weighted loss function, which enhance the encoding of intricate
design features. The proposed method is validated through case studies in
structural mechanics, demonstrating strong performance in capturing design
features and enabling accurate few-shot physics predictions. Comparisons with
traditional parametric surrogate modeling highlight its potential to bridge the
gap between geometric and physics-based representations, providing an effective
solution for surrogate modeling in data-scarce scenarios.

</details>


### [235] [Supervised Pretraining for Material Property Prediction](https://arxiv.org/abs/2504.20112)
*Chowdhury Mohammad Abid Rahman, Aldo H. Romero, Prashnna K. Gyawali*

Main category: cs.LG

TL;DR: The paper proposes supervised pretraining with surrogate labels for material property prediction, improving accuracy over baselines by 2% to 6.67% in MAE.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of supervised learning in material property prediction, which requires large labeled datasets, the paper explores self-supervised learning (SSL) with surrogate labels for pretraining.

Method: Introduces supervised pretraining using surrogate labels and a graph-based augmentation technique to enhance robustness. Evaluates on two SSL models and fine-tunes for six material property predictions.

Result: Achieves significant performance gains (2% to 6.67% improvement in MAE) over baselines, setting a new benchmark.

Conclusion: This work advances methodology in material property prediction by pioneering supervised pretraining with surrogate labels.

Abstract: Accurate prediction of material properties facilitates the discovery of novel
materials with tailored functionalities. Deep learning models have recently
shown superior accuracy and flexibility in capturing structure-property
relationships. However, these models often rely on supervised learning, which
requires large, well-annotated datasets an expensive and time-consuming
process. Self-supervised learning (SSL) offers a promising alternative by
pretraining on large, unlabeled datasets to develop foundation models that can
be fine-tuned for material property prediction. In this work, we propose
supervised pretraining, where available class information serves as surrogate
labels to guide learning, even when downstream tasks involve unrelated material
properties. We evaluate this strategy on two state-of-the-art SSL models and
introduce a novel framework for supervised pretraining. To further enhance
representation learning, we propose a graph-based augmentation technique that
injects noise to improve robustness without structurally deforming material
graphs. The resulting foundation models are fine-tuned for six challenging
material property predictions, achieving significant performance gains over
baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE)
and establishing a new benchmark in material property prediction. This study
represents the first exploration of supervised pertaining with surrogate labels
in material property prediction, advancing methodology and application in the
field.

</details>


### [236] [Benchmarking Transferability: A Framework for Fair and Robust Evaluation](https://arxiv.org/abs/2504.20121)
*Alireza Kazemi, Helia Rezvani, Mahsa Baktashmotlagh*

Main category: cs.LG

TL;DR: A benchmarking framework evaluates transferability scores, revealing inconsistencies in current metrics and advocating for standardized assessment protocols.


<details>
  <summary>Details</summary>
Motivation: To address the unreliability and lack of consensus in measuring transferability scores across domains.

Method: Introduces a comprehensive benchmarking framework to systematically evaluate transferability metrics under diverse settings.

Result: Highlights performance variations among metrics, with a 3.5% improvement using their proposed metric in head-training fine-tuning.

Conclusion: Standardized protocols are crucial for reliable transferability measures and informed model selection in cross-domain applications.

Abstract: Transferability scores aim to quantify how well a model trained on one domain
generalizes to a target domain. Despite numerous methods proposed for measuring
transferability, their reliability and practical usefulness remain
inconclusive, often due to differing experimental setups, datasets, and
assumptions. In this paper, we introduce a comprehensive benchmarking framework
designed to systematically evaluate transferability scores across diverse
settings. Through extensive experiments, we observe variations in how different
metrics perform under various scenarios, suggesting that current evaluation
practices may not fully capture each method's strengths and limitations. Our
findings underscore the value of standardized assessment protocols, paving the
way for more reliable transferability measures and better-informed model
selection in cross-domain applications. Additionally, we achieved a 3.5\%
improvement using our proposed metric for the head-training fine-tuning
experimental setup. Our code is available in this repository:
https://github.com/alizkzm/pert_robust_platform.

</details>


### [237] [LZ Penalty: An information-theoretic repetition penalty for autoregressive language models](https://arxiv.org/abs/2504.20131)
*Antonio A. Ginart, Naveen Kodali, Jason Lee, Caiming Xiong, Silvio Savarese, John R. Emmons*

Main category: cs.LG

TL;DR: The LZ penalty reduces degenerate repetitions in autoregressive language models without compromising capability, outperforming standard penalties like frequency and repetition penalties.


<details>
  <summary>Details</summary>
Motivation: To address the issue of degenerate repetitions in autoregressive language models, which standard penalties fail to mitigate effectively.

Method: The LZ penalty leverages codelengths from the LZ77 compression algorithm, sampling from the residual distribution after removing highly compressible information.

Result: The LZ penalty enables state-of-the-art models to use greedy decoding without degenerate repetitions, unlike standard penalties which fail up to 4% of the time.

Conclusion: The LZ penalty is a superior solution for eliminating degenerate repetitions in autoregressive models without loss of capability.

Abstract: We introduce the LZ penalty, a penalty specialized for reducing degenerate
repetitions in autoregressive language models without loss of capability. The
penalty is based on the codelengths in the LZ77 universal lossless compression
algorithm. Through the lens of the prediction-compression duality, decoding the
LZ penalty has the interpretation of sampling from the residual distribution
after removing the information that is highly compressible. We demonstrate the
LZ penalty enables state-of-the-art open-source reasoning models to operate
with greedy (temperature zero) decoding without loss of capability and without
instances of degenerate repetition. Both the industry-standard frequency
penalty and repetition penalty are ineffective, incurring degenerate repetition
rates of up to 4%.

</details>


### [238] [Causal Identification in Time Series Models](https://arxiv.org/abs/2504.20172)
*Erik Jahn, Karthik Karnik, Leonard J. Schulman*

Main category: cs.LG

TL;DR: The paper provides a bound for deciding causal effect identifiability in time series graphs with latent confounders, showing a constant-size graph segment suffices.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unbounded computation in causal identification for infinite time series graphs with latent confounders.

Method: Analyze the Causal Identification algorithm's applicability, focusing on graph segments of bounded size despite infinite time steps.

Result: A bound is established based on variables per time step and maximum time lag, enabling identifiability decisions with constant-size segments.

Conclusion: The Causal Identification algorithm can effectively decide identifiability in infinite time series graphs using bounded segments.

Abstract: In this paper, we analyze the applicability of the Causal Identification
algorithm to causal time series graphs with latent confounders. Since these
graphs extend over infinitely many time steps, deciding whether causal effects
across arbitrary time intervals are identifiable appears to require computation
on graph segments of unbounded size. Even for deciding the identifiability of
intervention effects on variables that are close in time, no bound is known on
how many time steps in the past need to be considered. We give a first bound of
this kind that only depends on the number of variables per time step and the
maximum time lag of any direct or latent causal effect. More generally, we show
that applying the Causal Identification algorithm to a constant-size segment of
the time series graph is sufficient to decide identifiability of causal
effects, even across unbounded time intervals.

</details>


### [239] [AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning](https://arxiv.org/abs/2504.20187)
*Weihao Sun, Heeseung Bang, Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: Adherence-aware RL for lane-changing in semi-autonomous driving to improve travel efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance travel efficiency by addressing human driver partial compliance with lane-changing recommendations.

Method: Adherence-aware deep Q network within a Markov decision process framework, tested in CARLA.

Result: Evaluated in realistic driving scenarios using CARLA.

Conclusion: Proposed method effectively optimizes lane-changing recommendations considering human adherence.

Abstract: In this paper, we present an adherence-aware reinforcement learning (RL)
approach aimed at seeking optimal lane-changing recommendations within a
semi-autonomous driving environment to enhance a single vehicle's travel
efficiency. The problem is framed within a Markov decision process setting and
is addressed through an adherence-aware deep Q network, which takes into
account the partial compliance of human drivers with the recommended actions.
This approach is evaluated within CARLA's driving environment under realistic
scenarios.

</details>


### [240] [ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition](https://arxiv.org/abs/2504.20193)
*Zhe Cui, Shuxian Zhang, Kangzhi Lou, Le-Nam Tran*

Main category: cs.LG

TL;DR: ProFi-Net is a few-shot learning framework for WiFi-based gesture recognition, using prototype-based metric learning and feature-level attention to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of limited training data and sparse feature representations in WiFi-based gesture recognition.

Method: Employs prototype-based metric learning with feature-level attention and a curriculum-inspired data augmentation strategy on the query set.

Result: Outperforms conventional prototype networks and state-of-the-art methods in accuracy and training efficiency.

Conclusion: ProFi-Net is effective for few-shot WiFi gesture recognition, offering improved generalization and robustness.

Abstract: This paper presents ProFi-Net, a novel few-shot learning framework for
WiFi-based gesture recognition that overcomes the challenges of limited
training data and sparse feature representations. ProFi-Net employs a
prototype-based metric learning architecture enhanced with a feature-level
attention mechanism, which dynamically refines the Euclidean distance by
emphasizing the most discriminative feature dimensions. Additionally, our
approach introduces a curriculum-inspired data augmentation strategy
exclusively on the query set. By progressively incorporating Gaussian noise of
increasing magnitude, the model is exposed to a broader range of challenging
variations, thereby improving its generalization and robustness to overfitting.
Extensive experiments conducted across diverse real-world environments
demonstrate that ProFi-Net significantly outperforms conventional prototype
networks and other state-of-the-art few-shot learning methods in terms of
classification accuracy and training efficiency.

</details>


### [241] [Representation Learning on a Random Lattice](https://arxiv.org/abs/2504.20197)
*Aryeh Brill*

Main category: cs.LG

TL;DR: The paper proposes a geometric framework to interpret deep neural network features by modeling data distributions as random lattices and categorizing features into context, component, and surface types.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and reliability of deep neural networks by decomposing their learned representations into interpretable features.

Method: Adopts a geometric perspective, modeling data distributions as random lattices and analyzing them using percolation theory. Features are categorized into context, component, and surface types.

Result: The model aligns with recent mechanistic interpretability findings and provides a framework for understanding learned features.

Conclusion: The geometric approach offers insights into feature interpretability and suggests future research directions.

Abstract: Decomposing a deep neural network's learned representations into
interpretable features could greatly enhance its safety and reliability. To
better understand features, we adopt a geometric perspective, viewing them as a
learned coordinate system for mapping an embedded data distribution. We
motivate a model of a generic data distribution as a random lattice and analyze
its properties using percolation theory. Learned features are categorized into
context, component, and surface features. The model is qualitatively consistent
with recent findings in mechanistic interpretability and suggests directions
for future research.

</details>


### [242] [Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework](https://arxiv.org/abs/2504.20213)
*Yuan Xia, Akanksha Atrey, Fadoua Khmaissia, Kedar S. Namjoshi*

Main category: cs.LG

TL;DR: The paper explores LLMs' logical reasoning by training them to construct proofs in Boolean logic, using synthesized data and a technique called Template Transformation to improve performance.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can genuinely learn logical reasoning, focusing on Boolean logic proofs as a tractable yet challenging task.

Method: Uses a trained LLM to generate proofs from assumptions and goals, validated by an automated checker. Introduces Template Transformation for data augmentation and tests reasoning ability with specific measures.

Result: LLMs show strong reasoning for short proofs, but performance declines with complexity. Template Transformation boosts accuracy, even for smaller models.

Conclusion: LLMs can learn logical reasoning to some extent, with Template Transformation enhancing their capabilities across model sizes.

Abstract: This paper investigates the logical reasoning capabilities of large language
models (LLMs). For a precisely defined yet tractable formulation, we choose the
conceptually simple but technically complex task of constructing proofs in
Boolean logic. A trained LLM receives as input a set of assumptions and a goal,
and produces as output a proof that formally derives the goal from the
assumptions. Incorrect proofs are caught by an automated proof checker. A
critical obstacle for training is the scarcity of real-world proofs. We propose
an efficient, randomized procedure for synthesizing valid proofs and introduce
Template Transformation, a data augmentation technique that enhances the
model's ability to handle complex logical expressions. The central evaluation
question is whether an LLM has indeed learned to reason. We propose tests to
measure the reasoning ability of a black-box LLM. By these measures,
experiments demonstrate strong reasoning capabilities for assertions with short
proofs, which decline with proof complexity. Notably, template transformation
improves accuracy even for smaller models, suggesting its effectiveness across
model scales.

</details>


### [243] [Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena](https://arxiv.org/abs/2504.20249)
*W. Diab, M. Al-Kobaisi*

Main category: cs.LG

TL;DR: The paper introduces the Temporal Neural Operator (TNO), an efficient neural operator for time-dependent PDEs, addressing limitations of existing neural operators in temporal dynamics and training cost.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators (e.g., DeepONet, FNO) struggle with temporal dynamics of time-dependent PDEs and are costly to train, especially for higher-dimensional problems.

Method: TNO extends DeepONet with a temporal-branch, incorporating architectural improvements and training strategies like Markov assumption, teacher forcing, and temporal bundling.

Result: TNO demonstrates superior temporal extrapolation, robustness to error accumulation, resolution invariance, and flexibility with multiple input functions.

Conclusion: TNO is a promising solution for spatio-temporal operator learning in time-dependent PDEs, overcoming key limitations of existing neural operators.

Abstract: Neural Operators (NOs) are machine learning models designed to solve partial
differential equations (PDEs) by learning to map between function spaces.
Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier
Neural Operator (FNO) have demonstrated excellent generalization properties
when mapping between spatial function spaces. However, they struggle in mapping
the temporal dynamics of time-dependent PDEs, especially for time steps not
explicitly seen during training. This limits their temporal accuracy as they do
not leverage these dynamics in the training process. In addition, most NOs tend
to be prohibitively costly to train, especially for higher-dimensional PDEs. In
this paper, we propose the Temporal Neural Operator (TNO), an efficient neural
operator specifically designed for spatio-temporal operator learning for
time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the
DeepONet framework, leveraging the best architectural design choices from
several other NOs, and a combination of training strategies including Markov
assumption, teacher forcing, temporal bundling, and the flexibility to
condition the output on the current state or past states. Through extensive
benchmarking and an ablation study on a diverse set of example problems we
demonstrate the TNO long range temporal extrapolation capabilities, robustness
to error accumulation, resolution invariance, and flexibility to handle
multiple input functions.

</details>


### [244] [Financial Data Analysis with Robust Federated Logistic Regression](https://arxiv.org/abs/2504.20250)
*Kun Yang, Nikhil Krishnan, Sanjeev R. Kulkarni*

Main category: cs.LG

TL;DR: A robust federated logistic regression framework is proposed for financial data analysis, balancing privacy, interpretability, and outlier robustness, achieving performance comparable to centralized methods.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns in federated learning while ensuring model interpretability and robustness to outliers in financial data analysis.

Method: Proposes a robust federated logistic regression framework, evaluated on both IID and non-IID data, including outlier scenarios.

Result: Achieves comparable performance to centralized algorithms (Logistic Regression, Decision Tree, K-Nearest Neighbors) in binary and multi-class tasks.

Conclusion: The framework successfully balances privacy, interpretability, and robustness, proving effective in federated financial data analysis.

Abstract: In this study, we focus on the analysis of financial data in a federated
setting, wherein data is distributed across multiple clients or locations, and
the raw data never leaves the local devices. Our primary focus is not only on
the development of efficient learning frameworks (for protecting user data
privacy) in the field of federated learning but also on the importance of
designing models that are easier to interpret. In addition, we care about the
robustness of the framework to outliers. To achieve these goals, we propose a
robust federated logistic regression-based framework that strives to strike a
balance between these goals. To verify the feasibility of our proposed
framework, we carefully evaluate its performance not only on independently
identically distributed (IID) data but also on non-IID data, especially in
scenarios involving outliers. Extensive numerical results collected from
multiple public datasets demonstrate that our proposed method can achieve
comparable performance to those of classical centralized algorithms, such as
Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary
and multi-class classification tasks.

</details>


### [245] [Investigating task-specific prompts and sparse autoencoders for activation monitoring](https://arxiv.org/abs/2504.20271)
*Henk Tillman, Dan Mossing*

Main category: cs.LG

TL;DR: The paper explores methods for monitoring language model outputs using internal activations, comparing prompted probing and SAE-based probing, with recommendations based on compute constraints.


<details>
  <summary>Details</summary>
Motivation: Language models can behave unexpectedly, so monitoring their outputs is crucial. Internal activations encode useful information for this purpose.

Method: The study compares linear probing variants, including prompted probing (using test-time computation) and SAE-based probing (using train-time computation).

Result: Zero-shot prompting is a decent baseline, but activation probing outperforms it with sufficient data. Prompted probing excels with inference-time compute, while SAE-based probing is better for limited compute.

Conclusion: Prompted probing is recommended for available inference-time compute due to efficiency, while SAE-based probing is better for limited compute.

Abstract: Language models can behave in unexpected and unsafe ways, and so it is
valuable to monitor their outputs. Internal activations of language models
encode additional information that could be useful for this. The baseline
approach for activation monitoring is some variation of linear probing on a
particular layer: starting from a labeled dataset, train a logistic regression
classifier on that layer's activations. Recent work has proposed several
approaches which may improve on naive linear probing, by leveraging additional
computation. One class of techniques, which we call "prompted probing,"
leverages test time computation to improve monitoring by (1) prompting the
model with a description of the monitoring task, and (2) applying a learned
linear probe to resulting activations. Another class of techniques uses
computation at train time: training sparse autoencoders offline to identify an
interpretable basis for the activations, and e.g. max-pooling activations
across tokens using that basis before applying a linear probe. However, one can
also prompt the model with a description of the monitoring task and use its
output directly. We develop and test novel refinements of these methods and
compare them against each other. We find asking the model zero-shot is a
reasonable baseline when inference-time compute is not limited; however,
activation probing methods can substantially outperform this baseline given
sufficient training data. Specifically, we recommend prompted probing when
inference-time compute is available, due to its superior data efficiency and
good generalization performance. Alternatively, if inference-time compute is
limited, we find SAE-based probing methods outperform raw activation probing.

</details>


### [246] [Generative Diffusion Models for Resource Allocation in Wireless Networks](https://arxiv.org/abs/2504.20277)
*Yigit Berkay Uslu, Samar Hadou, Shirin Saeedi Bidokhti, Alejandro Ribeiro*

Main category: cs.LG

TL;DR: A supervised training algorithm using generative diffusion models (GDMs) is proposed for stochastic resource allocation, achieving near-optimal performance by imitating expert policies and generalizing via GNNs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning optimal stochastic resource allocation policies under ergodic utility and QoS constraints.

Method: Train a GDM to imitate expert policies and generate samples from the optimal distribution, using a GNN for generalization.

Result: Near-optimal performance demonstrated in power control for multi-user interference networks.

Conclusion: GDMs with GNNs effectively learn and generalize stochastic resource allocation policies.

Abstract: This paper proposes a supervised training algorithm for learning stochastic
resource allocation policies with generative diffusion models (GDMs). We
formulate the allocation problem as the maximization of an ergodic utility
function subject to ergodic Quality of Service (QoS) constraints. Given samples
from a stochastic expert policy that yields a near-optimal solution to the
problem, we train a GDM policy to imitate the expert and generate new samples
from the optimal distribution. We achieve near-optimal performance through
sequential execution of the generated samples. To enable generalization to a
family of network configurations, we parameterize the backward diffusion
process with a graph neural network (GNN) architecture. We present numerical
results in a case study of power control in multi-user interference networks.

</details>


### [247] [FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting](https://arxiv.org/abs/2504.20282)
*Michael A. Helcig, Stefan Nastic*

Main category: cs.LG

TL;DR: FedCCL is a federated learning framework addressing data heterogeneity and client variability by combining static clustering with asynchronous FedAvg, achieving high accuracy and privacy.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods struggle with heterogeneous data and dynamic client availability, leading to inefficiency and delayed model specialization.

Method: FedCCL uses static pre-training clustering and an asynchronous FedAvg algorithm with a three-tier model topology (global, cluster-specific, local).

Result: Achieves 3.93% energy prediction error in photovoltaic installations, with minimal performance degradation (0.14%) for new clients.

Conclusion: FedCCL effectively balances privacy, accuracy, and adaptability in distributed learning with dynamic participants.

Abstract: Privacy-preserving distributed model training is crucial for modern machine
learning applications, yet existing Federated Learning approaches struggle with
heterogeneous data distributions and varying computational capabilities.
Traditional solutions either treat all participants uniformly or require costly
dynamic clustering during training, leading to reduced efficiency and delayed
model specialization. We present FedCCL (Federated Clustered Continual
Learning), a framework specifically designed for environments with static
organizational characteristics but dynamic client availability. By combining
static pre-training clustering with an adapted asynchronous FedAvg algorithm,
FedCCL enables new clients to immediately profit from specialized models
without prior exposure to their data distribution, while maintaining reduced
coordination overhead and resilience to client disconnections. Our approach
implements an asynchronous Federated Learning protocol with a three-tier model
topology - global, cluster-specific, and local models - that efficiently
manages knowledge sharing across heterogeneous participants. Evaluation using
photovoltaic installations across central Europe demonstrates that FedCCL's
location-based clustering achieves an energy prediction error of 3.93%
(+-0.21%), while maintaining data privacy and showing that the framework
maintains stability for population-independent deployments, with 0.14
percentage point degradation in performance for new installations. The results
demonstrate that FedCCL offers an effective framework for privacy-preserving
distributed learning, maintaining high accuracy and adaptability even with
dynamic participant populations.

</details>


### [248] [Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results](https://arxiv.org/abs/2504.20293)
*Stefan Kober*

Main category: cs.LG

TL;DR: A geometric enhancement to k-means clustering merges overlapping clusters post-processing, improving performance on non-convex shapes and reducing sensitivity to the exact choice of k.


<details>
  <summary>Details</summary>
Motivation: Traditional k-means struggles with non-convex shapes and requires precise k. This work aims to relax the k requirement and improve shape handling.

Method: After standard k-means, assign radii to cluster centers and merge overlapping clusters. Supports recursive partitioning for scalability.

Result: The method achieves high accuracy on benchmarks with minimal extra computation, handling non-convex shapes effectively.

Conclusion: The proposed enhancement is a lightweight, scalable solution for k-means' limitations, suitable for distributed systems.

Abstract: Traditional k-means clustering underperforms on non-convex shapes and
requires the number of clusters k to be specified in advance. We propose a
simple geometric enhancement: after standard k-means, each cluster center is
assigned a radius (the distance to its farthest assigned point), and clusters
whose radii overlap are merged. This post-processing step loosens the
requirement for exact k: as long as k is overestimated (but not excessively),
the method can often reconstruct non-convex shapes through meaningful merges.
We also show that this approach supports recursive partitioning: clustering can
be performed independently on tiled regions of the feature space, then globally
merged, making the method scalable and suitable for distributed systems.
Implemented as a lightweight post-processing step atop scikit-learn's k-means,
the algorithm performs well on benchmark datasets, achieving high accuracy with
minimal additional computation.

</details>


### [249] [The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting](https://arxiv.org/abs/2504.20295)
*Mohammadhossein Homaei, Victor Gonzalez Morales, Oscar Mogollon-Gutierrez, Andres Caro*

Main category: cs.LG

TL;DR: A DT platform for water supply networks uses LSTM for water consumption prediction but is vulnerable to adversarial attacks like FGSM and PGD, worsened by a Learning Automata-based approach, raising MAPE from 26% to 35%. Robust defenses are urgently needed.


<details>
  <summary>Details</summary>
Motivation: To highlight vulnerabilities in AI-driven digital twins for water distribution systems and demonstrate the impact of adversarial attacks on prediction accuracy.

Method: Uses LSTM networks for water consumption prediction and introduces Learning Automata (LA) and Random LA-based adversarial attacks to test model robustness.

Result: Adversarial attacks degrade forecasting accuracy, increasing MAPE from 26% to over 35%, with adaptive strategies amplifying the effect.

Conclusion: The study underscores the need for robust defenses like adversarial training and anomaly detection to secure AI-driven digital twins.

Abstract: Digital twins (DTs) are improving water distribution systems by using
real-time data, analytics, and prediction models to optimize operations. This
paper presents a DT platform designed for a Spanish water supply network,
utilizing Long Short-Term Memory (LSTM) networks to predict water consumption.
However, machine learning models are vulnerable to adversarial attacks, such as
the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD).
These attacks manipulate critical model parameters, injecting subtle
distortions that degrade forecasting accuracy. To further exploit these
vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based
approach that dynamically adjusts perturbations, making adversarial attacks
more difficult to detect. Experimental results show that this approach
significantly impacts prediction reliability, causing the Mean Absolute
Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack
strategies amplify this effect, highlighting cybersecurity risks in AI-driven
DTs. These findings emphasize the urgent need for robust defenses, including
adversarial training, anomaly detection, and secure data pipelines.

</details>


### [250] [FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization](https://arxiv.org/abs/2504.20307)
*Hui Chen, Xuhui Fan, Zhangkai Wu, Longbing Cao*

Main category: cs.LG

TL;DR: FigBO is a generalized acquisition function for Bayesian optimization that enhances myopic methods by incorporating future impact, improving performance and convergence.


<details>
  <summary>Details</summary>
Motivation: Myopic acquisition functions lack look-ahead capability, limiting their effectiveness in Bayesian optimization.

Method: Proposes FigBO, a plug-and-play method integrating future impact on global information gain with existing myopic functions.

Result: Theoretically and empirically, FigBO outperforms standard methods like EI, achieving faster convergence and state-of-the-art performance.

Conclusion: FigBO addresses the limitations of myopic acquisition functions, offering a scalable and effective solution for Bayesian optimization.

Abstract: Bayesian optimization is a powerful technique for optimizing
expensive-to-evaluate black-box functions, consisting of two main components: a
surrogate model and an acquisition function. In recent years, myopic
acquisition functions have been widely adopted for their simplicity and
effectiveness. However, their lack of look-ahead capability limits their
performance. To address this limitation, we propose FigBO, a generalized
acquisition function that incorporates the future impact of candidate points on
global information gain. FigBO is a plug-and-play method that can integrate
seamlessly with most existing myopic acquisition functions. Theoretically, we
analyze the regret bound and convergence rate of FigBO when combined with the
myopic base acquisition function expected improvement (EI), comparing them to
those of standard EI. Empirically, extensive experimental results across
diverse tasks demonstrate that FigBO achieves state-of-the-art performance and
significantly faster convergence compared to existing methods.

</details>


### [251] [A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning](https://arxiv.org/abs/2504.20310)
*Greg Gluch, Shafi Goldwasser*

Main category: cs.LG

TL;DR: The paper explores cryptographic defenses against adversarial inputs in ML, defining detection (DbD) and mitigation (DbM) strategies. It shows equivalence for classification tasks but a separation for generative tasks, proving DbM is possible where DbD is not under certain cryptographic assumptions.


<details>
  <summary>Details</summary>
Motivation: To theoretically study adversarial input defenses in ML, focusing on detection versus mitigation during inference time.

Method: Formal definitions of DbD and DbM via a 3-round protocol between a defender and attacker, with correctness, completeness, and soundness properties. Theoretical analysis of equivalence (classification) and separation (generative tasks) under cryptographic assumptions.

Result: DbD and DbM are equivalent for classification but separable for generative tasks, with DbM provably possible where DbD is not under specific cryptographic conditions.

Conclusion: The study highlights the nuanced effectiveness of DbD and DbM, especially in generative learning, and underscores the impact of cryptographic assumptions on defense feasibility.

Abstract: In this paper, we initiate a cryptographically inspired theoretical study of
detection versus mitigation of adversarial inputs produced by attackers of
Machine Learning algorithms during inference time.
  We formally define defense by detection (DbD) and defense by mitigation
(DbM). Our definitions come in the form of a 3-round protocol between two
resource-bounded parties: a trainer/defender and an attacker. The attacker aims
to produce inference-time inputs that fool the training algorithm. We define
correctness, completeness, and soundness properties to capture successful
defense at inference time while not degrading (too much) the performance of the
algorithm on inputs from the training distribution.
  We first show that achieving DbD and achieving DbM are equivalent for ML
classification tasks. Surprisingly, this is not the case for ML generative
learning tasks, where there are many possible correct outputs that can be
generated for each input. We show a separation between DbD and DbM by
exhibiting a generative learning task for which is possible to defend by
mitigation but is provably impossible to defend by detection under the
assumption that the Identity-Based Fully Homomorphic Encryption (IB-FHE),
publicly-verifiable zero-knowledge Succinct Non-Interactive Arguments of
Knowledge (zk-SNARK) and Strongly Unforgeable Signatures exist. The mitigation
phase uses significantly fewer samples than the initial training algorithm.

</details>


### [252] [Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training](https://arxiv.org/abs/2504.20314)
*Qitao Tan, Sung-En Chang, Rui Xia, Huidong Ji, Chence Yang, Ci Zhang, Jun Liu, Zheng Zhan, Zhou Zou, Yanzhi Wang, Jin Lu, Geng Yuan*

Main category: cs.LG

TL;DR: PeZO is a perturbation-efficient zeroth-order optimization framework that reduces hardware challenges by reusing random numbers and replacing Gaussian with uniform distributions, enabling feasible on-device DNN training.


<details>
  <summary>Details</summary>
Motivation: The mismatch between ZO optimization's need for Gaussian random numbers and hardware limitations (FPGAs, ASICs) makes it infeasible for on-device training.

Method: PeZO introduces random number reuse and a hardware-friendly adaptive scaling method using uniform distributions instead of Gaussian.

Result: PeZO reduces LUTs and FFs by 48.6% and 12.7%, saves up to 86% power, and maintains training performance.

Conclusion: PeZO makes ZO optimization feasible for on-device training, offering insights for future research in this area.

Abstract: Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)
training paradigm that offers computational simplicity and memory savings.
However, this seemingly promising approach faces a significant and long-ignored
challenge. ZO requires generating a substantial number of Gaussian random
numbers, which poses significant difficulties and even makes it infeasible for
hardware platforms, such as FPGAs and ASICs. In this paper, we identify this
critical issue, which arises from the mismatch between algorithm and hardware
designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO
framework. Specifically, we design random number reuse strategies to
significantly reduce the demand for random number generation and introduce a
hardware-friendly adaptive scaling method to replace the costly Gaussian
distribution with a uniform distribution. Our experiments show that PeZO
reduces the required LUTs and FFs for random number generation by 48.6\% and
12.7\%, and saves at maximum 86\% power consumption, all without compromising
training performance, making ZO optimization feasible for on-device training.
To the best of our knowledge, we are the first to explore the potential of
on-device ZO optimization, providing valuable insights for future research.

</details>


### [253] [Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach](https://arxiv.org/abs/2504.20319)
*Huchen Yang, Xinghao Dong, Jin-Long Wu*

Main category: cs.LG

TL;DR: A hybrid Bayesian experimental design (BED) framework using auto-differentiable ensemble Kalman inversion (AD-EKI) is proposed to address model discrepancy challenges, enabling efficient design optimization and robust parameter inference.


<details>
  <summary>Details</summary>
Motivation: Model discrepancy in BED leads to biased parameter estimates, and high-dimensional parameter spaces complicate Bayesian updating and design optimization.

Method: The hybrid framework combines AD-EKI for high-dimensional model discrepancy with standard BED for low-dimensional physical parameters, using gradient-free utility evaluation and gradient-based optimization.

Result: The method efficiently identifies optimal designs for model discrepancy calibration and robustly infers physical parameters in a convection-diffusion example.

Conclusion: AD-EKI not only solves BED challenges but also has broader applications in bilevel optimization problems like meta-learning and structure optimization.

Abstract: Bayesian experimental design (BED) offers a principled framework for
optimizing data acquisition by leveraging probabilistic inference. However,
practical implementations of BED are often compromised by model discrepancy,
i.e., the mismatch between predictive models and true physical systems, which
can potentially lead to biased parameter estimates. While data-driven
approaches have been recently explored to characterize the model discrepancy,
the resulting high-dimensional parameter space poses severe challenges for both
Bayesian updating and design optimization. In this work, we propose a hybrid
BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI)
that addresses these challenges by providing a computationally efficient,
gradient-free alternative to estimate the information gain for high-dimensional
network parameters. The AD-EKI allows a differentiable evaluation of the
utility function in BED and thus facilitates the use of standard gradient-based
methods for design optimization. In the proposed hybrid framework, we
iteratively optimize experimental designs, decoupling the inference of
low-dimensional physical parameters handled by standard BED methods, from the
high-dimensional model discrepancy handled by AD-EKI. The identified optimal
designs for the model discrepancy enable us to systematically collect
informative data for its calibration. The performance of the proposed method is
studied by a classical convection-diffusion BED example, and the hybrid
framework enabled by AD-EKI efficiently identifies informative data to
calibrate the model discrepancy and robustly infers the unknown physical
parameters in the modeled system. Besides addressing the challenges of BED with
model discrepancy, AD-EKI also potentially fosters efficient and scalable
frameworks in many other areas with bilevel optimization, such as meta-learning
and structure optimization.

</details>


### [254] [Generative Learning for Slow Manifolds and Bifurcation Diagrams](https://arxiv.org/abs/2504.20375)
*Ellis R. Crabtree, Dimitris G. Giovanis, Nikolaos Evangelou, Juan M. Bello-Rivas, Ioannis G. Kevrekidis*

Main category: cs.LG

TL;DR: The paper proposes using conditional score-based generative models (cSGMs) to initialize on slow manifolds and approximate steady states in bifurcation diagrams, leveraging machine learning for efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently sampling slow manifolds and bifurcation diagrams in dynamical systems, the paper explores the potential of cSGMs for conditional data generation.

Method: The framework employs cSGMs to (a) initialize on slow manifolds consistent with desired quantities of interest and (b) approximate steady states for new parameter values in bifurcation diagrams.

Result: The approach enables efficient sampling of slow manifolds and bifurcation diagrams, aiding in uncovering their geometry and filling missing segments.

Conclusion: The integration of cSGMs offers a promising tool for model reduction and numerical nonlinear dynamics, enhancing traditional methods with machine learning capabilities.

Abstract: In dynamical systems characterized by separation of time scales, the
approximation of so called ``slow manifolds'', on which the long term dynamics
lie, is a useful step for model reduction. Initializing on such slow manifolds
is a useful step in modeling, since it circumvents fast transients, and is
crucial in multiscale algorithms alternating between fine scale (fast) and
coarser scale (slow) simulations. In a similar spirit, when one studies the
infinite time dynamics of systems depending on parameters, the system
attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling
these manifolds gives us representative attractors (here, steady states of ODEs
or PDEs) at different parameter values. Algorithms for the systematic
construction of these manifolds are required parts of the ``traditional''
numerical nonlinear dynamics toolkit.
  In more recent years, as the field of Machine Learning develops, conditional
score-based generative models (cSGMs) have demonstrated capabilities in
generating plausible data from target distributions that are conditioned on
some given label. It is tempting to exploit such generative models to produce
samples of data distributions conditioned on some quantity of interest (QoI).
In this work, we present a framework for using cSGMs to quickly (a) initialize
on a low-dimensional (reduced-order) slow manifold of a multi-time-scale system
consistent with desired value(s) of a QoI (a ``label'') on the manifold, and
(b) approximate steady states in a bifurcation diagram consistent with a (new,
out-of-sample) parameter value. This conditional sampling can help uncover the
geometry of the reduced slow-manifold and/or approximately ``fill in'' missing
segments of steady states in a bifurcation diagram.

</details>


### [255] [Manifold Clustering with Schatten p-norm Maximization](https://arxiv.org/abs/2504.20390)
*Fangfang Li, Quanxue Gao*

Main category: cs.LG

TL;DR: A new clustering framework combines K-means and manifold learning, ensuring consistency between data structure and labels, and maintains class balance using Schatten p-norm.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook consistency between data structure and labels, and lack class balance in clustering.

Method: Fuses K-means and manifold learning, using labels to guide the manifold structure and Schatten p-norm for class balance.

Result: Superior performance confirmed by experiments on multiple databases.

Conclusion: The proposed framework is flexible, efficient, and outperforms existing methods.

Abstract: Manifold clustering, with its exceptional ability to capture complex data
structures, holds a pivotal position in cluster analysis. However, existing
methods often focus only on finding the optimal combination between K-means and
manifold learning, and overlooking the consistency between the data structure
and labels. To address this issue, we deeply explore the relationship between
K-means and manifold learning, and on this basis, fuse them to develop a new
clustering framework. Specifically, the algorithm uses labels to guide the
manifold structure and perform clustering on it, which ensures the consistency
between the data structure and labels. Furthermore, in order to naturally
maintain the class balance in the clustering process, we maximize the Schatten
p-norm of labels, and provide a theoretical proof to support this.
Additionally, our clustering framework is designed to be flexible and
compatible with many types of distance functions, which facilitates efficient
processing of nonlinear separable data. The experimental results of several
databases confirm the superiority of our proposed model.

</details>


### [256] [FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation](https://arxiv.org/abs/2504.20408)
*Jae Yong Lee, Gwang Jae Jung, Byung Chan Lim, Hyung Ju Hwang*

Main category: cs.LG

TL;DR: FourierSpecNet combines Fourier spectral methods with deep learning to efficiently approximate the Boltzmann equation's collision operator, achieving resolution-invariant learning and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: The Boltzmann equation's numerical solution is computationally demanding, especially for inelastic collisions and high-dimensional velocity domains, necessitating a more efficient approach.

Method: The proposed Fourier Neural Spectral Network (FourierSpecNet) integrates Fourier spectral methods with deep learning to approximate the collision operator in Fourier space.

Result: FourierSpecNet achieves competitive accuracy, supports zero-shot super-resolution, and reduces computational costs compared to traditional spectral solvers.

Conclusion: FourierSpecNet provides a robust and scalable solution for the Boltzmann equation, applicable to both elastic and inelastic regimes.

Abstract: The Boltzmann equation, a fundamental model in kinetic theory, describes the
evolution of particle distribution functions through a nonlinear,
high-dimensional collision operator. However, its numerical solution remains
computationally demanding, particularly for inelastic collisions and
high-dimensional velocity domains. In this work, we propose the Fourier Neural
Spectral Network (FourierSpecNet), a hybrid framework that integrates the
Fourier spectral method with deep learning to approximate the collision
operator in Fourier space efficiently. FourierSpecNet achieves
resolution-invariant learning and supports zero-shot super-resolution, enabling
accurate predictions at unseen resolutions without retraining. Beyond empirical
validation, we establish a consistency result showing that the trained operator
converges to the spectral solution as the discretization is refined. We
evaluate our method on several benchmark cases, including Maxwellian and
hard-sphere molecular models, as well as inelastic collision scenarios. The
results demonstrate that FourierSpecNet offers competitive accuracy while
significantly reducing computational cost compared to traditional spectral
solvers. Our approach provides a robust and scalable alternative for solving
the Boltzmann equation across both elastic and inelastic regimes.

</details>


### [257] [ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes](https://arxiv.org/abs/2504.20411)
*Amartya Mukherjee, Ruizhi Deng, He Zhao, Yuzhen Mao, Leonid Sigal, Frederick Tung*

Main category: cs.LG

TL;DR: A novel diffusion model with asynchronous noise schedules improves temporal point process modeling, achieving state-of-the-art results in event prediction and flexibility in forecasting settings.


<details>
  <summary>Details</summary>
Motivation: To enhance temporal point process modeling by introducing asynchronous noise schedules for better conditioning and forecasting of distant future events.

Method: Uses diffusion models with asynchronous noise schedules, training via conditional flow matching, and models joint distributions of event sequences.

Result: Achieves state-of-the-art performance in predicting inter-event times and event types, with superior long-horizon prediction results.

Conclusion: The method effectively improves forecasting accuracy and flexibility, outperforming existing baselines in temporal event prediction.

Abstract: This work introduces a novel approach to modeling temporal point processes
using diffusion models with an asynchronous noise schedule. At each step of the
diffusion process, the noise schedule injects noise of varying scales into
different parts of the data. With a careful design of the noise schedules,
earlier events are generated faster than later ones, thus providing stronger
conditioning for forecasting the more distant future. We derive an objective to
effectively train these models for a general family of noise schedules based on
conditional flow matching. Our method models the joint distribution of the
latent representations of events in a sequence and achieves state-of-the-art
results in predicting both the next inter-event time and event type on
benchmark datasets. Additionally, it flexibly accommodates varying lengths of
observation and prediction windows in different forecasting settings by
adjusting the starting and ending points of the generation process. Finally,
our method shows superior performance in long-horizon prediction tasks,
outperforming existing baseline methods.

</details>


### [258] [Understanding GNNs and Homophily in Dynamic Node Classification](https://arxiv.org/abs/2504.20421)
*Michael Ito, Danai Koutra, Jenna Wiens*

Main category: cs.LG

TL;DR: The paper introduces dynamic homophily, a new measure for analyzing GNN performance in dynamic graphs, showing current GCNs struggle with low dynamic homophily.


<details>
  <summary>Details</summary>
Motivation: Existing homophily measures are limited to static graphs, leaving a gap in understanding GNN performance in dynamic settings.

Method: Theoretical analysis of GCNs in dynamic settings, proposing dynamic homophily as a new measure, validated on dynamic node classification datasets.

Result: Dynamic homophily correlates with GNN performance, revealing limitations of current GCNs in low dynamic homophily scenarios.

Conclusion: The work advances understanding of homophily in dynamic graphs, guiding future GNN design for better performance.

Abstract: Homophily, as a measure, has been critical to increasing our understanding of
graph neural networks (GNNs). However, to date this measure has only been
analyzed in the context of static graphs. In our work, we explore homophily in
dynamic settings. Focusing on graph convolutional networks (GCNs), we
demonstrate theoretically that in dynamic settings, current GCN discriminative
performance is characterized by the probability that a node's future label is
the same as its neighbors' current labels. Based on this insight, we propose
dynamic homophily, a new measure of homophily that applies in the dynamic
setting. This new measure correlates with GNN discriminative performance and
sheds light on how to potentially design more powerful GNNs for dynamic graphs.
Leveraging a variety of dynamic node classification datasets, we demonstrate
that popular GNNs are not robust to low dynamic homophily. Going forward, our
work represents an important step towards understanding homophily and GNN
performance in dynamic node classification.

</details>


### [259] [Learning Laplacian Positional Encodings for Heterophilous Graphs](https://arxiv.org/abs/2504.20430)
*Michael Ito, Jiong Zhu, Dexiong Chen, Danai Koutra, Jenna Wiens*

Main category: cs.LG

TL;DR: Current graph positional encodings (PEs) may harm performance in heterophilous graphs. The proposed Learnable Laplacian Positional Encodings (LLPE) improves accuracy by leveraging the full Laplacian spectrum.


<details>
  <summary>Details</summary>
Motivation: Existing PEs fail in heterophilous graphs, which are common in real-world networks.

Method: Proposes LLPE, utilizing the full spectrum of the graph Laplacian to capture both homophilous and heterophilous structures.

Result: LLPE improves accuracy by up to 35% on synthetic and 14% on real-world graphs across various GNNs.

Conclusion: LLPE advances PEs for heterophilous graphs, demonstrating significant performance gains.

Abstract: In this work, we theoretically demonstrate that current graph positional
encodings (PEs) are not beneficial and could potentially hurt performance in
tasks involving heterophilous graphs, where nodes that are close tend to have
different labels. This limitation is critical as many real-world networks
exhibit heterophily, and even highly homophilous graphs can contain local
regions of strong heterophily. To address this limitation, we propose Learnable
Laplacian Positional Encodings (LLPE), a new PE that leverages the full
spectrum of the graph Laplacian, enabling them to capture graph structure on
both homophilous and heterophilous graphs. Theoretically, we prove LLPE's
ability to approximate a general class of graph distances and demonstrate its
generalization properties. Empirically, our evaluation on 12 benchmarks
demonstrates that LLPE improves accuracy across a variety of GNNs, including
graph transformers, by up to 35% and 14% on synthetic and real-world graphs,
respectively. Going forward, our work represents a significant step towards
developing PEs that effectively capture complex structures in heterophilous
graphs.

</details>


### [260] [GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2504.20437)
*DiJia Su, Andrew Gu, Jane Xu, Yuandong Tian, Jiawei Zhao*

Main category: cs.LG

TL;DR: GaLore 2 improves upon GaLore by addressing computational overhead and scalability issues, enabling efficient large-scale LLM pre-training.


<details>
  <summary>Details</summary>
Motivation: Memory bottlenecks in LLM training and limitations of GaLore (e.g., SVD overhead, parallelization challenges) motivate the development of GaLore 2.

Method: GaLore 2 leverages low-rank gradient structures, integrates advancements like low-bit quantization, and optimizes for scalability and efficiency.

Result: GaLore 2 successfully pre-trains Llama 7B on 500 billion tokens, demonstrating scalability and performance.

Conclusion: GaLore 2 is a scalable and efficient solution for LLM pre-training, addressing key challenges of its predecessor.

Abstract: Large language models (LLMs) have revolutionized natural language
understanding and generation but face significant memory bottlenecks during
training. GaLore, Gradient Low-Rank Projection, addresses this issue by
leveraging the inherent low-rank structure of weight gradients, enabling
substantial memory savings without sacrificing performance. Recent works
further extend GaLore from various aspects, including low-bit quantization and
higher-order tensor structures. However, there are several remaining challenges
for GaLore, such as the computational overhead of SVD for subspace updates and
the integration with state-of-the-art training parallelization strategies
(e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable
GaLore framework that addresses these challenges and incorporates recent
advancements. In addition, we demonstrate the scalability of GaLore 2 by
pre-training Llama 7B from scratch using up to 500 billion training tokens,
highlighting its potential impact on real LLM pre-training scenarios.

</details>


### [261] [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
*Gabe Guo, Stefano Ermon*

Main category: cs.LG

TL;DR: AS-ARMs enable parallel token generation while maintaining correct joint distributions, outperforming discrete diffusion models and matching larger models in performance.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of parallel token generation in language models without compromising distribution accuracy.

Method: Propose Any-Subset Autoregressive Models (AS-ARMs) and Any-Subset Speculative Decoding (ASSD) for parallelized joint probability density estimation.

Result: ASSD speeds up generation without quality loss; AS-ARMs achieve state-of-the-art performance in sub-200M parameter models.

Conclusion: AS-ARMs are a promising direction for language modeling, combining efficiency and accuracy.

Abstract: In arbitrary-order language models, it is an open question how to sample
tokens in parallel from the correct joint distribution. With discrete diffusion
models, the more tokens they generate in parallel, the less their predicted
distributions adhere to the originally learned data distribution, as they rely
on a conditional independence assumption that only works with infinitesimally
small timesteps. We find that a different class of models, any-subset
autoregressive models (AS-ARMs), holds the solution. As implied by the name,
AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs
support parallelized joint probability density estimation, allowing them to
correct their own parallel-generated token distributions, via our Any-Subset
Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of
tokens from the correct joint distribution, with the number of neural network
calls upper bounded by the number of tokens predicted. We empirically verify
that ASSD speeds up language generation, without sacrificing quality.
Furthermore, we provide a mathematically justified scheme for training AS-ARMs
for generation, and show that AS-ARMs achieve state-of-the-art performance
among sub-200M parameter models on infilling benchmark tasks, and nearly match
the performance of models 50X larger on code generation. Our theoretical and
empirical results indicate that the once-forgotten AS-ARMs are a promising
direction of language modeling.

</details>


### [262] [Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework](https://arxiv.org/abs/2504.20442)
*Yuchen Wang, Pengfei Jia, Zhitao Shu, Keyan Liu, Abdul Rashid Mohamed Shariff*

Main category: cs.LG

TL;DR: A CNN-LSTM hybrid model improves precipitation prediction accuracy, achieving an RMSE of 6.752, but faces computational challenges with large datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate precipitation prediction is vital for disaster prevention, agriculture, and urban planning amid climate change.

Method: Uses a CNN-LSTM framework to analyze 31 years of monthly precipitation data from Pune, India, capturing local and long-term patterns.

Result: The model outperforms traditional methods with an RMSE of 6.752, showing better accuracy and generalization.

Conclusion: While effective, the model needs optimization for large datasets and multidimensional data, suggesting future improvements.

Abstract: With the intensification of global climate change, accurate prediction of
weather indicators is of great significance in disaster prevention and
mitigation, agricultural production, and transportation. Precipitation, as one
of the key meteorological indicators, plays a crucial role in water resource
management, agricultural production, and urban flood control. This study
proposes a multidimensional precipitation index prediction model based on a
CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation
forecasts. The dataset is sourced from Pune, Maharashtra, India, covering
monthly mean precipitation data from 1972 to 2002. This dataset includes nearly
31 years (1972-2002) of monthly average precipitation, reflecting the long-term
fluctuations and seasonal variations of precipitation in the region. By
analyzing these time series data, the CNN-LSTM model effectively captures local
features and long-term dependencies. Experimental results show that the model
achieves a root mean square error (RMSE) of 6.752, which demonstrates a
significant advantage over traditional time series prediction methods in terms
of prediction accuracy and generalization ability. Furthermore, this study
provides new research ideas for precipitation prediction. However, the model
requires high computational resources when dealing with large-scale datasets,
and its predictive ability for multidimensional precipitation data still needs
improvement. Future research could extend the model to support and predict
multidimensional precipitation data, thereby promoting the development of more
accurate and efficient meteorological prediction technologies.

</details>


### [263] [FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks](https://arxiv.org/abs/2504.20446)
*Wenjing Xiao, Wenhao Song, Miaojiang Chen, Ruikun Luo, Min Chen*

Main category: cs.LG

TL;DR: FT-MoE, a mixture-of-experts model, improves fault-tolerant computing by decoupling dependencies and using dual networks for fault detection and classification, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based fault-tolerant algorithms struggle with heterogeneous fault knowledge and complex time-series log dependencies, limiting detection performance.

Method: FT-MoE uses decoder-based transformers for fault prototypes, dual mixture-of-experts networks for prediction, and a two-stage optimization scheme (offline training and online tuning).

Result: FT-MoE outperforms state-of-the-art methods in fault detection and classification, verified by extensive experiments.

Conclusion: FT-MoE offers a sustainable-learning approach for reliable fault-tolerant computing, adaptable to dynamic environments.

Abstract: Intelligent fault-tolerant (FT) computing has recently demonstrated
significant advantages of predicting and diagnosing faults in advance, enabling
reliable service delivery. However, due to heterogeneity of fault knowledge and
complex dependence relationships of time series log data, existing deep
learning-based FT algorithms further improve detection performance relying on
single neural network model with difficulty. To this end, we propose FT-MoE, a
sustainable-learning mixture-of-experts model for fault-tolerant computing with
multiple tasks, which enables different parameters learning distinct fault
knowledge to achieve high-reliability for service system. Firstly, we use
decoder-based transformer models to obtain fault prototype vectors of
decoupling long-distance dependencies. Followed by, we present a dual mixture
of experts networks for high-accurate prediction for both fault detection and
classification tasks. Then, we design a two-stage optimization scheme of
offline training and online tuning, which allows that in operation FT-MoE can
also keep learning to adapt to dynamic service environments. Finally, to verify
the effectiveness of FT-MoE, we conduct extensive experiments on the FT
benchmark. Experimental results show that FT-MoE achieves superior performance
compared to the state-of-the-art methods. Code will be available upon
publication.

</details>


### [264] [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
*Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen*

Main category: cs.LG

TL;DR: 1-shot RLVR significantly improves math reasoning in LLMs using just one training example, matching performance of larger datasets and showing cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency of reinforcement learning with verifiable reward (RLVR) in enhancing math reasoning capabilities of LLMs with minimal training data.

Method: Applied 1-shot RLVR to models like Qwen2.5-Math-1.5B, using single examples and RL algorithms (GRPO, PPO), with exploration promoted via entropy loss.

Result: Performance on MATH500 rose from 36.0% to 73.6%, and average across benchmarks improved from 17.6% to 35.7%. Entropy loss alone boosted performance by 27.4%.

Conclusion: 1-shot RLVR is highly effective, with policy gradient loss being key. Findings encourage re-evaluating RLVR mechanisms and data efficiency.

Abstract: We show that reinforcement learning with verifiable reward using one training
example (1-shot RLVR) is effective in incentivizing the math reasoning
capabilities of large language models (LLMs). Applying RLVR to the base model
Qwen2.5-Math-1.5B, we identify a single example that elevates model performance
on MATH500 from 36.0% to 73.6%, and improves the average performance across six
common mathematical reasoning benchmarks from 17.6% to 35.7%. This result
matches the performance obtained using the 1.2k DeepScaleR subset (MATH500:
73.6%, average: 35.9%), which includes the aforementioned example. Similar
substantial improvements are observed across various models (Qwen2.5-Math-7B,
Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and
PPO), and different math examples (many of which yield approximately 30% or
greater improvement on MATH500 when employed as a single training example). In
addition, we identify some interesting phenomena during 1-shot RLVR, including
cross-domain generalization, increased frequency of self-reflection, and
sustained test performance improvement even after the training accuracy has
saturated, a phenomenon we term post-saturation generalization. Moreover, we
verify that the effectiveness of 1-shot RLVR primarily arises from the policy
gradient loss, distinguishing it from the "grokking" phenomenon. We also show
the critical role of promoting exploration (e.g., by adding entropy loss with
an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe
that applying entropy loss alone, without any outcome reward, significantly
enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings
can inspire future work on RLVR data efficiency and encourage a re-examination
of both recent progress and the underlying mechanisms in RLVR. Our code, model,
and data are open source at https://github.com/ypwang61/One-Shot-RLVR

</details>


### [265] [The Estimation of Continual Causal Effect for Dataset Shifting Streams](https://arxiv.org/abs/2504.20471)
*Baining Chen, Yiming Zhang, Yuqiao Han, Ruyue Zhang, Ruihuan Du, Zhishuo Zhou, Zhengdan Zhu, Xun Liu, Jiecheng Guo*

Main category: cs.LG

TL;DR: The paper proposes ICE-PKD, a framework for causal effect estimation in marketing, addressing temporal dataset shifts with incremental training and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: To improve marketing optimization by handling temporal dataset shifts caused by changing user behavior and domain distributions.

Method: ICE-PKD includes a multi-treatment uplift network for bias elimination and an incremental training strategy with replay-based knowledge distillation.

Result: Experiments show ICE-PKD outperforms existing methods, and it's successfully deployed in a ride-hailing platform.

Conclusion: ICE-PKD effectively addresses temporal dataset shifts, enhancing causal effect estimation in dynamic marketing environments.

Abstract: Causal effect estimation has been widely used in marketing optimization. The
framework of an uplift model followed by a constrained optimization algorithm
is popular in practice. To enhance performance in the online environment, the
framework needs to be improved to address the complexities caused by temporal
dataset shift. This paper focuses on capturing the dataset shift from user
behavior and domain distribution changing over time. We propose an Incremental
Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle
this challenge. The ICE-PKD framework includes two components: (i) a
multi-treatment uplift network that eliminates confounding bias using
counterfactual regression; (ii) an incremental training strategy that adapts to
the temporal dataset shift by updating with the latest data and protects
generalization via replay-based knowledge distillation. We also revisit the
uplift modeling metrics and introduce a novel metric for more precise online
evaluation in multiple treatment scenarios. Extensive experiments on both
simulated and online datasets show that the proposed framework achieves better
performance. The ICE-PKD framework has been deployed in the marketing system of
Huaxiaozhu, a ride-hailing platform in China.

</details>


### [266] [Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias](https://arxiv.org/abs/2504.20482)
*Chao Li, Changhua Zhou, Jia Chen*

Main category: cs.LG

TL;DR: GRKD distills teacher knowledge by learning relative class rankings instead of absolute probabilities, improving generalization, especially in fine-grained tasks.


<details>
  <summary>Details</summary>
Motivation: Existing distillation methods focus on absolute probabilities, missing relational inductive biases in teacher predictions, leading to exposure bias.

Method: Proposes Group Relative Knowledge Distillation (GRKD) with a group relative loss to preserve pairwise preference orderings from teacher outputs.

Result: GRKD outperforms existing methods on classification benchmarks, particularly in fine-grained tasks.

Conclusion: GRKD offers a new perspective by emphasizing relational structure over absolute likelihood in knowledge distillation.

Abstract: Knowledge distillation typically transfers knowledge from a teacher model to
a student model by minimizing differences between their output distributions.
However, existing distillation approaches largely focus on mimicking absolute
probabilities and neglect the valuable relational inductive biases embedded in
the teacher's relative predictions, leading to exposure bias. In this paper, we
propose Group Relative Knowledge Distillation (GRKD), a novel framework that
distills teacher knowledge by learning the relative ranking among classes,
rather than directly fitting the absolute distribution. Specifically, we
introduce a group relative loss that encourages the student model to preserve
the pairwise preference orderings provided by the teacher's outputs. Extensive
experiments on classification benchmarks demonstrate that GRKD achieves
superior generalization compared to existing methods, especially in tasks
requiring fine-grained class differentiation. Our method provides a new
perspective on exploiting teacher knowledge, focusing on relational structure
rather than absolute likelihood.

</details>


### [267] [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
*Zhengfu He, Junxuan Wang, Rui Lin, Xuyang Ge, Wentao Shu, Qiong Tang, Junping Zhang, Xipeng Qiu*

Main category: cs.LG

TL;DR: Lorsa is a sparse replacement for Transformer attention layers, disentangling MHSA into interpretable components, achieving parity with SAE in interpretability and excelling in circuit discovery.


<details>
  <summary>Details</summary>
Motivation: To address attention superposition and better understand feature interactions in Transformer attention layers.

Method: Proposes Low-Rank Sparse Attention (Lorsa) to decompose MHSA into finer-grained, interpretable components.

Result: Lorsa discovers cleaner MHSA behaviors (e.g., induction heads) and arithmetic-specific heads, matching SAE in interpretability but excelling in circuit discovery.

Conclusion: Lorsa is effective for interpretability and circuit discovery, with potential for broader applications in Transformer analysis.

Abstract: We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of
Transformer attention layers to disentangle original Multi Head Self Attention
(MHSA) into individually comprehensible components. Lorsa is designed to
address the challenge of attention superposition to understand
attention-mediated interaction between features in different token positions.
We show that Lorsa heads find cleaner and finer-grained versions of previously
discovered MHSA behaviors like induction heads, successor heads and attention
sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse
Autoencoder (SAE) are both sparse dictionary learning methods applied to
different Transformer components, and lead to consistent findings in many ways.
For instance, we discover a comprehensive family of arithmetic-specific Lorsa
heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated
interpretability analysis indicates that Lorsa achieves parity with SAE in
interpretability while Lorsa exhibits superior circuit discovery properties,
especially for features computed collectively by multiple MHSA heads. We also
conduct extensive experiments on architectural design ablation, Lorsa scaling
law and error analysis.

</details>


### [268] [Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification](https://arxiv.org/abs/2504.20522)
*Gissel Velarde, Tillman Weyde, David Meredith*

Main category: cs.LG

TL;DR: A machine-learning method using Haar-wavelet filtering for segmenting and classifying folk songs into tune families outperforms Gestalt-based methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the classification of folk songs into tune families by leveraging symbolic representations and wavelet filtering.

Method: Symbolic pitch-time signals are processed with Haar-wavelet filtering, segmented using wavelet coefficients' local maxima, and classified via k-nearest neighbors with standard metrics.

Result: Wavelet-based segmentation and filtering yield better classification accuracy than Gestalt-based methods when parameters are optimized.

Conclusion: Haar-wavelet filtering enhances classification accuracy for folk song tune families compared to traditional methods.

Abstract: The aim of this study is to evaluate a machine-learning method in which
symbolic representations of folk songs are segmented and classified into tune
families with Haar-wavelet filtering. The method is compared with previously
proposed Gestalt-based method. Melodies are represented as discrete symbolic
pitch-time signals. We apply the continuous wavelet transform (CWT) with the
Haar wavelet at specific scales, obtaining filtered versions of melodies
emphasizing their information at particular time-scales. We use the filtered
signal for representation and segmentation, using the wavelet coefficients'
local maxima to indicate local boundaries and classify segments by means of
k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock),
and compare the results to a Gestalt-based segmentation method and metrics
applied directly to the pitch signal. We found that the wavelet based
segmentation and wavelet-filtering of the pitch signal lead to better
classification accuracy in cross-validated evaluation when the time-scale and
other parameters are optimized.

</details>


### [269] [DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction](https://arxiv.org/abs/2504.20535)
*Chris Child, Lam Ngo*

Main category: cs.LG

TL;DR: DeeP-Mod uses DDPN trained via DP to preserve state information, enabling task independence. A reduced DDPN outperforms the original, and DeeP-Mod eliminates the need for external environment models.


<details>
  <summary>Details</summary>
Motivation: Address the loss of state information in deeper DQN layers by using DP to train DDPN, ensuring state-value representation.

Method: Train DDPN via DP (Value Iteration), extract features for task independence, and use DeeP-Mod to model environment evolution from DDPN features.

Result: Reduced DDPN achieves faster convergence and outperforms the original. DeeP-Mod learns optimal policy without external environment models.

Conclusion: DeeP-Mod effectively preserves state information, improves performance, and broadens DDPN applicability by eliminating reliance on external models.

Abstract: The DeeP-Mod framework builds an environment model using features from a Deep
Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While
Deep Q-Learning is effective in decision-making, state information is lost in
deeper DQN layers due to mixed state-action representations. We address this by
using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures
the output represents state values, not state-action pairs. Extracting features
from the DDPN preserves state information, enabling task and action set
independence. We show that a reduced DDPN can be trained using features
extracted from the original DDPN trained on an identical problem. This reduced
DDPN achieves faster convergence under noise and outperforms the original DDPN.
Finally, we introduce the DeeP-Mod framework, which creates an environment
model using the evolution of features extracted from a DDPN in response to
actions. A second DDPN, which learns directly from this feature model rather
than raw states, can learn an effective feature-value representation and thus
optimal policy. A key advantage of DeeP-Mod is that an externally defined
environment model is not needed at any stage, making DDPN applicable to a wide
range of environments.

</details>


### [270] [Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning](https://arxiv.org/abs/2504.20566)
*Shunjie Wen, Thomas Heinis, Dong-Wan Choi*

Main category: cs.LG

TL;DR: BOIL is a replay-based method for Online Class-Incremental Learning (OCIL) that balances plasticity and stability using dual classifiers and implicit knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: The challenge in OCIL is balancing knowledge of old and new classes. Existing methods struggle with reduced plasticity or stability.

Method: BOIL uses dual classifiers for inclusive training separation and implicit knowledge transfer.

Result: BOIL outperforms state-of-the-art methods on three OCIL benchmarks, achieving balanced performance.

Conclusion: BOIL effectively balances plasticity and stability in OCIL, demonstrating superior performance.

Abstract: Online class-incremental learning (OCIL) focuses on gradually learning new
classes (called plasticity) from a stream of data in a single-pass, while
concurrently preserving knowledge of previously learned classes (called
stability). The primary challenge in OCIL lies in maintaining a good balance
between the knowledge of old and new classes within the continually updated
model. Most existing methods rely on explicit knowledge interaction through
experience replay, and often employ exclusive training separation to address
bias problems. Nevertheless, it still remains a big challenge to achieve a
well-balanced learner, as these methods often exhibit either reduced plasticity
or limited stability due to difficulties in continually integrating knowledge
in the OCIL setting. In this paper, we propose a novel replay-based method,
called Balanced Online Incremental Learning (BOIL), which can achieve both high
plasticity and stability, thus ensuring more balanced performance in OCIL. Our
BOIL method proposes an inclusive training separation strategy using dual
classifiers so that knowledge from both old and new classes can effectively be
integrated into the model, while introducing implicit approaches for
transferring knowledge across the two classifiers. Extensive experimental
evaluations over three widely-used OCIL benchmark datasets demonstrate the
superiority of BOIL, showing more balanced yet better performance compared to
state-of-the-art replay-based OCIL methods.

</details>


### [271] [Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network](https://arxiv.org/abs/2504.20568)
*Danilo Avola, Federica Bruni, Gian Luca Foresti, Daniele Pannone, Amedeo Ranaldi*

Main category: cs.LG

TL;DR: A deep learning model using RaGAN and Bi-LSTM for cross-domain Wi-Fi sensing achieves 96% accuracy in material discrimination, addressing environmental variability challenges.


<details>
  <summary>Details</summary>
Motivation: The rise of Wi-Fi sensing, driven by IEEE 802.11bf and privacy demands, faces challenges in robust generalization across changing environments.

Method: Proposes a RaGAN with Bi-LSTM architectures, trained on shielded and unshielded Wi-Fi signals, and tested with a multi-class SVM.

Result: Achieved 96% accuracy in material discrimination, demonstrating strong cross-domain performance.

Conclusion: The model shows promise for security applications, like identifying concealed objects, by mitigating environmental impacts on Wi-Fi sensing.

Abstract: Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze
environments, enabling tasks such as tracking people, detecting intrusions, and
recognizing gestures. The rise of this technology is driven by the IEEE
802.11bf standard and growing demand for tools that can ensure privacy and
operate through obstacles. However, the performance of Wi-Fi sensing is heavily
influenced by environmental conditions, especially when extracting spatial and
temporal features from the surrounding scene. A key challenge is achieving
robust generalization across domains, ensuring stable performance even when the
sensing environment changes significantly. This paper introduces a novel deep
learning model for cross-domain adaptation of Wi-Fi signals, inspired by
physical signal shielding. The model uses a Relativistic average Generative
Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM)
architectures for both the generator and discriminator. To simulate physical
shielding, an acrylic box lined with electromagnetic shielding fabric was
constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from
various materials both inside (domain-free) and outside (domain-dependent) the
box to train the model. A multi-class Support Vector Machine (SVM) was trained
on domain-free spectra and tested on signals denoised by the RaGAN. The system
achieved 96% accuracy and demonstrated strong material discrimination
capabilities, offering potential for use in security applications to identify
concealed objects based on their composition.

</details>


### [272] [Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects](https://arxiv.org/abs/2504.20579)
*Praharsh Nanavati, Ranjitha Prasad, Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: The paper proposes neural architectures to address hidden confounding and covariate mismatch in observational data, combining gradient matching and covariate matching for valid adjustment sets and improved causal effect estimation.


<details>
  <summary>Details</summary>
Motivation: Estimating treatment effects from observational data is difficult due to hidden confounding and covariate mismatch, with existing methods addressing only one issue at a time. A unified approach is needed.

Method: The authors propose neural architectures combining gradient matching (using causal side information) and covariate matching to learn valid adjustment representations. They prove these yield approximate valid adjustment sets.

Result: The method outperforms baselines on ATE and PEHE errors across benchmarks (IHDP, Jobs, Cattaneo, Crowd Management dataset) and provides testable bounds on effect estimates.

Conclusion: The proposed neural framework effectively addresses both hidden confounding and covariate mismatch, offering a unified solution with provable guarantees and empirical success.

Abstract: Estimating treatment effects from observational data is challenging due to
two main reasons: (a) hidden confounding, and (b) covariate mismatch (control
and treatment groups not having identical distributions). Long lines of works
exist that address only either of these issues. To address the former,
conventional techniques that require detailed knowledge in the form of causal
graphs have been proposed. For the latter, covariate matching and importance
weighting methods have been used. Recently, there has been progress in
combining testable independencies with partial side information for tackling
hidden confounding. A common framework to address both hidden confounding and
selection bias is missing. We propose neural architectures that aim to learn a
representation of pre-treatment covariates that is a valid adjustment and also
satisfies covariate matching constraints. We combine two different neural
architectures: one based on gradient matching across domains created by
subsampling a suitable anchor variable that assumes causal side information,
followed by the other, a covariate matching transformation. We prove that
approximately invariant representations yield approximate valid adjustment sets
which would enable an interval around the true causal effect. In contrast to
usual sensitivity analysis, where an unknown nuisance parameter is varied, we
have a testable approximation yielding a bound on the effect estimate. We also
outperform various baselines with respect to ATE and PEHE errors on causal
benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd
Management dataset.

</details>


### [273] [Independent Learning in Performative Markov Potential Games](https://arxiv.org/abs/2504.20593)
*Rilind Sahitaj, Paulius Sasnauskas, Yiğit Yalın, Debmalya Mandal, Goran Radanović*

Main category: cs.LG

TL;DR: The paper introduces Performative Reinforcement Learning (PRL) in multi-agent settings, defining a performatively stable equilibrium (PSE) and proving its existence. It shows convergence of algorithms like IPGA and INPG to PSE, with theoretical and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To address scenarios where deployed policies alter environment dynamics in multi-agent systems, extending Markov Potential Games (MPGs) with performative effects.

Method: Introduces PSE and analyzes convergence of IPGA and INPG algorithms, including a repeated retraining approach for a special case.

Result: Proves PSE existence, convergence of IPGA and INPG to approximate PSE, and asymptotic convergence of INPG. Experiments validate findings.

Conclusion: The work extends PRL to multi-agent systems, providing theoretical guarantees and empirical support for convergence under performative effects.

Abstract: Performative Reinforcement Learning (PRL) refers to a scenario in which the
deployed policy changes the reward and transition dynamics of the underlying
environment. In this work, we study multi-agent PRL by incorporating
performative effects into Markov Potential Games (MPGs). We introduce the
notion of a performatively stable equilibrium (PSE) and show that it always
exists under a reasonable sensitivity assumption. We then provide convergence
results for state-of-the-art algorithms used to solve MPGs. Specifically, we
show that independent policy gradient ascent (IPGA) and independent natural
policy gradient (INPG) converge to an approximate PSE in the best-iterate
sense, with an additional term that accounts for the performative effects.
Furthermore, we show that INPG asymptotically converges to a PSE in the
last-iterate sense. As the performative effects vanish, we recover the
convergence rates from prior work. For a special case of our game, we provide
finite-time last-iterate convergence results for a repeated retraining
approach, in which agents independently optimize a surrogate objective. We
conduct extensive experiments to validate our theoretical findings.

</details>


### [274] [Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation](https://arxiv.org/abs/2504.20635)
*Bradley Segal, Joshua Fieggen, David Clifton, Lei Clifton*

Main category: cs.LG

TL;DR: A structured synthetic data framework is proposed to benchmark ML model robustness, fairness, and generalisability in clinical settings, offering explicit control over data generation for systematic evaluation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ML model generalisability across diverse healthcare settings due to variability in data and biases in real-world datasets.

Method: A novel synthetic data framework with control over data generation, including site-specific variations and subgroup effects, for targeted model evaluation.

Result: The framework isolates site variations, supports fairness audits, and reveals generalisation failures, showing how model complexity interacts with site-specific effects.

Conclusion: The work provides a reproducible and interpretable tool to improve ML deployment reliability in clinical applications.

Abstract: Ensuring the generalisability of clinical machine learning (ML) models across
diverse healthcare settings remains a significant challenge due to variability
in patient demographics, disease prevalence, and institutional practices.
Existing model evaluation approaches often rely on real-world datasets, which
are limited in availability, embed confounding biases, and lack the flexibility
needed for systematic experimentation. Furthermore, while generative models aim
for statistical realism, they often lack transparency and explicit control over
factors driving distributional shifts. In this work, we propose a novel
structured synthetic data framework designed for the controlled benchmarking of
model robustness, fairness, and generalisability. Unlike approaches focused
solely on mimicking observed data, our framework provides explicit control over
the data generating process, including site-specific prevalence variations,
hierarchical subgroup effects, and structured feature interactions. This
enables targeted investigation into how models respond to specific
distributional shifts and potential biases. Through controlled experiments, we
demonstrate the framework's ability to isolate the impact of site variations,
support fairness-aware audits, and reveal generalisation failures, particularly
highlighting how model complexity interacts with site-specific effects. This
work contributes a reproducible, interpretable, and configurable tool designed
to advance the reliable deployment of ML in clinical settings.

</details>


### [275] [Decision-centric fairness: Evaluation and optimization for resource allocation problems](https://arxiv.org/abs/2504.20642)
*Simon De Vos, Jente Van Belle, Andres Algaba, Wouter Verbeke, Sam Verboven*

Main category: cs.LG

TL;DR: The paper proposes a decision-centric fairness method for binary classification models, focusing on fairness in the decision-making region to avoid unnecessary constraints on model performance.


<details>
  <summary>Details</summary>
Motivation: Binary classification models can exhibit discriminatory behavior in resource allocation, leading to unfair outcomes for certain demographic groups.

Method: A decision-centric fairness approach is introduced, enforcing fairness only in the decision-making region (relevant score thresholds) instead of globally.

Result: Empirical comparisons show the decision-centric method avoids degrading model quality while ensuring fairness where it matters.

Conclusion: Focusing fairness on the decision-making region is beneficial, balancing fairness and model performance effectively.

Abstract: Data-driven decision support tools play an increasingly central role in
decision-making across various domains. In this work, we focus on binary
classification models for predicting positive-outcome scores and deciding on
resource allocation, e.g., credit scores for granting loans or churn propensity
scores for targeting customers with a retention campaign. Such models may
exhibit discriminatory behavior toward specific demographic groups through
their predicted scores, potentially leading to unfair resource allocation. We
focus on demographic parity as a fairness metric to compare the proportions of
instances that are selected based on their positive outcome scores across
groups. In this work, we propose a decision-centric fairness methodology that
induces fairness only within the decision-making region -- the range of
relevant decision thresholds on the score that may be used to decide on
resource allocation -- as an alternative to a global fairness approach that
seeks to enforce parity across the entire score distribution. By restricting
the induction of fairness to the decision-making region, the proposed
decision-centric approach avoids imposing overly restrictive constraints on the
model, which may unnecessarily degrade the quality of the predicted scores. We
empirically compare our approach to a global fairness approach on multiple
(semi-synthetic) datasets to identify scenarios in which focusing on fairness
where it truly matters, i.e., decision-centric fairness, proves beneficial.

</details>


### [276] [Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection](https://arxiv.org/abs/2504.20644)
*Ziqing Fan, Siyuan Du, Shengchao Hu, Pingjie Wang, Li Shen, Ya Zhang, Dacheng Tao, Yanfeng Wang*

Main category: cs.LG

TL;DR: DiSF, a diversified file selection algorithm, improves LLM performance by enhancing diversity in pre-training data, outperforming domain-similarity methods.


<details>
  <summary>Details</summary>
Motivation: Current domain-similarity selection methods for LLM pre-training data cause a diversity dilemma, degrading generic performance despite domain-related improvements.

Method: DiSF selects decorrelated text files using a greedy algorithm to ensure uniform eigenvalues in the feature covariance matrix, framed as a γ-weakly submodular optimization problem.

Result: DiSF significantly improves overall performance, saving 98.5% of training files and achieving 1.5x training efficiency and 5x data efficiency.

Conclusion: DiSF effectively addresses the diversity dilemma, enhancing LLM performance and efficiency in pre-training.

Abstract: Selecting high-quality pre-training data for large language models (LLMs) is
crucial for enhancing their overall performance under limited computation
budget, improving both training and sample efficiency. Recent advancements in
file selection primarily rely on using an existing or trained proxy model to
assess the similarity of samples to a target domain, such as high quality
sources BookCorpus and Wikipedia. However, upon revisiting these methods, the
domain-similarity selection criteria demonstrates a diversity dilemma,
i.e.dimensional collapse in the feature space, improving performance on the
domain-related tasks but causing severe degradation on generic performance. To
prevent collapse and enhance diversity, we propose a DiverSified File selection
algorithm (DiSF), which selects the most decorrelated text files in the feature
space. We approach this with a classical greedy algorithm to achieve more
uniform eigenvalues in the feature covariance matrix of the selected texts,
analyzing its approximation to the optimal solution under a formulation of
$\gamma$-weakly submodular optimization problem. Empirically, we establish a
benchmark and conduct extensive experiments on the TinyLlama architecture with
models from 120M to 1.1B parameters. Evaluating across nine tasks from the
Harness framework, DiSF demonstrates a significant improvement on overall
performance. Specifically, DiSF saves 98.5% of 590M training files in
SlimPajama, outperforming the full-data pre-training within a 50B training
budget, and achieving about 1.5x training efficiency and 5x data efficiency.

</details>


### [277] [RuleKit 2: Faster and simpler rule learning](https://arxiv.org/abs/2504.20650)
*Adam Gudyś, Cezary Maszczyk, Joanna Badura, Adam Grzelak, Marek Sikora, Łukasz Wróbel*

Main category: cs.LG

TL;DR: RuleKit 2 is an upgraded version of a rule-based data analysis tool, offering faster performance, Python integration, and a GUI.


<details>
  <summary>Details</summary>
Motivation: To enhance computational performance and usability of rule-based data analysis.

Method: New algorithms and optimized implementations, Python package, and browser-based GUI.

Result: Significant speed improvements (up to 100x) and seamless integration with scikit-learn.

Conclusion: RuleKit 2 is a powerful, user-friendly tool for rule-based data analysis, now more accessible and efficient.

Abstract: Rules offer an invaluable combination of predictive and descriptive
capabilities. Our package for rule-based data analysis, RuleKit, has proven its
effectiveness in classification, regression, and survival problems. Here we
present its second version. New algorithms and optimized implementations of
those previously included, significantly improved the computational performance
of our suite, reducing the analysis time of some data sets by two orders of
magnitude. The usability of RuleKit 2 is provided by two new components: Python
package and browser application with a graphical user interface. The former
complies with scikit-learn, the most popular data mining library for Python,
allowing RuleKit 2 to be straightforwardly integrated into existing data
analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license
(https://github.com/adaa-polsl/RuleKit)

</details>


### [278] [Federated learning, ethics, and the double black box problem in medical AI](https://arxiv.org/abs/2504.20656)
*Joshua Hatherley, Anders Søgaard, Angela Ballantyne, Ruben Pauwels*

Main category: cs.LG

TL;DR: The paper examines the ethical risks of federated learning (FL) in medical AI, introducing the concept of 'federation opacity' and its implications for healthcare.


<details>
  <summary>Details</summary>
Motivation: FL is seen as a solution for patient privacy in medical AI, but its ethical risks are underexplored.

Method: The paper critiques FL's opacity and its 'double black box' problem in healthcare.

Result: It identifies exaggerated benefits of FL and ethical challenges.

Conclusion: Key challenges must be addressed to make FL ethically feasible in medicine.

Abstract: Federated learning (FL) is a machine learning approach that allows multiple
devices or institutions to collaboratively train a model without sharing their
local data with a third-party. FL is considered a promising way to address
patient privacy concerns in medical artificial intelligence. The ethical risks
of medical FL systems themselves, however, have thus far been underexamined.
This paper aims to address this gap. We argue that medical FL presents a new
variety of opacity -- federation opacity -- that, in turn, generates a
distinctive double black box problem in healthcare AI. We highlight several
instances in which the anticipated benefits of medical FL may be exaggerated,
and conclude by highlighting key challenges that must be overcome to make FL
ethically feasible in medicine.

</details>


### [279] [Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems](https://arxiv.org/abs/2504.20660)
*Sahil Tomar, Shamshe Alam, Sandeep Kumar, Amit Mathur*

Main category: cs.LG

TL;DR: A hybrid quantum-classical framework enhances reinforcement learning by combining quantum computing's parallelism with classical methods, improving training speed and adaptability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of training time and adaptability in reinforcement learning for autonomous navigation in complex, unpredictable environments.

Method: Proposes a quantum-classical hybrid framework integrating quantum computing's parallelism for robust Q tables and turn cost estimations with classical reinforcement learning.

Result: Demonstrates rapid training convergence, improved path efficiency, trajectory smoothness, and mission success rates in simulations and real-world tests like IIT Delhi campus.

Conclusion: The framework shows strong potential for real-time autonomous navigation in complex, dynamic environments.

Abstract: In this paper, a novel quantum classical hybrid framework is proposed that
synergizes quantum with Classical Reinforcement Learning. By leveraging the
inherent parallelism of quantum computing, the proposed approach generates
robust Q tables and specialized turn cost estimations, which are then
integrated with a classical Reinforcement Learning pipeline. The Classical
Quantum fusion results in rapid convergence of training, reducing the training
time significantly and improved adaptability in scenarios featuring static,
dynamic, and moving obstacles. Simulator based evaluations demonstrate
significant enhancements in path efficiency, trajectory smoothness, and mission
success rates, underscoring the potential of framework for real time,
autonomous navigation in complex and unpredictable environments. Furthermore,
the proposed framework was tested beyond simulations on practical scenarios,
including real world map data such as the IIT Delhi campus, reinforcing its
potential for real time, autonomous navigation in complex and unpredictable
environments.

</details>


### [280] [SFi-Former: Sparse Flow Induced Attention for Graph Transformer](https://arxiv.org/abs/2504.20666)
*Zhonghao Li, Ji Shi, Xinming Zhang, Miao Zhang, Bo Li*

Main category: cs.LG

TL;DR: SFi-Former introduces a sparse attention mechanism (SFi-attention) to address issues like weak inductive bias and overfitting in Graph Transformers, achieving competitive and SOTA performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Graph Transformers (GTs) struggle with weak inductive bias, overfitting, and over-globalizing due to dense attention. This paper aims to mitigate these issues.

Method: Proposes SFi-attention, an energy-based sparse attention mechanism with l1-norm regularization, and SFi-Former, a model leveraging this mechanism for selective feature aggregation.

Result: SFi-Former achieves competitive performance on GNN benchmarks and SOTA on LongRange Graph Benchmark datasets, with smaller generalization gaps.

Conclusion: SFi-Former effectively addresses GT limitations, offering robust performance and reduced overfitting.

Abstract: Graph Transformers (GTs) have demonstrated superior performance compared to
traditional message-passing graph neural networks in many studies, especially
in processing graph data with long-range dependencies. However, GTs tend to
suffer from weak inductive bias, overfitting and over-globalizing problems due
to the dense attention. In this paper, we introduce SFi-attention, a novel
attention mechanism designed to learn sparse pattern by minimizing an energy
function based on network flows with l1-norm regularization, to relieve those
issues caused by dense attention. Furthermore, SFi-Former is accordingly
devised which can leverage the sparse attention pattern of SFi-attention to
generate sparse network flows beyond adjacency matrix of graph data.
Specifically, SFi-Former aggregates features selectively from other nodes
through flexible adaptation of the sparse attention, leading to a more robust
model. We validate our SFi-Former on various graph datasets, especially those
graph data exhibiting long-range dependencies. Experimental results show that
our SFi-Former obtains competitive performance on GNN Benchmark datasets and
SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally,
our model gives rise to smaller generalization gaps, which indicates that it is
less prone to over-fitting. Click here for codes.

</details>


### [281] [Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability](https://arxiv.org/abs/2504.20667)
*Simone Piaggesi, Riccardo Guidotti, Fosca Giannotti, Dino Pedreschi*

Main category: cs.LG

TL;DR: ILLUME is a framework for post-hoc explainability of black-box models, combining global surrogates with instance-specific transformations to address limitations of traditional surrogate methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing surrogate-based explainability techniques, which are either computationally expensive (local) or struggle with complex behaviors (global).

Method: ILLUME integrates a globally trained surrogate with instance-specific linear transformations learned via a meta-encoder to generate local and global explanations.

Result: Empirical evaluations show ILLUME produces accurate, robust, and faithful feature attributions and decision rules.

Conclusion: ILLUME provides a unified and flexible framework for explainability, effectively addressing the shortcomings of traditional surrogate methods.

Abstract: Post-hoc explainability is essential for understanding black-box machine
learning models. Surrogate-based techniques are widely used for local and
global model-agnostic explanations but have significant limitations. Local
surrogates capture non-linearities but are computationally expensive and
sensitive to parameters, while global surrogates are more efficient but
struggle with complex local behaviors. In this paper, we present ILLUME, a
flexible and interpretable framework grounded in representation learning, that
can be integrated with various surrogate models to provide explanations for any
black-box classifier. Specifically, our approach combines a globally trained
surrogate with instance-specific linear transformations learned with a
meta-encoder to generate both local and global explanations. Through extensive
empirical evaluations, we demonstrate the effectiveness of ILLUME in producing
feature attributions and decision rules that are not only accurate but also
robust and faithful to the black-box, thus providing a unified explanation
framework that effectively addresses the limitations of traditional surrogate
methods.

</details>


### [282] [What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models](https://arxiv.org/abs/2504.20687)
*Jan Kapar, Niklas Koenen, Martin Jullum*

Main category: cs.LG

TL;DR: The paper proposes using explainable AI (XAI) techniques to evaluate synthetic tabular data, identifying specific weaknesses beyond traditional metrics.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for synthetic data quality often conflict and lack specificity in pinpointing weaknesses.

Method: Apply XAI techniques (feature importance, Shapley values, etc.) to a binary classifier distinguishing real from synthetic data.

Result: XAI reveals inconsistencies and missing patterns in synthetic data, overlooked by standard methods.

Conclusion: XAI enhances synthetic data evaluation transparency and provides actionable insights for improvement.

Abstract: Evaluating synthetic tabular data is challenging, since they can differ from
the real data in so many ways. There exist numerous metrics of synthetic data
quality, ranging from statistical distances to predictive performance, often
providing conflicting results. Moreover, they fail to explain or pinpoint the
specific weaknesses in the synthetic data. To address this, we apply
explainable AI (XAI) techniques to a binary detection classifier trained to
distinguish real from synthetic data. While the classifier identifies
distributional differences, XAI concepts such as feature importance and feature
effects, analyzed through methods like permutation feature importance, partial
dependence plots, Shapley values and counterfactual explanations, reveal why
synthetic data are distinguishable, highlighting inconsistencies, unrealistic
dependencies, or missing patterns. This interpretability increases transparency
in synthetic data evaluation and provides deeper insights beyond conventional
metrics, helping diagnose and improve synthetic data quality. We apply our
approach to two tabular datasets and generative models, showing that it
uncovers issues overlooked by standard evaluation techniques.

</details>


### [283] [Unsupervised Surrogate Anomaly Detection](https://arxiv.org/abs/2504.20733)
*Simon Klüttermann, Tim Katzke, Emmanuel Müller*

Main category: cs.LG

TL;DR: The paper introduces DEAN, a deep ensemble anomaly detection method, formalizing surrogate anomaly detection with axioms and showing competitive performance on 121 datasets.


<details>
  <summary>Details</summary>
Motivation: To improve unsupervised anomaly detection by learning neural network representations of normal data patterns, inspired by engineering concepts.

Method: Proposes DEAN, a deep ensemble algorithm designed to meet axioms for optimal surrogate models in anomaly detection.

Result: DEAN outperforms 19 existing methods on 121 benchmark datasets, demonstrating scalability and reliability.

Conclusion: DEAN is a robust and scalable solution for unsupervised anomaly detection, validated by extensive benchmarking.

Abstract: In this paper, we study unsupervised anomaly detection algorithms that learn
a neural network representation, i.e. regular patterns of normal data, which
anomalies are deviating from. Inspired by a similar concept in engineering, we
refer to our methodology as surrogate anomaly detection. We formalize the
concept of surrogate anomaly detection into a set of axioms required for
optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble
ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121
benchmark datasets, demonstrating its competitive performance against 19
existing methods, as well as the scalability and reliability of our method.

</details>


### [284] [Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency](https://arxiv.org/abs/2504.20735)
*Tariq Qayyum, Asadullah Tariq, Muhammad Ali, Mohamed Adel Serhani, Zouheir Trabelsi, Maite López-Sánchez*

Main category: cs.LG

TL;DR: A hybrid AI framework combining supervised learning, reinforcement learning, and PSO is proposed to optimize task offloading and resource allocation in VANETs, reducing latency and energy use while improving task success rates.


<details>
  <summary>Details</summary>
Motivation: The highly dynamic nature of VANETs causes challenges like unpredictable network conditions, high latency, energy inefficiency, and task failure, necessitating an efficient solution.

Method: The framework integrates supervised learning for predicting offloading strategies, reinforcement learning for adaptive decisions, and PSO for optimizing latency and energy.

Result: Simulations show significant reductions in latency and energy consumption, alongside improved task success rates and network throughput.

Conclusion: The framework provides an efficient, scalable solution for enhancing real-time applications in dynamic vehicular environments.

Abstract: Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation
systems, enabling vehicles to offload computational tasks to nearby roadside
units (RSUs) and mobile edge computing (MEC) servers for real-time processing.
However, the highly dynamic nature of VANETs introduces challenges, such as
unpredictable network conditions, high latency, energy inefficiency, and task
failure. This research addresses these issues by proposing a hybrid AI
framework that integrates supervised learning, reinforcement learning, and
Particle Swarm Optimization (PSO) for intelligent task offloading and resource
allocation. The framework leverages supervised models for predicting optimal
offloading strategies, reinforcement learning for adaptive decision-making, and
PSO for optimizing latency and energy consumption. Extensive simulations
demonstrate that the proposed framework achieves significant reductions in
latency and energy usage while improving task success rates and network
throughput. By offering an efficient, and scalable solution, this framework
sets the foundation for enhancing real-time applications in dynamic vehicular
environments.

</details>


### [285] [DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs](https://arxiv.org/abs/2504.20754)
*Hao Luan, See-Kiong Ng, Chun Kai Ling*

Main category: cs.LG

TL;DR: The paper introduces a method using discrete diffusion models to generate paths in layered graphs, ensuring explicit constraints via a padded adjacency-list matrix (PALM) and classifier guidance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of methods for generating constrained samples in diffusion models, particularly for paths in layered graphs.

Method: Uses PALM for path representation and classifier guidance to steer paths without retraining.

Result: Empirically outperforms alternatives that ignore path constraints.

Conclusion: The approach effectively generates constrained paths in layered graphs, demonstrating superior performance.

Abstract: Diffusion models form an important class of generative models today,
accounting for much of the state of the art in cutting edge AI research. While
numerous extensions beyond image and video generation exist, few of such
approaches address the issue of explicit constraints in the samples generated.
In this paper, we study the problem of generating paths in a layered graph (a
variant of a directed acyclic graph) using discrete diffusion models, while
guaranteeing that our generated samples are indeed paths. Our approach utilizes
a simple yet effective representation for paths which we call the padded
adjacency-list matrix (PALM). In addition, we show how to effectively perform
classifier guidance, which helps steer the sampled paths to specific preferred
edges without any retraining of the diffusion model. Our preliminary results
show that empirically, our method outperforms alternatives which do not
explicitly account for path constraints.

</details>


### [286] [JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation](https://arxiv.org/abs/2504.20770)
*Ji Shi, Chengxun Xie, Zhonghao Li, Xinming Zhang, Miao Zhang*

Main category: cs.LG

TL;DR: A novel graph transformer framework, JTreeformer, is introduced for molecular generation, outperforming existing methods by leveraging junction trees and integrating GCN with multi-head attention.


<details>
  <summary>Details</summary>
Motivation: Current transformer-based graph decoders fail to fully utilize graph information, limiting their ability to handle complex molecular structures.

Method: JTreeformer transforms graph generation into junction tree generation, combining GCN and multi-head attention in the encoder and using a directed acyclic GCN in the decoder. A diffusion model enhances sampling.

Result: The framework outperforms existing molecule generation methods, demonstrating superior performance.

Conclusion: JTreeformer offers a promising tool for advancing drug discovery by efficiently generating molecular structures.

Abstract: The discovery of new molecules based on the original chemical molecule
distributions is of great importance in medicine. The graph transformer, with
its advantages of high performance and scalability compared to traditional
graph networks, has been widely explored in recent research for applications of
graph structures. However, current transformer-based graph decoders struggle to
effectively utilize graph information, which limits their capacity to leverage
only sequences of nodes rather than the complex topological structures of
molecule graphs. This paper focuses on building a graph transformer-based
framework for molecular generation, which we call \textbf{JTreeformer} as it
transforms graph generation into junction tree generation. It combines GCN
parallel with multi-head attention as the encoder. It integrates a directed
acyclic GCN into a graph-based Transformer to serve as a decoder, which can
iteratively synthesize the entire molecule by leveraging information from the
partially constructed molecular structure at each step. In addition, a
diffusion model is inserted in the latent space generated by the encoder, to
enhance the efficiency and effectiveness of sampling further. The empirical
results demonstrate that our novel framework outperforms existing molecule
generation methods, thus offering a promising tool to advance drug discovery
(https://anonymous.4open.science/r/JTreeformer-C74C).

</details>


### [287] [Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM](https://arxiv.org/abs/2504.20789)
*Collin Beaudoin, Swaroop Ghosh*

Main category: cs.LG

TL;DR: The paper introduces a hybrid quantum-classical model (QK-LSTM) for predicting molecular properties and side effects, showing improved accuracy with augmented SELFIES over SMILES.


<details>
  <summary>Details</summary>
Motivation: Identifying molecular properties and side effects is crucial in drug development but time-consuming. Machine learning, especially hybrid quantum-classical models, offers a solution.

Method: The QK-LSTM integrates quantum kernel functions into classical LSTM, using augmented SMILES and SELFIES for molecular property prediction.

Result: Augmented SELFIES outperforms SMILES, with 5.97% and 5.91% improvements in classical and hybrid domains, respectively.

Conclusion: The study highlights the potential of augmented SELFIES and hybrid models for enhancing molecular property prediction and side effect identification.

Abstract: Identifying molecular properties, including side effects, is a critical yet
time-consuming step in drug development. Failing to detect these side effects
before regulatory submission can result in significant financial losses and
production delays, and overlooking them during the regulatory review can lead
to catastrophic consequences. This challenge presents an opportunity for
innovative machine learning approaches, particularly hybrid quantum-classical
models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network.
The QK-LSTM integrates quantum kernel functions into the classical LSTM
framework, enabling the capture of complex, non-linear patterns in sequential
data. By mapping input data into a high-dimensional quantum feature space, the
QK-LSTM model reduces the need for large parameter sets, allowing for model
compression without sacrificing accuracy in sequence-based tasks. Recent
advancements have been made in the classical domain using augmented variations
of the Simplified Molecular Line-Entry System (SMILES). However, to the best of
our knowledge, no research has explored the impact of augmented SMILES in the
quantum domain, nor the role of augmented Self-Referencing Embedded Strings
(SELFIES) in either classical or hybrid quantum-classical settings. This study
presents the first analysis of these approaches, providing novel insights into
their potential for enhancing molecular property prediction and side effect
identification. Results reveal that augmenting SELFIES yields in statistically
significant improvements from SMILES by a 5.97% improvement for the classical
domain and a 5.91% improvement for the hybrid quantum-classical domain.

</details>


### [288] [Q-Fusion: Diffusing Quantum Circuits](https://arxiv.org/abs/2504.20794)
*Collin Beaudoin, Swaroop Ghosh*

Main category: cs.LG

TL;DR: A diffusion-based algorithm using LayerDAG is proposed for Quantum Architecture Search (QAS) to automate quantum circuit design, achieving 100% valid outputs.


<details>
  <summary>Details</summary>
Motivation: Current NISQ devices face limitations in qubits and gate counts, and manual quantum algorithm design is time-consuming and expertise-heavy. QAS aims to automate this process.

Method: The paper introduces a diffusion-based algorithm within the LayerDAG framework to generate quantum circuits, differing from LLMs, RL, and VAEs.

Result: The proposed model consistently produces 100% valid quantum circuit outputs.

Conclusion: The diffusion-based approach is effective for QAS, offering a reliable alternative to existing methods.

Abstract: Quantum computing holds great potential for solving socially relevant and
computationally complex problems. Furthermore, quantum machine learning (QML)
promises to rapidly improve our current machine learning capabilities. However,
current noisy intermediate-scale quantum (NISQ) devices are constrained by
limitations in the number of qubits and gate counts, which hinder their full
capabilities. Furthermore, the design of quantum algorithms remains a laborious
task, requiring significant domain expertise and time. Quantum Architecture
Search (QAS) aims to streamline this process by automatically generating novel
quantum circuits, reducing the need for manual intervention. In this paper, we
propose a diffusion-based algorithm leveraging the LayerDAG framework to
generate new quantum circuits. This method contrasts with other approaches that
utilize large language models (LLMs), reinforcement learning (RL), variational
autoencoders (VAE), and similar techniques. Our results demonstrate that the
proposed model consistently generates 100% valid quantum circuit outputs.

</details>


### [289] [The When and How of Target Variable Transformations](https://arxiv.org/abs/2504.20821)
*Loren Nuyts, Jesse Davis*

Main category: cs.LG

TL;DR: The paper emphasizes the often-overlooked importance of transforming the target variable in machine learning, providing practical examples, guidelines, and recommendations for such transformations.


<details>
  <summary>Details</summary>
Motivation: Practitioners focus heavily on feature set manipulation but neglect target variable transformations, which can significantly impact model accuracy. The paper aims to address this gap.

Method: The paper highlights practical cases where target variable transformations were beneficial, proposes general guidelines for when such transformations are needed, and suggests appropriate transformations for different scenarios.

Result: The paper demonstrates the practical utility of target variable transformations and provides actionable insights for practitioners.

Conclusion: Transforming the target variable is a critical but underappreciated step in the machine learning pipeline, and the paper offers valuable guidance for its effective implementation.

Abstract: The machine learning pipeline typically involves the iterative process of (1)
collecting the data, (2) preparing the data, (3) learning a model, and (4)
evaluating a model. Practitioners recognize the importance of the data
preparation phase in terms of its impact on the ability to learn accurate
models. In this regard, significant attention is often paid to manipulating the
feature set (e.g., selection, transformations, dimensionality reduction). A
point that is less well appreciated is that transformations on the target
variable can also have a large impact on whether it is possible to learn a
suitable model. These transformations may include accounting for
subject-specific biases (e.g., in how someone uses a rating scale), contexts
(e.g., population size effects), and general trends (e.g., inflation). However,
this point has received a much more cursory treatment in the existing
literature. The goal of this paper is three-fold. First, we aim to highlight
the importance of this problem by showing when transforming the target variable
has been useful in practice. Second, we will provide a set of generic ``rules
of thumb'' that indicate situations when transforming the target variable may
be needed. Third, we will discuss which transformations should be considered in
a given situation.

</details>


### [290] [An approach to melodic segmentation and classification based on filtering with the Haar-wavelet](https://arxiv.org/abs/2504.20822)
*Gissel Velarde, Tillman Weyde, David Meredith*

Main category: cs.LG

TL;DR: A novel method for melody classification and segmentation using Haar-wavelet filtering and k-nearest neighbor algorithm outperforms unfiltered pitch signals in Bach's works but matches pitch signals for Dutch folk tunes.


<details>
  <summary>Details</summary>
Motivation: To improve melody classification and segmentation by leveraging Haar-wavelet filtering and k-nearest neighbor algorithms.

Method: Filter pitch signals with Haar-wavelet, segment using local maxima or zero-crossings, and classify segments with k-nearest neighbor (Euclidian/city-block distances).

Result: Outperforms unfiltered pitch and Gestalt-based methods for Bach's works; matches pitch signals but underperforms string-matching for Dutch folk tunes.

Conclusion: Haar-wavelet filtering enhances segmentation and classification for certain tasks but may not surpass feature-rich methods like string-matching.

Abstract: We present a novel method of classification and segmentation of melodies in
symbolic representation. The method is based on filtering pitch as a signal
over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered
signal corresponds to a single-scale signal ws from the continuous Haar wavelet
transform. The melodies are first segmented using local maxima or
zero-crossings of w_s. The segments of w_s are then classified using the
k-nearest neighbour algorithm with Euclidian and city-block distances. The
method proves more effective than using unfiltered pitch signals and
Gestalt-based segmentation when used to recognize the parent works of segments
from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch
folk tunes into 26 tune families, the performance of the method is comparable
to the use of pitch signals, but not as good as that of string-matching methods
based on multiple features.

</details>


### [291] [Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction](https://arxiv.org/abs/2504.20823)
*Olga Tsurkan, Aleksandra Konstantinova, Aleksandr Sedykh, Dmitrii Zhiganov, Arsenii Senokosov, Daniil Tarpanov, Matvei Anoshin, Leonid Fedichkin*

Main category: cs.LG

TL;DR: A Hybrid Quantum Recurrent Neural Network (HQRNN) improves Remaining Useful Life (RUL) forecasting in aerospace, outperforming classical methods like RNN, CNN, and MLP by up to 16.21% in error metrics.


<details>
  <summary>Details</summary>
Motivation: Accurate RUL estimation is critical for predictive maintenance in aerospace, and hybrid quantum-classical methods may enhance performance under limited data conditions.

Method: HQRNN combines Quantum Long Short-Term Memory (QLSTM) gates with classical dense layers, using Quantum Depth-Infused circuits for better high-frequency learning.

Result: HQRNN achieves a 5% improvement over classical RNNs and surpasses Random Forest, CNN, and MLP by 13.68%, 16.21%, and 7.87% in RMSE, respectively.

Conclusion: Hybrid quantum-classical approaches show promise for robust time-series forecasting in predictive maintenance, though advanced joint architectures still outperform them.

Abstract: Predictive maintenance in aerospace heavily relies on accurate estimation of
the remaining useful life of jet engines. In this paper, we introduce a Hybrid
Quantum Recurrent Neural Network framework, combining Quantum Long Short-Term
Memory layers with classical dense layers for Remaining Useful Life forecasting
on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each
Quantum Long Short-Term Memory gate replaces conventional linear
transformations with Quantum Depth-Infused circuits, allowing the network to
learn high-frequency components more effectively. Experimental results
demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum
Recurrent Neural Network achieves up to a 5% improvement over a Recurrent
Neural Network based on stacked Long Short-Term Memory layers in terms of mean
root mean squared error and mean absolute error. Moreover, a thorough
comparison of our method with established techniques, including Random Forest,
Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our
approach, which achieves a Root Mean Squared Error of 15.46, surpasses these
baselines by approximately 13.68%, 16.21%, and 7.87%, respectively.
Nevertheless, it remains outperformed by certain advanced joint architectures.
Our findings highlight the potential of hybrid quantum-classical approaches for
robust time-series forecasting under limited data conditions, offering new
avenues for enhancing reliability in predictive maintenance tasks.

</details>


### [292] [Reinforcement Learning for LLM Reasoning Under Memory Constraints](https://arxiv.org/abs/2504.20834)
*Alan Lee, Harry Tong*

Main category: cs.LG

TL;DR: S-GRPO and T-SPMO, two RL-based methods, improve LLM reasoning under hardware constraints, boosting SVAMP accuracy from 46% to 70% and excelling in multi-digit multiplication.


<details>
  <summary>Details</summary>
Motivation: Enhancing reasoning in LLMs under memory and compute constraints, particularly for academic settings with limited GPU resources.

Method: Introduces S-GRPO (memory-efficient GRPO variant) and T-SPMO (token-level prefix matching) for fine-grained credit assignment, compatible with LoRA fine-tuning on a single 40GB GPU.

Result: S-GRPO and T-SPMO significantly improve SVAMP benchmark accuracy (46% to 70%) and perform well in multi-digit multiplication tasks. Full-token GRPO baseline under LoRA did not improve performance.

Conclusion: Memory-efficient RL methods like S-GRPO and T-SPMO can stabilize training and enhance performance under hardware constraints, acting as a form of regularization.

Abstract: We explore reinforcement learning (RL) techniques to enhance reasoning within
targeted problem spaces in large language models (LLMs) under memory and
compute constraints. Our focus is on critic-free methods compatible with LoRA
fine-tuning on a single 40GB GPU, a common limitation in academic settings. We
introduce S-GRPO, a memory-efficient variant of Group Relative Policy
Optimization, and T-SPMO, a token-level prefix matching strategy for
fine-grained credit assignment. Despite limited resources, when used to
fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark
accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in
multi-digit multiplication tasks, underscoring the potential of RL fine-tuning
under hardware constraints. Additionally, we find that our full-token GRPO
baseline under LoRA fine-tuning did not improve model performance (compared to
base model) on either task, suggesting that our memory-efficient methods may
act as a form of regularization that stabilizes training when only a small
subset of parameters are updated.

</details>


### [293] [Mitigating the Structural Bias in Graph Adversarial Defenses](https://arxiv.org/abs/2504.20848)
*Junyuan Fang, Huimin Liu, Han Yang, Jiajing Wu, Zibin Zheng, Chi K. Tse*

Main category: cs.LG

TL;DR: Proposes a defense strategy for GNNs against adversarial attacks, focusing on mitigating structural bias for low-degree nodes via hetero-homo augmented graphs, $k$NN augmentation, and multi-view attention.


<details>
  <summary>Details</summary>
Motivation: Current GNN defense methods exhibit structural bias against low-degree nodes, similar to traditional GNNs, necessitating a solution to enhance robustness and fairness.

Method: Includes hetero-homo augmented graph construction (removing heterophilic links, adding homophilic links for low-degree nodes), $k$NN augmentation, and multi-view node-wise attention modules.

Result: Demonstrates improved defense and debiasing effects on benchmark datasets.

Conclusion: The proposed strategy effectively mitigates structural bias in GNNs under adversarial attacks, enhancing robustness for low-degree nodes.

Abstract: In recent years, graph neural networks (GNNs) have shown great potential in
addressing various graph structure-related downstream tasks. However, recent
studies have found that current GNNs are susceptible to malicious adversarial
attacks. Given the inevitable presence of adversarial attacks in the real
world, a variety of defense methods have been proposed to counter these attacks
and enhance the robustness of GNNs. Despite the commendable performance of
these defense methods, we have observed that they tend to exhibit a structural
bias in terms of their defense capability on nodes with low degree (i.e., tail
nodes), which is similar to the structural bias of traditional GNNs on nodes
with low degree in the clean graph. Therefore, in this work, we propose a
defense strategy by including hetero-homo augmented graph construction, $k$NN
augmented graph construction, and multi-view node-wise attention modules to
mitigate the structural bias of GNNs against adversarial attacks. Notably, the
hetero-homo augmented graph consists of removing heterophilic links (i.e.,
links connecting nodes with dissimilar features) globally and adding homophilic
links (i.e., links connecting nodes with similar features) for nodes with low
degree. To further enhance the defense capability, an attention mechanism is
adopted to adaptively combine the representations from the above two kinds of
graph views. We conduct extensive experiments to demonstrate the defense and
debiasing effect of the proposed strategy on benchmark datasets.

</details>


### [294] [Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data](https://arxiv.org/abs/2504.20862)
*Dayananda Herurkar, Jörn Hees, Vesselin Tzvetkov, Andreas Dengel*

Main category: cs.LG

TL;DR: The paper introduces Tabular Data Adapters (TDA) to generate soft labels for unlabeled tabular data in outlier detection, leveraging public datasets and autoencoders to address domain shift and labeling challenges.


<details>
  <summary>Details</summary>
Motivation: Challenges in applying deep learning to private datasets include structural differences, domain shift, and lack of labels. TDA aims to mitigate these issues by utilizing public datasets and models.

Method: TDA identifies similar public datasets, transforms private data via a shared autoencoder, and generates weak labels using state-of-the-art public models.

Result: Experiments on 50 datasets show TDA provides more accurate annotations than baselines while reducing computational time.

Conclusion: TDA offers a scalable, efficient solution to bridge public research models and industrial applications in outlier detection.

Abstract: The remarkable success of Deep Learning approaches is often based and
demonstrated on large public datasets. However, when applying such approaches
to internal, private datasets, one frequently faces challenges arising from
structural differences in the datasets, domain shift, and the lack of labels.
In this work, we introduce Tabular Data Adapters (TDA), a novel method for
generating soft labels for unlabeled tabular data in outlier detection tasks.
By identifying statistically similar public datasets and transforming private
data (based on a shared autoencoder) into a format compatible with
state-of-the-art public models, our approach enables the generation of weak
labels. It thereby can help to mitigate the cold start problem of labeling by
basing on existing outlier detection models for public datasets. In experiments
on 50 tabular datasets across different domains, we demonstrate that our method
is able to provide more accurate annotations than baseline approaches while
reducing computational time. Our approach offers a scalable, efficient, and
cost-effective solution, to bridge the gap between public research models and
real-world industrial applications.

</details>


### [295] [Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks](https://arxiv.org/abs/2504.20869)
*Junyuan Fang, Han Yang, Haixian Wen, Jiajing Wu, Zibin Zheng, Chi K. Tse*

Main category: cs.LG

TL;DR: The paper introduces a method to quantify adversarial attack strength on graph neural networks (GNNs) using noise and proposes three attack strategies for improved interpretability.


<details>
  <summary>Details</summary>
Motivation: Current GNNs lack robustness against adversarial attacks, and existing work focuses on attack performance without quantifying perturbation strength, leading to a black-box model.

Method: The authors define noise to measure attack strength and propose three attack strategies based on noise and classification margins, using single and multiple steps optimization.

Result: Experiments on benchmark datasets show the effectiveness of the proposed strategies, with analysis revealing patterns in effective adversarial perturbations.

Conclusion: The work enhances interpretability in adversarial attacks on GNNs by quantifying perturbation strength and identifying preferred perturbation patterns.

Abstract: Graph neural networks have been widely utilized to solve graph-related tasks
because of their strong learning power in utilizing the local information of
neighbors. However, recent studies on graph adversarial attacks have proven
that current graph neural networks are not robust against malicious attacks.
Yet much of the existing work has focused on the optimization objective based
on attack performance to obtain (near) optimal perturbations, but paid less
attention to the strength quantification of each perturbation such as the
injection of a particular node/link, which makes the choice of perturbations a
black-box model that lacks interpretability. In this work, we propose the
concept of noise to quantify the attack strength of each adversarial link.
Furthermore, we propose three attack strategies based on the defined noise and
classification margins in terms of single and multiple steps optimization.
Extensive experiments conducted on benchmark datasets against three
representative graph neural networks demonstrate the effectiveness of the
proposed attack strategies. Particularly, we also investigate the preferred
patterns of effective adversarial perturbations by analyzing the corresponding
properties of the selected perturbation nodes.

</details>


### [296] [Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation](https://arxiv.org/abs/2504.20887)
*Harry Mead, Clarissa Costen, Bruno Lacerda, Nick Hawes*

Main category: cs.LG

TL;DR: Reformulating CVaR optimization by capping trajectory returns improves sample efficiency over baseline methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for CVaR optimization with policy gradients discard many trajectories, leading to poor sample efficiency.

Method: Proposes capping the total return of trajectories during training instead of discarding them, ensuring equivalence to the original problem with appropriate cap settings.

Result: Empirical results show consistently improved performance across various environments.

Conclusion: Capping trajectory returns is a more efficient approach for CVaR optimization with policy gradients.

Abstract: When optimising for conditional value at risk (CVaR) using policy gradients
(PG), current methods rely on discarding a large proportion of trajectories,
resulting in poor sample efficiency. We propose a reformulation of the CVaR
optimisation problem by capping the total return of trajectories used in
training, rather than simply discarding them, and show that this is equivalent
to the original problem if the cap is set appropriately. We show, with
empirical results in an number of environments, that this reformulation of the
problem results in consistently improved performance compared to baselines.

</details>


### [297] [Does Feedback Help in Bandits with Arm Erasures?](https://arxiv.org/abs/2504.20894)
*Merve Karakas, Osama Hanna, Lin F. Yang, Christina Fragouli*

Main category: cs.LG

TL;DR: The paper studies a distributed multi-armed bandit problem over erasure channels, showing that feedback on erasures does not improve worst-case regret bounds but simplifies algorithm design.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the use of MAB algorithms in communication-constrained networks, where erasures can occur during arm transmission.

Method: The study compares two settings: one without feedback on erasures and one with feedback, analyzing regret bounds and designing a simpler algorithm for the latter.

Result: The paper proves that feedback does not improve the worst-case regret order, matching no-feedback bounds, but enables simpler algorithms with potentially better constants.

Conclusion: Feedback on erasures simplifies algorithm design but does not enhance the fundamental regret performance in the worst case.

Abstract: We study a distributed multi-armed bandit (MAB) problem over arm erasure
channels, motivated by the increasing adoption of MAB algorithms over
communication-constrained networks. In this setup, the learner communicates the
chosen arm to play to an agent over an erasure channel with probability
$\epsilon \in [0,1)$; if an erasure occurs, the agent continues pulling the
last successfully received arm; the learner always observes the reward of the
arm pulled. In past work, we considered the case where the agent cannot convey
feedback to the learner, and thus the learner does not know whether the arm
played is the requested or the last successfully received one. In this paper,
we instead consider the case where the agent can send feedback to the learner
on whether the arm request was received, and thus the learner exactly knows
which arm was played. Surprisingly, we prove that erasure feedback does not
improve the worst-case regret upper bound order over the previously studied
no-feedback setting. In particular, we prove a regret lower bound of
$\Omega(\sqrt{KT} + K / (1 - \epsilon))$, where $K$ is the number of arms and
$T$ the time horizon, that matches no-feedback upper bounds up to logarithmic
factors. We note however that the availability of feedback enables simpler
algorithm designs that may achieve better constants (albeit not better order)
regret bounds; we design one such algorithm and evaluate its performance
numerically.

</details>


### [298] [Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking](https://arxiv.org/abs/2504.20900)
*Dayananda Herurkar, Ahmad Ali, Andreas Dengel*

Main category: cs.LG

TL;DR: The paper proposes three new metrics (FAED, FPCAD, RFIS) to evaluate generative models for tabular data, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Generative models for tabular data lack comprehensive evaluation metrics due to structural complexity and mixed data types.

Method: Proposes FAED, FPCAD, and RFIS metrics, tested on network intrusion detection datasets and compared with existing methods.

Result: FAED effectively identifies overlooked issues; FPCAD shows promise but needs refinement.

Conclusion: The framework offers a robust approach for evaluating generative models in tabular data.

Abstract: Generative models have revolutionized multiple domains, yet their application
to tabular data remains underexplored. Evaluating generative models for tabular
data presents unique challenges due to structural complexity, large-scale
variability, and mixed data types, making it difficult to intuitively capture
intricate patterns. Existing evaluation metrics offer only partial insights,
lacking a comprehensive measure of generative performance. To address this
limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS.
Our extensive experimental analysis, conducted on three standard network
intrusion detection datasets, compares these metrics with established
evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results
demonstrate that FAED effectively captures generative modeling issues
overlooked by existing metrics. While FPCAD exhibits promising performance,
further refinements are necessary to enhance its reliability. Our proposed
framework provides a robust and practical approach for assessing generative
models in tabular data applications.

</details>


### [299] [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)
*Tejumade Afonja, Hui-Po Wang, Raouf Kerkouche, Mario Fritz*

Main category: cs.LG

TL;DR: The paper introduces DP-2Stage, a two-stage fine-tuning framework for generating differentially private tabular data using LLMs, addressing inefficiencies in privacy budget allocation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generating coherent tabular data under DP due to inefficient privacy budget allocation, especially for non-private elements like table structures.

Method: Proposes DP-2Stage: non-private fine-tuning on a pseudo dataset followed by DP fine-tuning on a private dataset.

Result: Empirical results show improved performance over direct DP fine-tuning of LLMs.

Conclusion: DP-2Stage effectively enhances tabular data generation under DP constraints, with code publicly available.

Abstract: Generating tabular data under differential privacy (DP) protection ensures
theoretical privacy guarantees but poses challenges for training machine
learning models, primarily due to the need to capture complex structures under
noisy supervision signals. Recently, pre-trained Large Language Models (LLMs)
-- even those at the scale of GPT-2 -- have demonstrated great potential in
synthesizing tabular data. However, their applications under DP constraints
remain largely unexplored. In this work, we address this gap by applying DP
techniques to the generation of synthetic tabular data. Our findings shows that
LLMs face difficulties in generating coherent text when fine-tuned with DP, as
privacy budgets are inefficiently allocated to non-private elements like table
structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning
framework for differentially private tabular data generation. The first stage
involves non-private fine-tuning on a pseudo dataset, followed by DP
fine-tuning on a private dataset. Our empirical results show that this approach
improves performance across various settings and metrics compared to directly
fine-tuned LLMs in DP contexts. We release our code and setup at
https://github.com/tejuafonja/DP-2Stage.

</details>


### [300] [Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning](https://arxiv.org/abs/2504.20988)
*Atul Sharma, Kavindu Herath, Saurabh Bagchi, Chaoyue Liu, Somali Chaterji*

Main category: cs.LG

TL;DR: HSL is a new collaborative ML framework combining FL and P2PL, outperforming ELL with better efficiency and consensus.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of FL (single point of failure) and P2PL (inefficiency), HSL offers a balanced solution.

Method: HSL uses a two-tier communication structure to optimize resource use and avoid FL's weaknesses.

Result: HSL matches ELL's performance with fewer edges (400 vs. 1000) and achieves stronger consensus.

Conclusion: HSL is practical for large-scale, resource-constrained collaborative learning.

Abstract: We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.

</details>


### [301] [MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability](https://arxiv.org/abs/2504.20908)
*Wenxin Chen, Weishen Pan, Kyra Gan, Fei Wang*

Main category: cs.LG

TL;DR: A model-agnostic framework for identifying optimal subgroups under multiple constraints, ensuring clinical relevance and reliability.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in existing methods by unifying subgroup identification with practical constraints like representativeness and confounder balance.

Method: Reformulates the problem as a min-max optimization, solved via gradient descent ascent, ensuring convergence to feasible solutions.

Result: Demonstrates effectiveness in identifying subgroups with higher treatment effects and better confounder balance across datasets.

Conclusion: Proposes a flexible, stable solution for clinically meaningful subgroup identification in personalized medicine.

Abstract: Identifying subgroups that benefit from specific treatments using
observational data is a critical challenge in personalized medicine. Most
existing approaches solely focus on identifying a subgroup with an improved
treatment effect. However, practical considerations, such as ensuring a minimum
subgroup size for representativeness or achieving sufficient confounder balance
for reliability, are also important for making findings clinically meaningful
and actionable. While some studies address these constraints individually, none
offer a unified approach to handle them simultaneously. To bridge this gap, we
propose a model-agnostic framework for optimal subgroup identification under
multiple constraints. We reformulate this combinatorial problem as an
unconstrained min-max optimization problem with novel modifications and solve
it by a gradient descent ascent algorithm. We further prove its convergence to
a feasible and locally optimal solution. Our method is stable and highly
flexible, supporting various models and techniques for estimating and
optimizing treatment effectiveness with observational data. Extensive
experiments on both synthetic and real-world datasets demonstrate its
effectiveness in identifying subgroups that satisfy multiple constraints,
achieving higher treatment effects and better confounder balancing results
across different group sizes.

</details>


### [302] [Toward Efficient Exploration by Large Language Model Agents](https://arxiv.org/abs/2504.20997)
*Dilip Arumugam, Thomas L. Griffiths*

Main category: cs.LG

TL;DR: The paper proposes using LLMs to explicitly implement a known RL algorithm (Posterior Sampling for RL) for data-efficient exploration in natural language tasks, outperforming implicit approaches.


<details>
  <summary>Details</summary>
Motivation: Autonomous decision-making agents powered by LLMs need data-efficient RL, but existing LLM-based designs struggle with exploration, a key challenge in RL.

Method: Instead of finetuning or in-context learning, the authors explicitly implement Posterior Sampling for RL using LLMs, leveraging its well-studied exploration capabilities.

Result: Empirical results show the LLM-based implementation is more effective in natural language tasks requiring prudent exploration.

Conclusion: Explicitly implementing a known RL algorithm with LLMs can enhance exploration efficiency in natural language tasks, outperforming implicit methods.

Abstract: A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.

</details>


### [303] [Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome](https://arxiv.org/abs/2504.20915)
*Milad Leyli-abadi, Jean-Patrick Brunet, Axel Tahmasebimoradi*

Main category: cs.LG

TL;DR: The paper analyzes factors influencing long COVID, using statistical and predictive models to identify key predictors and risk factors, with neural networks performing best.


<details>
  <summary>Details</summary>
Motivation: To understand the prolonged effects of COVID-19 (long COVID) by examining factors like vaccination timing, symptoms, and patient characteristics.

Method: Statistical analysis and predictive modeling (linear models, random forests, gradient boosting, neural networks) using data from the Lifelines COVID-19 cohort.

Result: Neural networks achieved the lowest prediction error (19% MAPE). Key predictors included loss of smell, headache, muscle pain, vaccination timing, chronic disease, and gender.

Conclusion: The study provides insights for understanding long COVID and developing targeted interventions, with neural networks as the most effective predictive tool.

Abstract: Based on recent studies, some COVID-19 symptoms can persist for months after
infection, leading to what is termed long COVID. Factors such as vaccination
timing, patient characteristics, and symptoms during the acute phase of
infection may contribute to the prolonged effects and intensity of long COVID.
Each patient, based on their unique combination of factors, develops a specific
risk or intensity of long COVID. In this work, we aim to achieve two
objectives: (1) conduct a statistical analysis to identify relationships
between various factors and long COVID, and (2) perform predictive analysis of
long COVID intensity using these factors. We benchmark and interpret various
data-driven approaches, including linear models, random forests, gradient
boosting, and neural networks, using data from the Lifelines COVID-19 cohort.
Our results show that Neural Networks (NN) achieve the best performance in
terms of MAPE, with predictions averaging 19\% error. Additionally,
interpretability analysis reveals key factors such as loss of smell, headache,
muscle pain, and vaccination timing as significant predictors, while chronic
disease and gender are critical risk factors. These insights provide valuable
guidance for understanding long COVID and developing targeted interventions.

</details>


### [304] [Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity](https://arxiv.org/abs/2504.20932)
*Taisuke Kobayashi*

Main category: cs.LG

TL;DR: The paper proposes improvements to Dark Experience Replay (DER) and Reservoir Sampling (RS) to better balance memory consolidation and plasticity in continual learning.


<details>
  <summary>Details</summary>
Motivation: Address the tradeoff between memory consolidation and plasticity in DER and RS, which can hinder learning if past outputs are incorrect or new data is underrepresented.

Method: Enhances DER with automatic weight adaptation, blocking erroneous data replay, and correcting past outputs. Improves RS with generalized acceptance probability, stratified buffers, and intentional data omission.

Result: The proposed methods show steady performance improvements across regression, classification, and reinforcement learning benchmarks.

Conclusion: The improvements effectively balance memory consolidation and plasticity, enhancing continual learning performance.

Abstract: Continual learning is the one of the most essential abilities for autonomous
agents, which can incrementally learn daily-life skills. For this ultimate
goal, a simple but powerful method, dark experience replay (DER), has been
proposed recently. DER mitigates catastrophic forgetting, in which the skills
acquired in the past are unintentionally forgotten, by stochastically storing
the streaming data in a reservoir sampling (RS) buffer and by relearning them
or retaining the past outputs for them. However, since DER considers multiple
objectives, it will not function properly without appropriate weighting of
them. In addition, the ability to retain past outputs inhibits learning if the
past outputs are incorrect due to distribution shift or other effects. This is
due to a tradeoff between memory consolidation and plasticity. The tradeoff is
hidden even in the RS buffer, which gradually stops storing new data for new
skills in it as data is continuously passed to it. To alleviate the tradeoff
and achieve better balance, this paper proposes improvement strategies to each
of DER and RS. Specifically, DER is improved with automatic adaptation of
weights, block of replaying erroneous data, and correction of past outputs. RS
is also improved with generalization of acceptance probability, stratification
of plural buffers, and intentional omission of unnecessary data. These
improvements are verified through multiple benchmarks including regression,
classification, and reinforcement learning problems. As a result, the proposed
methods achieve steady improvements in learning performance by balancing the
memory consolidation and plasticity.

</details>


### [305] [Scenario-based Compositional Verification of Autonomous Systems with Neural Perception](https://arxiv.org/abs/2504.20942)
*Christopher Watson, Rajeev Alur, Divya Gopinath, Ravi Mangal, Corina S. Pasareanu*

Main category: cs.LG

TL;DR: A probabilistic verification framework for autonomous systems using scenario-based modeling, probabilistic abstractions, and symbolic reasoning to address DNN complexity and environmental variability.


<details>
  <summary>Details</summary>
Motivation: Formal verification of autonomous systems is difficult due to complex DNNs and changing environments.

Method: Decompose tasks into scenarios, build probabilistic abstractions for each, and use symbolic reasoning with an acceleration proof rule.

Result: Efficient compositional verification demonstrated on autonomous airplane taxiing and F1Tenth car simulations.

Conclusion: The framework enables scalable verification of autonomous systems under varying conditions.

Abstract: Recent advances in deep learning have enabled the development of autonomous
systems that use deep neural networks for perception. Formal verification of
these systems is challenging due to the size and complexity of the perception
DNNs as well as hard-to-quantify, changing environment conditions. To address
these challenges, we propose a probabilistic verification framework for
autonomous systems based on the following key concepts: (1) Scenario-based
Modeling: We decompose the task (e.g., car navigation) into a composition of
scenarios, each representing a different environment condition. (2)
Probabilistic Abstractions: For each scenario, we build a compact abstraction
of perception based on the DNN's performance on an offline dataset that
represents the scenario's environment condition. (3) Symbolic Reasoning and
Acceleration: The abstractions enable efficient compositional verification of
the autonomous system via symbolic reasoning and a novel acceleration proof
rule that bounds the error probability of the system under arbitrary variations
of environment conditions. We illustrate our approach on two case studies: an
experimental autonomous system that guides airplanes on taxiways using
high-dimensional perception DNNs and a simulation model of an F1Tenth
autonomous car using LiDAR observations.

</details>


### [306] [QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing](https://arxiv.org/abs/2302.00671)
*Grace Zhang, Ayush Jain, Injune Hwang, Shao-Hua Sun, Joseph J. Lim*

Main category: cs.LG

TL;DR: Q-switch mixture of policies (QMP) enhances multi-task reinforcement learning by selectively sharing behaviors between tasks, improving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional MTRL methods share parameters or data, but QMP aims to improve off-policy data collection by leveraging behaviors from other tasks.

Method: QMP uses task Q-functions to evaluate and selectively share useful behaviors between tasks, enhancing trajectory quality.

Result: QMP outperforms existing MTRL methods and alternative behavior-sharing approaches in manipulation, locomotion, and navigation tasks.

Conclusion: QMP provides a complementary and effective framework for behavior sharing in MTRL, boosting sample efficiency.

Abstract: Multi-task reinforcement learning (MTRL) aims to learn several tasks
simultaneously for better sample efficiency than learning them separately.
Traditional methods achieve this by sharing parameters or relabeled data
between tasks. In this work, we introduce a new framework for sharing
behavioral policies across tasks, which can be used in addition to existing
MTRL methods. The key idea is to improve each task's off-policy data collection
by employing behaviors from other task policies. Selectively sharing helpful
behaviors acquired in one task to collect training data for another task can
lead to higher-quality trajectories, leading to more sample-efficient MTRL.
Thus, we introduce a simple and principled framework called Q-switch mixture of
policies (QMP) that selectively shares behavior between different task policies
by using the task's Q-function to evaluate and select useful shareable
behaviors. We theoretically analyze how QMP improves the sample efficiency of
the underlying RL algorithm. Our experiments show that QMP's behavioral policy
sharing provides complementary gains over many popular MTRL algorithms and
outperforms alternative ways to share behaviors in various manipulation,
locomotion, and navigation environments. Videos are available at
https://qmp-mtrl.github.io.

</details>


### [307] [Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements](https://arxiv.org/abs/2504.20944)
*Kleanthis Avramidis, Woojae Jeong, Aditya Kommineni, Sudarsana R. Kadiri, Marcus Ma, Colin McDaniel, Myzelle Hughes, Thomas McGee, Elsi Kaiser, Dani Byrd, Assal Habibi, B. Rael Cahn, Idan A. Blank, Kristina Lerman, Takfarinas Medani, Richard M. Leahy, Shrikanth Narayanan*

Main category: cs.LG

TL;DR: Eye tracking shows promise as an objective biomarker for depression and suicidal ideation, with a deep learning model achieving moderate to high accuracy in clinical status prediction.


<details>
  <summary>Details</summary>
Motivation: The lack of objective biomarkers for depression and suicidal ideation motivates the exploration of eye tracking as a potential screening tool.

Method: A deep learning framework analyzed eye-tracking sequences from 126 young adults reading affective sentences, using 2D time-series representations for intra- and inter-trial variations.

Result: The model achieved AUCs of 0.793 for depression/suicidal ideation and 0.826 for suicidality, with moderate accuracy (0.609 AUC) in differentiating depressed from suicidal participants.

Conclusion: Eye tracking is a viable objective tool for mental health assessment, with emotional stimuli significantly influencing oculomotor patterns.

Abstract: Identifying physiological and behavioral markers for mental health conditions
is a longstanding challenge in psychiatry. Depression and suicidal ideation, in
particular, lack objective biomarkers, with screening and diagnosis primarily
relying on self-reports and clinical interviews. Here, we investigate eye
tracking as a potential marker modality for screening purposes. Eye movements
are directly modulated by neuronal networks and have been associated with
attentional and mood-related patterns; however, their predictive value for
depression and suicidality remains unclear. We recorded eye-tracking sequences
from 126 young adults as they read and responded to affective sentences, and
subsequently developed a deep learning framework to predict their clinical
status. The proposed model included separate branches for trials of positive
and negative sentiment, and used 2D time-series representations to account for
both intra-trial and inter-trial variations. We were able to identify
depression and suicidal ideation with an area under the receiver operating
curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and
suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also
exhibited moderate, yet significant, accuracy in differentiating depressed from
suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative
patterns emerge more strongly when assessing the data relative to response
generation than relative to the onset time of the final word of the sentences.
The most pronounced effects were observed for negative-sentiment sentences,
that are congruent to depressed and suicidal participants. Our findings
highlight eye tracking as an objective tool for mental health assessment and
underscore the modulatory impact of emotional stimuli on cognitive processes
affecting oculomotor control.

</details>


### [308] [Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies](https://arxiv.org/abs/2401.10266)
*Maryam Ahang, Todd Charter, Oluwaseyi Ogunfowora, Maziyar Khadivi, Mostafa Abbasi, Homayoun Najjaran*

Main category: cs.LG

TL;DR: A survey of AI-based condition monitoring and fault detection methods for industrial systems, focusing on the Tennessee Eastman Process (TEP), covering DL/ML algorithms, challenges, and performance comparisons.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable and intelligent condition monitoring in industrial systems by leveraging AI techniques.

Method: Summarizes popular and state-of-the-art DL/ML algorithms, evaluates their pros/cons, and compares their performance using the TEP benchmark.

Result: Provides insights into algorithm accuracies, challenges like imbalanced data, and potential solutions.

Conclusion: The survey serves as a resource for both newcomers and experts, highlighting advancements and addressing challenges in industrial condition monitoring.

Abstract: Condition monitoring plays a significant role in the safety and reliability
of modern industrial systems. Artificial intelligence (AI) approaches are
gaining attention from academia and industry as a growing subject in industrial
applications and as a powerful way of identifying faults. This paper provides
an overview of intelligent condition monitoring and fault detection and
diagnosis methods for industrial plants with a focus on the open-source
benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and
state-of-the-art deep learning (DL) and machine learning (ML) algorithms for
industrial plant condition monitoring, fault detection, and diagnosis are
summarized and the advantages and disadvantages of each algorithm are studied.
Challenges like imbalanced data, unlabelled samples and how deep learning
models can handle them are also covered. Finally, a comparison of the
accuracies and specifications of different algorithms utilizing the Tennessee
Eastman Process (TEP) is conducted. This research will be beneficial for both
researchers who are new to the field and experts, as it covers the literature
on condition monitoring and state-of-the-art methods alongside the challenges
and possible solutions to them.

</details>


### [309] [AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security](https://arxiv.org/abs/2504.20965)
*Zikui Cai, Shayan Shabihi, Bang An, Zora Che, Brian R. Bartoldson, Bhavya Kailkhura, Tom Goldstein, Furong Huang*

Main category: cs.LG

TL;DR: AegisLLM is a multi-agent defense system for LLMs, enhancing robustness against adversarial attacks and information leakage without model retraining.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in LLMs, such as adversarial attacks and information leakage, by leveraging cooperative multi-agent reasoning and prompt optimization.

Method: Uses a structured workflow of autonomous agents (orchestrator, deflector, responder, evaluator) and automated prompt optimization (e.g., DSPy) for real-time adaptability.

Result: Achieves near-perfect unlearning with minimal training (20 examples, <300 LM calls) and 51% improvement on jailbreaking benchmarks with low false refusal rates (7.9%).

Conclusion: AegisLLM demonstrates the superiority of adaptive, agentic reasoning over static defenses, offering a runtime alternative to model modifications.

Abstract: We introduce AegisLLM, a cooperative multi-agent defense against adversarial
attacks and information leakage. In AegisLLM, a structured workflow of
autonomous agents - orchestrator, deflector, responder, and evaluator -
collaborate to ensure safe and compliant LLM outputs, while self-improving over
time through prompt optimization. We show that scaling agentic reasoning system
at test-time - both by incorporating additional agent roles and by leveraging
automated prompt optimization (such as DSPy)- substantially enhances robustness
without compromising model utility. This test-time defense enables real-time
adaptability to evolving attacks, without requiring model retraining.
Comprehensive evaluations across key threat scenarios, including unlearning and
jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning
benchmark, AegisLLM achieves near-perfect unlearning with only 20 training
examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve
51% improvement compared to the base model on StrongReject, with false refusal
rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our
results highlight the advantages of adaptive, agentic reasoning over static
defenses, establishing AegisLLM as a strong runtime alternative to traditional
approaches based on model modifications. Code is available at
https://github.com/zikuicai/aegisllm

</details>


### [310] [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
*Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus Müller, Jonas M. Kübler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar*

Main category: cs.LG

TL;DR: Wanda++ is a novel pruning framework for LLMs that uses regional gradients to improve pruning scores and minimize performance loss, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for LLMs often cause performance loss without full-model sparsity-aware fine-tuning. Wanda++ addresses this by leveraging regional gradients.

Method: Wanda++ introduces decoder-block-level regional gradients for pruning scores and an efficient regional optimization method to reduce output discrepancies between dense and sparse models.

Result: Wanda++ improves perplexity by up to 32% over Wanda in language modeling and generalizes well to downstream tasks. It is lightweight, pruning a 7B LLaMA model in under 10 minutes on an H100 GPU.

Conclusion: Wanda++ is an effective and efficient pruning method for LLMs, orthogonal to sparsity-aware fine-tuning, and can be combined with techniques like LoRA for further improvements.

Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for
inference speedup with minimal performance impact. However, existing methods
often suffer from performance loss without full-model sparsity-aware
fine-tuning. This paper presents Wanda++, a novel pruning framework that
outperforms the state-of-the-art methods by utilizing decoder-block-level
\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score
with regional gradients for the first time and proposes an efficient regional
optimization method to minimize pruning-induced output discrepancies between
the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up
to 32\% over Wanda in the language modeling task and generalizes effectively to
downstream tasks. Further experiments indicate our proposed method is
orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with
LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda
method. The proposed method is lightweight, pruning a 7B LLaMA model in under
10 minutes on a single NVIDIA H100 GPU.

</details>


### [311] [Softpick: No Attention Sink, No Massive Activations with Rectified Softmax](https://arxiv.org/abs/2504.20966)
*Zayd M. K. Zuhri, Erland Hilman Fuadi, Alham Fikri Aji*

Main category: cs.LG

TL;DR: Softpick replaces softmax in transformers, eliminating attention sink and massive activations while maintaining performance. It improves sparsity and quantization efficiency.


<details>
  <summary>Details</summary>
Motivation: To address issues like attention sink and massive activations in softmax-based transformers, which hinder performance and efficiency.

Method: Softpick is introduced as a rectified, not sum-to-one alternative to softmax in attention mechanisms, tested on 340M parameter models.

Result: Softpick achieves 0% sink rate, lower kurtosis (340 vs 33,510), and 46.97% sparsity. It outperforms softmax in quantization, especially at lower bit precisions.

Conclusion: Softpick offers potential benefits for quantization, low-precision training, sparsity optimization, pruning, and interpretability.

Abstract: We introduce softpick, a rectified, not sum-to-one, drop-in replacement for
softmax in transformer attention mechanisms that eliminates attention sink and
massive activations. Our experiments with 340M parameter models demonstrate
that softpick maintains performance parity with softmax on standard benchmarks
while achieving 0% sink rate. The softpick transformer produces hidden states
with significantly lower kurtosis (340 vs 33,510) and creates sparse attention
maps (46.97% sparsity). Models using softpick consistently outperform softmax
when quantized, with particularly pronounced advantages at lower bit
precisions. Our analysis and discussion shows how softpick has the potential to
open new possibilities for quantization, low-precision training, sparsity
optimization, pruning, and interpretability. Our code is available at
https://github.com/zaydzuhri/softpick-attention.

</details>


### [312] [Equivariant non-linear maps for neural networks on homogeneous spaces](https://arxiv.org/abs/2504.20974)
*Elias Nyholm, Oscar Carlsson, Maurice Weiler, Daniel Persson*

Main category: cs.LG

TL;DR: A novel framework for non-linear equivariant neural network layers on homogeneous spaces, generalizing steerability constraints and proving universality.


<details>
  <summary>Details</summary>
Motivation: To extend equivariant neural network layers from linear to non-linear settings, inspired by the success of non-linear layers like self-attention.

Method: Derives generalized steerability constraints for non-linear layers and proves the universality of the construction.

Result: Demonstrates how various equivariant architectures (e.g., G-CNNs, transformers) fit into the framework.

Conclusion: Provides insights for designing future equivariant layers and unifies existing architectures under a generalized framework.

Abstract: This paper presents a novel framework for non-linear equivariant neural
network layers on homogeneous spaces. The seminal work of Cohen et al. on
equivariant $G$-CNNs on homogeneous spaces characterized the representation
theory of such layers in the linear setting, finding that they are given by
convolutions with kernels satisfying so-called steerability constraints.
Motivated by the empirical success of non-linear layers, such as self-attention
or input dependent kernels, we set out to generalize these insights to the
non-linear setting. We derive generalized steerability constraints that any
such layer needs to satisfy and prove the universality of our construction. The
insights gained into the symmetry-constrained functional dependence of
equivariant operators on feature maps and group elements informs the design of
future equivariant neural network layers. We demonstrate how several common
equivariant network architectures - $G$-CNNs, implicit steerable kernel
networks, conventional and relative position embedded attention based
transformers, and LieTransformers - may be derived from our framework.

</details>


### [313] [Trustworthiness of Stochastic Gradient Descent in Distributed Learning](https://arxiv.org/abs/2410.21491)
*Hongyang Li, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry*

Main category: cs.LG

TL;DR: Compressed SGD shows higher resistance to privacy leaks (GradInv attacks) than uncompressed SGD, and MIA may not reliably assess privacy risks in distributed learning.


<details>
  <summary>Details</summary>
Motivation: Communication bottlenecks in distributed learning lead to compressed SGD adoption, but its trustworthiness against attacks like GradInv and MIA is unexplored.

Method: Empirical studies comparing compressed and uncompressed SGD under GradInv attacks.

Result: Compressed SGD resists privacy leaks better than uncompressed SGD; MIA is unreliable for privacy risk assessment.

Conclusion: Compressed SGD is more trustworthy against GradInv attacks, but MIA's reliability for privacy evaluation is questionable.

Abstract: Distributed learning (DL) uses multiple nodes to accelerate training,
enabling efficient optimization of large-scale models. Stochastic Gradient
Descent (SGD), a key optimization algorithm, plays a central role in this
process. However, communication bottlenecks often limit scalability and
efficiency, leading to increasing adoption of compressed SGD techniques to
alleviate these challenges. Despite addressing communication overheads,
compressed SGD introduces trustworthiness concerns, as gradient exchanges among
nodes are vulnerable to attacks like gradient inversion (GradInv) and
membership inference attacks (MIA). The trustworthiness of compressed SGD
remains unexplored, leaving important questions about its reliability
unanswered.
  In this paper, we provide a trustworthiness evaluation of compressed versus
uncompressed SGD. Specifically, we conducted empirical studies using GradInv
attacks, revealing that compressed SGD demonstrates significantly higher
resistance to privacy leakage compared to uncompressed SGD. In addition, our
findings suggest that MIA may not be a reliable metric for assessing privacy
risks in distributed learning.

</details>


### [314] [MADGEN: Mass-Spec attends to De Novo Molecular generation](https://arxiv.org/abs/2501.01950)
*Yinkai Wang, Xiaohui Chen, Liping Liu, Soha Hassoun*

Main category: cs.LG

TL;DR: MADGEN is a scaffold-based method for de novo molecular structure generation using MS/MS spectra, improving annotation accuracy by constraining the search space.


<details>
  <summary>Details</summary>
Motivation: The challenge of annotating MS/MS spectra due to molecular diversity and limited reference databases motivates the need for improved methods like MADGEN.

Method: MADGEN operates in two stages: scaffold retrieval using contrastive learning and spectra-conditioned molecular generation with an attention-based model.

Result: Evaluated on NIST23, CANOPUS, and MassSpecGym, MADGEN shows strong performance, especially with an oracle retriever.

Conclusion: MADGEN effectively integrates spectral information to enhance molecular generation accuracy.

Abstract: The annotation (assigning structural chemical identities) of MS/MS spectra
remains a significant challenge due to the enormous molecular diversity in
biological samples and the limited scope of reference databases. Currently, the
vast majority of spectral measurements remain in the "dark chemical space"
without structural annotations. To improve annotation, we propose MADGEN
(Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method
for de novo molecular structure generation guided by mass spectrometry data.
MADGEN operates in two stages: scaffold retrieval and spectra-conditioned
molecular generation starting with the scaffold. In the first stage, given an
MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ
contrastive learning to align mass spectra with candidate molecular scaffolds.
In the second stage, starting from the retrieved scaffold, we employ the MS/MS
spectrum to guide an attention-based generative model to generate the final
molecule. Our approach constrains the molecular generation search space,
reducing its complexity and improving generation accuracy. We evaluate MADGEN
on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's
performance with a predictive scaffold retriever and with an oracle retriever.
We demonstrate the effectiveness of using attention to integrate spectral
information throughout the generation process to achieve strong results with
the oracle retriever.

</details>


### [315] [SR-Reward: Taking The Path More Traveled](https://arxiv.org/abs/2501.02330)
*Seyed Mahdi B. Azad, Zahra Padar, Gabriel Kalweit, Joschka Boedecker*

Main category: cs.LG

TL;DR: A novel method, SR-Reward, learns reward functions from offline demonstrations without adversarial interactions, improving stability and efficiency. It uses successor representation and a negative sampling strategy for robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse reinforcement learning (IRL) involves adversarial interactions between reward functions and policies, leading to instability. This paper aims to decouple these for more stable and efficient training.

Method: The proposed SR-Reward leverages successor representation to encode states based on future state visitation under demonstration policies. It uses the Bellman equation for concurrent learning with RL algorithms and introduces negative sampling to reduce overestimation errors.

Result: Evaluated on the D4RL benchmark, SR-Reward achieves competitive results compared to offline RL with true rewards and imitation learning methods. Ablation studies highlight its advantages and limitations.

Conclusion: SR-Reward offers a stable and efficient alternative to traditional IRL, with competitive performance and robustness, though its effectiveness depends on data size and quality.

Abstract: In this paper, we propose a novel method for learning reward functions
directly from offline demonstrations. Unlike traditional inverse reinforcement
learning (IRL), our approach decouples the reward function from the learner's
policy, eliminating the adversarial interaction typically required between the
two. This results in a more stable and efficient training process. Our reward
function, called \textit{SR-Reward}, leverages successor representation (SR) to
encode a state based on expected future states' visitation under the
demonstration policy and transition dynamics. By utilizing the Bellman
equation, SR-Reward can be learned concurrently with most reinforcement
learning (RL) algorithms without altering the existing training pipeline. We
also introduce a negative sampling strategy to mitigate overestimation errors
by reducing rewards for out-of-distribution data, thereby enhancing robustness.
This strategy inherently introduces a conservative bias into RL algorithms that
employ the learned reward. We evaluate our method on the D4RL benchmark,
achieving competitive results compared to offline RL algorithms with access to
true rewards and imitation learning (IL) techniques like behavioral cloning.
Moreover, our ablation studies on data size and quality reveal the advantages
and limitations of SR-Reward as a proxy for true rewards.

</details>


### [316] [Test-time regression: a unifying framework for designing sequence models with associative memory](https://arxiv.org/abs/2501.12352)
*Ke Alexander Wang, Jiaxin Shi, Emily B. Fox*

Main category: cs.LG

TL;DR: A unifying framework for sequence models is introduced, formalizing associative recall as memorization and retrieval, linking diverse architectures like Transformers and recurrent models.


<details>
  <summary>Details</summary>
Motivation: To address the diversity of seemingly unrelated sequence model architectures and provide a unifying theoretical foundation.

Method: Associative recall is formalized as a two-step process (memorization and retrieval), with memorization framed as a regression problem. Layers combining these steps perform associative recall via test-time regression.

Result: The framework explains existing models (e.g., linear attention, softmax attention) and reveals unexplored design spaces, leading to novel higher-order generalizations of softmax attention.

Conclusion: The work unifies sequence modeling with regression methods, offering a principled approach for developing more powerful architectures.

Abstract: Sequence models lie at the heart of modern deep learning. However, rapid
advancements have produced a diversity of seemingly unrelated architectures,
such as Transformers and recurrent alternatives. In this paper, we introduce a
unifying framework to understand and derive these sequence models, inspired by
the empirical importance of associative recall, the capability to retrieve
contextually relevant tokens. We formalize associative recall as a two-step
process, memorization and retrieval, casting memorization as a regression
problem. Layers that combine these two steps perform associative recall via
``test-time regression'' over its input tokens. Prominent layers, including
linear attention, state-space models, fast-weight programmers, online learners,
and softmax attention, arise as special cases defined by three design choices:
the regression weights, the regressor function class, and the test-time
optimization algorithm. Our approach clarifies how linear attention fails to
capture inter-token correlations and offers a mathematical justification for
the empirical effectiveness of query-key normalization in softmax attention.
Further, it illuminates unexplored regions within the design space, which we
use to derive novel higher-order generalizations of softmax attention. Beyond
unification, our work bridges sequence modeling with classic regression
methods, a field with extensive literature, paving the way for developing more
powerful and theoretically principled architectures.

</details>


### [317] [LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2502.06820)
*Zhekai Du, Yinjie Min, Jingjing Li, Ke Lu, Changliang Zou, Liuhua Peng, Tingjin Chu, Mingming Gong*

Main category: cs.LG

TL;DR: LoCA introduces a frequency-domain fine-tuning method (LoCA) using iDCT, surpassing LoRA by dynamically selecting informative frequency components for better expressivity and efficiency.


<details>
  <summary>Details</summary>
Motivation: LoRA's low-rank decomposition limits hypothesis space; LoCA addresses this by leveraging frequency-domain decomposition for enhanced expressivity.

Method: LoCA uses iDCT with selective learnable components, dynamically choosing frequency components via finite-difference gradient approximation.

Result: LoCA outperforms LoRA in parameter efficiency and expressivity while maintaining computational feasibility.

Conclusion: LoCA provides a superior alternative to LoRA by combining frequency-domain tuning with dynamic component selection.

Abstract: Low-rank adaptation (LoRA) has become a prevalent method for adapting
pre-trained large language models to downstream tasks. However, the simple
low-rank decomposition form may constrain the hypothesis space. To address this
limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel
frequency-domain parameter-efficient fine-tuning method based on inverse
Discrete Cosine Transform (iDCT) with selective locations of learnable
components. We begin with a comprehensive theoretical comparison between
frequency-domain and low-rank decompositions for fine-tuning pre-trained large
models. Our analysis reveals that frequency-domain decomposition with carefully
selected frequency components can surpass the expressivity of traditional
low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more
efficient implementation compared to inverse Discrete Fourier Transform (iDFT),
allowing for better selection and tuning of frequency components while
maintaining equivalent expressivity to the optimal iDFT-based adaptation. By
employing finite-difference approximation to estimate gradients for discrete
locations of learnable coefficients on the DCT spectrum, LoCA dynamically
selects the most informative frequency components during training. Experiments
on diverse language and vision fine-tuning tasks demonstrate that LoCA offers
enhanced parameter efficiency while maintains computational feasibility
comparable to low-rank-based methods.

</details>


### [318] [Towards Principled Multi-Agent Task Agnostic Exploration](https://arxiv.org/abs/2502.08365)
*Riccardo Zamboni, Mirco Mutti, Marcello Restelli*

Main category: cs.LG

TL;DR: The paper explores task-agnostic exploration in multi-agent reinforcement learning, proposing a decentralized algorithm to maximize state distribution entropy.


<details>
  <summary>Details</summary>
Motivation: Little is known about task-agnostic exploration in multi-agent settings, despite its real-world ubiquity. The paper aims to address this gap.

Method: The authors generalize the single-agent entropy maximization approach to multi-agent settings, investigate alternative formulations, and introduce a scalable, decentralized trust-region policy search algorithm.

Result: Proof-of-concept experiments validate the theoretical findings and demonstrate feasibility for challenging multi-agent scenarios.

Conclusion: The work advances understanding of task-agnostic exploration in multi-agent systems, providing a practical algorithm and foundational insights.

Abstract: In reinforcement learning, we typically refer to task-agnostic exploration
when we aim to explore the environment without access to the task specification
a priori. In a single-agent setting the problem has been extensively studied
and mostly understood. A popular approach cast the task-agnostic objective as
maximizing the entropy of the state distribution induced by the agent's policy,
from which principles and methods follows. In contrast, little is known about
task-agnostic exploration in multi-agent settings, which are ubiquitous in the
real world. How should different agents explore in the presence of others? In
this paper, we address this question through a generalization to multiple
agents of the problem of maximizing the state distribution entropy. First, we
investigate alternative formulations, highlighting respective positives and
negatives. Then, we present a scalable, decentralized, trust-region policy
search algorithm to address the problem in practical settings. Finally, we
provide proof of concept experiments to both corroborate the theoretical
findings and pave the way for task-agnostic exploration in challenging
multi-agent settings.

</details>


### [319] [FuncGenFoil: Airfoil Generation and Editing Model in Function Space](https://arxiv.org/abs/2502.10712)
*Jinouwen Zhang, Junjie Ren, Aobo Yang, Yan Lu, Lu Chen, Hairun Xie, Jing Wang, Miao Zhang, Wanli Ouyang, Shixiang Tang*

Main category: cs.LG

TL;DR: FuncGenFoil introduces a function-space generative model for high-fidelity airfoil design, outperforming existing methods in expressiveness and flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods for airfoil geometry generation face trade-offs between expressiveness and resolution flexibility.

Method: FuncGenFoil, a function-space generative model, combines arbitrary resolution sampling, smoothness of parametric functions, and expressiveness of discrete point-based functions.

Result: Empirical results show a -74.4% label error reduction and +23.2% diversity increase on the AF-200K dataset.

Conclusion: FuncGenFoil offers a powerful framework for aerodynamic shape optimization, enhancing airfoil design flexibility and fidelity.

Abstract: Aircraft manufacturing is the jewel in the crown of industry, among which
generating high-fidelity airfoil geometries with controllable and editable
representations remains a fundamental challenge. While existing
deep-learning-based methods rely on predefined parametric function families,
e.g., B\'ezier curves and discrete point-based representations, they suffer
from inherent trade-offs between expressiveness and resolution flexibility. To
tackle this challenge, we introduce FuncGenFoil, a novel function-space
generative model that directly learns functional airfoil geometries. Our method
inherits both the advantages of arbitrary resolution sampling and the
smoothness of parametric functions, as well as the strong expressiveness of
discrete point-based functions. Empirical evaluations on the AFBench dataset
demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil
generation by achieving a relative -74.4 label error reduction and +23.2
diversity increase on the AF-200K dataset. Our results highlight the advantages
of function-space modeling for aerodynamic shape optimization, offering a
powerful and flexible framework for high-fidelity airfoil design. Our code will
be released.

</details>


### [320] [Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning](https://arxiv.org/abs/2502.18773)
*Yuqing Wang, Xiao Yang*

Main category: cs.LG

TL;DR: A DRL-based approach optimizes resource scheduling in edge-cloud computing, improving efficiency and reducing processing time, though further enhancements are needed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of resource scheduling optimization in edge-cloud collaborative computing to improve task processing efficiency and resource utilization.

Method: Utilizes deep reinforcement learning (DRL) to optimize scheduling, focusing on task allocation, dynamic workloads, and resource constraints.

Result: DRL outperforms traditional algorithms in efficiency, processing time, and resource utilization, but faces issues like training time and convergence.

Conclusion: Future work should enhance learning efficiency, fault tolerance, and adaptability to complex scenarios for smarter edge-cloud systems.

Abstract: This study addresses the challenge of resource scheduling optimization in
edge-cloud collaborative computing using deep reinforcement learning (DRL). The
proposed DRL-based approach improves task processing efficiency, reduces
overall processing time, enhances resource utilization, and effectively
controls task migrations. Experimental results demonstrate the superiority of
DRL over traditional scheduling algorithms, particularly in managing complex
task allocation, dynamic workloads, and multiple resource constraints. Despite
its advantages, further improvements are needed to enhance learning efficiency,
reduce training time, and address convergence issues. Future research should
focus on increasing the algorithm's fault tolerance to handle more complex and
uncertain scheduling scenarios, thereby advancing the intelligence and
efficiency of edge-cloud computing systems.

</details>


### [321] [One-Shot Clustering for Federated Learning](https://arxiv.org/abs/2503.04231)
*Maciej Krzysztof Zuziak, Roberto Pellungrini, Salvatore Rinzivillo*

Main category: cs.LG

TL;DR: One-Shot Clustered Federated Learning (OCFL) automates client clustering in FL by detecting the optimal moment using gradient similarity and convergence metrics, showing strong performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Standard FL lacks automated clustering for personalized models, and existing CFL methods require hyperparameter tuning. OCFL addresses this gap.

Method: OCFL uses cosine similarity between client gradients and a temperature measure to detect convergence, enabling one-shot clustering without hyperparameters.

Result: OCFL performs well across 30+ tasks on three datasets, demonstrating effective automated clustering in FL.

Conclusion: OCFL is a promising, hyperparameter-free solution for automated clustering in FL, advancing personalized model training.

Abstract: Federated Learning (FL) is a widespread and well adopted paradigm of
decentralized learning that allows training one model from multiple sources
without the need to directly transfer data between participating clients. Since
its inception in 2015, it has been divided into numerous sub-fields that deal
with application-specific issues, be it data heterogeneity or resource
allocation. One such sub-field, Clustered Federated Learning (CFL), is dealing
with the problem of clustering the population of clients into separate cohorts
to deliver personalized models. Although few remarkable works have been
published in this domain, the problem is still largely unexplored, as its basic
assumption and settings are slightly different from standard FL. In this work,
we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic
algorithm that can automatically detect the earliest suitable moment for
clustering. Our algorithm is based on the computation of cosine similarity
between gradients of the clients and a temperature measure that detects when
the federated model starts to converge. We empirically evaluate our methodology
by testing various one-shot clustering algorithms for over thirty different
tasks on three benchmark datasets. Our experiments showcase the good
performance of our approach when used to perform CFL in an automated manner
without the need to adjust hyperparameters.

</details>


### [322] [Training Plug-n-Play Knowledge Modules with Deep Context Distillation](https://arxiv.org/abs/2503.08727)
*Lucas Caccia, Alan Ansell, Edoardo Ponti, Ivan Vulić, Alessandro Sordoni*

Main category: cs.LG

TL;DR: The paper proposes Knowledge Modules (KMs), lightweight LoRA modules for dynamically integrating new document knowledge into language models, outperforming traditional methods like next-token prediction.


<details>
  <summary>Details</summary>
Motivation: Challenges in updating language models with new or specialized information, especially in low-data or private scenarios, motivate the need for efficient modular solutions.

Method: Train document-level KMs using Deep Context Distillation to simulate teacher model hidden states and logits, avoiding next-token prediction.

Result: KMs outperform next-token prediction and pre-instruction training across two datasets, showing synergies with RAG.

Conclusion: KMs offer an efficient, modular approach for integrating new knowledge into language models, complementing existing methods like RAG.

Abstract: Dynamically integrating new or rapidly evolving information after (Large)
Language Model pre-training remains challenging, particularly in low-data
scenarios or when dealing with private and specialized documents. In-context
learning and retrieval-augmented generation (RAG) face limitations, including
their high inference costs and their inability to capture global document
information. In this paper, we propose a way of modularizing knowledge by
training document-level Knowledge Modules (KMs). KMs are lightweight components
implemented as parameter-efficient LoRA modules, which are trained to store
information about new documents and can be easily plugged into models on
demand. We show that next-token prediction performs poorly as the training
objective for KMs. We instead propose Deep Context Distillation: we learn KMs
parameters such as to simulate hidden states and logits of a teacher that takes
the document in context. Our method outperforms standard next-token prediction
and pre-instruction training techniques, across two datasets. Finally, we
highlight synergies between KMs and RAG.

</details>


### [323] [Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting](https://arxiv.org/abs/2503.22809)
*Uddhav Bhattarai, Rajkishan Arikapudi, Steven A. Fennimore, Frank N Martin, Stavros G. Vougioukas*

Main category: cs.LG

TL;DR: A system using instrumented carts and a CNN-LSTM model was developed to classify strawberry pickers' activities, achieving high accuracy in estimating efficiency and tray fill time.


<details>
  <summary>Details</summary>
Motivation: Manual fruit harvesting is inefficient due to non-productive activities. Accurately identifying picking vs. non-picking activities can optimize labor management.

Method: Instrumented carts recorded fruit weight, geolocation, and movement. Data trained a CNN-LSTM model to classify activities as 'Pick' or 'NoPick'.

Result: The model achieved an F1 score of 0.97. Average picker efficiency was 75.07% (95.22% accuracy), and tray fill time was 6.79 minutes (96.43% accuracy).

Conclusion: The technology can optimize harvests by reducing non-productive time and improving efficiency.

Abstract: Manual fruit harvesting is common in agriculture, but the amount of time
pickers spend on non-productive activities can make it very inefficient.
Accurately identifying picking vs. non-picking activity is crucial for
estimating picker efficiency and optimising labour management and harvest
processes. In this study, a practical system was developed to calculate the
efficiency of pickers in commercial strawberry harvesting. Instrumented picking
carts were developed to record the harvested fruit weight, geolocation, and
cart movement in real time. These carts were deployed during the commercial
strawberry harvest season in Santa Maria, CA. The collected data was then used
to train a CNN-LSTM-based deep neural network to classify a picker's activity
into "Pick" and "NoPick" classes. Experimental evaluations showed that the
CNN-LSTM model showed promising activity recognition performance with an F1
score accuracy of over 0.97. The recognition results were then used to compute
picker efficiency and the time required to fill a tray. Analysis of the
season-long harvest data showed that the average picker efficiency was 75.07%
with an estimation accuracy of 95.22%. Furthermore, the average tray fill time
was 6.79 minutes with an estimation accuracy of 96.43%. When integrated into
commercial harvesting, the proposed technology can aid growers in monitoring
automated worker activity and optimising harvests to reduce non-productive time
and enhance overall harvest efficiency.

</details>


### [324] [Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets](https://arxiv.org/abs/2412.07775)
*Zhen Liu, Tim Z. Xiao, Weiyang Liu, Yoshua Bengio, Dinghuai Zhang*

Main category: cs.LG

TL;DR: The paper proposes Nabla-GFlowNet, a reinforcement learning method for finetuning diffusion models, addressing diversity, prior preservation, and convergence issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reward finetuning of diffusion models lack diversity, prior preservation, and slow convergence.

Method: The paper introduces $
abla$-GFlowNet, leveraging reward gradients for probabilistic diffusion finetuning.

Result: The method achieves fast, diversity- and prior-preserving finetuning of Stable Diffusion on realistic reward functions.

Conclusion: Nabla-GFlowNet effectively addresses key challenges in diffusion model finetuning.

Abstract: While one commonly trains large diffusion models by collecting datasets on
target downstream tasks, it is often desired to align and finetune pretrained
diffusion models with some reward functions that are either designed by experts
or learned from small-scale datasets. Existing post-training methods for reward
finetuning of diffusion models typically suffer from lack of diversity in
generated samples, lack of prior preservation, and/or slow convergence in
finetuning. In response to this challenge, we take inspiration from recent
successes in generative flow networks (GFlowNets) and propose a reinforcement
learning method for diffusion model finetuning, dubbed Nabla-GFlowNet
(abbreviated as $\nabla$-GFlowNet), that leverages the rich signal in reward
gradients for probabilistic diffusion finetuning. We show that our proposed
method achieves fast yet diversity- and prior-preserving finetuning of Stable
Diffusion, a large-scale text-conditioned image diffusion model, on different
realistic reward functions.

</details>


### [325] [Activated LoRA: Fine-tuned LLMs for Intrinsics](https://arxiv.org/abs/2504.12397)
*Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox*

Main category: cs.LG

TL;DR: aLoRA improves LoRA by enabling instant activation without recomputing KV cache, enhancing efficiency in multiturn settings.


<details>
  <summary>Details</summary>
Motivation: Switching between LoRAs in multiturn settings is inefficient due to KV cache recomputation.

Method: aLoRA adapts weights only for tokens after invocation, accepting the base model's KV cache.

Result: Competitive accuracy with standard LoRA and significant inference benefits.

Conclusion: aLoRA enables efficient, specialized model invocation in multiturn conversations.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for
finetuning the weights of large foundation models, and has become the go-to
method for data-driven customization of LLMs. Despite the promise of highly
customized behaviors and capabilities, switching between relevant LoRAs in a
multiturn setting is highly inefficient, as the key-value (KV) cache of the
entire turn history must be recomputed with the LoRA weights before generation
can begin. To address this problem, we propose Activated LoRA (aLoRA), which
modifies the LoRA framework to only adapt weights for the tokens in the
sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA
to accept the base model's KV cache of the input string, meaning that aLoRA can
be instantly activated whenever needed in a chain without recomputing the
cache. This enables building what we call \emph{intrinsics}, i.e. highly
specialized models invoked to perform well-defined operations on portions of an
input chain or conversation that otherwise uses the base model by default. We
use aLoRA to train a set of intrinsics models, demonstrating competitive
accuracy with standard LoRA while achieving significant inference benefits.

</details>


### [326] [\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks](https://arxiv.org/abs/2504.19013)
*Júlia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Physics-Informed Neural Networks (PINNs) are a novel computational approach
for solving partial differential equations (PDEs) with noisy and sparse initial
and boundary data. Although, efficient quantification of epistemic and
aleatoric uncertainties in big multi-scale problems remains challenging. We
propose \$PINN a novel method of computing global uncertainty in PDEs using a
Bayesian framework, by combining local Bayesian Physics-Informed Neural
Networks (BPINN) with domain decomposition. The solution continuity across
subdomains is obtained by imposing the flux continuity across the interface of
neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct
a series of computational experiments on PDEs in 1D and 2D spatial domains.
Although we have adopted conservative PINNs (cPINNs), the method can be
seamlessly extended to other domain decomposition techniques. The results infer
that the proposed method recovers the global uncertainty by computing the local
uncertainty exactly more efficiently as the uncertainty in each subdomain can
be computed concurrently. The robustness of \$PINN is verified by adding
uncorrelated random noise to the training data up to 15% and testing for
different domain sizes.

</details>


### [327] [Gaussian Pre-Activations in Neural Networks: Myth or Reality?](https://arxiv.org/abs/2205.12379)
*Pierre Wolinski, Julyan Arbel*

Main category: cs.LG

TL;DR: The paper constructs a family of activation functions and initialization distributions ensuring Gaussian pre-activations in finite-width neural networks, reviews Edge of Chaos claims, and provides a unified view on initialization procedures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of the Gaussian assumption for pre-activations in finite-width neural networks and provide a principled framework for initialization.

Method: Constructs pairs of activation functions and initialization distributions, analyzes constraints for Gaussian pre-activations, and reviews Edge of Chaos claims.

Result: Ensures Gaussian pre-activations in narrow networks, offers exact Edge of Chaos analysis, and unifies initialization frameworks.

Conclusion: Provides a principled answer to whether Gaussian pre-activations are desirable for initialization, with practical implications for neural network training.

Abstract: The study of feature propagation at initialization in neural networks lies at
the root of numerous initialization designs. An assumption very commonly made
in the field states that the pre-activations are Gaussian. Although this
convenient Gaussian hypothesis can be justified when the number of neurons per
layer tends to infinity, it is challenged by both theoretical and experimental
works for finite-width neural networks. Our major contribution is to construct
a family of pairs of activation functions and initialization distributions that
ensure that the pre-activations remain Gaussian throughout the network's depth,
even in narrow neural networks. In the process, we discover a set of
constraints that a neural network should fulfill to ensure Gaussian
pre-activations. Additionally, we provide a critical review of the claims of
the Edge of Chaos line of works and build an exact Edge of Chaos analysis. We
also propose a unified view on pre-activations propagation, encompassing the
framework of several well-known initialization procedures. Finally, our work
provides a principled framework for answering the much-debated question: is it
desirable to initialize the training of a neural network whose pre-activations
are ensured to be Gaussian? Our code is available on GitHub:
https://github.com/p-wol/gaussian-preact/ .

</details>


### [328] [What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization](https://arxiv.org/abs/2503.06698)
*Xavier Thomas, Deepti Ghadiyaram*

Main category: cs.LG

TL;DR: A method improves domain generalization by leveraging pre-trained features and pseudo-domains, outperforming baselines by up to 4% in test accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance model generalization to unseen data distributions by studying architecture and pre-training impacts.

Method: Discover latent pseudo-domains from pre-trained features and augment classifiers with these representations.

Result: Features from diffusion models excel at domain separation; method improves generalization by up to 4% over ERM.

Conclusion: The framework effectively leverages pseudo-domains for better domain generalization, even outperforming domain-label-based methods.

Abstract: Domain Generalization aims to develop models that can generalize to novel and
unseen data distributions. In this work, we study how model architectures and
pre-training objectives impact feature richness and propose a method to
effectively leverage them for domain generalization. Specifically, given a
pre-trained feature space, we first discover latent domain structures, referred
to as pseudo-domains, that capture domain-specific variations in an
unsupervised manner. Next, we augment existing classifiers with these
complementary pseudo-domain representations making them more amenable to
diverse unseen test domains. We analyze how different pre-training feature
spaces differ in the domain-specific variances they capture. Our empirical
studies reveal that features from diffusion models excel at separating domains
in the absence of explicit domain labels and capture nuanced domain-specific
information. On 5 datasets, we show that our very simple framework improves
generalization to unseen domains by a maximum test accuracy improvement of over
4% compared to the standard baseline Empirical Risk Minimization (ERM).
Crucially, our method outperforms most algorithms that access domain labels
during training.

</details>


### [329] [Settling the Sample Complexity of Online Reinforcement Learning](https://arxiv.org/abs/2307.13586)
*Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du*

Main category: cs.LG

TL;DR: The paper addresses the challenge of achieving minimax-optimal regret in online RL without burn-in cost, proposing a modified MVP algorithm that matches the lower bound for all sample sizes.


<details>
  <summary>Details</summary>
Motivation: To eliminate the burn-in cost in online RL and achieve minimax-optimal regret for finite-horizon inhomogeneous MDPs.

Method: A modified version of Monotonic Value Propagation (MVP), analyzed with a new regret decomposition strategy and statistical dependency decoupling.

Result: The algorithm achieves a regret of min{√(SAH³K), HK}, matching the minimax lower bound for all K ≥ 1, and a minimax-optimal PAC sample complexity.

Conclusion: The work solves the open problem of burn-in cost in RL, providing a minimax-optimal solution with broad implications for problem-dependent quantities.

Abstract: A central issue lying at the heart of online reinforcement learning (RL) is
data efficiency. While a number of recent works achieved asymptotically minimal
regret in online RL, the optimality of these results is only guaranteed in a
``large-sample'' regime, imposing enormous burn-in cost in order for their
algorithms to operate optimally. How to achieve minimax-optimal regret without
incurring any burn-in cost has been an open problem in RL theory.
  We settle this problem for the context of finite-horizon inhomogeneous Markov
decision processes. Specifically, we prove that a modified version of Monotonic
Value Propagation (MVP), a model-based algorithm proposed by
\cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log
factors) \begin{equation*}
  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the
number of states, $A$ is the number of actions, $H$ is the planning horizon,
and $K$ is the total number of episodes. This regret matches the minimax lower
bound for the entire range of sample size $K\geq 1$, essentially eliminating
any burn-in requirement. It also translates to a PAC sample complexity (i.e.,
the number of episodes needed to yield $\varepsilon$-accuracy) of
$\frac{SAH^3}{\varepsilon^2}$ up to log factor, which is minimax-optimal for
the full $\varepsilon$-range.
  Further, we extend our theory to unveil the influences of problem-dependent
quantities like the optimal value/cost and certain variances. The key technical
innovation lies in the development of a new regret decomposition strategy and a
novel analysis paradigm to decouple complicated statistical dependency -- a
long-standing challenge facing the analysis of online RL in the sample-hungry
regime.

</details>


### [330] [Common pitfalls to avoid while using multiobjective optimization in machine learning](https://arxiv.org/abs/2405.01480)
*Junaid Akhter, Paul David Fährmann, Konstantin Sonntag, Sebastian Peitz, Daniel Schwietert*

Main category: cs.LG

TL;DR: The paper provides an entry-level guide for ML practitioners on multiobjective optimization (MOO), highlighting pitfalls and comparing methods like weighted sum, MGDA, and NSGA-II.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of accessible literature on MOO for ML practitioners and the need to avoid common pitfalls in its application.

Method: Reviews MOO approaches (e.g., weighted sum, MGDA) and compares them with NSGA-II, using PINNs as a case study.

Result: Demonstrates challenges in MOO application, emphasizing the need for problem-specific understanding and proper convergence criteria.

Conclusion: Highlights the importance of careful MOO method selection and awareness of pitfalls to ensure accurate and effective optimization in ML.

Abstract: Recently, there has been an increasing interest in the application of
multiobjective optimization (MOO) in machine learning (ML). This interest is
driven by the numerous real-life situations where multiple objectives must be
optimized simultaneously. A key aspect of MOO is the existence of a Pareto set,
rather than a single optimal solution, which represents the optimal trade-offs
between different objectives. Despite its potential, there is a noticeable lack
of satisfactory literature serving as an entry-level guide for ML practitioners
aiming to apply MOO effectively. In this paper, our goal is to provide such a
resource and highlight pitfalls to avoid. We begin by establishing the
groundwork for MOO, focusing on well-known approaches such as the weighted sum
(WS) method, alongside more advanced techniques like the multiobjective
gradient descent algorithm (MGDA). We critically review existing studies across
various ML fields where MOO has been applied and identify challenges that can
lead to incorrect interpretations. One of these fields is physics informed
neural networks (PINNs), which we use as a guiding example to carefully
construct experiments illustrating these pitfalls. By comparing WS and MGDA
with one of the most common evolutionary algorithms, NSGA-II, we demonstrate
that difficulties can arise regardless of the specific MOO method used. We
emphasize the importance of understanding the specific problem, the objective
space, and the selected MOO method, while also noting that neglecting factors
such as convergence criteria can result in misleading experiments.

</details>


### [331] [Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning](https://arxiv.org/abs/2504.10143)
*Yichao Cai, Yuhang Liu, Erdun Gao, Tianjiao Jiang, Zhen Zhang, Anton van den Hengel, Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: The paper reconciles opposing views on misalignment in multimodal contrastive learning (MMCL), formalizing it via latent variable models and offering practical insights for ML system design.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of misalignment in real-world datasets for MMCL, reconciling mitigation and leveraging perspectives.

Method: Uses latent variable models to formalize misalignment (selection and perturbation biases) and analyzes MMCL's representation learning under these biases.

Result: MMCL captures semantic variables invariant to biases, providing a unified understanding of misalignment. Empirical validation supports the theory.

Conclusion: Offers actionable insights for ML system design, validated by synthetic and real-world data, clarifying misalignment's impact.

Abstract: Multimodal representation learning, exemplified by multimodal contrastive
learning (MMCL) using image-text pairs, aims to learn powerful representations
by aligning cues across modalities. This approach relies on the core assumption
that the exemplar image-text pairs constitute two representations of an
identical concept. However, recent research has revealed that real-world
datasets often exhibit misalignment. There are two distinct viewpoints on how
to address this issue: one suggests mitigating the misalignment, and the other
leveraging it. We seek here to reconcile these seemingly opposing perspectives,
and to provide a practical guide for practitioners. Using latent variable
models we thus formalize misalignment by introducing two specific mechanisms:
selection bias, where some semantic variables are missing, and perturbation
bias, where semantic variables are distorted -- both affecting latent variables
shared across modalities. Our theoretical analysis demonstrates that, under
mild assumptions, the representations learned by MMCL capture exactly the
information related to the subset of the semantic variables invariant to
selection and perturbation biases. This provides a unified perspective for
understanding misalignment. Based on this, we further offer actionable insights
into how misalignment should inform the design of real-world ML systems. We
validate our theoretical findings through extensive empirical studies on both
synthetic data and real image-text datasets, shedding light on the nuanced
impact of misalignment on multimodal representation learning.

</details>


### [332] [Scalable Surrogate Verification of Image-based Neural Network Control Systems using Composition and Unrolling](https://arxiv.org/abs/2405.18554)
*Feiyang Cai, Chuchu Fan, Stanley Bak*

Main category: cs.LG

TL;DR: The paper proposes methods to improve the verification of neural network control systems using images by reducing overapproximation errors in single and multi-step analyses, enhancing scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Verifying safety in neural network control systems with image inputs is challenging due to the lack of mathematical models for real-world image possibilities.

Method: The approach uses a conditional GAN (cGAN) as an image surrogate, composes system dynamics with the cGAN and controller, and unrolls multiple control steps into a large neural network for verification.

Result: The method significantly reduces reachable set sizes (e.g., 175% smaller in an aircraft taxiing system) and enables safety analysis where baseline methods fail (e.g., in an emergency braking system).

Conclusion: The proposed techniques improve the accuracy and scalability of formal verification for neural network control systems with image inputs.

Abstract: Verifying safety of neural network control systems that use images as input
is a difficult problem because, from a given system state, there is no known
way to mathematically model what images are possible in the real-world. We
build on recent work that considers a surrogate verification approach, training
a conditional generative adversarial network (cGAN) as an image generator in
place of the real world. This enables set-based formal analysis of the
closed-loop system, providing analysis beyond simulation and testing. While
existing work is effective on small examples, excessive overapproximation both
within a single control period and across multiple control periods limits its
scalability. We propose approaches to overcome these two sources of error.
First, we overcome one-step error by composing the system's dynamics along with
the cGAN and neural network controller, without losing the dependencies between
input states and the control outputs as in the monotonic analysis of the system
dynamics. Second, we reduce multi-step error by repeating the single-step
composition, essentially unrolling multiple steps of the control loop into a
large neural network. We then leverage existing network verification tools to
compute accurate reachable sets for multiple steps, avoiding the accumulation
of abstraction error at each step. We demonstrate the effectiveness of our
approach in terms of both accuracy and scalability using two case studies: an
autonomous aircraft taxiing system and an advanced emergency braking system. On
the aircraft taxiing system, the converged reachable set is 175% larger using
the prior baseline method compared with our proposed approach. On the emergency
braking system, with 24x the number of image output variables from the cGAN,
the baseline method fails to prove any states are safe, whereas our
improvements enable set-based safety analysis.

</details>


### [333] [Seamless Monitoring of Stress Levels Leveraging a Universal Model for Time Sequences](https://arxiv.org/abs/2407.03821)
*Davide Gabrielli, Bardh Prenkaj, Paola Velardi*

Main category: cs.LG

TL;DR: A methodology for stress detection using a wristband-based universal model (UniTS) is proposed, outperforming 12 top methods and enabling seamless monitoring with explainability.


<details>
  <summary>Details</summary>
Motivation: To improve stress monitoring in neurodegenerative disease patients by using less invasive methods like wristbands, addressing the limitations of current tools.

Method: Uses UniTS, a universal model for time series, finetuned for stress detection, framed as anomaly detection for adaptability and clinician control.

Result: Outperforms 12 top methods on three benchmark datasets and works comparably with both invasive and lightweight devices.

Conclusion: UniTS offers a non-invasive, adaptable, and high-performing solution for stress detection in patients, with potential for clinical integration.

Abstract: Monitoring the stress level in patients with neurodegenerative diseases can
help manage symptoms, improve patient's quality of life, and provide insight
into disease progression. In the literature, ECG, actigraphy, speech, voice,
and facial analysis have proven effective at detecting patients' emotions. On
the other hand, these tools are invasive and do not integrate smoothly into the
patient's daily life. HRV has also been proven to effectively indicate stress
conditions, especially in combination with other signals. However, when HRV is
derived from less invasive devices than the ECG, like wristbands and
smartwatches, the quality of measurements significantly degrades. This paper
presents a methodology for stress detection from a wristband based on a
universal model for time series, UniTS, which we finetuned for the task and
equipped with explainability features. We cast the problem as anomaly detection
rather than classification to favor model adaptation to individual patients and
allow the clinician to maintain greater control over the system's predictions.
We demonstrate that our proposed model considerably surpasses 12 top-performing
methods on three benchmark datasets. Furthermore, unlike other state-of-the-art
systems, UniTS enables seamless monitoring, as it shows comparable performance
when using signals from invasive or lightweight devices.

</details>


### [334] [A Self-organizing Interval Type-2 Fuzzy Neural Network for Multi-Step Time Series Prediction](https://arxiv.org/abs/2407.08010)
*Fulong Yao, Wanqing Zhao, Matthew Forshaw, Yang Song*

Main category: cs.LG

TL;DR: The paper proposes a self-organizing interval type-2 fuzzy neural network (SOIT2FNN-MO) for multi-step time series predictions, improving accuracy, interpretability, and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: Data uncertainty in real-world applications complicates multi-step predictions, and existing IT2FNN models struggle with interpretability and accuracy in this context.

Method: A nine-layer network architecture with new layers (co-antecedent, link, transformation) and a two-stage self-organizing learning mechanism is introduced.

Result: SOIT2FNN-MO outperforms state-of-the-art methods by 1.6% to 30% in accuracy and enhances interpretability.

Conclusion: The proposed model effectively addresses multi-step prediction challenges, offering improved performance and insights.

Abstract: Data uncertainty is inherent in many real-world applications and poses
significant challenges for accurate time series predictions. The interval type
2 fuzzy neural network (IT2FNN) has shown exceptional performance in
uncertainty modelling for single-step prediction tasks. However, extending it
for multi-step ahead predictions introduces further issues in uncertainty
handling as well as model interpretability and accuracy. To address these
issues, this paper proposes a new selforganizing interval type-2 fuzzy neural
network with multiple outputs (SOIT2FNN-MO). Differing from the traditional
six-layer IT2FNN, a nine-layer network architecture is developed. First, a new
co-antecedent layer and a modified consequent layer are devised to improve the
interpretability of the fuzzy model for multi-step time series prediction
problems. Second, a new link layer is created to improve the accuracy by
building temporal connections between multi-step predictions. Third, a new
transformation layer is designed to address the problem of the vanishing rule
strength caused by high-dimensional inputs. Furthermore, a two-stage,
self-organizing learning mechanism is developed to automatically extract fuzzy
rules from data and optimize network parameters. Experimental results on
chaotic and microgrid prediction problems demonstrate that SOIT2FNN-MO
outperforms state-of-the-art methods, by achieving a better accuracy ranging
from 1.6% to 30% depending on the level of noises in data. Additionally, the
proposed model is more interpretable, offering deeper insights into the
prediction process.

</details>


### [335] [Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems: An application on Hydrological Systems](https://arxiv.org/abs/2407.20152)
*Rahul Ghosh, Arvind Renganathan, Zac McEachran, Kelly Lindsay, Somya Sharma, Michael Steinbach, John Nieber, Christopher Duffy, Vipin Kumar*

Main category: cs.LG

TL;DR: The paper introduces a hierarchical recurrent neural network (FHNN) for multi-scale streamflow forecasting, outperforming baselines and excelling in low-runoff, cold-climate catchments.


<details>
  <summary>Details</summary>
Motivation: To improve streamflow forecasting by modeling multi-scale processes and capturing interactions between temporal scales.

Method: Proposes a hierarchical RNN with inverse and forward models: the inverse resolves temporal modes from data, and the forward predicts streamflow.

Result: FHNN outperforms physics-based and transformer models, especially in low-runoff, cold climates, and maintains accuracy with limited data.

Conclusion: FHNN is effective for multi-scale streamflow forecasting, validated on diverse datasets, and robust with limited training data.

Abstract: We present a framework for modeling multi-scale processes, and study its
performance in the context of streamflow forecasting in hydrology.
Specifically, we propose a novel hierarchical recurrent neural architecture
that factorizes the system dynamics at multiple temporal scales and captures
their interactions. This framework consists of an inverse and a forward model.
The inverse model is used to empirically resolve the system's temporal modes
from data (physical model simulations, observed data, or a combination of them
from the past), and these states are then used in the forward model to predict
streamflow. Experiments on several catchments from the National Weather Service
North Central River Forecast Center show that FHNN outperforms standard
baselines, including physics-based models and transformer-based approaches. The
model demonstrates particular effectiveness in catchments with low runoff
ratios and colder climates. We further validate FHNN on the CAMELS (Catchment
Attributes and MEteorology for Large-sample Studies), which is a widely used
continental-scale hydrology benchmark dataset, confirming consistent
performance improvements for 1-7 day streamflow forecasts across diverse
hydrological conditions. Additionally, we show that FHNN can maintain accuracy
even with limited training data through effective pre-training strategies and
training global models.

</details>


### [336] [DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction](https://arxiv.org/abs/2410.03883)
*Xinwei Zhang, Zhiqi Bu, Borja Balle, Mingyi Hong, Meisam Razaviyayn, Vahab Mirrokni*

Main category: cs.LG

TL;DR: DiSK enhances DP optimizers using Kalman filtering to denoise gradients, improving performance in large-scale training while maintaining privacy.


<details>
  <summary>Details</summary>
Motivation: Standard DP optimizers degrade in large-scale training due to excessive noise. DiSK addresses this by refining gradient estimates.

Method: DiSK employs simplified Kalman filtering to denoise gradients, reducing computational overhead while ensuring privacy.

Result: DiSK outperforms standard DP optimizers (e.g., DPSGD) in tasks like CIFAR-100, ImageNet-1k, and GLUE, achieving better performance under the same privacy constraints.

Conclusion: DiSK is a practical and effective solution for improving DP optimizers in large-scale training, validated by theoretical guarantees and experiments.

Abstract: Differential privacy (DP) offers a robust framework for safeguarding
individual data privacy. To utilize DP in training modern machine learning
models, differentially private optimizers have been widely used in recent
years. A popular approach to privatize an optimizer is to clip the individual
gradients and add sufficiently large noise to the clipped gradient. This
approach led to the development of DP optimizers that have comparable
performance with their non-private counterparts in fine-tuning tasks or in
tasks with a small number of training parameters. However, a significant
performance drop is observed when these optimizers are applied to large-scale
training. This degradation stems from the substantial noise injection required
to maintain DP, which disrupts the optimizer's dynamics. This paper introduces
DiSK, a novel framework designed to significantly enhance the performance of DP
optimizers. DiSK employs Kalman filtering, a technique drawn from control and
signal processing, to effectively denoise privatized gradients and generate
progressively refined gradient estimations. To ensure practicality for
large-scale training, we simplify the Kalman filtering process, minimizing its
memory and computational demands. We establish theoretical privacy-utility
trade-off guarantees for DiSK, and demonstrate provable improvements over
standard DP optimizers like DPSGD in terms of iteration complexity upper-bound.
Extensive experiments across diverse tasks, including vision tasks such as
CIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and
DART, validate the effectiveness of DiSK. The results showcase its ability to
significantly improve the performance of DP optimizers, surpassing
state-of-the-art results under the same privacy constraints on several
benchmarks.

</details>


### [337] [WearableMil: An End-to-End Framework for Military Activity Recognition and Performance Monitoring](https://arxiv.org/abs/2410.05452)
*Barak Gahtan, Shany Funk, Einat Kodesh, Itay Ketko, Tsvi Kuflik, Alex M. Bronstein*

Main category: cs.LG

TL;DR: An end-to-end framework for activity recognition in military training using wearable data achieves high accuracy and addresses missing data, with trade-offs in fine-grained activity detection.


<details>
  <summary>Details</summary>
Motivation: Prevent musculoskeletal injuries in military training by improving activity monitoring through wearable devices.

Method: Hierarchical deep learning approach using data from 135 soldiers wearing Garmin-55 smartwatches over six months, with physiologically-informed missing data handling.

Result: 93.8% accuracy in temporal splits, 83.8% in cross-user evaluation, and reduced unknown sleep states from 40.38% to 3.66%.

Conclusion: The framework provides actionable insights for optimizing training and preventing injuries, with trade-offs in fine-grained activity detection.

Abstract: Musculoskeletal injuries during military training significantly impact
readiness, making prevention through activity monitoring crucial. While Human
Activity Recognition (HAR) using wearable devices offers promising solutions,
it faces challenges in processing continuous data streams and recognizing
diverse activities without predefined sessions. This paper introduces an
end-to-end framework for preprocessing, analyzing, and recognizing activities
from wearable data in military training contexts. Using data from 135 soldiers
wearing \textit{Garmin--55} smartwatches over six months with over 15 million
minutes. We develop a hierarchical deep learning approach that achieves 93.8%
accuracy in temporal splits and 83.8% in cross-user evaluation. Our framework
addresses missing data through physiologically-informed methods, reducing
unknown sleep states from 40.38% to 3.66%. We demonstrate that while longer
time windows (45-60 minutes) improve basic state classification, they present
trade-offs in detecting fine-grained activities. Additionally, we introduce an
intuitive visualization system that enables real-time comparison of individual
performance against group metrics across multiple physiological indicators.
This approach to activity recognition and performance monitoring provides
military trainers with actionable insights for optimizing training programs and
preventing injuries.

</details>


### [338] [Regularized Robustly Reliable Learners and Instance Targeted Attacks](https://arxiv.org/abs/2410.10572)
*Avrim Blum, Donya Saless*

Main category: cs.LG

TL;DR: The paper addresses limitations in robustly-reliable learners by introducing regularized robustly-reliable learners and sublinear-time algorithms for specific cases.


<details>
  <summary>Details</summary>
Motivation: To improve upon Balcan et al.'s work by addressing vacuous guarantees for flexible hypothesis classes and impractical computational requirements.

Method: Introduces regularized robustly-reliable learners and uses dynamic algorithm design for sublinear-time solutions.

Result: Proposes a modified definition for robustly-reliable learners and efficient algorithms for certain cases.

Conclusion: The work advances robust learning by resolving key challenges in flexibility and computational efficiency.

Abstract: Instance-targeted data poisoning attacks, where an adversary corrupts a
training set to induce errors on specific test points, have raised significant
concerns. Balcan et al (2022) proposed an approach to addressing this challenge
by defining a notion of robustly-reliable learners that provide per-instance
guarantees of correctness under well-defined assumptions, even in the presence
of data poisoning attacks. They then give a generic optimal (but
computationally inefficient) robustly reliable learner as well as a
computationally efficient algorithm for the case of linear separators over
log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The
first is that the definition of robustly-reliable learners in Balcan et al
(2022) becomes vacuous for highly-flexible hypothesis classes: if there are two
classifiers h_0, h_1 \in H both with zero error on the training set such that
h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We
address this problem by defining a modified notion of regularized
robustly-reliable learners that allows for nontrivial statements in this case.
The second is that the generic algorithm of Balcan et al (2022) requires
re-running an ERM oracle (essentially, retraining the classifier) on each test
point x, which is generally impractical even if ERM can be implemented
efficiently. To tackle this problem, we show that at least in certain
interesting cases we can design algorithms that can produce their outputs in
time sublinear in training time, by using techniques from dynamic algorithm
design.

</details>


### [339] [Exploring the loss landscape of regularized neural networks via convex duality](https://arxiv.org/abs/2411.07729)
*Sungyoon Kim, Aaron Mishkin, Mert Pilanci*

Main category: cs.LG

TL;DR: The paper analyzes the loss landscape of regularized neural networks, focusing on stationary points, connectivity of solutions, and phase transitions in optima topology.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and behavior of optimal solutions in neural networks, particularly how width and architecture affect the loss landscape.

Method: The problem is cast into an equivalent convex problem and analyzed using its dual. Focus starts with two-layer scalar-output networks, then extends to other architectures.

Result: Phase transitions in optima topology occur with network width changes, and counterexamples show continua of optimal solutions. Results extend to vector-valued and three-layer networks.

Conclusion: The study provides insights into the loss landscape, revealing phase transitions and nonuniqueness of optima, with implications for neural network training and optimization.

Abstract: We discuss several aspects of the loss landscape of regularized neural
networks: the structure of stationary points, connectivity of optimal
solutions, path with nonincreasing loss to arbitrary global optimum, and the
nonuniqueness of optimal solutions, by casting the problem into an equivalent
convex problem and considering its dual. Starting from two-layer neural
networks with scalar output, we first characterize the solution set of the
convex problem using its dual and further characterize all stationary points.
With the characterization, we show that the topology of the global optima goes
through a phase transition as the width of the network changes, and construct
counterexamples where the problem may have a continuum of optimal solutions.
Finally, we show that the solution set characterization and connectivity
results can be extended to different architectures, including two-layer
vector-valued neural networks and parallel three-layer neural networks.

</details>


### [340] [PyAWD: A Library for Generating Large Synthetic Datasets of Acoustic Wave Propagation](https://arxiv.org/abs/2411.12636)
*Pascal Tribel, Gianluca Bontempi*

Main category: cs.LG

TL;DR: PyAWD is a Python library for generating synthetic seismic datasets to address the lack of real-world data for ML applications in earthquake analysis.


<details>
  <summary>Details</summary>
Motivation: High costs and logistical challenges limit real seismic data availability, hindering ML applications. Existing simulation tools lack scalability for large datasets.

Method: PyAWD simulates spatio-temporal acoustic wave propagation in 2D/3D heterogeneous media, with customizable parameters like wave speed and media composition.

Result: The library enables creation of ML-scale datasets and demonstrates effectiveness in tasks like epicenter retrieval.

Conclusion: PyAWD fills a critical gap by providing synthetic data for ML-driven seismic analysis, especially where real data is scarce.

Abstract: Seismic data is often sparse and unevenly distributed due to the high costs
and logistical challenges associated with deploying physical seismometers,
limiting the application of Machine Learning (ML) in earthquake analysis. While
simulation methods exist, no tool allows the generation of large datasets
containing simulated measurements of the ground motion. To address this gap, we
introduce PyAWD, a Python library designed to generate high-resolution
synthetic datasets simulating spatio-temporal acoustic wave propagation in both
two-dimensional and three-dimensional heterogeneous media. By allowing fine
control over parameters such as the wave speed, external forces, spatial and
temporal discretization, and media composition, PyAWD enables the creation of
ML-scale datasets that capture the complexity of seismic wave behavior. We
illustrate the library's potential with an epicenter retrieval task, showcasing
its suitability for designing complex, accurate seismic problems that require
advanced ML approaches in the absence or lack of dense real-world data. We also
show the usefulness of our tool to tackle the problem of data budgeting in the
framework of epicenter retrieval.

</details>


### [341] [Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning](https://arxiv.org/abs/2412.07062)
*Weihang Chen, Cheng Yang, Jie Ren, Zhiqiang Li, Zheng Wang*

Main category: cs.LG

TL;DR: FLAYER is a layer-wise learning method for personalized federated learning (pFL) that optimizes local model personalization by dynamically adjusting learning rates and selectively incorporating global knowledge, improving accuracy by up to 14.29%.


<details>
  <summary>Details</summary>
Motivation: Non-IID data in federated learning (FL) causes poor accuracy and slow convergence. Existing pFL methods lack efficient global knowledge integration and suffer from over-personalization.

Method: FLAYER uses layer-wise learning, dynamically adjusting learning rates for each layer and selectively uploading parameters for global aggregation. It initializes local models with global knowledge cost-effectively.

Result: FLAYER outperforms six state-of-the-art pFL methods, improving inference accuracy by 5.40% on average (up to 14.29%) across four datasets.

Conclusion: FLAYER effectively balances personalization and global knowledge, enhancing pFL performance with low computational overhead.

Abstract: Real-life deployment of federated Learning (FL) often faces non-IID data,
which leads to poor accuracy and slow convergence. Personalized FL (pFL)
tackles these issues by tailoring local models to individual data sources and
using weighted aggregation methods for client-specific learning. However,
existing pFL methods often fail to provide each local model with global
knowledge on demand while maintaining low computational overhead. Additionally,
local models tend to over-personalize their data during the training process,
potentially dropping previously acquired global information. We propose FLAYER,
a novel layer-wise learning method for pFL that optimizes local model
personalization performance. FLAYER considers the different roles and learning
abilities of neural network layers of individual local models. It incorporates
global information for each local model as needed to initialize the local model
cost-effectively. It then dynamically adjusts learning rates for each layer
during local training, optimizing the personalized learning process for each
local model while preserving global knowledge. Additionally, to enhance global
representation in pFL, FLAYER selectively uploads parameters for global
aggregation in a layer-wise manner. We evaluate FLAYER on four representative
datasets in computer vision and natural language processing domains. Compared
to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on
average, by 5.40\% (up to 14.29\%).

</details>


### [342] [A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems](https://arxiv.org/abs/2412.09009)
*Sumanth Kumar Boya, Deepak Subramani*

Main category: cs.LG

TL;DR: PINTO, a physics-informed transformer neural operator, generalizes to unseen initial/boundary conditions without retraining, using only physics loss and no simulation data. It outperforms existing methods with lower relative errors.


<details>
  <summary>Details</summary>
Motivation: Existing neural operators require retraining for new conditions and rely on large simulation data. PINTO addresses these limitations by generalizing efficiently and reducing data dependency.

Method: PINTO uses iterative kernel integral operator units with cross-attention to transform PDE solution domain points into condition-aware vectors, enabling simulation-free training with physics loss.

Result: PINTO achieves relative errors one-fifth to one-third of other methods for advection, Burgers, and Navier-Stokes equations under unseen conditions. It also solves equations at untrained time steps.

Conclusion: PINTO is a robust, efficient neural operator for solving PDEs with minimal data, outperforming existing methods in generalization and accuracy.

Abstract: Initial boundary value problems arise commonly in applications with
engineering and natural systems governed by nonlinear partial differential
equations (PDEs). Operator learning is an emerging field for solving these
equations by using a neural network to learn a map between infinite dimensional
input and output function spaces. These neural operators are trained using a
combination of data (observations or simulations) and PDE-residuals
(physics-loss). A major drawback of existing neural approaches is the
requirement to retrain with new initial/boundary conditions, and the necessity
for a large amount of simulation data for training. We develop a
physics-informed transformer neural operator (named PINTO) that efficiently
generalizes to unseen initial and boundary conditions, trained in a
simulation-free setting using only physics loss. The main innovation lies in
our new iterative kernel integral operator units, implemented using
cross-attention, to transform the PDE solution's domain points into an
initial/boundary condition-aware representation vector, enabling efficient
learning of the solution function for new scenarios. The PINTO architecture is
applied to simulate the solutions of important equations used in engineering
applications: advection, Burgers, and steady and unsteady Navier-Stokes
equations (three flow scenarios). For these five test cases, we show that the
relative errors during testing under challenging conditions of unseen
initial/boundary conditions are only one-fifth to one-third of other leading
physics informed operator learning methods. Moreover, our PINTO model is able
to accurately solve the advection and Burgers equations at time steps that are
not included in the training collocation points. The code is available at
https://github.com/quest-lab-iisc/PINTO

</details>


### [343] [Physics-informed deep learning for infectious disease forecasting](https://arxiv.org/abs/2501.09298)
*Ying Qian, Kui Zhang, Éric Marty, Avranil Basu, Eamon B. O'Dea, Xianqiao Wang, Spencer Fox, Pejman Rohani, John M. Drake, He Li*

Main category: cs.LG

TL;DR: A new infectious disease forecasting model using physics-informed neural networks (PINNs) integrates epidemiological theory with data, outperforming baseline and deep learning models while being simpler to implement.


<details>
  <summary>Details</summary>
Motivation: Accurate disease forecasting is crucial for public health policymaking and pandemic preparedness.

Method: The model embeds a compartmental model into the loss function of PINNs and includes a sub-network for covariates like mobility and vaccine doses.

Result: The PINN model accurately predicts COVID-19 cases, deaths, and hospitalizations in California, outperforming naive baselines and deep learning models.

Conclusion: The PINN model is a promising, efficient tool for infectious disease forecasting due to its accuracy and simplicity.

Abstract: Accurate forecasting of contagious diseases is critical for public health
policymaking and pandemic preparedness. We propose a new infectious disease
forecasting model based on physics-informed neural networks (PINNs), an
emerging scientific machine learning approach. By embedding a compartmental
model into the loss function, our method integrates epidemiological theory with
data, helping to prevent model overfitting. We further enhance the model with a
sub-network that accounts for covariates such as mobility and cumulative
vaccine doses, which influence the transmission rate. Using state-level
COVID-19 data from California, we demonstrate that the PINN model accurately
predicts cases, deaths, and hospitalizations, aligning well with existing
benchmarks. Notably, the PINN model outperforms naive baseline forecasts and
several sequence deep learning models, including Recurrent Neural Networks
(RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs),
and Transformers. It also achieves performance comparable to a sophisticated
Gaussian infection state forecasting model that combines compartmental
dynamics, a data observation model, and parameter regression. However, the PINN
model features a simpler structure and is easier to implement. In summary, we
systematically evaluate the PINN model's ability to forecast infectious disease
dynamics, demonstrating its potential as an efficient computational tool to
strengthen forecasting capabilities.

</details>


### [344] [Synthesising Activity Participations and Scheduling with Deep Generative Machine Learning](https://arxiv.org/abs/2501.10221)
*Fred Shone, Tim Hillel*

Main category: cs.LG

TL;DR: A deep generative model synthesizes human activity schedules, offering faster and simpler operation than traditional methods, with a novel representation and evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Activity schedules are crucial for transport, energy, and epidemiology models, but existing methods are complex and slow.

Method: Uses a deep generative machine learning approach to learn human preferences and scheduling logic, avoiding complex sub-models and custom rules.

Result: Generates large, diverse, and realistic synthetic activity schedules efficiently.

Conclusion: The approach is effective for synthesizing activity schedules, with potential applications in various fields.

Abstract: Using a deep generative machine learning approach, we synthesise human
activity participations and scheduling (the choices of what activities to
participate in and when). Activity schedules, which represent what people do
and when, are a core component of many applied transport, energy, and
epidemiology models. Our data-driven approach learns the distributions
resulting from human preferences and scheduling logic without the need for
complex interacting combinations of sub-models and custom rules, This makes our
approach significantly faster and simpler to operate than existing approaches
to synthesise or anonymise schedule data. We additionally contribute a novel
schedule representation and a comprehensive evaluation framework. We evaluate a
range of schedule encoding and deep model architecture combinations. The
evaluation shows our approach can rapidly generate large, diverse, novel, and
realistic synthetic samples of activity schedules.

</details>


### [345] [PAC Learning is just Bipartite Matching (Sort of)](https://arxiv.org/abs/2502.00607)
*Shaddin Dughmi*

Main category: cs.LG

TL;DR: The paper links PAC learning to bipartite matching via a transductive learning model and one-inclusion graphs, also connecting to recreational hat puzzles.


<details>
  <summary>Details</summary>
Motivation: To show the relationship between PAC learning and bipartite matching, and to explore transductive learning as a tool for learning theory.

Method: Uses a transductive learning model and one-inclusion graphs, generalizing recreational hat puzzles.

Result: Demonstrates connections between PAC learning, transductive models, and bipartite matching.

Conclusion: The transductive model, though not new, is valuable for deep learning theory questions, and the paper serves as a tutorial on PAC and transductive learning connections.

Abstract: The main goal of this article is to convince you, the reader, that supervised
learning in the Probably Approximately Correct (PAC) model is closely related
to -- of all things -- bipartite matching! En-route from PAC learning to
bipartite matching, I will overview a particular transductive model of
learning, and associated one-inclusion graphs, which can be viewed as a
generalization of some of the hat puzzles that are popular in recreational
mathematics. Whereas this transductive model is far from new, it has recently
seen a resurgence of interest as a tool for tackling deep questions in learning
theory. A secondary purpose of this article could be as a (biased) tutorial on
the connections between the PAC and transductive models of learning.

</details>


### [346] [An Inquiry into Datacenter TCO for LLM Inference with FP8](https://arxiv.org/abs/2502.01070)
*Jiwoo Kim, Joonhyung Lee, Gunho Park, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee, Youngjoo Lee*

Main category: cs.LG

TL;DR: The paper analyzes LLM inference challenges, focusing on TCO for CSPs, comparing Gaudi 2 and NVIDIA H100 accelerators, and evaluating FP8 quantization and workload impacts.


<details>
  <summary>Details</summary>
Motivation: Addressing high power consumption and cooling costs in datacenters for LLM inference, aiming to reduce TCO for CSPs.

Method: Develops a framework to compare AI accelerators, analyzes FP8 precision and workload characteristics, and studies power consumption and quantization.

Result: FP8 quantization improves throughput and cost efficiency; thin GEMMs in decode phases impact performance more than hardware peaks.

Conclusion: Provides insights for CSP deployment decisions and future accelerator designs to optimize LLM inference TCO.

Abstract: As large language models (LLMs) continue to scale, their inference demands
present significant challenges, particularly due to the high power consumption
of AI accelerators in datacenters. These facilities require specialized cooling
and power management systems, substantially increasing the total cost of
ownership (TCO) for cloud service providers (CSPs). In this work, we analyze
the computational characteristics and constraints of LLM inference from a TCO
perspective, focusing on two representative accelerators: the Gaudi 2 and
NVIDIA H100. We present a generalizable framework that enables CSPs to compare
and select AI accelerators according to diverse operational requirements. Using
this model, we analyze the impact of FP8 precision and LLM inference workload
characteristics as key factors influencing TCO. We investigate FP8
quantization, which is gaining adoption in LLM training, as a technique to
improve inference throughput while maintaining cost efficiency. Furthermore,
our analysis of LLM inference workloads reveals that performance on thin GEMMs,
which dominate the decode phase, can have a greater impact than theoretical
hardware peak performance. By studying the interaction between power
consumption, quantization strategies, and hardware architecture, we offer
insights that support informed deployment decisions and guide future
accelerator designs to improve the TCO of LLM inference.

</details>


### [347] [Learnable Residual-based Latent Denoising in Semantic Communication](https://arxiv.org/abs/2502.07319)
*Mingkai Xu, Yongpeng Wu, Yuxuan Shi, Xiang-Gen Xia, Wenjun Zhang, Ping Zhang*

Main category: cs.LG

TL;DR: A framework for robust image transmission using latent denoising in semantic communication, improving image quality by removing channel noise and adapting denoising steps based on predicted similarity scores.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of decoded images in noisy channels by effectively removing noise and recovering semantic information.

Method: Incorporates a learnable latent denoiser at the receiver, uses iterative residual learning for denoising, and adapts denoising steps based on predicted latent similarity scores from channel SNR.

Result: Simulations show effective noise removal at various levels and reconstruction of visually appealing images.

Conclusion: The proposed framework efficiently denoises and reconstructs images, reducing communication latency while maintaining stable performance.

Abstract: A latent denoising semantic communication (SemCom) framework is proposed for
robust image transmission over noisy channels. By incorporating a learnable
latent denoiser into the receiver, the received signals are preprocessed to
effectively remove the channel noise and recover the semantic information,
thereby enhancing the quality of the decoded images. Specifically, a latent
denoising mapping is established by an iterative residual learning approach to
improve the denoising efficiency while ensuring stable performance. Moreover,
channel signal-to-noise ratio (SNR) is utilized to estimate and predict the
latent similarity score (SS) for conditional denoising, where the number of
denoising steps is adapted based on the predicted SS sequence, further reducing
the communication latency. Finally, simulations demonstrate that the proposed
framework can effectively and efficiently remove the channel noise at various
levels and reconstruct visual-appealing images.

</details>


### [348] [Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting](https://arxiv.org/abs/2502.07608)
*Arvind Pillai, Dimitris Spathis, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell*

Main category: cs.LG

TL;DR: Time2Lang integrates time series foundation models (TFMs) and large language models (LLMs) for health applications, bypassing error-prone text conversion. It shows promise in mental health classification with efficient inference and preserved time-series features.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for combining sensor data with LLMs are error-prone and computationally expensive, especially for extended time series. Bridging TFMs and LLMs remains challenging.

Method: Time2Lang maps TFM outputs directly to LLM representations, trained on synthetic data using periodicity prediction, then evaluated on mental health tasks like depression and flourishing classification.

Result: Time2Lang maintains constant inference times, preserves time-series characteristics, and effectively integrates TFMs and LLMs, validated on wearable and mobile sensing datasets.

Conclusion: Time2Lang successfully bridges TFMs and LLMs for health applications, minimizing information loss and enabling performance transfer, setting a foundation for future research.

Abstract: Large language models (LLMs) show promise for health applications when
combined with behavioral sensing data. Traditional approaches convert sensor
data into text prompts, but this process is prone to errors, computationally
expensive, and requires domain expertise. These challenges are particularly
acute when processing extended time series data. While time series foundation
models (TFMs) have recently emerged as powerful tools for learning
representations from temporal data, bridging TFMs and LLMs remains challenging.
Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM
representations without intermediate text conversion. Our approach first trains
on synthetic data using periodicity prediction as a pretext task, followed by
evaluation on mental health classification tasks. We validate Time2Lang on two
longitudinal wearable and mobile sensing datasets: daily depression prediction
using step count data (17,251 days from 256 participants) and flourishing
classification based on conversation duration (46 participants over 10 weeks).
Time2Lang maintains near constant inference times regardless of input length,
unlike traditional prompting methods. The generated embeddings preserve
essential time-series characteristics such as auto-correlation. Our results
demonstrate that TFMs and LLMs can be effectively integrated while minimizing
information loss and enabling performance transfer across these distinct
modeling paradigms. To our knowledge, we are the first to integrate a TFM and
an LLM for health, thus establishing a foundation for future research combining
general-purpose large models for complex healthcare tasks.

</details>


### [349] [Cloud Computing Energy Consumption Prediction Based on Kernel Extreme Learning Machine Algorithm Improved by Vector Weighted Average Algorithm](https://arxiv.org/abs/2503.04088)
*Yuqing Wang, Xiao Yang*

Main category: cs.LG

TL;DR: A novel VWAA-KELM model improves energy consumption prediction in cloud computing by dynamically adjusting feature weights and optimizing kernel functions, achieving high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of cloud computing infrastructure has heightened energy consumption challenges, necessitating accurate prediction models.

Method: The study integrates a vector weighted average algorithm (VWAA) with kernel extreme learning machine (KELM) to dynamically adjust feature weights and optimize kernel functions.

Result: VWAA-KELM achieves 94.7% prediction accuracy within [0, 50] error units, with R2 scores of 0.987 (training) and 0.973 (test), demonstrating strong stability and generalization.

Conclusion: The model's adaptive feature weighting and hybrid framework offer scalable solutions for cloud computing, IoT, and edge computing, enhancing energy management and resource allocation.

Abstract: With the rapid expansion of cloud computing infrastructure, energy
consumption has become a critical challenge, driving the need for accurate and
efficient prediction models. This study proposes a novel Vector Weighted
Average Kernel Extreme Learning Machine (VWAA-KELM) model to enhance energy
consumption prediction in cloud computing environments. By integrating a vector
weighted average algorithm (VWAA) with kernel extreme learning machine (KELM),
the proposed model dynamically adjusts feature weights and optimizes kernel
functions, significantly improving prediction accuracy and generalization.
Experimental results demonstrate the superior performance of VWAA-KELM: 94.7%
of test set prediction errors fall within [0, 50] units, with only three cases
exceeding 100 units, indicating strong stability. The model achieves a
coefficient of determination (R2) of 0.987 in the training set (RMSE = 28.108,
RPD = 8.872) and maintains excellent generalization with R2 = 0.973 in the test
set (RMSE = 43.227, RPD = 6.202). Visual analysis confirms that predicted
values closely align with actual energy consumption trends, avoiding
overfitting while capturing nonlinear dependencies. A key innovation of this
study is the introduction of adaptive feature weighting, allowing the model to
dynamically assign importance to different input parameters, thereby enhancing
high-dimensional data processing. This advancement provides a scalable and
efficient approach for optimizing cloud data center energy consumption. Beyond
cloud computing, the proposed hybrid framework has broader applications in
Internet of Things (IoT) and edge computing, supporting real-time energy
management and intelligent resource allocation.

</details>


### [350] [RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction](https://arxiv.org/abs/2503.21971)
*Armin Abdollahi, Mehdi Kamal, Massoud Pedram*

Main category: cs.LG

TL;DR: A novel framework bridges the gap between Verilog code synthesis and PPA estimation using a curated dataset and fine-tuned CodeLlama with LoRA and mixture-of-experts, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of integrating code synthesis with accurate PPA (power, performance, area) estimation in hardware design.

Method: Utilizes a 21k dataset of cleaned Verilog modules annotated with PPA metrics, fine-tunes CodeLlama with LoRA, and employs a mixture-of-experts architecture for regression-based PPA prediction.

Result: Power estimation improved by 5.9% (20% threshold) and 7.2% (10%), delay by 5.1% and 3.9%, and area by 4% and 7.9%. Mixture-of-experts added 3-4% gains.

Conclusion: The framework sets a new benchmark for PPA-aware Verilog generation, enhancing next-gen EDA workflows with integrated data and modeling.

Abstract: Large language models have recently transformed hardware design, yet bridging
the gap between code synthesis and PPA (power, performance, and area)
estimation remains a challenge. In this work, we introduce a novel framework
that leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog
modules, each annotated with detailed power, delay, and area metrics. By
employing chain-of-thought techniques, we automatically debug and curate this
dataset to ensure high fidelity in downstream applications. We then fine-tune
CodeLlama using LoRA-based parameter-efficient methods, framing the task as a
regression problem to accurately predict PPA metrics from Verilog code.
Furthermore, we augment our approach with a mixture-of-experts
architecture-integrating both LoRA and an additional MLP expert layer-to
further refine predictions. Experimental results demonstrate significant
improvements: power estimation accuracy is enhanced by 5.9% at a 20% error
threshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and
3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10%
thresholds, respectively. Notably, the incorporation of the mixture-of-experts
module contributes an additional 3--4% improvement across these tasks. Our
results establish a new benchmark for PPA-aware Verilog generation,
highlighting the effectiveness of our integrated dataset and modeling
strategies for next-generation EDA workflows.

</details>


### [351] [Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model](https://arxiv.org/abs/2503.22480)
*Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, Jun Zhao, Kang Liu*

Main category: cs.LG

TL;DR: The paper introduces PURM, a probabilistic uncertain reward model, to address reward hacking in RLHF by learning reward distributions from preference data and dynamically balancing reward optimization with exploration.


<details>
  <summary>Details</summary>
Motivation: Reward hacking in RLHF hinders robust and scalable intelligence, and existing methods lack systematic foundations to model uncertainty from preference data.

Method: Proposes PURM, a generalization of the Bradley-Terry model, with a derived loss function and uncertainty calculation. Integrates an uncertainty-aware penalty into PPO.

Result: PURM effectively models rewards and uncertainties, delaying reward hacking and improving reward performance compared to existing methods.

Conclusion: PURM provides a systematic and practical solution to mitigate reward hacking in RLHF, enhancing long-term training and exploration.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical
technique for training large language models. However, reward hacking-a
phenomenon where models exploit flaws in the reward model-remains a significant
barrier to achieving robust and scalable intelligence through long-term
training. Existing studies have proposed the uncertain reward models to address
reward hacking, however, they often lack systematic or theoretical foundations,
failing to model the uncertainty intrinsically emerging from preference data,
and thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF
training and exploration. In this paper, we propose a Probabilistic Uncertain
Reward Model (PURM), a natural generalization of the classical Bradley-Terry
reward model, that can directly learn the reward distribution emerged from the
preference data. We theoretically derived PURM's loss function and the reward
distribution uncertainty calculation based on Bhattacharyya Coefficient. To
mitigate reward hacking with PURM, we further introduce an uncertainty-aware
penalty into Proximal Policy Optimization (PPO), which leverages the learned
uncertainty to dynamically balance reward optimization and exploration. We
propose a lightweight and easy-to-use implementation of PURM. Experiments
demonstrate that PURM effectively models the rewards and uncertainties, and
significantly delays the onset of reward hacking while improving final reward
performance compared with existing methods.

</details>


### [352] [UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows](https://arxiv.org/abs/2503.23236)
*Ismaël Zighed, Nicolas Thome, Patrick Gallinari, Taraneh Sayadi*

Main category: cs.LG

TL;DR: The paper introduces a nonlinear reduction strategy for transient flows using a VAE and attention mechanisms, incorporating uncertainty quantification for robust predictions.


<details>
  <summary>Details</summary>
Motivation: ROMs need to generalize well and provide confidence in predictions for engineering applications, but current methods lack robustness and parametrization.

Method: A VAE with variational inference for confidence measurement and a latent space transformer using attention mechanisms for dynamical system prediction.

Result: The method improves generalization and robustness, enabling informed decision-making and cost-effective parameter space sampling.

Conclusion: The proposed strategy addresses ROM limitations, offering a robust and generalizable solution for transient flows with uncertainty quantification.

Abstract: Reduced order models (ROMs) play a critical role in fluid mechanics by
providing low-cost predictions, making them an attractive tool for engineering
applications. However, for ROMs to be widely applicable, they must not only
generalise well across different regimes, but also provide a measure of
confidence in their predictions. While recent data-driven approaches have begun
to address nonlinear reduction techniques to improve predictions in transient
environments, challenges remain in terms of robustness and parametrisation. In
this work, we present a nonlinear reduction strategy specifically designed for
transient flows that incorporates parametrisation and uncertainty
quantification. Our reduction strategy features a variational auto-encoder
(VAE) that uses variational inference for confidence measurement. We use a
latent space transformer that incorporates recent advances in attention
mechanisms to predict dynamical systems. Attention's versatility in learning
sequences and capturing their dependence on external parameters enhances
generalisation across a wide range of dynamics. Prediction, coupled with
confidence, enables more informed decision making and addresses the need for
more robust models. In addition, this confidence is used to cost-effectively
sample the parameter space, improving model performance a priori across the
entire parameter space without requiring evaluation data for the entire domain.

</details>


### [353] [Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning](https://arxiv.org/abs/2504.14268)
*Xinye Chen*

Main category: cs.LG

TL;DR: A reinforcement learning framework optimizes numerical precision in the CG method using Q-learning, balancing efficiency and accuracy without retraining for new datasets.


<details>
  <summary>Details</summary>
Motivation: To dynamically optimize numerical precision in iterative solvers like CG, ensuring computational efficiency and numerical accuracy without recalibration for new data.

Method: Model precision selection as an MDP, employ Q-learning to adaptively assign precision levels, and ensure stability with double-precision computations.

Result: The RL framework enhances solver performance, demonstrating robustness and scalability, and marks the first RL application in mixed-precision numerical methods.

Conclusion: The approach offers practical advantages for scientific computing, paving the way for AI-driven advancements in iterative solvers.

Abstract: This paper presents a novel reinforcement learning (RL) framework for
dynamically optimizing numerical precision in the preconditioned conjugate
gradient (CG) method. By modeling precision selection as a Markov Decision
Process (MDP), we employ Q-learning to adaptively assign precision levels to
key operations, striking an optimal balance between computational efficiency
and numerical accuracy, while ensuring stability through double-precision
scalar computations and residual computing. In practice, the algorithm is
trained on a set of data and subsequently performs inference for precision
selection on out-of-sample data, without requiring re-analysis or retraining
for new datasets. This enables the method to adapt seamlessly to new problem
instances without the computational overhead of recalibration. Our results
demonstrate the effectiveness of RL in enhancing solver's performance, marking
the first application of RL to mixed-precision numerical methods. The findings
highlight the approach's practical advantages, robustness, and scalability,
providing valuable insights into its integration with iterative solvers and
paving the way for AI-driven advancements in scientific computing.

</details>


### [354] [High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures](https://arxiv.org/abs/2504.17857)
*AJ Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian*

Main category: cs.LG

TL;DR: First public demo of RL policy deployment on Boston Dynamics Spot, using Wasserstein Distance and MMD for sim2real gap measurement, achieving high-speed locomotion and agility.


<details>
  <summary>Details</summary>
Motivation: To demonstrate an end-to-end RL policy deployment on Spot hardware, addressing sim2real challenges and optimizing unknown parameters.

Method: Uses Wasserstein Distance and Maximum Mean Discrepancy to measure sim2real gap, and Covariance Matrix Adaptation Evolution Strategy for parameter optimization.

Result: Achieved over 5.2ms locomotion, triple Spot's default speed, with robustness to slippery surfaces and disturbance rejection.

Conclusion: Successful deployment of high-performance RL policies on Spot, with code released to support future work.

Abstract: This work presents an overview of the technical details behind a high
performance reinforcement learning policy deployment with the Spot RL
Researcher Development Kit for low level motor access on Boston Dynamics Spot.
This represents the first public demonstration of an end to end end
reinforcement learning policy deployed on Spot hardware with training code
publicly available through Nvidia IsaacLab and deployment code available
through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean
Discrepancy to quantify the distributional dissimilarity of data collected on
hardware and in simulation to measure our sim2real gap. We use these measures
as a scoring function for the Covariance Matrix Adaptation Evolution Strategy
to optimize simulated parameters that are unknown or difficult to measure from
Spot. Our procedure for modeling and training produces high quality
reinforcement learning policies capable of multiple gaits, including a flight
phase. We deploy policies capable of over 5.2ms locomotion, more than triple
Spots default controller maximum speed, robustness to slippery surfaces,
disturbance rejection, and overall agility previously unseen on Spot. We detail
our method and release our code to support future work on Spot with the low
level API.

</details>


### [355] [PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation](https://arxiv.org/abs/2504.18583)
*Zihao An, Huajun Bai, Ziqiong Liu, Dong Li, Emad Barsoum*

Main category: cs.LG

TL;DR: PARD introduces a parallel speculative decoding method to accelerate LLM inference by predicting multiple tokens in one draft pass, reducing training costs and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs are slow due to single-token generation and memory bottlenecks. Speculative decoding helps but has draft-phase overhead and high training costs.

Method: PARD uses parallel draft models to predict multiple tokens in one forward pass and a conditional drop token method to speed up training. It's target-independent, allowing reuse across models.

Result: PARD improves draft model training efficiency by 3x and accelerates LLaMA3.1-8B inference by 4.08x, achieving 311.5 tokens per second.

Conclusion: PARD effectively addresses the limitations of speculative decoding, offering significant speedups and cost reductions for LLM inference.

Abstract: The autoregressive nature of large language models (LLMs) limits inference
speed. Each forward pass generates only a single token and is often
bottlenecked by memory bandwidth. Speculative decoding alleviates this issue
using a draft-then-verify approach to accelerate token generation. However, the
overhead introduced during the draft phase and the training cost of the draft
model limit the efficiency and adaptability of speculative decoding. In this
work, we introduce PARallel Draft (PARD), a novel speculative decoding method
that enables low-cost adaptation of autoregressive draft models into parallel
draft models. PARD enhances inference efficiency by predicting multiple future
tokens in a single forward pass of the draft phase, and incorporates a
conditional drop token method to accelerate training. Its target-independence
property allows a single draft model to be applied to an entire family of
different models, minimizing the adaptation cost. Our proposed conditional drop
token method can improves draft model training efficiency by 3x. On our
optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x,
achieving 311.5 tokens per second.

</details>


### [356] [Geometry-Informed Neural Operator Transformer](https://arxiv.org/abs/2504.19452)
*Qibang Liu, Vincient Zhong, Hadi Meidani, Diab Abueidda, Seid Koric, Philippe Geubelle*

Main category: cs.LG

TL;DR: GINOT combines transformers with neural operators for efficient PDE predictions on arbitrary geometries, achieving high accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional numerical methods for PDEs, especially for repeated evaluations on varying geometries.

Method: Integrates transformer architecture with neural operators, using surface point clouds and attention mechanisms for geometry encoding and solution decoding.

Result: Validated on challenging datasets, GINOT shows high accuracy and strong generalization for complex 2D/3D geometries.

Conclusion: GINOT is a robust and efficient surrogate model for PDE predictions on arbitrary geometries.

Abstract: Machine-learning-based surrogate models offer significant computational
efficiency and faster simulations compared to traditional numerical methods,
especially for problems requiring repeated evaluations of partial differential
equations. This work introduces the Geometry-Informed Neural Operator
Transformer (GINOT), which integrates the transformer architecture with the
neural operator framework to enable forward predictions for arbitrary
geometries. GINOT encodes the surface points cloud of a geometry using a
sampling and grouping mechanism combined with an attention mechanism, ensuring
invariance to point order and padding while maintaining robustness to
variations in point density. The geometry information is seamlessly integrated
with query points in the solution decoder through the attention mechanism. The
performance of GINOT is validated on multiple challenging datasets, showcasing
its high accuracy and strong generalization capabilities for complex and
arbitrary 2D and 3D geometries.

</details>


### [357] [If Concept Bottlenecks are the Question, are Foundation Models the Answer?](https://arxiv.org/abs/2504.19774)
*Nicola Debole, Pietro Barbiero, Francesco Giannini, Andrea Passerini, Stefano Teso, Emanuele Marconato*

Main category: cs.LG

TL;DR: VLM-CBMs replace expert annotations with weak supervision from foundation models, but their concept quality varies and doesn't strongly correlate with accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess the impact of using weak supervision from foundation models (VLMs) instead of expert annotations on the quality of learned concepts in CBMs.

Method: Empirical analysis of state-of-the-art VLM-CBMs using significant metrics to evaluate concept quality.

Result: VLM supervision can differ from expert annotations, and concept accuracy doesn't strongly correlate with quality.

Conclusion: Weak supervision from VLMs is a viable but inconsistent alternative to expert annotations in CBMs.

Abstract: Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high
performance with ante-hoc interpretability. CBMs work by first mapping inputs
(e.g., images) to high-level concepts (e.g., visible objects and their
properties) and then use these to solve a downstream task (e.g., tagging or
scoring an image) in an interpretable manner. Their performance and
interpretability, however, hinge on the quality of the concepts they learn. The
go-to strategy for ensuring good quality concepts is to leverage expert
annotations, which are expensive to collect and seldom available in
applications. Researchers have recently addressed this issue by introducing
"VLM-CBM" architectures that replace manual annotations with weak supervision
from foundation models. It is however unclear what is the impact of doing so on
the quality of the learned concepts. To answer this question, we put
state-of-the-art VLM-CBMs to the test, analyzing their learned concepts
empirically using a selection of significant metrics. Our results show that,
depending on the task, VLM supervision can sensibly differ from expert
annotations, and that concept accuracy and quality are not strongly correlated.
Our code is available at https://github.com/debryu/CQA.

</details>


### [358] [Transfer Learning Under High-Dimensional Network Convolutional Regression Model](https://arxiv.org/abs/2504.19979)
*Liyuan Wang, Jiachen Chen, Kathryn L. Lunetta, Danyang Huang, Huimin Cheng, Debarghya Mukherjee*

Main category: cs.LG

TL;DR: A high-dimensional transfer learning framework (NCR) is proposed for networked data, improving prediction accuracy with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Handling dependencies in networked data for transfer learning is challenging, especially under domain shifts.

Method: Proposes a network convolutional regression (NCR) model with a two-step transfer learning algorithm and source detection.

Result: Theoretical analysis shows improved convergence rates; empirical tests confirm accuracy gains, especially with scarce labeled data.

Conclusion: NCR effectively addresses networked data dependencies, enhancing transfer learning performance in data-scarce scenarios.

Abstract: Transfer learning enhances model performance by utilizing knowledge from
related domains, particularly when labeled data is scarce. While existing
research addresses transfer learning under various distribution shifts in
independent settings, handling dependencies in networked data remains
challenging. To address this challenge, we propose a high-dimensional transfer
learning framework based on network convolutional regression (NCR), inspired by
the success of graph convolutional networks (GCNs). The NCR model incorporates
random network structure by allowing each node's response to depend on its
features and the aggregated features of its neighbors, capturing local
dependencies effectively. Our methodology includes a two-step transfer learning
algorithm that addresses domain shift between source and target networks, along
with a source detection mechanism to identify informative domains.
Theoretically, we analyze the lasso estimator in the context of a random graph
based on the Erdos-Renyi model assumption, demonstrating that transfer learning
improves convergence rates when informative sources are present. Empirical
evaluations, including simulations and a real-world application using Sina
Weibo data, demonstrate substantial improvements in prediction accuracy,
particularly when labeled data in the target domain is limited.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [359] [AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury](https://arxiv.org/abs/2504.20368)
*David Gordon, Panayiotis Petousis, Susanne B. Nicholas, Alex A. T. Bui*

Main category: cs.MA

TL;DR: STRUC-MAS automates learning global models for multiagent systems, improving diagnostic reasoning in complex medical scenarios like AKI prediction.


<details>
  <summary>Details</summary>
Motivation: To enhance consensus-driven diagnostic reasoning by integrating multiple expert perspectives into a shared global model for better decision-making.

Method: Introduces STRUC-MAS, a framework for learning and incorporating global models as prior beliefs in multiagent systems, tested on AKI prediction.

Result: Improved performance (higher AP) in AKI prediction with structure-following agents (SF-FT, SF-FT-RAG) vs. baseline (NSF-FT, NSF-FT-RAG). Agents showed reinforced or new beliefs after interactions.

Conclusion: Learning and leveraging global structures in MAS is crucial for competitive classification and diagnostic reasoning.

Abstract: Diagnostic reasoning entails a physician's local (mental) model based on an
assumed or known shared perspective (global model) to explain patient
observations with evidence assigned towards a clinical assessment. But in
several (complex) medical situations, multiple experts work together as a team
to optimize health evaluation and decision-making by leveraging different
perspectives. Such consensus-driven reasoning reflects individual knowledge
contributing toward a broader perspective on the patient. In this light, we
introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework
automating the learning of these global models and their incorporation as prior
beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof
of concept with a prosocial MAS application for predicting acute kidney
injuries (AKIs). In this case, we found that incorporating a global structure
enabled multiple agents to achieve better performance (average precision, AP)
in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT,
AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs.
baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180)
for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents
with higher recall scores reported lower confidence levels in the initial round
on true positive and false negative cases. But after explicit interactions,
their confidence in their decisions increased (suggesting reinforced belief).
In contrast, the SF-FT agent with the lowest recall decreased its confidence in
true positive and false negative cases (suggesting a new belief). This approach
suggests that learning and leveraging global structures in MAS is necessary
prior to achieving competitive classification and diagnostic reasoning
performance.

</details>


### [360] [Modeling AI-Human Collaboration as a Multi-Agent Adaptation](https://arxiv.org/abs/2504.20903)
*Prothit Sen, Sai Mihir Jakkaraju*

Main category: cs.MA

TL;DR: The paper presents an agent-based simulation to study AI-human collaboration, focusing on task structure (modular vs. sequenced) and its impact on performance. Key findings include substitution in modular tasks and complementarity in sequenced tasks, with task structure being the critical factor.


<details>
  <summary>Details</summary>
Motivation: To understand how AI and humans can collaborate effectively in organizational decision-making, emphasizing the role of task structure over context or industry.

Method: Uses an NK model to simulate interactions between heuristic-based human adaptation and rule-based AI search across modular and sequenced tasks.

Result: In modular tasks, AI substitutes humans unless human expertise is high. In sequenced tasks, complementarity emerges when humans initiate and AI refines. AI can also aid low-capability humans by escaping local optima.

Conclusion: Task structure, not context, determines AI-human collaboration effectiveness. The model offers a generalizable framework for strategic decision-making in diverse settings.

Abstract: We develop an agent-based simulation to formalize AI-human collaboration as a
function of task structure, advancing a generalizable framework for strategic
decision-making in organizations. Distinguishing between heuristic-based human
adaptation and rule-based AI search, we model interactions across modular
(parallel) and sequenced (interdependent) tasks using an NK model. Our results
reveal that in modular tasks, AI often substitutes for humans - delivering
higher payoffs unless human expertise is very high, and the AI search space is
either narrowly focused or extremely broad. In sequenced tasks, interesting
complementarities emerge. When an expert human initiates the search and AI
subsequently refines it, aggregate performance is maximized. Conversely, when
AI leads, excessive heuristic refinement by the human can reduce payoffs. We
also show that even "hallucinatory" AI - lacking memory or structure - can
improve outcomes when augmenting low-capability humans by helping escape local
optima. These results yield a robust implication: the effectiveness of AI-human
collaboration depends less on context or industry, and more on the underlying
task structure. By elevating task decomposition as the central unit of
analysis, our model provides a transferable lens for strategic decision-making
involving humans and an agentic AI across diverse organizational settings.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [361] [TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution](https://arxiv.org/abs/2504.20532)
*Yue Li, Weizhi Liu, Dongdong Lin*

Main category: cs.MM

TL;DR: Proposes TriniMark, a robust watermarking method for synthetic speech to protect intellectual property and trace diffusion models.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of robust watermarking for synthetic speech and generative models as deepfake detection becomes less effective.

Method: Uses a lightweight watermark encoder for embedding, a temporal-aware decoder for recovery, and waveform-guided fine-tuning for diffusion models.

Result: Demonstrates superior robustness against attacks, ensuring watermark transferability to surrogate models.

Conclusion: TriniMark effectively safeguards synthetic speech copyrights and enables traceability of generative models.

Abstract: The emergence of diffusion models has facilitated the generation of speech
with reinforced fidelity and naturalness. While deepfake detection technologies
have manifested the ability to identify AI-generated content, their efficacy
decreases as generative models become increasingly sophisticated. Furthermore,
current research in the field has not adequately addressed the necessity for
robust watermarking to safeguard the intellectual property rights associated
with synthetic speech and generative models. To remedy this deficiency, we
propose a \textbf{ro}bust generative \textbf{s}peech wat\textbf{e}rmarking
method (TriniMark) for authenticating the generated content and safeguarding
the copyrights by enabling the traceability of the diffusion model. We first
design a structure-lightweight watermark encoder that embeds watermarks into
the time-domain features of speech and reconstructs the waveform directly. A
temporal-aware gated convolutional network is meticulously designed in the
watermark decoder for bit-wise watermark recovery. Subsequently, the
waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion
model, which leverages the transferability of watermarks and enables the
diffusion model to incorporate watermark knowledge effectively. When an
attacker trains a surrogate model using the outputs of the target model, the
embedded watermark can still be learned by the surrogate model and correctly
extracted. Comparative experiments with state-of-the-art methods demonstrate
the superior robustness of our method, particularly in countering compound
attacks.

</details>


### [362] [ABO: Abandon Bayer Filter for Adaptive Edge Offloading in Responsive Augmented Reality](https://arxiv.org/abs/2504.20370)
*Yongxuan Han, Shengzhong Liu, Fan Wu, Guihai Chen*

Main category: cs.MM

TL;DR: ABO is an adaptive RAW frame offloading framework that optimizes DNN analytics by parallelizing demosaicing with DNN computation, improving throughput, latency, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current demosaicing in AR increases transmission overheads without benefiting DNN analytics, prompting the need for optimization in frame preprocessing.

Method: ABO uses a tile-wise RAW image neural codec, dynamic transmission control, and system pipelining to adaptively compress frames and maximize DNN accuracy under bandwidth constraints.

Result: ABO achieves 40% higher throughput, 30% lower latency, and up to 15% better DNN accuracy than baselines, with robustness in dim lighting and motion blur.

Conclusion: ABO effectively addresses the inefficiencies of traditional demosaicing in DNN offloading, offering significant performance improvements.

Abstract: Bayer-patterned color filter array (CFA) has been the go-to solution for
color image sensors. In augmented reality (AR), although color interpolation
(i.e., demosaicing) of pre-demosaic RAW images facilitates a user-friendly
rendering, it creates no benefits in offloaded DNN analytics but increases the
image channels by 3 times inducing higher transmission overheads. The potential
optimization in frame preprocessing of DNN offloading is yet to be
investigated. To that end, we propose ABO, an adaptive RAW frame offloading
framework that parallelizes demosaicing with DNN computation. Its contributions
are three-fold: First, we design a configurable tile-wise RAW image neural
codec to compress frame sizes while sustaining downstream DNN accuracy under
bandwidth constraints. Second, based on content-aware tiles-in-frame selection
and runtime bandwidth estimation, a dynamic transmission controller adaptively
calibrates codec configurations to maximize the DNN accuracy. Third, we further
optimize the system pipelining to achieve lower end-to-end frame processing
latency and higher throughput. Through extensive evaluations on a prototype
platform, ABO consistently achieves 40% more frame processing throughput and
30% less end-to-end latency while improving the DNN accuracy by up to 15% than
SOTA baselines. It also exhibits improved robustness against dim lighting and
motion blur situations.

</details>


### [363] [TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks](https://arxiv.org/abs/2504.20658)
*Stefano Dell'Anna, Andrea Montibeller, Giulia Boato*

Main category: cs.MM

TL;DR: TrueFake is a large-scale dataset for evaluating fake image detectors under realistic social media conditions, addressing challenges like compression and misinformation.


<details>
  <summary>Details</summary>
Motivation: AI-generated synthetic media spreads misinformation, and current forensic tools often fail under real-world conditions like social media compression.

Method: Created TrueFake, a dataset of 600,000 images from top generative techniques, shared via three social networks, to test detectors.

Result: Analysis shows social media sharing impacts detection performance, revealing effective detection and training strategies.

Conclusion: Forensic models must be evaluated under real-world conditions to improve fake image detection.

Abstract: AI-generated synthetic media are increasingly used in real-world scenarios,
often with the purpose of spreading misinformation and propaganda through
social media platforms, where compression and other processing can degrade fake
detection cues. Currently, many forensic tools fail to account for these
in-the-wild challenges. In this work, we introduce TrueFake, a large-scale
benchmarking dataset of 600,000 images including top notch generative
techniques and sharing via three different social networks. This dataset allows
for rigorous evaluation of state-of-the-art fake image detectors under very
realistic and challenging conditions. Through extensive experimentation, we
analyze how social media sharing impacts detection performance, and identify
current most effective detection and training strategies. Our findings
highlight the need for evaluating forensic models in conditions that mirror
real-world use.

</details>


### [364] [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
*Taoyu Su, Jiawei Sheng, Duohe Ma, Xiaodong Li, Juwei Yue, Mengxiao Song, Yingkai Tang, Tingwen Liu*

Main category: cs.MM

TL;DR: CDMEA is a counterfactual debiasing framework for Multi-Modal Entity Alignment (MMEA) that reduces visual modality bias by leveraging causal effects, outperforming 14 state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing MMEA methods overly rely on visual features, which can bias the model and degrade performance for entities with low-similarity images.

Method: CDMEA uses a causal perspective to estimate and exclude the direct effect of visual modality, focusing on the indirect effects of both visual and graph modalities.

Result: CDMEA outperforms 14 state-of-the-art methods, particularly in low-similarity, high-noise, and low-resource scenarios.

Conclusion: The framework effectively reduces visual modality bias and improves MMEA performance by leveraging causal analysis.

Abstract: Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from
different Multi-Modal Knowledge Graphs (MMKGs), a critical information
retrieval task. Existing studies have explored various fusion paradigms and
consistency constraints to improve the alignment of equivalent entities, while
overlooking that the visual modality may not always contribute positively.
Empirically, entities with low-similarity images usually generate
unsatisfactory performance, highlighting the limitation of overly relying on
visual features. We believe the model can be biased toward the visual modality,
leading to a shortcut image-matching task. To address this, we propose a
counterfactual debiasing framework for MMEA, termed CDMEA, which investigates
visual modality bias from a causal perspective. Our approach aims to leverage
both visual and graph modalities to enhance MMEA while suppressing the direct
causal effect of the visual modality on model predictions. By estimating the
Total Effect (TE) of both modalities and excluding the Natural Direct Effect
(NDE) of the visual modality, we ensure that the model predicts based on the
Total Indirect Effect (TIE), effectively utilizing both modalities and reducing
visual modality bias. Extensive experiments on 9 benchmark datasets show that
CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity,
high-noise, and low-resource data scenarios.

</details>


### [365] [WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution](https://arxiv.org/abs/2504.19595)
*Pietro Bongini, Sara Mandelli, Andrea Montibeller, Mirko Casu, Orazio Pontorno, Claudio Vittorio Ragaglia, Luca Zanchetta, Mattia Aquilina, Taiba Majid Wani, Luca Guarnera, Benedetta Tondi, Giulia Boato, Paolo Bestagini, Irene Amerini, Francesco De Natale, Sebastiano Battiato, Mauro Barni*

Main category: cs.MM

TL;DR: WILD is a dataset for training and benchmarking synthetic image source attribution models, featuring 20,000 images from 20 generators, including post-processed and adversarial examples.


<details>
  <summary>Details</summary>
Motivation: The challenge of synthetic image source attribution due to the growing number of generators and lack of diverse datasets.

Method: Creation of the WILD dataset with closed and open sets of 10 generators each, totaling 20,000 images, half post-processed.

Result: Benchmarking of seven baseline methodologies for closed and open set attribution, including robustness tests.

Conclusion: WILD provides a valuable tool for improving synthetic image attribution models, with baseline results demonstrating its utility.

Abstract: Synthetic image source attribution is an open challenge, with an increasing
number of image generators being released yearly. The complexity and the sheer
number of available generative techniques, as well as the scarcity of
high-quality open source datasets of diverse nature for this task, make
training and benchmarking synthetic image source attribution models very
challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to
provide a powerful training and benchmarking tool for synthetic image
attribution models. The dataset is built out of a closed set of 10 popular
commercial generators, which constitutes the training base of attribution
models, and an open set of 10 additional generators, simulating a real-world
in-the-wild scenario. Each generator is represented by 1,000 images, for a
total of 10,000 images in the closed set and 10,000 images in the open set.
Half of the images are post-processed with a wide range of operators. WILD
allows benchmarking attribution models in a wide range of tasks, including
closed and open set identification and verification, and robust attribution
with respect to post-processing and adversarial attacks. Models trained on WILD
are expected to benefit from the challenging scenario represented by the
dataset itself. Moreover, an assessment of seven baseline methodologies on
closed and open set attribution is presented, including robustness tests with
respect to post-processing.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [366] [Towards Flow-Matching-based TTS without Classifier-Free Guidance](https://arxiv.org/abs/2504.20334)
*Yuzhe Liang, Wenzhe Liu, Chunyu Qiang, Zhikang Niu, Yushen Chen, Ziyang Ma, Wenxi Chen, Nan Li, Chen Zhang, Xie Chen*

Main category: eess.AS

TL;DR: The paper proposes a method to remove Classifier-Free Guidance (CFG) from flow-matching-based TTS models to improve inference efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: CFG incurs high computational costs in flow-matching-based TTS models, hindering real-time applicability.

Method: Reformulate the flow matching training target to approximate the CFG optimization trajectory, eliminating the need for unconditional model evaluation and guided tuning during inference.

Result: Achieves a 9× inference speed-up while preserving comparable speech quality on the LibriTTS dataset.

Conclusion: The method significantly improves efficiency without sacrificing performance and is compatible with existing sampling strategies.

Abstract: Flow matching has demonstrated strong generative capabilities and has become
a core component in modern Text-to-Speech (TTS) systems. To ensure high-quality
speech synthesis, Classifier-Free Guidance (CFG) is widely used during the
inference of flow-matching-based TTS models. However, CFG incurs substantial
computational cost as it requires two forward passes, which hinders its
applicability in real-time scenarios. In this paper, we explore removing CFG
from flow-matching-based TTS models to improve inference efficiency, while
maintaining performance. Specifically, we reformulated the flow matching
training target to directly approximate the CFG optimization trajectory. This
training method eliminates the need for unconditional model evaluation and
guided tuning during inference, effectively cutting the computational overhead
in half. Furthermore, It can be seamlessly integrated with existing optimized
sampling strategies. We validate our approach using the F5-TTS model on the
LibriTTS dataset. Experimental results show that our method achieves a
9$\times$ inference speed-up compared to the baseline F5-TTS, while preserving
comparable speech quality. We will release the code and models to support
reproducibility and foster further research in this area.

</details>


### [367] [ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting](https://arxiv.org/abs/2504.20630)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Tao Jin, Zhou Zhao*

Main category: eess.AS

TL;DR: The paper introduces ISDrama, a model for generating immersive spatial drama from multimodal prompts, addressing challenges like spatial and prosody modeling.


<details>
  <summary>Details</summary>
Motivation: The need for generating continuous multi-speaker binaural speech with dramatic prosody in AR/VR applications, despite high data collection costs.

Method: Proposes ISDrama with a Multimodal Pose Encoder and Immersive Drama Transformer, using contrastive learning and flow-based mamba-transformer for drama generation.

Result: ISDrama outperforms baselines in objective and subjective metrics, demonstrated by the MRSDrama dataset.

Conclusion: ISDrama successfully addresses multimodal spatial drama generation, offering a novel solution with practical applications.

Abstract: Multimodal immersive spatial drama generation focuses on creating continuous
multi-speaker binaural speech with dramatic prosody based on multimodal
prompts, with potential applications in AR, VR, and others. This task requires
simultaneous modeling of spatial information and dramatic prosody based on
multimodal inputs, with high data collection costs. To the best of our
knowledge, our work is the first attempt to address these challenges. We
construct MRSDrama, the first multimodal recorded spatial drama dataset,
containing binaural drama audios, scripts, videos, geometric poses, and textual
prompts. Then, we propose ISDrama, the first immersive spatial drama generation
model through multimodal prompting. ISDrama comprises these primary components:
1) Multimodal Pose Encoder, based on contrastive learning, considering the
Doppler effect caused by moving speakers to extract unified pose information
from multimodal prompts. 2) Immersive Drama Transformer, a flow-based
mamba-transformer model that generates high-quality drama, incorporating
Drama-MOE to select proper experts for enhanced prosody and pose control. We
also design a context-consistent classifier-free guidance strategy to
coherently generate complete drama. Experimental results show that ISDrama
outperforms baseline models on objective and subjective metrics. The demos and
dataset are available at https://aaronz345.github.io/ISDramaDemo.

</details>


### [368] [Design and Analysis of Binaural Signal Matching with Arbitrary Microphone Arrays and Listener Head Rotations](https://arxiv.org/abs/2408.03581)
*Lior Madmoni, Zamir Ben-Hur, Jacob Donley, Vladimir Tourbabin, Boaz Rafaely*

Main category: eess.AS

TL;DR: The paper enhances the Binaural Signal Matching (BSM) method to improve binaural reproduction accuracy, especially for head rotations and high frequencies, using a magnitude least-squares (MagLS) extension and validating it with simulations and listening tests.


<details>
  <summary>Details</summary>
Motivation: The rise of devices like VR headsets and smart glasses necessitates realistic binaural signal generation, but current methods like BSM struggle with head rotations and high frequencies.

Method: The BSM method is analyzed and extended with a MagLS formulation to address high-frequency degradation. Simulations and listening tests with a six-microphone semi-circular array and a four-microphone glasses array validate the approach.

Result: The BSM-MagLS method improves accuracy for head rotations and high frequencies, as confirmed by simulations and a listening experiment in a reverberant speech environment.

Conclusion: BSM-MagLS effectively enhances binaural reproduction quality, making it suitable for immersive audio applications with head rotations.

Abstract: Binaural reproduction is rapidly becoming a topic of great interest in the
research community, especially with the surge of new and popular devices, such
as virtual reality headsets, smart glasses, and head-tracked headphones. In
order to immerse the listener in a virtual or remote environment with such
devices, it is essential to generate realistic and accurate binaural signals.
This is challenging, especially since the microphone arrays mounted on these
devices are typically composed of an arbitrarily-arranged small number of
microphones, which impedes the use of standard audio formats like Ambisonics,
and provides limited spatial resolution. The binaural signal matching (BSM)
method was developed recently to overcome these challenges. While it produced
binaural signals with low error using relatively simple arrays, its performance
degraded significantly when head rotation was introduced. This paper aims to
develop the BSM method further and overcome its limitations. For this purpose,
the method is first analyzed in detail, and a design framework that guarantees
accurate binaural reproduction for relatively complex acoustic environments is
presented. Next, it is shown that the BSM accuracy may significantly degrade at
high frequencies, and thus, a perceptually motivated extension to the method is
proposed, based on a magnitude least-squares (MagLS) formulation. These
insights and developments are then analyzed with the help of an extensive
simulation study of a simple six-microphone semi-circular array. It is further
shown that the BSM-MagLS method can be very useful in compensating for head
rotations with this array. Finally, a listening experiment is conducted with a
four-microphone array on a pair of glasses in a reverberant speech environment
and including head rotations, where it is shown that BSM-MagLS can indeed
produce binaural signals with a high perceived quality.

</details>


### [369] [Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion](https://arxiv.org/abs/2504.08524)
*Na Li, Chuke Wang, Yu Gu, Zhifeng Li*

Main category: eess.AS

TL;DR: A residual block with a Content Feature Re-expression (CFR) module is introduced to mitigate timbre leakage in voice conversion, improving target speaker similarity.


<details>
  <summary>Details</summary>
Motivation: Timbre leakage from the source speaker in voice conversion reduces similarity to the target speaker.

Method: A residual block with two branches: a CFR module for timbre-free content and a skip connection for fine-grained details. The CFR uses a universal semantic dictionary for phoneme-based representation.

Result: The approach reduces timbre leakage and enhances target speaker similarity across various VC frameworks.

Conclusion: The proposed method effectively addresses timbre leakage, improving voice conversion quality.

Abstract: Voice conversion (VC) transforms source speech into a target voice by
preserving the content. However, timbre information from the source speaker is
inherently embedded in the content representations, causing significant timbre
leakage and reducing similarity to the target speaker. To address this, we
introduce a residual block to a content extractor. The residual block consists
of two weighted branches: 1) universal semantic dictionary based Content
Feature Re-expression (CFR) module, supplying timbre-free content
representation. 2) skip connection to the original content layer, providing
complementary fine-grained information. In the CFR module, each dictionary
entry in the universal semantic dictionary represents a phoneme class, computed
statistically using speech from multiple speakers, creating a stable,
speaker-independent semantic set. We introduce a CFR method to obtain
timbre-free content representations by expressing each content frame as a
weighted linear combination of dictionary entries using corresponding phoneme
posteriors as weights. Extensive experiments across various VC frameworks
demonstrate that our approach effectively mitigates timbre leakage and
significantly improves similarity to the target speaker.

</details>


### [370] [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
*Yu Zhang, Wenxiang Guo, Changhao Pan, Zhiyuan Zhu, Ruiqi Li, Jingyu Lu, Rongjie Huang, Ruiyuan Zhang, Zhiqing Hong, Ziyue Jiang, Zhou Zhao*

Main category: eess.AS

TL;DR: VersBand is a multi-task song generation framework that addresses challenges in prompt-based control and alignment of vocals and accompaniments, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack proper alignment and control in song generation, prompting the need for a versatile solution.

Method: VersBand includes VocalBand for vocals, AccompBand for accompaniments, and additional models for lyrics and melodies, leveraging flow-matching and transformer techniques.

Result: VersBand outperforms baseline models in quality, alignment, and control across multiple tasks.

Conclusion: VersBand provides a comprehensive solution for high-quality, controllable song generation with proper alignment.

Abstract: Song generation focuses on producing controllable high-quality songs based on
various prompts. However, existing methods struggle to generate vocals and
accompaniments with prompt-based control and proper alignment. Additionally,
they fall short in supporting various tasks. To address these challenges, we
introduce VersBand, a multi-task song generation framework for synthesizing
high-quality, aligned songs with prompt-based control. VersBand comprises these
primary models: 1) VocalBand, a decoupled model, leverages the flow-matching
method for generating singing styles, pitches, and mel-spectrograms, allowing
fast, high-quality vocal generation with style control. 2) AccompBand, a
flow-based transformer model, incorporates the Band-MOE, selecting suitable
experts for enhanced quality, alignment, and control. This model allows for
generating controllable, high-quality accompaniments aligned with vocals. 3)
Two generation models, LyricBand for lyrics and MelodyBand for melodies,
contribute to the comprehensive multi-task song generation system, allowing for
extensive control based on multiple prompts. Experimental results demonstrate
that VersBand performs better over baseline models across multiple song
generation tasks using objective and subjective metrics. Audio samples are
available at https://aaronz345.github.io/VersBandDemo.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [371] [SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses](https://arxiv.org/abs/2504.20405)
*Sahil Sethi, Sai Reddy, Mansi Sakarvadia, Jordan Serotte, Darlington Nwaudo, Nicholas Maassen, Lewis Shi*

Main category: eess.IV

TL;DR: Deep learning models for detecting Bankart lesions on standard MRIs and MRAs achieve radiologist-level performance, reducing reliance on invasive MRAs. The study introduces ScopeMRI, a public dataset, and a modular codebase to advance research in musculoskeletal imaging.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning work in musculoskeletal imaging focuses on less challenging pathologies, leaving difficult cases like Bankart lesions underexplored. Diagnosing these lesions is hard due to subtle features, often requiring invasive MRAs.

Method: A deep learning framework using CNNs and transformers was developed, with models trained on ScopeMRI (586 shoulder MRIs). Predictions from sagittal, axial, and coronal views were ensembled.

Result: Models achieved AUCs of 0.91 (standard MRIs) and 0.93 (MRAs), with high sensitivity and specificity. Performance on standard MRIs matched or surpassed radiologists interpreting MRAs.

Conclusion: DL models can replace invasive MRAs for Bankart lesion diagnosis. The release of ScopeMRI and codebase aims to spur research in musculoskeletal imaging for challenging tasks.

Abstract: While deep learning has shown strong performance in musculoskeletal imaging,
existing work has largely focused on pathologies where diagnosis is not a
clinical challenge, leaving more difficult problems underexplored, such as
detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard
MRIs. Diagnosing these lesions is challenging due to their subtle imaging
features, often leading to reliance on invasive MRI arthrograms (MRAs). This
study introduces ScopeMRI, the first publicly available, expert-annotated
dataset for shoulder pathologies, and presents a deep learning (DL) framework
for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes
586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent
arthroscopy. Ground truth labels were derived from intraoperative findings, the
gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were
trained using a combination of CNNs and transformers. Predictions from
sagittal, axial, and coronal views were ensembled to optimize performance. The
models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71
standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83%
and 94%, and specificity of 91% and 86% for standard MRIs and MRAs,
respectively. Notably, model performance on non-invasive standard MRIs matched
or surpassed radiologists interpreting MRAs. External validation demonstrated
initial generalizability across imaging protocols. This study demonstrates that
DL models can achieve radiologist-level diagnostic performance on standard
MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular
codebase for training and evaluating deep learning models on 3D medical imaging
data, we aim to accelerate research in musculoskeletal imaging and support the
development of new datasets for clinically challenging diagnostic tasks.

</details>


### [372] [LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight](https://arxiv.org/abs/2504.20454)
*Jiajun Ding, Beiyao Zhu, Xiaosheng Liu, Lishen Zhang, Zhao Liu*

Main category: eess.IV

TL;DR: The study creates a 3D multimodal segmentation dataset for lymphoma using PET/CT data, validated for accurate lesion segmentation via deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized multimodal segmentation datasets for haematological malignancies.

Method: Retrospective collection of 483 PET/CT datasets, preprocessing, annotation, and validation using nnUNet and deep learning models.

Result: High accuracy, robustness, and reproducibility in lymphoma lesion segmentation, aiding diagnosis and treatment.

Conclusion: The dataset enhances PET/CT fusion for precise tumor analysis, supporting automated segmentation and precision medicine.

Abstract: This study integrates PET metabolic information with CT anatomical structures
to establish a 3D multimodal segmentation dataset for lymphoma based on
whole-body FDG PET/CT examinations, which bridges the gap of the lack of
standardised multimodal segmentation datasets in the field of haematological
malignancies. We retrospectively collected 483 examination datasets acquired
between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin
lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were
rigorously de-identified. Complete 3D structural information was preserved
during data acquisition, preprocessing and annotation, and a high-quality
dataset was constructed based on the nnUNet format. By systematic technical
validation and evaluation of the preprocessing process, annotation quality and
automatic segmentation algorithm, the deep learning model trained based on this
dataset is verified to achieve accurate segmentation of lymphoma lesions in
PET/CT images with high accuracy, good robustness and reproducibility, which
proves the applicability and stability of this dataset in accurate segmentation
and quantitative analysis. The deep fusion of PET/CT images achieved with this
dataset not only significantly improves the accurate portrayal of the
morphology, location and metabolic features of tumour lesions, but also
provides solid data support for early diagnosis, clinical staging and
personalized treatment, and promotes the development of automated image
segmentation and precision medicine based on deep learning. The dataset and
related resources are available at https://github.com/SuperD0122/LymphAtlas-.

</details>


### [373] [Full-field surrogate modeling of cardiac function encoding geometric variability](https://arxiv.org/abs/2504.20479)
*Elena Martinez, Beatrice Moscoloni, Matteo Salvador, Fanwei Kong, Mathias Peirlinck, Alison Lesley Marsden*

Main category: eess.IV

TL;DR: The paper proposes a computational pipeline combining physics-based modeling and machine learning to create generalizable surrogate models for cardiac function, tested on pediatric patients with Tetralogy of Fallot.


<details>
  <summary>Details</summary>
Motivation: To enable clinical translation of computational methods in cardiology by addressing the need for patient-specific, efficient models with uncertainty quantification.

Method: Uses Branched Latent Neural Maps (BLNMs) to encode activation maps from simulations, builds an anatomical atlas, and generates a synthetic cohort for training.

Result: The surrogate model achieves robust generalization with an average error of 0.0034.

Conclusion: The approach successfully generalizes across complex patient anatomies, with open-source implementation available.

Abstract: Combining physics-based modeling with data-driven methods is critical to
enabling the translation of computational methods to clinical use in
cardiology. The use of rigorous differential equations combined with machine
learning tools allows for model personalization with uncertainty quantification
in time frames compatible with clinical practice. However, accurate and
efficient surrogate models of cardiac function, built from physics-based
numerical simulation, are still mostly geometry-specific and require retraining
for different patients and pathological conditions. We propose a novel
computational pipeline to embed cardiac anatomies into full-field surrogate
models. We generate a dataset of electrophysiology simulations using a complex
multi-scale mathematical model coupling partial and ordinary differential
equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective
scientific machine learning method to encode activation maps extracted from
physics-based numerical simulations into a neural network. Leveraging large
deformation diffeomorphic metric mappings, we build a biventricular anatomical
atlas and parametrize the anatomical variability of a small and challenging
cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a
novel statistical shape modeling based z-score sampling approach to generate a
new synthetic cohort of 52 biventricular geometries that are compatible with
the original geometrical variability. This synthetic cohort acts as the
training set for BLNMs. Our surrogate model demonstrates robustness and great
generalization across the complex original patient cohort, achieving an average
adimensional mean squared error of 0.0034. The Python implementation of our
BLNM model is publicly available under MIT License at
https://github.com/StanfordCBCL/BLNM.

</details>


### [374] [SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation](https://arxiv.org/abs/2504.20501)
*Jia Wang, Yunan Mei, Jiarui Liu, Xin Fan*

Main category: eess.IV

TL;DR: RRL-MedSAM adapts SAM for one-shot 3D medical image segmentation using knowledge distillation and mutual-EMA, achieving superior performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SAM in one-shot MIS, such as labor-intensive interactions and high computational costs.

Method: Proposes RRL-MedSAM with dual-stage knowledge distillation, mutual-EMA for encoder updates, and auto-prompting segmentation decoder.

Result: Outperforms state-of-the-art methods on OASIS and CT-lung datasets, using only 3% of SAM-Base's encoder parameters.

Conclusion: RRL-MedSAM effectively adapts SAM for one-shot MIS, offering a lightweight and efficient solution.

Abstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due
to the burden of medical experts on manual annotation. The recent emergence of
the segment anything model (SAM) has demonstrated remarkable adaptation in MIS
but cannot be directly applied to one-shot medical image segmentation (MIS) due
to its reliance on labor-intensive user interactions and the high computational
cost. To cope with these limitations, we propose a novel SAM-guided robust
representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot
3D MIS, which exploits the strong generalization capabilities of the SAM
encoder to learn better feature representation. We devise a dual-stage
knowledge distillation (DSKD) strategy to distill general knowledge between
natural and medical images from the foundation model to train a lightweight
encoder, and then adopt a mutual exponential moving average (mutual-EMA) to
update the weights of the general lightweight encoder and medical-specific
encoder. Specifically, pseudo labels from the registration network are used to
perform mutual supervision for such two encoders. Moreover, we introduce an
auto-prompting (AP) segmentation decoder which adopts the mask generated from
the general lightweight model as a prompt to assist the medical-specific model
in boosting the final segmentation performance. Extensive experiments conducted
on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed
RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both
segmentation and registration tasks. Especially, our lightweight encoder uses
only 3\% of the parameters compared to the encoder of SAM-Base.

</details>


### [375] [Quality-factor inspired deep neural network solver for solving inverse scattering problems](https://arxiv.org/abs/2504.20504)
*Yutong Du, Zicheng Liu, Miao Cao, Zupeng Liang, Yali Zong, Changyou Li*

Main category: eess.IV

TL;DR: The paper proposes a deep neural network (QuaDNN) for electromagnetic inverse scattering problems, optimizing data quality, network architecture, and loss function to improve imaging performance.


<details>
  <summary>Details</summary>
Motivation: To enhance imaging performance in electromagnetic inverse scattering problems by addressing limitations in training data, network architecture, and loss function.

Method: Optimizes training dataset composition using a quality factor, integrates residual connections and channel attention in the network, and designs a loss function combining data-fitting, physical constraints, and solution features.

Result: Demonstrates superior performance in numerical analyses and experimental tests, reducing artifacts and improving reconstruction accuracy.

Conclusion: The QuaDNN solver effectively improves imaging performance in electromagnetic inverse scattering problems through optimized data, architecture, and loss function.

Abstract: Deep neural networks have been applied to address electromagnetic inverse
scattering problems (ISPs) and shown superior imaging performances, which can
be affected by the training dataset, the network architecture and the applied
loss function. Here, the quality of data samples is cared and valued by the
defined quality factor. Based on the quality factor, the composition of the
training dataset is optimized. The network architecture is integrated with the
residual connections and channel attention mechanism to improve feature
extraction. A loss function that incorporates data-fitting error,
physical-information constraints and the desired feature of the solution is
designed and analyzed to suppress the background artifacts and improve the
reconstruction accuracy. Various numerical analysis are performed to
demonstrate the superiority of the proposed quality-factor inspired deep neural
network (QuaDNN) solver and the imaging performance is finally verified by
experimental imaging test.

</details>


### [376] [Imaging on the Edge: Mapping Object Corners and Edges with Stereo X-ray Tomography](https://arxiv.org/abs/2504.20892)
*Zhenduo Shang, Thomas Blumensath*

Main category: eess.IV

TL;DR: The paper extends stereo X-ray imaging to map sharp object corners and uses synthetic data for training, improving 3D mapping without real annotated data.


<details>
  <summary>Details</summary>
Motivation: Current X-ray tomography is slow for dynamic processes; stereo X-ray imaging with two projections offers faster temporal resolution.

Method: Extends stereo X-ray imaging to map sharp object corners and replaces real training data with synthetic data mimicking real aspects.

Result: Demonstrates 3D mapping of sharp corners and successful application to real stereo X-ray data without annotated training data.

Conclusion: The method enhances 3D mapping efficiency and applicability, especially for dynamic processes and manufactured component deformation.

Abstract: X-ray computed tomography is a powerful tool for volumetric imaging, where
three-dimensional (3D) images are generated from a large number of individual
X-ray projection images. Collecting the required number of low noise projection
images is however time-consuming and so the technique is not currently
applicable when spatial information needs to be collected with high temporal
resolution, such as in the study of dynamic processes. In our previous work,
inspired by stereo vision, we developed stereo X-ray imaging methods that
operate with only two X-ray projection images. Previously we have shown how
this allowed us to map point and line fiducial markers into 3D space at
significantly faster temporal resolutions. In this paper, we make two further
contributions. Firstly, instead of utilising internal fiducial markers, we
demonstrate the applicability of the method to the 3D mapping of sharp object
corners, a problem of interest in measuring the deformation of manufactured
components under different loads. Furthermore, we demonstrate how the approach
can be applied to real stereo X-ray data, even in settings where we do not have
the annotated real training data that was required for the training of our
previous Machine Learning approach. This is achieved by substituting the real
data with a relatively simple synthetic training dataset designed to mimic key
aspects of the real data.

</details>


### [377] [Ring deconvolution microscopy: exploiting symmetry for efficient spatially varying aberration correction](https://arxiv.org/abs/2206.08928)
*Amit Kohli, Anastasios N. Angelopoulos, David McAllister, Esther Whang, Sixian You, Kyrollos Yanny, Federico M. Gasparoli, Bo-Jui Chang, Reto Fiolka, Laura Waller*

Main category: eess.IV

TL;DR: A new imaging pipeline, Ring Deconvolution Microscopy (RDM), leverages symmetry for fast, spatially-varying aberration correction, outperforming standard deconvolution in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Standard deconvolution assumes uniform point spread function, which is often inadequate. Space-variant deblurring methods are impractical due to calibration and computation demands.

Method: RDM uses rotational symmetry of microscopes and cameras, extending to lateral symmetry for sheet deconvolution. Includes theory, algorithms, and a neural network based on Seidel coefficients for fast recovery.

Result: Demonstrates significant improvements in speed and image quality across various microscope modalities, achieving near-isotropic, subcellular resolution.

Conclusion: RDM provides a practical, efficient solution for spatially-varying aberration correction, enhancing microscopy applications.

Abstract: The most ubiquitous form of computational aberration correction for
microscopy is deconvolution. However, deconvolution relies on the assumption
that the point spread function is the same across the entire field-of-view.
This assumption is often inadequate, but space-variant deblurring techniques
generally require impractical amounts of calibration and computation. We
present a new imaging pipeline that leverages symmetry to provide simple and
fast spatially-varying aberration correction. Our ring deconvolution microscopy
(RDM) method leverages the rotational symmetry of most microscopes and cameras,
and naturally extends to sheet deconvolution in the case of lateral symmetry.
We formally derive theory and algorithms for image recovery and additionally
propose a neural network based on Seidel coefficients as a fast alternative, as
well as extension of RDM to blind deconvolution. We demonstrate significant
improvements in speed and image quality as compared to standard deconvolution
and existing spatially-varying deconvolution across a diverse range of
microscope modalities, including miniature microscopy, multicolor fluorescence
microscopy, point-scanning multimode fiber micro-endoscopy, and light-sheet
fluorescence microscopy. Our approach enables near-isotropic, subcellular
resolution in each of these applications.

</details>


### [378] [3D ReX: Causal Explanations in 3D Neuroimaging Classification](https://arxiv.org/abs/2502.12181)
*Melane Navaratnarajah, Sophie A. Martin, David A. Kelly, Nathan Blake, Hana Chockler*

Main category: eess.IV

TL;DR: 3D ReX is a causality-based explainability tool for 3D medical imaging models, generating responsibility maps to highlight decision-critical regions.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of trust in AI-driven medical predictions due to poor explainability.

Method: Uses actual causality theory to create responsibility maps for 3D models, tested on stroke detection.

Result: Provides spatial insights into features relevant to stroke detection.

Conclusion: 3D ReX enhances trust in AI by improving explainability for medical imaging models.

Abstract: Explainability remains a significant problem for AI models in medical
imaging, making it challenging for clinicians to trust AI-driven predictions.
We introduce 3D ReX, the first causality-based post-hoc explainability tool for
3D models. 3D ReX uses the theory of actual causality to generate
responsibility maps which highlight the regions most crucial to the model's
decision. We test 3D ReX on a stroke detection model, providing insight into
the spatial distribution of features relevant to stroke.

</details>


### [379] [RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments](https://arxiv.org/abs/2503.04821)
*Zelin Meng, Takanori Fukao*

Main category: eess.IV

TL;DR: RTFusion, a multimodal depth estimation model, combines RGB and thermal infrared (THR) data for improved accuracy and robustness in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Single-modality depth estimation struggles in adverse conditions; RGB and THR data offer complementary strengths.

Method: Uses EGFusion with Mutual Complementary Attention (MCA) and Edge Saliency Enhancement Module (ESEM) for cross-modal alignment and edge preservation.

Result: Outperforms on MS2 and ViViD++ datasets, excelling in nighttime, rainy, and high-glare conditions.

Conclusion: RTFusion shows promise for reliable depth estimation in autonomous driving, robotics, and augmented reality.

Abstract: Depth estimation in complex real-world scenarios is a challenging task,
especially when relying solely on a single modality such as visible light or
thermal infrared (THR) imagery. This paper proposes a novel multimodal depth
estimation model, RTFusion, which enhances depth estimation accuracy and
robustness by integrating the complementary strengths of RGB and THR data. The
RGB modality provides rich texture and color information, while the THR
modality captures thermal patterns, ensuring stability under adverse lighting
conditions such as extreme illumination. The model incorporates a unique fusion
mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)
module for cross-modal feature alignment and the Edge Saliency Enhancement
Module (ESEM) to improve edge detail preservation. Comprehensive experiments on
the MS2 and ViViD++ datasets demonstrate that the proposed model consistently
produces high-quality depth maps across various challenging environments,
including nighttime, rainy, and high-glare conditions. The experimental results
highlight the potential of the proposed method in applications requiring
reliable depth estimation, such as autonomous driving, robotics, and augmented
reality.

</details>


### [380] [Adaptive Weight Modified Riesz Mean Filter For High-Density Salt and Pepper Noise Removal](https://arxiv.org/abs/2504.18251)
*Md Jahidul Islam*

Main category: eess.IV

TL;DR: AWMRmF outperforms other filters in removing high-density salt and pepper noise, achieving higher PSNR and SSIM scores.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective filter for high-density salt and pepper noise removal, improving upon existing methods.

Method: Integrates a pixel weight function and adaptivity condition, tested against established filters on 26 test images with 60%-95% noise levels.

Result: AWMRmF achieved superior PSNR and SSIM metrics compared to other filters.

Conclusion: AWMRmF is highly effective for high-density SPN removal, outperforming state-of-the-art alternatives.

Abstract: This paper introduces a novel filter, the Adaptive Weight Modified Riesz Mean
Filter (AWMRmF), designed for the effective removal of high-density salt and
pepper noise (SPN). AWMRmF integrates a pixel weight function and adaptivity
condition inspired by the Different Adaptive Modified Riesz Mean Filter
(DAMRmF). In my simulations, I evaluated the performance of AWMRmF against
established filters such as Adaptive Frequency Median Filter (AFMF), Adaptive
Weighted Mean Filter (AWMF), Adaptive Cesaro Mean Filter (ACmF), Adaptive Riesz
Mean Filter (ARmF), and Improved Adaptive Weighted Mean Filter (IAWMF). The
assessment was conducted on 26 typical test images, varying noise levels from
60% to 95%. The findings indicate that, in terms of both Peak Signal to Noise
Ratio (PSNR) and Structural Similarity (SSIM) metrics, AWMRmF outperformed
other state-of-the-art filters. Furthermore, AWMRmF demonstrated superior
performance in mean PSNR and SSIM results as well.

</details>


### [381] [Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction](https://arxiv.org/abs/2504.19203)
*Ehsan Karami, Hamid Soltanian-Zadeh*

Main category: eess.IV

TL;DR: The paper improves deep learning model generalization for knee osteoarthritis prediction by using instance normalization, data augmentation, and contrastive loss, achieving better accuracy across different MRI data sources.


<details>
  <summary>Details</summary>
Motivation: MRI-based deep learning models for KOA prediction struggle with generalizability across different imaging sources.

Method: Replaced batch normalization with instance normalization, used data augmentation, and applied contrastive loss in a baseline deep learning model.

Result: Statistically significant improvement in classification accuracy across source and target domains, outperforming the baseline.

Conclusion: The proposed enhancements improve model generalization for KOA prediction, making it more robust across diverse MRI data.

Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and
mobility issues. While MRI-based deep learning models have demonstrated
superior performance in predicting total knee replacement (TKR) and disease
progression, their generalizability remains challenging, particularly when
applied to imaging data from different sources. In this study, we have shown
that replacing batch normalization with instance normalization, using data
augmentation, and applying contrastive loss improves model generalization in a
baseline deep learning model for knee osteoarthritis (KOA) prediction. We
trained and evaluated our model using MRI data from the Osteoarthritis
Initiative (OAI) database, considering sagittal fat-suppressed
intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain
and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state
(DESS) images as the target domain. The results demonstrate a statistically
significant improvement in classification accuracy across both domains, with
our approach outperforming the baseline model.

</details>
